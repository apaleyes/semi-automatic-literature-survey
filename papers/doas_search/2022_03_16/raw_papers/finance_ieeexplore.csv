doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/IT48810.2020.9070689,A Private Blockchain Implementation Using Multichain Open Source Platform,IEEE,Conferences,"The impact of digital transformation is becoming visible in every aspect of our lives. Digital transformation strategies mostly rely on disruptive technologies such as Internet of Things, augmented reality, artificial intelligence, and blockchain. Blockchain provides decentralized, immutable, and trustless database distributed across all participants and it was initially proposed as distributed ledger for digital cryptocurrency. However, besides in the finance sector, blockchain technology is considered to be a major innovation enabler in various industries. The paper describes an implementation of a private blockchain using the Multichain open source platform with possible application to agrifood use cases such as food tracking and tracing, product lifecycle management, and counterfeit prevention.",https://ieeexplore.ieee.org/document/9070689/,2020 24th International Conference on Information Technology (IT),18-22 Feb. 2020,ieeexplore
10.1109/ICISS49785.2020.9316056,A Proposal for a novel approach to analyze and detect the fake news using AI techniques,IEEE,Conferences,"Fake news is a type of news that contains deliberately incorrect information and is spread with malicious intent in fields that include and are not limited to politics, finance and entertainment. Fake news and misinformation are abundant these days due to increased access to the internet and social media websites and other sharing platforms. These can have harmful effects on society and its spread must be contained. The first step in containing the proliferation of fake news is to identify a piece of news as fake or real. The goal of our system is to train it with articles using reliable news sources using web mining techniques and subsequently analyze a piece of news using natural language processing and machine learning algorithms to classify it as fake or real.",https://ieeexplore.ieee.org/document/9316056/,2020 3rd International Conference on Intelligent Sustainable Systems (ICISS),3-5 Dec. 2020,ieeexplore
10.1109/VTCSpring.2014.7022797,A Standardized Path Loss Model for the GSM-Railway Based High-Speed Railway Communication Systems,IEEE,Conferences,"High-speed railway (HSR) has been widely introduced to meet the increasing demand for passenger rail travel. While it provides more and more conveniences to the people, the huge cost of the HSR has laid big burden on the government finance. Reducing the cost of HSR has been necessary and urgent. Optimizing the arrangement of the base stations (BS) by improving the prediction of the communication link is one of the most effective methods, which could reduce the number of the BSs to a reasonable number. However, it requires a carefully developed propagation model, which has been largely neglected before in the research on the HSR. In this paper, we propose a standardized path loss model for HSR channels based on an extensive measurement campaign in 4594 HSR cells of the ""Zhengzhou-Xian"" HSR line. The measurements are conducted using a practically deployed and operative GSM-Railway (GSM-R) system to reflect the real conditions of the HSR channels. The proposed model is validated by the measurements conducted in another operative railway - ""Beijing-Shanghai"" HSR line. The results are helpful for the HSR communications system designers to gain a better tool in the system planning, and propagation researchers to assess where the most pressing needs in the modeling of HSR channels lie.",https://ieeexplore.ieee.org/document/7022797/,2014 IEEE 79th Vehicular Technology Conference (VTC Spring),18-21 May 2014,ieeexplore
10.1109/ISPA.2012.145,Anomaly Detection Algorithms on IBM InfoSphere Streams: Anomaly Detection for Data in Motion,IEEE,Conferences,"This paper presents and shares excerpts from our implementation of near real-time anomaly detection algorithms on the IBM InfoSphere Streams platform. The purpose of this article is to: 1) Describe how to design and implement known anomaly detection algorithms on IBM InfoSphere Streams. 2) Present some performance optimization capabilities of IBM InfoSphere Streams platform and propose a method to use them in anomaly detection applications. 3) Present some IBM InfoSphere Streams best practices and describe how their adoption in the context of anomaly detection application. The document describes the architecture and design of anomaly detection algorithms developed on IBM InfoSphere Streams. Although the solution was designed to be used for cyber security, the implemented algorithms are agnostic regarding the data type that they monitor and therefore can detect anomalies in data from various industries such as healthcare, finance and retail. The document describes the implementation of two anomaly detection algorithms: KOAD and PCA. The KOAD algorithm performs online anomaly detection with incremental learning and the PCA algorithm in performs offline anomaly detection. The solution was designed to provide near real-time insight into low latency on large data volume observation.",https://ieeexplore.ieee.org/document/6280306/,2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications,10-13 July 2012,ieeexplore
10.1109/Confluence51648.2021.9377160,Application of Artificial Intelligence in Human Resource Management Practices,IEEE,Conferences,"In the dynamic and competitive world, technology has changed the pace of all the industry. Artificial intelligence is a technology which enables the industry to grow at faster pace and efficiently finishing their work. This technology has entered into various departments such as finance department, human resource department, marketing, production etc. AI system has enabled the organization to enhance their existing performance and efficiently performing functions on a day-to-day basis. Currently, due to dynamic and competitive environment people working at different managerial level are working under pressure and understanding the need of artificial intelligence at workplace. Authors have used quantitative research to conduct the research and regression methods has been used to analyse the data. AI as a technology has a role in the different HR practices starting from talent acquisition and extending it to the assessing the performance of the people at work place. This research will study the relation of artificial intelligence and HR functions and different functions performed by HR department. The objective is to understand the factor like innovativeness and how use of HR operations. To conduct the study HR professionals from different IT companies were considered. Through the analysis the result indicated the positive linkage between different factors such ease of use and innovativeness which clearly indicates that AI has a influence on both the factors. This research paper will provide indepth knowledge about artificial intelligence which is at present a new revolution in industry and referred as industry 4.0.",https://ieeexplore.ieee.org/document/9377160/,"2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",28-29 Jan. 2021,ieeexplore
10.1109/IGSC51522.2020.9291232,Conversion of an Unsupervised Anomaly Detection System to Spiking Neural Network for Car Hacking Identification,IEEE,Conferences,"Across industry, there is an increasing availability of streaming, time-varying data, where it is important to detect anomalous behavior. These data are found in an enormous number of sensor-based applications, in cybersecurity (where anomalous behavior could indicate an attack), and in finance. Spiking Neural Networks (SNNs) have come under the spotlight for machine learning applications due to the extreme energy efficiency of their implementation on neuromorphic processors like the Intel Loihi research chip. In this paper we explore the applicability of spiking neural networks for in vehicle cyberattack detection. We show exemplary results by converting an autoencoder model to spiking form. We present a learning model comparison that shows the proposed SNN autoencoder outperforms a One Class Support Vector Machine and an Isolation Forest. Furthermore, only a slight reduction in accuracy is observed when compared to a traditional autoencoder.",https://ieeexplore.ieee.org/document/9291232/,2020 11th International Green and Sustainable Computing Workshops (IGSC),19-22 Oct. 2020,ieeexplore
10.1109/IJCNN.2018.8489360,Cross-Domain Deep Learning Approach For Multiple Financial Market Prediction,IEEE,Conferences,"Over recent decades, globalization has resulted in a steady increase in cross-border financial flows around the world. To build an abstract representation of a real-world financial market situation, we structure the fundamental influences among homogeneous and heterogeneous markets with three types of correlations: the inner-domain correlation between homogeneous markets in various countries, the cross-domain correlation between heterogeneous markets, and the time-series correlation between current and past markets. Such types of correlations in global finance challenge traditional machine learning approaches due to model complexity and nonlinearity. In this paper, we propose a novel cross-domain deep learning approach (Cd-DLA) to learn real-world complex correlations for multiple financial market prediction. Based on recurrent neural networks, which capture the time-series interactions in financial data, our model utilizes the attention mechanism to analyze the inner-domain and cross-domain correlations, and then aggregates all of them for financial forecasting. Experiment results on ten-year financial data on currency and stock markets from three countries prove the performance of our approach over other baselines.",https://ieeexplore.ieee.org/document/8489360/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore
10.1109/IJCNN48605.2020.9206730,Data-efficient Online Classification with Siamese Networks and Active Learning,IEEE,Conferences,"An ever increasing volume of data is nowadays becoming available in a streaming manner in many application areas, such as, in critical infrastructure systems, finance and banking, security and crime and web analytics. To meet this new demand, predictive models need to be built online where learning occurs on-the-fly. Online learning poses important challenges that affect the deployment of online classification systems to real-life problems. In this paper we investigate learning from limited labelled, nonstationary and imbalanced data in online classification. We propose a learning method that synergistically combines siamese neural networks and active learning. The proposed method uses a multi-sliding window approach to store data, and maintains separate and balanced queues for each class. Our study shows that the proposed method is robust to data nonstationarity and imbalance, and significantly outperforms baselines and state-of-the-art algorithms in terms of both learning speed and performance. Importantly, it is effective even when only 1% of the labels of the arriving instances are available.",https://ieeexplore.ieee.org/document/9206730/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/ITAIC49862.2020.9339148,Design and Implementation of Network Course-choosing Based on the F5 Load Balancing Technology,IEEE,Conferences,"The paper based the exploration of improving the strategy of large amount of concurrent processing during the network course-choosing on the existing network condition of Yunnan University of Finance and Economics, which combined network with server load balancing by F5_BIG_IP LTM, in order to solve the problem that server overload can't response to the request of the students may be coursed by a large number of students access the elective course platform concurrently. This architecture greatly solved the problem of network congestion coursed by concurrency access in the process of actual practice. Load balancing technology service provides a reliable and stable architecture for increasing Web application service and a reference scheme for real-timely dealing with the congestion problem online of concurrency access.",https://ieeexplore.ieee.org/document/9339148/,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),11-13 Dec. 2020,ieeexplore
10.1109/ICECA.2019.8821811,Detection of Permission Driven Malware in Android Using Deep Learning Techniques,IEEE,Conferences,"In today's busy world, usage of mobile applications is increasing in all aspects of life including banking and finance. Taking this as an opportunity, cyber crimes are taking place in the form of hacking and malware. While installing the mobile apps, everyone may not be aware of which permissions to accept and which one to deny. If the user start accepting all permissions, malware or malicious apk files may enter the mobile through some of them. Many machine learning techniques have been introduced to resolve this problem but failed to get considerable accuracy in real-time applications. The present research concentrates on detecting the malware which can enter through permissions in android using deep neural network model. The proposed approach detects the permission driven malware in real time android apk files with more than 85% accuracy.",https://ieeexplore.ieee.org/document/8821811/,"2019 3rd International conference on Electronics, Communication and Aerospace Technology (ICECA)",12-14 June 2019,ieeexplore
10.1109/TELFOR48224.2019.8971360,Development of intelligent systems and application of gamification in artificial intelligent learning,IEEE,Conferences,"CToday, intelligent systems are used in many fields - medicine, agriculture, transport, telecommunications, industrial process management and control, finance, commerce, the computer game industry and many others. This paper describes a complete way to develop an intelligent software system for simulation and visualization of artificial intelligence algorithms. The system includes artificial intelligence algorithms from basic search strategies and game theory, inference algorithms and knowledge representation models, to advanced search techniques, machine and inductive learning. The application of these algorithms to real everyday problems and the application of this software system as an auxiliary tool for analysis of input data and inference in various fields are presented. The system is also applicable to education in an introductory artificial intelligence course at the university, so the last phase of the research involved the transition of the software system into a game-based tool and application of gamification.",https://ieeexplore.ieee.org/document/8971360/,2019 27th Telecommunications Forum (TELFOR),26-27 Nov. 2019,ieeexplore
10.1109/ITNT52450.2021.9649038,"Digital twin of rice as a decision-making service for precise farming, based on environmental datasets from the fields",IEEE,Conferences,"In this paper a ready-to-use software component, which simulates real state of rice crop in the field and called “Digital Twin of rice” (DT), is studied. DT uses ontology-based knowledge base of plant cultivation to execute the rules of plant growth. The software provides real-time data collection from the fields and distributed decision making to find the optimum solution in planning process of rice growth stages. Rice DT is developed as an autonomous service and can be integrated to any existing digital agricultural platform. A pilot integration with cyber-physical system (CPS) for precise farming is described in the paper. The CPS has a number of services to provide digital transformation in plant cultivation enterprises and big farms. The system performs adaptive scheduling of resources, such as fertilizers, protection agents, vehicles, personnel and finance. Results of DT implementation shows adequate decision-making of the service compared to experiments on the pilot farms. So, DT of plant could be a next step in digital transformation of agriculture, providing improvement of ROI from precision farming, automate decision-making processes for farmers and service companies and make their business smarter, more flexible, and more cost-efficient, providing better productivity of plant cultivation and sustainability of agriculture under global climate changes.",https://ieeexplore.ieee.org/document/9649038/,2021 International Conference on Information Technology and Nanotechnology (ITNT),20-24 Sept. 2021,ieeexplore
10.1109/DESSERT.2018.8409186,Digitization of the economy of Ukraine: Strategic challenges and implementation technologies,IEEE,Conferences,"The main directions, challenges, threats of digitization of the national economy of Ukraine have been considered in the paper. The attention is focused on the found weaknesses and the imperfection of the strategy and the state policy of digitization of Ukraine's economy. The authors have proven the potential and new possibilities of solving public finance management problems with the usage of blockchain technology. It has been justified that activation of transformation processes in the real economy sector due to the introduction of Industry 4.0 concept is important for Ukraine. The paper reveals basic principles and technologies, the experience of the European Union, and characterizes Industry 4.0 view in Ukraine. The development of the latest financial technologies - FinTech - has been recognized as the driver of digital transformation of financial services. The types of FinTech innovations, the features of increasing competition between FinTech companies and traditional financial intermediaries, the tendencies of FinTech development in Ukraine have been characterized.",https://ieeexplore.ieee.org/document/8409186/,"2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)",24-27 May 2018,ieeexplore
10.1109/KCIC.2018.8628512,Estimating Adaptive Individual Interests and Needs Based on Online Local Variational Inference for a Logistic Regression Mixture Model,IEEE,Conferences,"In real companies engaged in economic activities through transactions involving consumer items, such as retail, distribution, finance, and information materials, supplying an opportunity to customers to choose specialized items is an important factor that can improve customer satisfaction and convenience allowing their diverse and time-dependent needs to be met. However, capturing the specialized needs of customers accurately is a difficult task because their needs depend on time, context, situation, and meaning. Recently, physical computational environments have been developing rapidly, thereby allowing easy implementation to sense a customer's action and deal with it sequentially. In this paper, we propose a personalized method to predict individual interests and demands appropriately. In particular, the system learns the customers' situation, meaning, and action from their action history, and reflects a feedback of the result to predict the next action. To realize this method, we utilize the following two methodologies: the mathematical model of meaning (MMM), which is a semantic associative search technology; and the local variational inference (LVI), which is an approximation of the Bayesian inference. A numerical experiment shows that the proposed method performed better than a typical method.",https://ieeexplore.ieee.org/document/8628512/,2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC),29-30 Oct. 2018,ieeexplore
10.1109/ICMLA.2015.141,Evaluating Real-Time Anomaly Detection Algorithms -- The Numenta Anomaly Benchmark,IEEE,Conferences,"Much of the world's data is streaming, time-series data, where anomalies give significant information in critical situations, examples abound in domains such as finance, IT, security, medical, and energy. Yet detecting anomalies in streaming data is a difficult task, requiring detectors to process data in real-time, not batches, and learn while simultaneously making predictions. There are no benchmarks to adequately test and score the efficacy of real-time anomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which attempts to provide a controlled and repeatable environment of open-source tools to test and measure anomaly detection algorithms on streaming data. The perfect detector would detect all anomalies as soon as possible, trigger no false alarms, work with real-world time-series data across a variety of domains, and automatically adapt to changing statistics. Rewarding these characteristics is formalized in NAB, using a scoring algorithm designed for streaming data. NAB evaluates detectors on a benchmark dataset with labeled, real-world time-series data. We present these components, and give results and analyses for several open source, commercially-used algorithms. The goal for NAB is to provide a standard, open source framework with which the research community can compare and evaluate different algorithms for detecting anomalies in streaming data.",https://ieeexplore.ieee.org/document/7424283/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore
10.1109/ASE.2019.00118,FPChecker: Detecting Floating-Point Exceptions in GPU Applications,IEEE,Conferences,"Floating-point arithmetic is widely used in applications from several fields including scientific computing, machine learning, graphics, and finance. Many of these applications are rapidly adopting the use of GPUs to speedup computations. GPUs, however, have limited support to detect floating-point exceptions, which hinders the development of reliable applications in GPU-based systems. We present FPCHECKER, the first tool to automatically detect floating-point exceptions in GPU applications. FPCHECKER uses the clang/LLVM compiler to instrument GPU kernels and to detect exceptions at runtime. Once an exception is detected, it reports to the programmer the code location of the exception as well as other useful information. The programmer can then use this report to avoid the exception, e.g., by modifying the application algorithm or changing the input. We present the design of FPCHECKER, an evaluation of the overhead of the tool, and a real-world case scenario on which the tool is used to identify a hidden exception. The slowdown of FPCHECKER is moderate and the code is publicly available as open source.",https://ieeexplore.ieee.org/document/8952258/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/SNPD.2007.519,Fuzzy Integrative Performance Evaluation for Supply Chain System,IEEE,Conferences,"Based on the balanced scorecards proposed by Kaplan and Norton, it is developed in this paper the enhanced balanced scorecards and their novel application field regarding society environment and future development, which includes construction of performance evaluation index system from five aspects such as finance, customer service, intra flow process, learning and development, and society development within supply chain. Another original point of this paper is that the logarithm triangular fuzzy number- AHP method (LTFN-AHP) is creatively used to evaluate the integrative performance evaluation index system. This special AHP method can overcome some disadvantages that experts can directly give definite numbers when the traditional AHP method is used to value certainty and evaluate the qualitative index. More important, LTFN-AHP is a scientific qualitative and quantitative evaluation tool. Then the real cases are researched based on all discussions above.",https://ieeexplore.ieee.org/document/4287649/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore
10.1109/COMPSAC.2019.10210,Fuzzy Value-at-Risk Forecasts Using a Novel Data-Driven Neuro Volatility Predictive Model,IEEE,Conferences,"Quantitative finance has been evolving over last several decades and combining randomness and fuzziness of the parameters has found growing interest among researchers to solve forecasting problems. Superiority of the fuzzy forecasting method over the minimum mean square forecasting had been demonstrated for fuzzy coefficient (linear as well as nonlinear) time series models in Thavaneswaran et al. [5]. However, many proposed fuzzy forecasting methods remain difficult to use in practice and there is a need for data-driven approach to fit the fuzzy coefficient volatility models. A neural network (NN) system can uniformly approximate any real nonlinear function on a compact domain to any degree of accuracy. Artificial NN (ANNs) have been applied to finance problems such as stock index prediction and bankruptcy prediction. In this paper, we introduce a novel direct data-driven neuro predictive model for conditional volatility and study the fuzzy value-at-risk (VaR) forecasts. We apply this model to forecast VaR with actual financial data. Our model shows considerable promise as a decision making and risk managing tool.",https://ieeexplore.ieee.org/document/8753916/,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),15-19 Jul 2019,ieeexplore
10.1109/RTEICT.2016.7808177,High speed and area efficient single precision floating point arithmetic unit,IEEE,Conferences,"Many fields of science, engineering, finance, mathematical optimization methods, Artificial Neural Networks, signal and image processing algorithms requires the operations and manipulations of real numbers. Floating-point operations are most extensively adopted approach for exploiting real numbers. The speed of Floating-point arithmetic unit is very crucial performance parameter which impinges the operation of the system. On that account a 32 bit floating point arithmetic unit is designed for different applications which insists for eminent speed. The intent of this design is to reduce the area and combinational path delay to enhance the speed of operation which is attained by parallelism in multiplier which is used for mantissa multiplication. For Floating-point multiplier Booth recoded multiplier is used where the number of partial product are reduced which in turns boost the speed of multiplication. The proposed module is implemented on Spartan 6 FPGA. Performance of the floating point arithmetic unit is compared with latest research papers regarding delay and it is ascertained that there is 59% of optimization in critical path delay of floating point multiplier and 50 % of optimization of floating point adder. The result illustrates that proposed arithmetic unit has a great impact on convalescent the speed and area of the design.",https://ieeexplore.ieee.org/document/7808177/,"2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)",20-21 May 2016,ieeexplore
10.2498/iti.2012.0379,Model of the new sales planning optimization and sales force deployment ERP business intelligence module for direct sales of the products and services with temporal characteristics,IEEE,Conferences,"Considering that direct sales today has significant share in the whole sales niche and that existing travelling salesman problem solutions still do not provide a comprehensive resolution for sales force deployment when selling products and services with temporal or periodic recurring, here we present a new method that combines business intelligence and easy-to-use graphical user interface to give the answers. Module is self-learning due to implemented statistics with tendency for more accurate sales predictions as time goes by. Presented model is applicable in various industries like telecommunications, finance, pharmaceuticals etc. and capable of solving large-scale real-world problems in real time.",https://ieeexplore.ieee.org/document/6307985/,Proceedings of the ITI 2012 34th International Conference on Information Technology Interfaces,25-28 June 2012,ieeexplore
10.1109/CONTROL.2018.8516834,OSQP: An Operator Splitting Solver for Quadratic Programs,IEEE,Conferences,"We present a general purpose solver for quadratic programs based on the alternating direction method of multipliers, employing a novel operator splitting technique that requires the solution of a quasi-definite linear system with the same coefficient matrix in each iteration. Our algorithm is very robust, placing no requirements on the problem data such as positive definiteness of the objective function or linear independence of the constraint functions. It is division-free once an initial matrix factorization is carried out, making it suitable for real-time applications in embedded systems. In addition, our technique is the first operator splitting method for quadratic programs able to reliably detect primal and dual infeasible problems from the algorithm iterates. The method also supports factorization caching and warm starting, making it particularly efficient when solving parametrized problems arising in finance, control, and machine learning. Our open-source C implementation OSQP has a small footprint, is library-free, and has been extensively tested on many problem instances from a wide variety of application areas. It is typically ten times faster than competing interior point methods, and sometimes much more when factorization caching or warm start is used.",https://ieeexplore.ieee.org/document/8516834/,2018 UKACC 12th International Conference on Control (CONTROL),5-7 Sept. 2018,ieeexplore
10.1109/AIMSEC.2011.6011012,On constructing the practical teaching system of taxation major in newly-built academies,IEEE,Conferences,"Enterprising spirit and practice competence are of great importance in talents training of higher education in 21st century. The prime objective of newly-built academies is to train qualified specialists with enterprising spirit and practice competence, which must be guaranteed by a series of practical teaching steps. However, the reality is that a set of scientific practice teaching system has not come into being, and that teaching staff, base construction, and related auxiliary regulations establishment are in need of improvement. This article explores the construction and implementation of practical teaching system of taxation major in newly-built finance and economics academies, which expects to provide reference for similar universities and colleges‥",https://ieeexplore.ieee.org/document/6011012/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore
10.1109/PESGRE52268.2022.9715908,Online Prediction of DGA Results for Intelligent Condition Monitoring of Power Transformers,IEEE,Conferences,"Transformers form a major part of a power system in transmission as well as distribution of power. Considering the criticality, finance, and time involved in repair, periodic condition monitoring and maintenance of transformers are the key to ensure electrical safety as well as stable operation of the large interconnected power system. Dissolved Gas Analysis (DGA) is an established tool used to determine the incipient faults within the transformer by analyzing the concentration of different gases in the transformer oil and giving early warnings and diagnoses. Currently, transformers worldwide utilise online sensors to monitor dissolved gases and moisture content in oil. The online DGA sensor uses a small amount of oil from transformer to perform real-time DGA analysis and gives the ppm content of dissolved gases for further course of action. Considering the large quantity of assets and the huge amount of data produced, it is imperative to develop a tool to aid the operators in assimilating the available data for diagnosis and proactive decision making. The present study improvises AI techniques to predict future dissolved gas concentrations using real time DGA data collected from the transmission utility of the country. The prediction helps to forecast the trend of development of incipient faults in the transformer. The complete project scope is to develop a highly reliable diagnostic tool to emulate the decision-making ability of a human expert in transformer DGA analysis to enhance transformer life. In the present paper, models based on Auto-regressive Integrated Moving Average (ARIMA), Long Short-Term Memory (LSTM), and Vector Auto Regression (VAR) are implemented to predict DGA data of three in-service transformers. DGA data is forecasted for up to 8 monthly samples in the future, and the accuracy of results is compared with each other. The LSTM-VAR combined model is seen to provide the best results among them.",https://ieeexplore.ieee.org/document/9715908/,"2022 IEEE International Conference on Power Electronics, Smart Grid, and Renewable Energy (PESGRE)",2-5 Jan. 2022,ieeexplore
10.1109/CEC48606.2020.9185545,Optimizing LSTM Based Network For Forecasting Stock Market,IEEE,Conferences,"In this modern era, the financial market, more specifically, the stock markets all over the world, deal with an enormous amount of real-time data that facilitates the data analytics and prediction in the field of finance. The main objective of this paper is to propose a novel model of neural network based on Long-Short Term Memory (LSTM) and utilizing one of the most powerful evolutionary algorithms, namely the Differential Evolution (DE), to forecast the next day's stock price of a company. This study focuses on optimizing the ten network hyperparameters related to the detection of temporal patterns of a given dataset, namely, the size of the time window, batch size, the number of LSTM units in hidden layers, the number of hidden layers (LSTM and dense), dropout coefficient for each layer, and the network training optimization algorithm. To the best of our knowledge, this is the first time that all this set of parameters have been optimized simultaneously. Then, the LSTM has been optimized by DE to gain the lower root mean squared error (RMSE) for prediction. The proposed model achieved 8.092 RMSE as its objective value, which is better in comparison with the best statistical forecasting models such as NAIVE, ETS, and SARIMA, which are the-state-of-the-art methods in this filed. Moreover, for shortening the training time as the main source of computational expensiveness, the proposed method works with a lower number of epochs. By this way, DE tries to find a shallower and faster network even with higher accuracy, which is a remarkable approach.",https://ieeexplore.ieee.org/document/9185545/,2020 IEEE Congress on Evolutionary Computation (CEC),19-24 July 2020,ieeexplore
10.1109/SCC49832.2020.00053,Ponzi Contracts Detection Based on Improved Convolutional Neural Network,IEEE,Conferences,"As one of the leading blockchain systems in operation, Ethereum has numerous smart contracts deployed to implement a variety of functions. Unfortunately, speculators introduce scams such as Ponzi scheme in the traditional financial sector into some of these smart contracts, causing millions of dollars of losses to investors. At present, there are a few of quantitative identification methods for new fraud modes under the background of Internet finance, and detection methods for the Ponzi scheme contracts on Ethereum are even less. In this paper, we propose an improved convolutional neural network as a detection model for Ponzi schemes in smart contracts. We use real smart contracts to evaluate the feasibility and usefulness of our mode. Results show that our improved convolutional neural network can overcome difficulties in training caused by different length of smart contracts' bytecodes. Compared with the state-of-the-art methods, the precision and recall rate of our model for Ponzi scheme detection are improved by 3.2% and 24.8% respectively.",https://ieeexplore.ieee.org/document/9284582/,2020 IEEE International Conference on Services Computing (SCC),7-11 Nov. 2020,ieeexplore
,Prediction of Earnings Per Share for industry,IEEE,Conferences,"Prediction of Earnings Per Share (EPS) is the fundamental problem in finance industry. Various Data Mining technologies have been widely used in computational finance. This research work aims to predict the future EPS with previous values through the use of data mining technologies, thus to provide decision makers a reference or evidence for their economic strategies and business activity. We created three models LR, RBF and MLP for the regression problem. Our experiments with these models were carried out on the real datasets provided by a software company. The performance assessment was based on Correlation Coefficient and Root Mean Squared Error. These algorithms were validated with the data of six different companies. Some differences between the models have been observed. In most cases, Linear Regression and Multilayer Perceptron are effectively capable of predicting the future EPS. But for the high nonlinear data, MLP gives better performance.",https://ieeexplore.ieee.org/document/7526950/,"2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)",12-14 Nov. 2015,ieeexplore
10.1109/ICCCE.2008.4580693,Real time implementation of NARMA L2 feedback linearization and smoothed NARMA L2 controls of a single link manipulator,IEEE,Conferences,"Robotics is a field of modern technology which requires knowledge in vast areas such as electrical engineering, mechanical engineering, computer science as well as finance. Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robots in industrial plants. Tracking control of a single link manipulator driven by a permanent magnet brushed DC motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore uncertainties arise because of changes of the rotor resistance with temperature and random variations of friction while operating. Due to this fact classical PID controller can not be used effectively since it is developed based on linear system theory. Neural network control schemes for manipulator control problem have been proposed by researchers; in which their competency is validated through simulation studies. On the other hand, actual real time applications are rarely established. Instead of simulation studies, this paper is aimed to implement neural network controller in real time for controlling a DC motor driven single link manipulator. The work presented in this paper is concentrating on neural NARMA L2 control and its improvement called to as Smoothed NARMA L2 control. As proposed by K. S Narendra and Mukhopadhyay, Narma L2 control is one of the popular neural network architectures for prediction and control. The real time experimentation showed that the Smoothed NARMA L2 is effective for controlling the single link manipulator for both point-to-point and continuous path motion control.",https://ieeexplore.ieee.org/document/4580693/,2008 International Conference on Computer and Communication Engineering,13-15 May 2008,ieeexplore
10.1109/ISAMSR53229.2021.9567891,Recent and Future Innovative Artificial Intelligence Services and Fields,IEEE,Conferences,"Recent innovative Artificial Intelligence (AI) solutions accelerate digital transformations in different fields. It is important to highlight and explore this innovative AI service in different domains so that digital transformation can be planned, designed, and implemented for maximum society benefits. This paper investigates the different fields of AI services that can be utilized towards achieving highly beneficial digital transformations in different societal domains. This includes marketing, finance and banking, healthcare industry, emotion, and creative AI, as well as recent AI fields as explainable and responsible AI. Exploring and understanding the innovative AI services in these domains widen researcher capabilities to achieve effective and highly beneficial digital transformations.",https://ieeexplore.ieee.org/document/9567891/,"2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)",6-8 Sept. 2021,ieeexplore
10.1109/ISCON52037.2021.9702445,Role of Artificial Intelligence and Internet of Things in Promoting Banking and Financial Services During COVID-19: Pre and Post Effect,IEEE,Conferences,"The covid-19 outbreak has triggered an economic crisis and severely affected the global economy. The revolution of AI and the Internet of things (IoT) is helping in reshaping the traditional financial sector by consolidating technology, finance, and economics. This paper highlights how the integration of Artificial intelligence (AI) in IoT is helping in promoting digital financial inclusion during the COVID-19. Through multiple real-life case studies, this article examines the successful implementation of AI and IoT in banking and financial institutions. The paper also explores the advantages and opportunities arising by the use of AI and IoT in the financial sector. A descriptive research approach has been followed to investigate the pre and post effect of COVID-19 and how it is leading towards economic efficiency.",https://ieeexplore.ieee.org/document/9702445/,2021 5th International Conference on Information Systems and Computer Networks (ISCON),22-23 Oct. 2021,ieeexplore
10.1109/UKSim.2014.9,Table of contents,IEEE,Conferences,The following topics are dealt with: computer modeling; computer simulation; neural networks; evolutionary computation; adaptive dynamic programming; reinforcement learning; bioinformatics; bioengineering; computational finance; computational economics; data mining; semantics mining; computer games; virtual reality; data visualization; artificial intelligence; soft computing; robotics; discrete event systems; realtime systems; operations research; image processing; speech processing; signal processing; natural language processing; social sciences; business management; parallel processing; distributed processing; software architecture; semantic Web; ontologies; ad hoc wireless networks; mobile networks; target tracking; circuits; and sensors.,https://ieeexplore.ieee.org/document/7045641/,2014 UKSim-AMSS 16th International Conference on Computer Modelling and Simulation,26-28 March 2014,ieeexplore
10.1109/CEC.2011.5949582,Table of contents,IEEE,Conferences,The following topics are dealt with: structural design; complex problems; distribution algorithm estimation; artificial bee colony algorithm; evolutionary robotics; evolutionary computation theory; finance decision making; bio-inspired architectures; bio-inspired systems; evolutionary computer vision; computational intelligence; bioinformatics; computational biology; many-core architecture; clustering; data mining; evolvable hardware; evolvable software; fitness landscapes; nature-inspired constrained optimization; ant approach; evolutionary algorithms; real-world numerical optimization problems; art; music; genetic programming; memetic algorithms; cultural algorithms; immune algorithms; developmental systems; generative systems; meta-heuristic method; global continuous optimization; operators; learning classifier systems; TSP; routing problems; multi-objective optimization; greedy selection; autonomous agent learning; statistical learning techniques; machine learning techniques; PSO algorithms; medical image analysis; evolutionary programming; differential evolution; evolutionary games; evolved neural networks; complex networks; multi-agent systems; biometrics; biomedical applications; hyper heuristics; coevolutionary systems; artificial ecology and artificial life.,https://ieeexplore.ieee.org/document/5949582/,2011 IEEE Congress of Evolutionary Computation (CEC),5-8 June 2011,ieeexplore
10.1109/WSC.2017.8248046,The role of learning on industrial simulation design and analysis,IEEE,Conferences,"The capability of modeling real-world system operations has turned simulation into an indispensable problem-solving methodology for business system design and analysis. Today, simulation supports decisions ranging from sourcing to operations to finance, starting at the strategic level and proceeding towards tactical and operational levels of decision-making. In such a dynamic setting, the practice of simulation goes beyond being a static problem-solving exercise and requires integration with learning. This article discusses the role of learning in simulation design and analysis motivated by the needs of industrial problems and describes how selected tools of statistical learning can be utilized for this purpose.",https://ieeexplore.ieee.org/document/8248046/,2017 Winter Simulation Conference (WSC),3-6 Dec. 2017,ieeexplore
10.1109/BigData50022.2020.9378023,Zero-Shot Machine Learning Technique for Classification of Multi-User Big Data Workloads,IEEE,Conferences,"During the last decade machine learning has revolutionized computer science applications. Supervised machine learning algorithms have become especially successful in many industries including health, legal, security, finance, travel, and others. Training supervised learning algorithms, however, is expensive because the real world contains a very large number of different classes that need to be covered by the training set. This is especially true for the highly variable, multi-user workload data produced by the strategically vital big data and cloud software stacks. It is very important, however, to be able to accurately classify these complex workloads in order to enable autonomic management and optimization. Zero-Shot Learning (ZSL) is an advanced machine learning approach that enables classification of objects without having to explicitly train on examples of those objects. In this paper we present a new ZSL technique intended to reduce the expense of assembling workload training sets for big data analytic workloads. We demonstrate that multi-user big data workloads can be treated as hybrids of simpler, single-user workload classes, and classified accurately without having to explicitly train on example instances of multi-user workloads. Our technique is able to accurately classify both unseen multi-user workloads, and seen single-user workloads using the same classifier. We demonstrate 83% classification accuracy for the unseen multi-user workloads, and 92% classification accuracy for the seen, single-user workload classes.",https://ieeexplore.ieee.org/document/9378023/,2020 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2020,ieeexplore
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore
10.1109/UKSim.2013.1,[Title page i],IEEE,Conferences,The following topics are dealt with: neural networks; fuzzy systems; evolutionary computation; dynamic programming; reinforcement learning; bioinformatics; bioengineering; computational finance; computational economics; computer games; virtual reality; data visualization; computer networks; intelligent systems; soft computing; intelligent control; e-science; e-systems; robotics; cybernetics; manufacturing systems; operations research; discrete event systems; realtime systems; image processing; speech processing; signal processing; natural language processing; business management; human factors; renewable energy; logistics; parallel architecture; distributed architecture; software architecture; Internet; ontologies; wireless networks; target tracking; performance engineering; circuits; and sensors.,https://ieeexplore.ieee.org/document/6527370/,2013 UKSim 15th International Conference on Computer Modelling and Simulation,10-12 April 2013,ieeexplore
10.1109/ICAS.2009.2,[Title page iii],IEEE,Conferences,The following topics are dealt with: real-time chain-structured synchronous dataflow; memory requirement formal determination; linear singular descriptor differential system; execution-optimized paths; greedy strategy; load trend evaluation; self-managed P2P streaming; context-aware ambient assisted living application; self-adaptive distributed model; autonomic systems; wireless sensor networks; topology control; learning based method; self-recovery method; mobile data sharing; heterogeneous QoS resource manager; component-based self-healing; NGN mobility; interactive user activity; .NET Windows service agent technology; agent based Web browser; resource-definition policies; autonomic computing; autonomic system administration; automatic database performance tuning; knowledge management; adaptive reinforcement learning; VoIP services; autonomic RSS: distributed virtual reality simulations; virtual machines resources allocation; multi-lier distributed systems; network I/O extensibility; virtual keyboards; self-configuring smart homes; legged underwater vehicles; particle filters; reusable semantic components; multi-agent systems; fixed-wing unmanned aerial vehicles; fuzzy inference system; robot swarms; mobile robots; optimization architecture; autonomous unmanned helicopter landing system design; heterogeneous multi-database environments; autonomic software license management system; Web server crashes prediction; laser range finder; video quality; wireless networks; ITU-T G.1030; open IMS core; context-aware data mining methodology; supply chain finance cooperative systems; autonomous pervasive environments; distributed generic stress tool; dynamic adaptive systems; multisensory media effects and user preference.,https://ieeexplore.ieee.org/document/4976566/,2009 Fifth International Conference on Autonomic and Autonomous Systems,20-25 April 2009,ieeexplore
10.1109/WICT.2011.6141190,[Title page],IEEE,Conferences,The following topics are dealt with: bioinformatics; computational biology; computer graphics; virtual reality; data mining; e-learning; e-business; e-goverment; artificial intelligence; Web services; semantic Web; grid computing; cloud computing; ambient intelligence; body sensor networks; computational finance; computational economics; cybercrime; computer network security; information security; academic integrity; plagiarism detection; software misuse; intrusion detection; forensics; large scale distributed system scheduling; educational technology; health technology; nature inspired optimization algorithm; and data management.,https://ieeexplore.ieee.org/document/6141190/,2011 World Congress on Information and Communication Technologies,11-14 Dec. 2011,ieeexplore
10.1109/ACCESS.2019.2948949,A Big Data Mining Approach of PSO-Based BP Neural Network for Financial Risk Management With IoT,IEEE,Journals,"In recent years, the technology about IoT (Internet of Things) has been applied into finance domain, and the generated data, such as the real-time data of chattel mortgage supervision with GPS, sensors, network cameras, mobile devices, etc., has been used to improve the capability of financial credit risk management of bank loans. Financial credit risk is by far one of the most significant risks that commercial banks have to face, however, when confronting to the massively growing financial data from multiple sources including Internet, mobile networks or IoT, traditional statistical models and neural network models might not operate fairly or accurately enough for credit risk assessment with those diverse data. Hence, there is a practical need to establish more powerful risk prediction models with artificial intelligence based on big data analytics to predict default behaviors with better accuracy and capacity. In this article, a big data mining approach of Particle Swarm Optimization (PSO) based Backpropagation (BP) neural network is proposed for financial risk management in commercial banks with IoT deployment, which constructs a nonlinear parallel optimization model with Apache Spark and Hadoop HDFS techniques on the dataset of on-balance sheet item and off-balance sheet item. The experiment results indicate that this parallel risk management model has fast convergence rate and powerful predictive capacity, and performs efficiently in screening default behaviors. In the meanwhile, the distributed implementation on big data clusters largely reduces the processing time of model training and testing.",https://ieeexplore.ieee.org/document/8879579/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2021.3111229,AdaptiveSystems: An Integrated Framework for Adaptive Systems Design and Development Using MPS JetBrains Domain-Specific Modeling Environment,IEEE,Journals,"This paper contains the design and development of an adaptive systems (<i>AdaptiveSystems</i> Domain-Specific Language - DSL) framework to assist language developers and data scientists in their attempt to apply Artificial Intelligence (AI) algorithms in several application domains. Big-data processing and AI algorithms are at the heart of autonomics research groups among industry and academia. Major advances in the field have traditionally focused on algorithmic research and increasing the performance of the developed algorithms. However, it has been recently recognized by the AI community that the applicability of these algorithms and their consideration in context is of paramount importance for their adoption. Current approaches to address AI in context lie in two areas: adaptive systems research that mainly focuses on implementing adaptivity mechanisms (technical perspective) and AI in context research that focuses on business aspects (business perspective). There is currently no approach that combines all aspects required from business considerations to an appropriate level of abstraction. In this paper, we attempt to address the problem of designing adaptive systems and therefore providing AI in context by utilising DSL technology. We propose a new DSL (<i>AdaptiveSystems</i>) and a methodology to apply this to the creation of a DSL for specific application domains such as <i>AdaptiveVLE (Adaptive Virtual Learning Environment)</i> DSL. The language developer will be able to instantiate the <i>AdaptiveSystems</i> DSL to any application domain by using the guidelines in this paper with an integrated path from design to implementation. The domain expert will then be able to use the developed DSL (e.g. <i>AdaptiveVLE</i> DSL) to design and develop their application. Future work will include extension and experimentation of the applicability of this work to more application domains within British Telecom (BT) and other areas such as health care, finance, etc.",https://ieeexplore.ieee.org/document/9531598/,IEEE Access,2021,ieeexplore
10.1109/TKDE.2018.2822307,Ensemble Learning for Multi-Type Classification in Heterogeneous Networks,IEEE,Journals,"Heterogeneous networks are networks consisting of different types of objects and links. They can be found in several fields, ranging from the Internet to social sciences, biology, epidemiology, geography, finance, and many others. In the literature, several methods have been proposed for the analysis of network data, but they usually focus on homogeneous networks, where all the objects are of the same type, and links among them describe a single type of relationship. More recently, the complexity of real scenarios has impelled researchers to design methods for the analysis of heterogeneous networks, especially focused on classification and clustering tasks. However, they often make assumptions on the structure of the network that are too restrictive or do not fully exploit different forms of network correlation and autocorrelation. Moreover, when nodes which are the main subject of the classification task are linked to several nodes of the network having missing values, standard methods can lead to either building incomplete classification models or to discarding possibly relevant dependencies (correlation or autocorrelation). In this paper, we propose an ensemble learning approach for multi-type classification. We adopt the system Mr-SBC, which is originally able to analyze heterogeneous networks of arbitrary structure, within an ensemble learning approach. The ensemble allows us to improve the classification accuracy of Mr-SBC by exploiting i) the possible presence of correlation and autocorrelation phenomena and ii) the classification of instances (which contain missing values) of other node types in the network. As a beneficial side effect, we have also that the models are more stable in terms of standard deviation of the accuracy, over different samples used for training. Experiments performed on real-world datasets show that the proposed method is able to significantly outperform the standard implementation of Mr-SBC. Moreover, it gives Mr-SBC the advantage of outperforming four other well-known algorithms for the classification of data organized in a network.",https://ieeexplore.ieee.org/document/8329517/,IEEE Transactions on Knowledge and Data Engineering,1 Dec. 2018,ieeexplore
10.1109/TCIAIG.2014.2345842,Multiobjective Monte Carlo Tree Search for Real-Time Games,IEEE,Journals,"Multiobjective optimization has been traditionally a matter of study in domains like engineering or finance, with little impact on games research. However, action-decision based on multiobjective evaluation may be beneficial in order to obtain a high quality level of play. This paper presents a multiobjective Monte Carlo tree search algorithm for planning and control in real-time game domains, those where the time budget to decide the next move to make is close to 40 ms. A comparison is made between the proposed algorithm, a single-objective version of Monte Carlo tree search and a rolling horizon implementation of nondominated sorting evolutionary algorithm II (NSGA-II). Two different benchmarks are employed, deep sea treasure (DST) and the multiobjective physical traveling salesman problem (MO-PTSP). Using the same heuristics on each game, the analysis is focused on how well the algorithms explore the search space. Results show that the algorithm proposed outperforms NSGA-II. Additionally, it is also shown that the algorithm is able to converge to different optimal solutions or the optimal Pareto front (if achieved during search).",https://ieeexplore.ieee.org/document/6872573/,IEEE Transactions on Computational Intelligence and AI in Games,Dec. 2015,ieeexplore
10.1109/TITS.2015.2390614,Reducing the Cost of High-Speed Railway Communications: From the Propagation Channel View,IEEE,Journals,"High-speed railways (HSRs) have been widely introduced to meet the increasing demand for passenger rail travel. While it provides more and more conveniences to people, the huge cost of the HSR has laid big burden on the government finance. Reducing the cost of HSR has been necessary and urgent. Optimizing arrangement of base stations (BS) by improving prediction of the communication link is one of the most effective methods, which could reduce the number of BSs to a reasonable number. However, it requires a carefully developed propagation model, which has been largely neglected before in the research on the HSR. In this paper, we propose a standardized path loss/shadow fading model for HSR channels based on an extensive measurement campaign in 4594 HSR cells. The measurements are conducted using a practically deployed and operative GSM-Railway (GSM-R) system to reflect the real conditions of the HSR channels. The proposed model is validated by the measurements conducted in a different operative HSR line. Finally, a heuristic method to design the BS separation distance is proposed, and it is found that using an improved propagation model can theoretically save around 2/5 cost of the BSs.",https://ieeexplore.ieee.org/document/7052340/,IEEE Transactions on Intelligent Transportation Systems,Aug. 2015,ieeexplore
10.1109/COMST.2018.2843533,The Dark Side(-Channel) of Mobile Devices: A Survey on Network Traffic Analysis,IEEE,Journals,"In recent years, mobile devices (e.g., smartphones and tablets) have met an increasing commercial success and have become a fundamental element of the everyday life for billions of people all around the world. Mobile devices are used not only for traditional communication activities (e.g., voice calls and messages) but also for more advanced tasks made possible by an enormous amount of multi-purpose applications (e.g., finance, gaming, and shopping). As a result, those devices generate a significant network traffic (a consistent part of the overall Internet traffic). For this reason, the research community has been investigating security and privacy issues that are related to the network traffic generated by mobile devices, which could be analyzed to obtain information useful for a variety of goals (ranging from fine-grained user profiling to device security and network optimization). In this paper, we review the works that contributed to the state of the art of network traffic analysis targeting mobile devices. In particular, we present a systematic classification of the works in the literature according to three criteria: 1) the goal of the analysis; 2) the point where the network traffic is captured; and 3) the targeted mobile platforms. In this survey, we consider points of capturing such as Wi-Fi access points, software simulation, and inside real mobile devices or emulators. For the surveyed works, we review and compare analysis techniques, validation methods, and achieved results. We also discuss possible countermeasures, challenges, and possible directions for future research on mobile traffic analysis and other emerging domains (e.g., Internet of Things). We believe our survey will be a reference work for researchers and practitioners in this research field.",https://ieeexplore.ieee.org/document/8371242/,IEEE Communications Surveys & Tutorials,Fourthquarter 2018,ieeexplore
10.1109/MCIT.2010.5444840,Associative Classification techniques for predicting e-banking phishing websites,IEEE,Conferences,"This paper presents a novel approach to overcome the difficulty and complexity in detecting and predicting e-banking phishing website. We proposed an intelligent resilient and effective model that is based on using association and classification Data Mining algorithms. These algorithms were used to characterize and identify all the factors and rules in order to classify the phishing website and the relationship that correlate them with each other. We implemented six different classification algorithm and techniques to extract the phishing training data sets criteria to classify their legitimacy. We also compared their performances, accuracy, number of rules generated and speed. The rules generated from the associative classification model showed the relationship between some important characteristics like URL and Domain Identity, and Security and Encryption criteria in the final phishing detection rate. The experimental results demonstrated the feasibility of using Associative Classification techniques in real applications and its better performance as compared to other traditional classifications algorithms.",https://ieeexplore.ieee.org/document/5444840/,2010 International Conference on Multimedia Computing and Information Technology (MCIT),2-4 March 2010,ieeexplore
10.1109/SOLI.2008.4686380,Banking Intelligence: Application of data warehouse in bank operations,IEEE,Conferences,"Along with the development of information technology, business intelligence plays an important role in the bank operation process. Bank Intelligence is a method of storing and presenting key bank business data so that anyone in the bank can quickly and easily ask questions of accurate and timely data. In a bank network, hundreds of millions of customer data records are generated daily. The growing competition and increased speed of business changes has dramatically shown the need for bank intelligence. In this paper, we propose an data warehouse based bank intelligence framework and implementation. Specifically, we introduce the application of data warehouse in the analysis of bank customer data and bank branch data. This paper also summarizes experiences and results of applying this this solution into real-world applications. Our results has revealed the simplicity and power of OLAP-based solutions to scalable bank intelligence.",https://ieeexplore.ieee.org/document/4686380/,"2008 IEEE International Conference on Service Operations and Logistics, and Informatics",12-15 Oct. 2008,ieeexplore
10.1109/BIGCOMP.2019.8679121,Question Understanding Based on Sentence Embedding on Dialog Systems for Banking Service,IEEE,Conferences,"This paper introduce a question understanding system to respond appropriate answers in a dialog system for banking services. The question understanding system provides an automated response service in a specific domain (e.g. banking). This can increase response rate of a customer counseling service, and improve business efficiency and expertise. The question understanding system classify domains, specific categories, and speech acts of questions. Finally, the system analyze meanings and intents of the questions, and searching correct answers even various input sentences. In this paper, we describe methods of keyword tokenizing, pattern recognition, sentence embedding, analyzing dialogue intention, and searching similar FAQs. Through these methods, we have developed the question understanding unit in a real interactive system for financial services for real insurance companies and banks, and analyze the usefulness of the system through practical system implementation examples.",https://ieeexplore.ieee.org/document/8679121/,2019 IEEE International Conference on Big Data and Smart Computing (BigComp),27 Feb.-2 March 2019,ieeexplore
10.1109/MIPRO.2016.7522376,Reciprocal payers identification in banking logs using SAT solvers,IEEE,Conferences,"In this paper we presented solvers for satisfiability testing (SAT) as a novel approach to finding reciprocal payers in banking logs. A term “reciprocal payers” is usually treated as general fraud by using standard techniques such as expert systems, machine learning and in recent times social network analysis. SAT as a technique for data analysis was abandoned due to the unfeasibility of SAT solvers. SAT solvers, however continued to develop in the hardware and software verification communities. We presented a proof-of-concept solution for identification of reciprocal payers (formally called a clique), which is a group of bank clients that issue payments to each other (each member to each member). We do not use real data due to client confidentiality, but the reader can see the principle. In the basic approach it is assumed that each client has only one account, and in the extended, second approach, it was allowed that a client can have more than one account.",https://ieeexplore.ieee.org/document/7522376/,"2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",30 May-3 June 2016,ieeexplore
10.1109/ISCON52037.2021.9702445,Role of Artificial Intelligence and Internet of Things in Promoting Banking and Financial Services During COVID-19: Pre and Post Effect,IEEE,Conferences,"The covid-19 outbreak has triggered an economic crisis and severely affected the global economy. The revolution of AI and the Internet of things (IoT) is helping in reshaping the traditional financial sector by consolidating technology, finance, and economics. This paper highlights how the integration of Artificial intelligence (AI) in IoT is helping in promoting digital financial inclusion during the COVID-19. Through multiple real-life case studies, this article examines the successful implementation of AI and IoT in banking and financial institutions. The paper also explores the advantages and opportunities arising by the use of AI and IoT in the financial sector. A descriptive research approach has been followed to investigate the pre and post effect of COVID-19 and how it is leading towards economic efficiency.",https://ieeexplore.ieee.org/document/9702445/,2021 5th International Conference on Information Systems and Computer Networks (ISCON),22-23 Oct. 2021,ieeexplore
10.1109/ICRITO51393.2021.9596273,Trust in Banking Management System using Firebase in Python using AI,IEEE,Conferences,"The Banking Management System (BMS) is windows based Trusted GUI application which can be used to store the data of bank account holders into a trusted real time database in Firebase which could be later used to fetch the details of account by any system using internet. Multiple users can have access at the same time. The main objective of Banking Management System is to allow the administrator/client or the user to edit and access the details of the bank account online. It also allows multiple users/clients to have access from different systems at the same time using internet. Hence, the same data will access by the banks as well as the account holders. It fetches the data from real time database of firebase very quick within seconds. The Scope of BMS is to help the banks to allow their account holders or users to gain access onto their particular account from sitting at home itself using internet. Making their data accessible to them online. This helps the users to make their data safe in database and also leads to built trust among the banks and their customers.",https://ieeexplore.ieee.org/document/9596273/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore
10.1109/TII.2012.2232935,Fuzzy Refinement Domain Adaptation for Long Term Prediction in Banking Ecosystem,IEEE,Journals,"Long-term bank failure prediction is a challenging real world problem in banking ecosystem and machine learning methods have been recently applied to improve the prediction accuracy. However, traditional machine learning methods assume that the training data and the test data are drawn from the same distribution, which is hard to be met in real world banking applications. This paper proposes a novel algorithm known as fuzzy refinement domain adaptation to solve this problem based on the ecosystem-oriented architecture. The algorithm utilizes the fuzzy system and similarity/dissimilarity concepts to modify the target instances' labels which were initially predicted by a shift-unaware prediction model. It employs a classifier to modify the label values of target instances based on their similarity/dissimilarity to the candidate positive and negative instances in mixture domains. Thirty six experiments are performed using three different shift-unaware prediction models. In these experiments bank failure financial data is used to evaluate the algorithm. The results demonstrate that the proposed algorithm significantly improves predictive accuracy and outperforms other refinement algorithms.",https://ieeexplore.ieee.org/document/6419821/,IEEE Transactions on Industrial Informatics,May 2014,ieeexplore
10.1109/ACCESS.2020.3015616,Intelligent Performance-Aware Adaptation of Control Policies for Optimizing Banking Teller Process Using Machine Learning,IEEE,Journals,"In the current banking systems and business processes, the permission granted to employees is controlled and managed by the configured access control methods, in which static role-based models focus on access to information and functions. The deployed configuration is not reviewed/updated systematically and is handled manually by managers. Consequently, banks and companies are looking for systems and applications to automate and optimize their business processes and data management intelligently. In this context, the notion of integrating machine learning (ML) techniques in banking business processes has emerged. In order to build an intelligent and systematic solution, we combine in this paper ML and dynamic authorization techniques to enable performance-based policy evaluation into the banking teller process, where policies adapt to the changes recognized by the ML model. The objective of this work is to focus on the banking teller process that may be generalized to other operational banking processes. In this context, we propose in this paper a new model providing Intelligent Performance-Aware Adaptation of Roles and Policy Control using a support vector machine (SVM). We demonstrate that our model is capable of assessing the deployed control policies and updating them systematically with new roles and authorization levels based on tellers' performance, work history, and system constraints. We evaluated different machine learning models on a real dataset generated from a real-life banking environment. Experimental results explore the relevance and efficiency of our proposed scheme in terms of prediction accuracy, required authorizations, transaction time, and employees' working hours.",https://ieeexplore.ieee.org/document/9163345/,IEEE Access,2020,ieeexplore
10.1109/ICISC.2018.8398982,A rule-based classification of short message service type,IEEE,Conferences,"Short message service (SMS) is one of the most popular means of communication due to its type of usages like one-time passwords(OTPs), banking transaction alerts, and other promotional messages. SMS is the most time-sensitive channel of communication that demands service providers to send any information globally without any delay in respective time zone. For successful implementation of this service, the Telecom Regulatory of India (TRAI) has recommended certain guidelines that must be followed for the delivery of short messages. The prevalent practice in the industry is to store rules or raw string in a database and to match with incoming SMSs in synchronous mode. However, there are various shortcomings in this traditional approach that have been also discussed in this paper. Further, to address these issues, this research work proposes a novel approach for automated classification of SMSs in real time by the SMS service using a rule-based system of database template matching. Further to validate the proposed approach, SMS database of Netcore Solutions Pvt Limited, India, has been used for training the proposed algorithm. Proposed rule-based classifier keeps learning and updating the training dataset as more SMSs are accumulated and processed through the server. Other advantages, in terms of performance metrics, of the classification using the proposed rule-based database template matching over the traditional database matching approach have been reported in this work as evaluation measures. Reported rule-based SMS classification algorithm shows highest average classification accuracy of 100% which is better as compared to the similar research works already available in the literature.",https://ieeexplore.ieee.org/document/8398982/,2018 2nd International Conference on Inventive Systems and Control (ICISC),19-20 Jan. 2018,ieeexplore
10.1109/CCWC47524.2020.9031269,An Automated Framework for Real-time Phishing URL Detection,IEEE,Conferences,"An increasing number of services, including banking and social networking, are being integrated with world wide web in recent years. The crux of this increasing dependence on the internet is the rise of different kinds of cyberattacks on unsuspecting users. One such attack is phishing, which aims at stealing user information via deceptive websites. The primary defense against phishing consists of maintaining a black list of the phishing URLs. However, a black list approach is reactive and cannot defend against new phishing websites. For this reason, a number of research have been done on using machine learning techniques to detect previously unseen phishing URLs. While they show promising results, any such implementation is yet to be seen. This is because 1) little work has been done on developing a complete end-to-end framework for phishing URL detection 2) it is prohibitively slow to detect phishing URLs using machine learning algorithms. In this work we address these two issues by formulating a robust framework for fast and automated detection of phishing URLs. We have validated our framework with a real dataset achieving 87% accuracy in a real-time setup.",https://ieeexplore.ieee.org/document/9031269/,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),6-8 Jan. 2020,ieeexplore
10.1109/ICCSP.2019.8698029,Artificial Intelligence based Network Intrusion Detection with Hyper-Parameter Optimization Tuning on the Realistic Cyber Dataset CSE-CIC-IDS2018 using Cloud Computing,IEEE,Conferences,"One of the latest emerging technologies is artificial intelligence, which makes the machine mimic human behavior. The most important component used to detect cyber attacks or malicious activities is the Intrusion Detection System (IDS). Artificial intelligence plays a vital role in detecting intrusions and widely considered as the better way in adapting and building IDS. In trendy days, artificial intelligence algorithms are rising as a brand new computing technique which will be applied to actual time issues. In modern days, neural network algorithms are emerging as a new artificial intelligence technique that can be applied to real-time problems. The proposed system is to detect a classification of botnet attack which poses a serious threat to financial sectors and banking services. The proposed system is created by applying artificial intelligence on a realistic cyber defense dataset (CSE-CIC-IDS2018), the very latest Intrusion Detection Dataset created in 2018 by Canadian Institute for Cybersecurity (CIC) on AWS (Amazon Web Services). The proposed system of Artificial Neural Networks provides an outstanding performance of Accuracy score is 99.97% and an average area under ROC (Receiver Operator Characteristic) curve is 0.999 and an average False Positive rate is a mere value of 0.001. The proposed system using artificial intelligence of botnet attack detection is powerful, more accurate and precise. The novel proposed system can be implemented in n machines to conventional network traffic analysis, cyber-physical system traffic data and also to the real-time network traffic analysis.",https://ieeexplore.ieee.org/document/8698029/,2019 International Conference on Communication and Signal Processing (ICCSP),4-6 April 2019,ieeexplore
10.1109/MCIT.2010.5444840,Associative Classification techniques for predicting e-banking phishing websites,IEEE,Conferences,"This paper presents a novel approach to overcome the difficulty and complexity in detecting and predicting e-banking phishing website. We proposed an intelligent resilient and effective model that is based on using association and classification Data Mining algorithms. These algorithms were used to characterize and identify all the factors and rules in order to classify the phishing website and the relationship that correlate them with each other. We implemented six different classification algorithm and techniques to extract the phishing training data sets criteria to classify their legitimacy. We also compared their performances, accuracy, number of rules generated and speed. The rules generated from the associative classification model showed the relationship between some important characteristics like URL and Domain Identity, and Security and Encryption criteria in the final phishing detection rate. The experimental results demonstrated the feasibility of using Associative Classification techniques in real applications and its better performance as compared to other traditional classifications algorithms.",https://ieeexplore.ieee.org/document/5444840/,2010 International Conference on Multimedia Computing and Information Technology (MCIT),2-4 March 2010,ieeexplore
10.1109/ACOMP.2018.00025,Automatic Extract Handwriting Marked Regions in Business Document Images,IEEE,Conferences,"One of the difficulties in the automation of market trading, data crawling in commercial activities, such as banking operation is the identification of handwriting marked regions. This is the first and important step in mixed-document image understanding. In this paper, we propose a method to extract the handwriting marked regions in business forms. Our method is a combination of printed form analysis and colored handwritten symbols detection. This program is primarily designed to support commercial activities and assist users in document layout analysis. Evaluated on the real commercial dataset, the method achieved high performance.",https://ieeexplore.ieee.org/document/8589497/,2018 International Conference on Advanced Computing and Applications (ACOMP),27-29 Nov. 2018,ieeexplore
10.1109/IJCNN48605.2020.9206730,Data-efficient Online Classification with Siamese Networks and Active Learning,IEEE,Conferences,"An ever increasing volume of data is nowadays becoming available in a streaming manner in many application areas, such as, in critical infrastructure systems, finance and banking, security and crime and web analytics. To meet this new demand, predictive models need to be built online where learning occurs on-the-fly. Online learning poses important challenges that affect the deployment of online classification systems to real-life problems. In this paper we investigate learning from limited labelled, nonstationary and imbalanced data in online classification. We propose a learning method that synergistically combines siamese neural networks and active learning. The proposed method uses a multi-sliding window approach to store data, and maintains separate and balanced queues for each class. Our study shows that the proposed method is robust to data nonstationarity and imbalance, and significantly outperforms baselines and state-of-the-art algorithms in terms of both learning speed and performance. Importantly, it is effective even when only 1% of the labels of the arriving instances are available.",https://ieeexplore.ieee.org/document/9206730/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/ICSESS47205.2019.9040821,DeepDroid: Feature Selection approach to detect Android malware using Deep Learning,IEEE,Conferences,"Smartphones are now able to use for various purposes such as online banking, social networking, web browsing, ubiquitous services, MMS, and more daily essential needs through various apps. However, these apps are highly vulnerable to various types of malware attacks attributed to their open nature and high popularity in the market. The fault lies in the underneath permission model of Android apps. These apps need several sensitive permissions during their installation and runtime, which enables possible security breaches by malware. Hence, there is a requirement to develop a malware detection that can provide an effective solution to defense the mobile user from any malicious threat. In this paper, we proposed a framework which works on the principals of feature selection methods and Deep Neural Network (DNN) as a classifier. In this study, we empirically evaluate 1,20,000 Android apps and applied five different feature selection techniques. Out of which by using a set of features formed by Principal component analysis (PCA)can able to detect 94% Android malware from real-world apps.",https://ieeexplore.ieee.org/document/9040821/,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),18-20 Oct. 2019,ieeexplore
10.1109/ICEIC51217.2021.9369796,Detection and Prevention of Cross-site Scripting Attack with Combined Approaches,IEEE,Conferences,"Cross-site scripting (XSS) attack is a kind of code injection that allows an attacker to inject malicious scripts code into a trusted web application. When a user tries to request the injected web page, he is not aware that the malicious script code might be affecting his computer. Nowadays, attackers are targeting the web applications that holding a sensitive data (e.g., bank transaction, e-mails, healthcare, and e-banking) to steal users' information and gain full access to the data which make the web applications to be more vulnerable. In this research, we applied three approaches to find a solution to this most challenging attacks issues. In the first approach, we implemented Random Forest (RF), Logistic Regression (LR), k-Nearest Neighbors (k-NN), and Support Vector Machine (SVM) algorithms to discover and classify XSS attack. In the second approach, we implemented the Content Security Policy (CSP) approach to detect XSS attacks in real-time. In the last approach, we propose a new approach that combines the Web Application Firewall (WAF), Intrusion Detection System (IDS), and Intrusion Prevention System (IPS) to detect and prevent XSS attack in real-time. Our experiment results demonstrated the high performance of AI algorithms. The CSP approach shows the results for the detection system report in real-time. In the third approach, we got more expected system results that make our third model system a more powerful tool to address this research problem than the other two approaches.",https://ieeexplore.ieee.org/document/9369796/,"2021 International Conference on Electronics, Information, and Communication (ICEIC)",31 Jan.-3 Feb. 2021,ieeexplore
10.1109/ICECA.2019.8821811,Detection of Permission Driven Malware in Android Using Deep Learning Techniques,IEEE,Conferences,"In today's busy world, usage of mobile applications is increasing in all aspects of life including banking and finance. Taking this as an opportunity, cyber crimes are taking place in the form of hacking and malware. While installing the mobile apps, everyone may not be aware of which permissions to accept and which one to deny. If the user start accepting all permissions, malware or malicious apk files may enter the mobile through some of them. Many machine learning techniques have been introduced to resolve this problem but failed to get considerable accuracy in real-time applications. The present research concentrates on detecting the malware which can enter through permissions in android using deep neural network model. The proposed approach detects the permission driven malware in real time android apk files with more than 85% accuracy.",https://ieeexplore.ieee.org/document/8821811/,"2019 3rd International conference on Electronics, Communication and Aerospace Technology (ICECA)",12-14 June 2019,ieeexplore
10.1109/ICNN.1996.548941,Determining the number of dimensions underlying customer-choices with a competitive neural network,IEEE,Conferences,"More and more data about consumer-behaviour is becoming available through the use of database systems and electronic-banking systems. In this paper we describe a neural network (NN)-based technique to extract some useful information from a database holding data about customers of an insurance-company. Using this NN, we try to learn more about the needs and wishes that lead consumers towards specific behaviour, so that better service can be provided in the future. We formulate a more formal version of this problem, describe the NN used, and show results on artificial and real data.",https://ieeexplore.ieee.org/document/548941/,Proceedings of International Conference on Neural Networks (ICNN'96),3-6 June 1996,ieeexplore
10.1109/ICDMW.2016.0185,Financial Data Analysis with PGMs Using AMIDST,IEEE,Conferences,"The AMIDST Toolbox an open source Java 8 library for scalable learning of probabilistic graphical models (PGMs) based on both batch and streaming data. An important application domain with streaming data characteristics is the banking sector, where we may want to monitor individual customers (based on their financial situation and behavior) as well as the general economic climate. Using a real financial data set from a Spanish bank, we have previously proposed and demonstrated a novel PGM framework for performing this type of data analysis with particular focus on concept drift. The framework is implemented in the AMIDST Toolbox, which was also used to conduct the reported analyses. In this paper, we provide an overview of the toolbox and illustrate with code examples how the toolbox can be used for setting up and performing analyses of this particular type.",https://ieeexplore.ieee.org/document/7836816/,2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),12-15 Dec. 2016,ieeexplore
10.1109/ICONSTEM.2017.8261274,Forecasting stock price using soft computing techniques,IEEE,Conferences,"Forecasting stock price is a important challenging task in the real world because more and more money is involved and they are affected by many social, economic, political and psychological factors. Numerous machine learning procedures have been used to fore see developments in stock cost. Machine learning classifiers include expanding the past encounters into the future actives. The proposed framework presents another hereditary calculation for forecast of monetary execution with information sets from a known source. The objective is to deliver a GA-based procedure for expectation of securities exchange execution alongside an acquainted principle classifier from an information set. We developed a genetic fuzzy based model with the ability of rule based extraction to predict next day stock price. We evaluate capacity of the proposed approach by applying it to the banking sector stocks and compare the outcome with the previous models.",https://ieeexplore.ieee.org/document/8261274/,2017 Third International Conference on Science Technology Engineering & Management (ICONSTEM),23-24 March 2017,ieeexplore
10.1109/ICIT52682.2021.9491636,Improvement of personal loans granting methods in banks using machine learning methods and approaches in Palestine,IEEE,Conferences,"For banking organizations, loan approval and risk assessment which is related is a very complex and significant process which is needs a high effort for relevant employee or manager to take a decision, because of manual or traditional methods that used in banks. The banking industry still needs a more precise method of predictive modeling for several problems. In general, for financial institutions and especially for banks forecasting credit defaulters is a hard challenge. The primary role of the current systems is to accept, or sending loan application to a specific level of approval to be studied and it is very difficult to foresee the probability of the borrower for paying the due dues amount without using methods to predict. Machine learning (ML) techniques and the algorithm that belongs to are a very amazing and promising technique in predicting for a large amount of data. Our research proposed to study three machine learning algorithms [1], Decision Tree (DT), Logistic Regression (LR), and Random Forest (RF), by using real data collected from Quds Bank with a variables that cover credit restriction and regulator instructions. The algorithm has been implemented to predict the loan approval of customers and the output tested in terms of the predicted accuracy.",https://ieeexplore.ieee.org/document/9491636/,2021 International Conference on Information Technology (ICIT),14-15 July 2021,ieeexplore
10.1109/SIST50301.2021.9465891,Information - Education Milieu: Methodology of Forming Knowledge Content and Practice Oriented Training of IT-disciplines,IEEE,Conferences,"The article herein considers the innovative methodology of forming the knowledge content, based on ontological model. The basic methodology concepts are project-oriented technology of CDIO training and graduate's competence model, and implementation tool is information - education milieu. Information - education milieu allows implement adaptive, in compliance with production requirements, knowledge trend planning and knowledge content forming both for specialty degree programs disciplines and individual training trajectory, using for those aims the smart-contract.For the discipline Technologies of processing the distributed applications there have been shown examples of using the technologies on modeling the knowledge, connected with designing the real time systems of parallel and distributed applications, where as a project there has been used the banking system.The methodology's software implementation has been fulfilled in the form of a web-application and currently it is going through approbation at the chair Computer and software engineering of Turan university.",https://ieeexplore.ieee.org/document/9465891/,2021 IEEE International Conference on Smart Information Systems and Technologies (SIST),28-30 April 2021,ieeexplore
10.1109/AI4G50087.2020.9311045,Innovative Multi-Step Anomaly Detection Algorithm with Real-World Implementation: Case Study in Supply Chain Management,IEEE,Conferences,"In all information systems it is very important to operate with correct information. Incorrect information can lead to many problems that can cause direct financial and reputation loss of the company. Data used by the system can be gathered by sensors, scripts or by hand. In all those cases, mistakes are possible. It is important to detect mistakes on time and stop them from propagating further into the system. In this paper, a novel multi-step anomaly detection algorithm based on the greatest common divisor and median value is described. The algorithm for anomaly detection in historical sales data is used as a part of the smart warehouse management system which is implemented in some of the largest distribution companies in Bosnia and Herzegovina. The algorithm showed significant results in anomaly detection on company orders and improved a number of processes in the operation of the smart warehouse management system. The algorithm described can also be used in other areas where the transaction data is collected, such as sales and banking,",https://ieeexplore.ieee.org/document/9311045/,2020 IEEE / ITU International Conference on Artificial Intelligence for Good (AI4G),21-25 Sept. 2020,ieeexplore
10.1109/FSKD.2013.6816316,Integration of heterogeneous data for real world domain,IEEE,Conferences,"There are a large number of companies that have abundant amount of data presented in unstructured forms. For example, accounting firms and banking sectors have clients who may provide financial data for them. However, these clients have data which is of diverse nature. To make this data presentable in a meaningful way, companies usually use their employees to key in data manually to their own systems. This approach is tedious and labor intensive. To facilitate the data processing, these companies normally prefer to have automated softwares that can easily recognize the different formats of data, process and present them in a usable form suitable for further processing. However, there are currently no automated software tools which can perform this task. In this paper, we propose a novel ontological tool in this study to extract data from heterogeneous sources. Our tool will be able to analyze the semantic properties of the various data.",https://ieeexplore.ieee.org/document/6816316/,2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),23-25 July 2013,ieeexplore
10.1109/SDS.2018.8370412,Intel Optane™ technology as differentiator for internet of everything and fog computing,IEEE,Conferences,"Traditional network and cloud solutions cannot address effectively the infrastructure and data architecture challenges introduced by the emergence of the Internet of Everything (IoE). IoE creates unprecedented volume of complex data which must be stored, transferred, processed and analyzed in real time for time sensitive applications. Indeed, most of the applications in IoE domain (autonomous driving, health monitoring, banking, industrial control systems ...) are time and quality of service sensitive and need new solutions for data integrity and data availability for successful decision. At Intel, we are working on new computer paradigm and storage technologies such as Intel® Optane™ SSDs and Intel® 3D NAND SSDs that can address some of these challenges. With Intel® Optane™, it is now possible to create a high performance and real time available Software-Defined Infrastructure (SDI) by enabling full dis-aggregate and pool of the underlying hardware resources, creating distributed memory/storage imperatives and giving research community and enterprise the performance and capabilities to benefit from revolutionary technologies such as Fog Computing and Artificial Intelligence by dynamically assigning compute, storage and network resource in real-time time sensitive workloads. In this keynote, we will discuss data storage, availability, integrity, quality of service and movement in this compute paradigm shift. We will also touch on how it impacts mobile edge, fog computing and cloud infrastructures. We will then conclude with the challenges and opportunities this new solution will bring.",https://ieeexplore.ieee.org/document/8370412/,2018 Fifth International Conference on Software Defined Systems (SDS),23-26 April 2018,ieeexplore
10.1109/FMEC.2018.8364035,Keynote speech 3: Intel Optane™ technology as differentiator for Internet of everything and fog computing,IEEE,Conferences,"Traditional network and cloud solutions cannot address effectively the infrastructure and data architecture challenges introduced by the emergence of the Internet of Everything (IoE). IoE creates unprecedented volume of complex data which must be stored, transferred, processed and analyzed in real time for time sensitive applications. Indeed, most of the applications in IoE domain (autonomous driving, health monitoring, banking, industrial control systems ...) are time and quality of service sensitive and need new solutions for data integrity and data availability for successful decision. At Intel, we are working on new computer paradigm and storage technologies such as Intel® Optane™ SSDs and Intel® 3D NAND SSDs that can address some of these challenges. With Intel® Optane™, it is now possible to create a high performance and real time available Software-Defined Infrastructure (SDI) by enabling full dis-aggregate and pool of the underlying hardware resources, creating distributed memory/storage imperatives and giving research community and enterprise the performance and capabilities to benefit from revolutionary technologies such as Fog Computing and Artificial Intelligence by dynamically assigning compute, storage and network resource in real-time time sensitive workloads. In this keynote, we will discuss data storage, availability, integrity, quality of service and movement in this compute paradigm shift. We will also touch on how it impacts mobile edge, fog computing and cloud infrastructures. We will then conclude with the challenges and opportunities this new solution will bring.",https://ieeexplore.ieee.org/document/8364035/,2018 Third International Conference on Fog and Mobile Edge Computing (FMEC),23-26 April 2018,ieeexplore
10.1109/i-PACT52855.2021.9696448,Loan Prediction Using Machine Learning and Its Deployement On Web Application,IEEE,Conferences,"Loan prediction is one of the most important and most prominent research areas in the field of banking and insurance sectors. In the modern environment identifying and analyzing the patterns of the obtained sample dataset plays a vital role in this era. The loan prediction involves the application of various machine learning algorithms. There are some prediction systems in the market using deep learning and so on. But those are limited with certain features and cannot assist the users beyond those limits. The loan prediction project is developed using machine learning algorithms such as logistic regression. The Python programming language is used for the implementation of the code which has been developed in Colab and the html pages are developed for deployment of website using Visual Studio code. The proposed system can deliver high accuracy results and moderate loss for training and validate data. Finally, the results show the model implemented with high accuracy. Further, this work can be extended in order to improve the focus where the high accuracy can be obtained.",https://ieeexplore.ieee.org/document/9696448/,2021 Innovations in Power and Advanced Computing Technologies (i-PACT),27-29 Nov. 2021,ieeexplore
10.1109/ICMLA.2015.72,Malware Detection in Android-Based Mobile Environments Using Optimum-Path Forest,IEEE,Conferences,"Nowadays, people use smartphones and tablets with the very same purposes as desktop computers: web browsing, social networking and home-banking, just to name a few. However, we are often facing the problem of keeping our information protected and trustworthy. As a result of their popularity and functionality, mobile devices are a growing target for malicious activities. In such context, mobile malwares have gained significant ground since the emergence and growth of smartphones and handheld devices, becoming a real threat. In this paper, we introduced a recently developed pattern recognition technique called Optimum-Path Forest in the context of malware detection, as well we present ""DroidWare"", a new public dataset to foster the research on mobile malware detection. In addition, we also proposed to use Restricted Boltzmann Machines for unsupervised feature learning in the context of malware identification.",https://ieeexplore.ieee.org/document/7424412/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore
10.1109/SNPD-SAWN.2005.54,Modeling and analysis of non-functional requirements as aspects in a UML based architecture design,IEEE,Conferences,"The problem of effectively designing and analyzing software system to meet its nonfunctional requirements such as performance, security, and adaptability is critical to the system's success. The significant benefits of such work include detecting and removing defects earlier, reducing development time and cost while improving the quality. The formal design analysis framework (FDAF) is an aspect-oriented approach that supports the design and analysis of non-functional requirements for distributed, real-time systems. In the FDAF, nonfunctional requirements are defined as reusable aspects in the repository and the conventional UML has been extended to support the design of these aspects. FDAF supports the automated translation of extended, aspect-oriented UML designs into existing formal notations, leveraging an extensive body of formal methods work. In this paper, the design and analysis of response time performance aspect is described. An example system, the ATM/banking system has been used to illustrate this process.",https://ieeexplore.ieee.org/document/1434886/,"Sixth International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing and First ACIS International Workshop on Self-Assembling Wireless Network",23-25 May 2005,ieeexplore
10.1109/SMCIA.2005.1466968,Neuro-classification of fatigued bill based on tensional acoustic signal,IEEE,Conferences,"In the practical use of automated teller machines (ATM's), dealing with much fatigued bills causes serious trouble. To avoid this problem, rapid development of automatic classification methods that can be implemented on banking machines is desired. We propose a new automatic classification method of fatigued bill based on acoustic signal feature of a banking machine. Feeding a bill to a banking machine, a typical acoustic signal is emitted in the transportation part of the machine by tensioning the slackness of the bill transportation. The proposed method focuses on the fact that the tensional acoustic signal features differ in fatigue level of the bill, and uses spectral information of the tensional acoustic signal as the feature for classification of fatigued bill. The proposed method also uses the self organizing map (SOM) type neural network as the classifier to get high classification performance. Simulation results by using real tensional acoustic signal show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/1466968/,"Proceedings of the 2005 IEEE Midnight-Summer Workshop on Soft Computing in Industrial Applications, 2005. SMCia/05.",28-30 June 2005,ieeexplore
10.1109/SMARTCOMP.2017.7946997,PhD Forum: Deep Learning-Based Real-Time Malware Detection with Multi-Stage Analysis,IEEE,Conferences,"Protecting computer systems is a critical and ongoing problem, given that real-time malware detection is hard. The state-of-the-art for defense cannot keep pace with the increasing level of sophistication of malware. The industry, for instance, relies heavily on anti-virus technology for threat, which is effective for malware with known signatures, but not sustainable given the massive amount of malware samples released daily, as well as and its inefficacy in dealing with zero-day and polymorphic/metamorphic malware (practical detection rates range from 25% to 50%). Behavior-based approaches attempt to identify malware behaviors using instruction sequences, computation trace logic, and system (or API) call sequences. These solutions have been mostly based on conventional machine learning (ML) models with hand-craft features, such as K-nearest neighbor, SVM, and decision tree algorithms. However, current solutions based on ML suffer from high false-positive rates, mainly because of (i) the complexity and diversity of current software and malware, which are hard to capture during the learning phase of thealgorithms, (ii) sub-optimal feature extraction, and (iii) limited/outdated dataset. Since malware has been continuously evolving, existing protection mechanisms do not cope well with the increasedsophistication and complexity of these attacks, especially those performed by advanced persistent threats (APT), which are multi-module, stealthy, and target- focused. Furthermore, malware campaigns are not homogeneous--malware sophistication varies depending on the target, the type of service exploited as part of the attack (e.g., Internet Banking, relationship sites), the attack spreading source (e.g., phishing, drive-by downloads), and the location of the target. The accuracy of malware classification depends on gaining sufficient context information and extracting meaningful abstraction of behaviors. In problems about detecting malicious behavior based on sequence of system calls, longer sequences likely contain more information. However, classical ML- based detectors (i.e., Random Forest, Naive Bayes) often use short windows of system calls during the decision process and may not be able to extract enough features for accurate detection in a long term window. Thus, the main drawback of such approaches is to accomplish accurate detection, since it is difficult to analyze complex and longer sequences of malicious behaviors with limited window sizes, especially when malicious and benign behaviors are interposed. In contrast, Deep Learning models are capable of analyzing longer sequences of system calls and making better decisions through higher level information extraction and semantic knowledge learning. However, Deep Learning requires more computation time to estimate the probability of detection when the model needs to be retrained incrementally, a common requirement for malware detection when new variants and samples are frequently added to the training set. The trade-off is challenging: fast and not-so-accurate (classical ML methods) versus time-consuming and accurate detection (emerging Deep Learning methods). Our proposal is to leverage the best of the two worlds with Spectrum, a practical multi-stage malware- detection system operating in collaboration with the operating system (OS).",https://ieeexplore.ieee.org/document/7946997/,2017 IEEE International Conference on Smart Computing (SMARTCOMP),29-31 May 2017,ieeexplore
10.1109/ITNG.2010.117,Predicting Phishing Websites Using Classification Mining Techniques with Experimental Case Studies,IEEE,Conferences,"Classification Data Mining (DM) Techniques can be a very useful tool in detecting and identifying e-banking phishing websites. In this paper, we present a novel approach to overcome the difficulty and complexity in detecting and predicting e-banking phishing website. We proposed an intelligent resilient and effective model that is based on using association and classification Data Mining algorithms. These algorithms were used to characterize and identify all the factors and rules in order to classify the phishing website and the relationship that correlate them with each other. We implemented six different classification algorithm and techniques to extract the phishing training data sets criteria to classify their legitimacy. We also compared their performances, accuracy, number of rules generated and speed. A Phishing Case study was applied to illustrate the website phishing process. The rules generated from the associative classification model showed the relationship between some important characteristics like URL and Domain Identity, and Security and Encryption criteria in the final phishing detection rate. The experimental results demonstrated the feasibility of using Associative Classification techniques in real applications and its better performance as compared to other traditional classifications algorithms.",https://ieeexplore.ieee.org/document/5501434/,2010 Seventh International Conference on Information Technology: New Generations,12-14 April 2010,ieeexplore
10.1109/BIGCOMP.2019.8679121,Question Understanding Based on Sentence Embedding on Dialog Systems for Banking Service,IEEE,Conferences,"This paper introduce a question understanding system to respond appropriate answers in a dialog system for banking services. The question understanding system provides an automated response service in a specific domain (e.g. banking). This can increase response rate of a customer counseling service, and improve business efficiency and expertise. The question understanding system classify domains, specific categories, and speech acts of questions. Finally, the system analyze meanings and intents of the questions, and searching correct answers even various input sentences. In this paper, we describe methods of keyword tokenizing, pattern recognition, sentence embedding, analyzing dialogue intention, and searching similar FAQs. Through these methods, we have developed the question understanding unit in a real interactive system for financial services for real insurance companies and banks, and analyze the usefulness of the system through practical system implementation examples.",https://ieeexplore.ieee.org/document/8679121/,2019 IEEE International Conference on Big Data and Smart Computing (BigComp),27 Feb.-2 March 2019,ieeexplore
10.1109/ISAMSR53229.2021.9567891,Recent and Future Innovative Artificial Intelligence Services and Fields,IEEE,Conferences,"Recent innovative Artificial Intelligence (AI) solutions accelerate digital transformations in different fields. It is important to highlight and explore this innovative AI service in different domains so that digital transformation can be planned, designed, and implemented for maximum society benefits. This paper investigates the different fields of AI services that can be utilized towards achieving highly beneficial digital transformations in different societal domains. This includes marketing, finance and banking, healthcare industry, emotion, and creative AI, as well as recent AI fields as explainable and responsible AI. Exploring and understanding the innovative AI services in these domains widen researcher capabilities to achieve effective and highly beneficial digital transformations.",https://ieeexplore.ieee.org/document/9567891/,"2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)",6-8 Sept. 2021,ieeexplore
10.1109/MIPRO.2016.7522376,Reciprocal payers identification in banking logs using SAT solvers,IEEE,Conferences,"In this paper we presented solvers for satisfiability testing (SAT) as a novel approach to finding reciprocal payers in banking logs. A term “reciprocal payers” is usually treated as general fraud by using standard techniques such as expert systems, machine learning and in recent times social network analysis. SAT as a technique for data analysis was abandoned due to the unfeasibility of SAT solvers. SAT solvers, however continued to develop in the hardware and software verification communities. We presented a proof-of-concept solution for identification of reciprocal payers (formally called a clique), which is a group of bank clients that issue payments to each other (each member to each member). We do not use real data due to client confidentiality, but the reader can see the principle. In the basic approach it is assumed that each client has only one account, and in the extended, second approach, it was allowed that a client can have more than one account.",https://ieeexplore.ieee.org/document/7522376/,"2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",30 May-3 June 2016,ieeexplore
10.1109/ISCON52037.2021.9702445,Role of Artificial Intelligence and Internet of Things in Promoting Banking and Financial Services During COVID-19: Pre and Post Effect,IEEE,Conferences,"The covid-19 outbreak has triggered an economic crisis and severely affected the global economy. The revolution of AI and the Internet of things (IoT) is helping in reshaping the traditional financial sector by consolidating technology, finance, and economics. This paper highlights how the integration of Artificial intelligence (AI) in IoT is helping in promoting digital financial inclusion during the COVID-19. Through multiple real-life case studies, this article examines the successful implementation of AI and IoT in banking and financial institutions. The paper also explores the advantages and opportunities arising by the use of AI and IoT in the financial sector. A descriptive research approach has been followed to investigate the pre and post effect of COVID-19 and how it is leading towards economic efficiency.",https://ieeexplore.ieee.org/document/9702445/,2021 5th International Conference on Information Systems and Computer Networks (ISCON),22-23 Oct. 2021,ieeexplore
10.1109/COMITCon.2019.8862225,Stock Market Analysis using Supervised Machine Learning,IEEE,Conferences,"Stock market or Share market is one of the most complicated and sophisticated way to do business. Small ownerships, brokerage corporations, banking sector, all depend on this very body to make revenue and divide risks; a very complicated model. However, this paper proposes to use machine learning algorithm to predict the future stock price for exchange by using open source libraries and preexisting algorithms to help make this unpredictable format of business a little more predictable. We shall see how this simple implementation will bring acceptable results. The outcome is completely based on numbers and assumes a lot of axioms that may or may not follow in the real world so as the time of prediction.",https://ieeexplore.ieee.org/document/8862225/,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",14-16 Feb. 2019,ieeexplore
10.1109/ICDIM.2012.6360106,Toward a new classification model for analysing financial datasets,IEEE,Conferences,"Nowadays, financial data analysis is becoming increasingly urgent in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make suitable decisions for new customer requests, e.g. user credit category, confidence of expected return, etc. Banking and financial institutions have applied various data mining techniques to improve their decision-making processes. However, naive approaches of data mining techniques could raise performance issues in analysing very large and complex financial data. In this paper, we present a classification model for analysing efficiently these financial data. We also evaluate the performance of our model with different real-world data from transaction to stock and credit rating, etc., and we show that it is efficient, robust, and well suited for these data.",https://ieeexplore.ieee.org/document/6360106/,Seventh International Conference on Digital Information Management (ICDIM 2012),22-24 Aug. 2012,ieeexplore
10.1109/ICRITO51393.2021.9596273,Trust in Banking Management System using Firebase in Python using AI,IEEE,Conferences,"The Banking Management System (BMS) is windows based Trusted GUI application which can be used to store the data of bank account holders into a trusted real time database in Firebase which could be later used to fetch the details of account by any system using internet. Multiple users can have access at the same time. The main objective of Banking Management System is to allow the administrator/client or the user to edit and access the details of the bank account online. It also allows multiple users/clients to have access from different systems at the same time using internet. Hence, the same data will access by the banks as well as the account holders. It fetches the data from real time database of firebase very quick within seconds. The Scope of BMS is to help the banks to allow their account holders or users to gain access onto their particular account from sitting at home itself using internet. Making their data accessible to them online. This helps the users to make their data safe in database and also leads to built trust among the banks and their customers.",https://ieeexplore.ieee.org/document/9596273/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore
10.1109/eSmarTA52612.2021.9515728,Yemeni Banknote Recognition Model based on Convolution Neural Networks,IEEE,Conferences,"Differentiating between paper currencies with different designs is a challenging task for visually impaired individuals and automated banking machines. Safe and accurate paper currency recognition systems are highly required, because of the wide use of Automated Teller Machines (ATMs), foreign exchange, automatic selling of goods, and automated banking services. With the advances of pattern recognition techniques, many real-life problems have been resolved. This paper presents a robust method to recognize various Yemeni paper currency using pre-trained models. To perform effective recognition processes, deep learning approach is used. Three pre-trained models are implemented which are AlexNet, DenseNet121, and ResNet50. The obtained results of the proposed method achieved high validation accuracy. This robust model reaches a value of 96.99%, 96.83%, and 99.04% in terms of validation accuracy using ResNet-50, DenseNet121, and AlexNet respectively.",https://ieeexplore.ieee.org/document/9515728/,2021 1st International Conference on Emerging Smart Technologies and Applications (eSmarTA),10-12 Aug. 2021,ieeexplore
10.1109/JIOT.2020.2975779,AUTo<italic>Sen</italic>: Deep-Learning-Based Implicit Continuous Authentication Using Smartphone Sensors,IEEE,Journals,"Smartphones have become crucial for our daily life activities and are increasingly loaded with our personal information to perform several sensitive tasks, including, mobile banking and communication, and are used for storing private photos and files. Therefore, there is a high demand for applying usable authentication techniques that prevent unauthorized access to sensitive information. In this article, we propose AUToSen, a deep-learning-based active authentication approach that exploits sensors in consumer-grade smartphones to authenticate a user. Unlike conventional approaches, AUToSen is based on deep learning to identify user distinct behavior from the embedded sensors with and without the user's interaction with the smartphone. We investigate different deep learning architectures in modeling and capturing users' behavioral patterns for the purpose of authentication. Moreover, we explore the sufficiency of sensory data required to accurately authenticate users. We evaluate AUToSen on a real-world data set that includes sensors data of 84 participants' smartphones collected using our designed data-collection application. The experiments show that AUToSen operates accurately using readings of only three sensors (accelerometer, gyroscope, and magnetometer) with a high authentication frequency, e.g., one authentication attempt every 0.5 s. Using sensory data of one second enables an authentication F1-score of approximately 98%, false acceptance rate (FAR) of 0.95%, false rejection rate (FRR) of 6.67%, and equal error rate (EER) of 0.41%. While using sensory data of half a second enables an authentication F1-score of 97.52%, FAR of 0.96%, FRR of 8.08%, and EER of 0.09%. Moreover, we investigate the effects of using different sensory data at variable sampling periods on the performance of the authentication models under various settings and learning architectures.",https://ieeexplore.ieee.org/document/9007368/,IEEE Internet of Things Journal,June 2020,ieeexplore
10.1109/TII.2012.2232935,Fuzzy Refinement Domain Adaptation for Long Term Prediction in Banking Ecosystem,IEEE,Journals,"Long-term bank failure prediction is a challenging real world problem in banking ecosystem and machine learning methods have been recently applied to improve the prediction accuracy. However, traditional machine learning methods assume that the training data and the test data are drawn from the same distribution, which is hard to be met in real world banking applications. This paper proposes a novel algorithm known as fuzzy refinement domain adaptation to solve this problem based on the ecosystem-oriented architecture. The algorithm utilizes the fuzzy system and similarity/dissimilarity concepts to modify the target instances' labels which were initially predicted by a shift-unaware prediction model. It employs a classifier to modify the label values of target instances based on their similarity/dissimilarity to the candidate positive and negative instances in mixture domains. Thirty six experiments are performed using three different shift-unaware prediction models. In these experiments bank failure financial data is used to evaluate the algorithm. The results demonstrate that the proposed algorithm significantly improves predictive accuracy and outperforms other refinement algorithms.",https://ieeexplore.ieee.org/document/6419821/,IEEE Transactions on Industrial Informatics,May 2014,ieeexplore
10.1109/ACCESS.2020.3015616,Intelligent Performance-Aware Adaptation of Control Policies for Optimizing Banking Teller Process Using Machine Learning,IEEE,Journals,"In the current banking systems and business processes, the permission granted to employees is controlled and managed by the configured access control methods, in which static role-based models focus on access to information and functions. The deployed configuration is not reviewed/updated systematically and is handled manually by managers. Consequently, banks and companies are looking for systems and applications to automate and optimize their business processes and data management intelligently. In this context, the notion of integrating machine learning (ML) techniques in banking business processes has emerged. In order to build an intelligent and systematic solution, we combine in this paper ML and dynamic authorization techniques to enable performance-based policy evaluation into the banking teller process, where policies adapt to the changes recognized by the ML model. The objective of this work is to focus on the banking teller process that may be generalized to other operational banking processes. In this context, we propose in this paper a new model providing Intelligent Performance-Aware Adaptation of Roles and Policy Control using a support vector machine (SVM). We demonstrate that our model is capable of assessing the deployed control policies and updating them systematically with new roles and authorization levels based on tellers' performance, work history, and system constraints. We evaluated different machine learning models on a real dataset generated from a real-life banking environment. Experimental results explore the relevance and efficiency of our proposed scheme in terms of prediction accuracy, required authorizations, transaction time, and employees' working hours.",https://ieeexplore.ieee.org/document/9163345/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2927417,MLPXSS: An Integrated XSS-Based Attack Detection Scheme in Web Applications Using Multilayer Perceptron Technique,IEEE,Journals,"Dynamic web applications play a vital role in providing resources manipulation and interaction between clients and servers. The features presently supported by browsers have raised business opportunities, by supplying high interactivity in web-based services, like web banking, e-commerce, social networking, forums, and at the same time, these features have brought serious risks and increased vulnerabilities in web applications that enable cyber-attacks to be executed. One of the common high-risk cyber-attack of web application vulnerabilities is cross-site scripting (XSS). Nowadays, XSS is still dramatically increasing and considered as one of the most severe threats for organizations, users, and developers. If the ploy is successful, the victim is at the mercy of the cybercriminals. In this research, a robust artificial neural network-based multilayer perceptron (MLP) scheme integrated with the dynamic feature extractor is proposed for XSS attack detection. The detection scheme adopts a large real-world dataset, the dynamic features extraction mechanism, and MLP model, which successfully surpassed several tests on an employed unique dataset under careful experimentation, and achieved promising and state-of-the-art results with accuracy, detection probabilities, false positive rate, and AUC-ROC scores of 99.32%, 98.35 %, 0.3%, and 99.02%, respectively. Therefore, it has the potentials to be applied for XSS-based attack detection in either the client-side or the server-side.",https://ieeexplore.ieee.org/document/8756243/,IEEE Access,2019,ieeexplore
10.1109/TNN.2007.912300,PSECMAC: A Novel Self-Organizing Multiresolution Associative Memory Architecture,IEEE,Journals,"The cerebellum constitutes a vital part of the human brain system that possesses the capability to model highly nonlinear physical dynamics. The cerebellar model articulation controller (CMAC) associative memory network is a computational model inspired by the neurophysiological properties of the cerebellum, and it has been widely used for control, optimization, and various pattern recognition tasks. However, the CMAC network's highly regularized computing structure often leads to the following: (1) a suboptimal modeling accuracy, (2) poor memory utilization, and (3) the generalization-accuracy dilemma. Previous attempts to address these shortcomings have limited success and the proposed solutions often introduce a high operational complexity to the CMAC network. This paper presents a novel neurophysiologically inspired associative memory architecture named pseudo-self-evolving CMAC (PSECMAC) that nonuniformly allocates its computing cells to overcome the architectural deficiencies encountered by the CMAC network. The nonuniform memory allocation scheme employed by the proposed PSECMAC network is inspired by the cerebellar experience-driven synaptic plasticity phenomenon observed in the cerebellum, where significantly higher densities of synaptic connections are located in the frequently accessed regions. In the PSECMAC network, this biological synaptic plasticity phenomenon is emulated by employing a data-driven adaptive memory quantization scheme that defines its computing structure. A neighborhood-based activation process is subsequently implemented to facilitate the learning and computation of the PSECMAC structure. The training stability of the PSECMAC network is theoretically assured by the proof of its learning convergence, which will be presented in this paper. The performance of the proposed network is subsequently bench- marked against the CMAC network and several representative CMAC variants on three real-life applications, namely, pricing of currency futures option, banking failure classification, and modeling of the glucose-insulin dynamics of the human glucose metabolic process. The experimental results have strongly demonstrated the effectiveness of the PSECMAC network in addressing the architectural deficiencies of the CMAC network by achieving significant improvements in the memory utilization, output accuracy as well as the generalization capability of the network.",https://ieeexplore.ieee.org/document/4469949/,IEEE Transactions on Neural Networks,April 2008,ieeexplore
