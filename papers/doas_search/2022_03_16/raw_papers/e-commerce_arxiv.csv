id,updated,published,title,summary,database
http://arxiv.org/abs/2202.10574v1,2022-02-21T23:36:40Z,2022-02-21T23:36:40Z,"A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation
  in Two-sided Markets","The two-sided markets such as ride-sharing companies often involve a group of
subjects who are making sequential decisions across time and/or location. With
the rapid development of smart phones and internet of things, they have
substantially transformed the transportation landscape of human beings. In this
paper we consider large-scale fleet management in ride-sharing companies that
involve multiple units in different areas receiving sequences of products (or
treatments) over time. Major technical challenges, such as policy evaluation,
arise in those studies because (i) spatial and temporal proximities induce
interference between locations and times; and (ii) the large number of
locations results in the curse of dimensionality. To address both challenges
simultaneously, we introduce a multi-agent reinforcement learning (MARL)
framework for carrying policy evaluation in these studies. We propose novel
estimators for mean outcomes under different products that are consistent
despite the high-dimensionality of state-action space. The proposed estimator
works favorably in simulation experiments. We further illustrate our method
using a real dataset obtained from a two-sided marketplace company to evaluate
the effects of applying different subsidizing policies. A Python implementation
of the proposed method is available at
https://github.com/RunzheStat/CausalMARL.",arxiv
http://arxiv.org/abs/2108.02419v1,2021-08-05T07:29:24Z,2021-08-05T07:29:24Z,Implementing the BBE Agent-Based Model of a Sports-Betting Exchange,"We describe three independent implementations of a new agent-based model
(ABM) that simulates a contemporary sports-betting exchange, such as those
offered commercially by companies including Betfair, Smarkets, and Betdaq. The
motivation for constructing this ABM, which is known as the Bristol Betting
Exchange (BBE), is so that it can serve as a synthetic data generator,
producing large volumes of data that can be used to develop and test new
betting strategies via advanced data analytics and machine learning techniques.
Betting exchanges act as online platforms on which bettors can find willing
counterparties to a bet, and they do this in a way that is directly comparable
to the manner in which electronic financial exchanges, such as major stock
markets, act as platforms that allow traders to find willing counterparties to
buy from or sell to: the platform aggregates and anonymises orders from
multiple participants, showing a summary of the market that is updated in
real-time. In the first instance, BBE is aimed primarily at producing synthetic
data for in-play betting (also known as in-race or in-game betting) where
bettors can place bets on the outcome of a track-race event, such as a horse
race, after the race has started and for as long as the race is underway, with
betting only ceasing when the race ends. The rationale for, and design of, BBE
has been described in detail in a previous paper that we summarise here, before
discussing our comparative results which contrast a single-threaded
implementation in Python, a multi-threaded implementation in Python, and an
implementation where Python header-code calls simulations of the track-racing
events written in OpenCL that execute on a 640-core GPU -- this runs
approximately 1000 times faster than the single-threaded Python. Our
source-code for BBE is freely available on GitHub.",arxiv
http://arxiv.org/abs/2107.13212v1,2021-07-28T07:52:09Z,2021-07-28T07:52:09Z,"The Trip to The Enterprise Gourmet Data Product Marketplace through a
  Self-service Data Platform","Data Analytics provides core business reporting needs in many software
companies, acts as a source of truth for key information, and enables building
advanced solutions, e.g., predictive models, machine learning, real-time
recommendations, to grow the business.
  A self-service, multi-tenant, API-first, and scalable data platform is the
foundational requirement in creating an enterprise data marketplace, which
enables the creation, publishing, and exchange of data products. Such a
marketplace enables the exploration and discovery of data products, further
providing high-level data governance and oversight on marketplace contents. In
this paper, we describe our way to the gourmet data product marketplace. We
cover the design principles, the implementation details, technology choices,
and the journey to build an enterprise data platform that meets the above
characteristics. The platform consists of ingestion, streaming, storage,
transformation, schema generation, fail-safe, data sharing, access management,
PII data automatic identification, self-service storage optimization
recommendations, and CI/CD integration.
  We then show how the platform enables and operates the data marketplace,
facilitating the exchange of stable data products across users and tenants. We
motivate and show how we run scalable decentralized data governance. All of
this is built and run for Cimpress Technology (CT), which operates the Mass
Customization Platform for Cimpress and its businesses. The CT data platform
serves 1000s of users from different platform participants, with data sourced
from heterogeneous sources. Data is ingested at a rate of well over 1000
individual messages per second and serves more than 100k analytical queries
daily.",arxiv
http://arxiv.org/abs/2106.08437v1,2021-06-15T21:09:19Z,2021-06-15T21:09:19Z,Deep reinforcement learning on a multi-asset environment for trading,"Financial trading has been widely analyzed for decades with market
participants and academics always looking for advanced methods to improve
trading performance. Deep reinforcement learning (DRL), a recently
reinvigorated method with significant success in multiple domains, still has to
show its benefit in the financial markets. We use a deep Q-network (DQN) to
design long-short trading strategies for futures contracts. The state space
consists of volatility-normalized daily returns, with buying or selling being
the reinforcement learning action and the total reward defined as the
cumulative profits from our actions. Our trading strategy is trained and tested
both on real and simulated price series and we compare the results with an
index benchmark. We analyze how training based on a combination of artificial
data and actual price series can be successfully deployed in real markets. The
trained reinforcement learning agent is applied to trading the E-mini S&P 500
continuous futures contract. Our results in this study are preliminary and need
further improvement.",arxiv
http://arxiv.org/abs/2102.04667v1,2021-02-09T06:31:20Z,2021-02-09T06:31:20Z,"Virtual ID Discovery from E-commerce Media at Alibaba: Exploiting
  Richness of User Click Behavior for Visual Search Relevance","Visual search plays an essential role for E-commerce. To meet the search
demands of users and promote shopping experience at Alibaba, visual search
relevance of real-shot images is becoming the bottleneck. Traditional visual
search paradigm is usually based upon supervised learning with labeled data.
However, large-scale categorical labels are required with expensive human
annotations, which limits its applicability and also usually fails in
distinguishing the real-shot images. In this paper, we propose to discover
Virtual ID from user click behavior to improve visual search relevance at
Alibaba. As a totally click-data driven approach, we collect various types of
click data for training deep networks without any human annotations at all. In
particular, Virtual ID are learned as classification supervision with co-click
embedding, which explores image relationship from user co-click behaviors to
guide category prediction and feature learning. Concretely, we deploy Virtual
ID Category Network by integrating first-clicks and switch-clicks as
regularizer. Incorporating triplets and list constraints, Virtual ID Feature
Network is trained in a joint classification and ranking manner. Benefiting
from exploration of user click data, our networks are more effective to encode
richer supervision and better distinguish real-shot images in terms of category
and feature. To validate our method for visual search relevance, we conduct an
extensive set of offline and online experiments on the collected real-shot
images. We consistently achieve better experimental results across all
components, compared with alternative and state-of-the-art methods.",arxiv
http://arxiv.org/abs/2102.00333v1,2021-01-30T23:05:04Z,2021-01-30T23:05:04Z,"Deep Reinforcement Learning-Based Product Recommender for Online
  Advertising","In online advertising, recommender systems try to propose items from a list
of products to potential customers according to their interests. Such systems
have been increasingly deployed in E-commerce due to the rapid growth of
information technology and availability of large datasets. The ever-increasing
progress in the field of artificial intelligence has provided powerful tools
for dealing with such real-life problems. Deep reinforcement learning (RL) that
deploys deep neural networks as universal function approximators can be viewed
as a valid approach for design and implementation of recommender systems. This
paper provides a comparative study between value-based and policy-based deep RL
algorithms for designing recommender systems for online advertising. The
RecoGym environment is adopted for training these RL-based recommender systems,
where the long short term memory (LSTM) is deployed to build value and policy
networks in these two approaches, respectively. LSTM is used to take account of
the key role that order plays in the sequence of item observations by users.
The designed recommender systems aim at maximising the click-through rate (CTR)
for the recommended items. Finally, guidelines are provided for choosing proper
RL algorithms for different scenarios that the recommender system is expected
to handle.",arxiv
http://arxiv.org/abs/2101.04285v1,2021-01-12T04:12:18Z,2021-01-12T04:12:18Z,"Explainable Deep Behavioral Sequence Clustering for Transaction Fraud
  Detection","In e-commerce industry, user behavior sequence data has been widely used in
many business units such as search and merchandising to improve their products.
However, it is rarely used in financial services not only due to its 3V
characteristics - i.e. Volume, Velocity and Variety - but also due to its
unstructured nature. In this paper, we propose a Financial Service scenario
Deep learning based Behavior data representation method for Clustering
(FinDeepBehaviorCluster) to detect fraudulent transactions. To utilize the
behavior sequence data, we treat click stream data as event sequence, use time
attention based Bi-LSTM to learn the sequence embedding in an unsupervised
fashion, and combine them with intuitive features generated by risk experts to
form a hybrid feature representation. We also propose a GPU powered HDBSCAN
(pHDBSCAN) algorithm, which is an engineering optimization for the original
HDBSCAN algorithm based on FAISS project, so that clustering can be carried out
on hundreds of millions of transactions within a few minutes. The computation
efficiency of the algorithm has increased 500 times compared with the original
implementation, which makes flash fraud pattern detection feasible. Our
experimental results show that the proposed FinDeepBehaviorCluster framework is
able to catch missed fraudulent transactions with considerable business values.
In addition, rule extraction method is applied to extract patterns from risky
clusters using intuitive features, so that narrative descriptions can be
attached to the risky clusters for case investigation, and unknown risk
patterns can be mined for real-time fraud detection. In summary,
FinDeepBehaviorCluster as a complementary risk management strategy to the
existing real-time fraud detection engine, can further increase our fraud
detection and proactive risk defense capabilities.",arxiv
http://arxiv.org/abs/2012.10853v1,2020-12-20T06:06:08Z,2020-12-20T06:06:08Z,eTREE: Learning Tree-structured Embeddings,"Matrix factorization (MF) plays an important role in a wide range of machine
learning and data mining models. MF is commonly used to obtain item embeddings
and feature representations due to its ability to capture correlations and
higher-order statistical dependencies across dimensions. In many applications,
the categories of items exhibit a hierarchical tree structure. For instance,
human diseases can be divided into coarse categories, e.g., bacterial, and
viral. These categories can be further divided into finer categories, e.g.,
viral infections can be respiratory, gastrointestinal, and exanthematous viral
diseases. In e-commerce, products, movies, books, etc., are grouped into
hierarchical categories, e.g., clothing items are divided by gender, then by
type (formal, casual, etc.). While the tree structure and the categories of the
different items may be known in some applications, they have to be learned
together with the embeddings in many others. In this work, we propose eTREE, a
model that incorporates the (usually ignored) tree structure to enhance the
quality of the embeddings. We leverage the special uniqueness properties of
Nonnegative MF (NMF) to prove identifiability of eTREE. The proposed model not
only exploits the tree structure prior, but also learns the hierarchical
clustering in an unsupervised data-driven fashion. We derive an efficient
algorithmic solution and a scalable implementation of eTREE that exploits
parallel computing, computation caching, and warm start strategies. We showcase
the effectiveness of eTREE on real data from various application domains:
healthcare, recommender systems, and education. We also demonstrate the
meaningfulness of the tree obtained from eTREE by means of domain experts
interpretation.",arxiv
http://arxiv.org/abs/2011.14925v1,2020-11-26T02:37:39Z,2020-11-26T02:37:39Z,"Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy
  Trade-off","Graph data is ubiquitous in academia and industry, from social networks to
bioinformatics. The pervasiveness of graphs today has raised the demand for
algorithms that can answer various questions: Which products would a user like
to purchase given her order list? Which users are buying fake followers to
increase their public reputation? Myriads of new graph mining algorithms are
proposed every year to answer such questions - each with a distinct problem
formulation, computational time, and memory footprint. This lack of unity makes
it difficult for a practitioner to compare different algorithms and pick the
most suitable one for a specific application. These challenges - even more
severe for non-experts - create a gap in which state-of-the-art techniques
developed in academic settings fail to be optimally deployed in real-world
applications. To bridge this gap, we propose AUTOGM, an automated system for
graph mining algorithm development. We first define a unified framework
UNIFIEDGM that integrates various message-passing based graph algorithms,
ranging from conventional algorithms like PageRank to graph neural networks.
Then UNIFIEDGM defines a search space in which five parameters are required to
determine a graph algorithm. Under this search space, AUTOGM explicitly
optimizes for the optimal parameter set of UNIFIEDGM using Bayesian
Optimization. AUTOGM defines a novel budget-aware objective function for the
optimization to incorporate a practical issue - finding the best speed-accuracy
trade-off under a computation budget - into the graph algorithm generation
problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM
generates novel graph mining algorithms with the best speed/accuracy trade-off
compared to existing models with heuristic parameters.",arxiv
http://arxiv.org/abs/2011.12771v1,2020-11-25T14:33:18Z,2020-11-25T14:33:18Z,"Learning to Expand: Reinforced Pseudo-relevance Feedback Selection for
  Information-seeking Conversations","Intelligent personal assistant systems for information-seeking conversations
are increasingly popular in real-world applications, especially for e-commerce
companies. With the development of research in such conversation systems, the
pseudo-relevance feedback (PRF) has demonstrated its effectiveness in
incorporating relevance signals from external documents. However, the existing
studies are either based on heuristic rules or require heavy manual labeling.
In this work, we treat the PRF selection as a learning task and proposed a
reinforced learning based method that can be trained in an end-to-end manner
without any human annotations. More specifically, we proposed a reinforced
selector to extract useful PRF terms to enhance response candidates and a BERT
based response ranker to rank the PRF-enhanced responses. The performance of
the ranker serves as rewards to guide the selector to extract useful PRF terms,
and thus boost the task performance. Extensive experiments on both standard
benchmark and commercial datasets show the superiority of our reinforced PRF
term selector compared with other potential soft or hard selection methods.
Both qualitative case studies and quantitative analysis show that our model can
not only select meaningful PRF terms to expand response candidates but also
achieve the best results compared with all the baseline methods on a variety of
evaluation metrics. We have also deployed our method on online production in an
e-commerce company, which shows a significant improvement over the existing
online ranking system.",arxiv
http://arxiv.org/abs/2011.06458v5,2021-05-03T11:07:35Z,2020-11-12T15:59:18Z,"Golden Grain: Building a Secure and Decentralized Model Marketplace for
  MLaaS","ML-as-a-service (MLaaS) becomes increasingly popular and revolutionizes the
lives of people. A natural requirement for MLaaS is, however, to provide highly
accurate prediction services. To achieve this, current MLaaS systems integrate
and combine multiple well-trained models in their services. Yet, in reality,
there is no easy way for MLaaS providers, especially for startups, to collect
sufficiently well-trained models from individual developers, due to the lack of
incentives. In this paper, we aim to fill this gap by building up a model
marketplace, called as Golden Grain, to facilitate model sharing, which
enforces the fair model-money swapping process between individual developers
and MLaaS providers. Specifically, we deploy the swapping process on the
blockchain, and further introduce a blockchain-empowered model benchmarking
process for transparently determining the model prices according to their
authentic performances, so as to motivate the faithful contributions of
well-trained models. Especially, to ease the blockchain overhead for model
benchmarking, our marketplace carefully offloads the heavy computation and
designs a secure off-chain on-chain interaction protocol based on a trusted
execution environment (TEE), for ensuring both the integrity and authenticity
of benchmarking. We implement a prototype of our Golden Grain on the Ethereum
blockchain, and conduct extensive experiments using standard benchmark datasets
to demonstrate the practically affordable performance of our design.",arxiv
http://arxiv.org/abs/2010.12837v3,2021-06-17T08:03:30Z,2020-10-24T08:37:04Z,"XDM: Improving Sequential Deep Matching with Unclicked User Behaviors
  for Recommender System","Deep learning-based sequential recommender systems have recently attracted
increasing attention from both academia and industry. Most of industrial
Embedding-Based Retrieval (EBR) system for recommendation share the similar
ideas with sequential recommenders. Among them, how to comprehensively capture
sequential user interest is a fundamental problem. However, most existing
sequential recommendation models take as input clicked or purchased behavior
sequences from user-item interactions. This leads to incomprehensive user
representation and sub-optimal model performance, since they ignore the
complete user behavior exposure data, i.e., items impressed yet unclicked by
users. In this work, we attempt to incorporate and model those unclicked item
sequences using a new learning approach in order to explore better sequential
recommendation technique. An efficient triplet metric learning algorithm is
proposed to appropriately learn the representation of unclicked items. Our
method can be simply integrated with existing sequential recommendation models
by a confidence fusion network and further gain better user representation. The
offline experimental results based on real-world E-commerce data demonstrate
the effectiveness and verify the importance of unclicked items in sequential
recommendation. Moreover we deploy our new model (named XDM) into EBR of
recommender system at Taobao, outperforming the deployed previous generation
SDM.",arxiv
http://arxiv.org/abs/2009.14610v2,2021-06-18T13:43:00Z,2020-09-30T12:34:56Z,Concurrent Neural Network : A model of competition between times series,"Competition between times series often arises in sales prediction, when
similar products are on sale on a marketplace. This article provides a model of
the presence of cannibalization between times series. This model creates a
""competitiveness"" function that depends on external features such as price and
margin. It also provides a theoretical guaranty on the error of the model under
some reasonable conditions, and implement this model using a neural network to
compute this competitiveness function. This implementation outperforms other
traditional time series methods and classical neural networks for market share
prediction on a real-world data set.",arxiv
http://arxiv.org/abs/2009.09756v1,2020-09-21T10:58:07Z,2020-09-21T10:58:07Z,"Demand Prediction Using Machine Learning Methods and Stacked
  Generalization","Supply and demand are two fundamental concepts of sellers and customers.
Predicting demand accurately is critical for organizations in order to be able
to make plans. In this paper, we propose a new approach for demand prediction
on an e-commerce web site. The proposed model differs from earlier models in
several ways. The business model used in the e-commerce web site, for which the
model is implemented, includes many sellers that sell the same product at the
same time at different prices where the company operates a market place model.
The demand prediction for such a model should consider the price of the same
product sold by competing sellers along the features of these sellers. In this
study we first applied different regression algorithms for specific set of
products of one department of a company that is one of the most popular online
e-commerce companies in Turkey. Then we used stacked generalization or also
known as stacking ensemble learning to predict demand. Finally, all the
approaches are evaluated on a real world data set obtained from the e-commerce
company. The experimental results show that some of the machine learning
methods do produce almost as good results as the stacked generalization method.",arxiv
http://arxiv.org/abs/2008.00181v2,2021-01-14T00:15:23Z,2020-08-01T06:02:16Z,"Relation-aware Meta-learning for Market Segment Demand Prediction with
  Limited Records","E-commerce business is revolutionizing our shopping experiences by providing
convenient and straightforward services. One of the most fundamental problems
is how to balance the demand and supply in market segments to build an
efficient platform. While conventional machine learning models have achieved
great success on data-sufficient segments, it may fail in a large-portion of
segments in E-commerce platforms, where there are not sufficient records to
learn well-trained models. In this paper, we tackle this problem in the context
of market segment demand prediction. The goal is to facilitate the learning
process in the target segments by leveraging the learned knowledge from
data-sufficient source segments. Specifically, we propose a novel algorithm,
RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a
meta-learning paradigm. The multi-pattern fusion network considers both local
and seasonal temporal patterns for segment demand prediction. In the
meta-learning paradigm, transferable knowledge is regarded as the model
parameter initialization of MPFN, which are learned from diverse source
segments. Furthermore, we capture the segment relations by combining
data-driven segment representation and segment knowledge graph representation
and tailor the segment-specific relations to customize transferable model
parameter initialization. Thus, even with limited data, the target segment can
quickly find the most relevant transferred knowledge and adapt to the optimal
parameters. We conduct extensive experiments on two large-scale industrial
datasets. The results justify that our RMLDP outperforms a set of
state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a
real-world E-commerce platform. The online A/B testing results further
demonstrate the practicality of RMLDP.",arxiv
http://arxiv.org/abs/2007.14573v2,2021-06-02T03:00:12Z,2020-07-29T03:33:18Z,FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data,"High-order interactive features capture the correlation between different
columns and thus are promising to enhance various learning tasks on ubiquitous
tabular data. To automate the generation of interactive features, existing
works either explicitly traverse the feature space or implicitly express the
interactions via intermediate activations of some designed models. These two
kinds of methods show that there is essentially a trade-off between feature
interpretability and search efficiency. To possess both of their merits, we
propose a novel method named Feature Interaction Via Edge Search (FIVES), which
formulates the task of interactive feature generation as searching for edges on
the defined feature graph. Specifically, we first present our theoretical
evidence that motivates us to search for useful interactive features with
increasing order. Then we instantiate this search strategy by optimizing both a
dedicated graph neural network (GNN) and the adjacency tensor associated with
the defined feature graph. In this way, the proposed FIVES method simplifies
the time-consuming traversal as a typical training course of GNN and enables
explicit feature generation according to the learned adjacency tensor.
Experimental results on both benchmark and real-world datasets show the
advantages of FIVES over several state-of-the-art methods. Moreover, the
interactive features identified by FIVES are deployed on the recommender system
of Taobao, a worldwide leading e-commerce platform. Results of an online A/B
testing further verify the effectiveness of the proposed method FIVES, and we
further provide FIVES as AI utilities for the customers of Alibaba Cloud.",arxiv
http://arxiv.org/abs/2006.07042v1,2020-06-12T09:53:10Z,2020-06-12T09:53:10Z,Recurrent Neural Networks for Stochastic Control in Real-Time Bidding,"Bidding in real-time auctions can be a difficult stochastic control task;
especially if underdelivery incurs strong penalties and the market is very
uncertain. Most current works and implementations focus on optimally delivering
a campaign given a reasonable forecast of the market. Practical implementations
have a feedback loop to adjust and be robust to forecasting errors, but no
implementation, to the best of our knowledge, uses a model of market risk and
actively anticipates market shifts. Solving such stochastic control problems in
practice is actually very challenging. This paper proposes an approximate
solution based on a Recurrent Neural Network (RNN) architecture that is both
effective and practical for implementation in a production environment. The RNN
bidder provisions everything it needs to avoid missing its goal. It also
deliberately falls short of its goal when buying the missing impressions would
cost more than the penalty for not reaching it.",arxiv
http://arxiv.org/abs/2005.09347v2,2020-08-03T02:16:38Z,2020-05-19T10:18:43Z,Controllable Multi-Interest Framework for Recommendation,"Recently, neural networks have been widely used in e-commerce recommender
systems, owing to the rapid development of deep learning. We formalize the
recommender system as a sequential recommendation problem, intending to predict
the next items that the user might be interacted with. Recent works usually
give an overall embedding from a user's behavior sequence. However, a unified
user embedding cannot reflect the user's multiple interests during a period. In
this paper, we propose a novel controllable multi-interest framework for the
sequential recommendation, called ComiRec. Our multi-interest module captures
multiple interests from user behavior sequences, which can be exploited for
retrieving candidate items from the large-scale item pool. These items are then
fed into an aggregation module to obtain the overall recommendation. The
aggregation module leverages a controllable factor to balance the
recommendation accuracy and diversity. We conduct experiments for the
sequential recommendation on two real-world datasets, Amazon and Taobao.
Experimental results demonstrate that our framework achieves significant
improvements over state-of-the-art models. Our framework has also been
successfully deployed on the offline Alibaba distributed cloud platform.",arxiv
http://arxiv.org/abs/2004.10251v1,2020-04-21T19:40:16Z,2020-04-21T19:40:16Z,"Industrial Robot Grasping with Deep Learning using a Programmable Logic
  Controller (PLC)","Universal grasping of a diverse range of previously unseen objects from heaps
is a grand challenge in e-commerce order fulfillment, manufacturing, and home
service robotics. Recently, deep learning based grasping approaches have
demonstrated results that make them increasingly interesting for industrial
deployments. This paper explores the problem from an automation systems
point-of-view. We develop a robotics grasping system using Dex-Net, which is
fully integrated at the controller level. Two neural networks are deployed on a
novel industrial AI hardware acceleration module close to a PLC with a power
footprint of less than 10 W for the overall system. The software is tightly
integrated with the hardware allowing for fast and efficient data processing
and real-time communication. The success rate of grasping an object form a bin
is up to 95 percent with more than 350 picks per hour, if object and receptive
bins are in close proximity. The system was presented at the Hannover Fair 2019
(world s largest industrial trade fair) and other events, where it performed
over 5,000 grasps per event.",arxiv
http://arxiv.org/abs/2003.00097v2,2020-06-19T00:22:54Z,2020-02-28T22:32:05Z,Jointly Learning to Recommend and Advertise,"Online recommendation and advertising are two major income channels for
online recommendation platforms (e.g. e-commerce and news feed site). However,
most platforms optimize recommending and advertising strategies by different
teams separately via different techniques, which may lead to suboptimal overall
performances. To this end, in this paper, we propose a novel two-level
reinforcement learning framework to jointly optimize the recommending and
advertising strategies, where the first level generates a list of
recommendations to optimize user experience in the long run; then the second
level inserts ads into the recommendation list that can balance the immediate
advertising revenue from advertisers and the negative influence of ads on
long-term user experience. To be specific, the first level tackles high
combinatorial action space problem that selects a subset items from the large
item space; while the second level determines three internally related tasks,
i.e., (i) whether to insert an ad, and if yes, (ii) the optimal ad and (iii)
the optimal location to insert. The experimental results based on real-world
data demonstrate the effectiveness of the proposed framework. We have released
the implementation code to ease reproductivity.",arxiv
http://arxiv.org/abs/2002.01711v5,2021-12-29T19:18:42Z,2020-02-05T10:25:02Z,"Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement
  Learning Framework","A/B testing, or online experiment is a standard business strategy to compare
a new product with an old one in pharmaceutical, technological, and traditional
industries. Major challenges arise in online experiments of two-sided
marketplace platforms (e.g., Uber) where there is only one unit that receives a
sequence of treatments over time. In those experiments, the treatment at a
given time impacts current outcome as well as future outcomes. The aim of this
paper is to introduce a reinforcement learning framework for carrying A/B
testing in these experiments, while characterizing the long-term treatment
effects. Our proposed testing procedure allows for sequential monitoring and
online updating. It is generally applicable to a variety of treatment designs
in different industries. In addition, we systematically investigate the
theoretical properties (e.g., size and power) of our testing procedure.
Finally, we apply our framework to both simulated data and a real-world data
example obtained from a technological company to illustrate its advantage over
the current practice. A Python implementation of our test is available at
https://github.com/callmespring/CausalRL.",arxiv
http://arxiv.org/abs/1911.08729v1,2019-11-20T06:36:07Z,2019-11-20T06:36:07Z,"Response Transformation and Profit Decomposition for Revenue Uplift
  Modeling","Uplift models support decision-making in marketing campaign planning.
Estimating the causal effect of a marketing treatment, an uplift model
facilitates targeting communication to responsive customers and efficient
allocation of marketing budgets. Research into uplift models focuses on
conversion models to maximize incremental sales. The paper introduces uplift
modeling strategies for maximizing incremental revenues. If customers differ in
their spending behavior, revenue maximization is a more plausible business
objective compared to maximizing conversions. The proposed methodology entails
a transformation of the prediction target, customer-level revenues, that
facilitates implementing a causal uplift model using standard machine learning
algorithms. The distribution of campaign revenues is typically zero-inflated
because of many non-buyers. Remedies to this modeling challenge are
incorporated in the proposed revenue uplift strategies in the form of two-stage
models. Empirical experiments using real-world e-commerce data confirm the
merits of the proposed revenue uplift strategy over relevant alternatives
including uplift models for conver-sion and recently developed causal machine
learning algorithms. To quantify the degree to which improved targeting
decisions raise return on marketing, the paper develops a decomposition of
campaign profit. Applying the decomposition to a digital coupon targeting
campaign, the paper provides evidence that revenue uplift modeling, as well as
causal machine learning, can improve cam-paign profit substantially.",arxiv
http://arxiv.org/abs/1908.06698v1,2019-08-19T11:16:33Z,2019-08-19T11:16:33Z,"Learning to Advertise for Organic Traffic Maximization in E-Commerce
  Product Feeds","Most e-commerce product feeds provide blended results of advertised products
and recommended products to consumers. The underlying advertising and
recommendation platforms share similar if not exactly the same set of candidate
products. Consumers' behaviors on the advertised results constitute part of the
recommendation model's training data and therefore can influence the
recommended results. We refer to this process as Leverage. Considering this
mechanism, we propose a novel perspective that advertisers can strategically
bid through the advertising platform to optimize their recommended organic
traffic. By analyzing the real-world data, we first explain the principles of
Leverage mechanism, i.e., the dynamic models of Leverage. Then we introduce a
novel Leverage optimization problem and formulate it with a Markov Decision
Process. To deal with the sample complexity challenge in model-free
reinforcement learning, we propose a novel Hybrid Training Leverage Bidding
(HTLB) algorithm which combines the real-world samples and the
emulator-generated samples to boost the learning speed and stability. Our
offline experiments as well as the results from the online deployment
demonstrate the superior performance of our approach.",arxiv
http://arxiv.org/abs/1908.08998v2,2019-10-23T14:39:47Z,2019-08-13T10:15:39Z,AIBench: An Industry Standard Internet Service AI Benchmark Suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.",arxiv
http://arxiv.org/abs/1908.00754v1,2019-08-02T08:31:36Z,2019-08-02T08:31:36Z,"A Visual Technique to Analyze Flow of Information in a Machine Learning
  System","Machine learning (ML) algorithms and machine learning based software systems
implicitly or explicitly involve complex flow of information between various
entities such as training data, feature space, validation set and results.
Understanding the statistical distribution of such information and how they
flow from one entity to another influence the operation and correctness of such
systems, especially in large-scale applications that perform classification or
prediction in real time. In this paper, we propose a visual approach to
understand and analyze flow of information during model training and serving
phases. We build the visualizations using a technique called Sankey Diagram -
conventionally used to understand data flow among sets - to address various use
cases of in a machine learning system. We demonstrate how the proposed
technique, tweaked and twisted to suit a classification problem, can play a
critical role in better understanding of the training data, the features, and
the classifier performance. We also discuss how this technique enables
diagnostic analysis of model predictions and comparative analysis of
predictions from multiple classifiers. The proposed concept is illustrated with
the example of categorization of millions of products in the e-commerce domain
- a multi-class hierarchical classification problem.",arxiv
http://arxiv.org/abs/1907.02797v2,2020-03-14T14:25:49Z,2019-07-03T16:37:48Z,"Predicting e-commerce customer conversion from minimal temporal patterns
  on symbolized clickstream trajectories","Knowing if a user is a buyer or window shopper solely based on clickstream
data is of crucial importance for e-commerce platforms seeking to implement
real-time accurate NBA (next best action) policies. However, due to the low
frequency of conversion events and the noisiness of browsing data, classifying
user sessions is very challenging. In this paper, we address the clickstream
classification problem in the eCommerce industry and present three major
contributions to the burgeoning field of AI-for-retail: first, we collected,
normalized and prepared a novel dataset of live shopping sessions from a major
European e-commerce website; second, we use the dataset to test in a controlled
environment strong baselines and SOTA models from the literature; finally, we
propose a new discriminative neural model that outperforms neural architectures
recently proposed at Rakuten labs.",arxiv
http://arxiv.org/abs/1907.00400v1,2019-06-30T15:42:53Z,2019-06-30T15:42:53Z,"Prediction is very hard, especially about conversion. Predicting user
  purchases from clickstream data in fashion e-commerce","Knowing if a user is a buyer vs window shopper solely based on clickstream
data is of crucial importance for ecommerce platforms seeking to implement
real-time accurate NBA (next best action) policies. However, due to the low
frequency of conversion events and the noisiness of browsing data, classifying
user sessions is very challenging. In this paper, we address the clickstream
classification problem in the fashion industry and present three major
contributions to the burgeoning field of AI in fashion: first, we collected,
normalized and prepared a novel dataset of live shopping sessions from a major
European e-commerce fashion website; second, we use the dataset to test in a
controlled environment strong baselines and SOTA models from the literature;
finally, we propose a new discriminative neural model that outperforms neural
architectures recently proposed at Rakuten labs.",arxiv
http://arxiv.org/abs/1904.01735v1,2019-04-03T01:29:48Z,2019-04-03T01:29:48Z,"Multi-Modal Generative Adversarial Network for Short Product Title
  Generation in Mobile E-Commerce","Nowadays, more and more customers browse and purchase products in favor of
using mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are
usually inclined to describe redundant and over-informative product titles to
attract attentions from customers, it is important to concisely display short
product titles on limited screen of mobile phones. To address this discrepancy,
previous studies mainly consider textual information of long product titles and
lacks of human-like view during training and evaluation process. In this paper,
we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short
product title generation in E-Commerce, which innovatively incorporates image
information and attribute tags from product, as well as textual information
from original long titles. MM-GAN poses short title generation as a
reinforcement learning process, where the generated titles are evaluated by the
discriminator in a human-like view. Extensive experiments on a large-scale
E-Commerce dataset demonstrate that our algorithm outperforms other
state-of-the-art methods. Moreover, we deploy our model into a real-world
online E-Commerce environment and effectively boost the performance of click
through rate and click conversion rate by 1.66% and 1.87%, respectively.",arxiv
http://arxiv.org/abs/1903.12457v3,2019-06-05T07:35:08Z,2019-03-29T11:57:24Z,"Towards Knowledge-Based Personalized Product Description Generation in
  E-commerce","Quality product descriptions are critical for providing competitive customer
experience in an e-commerce platform. An accurate and attractive description
not only helps customers make an informed decision but also improves the
likelihood of purchase. However, crafting a successful product description is
tedious and highly time-consuming. Due to its importance, automating the
product description generation has attracted considerable interests from both
research and industrial communities. Existing methods mainly use templates or
statistical methods, and their performance could be rather limited. In this
paper, we explore a new way to generate the personalized product description by
combining the power of neural networks and knowledge base. Specifically, we
propose a KnOwledge Based pErsonalized (or KOBE) product description generation
model in the context of e-commerce. In KOBE, we extend the encoder-decoder
framework, the Transformer, to a sequence modeling formulation using
self-attention. In order to make the description both informative and
personalized, KOBE considers a variety of important factors during text
generation, including product aspects, user categories, and knowledge base,
etc. Experiments on real-world datasets demonstrate that the proposed method
out-performs the baseline on various metrics. KOBE can achieve an improvement
of 9.7% over state-of-the-arts in terms of BLEU. We also present several case
studies as the anecdotal evidence to further prove the effectiveness of the
proposed approach. The framework has been deployed in Taobao, the largest
online e-commerce platform in China.",arxiv
http://arxiv.org/abs/1902.08730v1,2019-02-23T03:45:31Z,2019-02-23T03:45:31Z,AliGraph: A Comprehensive Graph Neural Network Platform,"An increasing number of machine learning tasks require dealing with large
graph datasets, which capture rich and complex relationship among potentially
billions of elements. Graph Neural Network (GNN) becomes an effective way to
address the graph learning problem by converting the graph data into a low
dimensional space while keeping both the structural and property information to
the maximum extent and constructing a neural network for training and
referencing. However, it is challenging to provide an efficient graph storage
and computation capabilities to facilitate GNN training and enable development
of new GNN algorithms. In this paper, we present a comprehensive graph neural
network system, namely AliGraph, which consists of distributed graph storage,
optimized sampling operators and runtime to efficiently support not only
existing popular GNNs but also a series of in-house developed ones for
different scenarios. The system is currently deployed at Alibaba to support a
variety of business scenarios, including product recommendation and
personalized search at Alibaba's E-Commerce platform. By conducting extensive
experiments on a real-world dataset with 492.90 million vertices, 6.82 billion
edges and rich attributes, AliGraph performs an order of magnitude faster in
terms of graph building (5 minutes vs hours reported from the state-of-the-art
PowerGraph platform). At training, AliGraph runs 40%-50% faster with the novel
caching strategy and demonstrates around 12 times speed up with the improved
runtime. In addition, our in-house developed GNN models all showcase their
statistically significant superiorities in terms of both effectiveness and
efficiency (e.g., 4.12%-17.19% lift by F1 scores).",arxiv
http://arxiv.org/abs/1809.03291v1,2018-09-07T14:37:30Z,2018-09-07T14:37:30Z,Action-conditional Sequence Modeling for Recommendation,"In many online applications interactions between a user and a web-service are
organized in a sequential way, e.g., user browsing an e-commerce website. In
this setting, recommendation system acts throughout user navigation by showing
items. Previous works have addressed this recommendation setup through the task
of predicting the next item user will interact with. In particular, Recurrent
Neural Networks (RNNs) has been shown to achieve substantial improvements over
collaborative filtering baselines. In this paper, we consider interactions
triggered by the recommendations of deployed recommender system in addition to
browsing behavior. Indeed, it is reported that in online services interactions
with recommendations represent up to 30\% of total interactions. Moreover, in
practice, recommender system can greatly influence user behavior by promoting
specific items. In this paper, we extend the RNN modeling framework by taking
into account user interaction with recommended items. We propose and evaluate
RNN architectures that consist of the recommendation action module and the
state-action fusion module. Using real-world large-scale datasets we
demonstrate improved performance on the next item prediction task compared to
the baselines.",arxiv
http://arxiv.org/abs/1805.05491v1,2018-05-14T22:59:56Z,2018-05-14T22:59:56Z,"Crowdbreaks: Tracking Health Trends using Public Social Media Data and
  Crowdsourcing","In the past decade, tracking health trends using social media data has shown
great promise, due to a powerful combination of massive adoption of social
media around the world, and increasingly potent hardware and software that
enables us to work with these new big data streams. At the same time, many
challenging problems have been identified. First, there is often a mismatch
between how rapidly online data can change, and how rapidly algorithms are
updated, which means that there is limited reusability for algorithms trained
on past data as their performance decreases over time. Second, much of the work
is focusing on specific issues during a specific past period in time, even
though public health institutions would need flexible tools to assess multiple
evolving situations in real time. Third, most tools providing such capabilities
are proprietary systems with little algorithmic or data transparency, and thus
little buy-in from the global public health and research community. Here, we
introduce Crowdbreaks, an open platform which allows tracking of health trends
by making use of continuous crowdsourced labelling of public social media
content. The system is built in a way which automatizes the typical workflow
from data collection, filtering, labelling and training of machine learning
classifiers and therefore can greatly accelerate the research process in the
public health domain. This work introduces the technical aspects of the
platform and explores its future use cases.",arxiv
http://arxiv.org/abs/1803.00259v1,2018-03-01T09:04:37Z,2018-03-01T09:04:37Z,Deep Reinforcement Learning for Sponsored Search Real-time Bidding,"Bidding optimization is one of the most critical problems in online
advertising. Sponsored search (SS) auction, due to the randomness of user query
behavior and platform nature, usually adopts keyword-level bidding strategies.
In contrast, the display advertising (DA), as a relatively simpler scenario for
auction, has taken advantage of real-time bidding (RTB) to boost the
performance for advertisers. In this paper, we consider the RTB problem in
sponsored search auction, named SS-RTB. SS-RTB has a much more complex dynamic
environment, due to stochastic user query behavior and more complex bidding
policies based on multiple keywords of an ad. Most previous methods for DA
cannot be applied. We propose a reinforcement learning (RL) solution for
handling the complex dynamic environment. Although some RL methods have been
proposed for online advertising, they all fail to address the ""environment
changing"" problem: the state transition probabilities vary between two days.
Motivated by the observation that auction sequences of two days share similar
transition patterns at a proper aggregation level, we formulate a robust MDP
model at hour-aggregation level of the auction data and propose a
control-by-model framework for SS-RTB. Rather than generating bid prices
directly, we decide a bidding model for impressions of each hour and perform
real-time bidding accordingly. We also extend the method to handle the
multi-agent problem. We deployed the SS-RTB system in the e-commerce search
auction platform of Alibaba. Empirical experiments of offline evaluation and
online A/B test demonstrate the effectiveness of our method.",arxiv
http://arxiv.org/abs/1709.08920v1,2017-09-26T10:07:22Z,2017-09-26T10:07:22Z,"SCARFF: a Scalable Framework for Streaming Credit Card Fraud Detection
  with Spark","The expansion of the electronic commerce, together with an increasing
confidence of customers in electronic payments, makes of fraud detection a
critical factor. Detecting frauds in (nearly) real time setting demands the
design and the implementation of scalable learning techniques able to ingest
and analyse massive amounts of streaming data. Recent advances in analytics and
the availability of open source solutions for Big Data storage and processing
open new perspectives to the fraud detection field. In this paper we present a
SCAlable Real-time Fraud Finder (SCARFF) which integrates Big Data tools
(Kafka, Spark and Cassandra) with a machine learning approach which deals with
imbalance, nonstationarity and feedback latency. Experimental results on a
massive dataset of real credit card transactions show that this framework is
scalable, efficient and accurate over a big stream of transactions.",arxiv
http://arxiv.org/abs/1702.03488v2,2017-08-15T09:14:08Z,2017-02-12T04:53:25Z,Octopus: A Framework for Cost-Quality-Time Optimization in Crowdsourcing,"We present Octopus, an AI agent to jointly balance three conflicting task
objectives on a micro-crowdsourcing marketplace - the quality of work, total
cost incurred, and time to completion. Previous control agents have mostly
focused on cost-quality, or cost-time tradeoffs, but not on directly
controlling all three in concert. A naive formulation of three-objective
optimization is intractable; Octopus takes a hierarchical POMDP approach, with
three different components responsible for setting the pay per task, selecting
the next task, and controlling task-level quality. We demonstrate that Octopus
significantly outperforms existing state-of-the-art approaches on real
experiments. We also deploy Octopus on Amazon Mechanical Turk, showing its
ability to manage tasks in a real-world dynamic setting.",arxiv
