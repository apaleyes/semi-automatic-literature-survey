id,datePublished,description,publisher,title,downloadUrl,doi,journals,database
395002254,2021-01-11T17:09:03,"Both the military and civil worlds are being transformed by the development and deployment of unmanned systems to a wider range of scenarios. As the field of unmanned systems has grown and matured, it has continuously advanced towards increasing levels of autonomy. As an example, the cars of today have gone from ""maintain this speed"" to ""drive on this highway."" As this push towards greater levels of autonomy continues, new methods for developing policies for control of these systems are required. Recent breakthroughs in reinforcement learning hope to address this problem. The primary advantage of reinforcement learning based systems is their focus on goal driven behavior. In developing reinforcement learning based policies, there is a need for significant exploration of a system’s possible state-action space. As such, modeling and simulation has been an indispensable tool. However, transferring policies from a simulated world to the real world presents its own challenges. This work develops a method for evaluating the relative importance of different possible simplifications that can be taken during the modeling process. This relies on a sampling-based method to explore the possible simplification space of a given referent model. Experiments show that this method compares favorably to a number of baseline model development strategies and can lead to significantly simplified system models that maintain similar transference properties. Additional experiments are presented that evaluate different choices within this sampling-based strategy and the effects of imperfect referent models on the resulting evaluations.Ph.D",Georgia Institute of Technology,Evaluating the Effects of Model Simplifications on the Transference of Policies Learned in Simulation,https://core.ac.uk/download/395002254.pdf,,,core
478906302,2021-08-09T17:00:00,"In the context of fierce competition arising in the space economy, the number of satellites and constellations that will be placed in orbit is set to increase considerably in the upcoming years. In such a dynamic environment, raising the autonomy level of the next space missions is key to maintaining a competitive edge in terms of the scientific, technological, and commercial outcome.
We propose the adoption of an AI-based autonomous agent aiming to fully enable spacecraft’s goal-oriented autonomy. The implemented cognitive architecture collects input starting from the sensing of the surrounding operating environment and defines a low-level schedule of tasks that will be carried out throughout the specified horizon. Furthermore, the agent provides a planner module designed to find optimal solutions that maximize the outcome of the pursued objective goal. The autonomous loop is closed by comparing the expected outcome of these scheduled tasks against the real environment measurements.
The entire algorithmic pipeline was tested in a simulated operational environment, specifically developed for replicating inputs and resources relative to Earth Observation missions. The autonomous reasoning agent was evaluated against the classical, non-autonomous, mission control approach, considering both the quantity and the quality of collected observation data in addition to the quantity of the observation opportunities exploited throughout the simulation time. The preliminary simulation results point out that the adoption of our software agent enhances dramatically the effectiveness of the entire mission, increasing and optimizing in-orbit activities, on the one hand, reducing events\u27 response latency (opportunities, failures, malfunctioning, etc.) on the other.
In the presentation, we will cover the description of the high-level algorithmic structure of the proposed goal-oriented reasoning model, as well as a brief explanation of each internal module’s contribution to the overall agent’s architecture. Besides, an overview of the parameters processed as input and the expected algorithms\u27 output will be provided, to contextualize the placement of the proposed solution. Finally, an Earth Observation use case will be used as the benchmark to test the performances of the proposed approach against the classical one, highlighting promising conclusions regarding our autonomous agent’s adoption",DigitalCommons@USU,An AI-Based Goal-Oriented Agent for Advanced On-Board Automation,https://core.ac.uk/download/478906302.pdf,,,core
483548310,2021-01-01T00:00:00,"The ability to mentally evaluate variations of the future may well be the key to intelligence. Combined with the ability to reason, it makes humans excellent at handling new and complex situations. If we want robots to solve varying tasks autonomously, we need to endow them with such kind of ‘mental rehearsal’. Physics simulations allow predicting how the environment will change depending on a sequence of actions. For example, robots can simulate multiple control policies in different simulations instances, collect the results, and subsequently reason about which policy to execute in the real world. As such physics simulations are highly customizable, they enable generating vast amounts of diverse data at a relatively low cost. Therefore, they make it possible to apply deep learning methods for physical systems despite the exorbitant demand for data. Since state-of-the-art deep learning methods come with few guarantees, it is essential to test them in many simulated scenarios before deployment on the real system.
Over the last decade, the speed and modeling power of general-purpose physics engines increased significantly. State-of-the-art simulators feature rigid body, soft body, and fluid dynamics, as well as massive GPU-based parallelization. Despite the impressive progress, simulations will always remain an idealized model of the real world, thus are inevitably flawed. Typical error sources are unmodeled physical phenomena, or suboptimal parameter values of the underlying generative model. These discrepancies between the real and the simulated world are summarized by the term ‘reality gap’. This gap can manifest in various ways when learning from simulations. In the best case, it is only a performance drop, e.g., a lower success rate, or a reduced tracking accuracy. More likely, the learned policy is not transferable to the robot because of unknown friction effects, which lead to underestimating the friction in simulation. Thus, the commanded actions are in this case not strong enough to get the robot moving. Another reason for failure are small parameter estimation errors, which can quickly lead to unstable system dynamics. This case is particularly dangerous for humans and robots. For these reasons, bridging the reality gap is the essential step to endow robots with the ability to learn from simulated experience.
In this thesis, we will tackle the challenge of learning robot control policies from simulations such that the results can be (directly) transferred to the real world. We focus on scenarios where the source domain is a randomized simulator and the target domain is either a different simulation instance (sim-to-sim) or the physical robot (sim-to-real).
We strive to answer the following research questions:
1. How can we quantitatively estimate the transferability of a control policy from one domain to another?
2. Does randomizing the simulator during learning make the resulting policy more robust against modeling imperfections?
3. How do we adapt the randomized simulator based on real-world evaluations?
4. Can we infer the source domain parameter distribution from data and subsequently use it for learning?
5. What are the necessary assumptions and technical requirements to learn robot
control policies from randomized simulations?
Despite the recent popularity of sim-to-real methods, the first question has been unanswered up to this point in time. As a consequence, state-of-the-art algorithms can not make a quantitative statement about the transferability of the resulting control policies. Moreover, they stop training according to some heuristic like a fixed number of iterations, which can lead to a waste of computation time. In Chapter 3, we derive the simulation optimization bias as a measure of the reality gap, and show that policies learned from a source domain are optimistically biased in terms of their performance in the target domain, even if they originate from the same distribution. To mitigate this problem, we propose a policy search algorithm that estimates simulation optimization bias and continues training until an estimated upper confidence bound on this bias is below a given threshold. Thus, the resulting policy satisfies a probabilistic guarantee on the performance loss when transferring the policy to a different environment from the same source domain distribution. Moreover, our sim-to-real evaluations answer the second question with a clear “yes”.
Straightforwardly learning from randomized source domains shows the tendency to be slower and have lower performance at the nominal model than methods that close the sim-to-real loop by adapting the domain parameter distribution. Therefore, we tackle the third question in Chapter 4 by introducing a policy search algorithm which incorporates Bayesian optimization to adapt the domain parameter distribution based on real-world data. The sample-efficiency of Bayesian optimization allows updating the distribution’s parameters, including the uncertainty, while only requiring few evaluations on the physical device. Most notably, the data yielded from these evaluations can be very scarce, e.g., a scalar return value per trial. This way, the connection between distribution over simulator parameters and the target domain performance is captured by a probabilistic model. At the same time, we can eliminate the common assumption of knowing the distribution’s mean and variance a priori.
So far, existing domain randomization approaches assume that each domain parameter is independent and obeys a known probability distribution type, typically chosen to be a normal or uniform distribution. These and other assumptions impose unnecessary restrictions on the posterior distribution over simulators, and prevent us from utilizing the full power of domain randomization. In order to overcome this limitation, we propose to combine reinforcement learning with state-of-the-art likelihood-free inference methods, powered by flexible neural density estimators, to learn the posterior over domain parameters. The proposed method only requires a parametric generative model, e.g., a physics simulator, coarse prior ranges, and a small set of real-world trajectories. Together with a policy optimization algorithm, this approach iteratively updates the posterior over simulators and learns how to solve a given task. Most importantly, the generative model does not need to be differentiable, and the neural posterior can capture dependencies between domain parameters. By drastically reducing the quality and quantity of assumptions while still successfully learning transferable control policies, this procedure answers the fourth and the fifth question in Chapter 5. The methods presented in this thesis will greatly benefit from the continuous increase in computational power, allowing the randomization schemes to perform more exhaustive searches through the domain parameter space. In consequence, the required computation time as well as the variance will be reduced, alleviating the two biggest drawbacks of the domain randomization approaches. Meanwhile, financially strong actors like the video gaming industry are heavily pushing the development of physics simulators. Thus, current niche applications like simulations of muscles or interactions between fluid and solid particles are going to be consumer standard in the near future. The facilitated access to high-fidelity simulators will open the door to a whole new range of tasks which can be solved with methods presented in this thesis. One example could be to train control policies for active robotic prostheses in simulation such that to support human motion. In a subsequent step, these controllers could be customized based on user-specific data. The foreseeable establishment of (differentiable) probabilistic simulation engines will
provide access to the simulator’s likelihood function, hence boost the applicability of Bayesian inference. As a consequence, the popularity of research on highly data-efficient simulation-based inference methods will increase, leading to new algorithms that can perform complex inference in real time. These approaches have the potential to become the next mega trend in robotics research after the era deep learning",,Randomizing Physics Simulations for Robot Learning,,10.26083/tuprints-00019940,,core
291773415,10000-01-01,"200 p.Thesis (Ph.D.)--University of Illinois at Urbana-Champaign, 1997.In the past, AI systems have strived to automate problem-solving processes completely. However, in recent years researchers have come to realize that it is not always possible or desirable to aim for total automation. Researchers are realizing the importance of human-computer collaborative systems in which the human and the computer work as a team in solving problems. This approach raises the question of how to design systems that support effective collaborative problem-solving between humans and computers. The primary contribution of this thesis is an architecture for coordination of collaborative problem-solving (CO-SOLVE) for an important class of human-computer collaborative systems called associate systems. Associate systems are knowledge-based systems which share the cognitive workload with their human partners. Designers of associate systems must deal with the complexities of integrating mixed-initiative (i.e. human and computer) control with the general issues of problem-solving control faced by traditional AI systems. CO-SOLVE provides mechanisms for attention synchronization and collaborative alternatives exploration. The Attention Synchronization Model (ASM) allows the system to track (rather than direct) the user's activities in order to provide advice relevant to the current user activities. The Collaborative Alternatives Exploration Model (CAEM) is a mixed-initiative approach to exploring large, complex solution spaces. In this collaborative framework the user serves as solution evaluator and system controller and the computer as solution alternative generator. System developers using CAEM explicitly lay out steps in the problem-solving process for a given task and define points of human-computer interaction within the sequence of process steps. The other contribution of this thesis is a proof-of-concept prototype of CO-SOLVE, called SEDAR, which is implemented for a real-world, complex domain (flat and low-slope roof design). Two evaluations were conducted on SEDAR. The first assessed the effectiveness and usability of the ASM and its critiquing strategies as implemented in SEDAR. The evaluation showed that SEDAR reduced the error rate of experienced architects and also which advising strategies they preferred. The second evaluation assessed the effectiveness of the CAEM as implemented in SEDAR and showed that SEDAR (1) helped experienced architects reduce the amount of time spent developing solutions, and (2) increased the number of alternatives searched in the solution space.U of I OnlyRestricted to the U of I community idenfinitely during batch ingest of legacy ETD",,An Architecture for Collaborative Problem-Solving Control in Associate Systems,,,,core
395037775,2021-04-08T00:00:00,"With electric power systems becoming more compact and increasingly powerful,
the relevance of thermal stress especially during overload operation is
expected to increase ceaselessly. Whenever critical temperatures cannot be
measured economically on a sensor base, a thermal model lends itself to
estimate those unknown quantities. Thermal models for electric power systems
are usually required to be both, real-time capable and of high estimation
accuracy. Moreover, ease of implementation and time to production play an
increasingly important role. In this work, the thermal neural network (TNN) is
introduced, which unifies both, consolidated knowledge in the form of
heat-transfer-based lumped-parameter models, and data-driven nonlinear function
approximation with supervised machine learning. A quasi-linear
parameter-varying system is identified solely from empirical data, where
relationships between scheduling variables and system matrices are inferred
statistically and automatically. At the same time, a TNN has physically
interpretable states through its state-space representation, is end-to-end
trainable -- similar to deep learning models -- with automatic differentiation,
and requires no material, geometry, nor expert knowledge for its design.
Experiments on an electric motor data set show that a TNN achieves higher
temperature estimation accuracies than previous white-/grey- or black-box
models with a mean squared error of $3.18~\text{K}^2$ and a worst-case error of
$5.84~\text{K}$ at 64 model parameters.Comment: Preprint; Fix typos, streamline math. notation; 10 page",,"Thermal Neural Networks: Lumped-Parameter Thermal Modeling With
  State-Space Machine Learning",http://arxiv.org/abs/2103.16323,,,core
395174551,2021-02-25T13:22:07,"We address the detection, tracking, and relative localization of the agents of a drone swarm from a human perspective using a headset equipped with a single camera and an Inertial Measurement Unit (IMU). We train and deploy a deep neural network detector on image data to detect the drones. A joint probabilistic data association filter resolves the detection problems and couples this information with the headset IMU data to track the agents. In order to estimate the drones’ relative poses in 3D space with respect to the human, we use an additional deep neural network that processes image regions of the drones provided by the tracker. Finally, to speed up the deep neural networks’ training, we introduce an automated labeling process relying on a motion capture system. Several experimental results validate the effectiveness of the proposed approach. The approach is real-time, does not rely on any communication between the human and the drones, and can scale to a large number of agents, often called swarms. It can be used to spatially task a swarm of drones and also employed without a headset for formation control and coordination of terrestrial vehicles",'Institute of Electrical and Electronics Engineers (IEEE)',Tracking and Relative Localization of Drone Swarms With a Vision-Based Headset,,10.1109/LRA.2021.3051565,,core
337299269,2021-01-06T00:00:00,"Domains where supervised models are deployed often come with task-specific
constraints, such as prior expert knowledge on the ground-truth function, or
desiderata like safety and fairness. We introduce a novel probabilistic
framework for reasoning with such constraints and formulate a prior that
enables us to effectively incorporate them into Bayesian neural networks
(BNNs), including a variant that can be amortized over tasks. The resulting
Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework
for uncertainty quantification and is amenable to black-box inference. Unlike
typical BNN inference in uninterpretable parameter space, OC-BNNs widen the
range of functional knowledge that can be incorporated, especially for model
users without expertise in machine learning. We demonstrate the efficacy of
OC-BNNs on real-world datasets, spanning multiple domains such as healthcare,
criminal justice, and credit scoring.Comment: 11 pages, with six supplementary pages. 34th Conference on Neural
  Information Processing Systems (NeurIPS 2020), Vancouver, Canada. Code
  available at: https://github.com/dtak/ocbnn-public. Updated version (final,
  official submission to NeurIPS in January 2021) includes post-conference
  revisions: improved results in Section 6.2, and corrected minor errata in
  Appendix ",,"Incorporating Interpretable Output Constraints in Bayesian Neural
  Networks",http://arxiv.org/abs/2010.10969,,,core
337301296,2021-04-20T00:00:00,"We explore the potential of feed-forward deep neural networks (DNNs) for
emulating cloud superparameterization in realistic geography, using offline
fits to data from the Super Parameterized Community Atmospheric Model. To
identify the network architecture of greatest skill, we formally optimize
hyperparameters using ~250 trials. Our DNN explains over 70 percent of the
temporal variance at the 15-minute sampling scale throughout the mid-to-upper
troposphere. Autocorrelation timescale analysis compared against DNN skill
suggests the less good fit in the tropical, marine boundary layer is driven by
neural network difficulty emulating fast, stochastic signals in convection.
However, spectral analysis in the temporal domain indicates skillful emulation
of signals on diurnal to synoptic scales. A close look at the diurnal cycle
reveals correct emulation of land-sea contrasts and vertical structure in the
heating and moistening fields, but some distortion of precipitation.
Sensitivity tests targeting precipitation skill reveal complementary effects of
adding positive constraints vs. hyperparameter tuning, motivating the use of
both in the future. A first attempt to force an offline land model with DNN
emulated atmospheric fields produces reassuring results further supporting
neural network emulation viability in real-geography settings. Overall, the fit
skill is competitive with recent attempts by sophisticated Residual and
Convolutional Neural Network architectures trained on added information,
including memory of past states. Our results confirm the parameterizability of
superparameterized convection with continents through machine learning and we
highlight advantages of casting this problem locally in space and time for
accurate emulation and hopefully quick implementation of hybrid climate models.Comment: 32 Pages, 13 Figures, Revised Version Submitted to Journal of
  Advances in Modeling Earth Systems April 202",'American Geophysical Union (AGU)',"Assessing the Potential of Deep Learning for Emulating Cloud
  Superparameterization in Climate Models with Real-Geography Boundary
  Conditions",http://arxiv.org/abs/2010.12996,10.1029/2020MS002385,,core
388192614,2021-01-01T00:00:00,"Hard time constraints in space missions bring in the problem of fast video processing for numerous autonomous tasks. Video processing involves the separation of distinct image frames, fetching image descriptors, applying different machine learning algorithms for object detection, obstacle avoidance, and many more tasks involved in the automatic maneuvering of a spacecraft. These tasks require the most informative descriptions of an image within the time constraints. Tracking these informative points from consecutive image frames is needed in flow estimation applications. Classical algorithms like SIFT and SURF are the milestones in the feature description development. But computational complexity and high time requirements force the critical missions to avoid these techniques to get adopted in real-time processing. Hence a time conservative and less complex pre-trained Convolutional Neural Network (CNN) model is chosen in this paper as a feature descriptor. 7-layer CNN model is designed and implemented with pre-trained VGG model parameters and then these CNN features are used to match the points of interests from consecutive image frames of a lunar descent video. The performance of the system is evaluated based on visual and empirical keypoints matching. The scores of matches between two consecutive images from the video using CNN features are then compared with state-of-the-art algorithms like SIFT and SURF. The results show that CNN features are more reliable and robust in case of time-critical video processing tasks for keypoint tracking applications of space missions",'University North',Tracking Keypoints from Consecutive Video Frames Using CNN Features for Space Applications,https://core.ac.uk/download/388192614.pdf,10.31803/tg-20210204161210,,core
459156956,2021-11-01T00:00:00,"Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model",'Elsevier BV',A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand,,10.1016/j.renene.2021.05.155,,core
475521424,,"Article no. e12746Improvement of deep learning algorithms in smart agriculture is important to support the early detection of plant diseases, thereby improving crop yields. Data acquisition for machine learning applications is an expensive task due to the requirements of expert knowledge and professional equipment. The usability of any application in a real-world setting is often limited by unskilled users and the limitations of devices used for acquiring images for classification. We aim to improve the accuracy of deep learning models on low-quality test images using data augmentation techniques for neural network training. We generate synthetic images with a modified colour value distribution to expand the trainable image colour space and to train the neural network to recognize important colour-based features, which are less sensitive to the deficiencies of low-quality images such as those affected by blurring or motion. This paper introduces a novel image colour histogram transformation technique for generating synthetic images for data augmentation in image classification tasks. The approach is based on the convolution of the Chebyshev orthogonal functions with the probability distribution functions of image colour histograms. To validate our proposed model, we used four methods (resolution down-sampling, Gaussian blurring, motion blur, and overexposure) for reducing image quality from the Cassava leaf disease dataset. The results based on the modified MobileNetV2 neural network showed a statistically significant improvement of cassava leaf disease recognition accuracy on lower-quality testing images when compared with the baseline network. The model can be easily deployed for recognizing and detecting cassava leaf diseases in lower quality images, which is a major factor in practical data acquisitionKauno technologijos universitetasTaikomosios informatikos katedraVytauto Didžiojo universiteta",'Wiley',Cassava disease recognition from low-quality images using enhanced data augmentation model and deep learning,,10.1111/exsy.12746,,core
489517689,2021-12-29T00:00:00,"The purpose of this article is to identify the risks, threats, and challenges associated with possible social changes in the processes of digitalization of society and transformations of traditional communication practices, which is associated with the emergence of new digital subjects of mass public communication that form the pseudo structure of digital interaction of people. The primary tasks of the work were to identify the potential of artificial intelligence technologies and neural networks in the field of social and political communications, as well as to analyze the features of “smart” communications in terms of their subjectness. As a methodological optics, the work used the method of discourse analysis of scientific research devoted to the implementation and application of artificial intelligence technologies and self-learning neural networks in the processes of social and political digitalization, as well as the method of critical analysis of current communication practices in the socio-political sphere. At the same time, when analyzing the current digitalization practices, the case study method was used. The authors substantiate the thesis that introducing technological solutions based on artificial intelligence algorithms and self-learning neural networks into contemporary processes of socio-political communication creates the potential for a wide range of challenges, threats, and risks, the key of which is the problem of identifying the actual subjects of digital communication acts. The article also discusses the problem of increasing the manipulative potential of “smart” communications, for which the authors used the concepts of cyber simulacrum and information capsule developed by them. The paper shows that artificial intelligence and self-learning neural network algorithms, being increasingly widely introduced into the current practice of contemporary digital communications, form a high potential for information and communication impact on the mass consciousness from technological solutions that no longer require control by operators – humans. As a result, conditions arise to form a hybrid socio-technical reality – a communication reality of a new type with mixed subjectness. The paper also concludes that in the current practices of social interactions in the digital space, a person faces a new phenomenon – interfaceization, within which self-communication stimulates the universalization and standardization of digital behavior, creating, disseminating, strengthening, and imposing special digital rituals. In the article, the authors suggest that digital rituals blur the line between the activity of digital avatars based on artificial intelligence and the activity of actual people, resulting in the potential for a person to lose his own subjectness in the digital communications space.Celem niniejszego artykułu jest identyfikacja ryzyk, zagrożeń i wyzwań związanych z możliwymi zmianami społecznymi w procesach cyfryzacji społeczeństwa oraz przekształceniami tradycyjnych praktyk komunikacyjnych, co wiąże się z pojawieniem się nowych cyfrowych podmiotów masowej komunikacji publicznej tworzących pseudostrukturę cyfrowej interakcji pomiędzy ludźmi. Podstawowymi zadaniami pracy była identyfikacja potencjału technologii sztucznej inteligencji i sieci neuronowych w obszarze komunikacji społecznej i politycznej, a także analiza cech komunikacji „inteligentnej” pod kątem jej podmiotowości. Jako optykę metodologiczną w pracy wykorzystano metodę analizy dyskursu badań naukowych poświęconych wdrożeniu i zastosowaniu technologii sztucznej inteligencji oraz samouczących się sieci neuronowych w procesach cyfryzacji społecznej i politycznej, a także metodę krytycznej analizy aktualnych praktyk komunikacyjnych w sferze społeczno-politycznej. Jednocześnie przy analizie aktualnych praktyk digitalizacyjnych zastosowano metodę studium przypadku. Autorzy uzasadniają tezę, że wprowadzenie do współczesnych procesów komunikacji społeczno-politycznej rozwiązań technologicznych opartych na algorytmach sztucznej inteligencji i samouczących się sieciach neuronowych stwarza potencjał dla szerokiego wachlarza wyzwań, zagrożeń i ryzyka, których kluczem jest problem identyfikacji rzeczywistych podmiotów aktów komunikacji cyfrowej. W artykule omówiono również problem zwiększenia potencjału manipulacyjnego „inteligentnej” komunikacji, do czego autorzy wykorzystali opracowane przez siebie koncepcje cyber simulacrum i kapsuły informacyjnej. Artykuł pokazuje, że sztuczna inteligencja i samouczące się algorytmy sieci neuronowych, coraz szerzej wprowadzane do obecnej praktyki współczesnej komunikacji cyfrowej, stwarzają duży potencjał oddziaływania informacyjno-komunikacyjnego na świadomość masową z rozwiązań technologicznych, które nie wymagają już kontroli przez operatorów – ludzi. W efekcie powstają warunki do uformowania hybrydowej rzeczywistości społeczno-technicznej – rzeczywistości komunikacyjnej nowego typu o mieszanej podmiotowości. W artykule stwierdzono również, że w obecnych praktykach interakcji społecznych w przestrzeni cyfrowej człowiek staje przed nowym zjawiskiem – interfaceization, w ramach którego autokomunikacja stymuluje uniwersalizację i standaryzację zachowań cyfrowych, tworzenie, rozpowszechnianie, wzmacnianie i narzucanie szczególnego cyfrowego rytuału. W artykule autorzy sugerują, że cyfrowe rytuały zacierają granicę między aktywnością cyfrowych awatarów opartych na sztucznej inteligencji a aktywnością rzeczywistych ludzi, co skutkuje możliwością utraty przez człowieka własnej podmiotowości w cyfrowej przestrzeni komunikacyjnej",'Adam Mickiewicz University Poznan',"Tematyka komunikacji cyfrowej w kontekście ewolucji technologicznej współczesnego społeczeństwa: zagrożenia, wyzwania, ryzyka",,10.14746/ps.2021.1.25,,core
467109424,2021-05-22T07:00:00,"The global energy market has seen a massive increase in investment and capital flow in the last few decades. This has completely transformed the way power grids operate - legacy systems are now being replaced by advanced smart grid infrastructures that attest to better connectivity and increased reliability. One popular example is the extensive deployment of phasor measurement units, which is referred to PMUs, that constantly provide time-synchronized phasor measurements at a high resolution compared to conventional meters. This enables system operators to monitor in real-time the vast electrical network spanning thousands of miles.  However, a targeted cyber attack on PMUs can prompt operators to take wrong actions that can eventually jeopardize the power system reliability. Such threats originating from the cyber-space continue to increase as power grids become more dependent on PMU communication networks. Additionally, these threats are becoming increasingly efficient in remaining undetected for longer periods while gaining deep access into the power networks. An attack on the energy sector immediately impacts national defense, emergency services, and all aspects of human life. Cyber attacks against the electric grid may soon become a tactic of high-intensity warfare between nations in near future and lead to social disorder. Within this context, this dissertation investigates the cyber security of PMUs that affects critical decision-making for a reliable operation of the power grid. In particular, this dissertation focuses on false data attacks, a key vulnerability in the PMU architecture, that inject, alter, block, or delete data in devices or in communication network channels.
This dissertation addresses three important cyber security aspects - (1) impact assessment, (2)  detection, and (3) mitigation of false data attacks. A comprehensive background of false data attack models targeting various steady-state control blocks is first presented. By investigating inter-dependencies between the cyber and the physical layers, this dissertation then identifies possible points of ingress and categorizes risk at different levels of threats. In particular, the likelihood of cyber attacks against the steady-state power system control block causing the worst-case impacts such as cascading failures is investigated. The case study results indicate that false data attacks do not often lead to widespread blackouts, but do result in subsequent line overloads and load shedding. The impacts are magnified when attacks are coordinated with physical failures of generators, transformers, or heavily loaded lines. Further, this dissertation develops a data-driven false data attack detection method that is independent of existing in-built security mechanisms in the state estimator. It is observed that a convolutional neural network classifier can quickly detect and isolate false measurements compared to other deep learning and traditional classifiers. Finally, this dissertation develops a recovery plan that minimizes the consequence of threats when sophisticated attacks remain undetected and have already caused multiple failures. Two new controlled islanding methods are developed that minimize the impact of attacks under the lack of, or partial information on the threats. The results indicate that the system operators can successfully contain the negative impacts of cyber attacks while creating stable and observable islands. Overall, this dissertation presents a comprehensive plan for fast and effective detection and mitigation of false data attacks, improving cyber security preparedness, and enabling continuity of operations",SURFACE at Syracuse University,"Impact Assessment, Detection, And Mitigation Of False Data Attacks In Electrical Power Systems",https://core.ac.uk/download/467109424.pdf,,,core
326412324,2021-01-20T00:00:00,"Huge amount of applications in various fields, such as gene expression
analysis or computer vision, undergo data sets with high-dimensional
low-sample-size (HDLSS), which has putted forward great challenges for standard
statistical and modern machine learning methods. In this paper, we propose a
novel classification criterion on HDLSS, tolerance similarity, which emphasizes
the maximization of within-class variance on the premise of class separability.
According to this criterion, a novel linear binary classifier is designed,
denoted by No-separated Data Maximum Dispersion classifier (NPDMD). The
objective of NPDMD is to find a projecting direction w in which all of training
samples scatter in as large an interval as possible. NPDMD has several
characteristics compared to the state-of-the-art classification methods. First,
it works well on HDLSS. Second, it combines the sample statistical information
and local structural information (supporting vectors) into the objective
function to find the solution of projecting direction in the whole feature
spaces. Third, it solves the inverse of high dimensional matrix in low
dimensional space. Fourth, it is relatively simple to be implemented based on
Quadratic Programming. Fifth, it is robust to the model specification for
various real applications. The theoretical properties of NPDMD are deduced. We
conduct a series of evaluations on one simulated and six real-world benchmark
data sets, including face classification and mRNA classification. NPDMD
outperforms those widely used approaches in most cases, or at least obtains
comparable results.Comment: arXiv admin note: text overlap with arXiv:1901.0137",,The classification for High-dimension low-sample size data,http://arxiv.org/abs/2006.13018,,,core
323049899,2021-06-05T00:00:00,"Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.Comment: To appear in SIAM Journal on Scientific Computing; 32 pages, 9
  figure",,The Random Feature Model for Input-Output Maps between Banach Spaces,http://arxiv.org/abs/2005.10224,,,core
443932353,2021-06-25T00:00:00,"Collaborative inference enables resource-constrained edge devices to make
inferences by uploading inputs (e.g., images) to a server (i.e., cloud) where
the heavy deep learning models run. While this setup works cost-effectively for
successful inferences, it severely underperforms when the model faces input
samples on which the model was not trained (known as Out-of-Distribution (OOD)
samples). If the edge devices could, at least, detect that an input sample is
an OOD, that could potentially save communication and computation resources by
not uploading those inputs to the server for inference workload. In this paper,
we propose a novel lightweight OOD detection approach that mines important
features from the shallow layers of a pretrained CNN model and detects an input
sample as ID (In-Distribution) or OOD based on a distance function defined on
the reduced feature space. Our technique (a) works on pretrained models without
any retraining of those models, and (b) does not expose itself to any OOD
dataset (all detection parameters are obtained from the ID training dataset).
To this end, we develop EARLIN (EARLy OOD detection for Collaborative
INference) that takes a pretrained model and partitions the model at the OOD
detection layer and deploys the considerably small OOD part on an edge device
and the rest on the cloud. By experimenting using real datasets and a prototype
implementation, we show that our technique achieves better results than other
approaches in terms of overall accuracy and cost when tested against popular
OOD datasets on top of popular deep learning models pretrained on benchmark
datasets.Comment: To Appear in the proceedings of ECML-PKDD'202",,"EARLIN: Early Out-of-Distribution Detection for Resource-efficient
  Collaborative Inference",http://arxiv.org/abs/2106.13842,,,core
478912746,2021-10-18T00:00:00,"Online Network Resource Allocation (ONRA) for service provisioning is a
fundamental problem in communication networks. As a sequential decision-making
under uncertainty problem, it is promising to approach ONRA via Reinforcement
Learning (RL). But, RL solutions suffer from the sample complexity issue; i.e.,
a large number of interactions with the environment needed to find an efficient
policy. This is a barrier to utilize RL for ONRA as on one hand, it is not
practical to train the RL agent offline due to lack of information about future
requests, and on the other hand, online training in the real network leads to
significant performance loss because of the sub-optimal policy during the
prolonged learning time. This performance degradation is even higher in
non-stationary ONRA where the agent should continually adapt the policy with
the changes in service requests. To deal with this issue, we develop a general
resource allocation framework, named RADAR, using model-based RL for a class of
ONRA problems with the known immediate reward of each action. RADAR improves
sample efficiency via exploring the state space in the background and
exploiting the policy in the decision-time using synthetic samples by the model
of the environment, which is trained by real interactions. Applying RADAR on
the multi-domain service federation problem, to maximize profit via selecting
proper domains for service requests deployment, shows its continual learning
capability and up to 44% performance improvement w.r.t. the standard model-free
RL solution.Comment: This is the version of the paper submitted to ICC 202",,"Model-Based Reinforcement Learning Framework of Online Network Resource
  Allocation",http://arxiv.org/abs/2110.09236,,,core
199173687,2088-10-05T00:00:00,"The explosion of big data poses a serious problem to the efficient retrieval and management of information. Conventional indexes such as B-tree and its variants scale both in space and in time with the number of keys, and this limitation will become more and more severe in the long run. To further complicate the situation, a new generation of applications and paradigms, such as IoT, fog and edge computing, demand strict latency, energy and storage constraints that also vary among devices and users. Traditional algorithms are unable to offer these flexibility and adaptability features in a principled way.
Recent research on external memory, cache-oblivious, and compressed data structures, has tried to tackle these problems but, unfortunately, apart from some few and specific results, we are still far from achieving the above goals, let alone offer tools to ease the work of software engineers in choosing the best solution for a constrained application.
To address these challenges, we propose a novel data structure that exploits a simple yet effective observation: not all datasets should be indexed in the same way, as they differ both in distribution and regularities. Indeed, one would neither use a tree nor a hash table if the dataset has increasing consecutive integer keys, as it is sufficient to use a linear function mapping from keys to positions. Starting from this trivial observation, our strategy builds a piecewise linear representation of the 2D data distribution (key, position), which is then used at query time to find the approximate position of a key in constant time. We were able to show that the piecewise representation is effective on various input datasets and, moreover, that depending on the context of use, one can design via an optimisation process a data structure that given a maximum query time minimises the space occupancy, or that given a maximum space minimises the query time.
We experiment our data structure, which we call Top-Down Regression index (TDR-index), on four real-world datasets: timestamps of IoT sensors events, taxi pickup times, longitude of points-of-interest in a map, and timestamps of requests to a web server. Compared to a popular in-memory B+ tree implementation, our data structure is able to achieve faster query time while reducing the memory occupancy by four orders of magnitude. Compared to the cache-sensitive search tree, our data structure is able to achieve its efficient query performance but with a gain of 74× in space reduction.
Our last contribution is to explore the possibility of improving the piecewise representation through the use of nonlinear regression models, such as neural networks",'Pisa University Press',On Achieving Principled Space-Time Trade-Offs by Novel Indexing Data Structures,,,,core
491158850,2022-02-11T00:00:00,"Designing and maintaining distributed systems remains highly challenging: there is a high-dimensional design space of potential ways to distribute a system’s sub-components over a large-scale infrastructure; and the deployment environment for a system tends to change in unforeseen ways over time. For engineers, this is a complex prediction problem to gauge which distributed design may best suit a given environment. We present the concept of self-distributing systems, in which any local system built using our framework can learn, at runtime, the most appropriate distributed design given its perceived operating conditions. Our concept abstracts distribution of a system’s sub-components to a list of simple actions in a reward matrix of distributed design alternatives to be used by reinforcement learning algorithms. By doing this, we enable software to experiment, in a live production environment, with different ways in which to distribute its software modules by placing them in different hosts throughout the system’s infrastructure. We implement this concept in a framework we call Hatch, which has three major elements: (i) a transparent and generalized RPC layer that supports seamless relocation of any local component to a remote host during execution; (ii) a set of primitives, including relocation, replication and sharding, from which to create an action/reward matrix of possible distributed designs of a system; and (iii) a decentralized reinforcement learning approach to converge towards more optimal designs in real time. Using an example of a self-distributing webserving infrastructure, Hatch is able to autonomously select the most suitable distributed design from among ù700,000 alternatives in about 5 minutes",,Hatch: Self-Distributing Systems for Data Centers,,,,core
491008628,2021-11-01T00:00:00,"In this article, we study activity recognition in the context of sensor-rich environments. In these environments, many different constraints arise at various levels during the data generation process, such as the intrinsic characteristics of the sensing devices, their energy and computational constraints, and their collective (collaborative) dimension. These constraints have a fundamental impact on the final activity recognition models as the quality of the data, its availability, and its reliability, among other things, are not ensured during model deployment in real-world configurations. Current approaches for activity recognition rely on the activity recognition chain which defines several steps that the sensed data undergo: This is an inductive process that involves exploring a hypothesis space to find a theory able to explain the observations. For activity recognition to be effective and robust, this inductive process must consider the constraints at all levels and model them explicitly. Whether it is a bias related to sensor measurement, transmission protocol, sensor deployment topology, heterogeneity, dynamicity, or stochastic effects, it is essential to understand their substantial impact on the quality of the data and ultimately on activity recognition models. This study highlights the need to exhibit the different types of biases arising in real situations so that machine learning models, e.g., can adapt to the dynamicity of these environments, resist sensor failures, and follow the evolution of the sensors’ topology. We propose a metamodeling approach in which these biases are specified as hyperparameters that can control the structure of the activity recognition models. Via these hyperparameters, it becomes easier to optimize the inductive processes, reason about them, and incorporate additional knowledge. It also provides a principled strategy to adapt the models to the evolutions of the environment. We illustrate our approach on the SHL dataset, which features motion sensor data for a set of human activities collected in real conditions. The obtained results make a case for the proposed metamodeling approach; noticeably, the robustness gains achieved when the deployed models are confronted with the evolution of the initial sensing configurations. The trade-offs exhibited and the broader implications of the proposed approach are discussed with alternative techniques to encode and incorporate knowledge into activity recognition models",'MDPI AG',Human Activity Recognition: A Dynamic Inductive Bias Selection Perspective,,10.3390/s21217278,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
479136994,2021-01-01T08:00:00,"Mobile robots are already in use for mapping, agriculture, entertainment, and the delivery of goods and people. As robotic systems continue to become more affordable, large numbers of mobile robots may be deployed concurrently to accomplish tasks faster and more efficiently. Practical deployments of very large teams will require scalable algorithms to enable the distributed cooperation of autonomous agents. This thesis focuses on the three main algorithmic obstacles to the scalability of robot teams: coordination, control, and communication. To address these challenges, we design graph-based abstractions that allow us to apply Graph Neural Networks (GNNs).First, a team of robots must continually coordinate to divide up mission requirements among all agents. We focus on the case studies of exploration and coverage to develop a spatial GNN controller that can coordinate a team of dozens of agents as they visit thousands of landmarks. A routing problem of this size is intractable for existing optimization-based approaches. Second, a robot in a team must be able to execute the trajectory that will accomplish its given sub-task. In large teams with high densities of robots, planning and execution of safe, collision-free trajectories requires the joint optimization over all agent trajectories, which may be impractical in large teams. We present two approaches to scalable control: a) a controller for flocking that uses delayed communication formalized via a GNN; and b) an inverse optimal planning method that learns from real air traffic data. Third, robot teams may need to operate in harsh environments without existing communication infrastructure, requiring the formation of ad-hoc networks to exchange information. Many algorithms for control of multi-robot teams operate under the assumption that low-latency, global state information necessary to coordinate agent actions can readily be disseminated among the team. Our approach leverages GNNs to control the connectivity within the ad-hoc network and to provide the data distribution infrastructure necessary for countless multi-robot algorithms. Finally, this thesis develops a framework for distributed learning to be used when centralized information is unavailable during training. Our approach allows robots to train controllers independently and then share their experiences by composing multiple models represented in a Reproducing Kernel Hilbert Space",ScholarlyCommons,Scalable Learning In Distributed Robot Teams,https://core.ac.uk/download/479136994.pdf,,,core
479633153,2021-08-01T00:00:00,"This paper presents a novel method for attitude estimation of an object in 3D space by incremental learning of the Long-Short Term Memory (LSTM) network. Gyroscope, accelerometer, and magnetometer are few widely used sensors in attitude estimation applications. Traditionally, multi-sensor fusion methods such as the Extended Kalman Filter and Complementary Filter are employed to fuse the measurements from these sensors. However, these methods exhibit limitations in accounting for the uncertainty, unpredictability, and dynamic nature of the motion in real-world situations. In this paper, the inertial sensors data are fed to the LSTM network which are then updated incrementally to incorporate the dynamic changes in motion occurring in the run time. The robustness and efficiency of the proposed framework is demonstrated on the dataset collected from a commercially available inertial measurement unit. The proposed framework offers a significant improvement in the results compared to the traditional method, even in the case of a highly dynamic environment. The LSTM framework-based attitude estimation approach can be deployed on a standard AI-supported processing module for real-time applications",'PeerJ',Incremental learning of LSTM framework for sensor fusion in attitude estimation,,10.7717/peerj-cs.662,"[{'title': 'PeerJ Computer Science', 'identifiers': ['2376-5992', 'issn:2376-5992']}]",core
479362413,2021-01-01T00:00:00,"Short-term load forecasting is a key task for planning and stability of the current and future distribution grid, as it can significantly contribute to the management of energy market for ancillary services. In this paper we introduce the beneficial properties of applications of sparse representation and corresponding dictionary learning to the net load forecasting problem on a substation level. In this context, sparse representation theory can provide parsimonial predictive models, which become attractive mainly due to their ability to successfully model the input space in a self-learning manner, by interacting between theory, algorithms, and applications. Several techniques are implemented, incorporating numerous dictionary learning and sparse decomposition algorithms, and a hierarchical structured model is proposed. The concept of sparsity in each case is embedded throughout the utilization of different regularization forms which include the  $\ell _{0} $ ,  $\ell _{1} $ ,  $\ell _{2} $  and  $\ell _{0}^{tree} $  norms. The observed superiority of the proposed theory, especially the one which embeds the atoms and corresponding coefficients in a tree structure, stems from the construction of the dictionary so as to represent efficiently the ambient electricity signal space and the consequent extraction of sparse basis-vectors. The performance of each model is evaluated using real hourly load measurements from a high voltage/medium voltage (HV/MV) substation and compared with that of widely used machine learning methods. The provided analytical results, verify the effectiveness of hierarchical sparse representation in short-term load forecasting applications, in terms of common accuracy indices",'Institute of Electrical and Electronics Engineers (IEEE)',Short-Term Electric Load Forecasting With Sparse Coding Methods,,10.1109/ACCESS.2021.3098121,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
395676524,2021-01-28T00:00:00,"Ever more frequently network management tasks apply machine learning on network traffic. Both the accuracy of a machine learning model and its effectiveness in practice ultimately depend on the representation of raw network traffic as features. Often, the representation of the traffic is as important as the choice of the model itself; furthermore, the features that the model relies on will ultimately determine where (and even whether) the model can be deployed in practice. This paper develops a new framework and system that enables a joint evaluation of both the conventional notions of machine learning performance (e.g., model accuracy) and the systems-level costs of different representations of network traffic. We highlight these two dimensions for a practical network management task, video streaming quality inference, and show that the appropriate operating point for these two dimensions depends on the deployment scenario. We demonstrate the benefit of exploring a range of representations of network traffic and present Traffic Refinery, a proof-of-concept reference implementation that both monitors network traffic at 10 Gbps and transforms the traffic in real time to produce a variety of feature representations for machine learning models. Traffic Refinery both highlights this design space and makes it possible for network operators to easily explore different representations for learning, balancing systems costs related to feature extraction and model training against the resulting model performance",HAL CCSD,Traffic Refinery: Cost-Aware Traffic Representation for Machine Learning in Networks,,,,core
440321756,2021-06-18T00:00:00,"The utilization of computer technology to solve problems in medical scenarios
has attracted considerable attention in recent years, which still has great
potential and space for exploration. Among them, machine learning has been
widely used in the prediction, diagnosis and even treatment of Sepsis. However,
state-of-the-art methods require large amounts of labeled medical data for
supervised learning. In real-world applications, the lack of labeled data will
cause enormous obstacles if one hospital wants to deploy a new Sepsis detection
system. Different from the supervised learning setting, we need to use known
information (e.g., from another hospital with rich labeled data) to help build
a model with acceptable performance, i.e., transfer learning. In this paper, we
propose a semi-supervised optimal transport with self-paced ensemble framework
for Sepsis early detection, called SPSSOT, to transfer knowledge from the other
that has rich labeled data. In SPSSOT, we first extract the same clinical
indicators from the source domain (e.g., hospital with rich labeled data) and
the target domain (e.g., hospital with little labeled data), then we combine
the semi-supervised domain adaptation based on optimal transport theory with
self-paced under-sampling to avoid a negative transfer possibly caused by
covariate shift and class imbalance. On the whole, SPSSOT is an end-to-end
transfer learning method for Sepsis early detection which can automatically
select suitable samples from two domains respectively according to the number
of iterations and align feature space of two domains. Extensive experiments on
two open clinical datasets demonstrate that comparing with other methods, our
proposed SPSSOT, can significantly improve the AUC values with only 1% labeled
data in the target domain in two transfer learning scenarios, MIMIC
$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.Comment: 14 pages, 9 figure",,"Semi-supervised Optimal Transport with Self-paced Ensemble for
  Cross-hospital Sepsis Early Detection",http://arxiv.org/abs/2106.10352,,,core
480201885,2021-08-01T00:00:00,"The increasing digitalization and advancement in information communication technologies has greatly changed how humans interact with digital information. Nowadays, it is not sufficient to only display relevant data in production activities, as the enormous amount of data generated from smart devices can overwhelm operators without being fully utilized. Operators often require extensive knowledge of the machines in use to make informed decisions during processes such as maintenance and production. To enable novice operators to access such knowledge, it is important to reinvent the way of interacting with digitally enhanced smart devices. In this research, a mobile augmented reality remote monitoring system is proposed to help operators with low knowledge and experience level comprehend digital twin data of a device and interact with the device. It analyses both historic logs as well as real-time data through a cloud server and enriches 2D data with 3D models and animations in the 3D physical space. A cloud-based machine learning algorithm is applied to transform learned knowledge into live presentations on a mobile device for users to interact with. A scaled-down case study is conducted using a tower crane model to demonstrate the potential benefits as well as implications when the system is deployed in industrial environments. This user study verifies that the proposed solution yields consistent measurable improvements for novice users in human-device interaction that is statistically significant",'MDPI AG',An Integrated Mobile Augmented Reality Digital Twin Monitoring System,,10.3390/computers10080099,"[{'title': 'Computers', 'identifiers': ['issn:2073-431X', '2073-431x']}]",core
491308387,2022-01-14T16:14:01,"Numerical simulation is a critical part of research into and development of engineering systems. Engineers often use simulation to explore design settings both analytically and numerically before prototypes are built and tested. Even with the most advanced high performance computing facility, however, high-fidelity numerical simulations are extremely costly in time and resources. For example, a survey of the design parameter space for a single-element injector for a propulsion application (such as the RD-170 rocket engine) using the large eddy simulation technique may require several tens of millions of CPU-hours on a major computer cluster. This is because the flowfields can only be fully characterized by resolving a multitude of strongly coupled fluid dynamic, thermodynamic, transport, multiphase, and combustion processes. The cost is further increased by grid resolution requirements and by the effects of turbulence and high-pressure phenomena, which require treatment of real-fluid physics at supercritical conditions. If such models are used for statistical analysis or design optimization, the total computation time and resource requirements may render the work unfeasible.

Recent developments in deep learning techniques offer the possibility of significant advances in dealing with these challenges and significant shortening of the time-to-solution. The general scope of this thesis research is to set the foundations for new paradigms in modeling, simulation, and design by applying deep learning techniques to recent developments in computational science. More specifically, the research aims at developing an integrated suite of data-driven surrogate modeling approaches and software for large-scale simulation problems. The techniques to be put into practice include: (1) deep neural networks for function approximation and solver acceleration, (2) deep autoencoders for nonlinear dimensionality reduction, and (3) spatiotemporal emulators based on multi-level neural networks for simulator approximation and rapid exploration of design spaces.

A hierarchy of benchmark cases has been studied to generate databases to enable and support the development and verification of the proposed approaches. Emphasis is placed on canonical examples, as well as on engineering problems for aerospace and automotive applications, including supercritical turbulent flows in a rocket-engine swirl injector, and  multiphase cavitating flows in a diesel engine injector.Ph.D",Georgia Institute of Technology,DEEP-LEARNING-ENHANCED MULTIPHYSICS FLOW COMPUTATIONS FOR PROPULSION APPLICATIONS,https://core.ac.uk/download/491308387.pdf,,,core
387702556,2021-01-01T00:00:00,"Linear Approximate Dynamic Programming (LADP) and Incremental Approximate Dynamic Programming (IADP) are Reinforcement Learning methods that seek to contribute to the field of Adaptive Flight Control. This paper assesses their performance and convergence, as well as the impact of sensor noise on policy convergence, online system identification, performance and control surface deflection. After summarising their theory and derivation with full state (FS) and output feedback (OPFB), they are implemented on the linearised longitudinal F16 model. In order to establish an objective performance comparison, their hyper-parameters were tuned with an evolutionary algorithm: Particle Swarm Optimisation (PSO). Results show that LADP and IADP have the same performance in the presence of FS feedback, whereas LADP outperforms IADP when only OPFB is available. Output noise causes LADP based on OPFB to diverge. In the case of IADP based on OPFB, sensor noise improves the performance due to a better exploration of the solution space. The present research aims at bridging the gap between the discussed ADP algorithms and real world systems.Virtual/online event due to COVID-19Control & Simulatio",'American Institute of Aeronautics and Astronautics (AIAA)',Intelligent Adaptive Control Using LADP and IADP Applied to F-16 Aircraft with Imperfect Measurements,,10.2514/6.2021-1119,,core
475039204,2021-05-25T00:00:00,"Photonic brain-inspired platforms are emerging as novel analog computing
devices, enabling fast and energy-efficient operations for machine learning.
These artificial neural networks generally require tailored optical elements,
such as integrated photonic circuits, engineered diffractive layers,
nanophotonic materials, or time-delay schemes, which are challenging to train
or stabilize. Here we present a neuromorphic photonic scheme - photonic extreme
learning machines - that can be implemented simply by using an optical encoder
and coherent wave propagation in free space. We realize the concept through
spatial light modulation of a laser beam, with the far field that acts as
feature mapping space. We experimentally demonstrated learning from data on
various classification and regression tasks, achieving accuracies comparable to
digital extreme learning machines. Our findings point out an optical machine
learning device that is easy-to-train, energetically efficient, scalable and
fabrication-constraint free. The scheme can be generalized to a plethora of
photonic systems, opening the route to real-time neuromorphic processing of
optical data.Comment: 12 pages, 4 figure",'The Optical Society',Photonic extreme learning machine by free-space optical propagation,http://arxiv.org/abs/2105.12123,10.1364/PRJ.423531,,core
401914974,2021-03-29T00:00:00,"Purpose: This paper discusses the impact of COVID-19 on the Visual Arts industry in Malaysia. In general, this pandemic has affected various forms of artistic activities and the income of visual arts artists and galleries. The cancellation of art projects and exhibitions has greatly affected the artist's source of income as well as disrupted the sale of works and forms of art appreciation. The crisis has also opened up a new form to the visual arts industry by looking at alternative approaches to the continuity of the arts field by switching to virtual or online methods. This emerging crisis of COVID-19 might be the starting point for all art practitioners including artists, art critics, galleries/museums, collectors, and curators in using the online space to continue to capitalize on and expand the Visual Arts industry.
Design/methodology/approach: Review approach.
Findings: The COVID-19 pandemic has made a huge impact on the country's Visual Arts industry where a wide range of art activities cannot be implemented and opened up opportunities for online activities
Practical implications: Exhibition and sale of works through online approach has become one of the main methods that support the Visual Arts industry with the application of a combination of the latest technologies such as VR and AI that enable the representation of real experiences in the context of art appreciation.
Originality/value: This paper is original.
Paper type: This paper can be categorized as a viewpoin",'Narotama University',Covid -19: The Impact On Malaysian Visual Arts Scene,https://core.ac.uk/download/401914974.pdf,10.29138/ijebd.v4i2.1117,,core
479627403,2021-08-01T00:00:00,"Seed purity directly affects the quality of seed breeding and subsequent processing products. Seed sorting based on machine vision provides an effective solution to this problem. The deep learning technology, particularly convolutional neural networks (CNNs), have exhibited impressive performance in image recognition and classification, and have been proven applicable in seed sorting. However the huge computational complexity and massive storage requirements make it a great challenge to deploy them in real-time applications, especially on devices with limited resources. In this study, a rapid and highly efficient lightweight CNN based on visual attention, namely SeedSortNet, is proposed for seed sorting. First, a dual-branch lightweight feature extraction module Shield-block is elaborately designed by performing identity mapping, spatial transformation at higher dimensions and different receptive field modeling, and thus it can alleviate information loss and effectively characterize the multi-scale feature while utilizing fewer parameters and lower computational complexity. In the down-sampling layer, the traditional MaxPool is replaced as MaxBlurPool to improve the shift-invariant of the network. Also, an extremely lightweight sub-feature space attention module (SFSAM) is presented to selectively emphasize fine-grained features and suppress the interference of complex backgrounds. Experimental results show that SeedSortNet achieves the accuracy rates of 97.33% and 99.56% on the maize seed dataset and sunflower seed dataset, respectively, and outperforms the mainstream lightweight networks (MobileNetv2, ShuffleNetv2, etc.) at similar computational costs, with only 0.400M parameters (vs. 4.06M, 5.40M)",'PeerJ',SeedSortNet: a rapid and highly effificient lightweight CNN based on visual attention for seed sorting,,10.7717/peerj-cs.639,"[{'title': 'PeerJ Computer Science', 'identifiers': ['2376-5992', 'issn:2376-5992']}]",core
426984051,2021-03-24T00:00:00,"In recent years, the development environment for mobile applications using Augmented Reality (AR) technology, which superimposes virtual objects on real space, has improved dramatically, enabling the implementation of advanced applications by combining image processing using artificial intelligence (AI) and space sharing using network communication. In particular, the technology for simultaneous presentation of virtual objects to multiple users through space sharing is important for applications such as product demonstrations, design meetings, surgical support, or competitive games. However, such space-sharing technology alone can share virtual objects in the same space, but it cannot share remote objects as virtual objects with each other.In this research, as a proposal for the use of technology to mutually share remote objects as virtual objects, I will develop an AR game system that can communicate and compete with each other in the same way as real competitive games even in remote areas, by using a 360-degree omni-directional laser scanner to acquire the position and posture of a real radio-controlled car in each user\u27s space, and using network communication technology to mutually share this information and reproduce it as a virtual object in real time",法政大学大学院デザイン工学研究科,DEVELOPMENT OF AN OBJECT POSITION AND ATTITUDE SHARING NETWORKED AR GAME SYSTEM USING OMNI DIRECTIONAL LASER SCANNER,https://core.ac.uk/download/426984051.pdf,,,core
483410798,2021-01-01T00:00:00,"В статті зроблено спробу представити низку ключових технологій, що визначають нову якість життя людей, серед чого названо та розкрито зміст: автономного штучного інтелекту у смартфоні, професійних роботів помічників, доступний супутниковий інтелект, подкасти, цифрові інструменти міського планування. У статті авторами висунута гіпотеза про те, що Індустрія Х.0 є на сьогодні найвищою стадією цифровізації і являє собою концепцію інноваційно-цифрового виробництва, складниками якого є розумні активи, розумні сервіси, розумний бізнес та розумний уряд. Вказано структурні елементи авторської концепції Індустрії Х.0 представлено візуальний її зріз в умовах віртуальної реальності та функціонування даної Індустрії виключно в рамках 7-го технологічного укладу. 

Авторами розроблено та представлено протокол становлення Індустрії Х.0 крізь призму інновацій, технологій в управлінні галуззю та бізнесом. Визначено 4 етапи реалізації даного протоколу, а саме: визначення інноваційного ландшафту “технологічного прориву” у тій чи іншій галузі, формуючи Індустрію Х.0; здійснення оцінки загроз; визначення курсу подальшого розвитку та плану дій (чотири основних підходи до яких можуть вдаватись організації: захист, прийняття інновацій, ініціювання підривних інновацій, відступ); впровадження структурних змін на рівні ДНК організації. Аргументовано низкою чинників, що сьогоднішні реалії цифрового простору потребують відпрацювання нової логіки ведення платформного бізнесу в частинні його відцифрування. Зроблено висновок, що на практиці слід утворити широку коаліцію з освітян, урядовців, аналітиків, хайтек, економістів, промисловців, науковців, які всеціло долучаться до становлення Індустрії Х.0 на засадах цифровізації та інноватизації. В ході нашого дослідження, автори дійшли висновку, що Індустрія Х.0 являє собою новий підхід до організації виробництва в умовах віртуальної реальності в основі якого лежать високоінтелектуальні інтегровані новітні продукти та цифрові екосистеми, які формують повністю інноваційно-цифровий ланцюг створення вартості, додають нові компетенції та реалізують глибинні культурні зміни в напрямі становлення нової віртуальної реальності.В статье сделана попытка представить ряд ключевых технологий, определяющих новое качество жизни людей, среди чего названо и раскрыто содержание: автономного искусственного интеллекта в смартфоне, профессиональных роботов помощников, доступный спутниковый интеллект, подкасты, цифровые инструменты городского планирования. В статье авторами выдвинута гипотеза о том, что Индустрия Х.0 является сегодня высшей стадией цифровизации и представляет собой концепцию инновационно-цифрового производства, составляющими которого есть умные активы, умные сервисы, умный бизнес и умный правительство. Указано структурные элементы авторской концепции Индустрии Х.0 и представлен визуальный ее срез в условиях виртуальной реальности и функционирования данной Индустрии исключительно в рамках 7-го технологического уклада.

Авторами разработан и представлен протокол становления Индустрии Х.0 сквозь призму инноваций, технологий в управлении отраслью и бизнесом. Определены 4 этапа реализации данного протокола, а именно: определение инновационного ландшафта “технологического прорыва” в той или иной отрасли, формируя Индустрию Х.0; осуществление оценки угроз; определения курса дальнейшего развития и плана действий (четыре основных подхода к которым могут прибегать организации: защита, принятия инноваций, инициирование подрывных инноваций, отступление) внедрение структурных изменений на уровне ДНК организации. Аргументировано рядом факторов, что сегодняшние реалии цифрового пространства требуют отработки новой логики ведения платформенного бизнеса в частные его оцифровки. Сделан вывод, что на практике следует создать широкую коалицию с педагогов, чиновников, аналитиков, хайтек, экономистов, промышленников, ученых, которые всецело присоединятся к становлению Индустрии Х.0 на основе цифровизации и инноватизации. В ходе исследования, авторы пришли к выводу, что Индустрия Х.0 представляет собой новый подход к организации производства в условиях виртуальной реальности в основе которого лежат высокоинтеллектуальные интегрированы новейшие продукты и цифровые экосистемы, которые формируют полностью инновационно-цифровой цепь создания стоимости, добавляют новые компетенции и реализуют глубинные культурные изменения в направлении становления новой виртуальной реальности.The article attempts to present a number of key technologies that determine the new quality of life of people. The following content is specified and disclosed:

autonomous artificial intelligence in a smartphone, professional robot assistants, available satellite intelligence, podcasts, digital urban planning tools. In the

article, the authors hypothesize that Industry X.0 is by far the highest stage of digitalization and represents a concept of innovative and digital production,

the components of which are «smart assets», «smart services», «smart business», and «smart government». Structural elements of the authors’ concept of

Industry X.0 are indicated, its visual cut in virtual reality conditions is provided and the functioning of this Industry exclusively within the framework of the 7th

technological mode is characterized. The authors have developed and presented the protocol of formation of the Industry X.0 through the prism of innovations,

technologies in both the industry sector and business management. 4 stages of implementation of this protocol are defined, namely: determination of the innovative

landscape of «technological breakthrough» in a particular industry within the formation of Industry X.0; assessment of threats; determining the course

of further development and the action plan (four main approaches to which organizations can apply: protection, adoption of innovations, initiation of subversive

innovations, retreat); implementation of structural changes at the DNA level of the organization. The authors on the basis of a number of factors bring forward

the argument that today’s realities of the digital space require the development of a new logic of running a platform business in terms of its digitization. It is

concluded that in practice it is necessary to form a broad coalition of educators, government officials, analysts, high-tech specialists, economists, industrialists,

scientists who will join the formation of the Industry X.0 on the basis of digitalization and innovatizing. The authors concluded that Industry X.0 is a new approach

to the organization of production in the context of virtual reality, which is based on highly intelligent integrated new products and digital ecosystems that

form an innovative digital value chain, add new competencies and implement deep cultural changes in the direction of the formation of a new virtual reality","Видавничий дім «ІНЖЕК» (Харків, Україна)",Formation of Industry X.0 on the Basis of Innovative-Digital Entrepreneurship and Virtual Mobility,https://core.ac.uk/download/483410798.pdf,,,core
427431798,2021-01-01T00:00:00,"Scanning probe microscopies allow investigating surfaces at the nanoscale, in real space and with unparalleled signal-to-noise ratio. However, these microscopies are not used as much as it would be expected considering their potential. The main limitations preventing a broader use are the need of experienced users, the difficulty in data analysis and the time-consuming nature of experiments that require continuous user supervision. In this work, we addressed the latter and developed an algorithm that controlled the operation of an Atomic Force Microscope (AFM) that, without the need of user intervention, allowed acquiring multiple high-resolution images of different molecules. We used DNA on mica as a model sample to test our control algorithm, which made use of two deep learning techniques that so far have not been used for real time SPM automation. One was an object detector, YOLOv3, which provided the location of molecules in the captured images. The second was a Siamese network that could identify the same molecule in different images. This allowed both performing a series of images on selected molecules while incrementing the resolution, as well as keeping track of molecules already imaged at high resolution, avoiding loops where the same molecule would be imaged an unlimited number of times. Overall, our implementation of deep learning techniques brings SPM a step closer to full autonomous operation",'Royal Society of Chemistry (RSC)',Enabling autonomous scanning probe microscopy imaging of single molecules with deep learning,,10.1039/d1nr01109j,,core
334947592,2021-06-30T00:00:00,"Dimension reduction (DR) aims to learn low-dimensional representations of
high-dimensional data with the preservation of essential information. In the
context of manifold learning, we define that the representation after
information-lossless DR preserves the topological and geometric properties of
data manifolds formally, and propose a novel two-stage DR method, called
invertible manifold learning (inv-ML) to bridge the gap between theoretical
information-lossless and practical DR. The first stage includes a homeomorphic
sparse coordinate transformation to learn low-dimensional representations
without destroying topology and a local isometry constraint to preserve local
geometry. In the second stage, a linear compression is implemented for the
trade-off between the target dimension and the incurred information loss in
excessive DR scenarios. Experiments are conducted on seven datasets with a
neural network implementation of inv-ML, called i-ML-Enc. Empirically, i-ML-Enc
achieves invertible DR in comparison with typical existing methods as well as
reveals the characteristics of the learned manifolds. Through latent space
interpolation on real-world datasets, we find that the reliability of tangent
space approximated by the local neighborhood is the key to the success of
manifold-based DR algorithms.Comment: ECML-PKDD 2021 camera-ready. 15 pages (main) with 10 pages appendi",,Invertible Manifold Learning for Dimension Reduction,http://arxiv.org/abs/2010.04012,,,core
484088414,2021-01-01T00:00:00,"Reinforcement Learning (RL) has the potential of solving complex continuous control tasks, with direct applications to robotics. Nevertheless, current state-of-the-art methods are generally unsafe to learn directly on a physical robot as exploration by trial-and-error can cause harm to the real world systems. In this paper, we leverage a framework for learning latent action spaces for RL agents from demonstrated trajectories. We extend this framework by connecting it to a variable impedance Cartesian space controller, allowing us to learn contact-rich tasks safely and efficiently. Our method learns from trajectories that incorporate both positional, but also crucially impedance-space information. We evaluate our method on a number of peg-in-hole task variants with a Franka Panda arm and demonstrate that learning variable impedance actions for RL in Cartesian space can be safely deployed on the real robot directly, without resorting to learning in simulation and a subsequent policy transfer","Department of Computer Science, Lund University, Sweden",Learning Impedance Actions for Safe Reinforcement Learning in Contact-Rich Tasks,,,,core
479728907,2021-01-01T00:00:00,"The large number of visual applications in multimedia sharing websites and social networks contribute to the increasing amounts of multimedia data in cyberspace. Video data is a rich source of information and considered the most demanding in terms of storage space. With the huge development of digital video production, video management becomes a challenging task. Video content analysis (VCA) aims to provide big data solutions by automating the video management. To this end, shot boundary detection (SBD) is considered an essential step in VCA. It aims to partition the video sequence into shots by detecting shot transitions. High computational cost in transition detection is considered a bottleneck for real-time applications. Thus, in this paper, a balance between detection accuracy and speed for SBD is addressed by presenting a new method for fast video processing. The proposed SBD framework is based on the concept of candidate segment selection with frame active area and separable moments. First, for each frame, the active area is selected such that only the informative content is considered. This leads to a reduction in the computational cost and disturbance factors. Second, for each active area, the moments are computed using orthogonal polynomials. Then, an adaptive threshold and inequality criteria are used to eliminate most of the non-transition frames and preserve candidate segments. For further elimination, two rounds of bisection comparisons are applied. As a result, the computational cost is reduced in the subsequent stages. Finally, machine learning statistics based on the support vector machine is implemented to detect the cut transitions. The enhancement of the proposed fast video processing method over existing methods in terms of computational complexity and accuracy is verified. The average improvements in terms of frame percentage and transition accuracy percentage are 1.63&#x0025; and 2.05&#x0025;, respectively. Moreover, for the proposed SBD algorithm, a comparative study is performed with state-of-the-art algorithms. The comparison results confirm the superiority of the proposed algorithm in computation time with improvement of over 38&#x0025;",'Institute of Electrical and Electronics Engineers (IEEE)',Fast Shot Boundary Detection Based on Separable Moments and Support Vector Machine,,10.1109/ACCESS.2021.3100139,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
430678251,2021-01-01T00:00:00,"In millimeter-wave (MMW) networks, the channel state information (CSI) carries essential information from the user to the base station (BS). The CSI values depend highly on the geometrical and physical features of the environment. Therefore, it is impossible to generate CSI data for computer simulations or analysis through mathematical models. The CSI in MMW networks can only be acquired through physical measurement(s) or with the help of expensive and complicated ray-tracing software. For many users, both these options are infeasible. This work aims to propose a simple and fast method that can generate artificial samples from the real data samples while ensuring that the artificial samples look similar to the real ones. The proposed method helps increase the size of existing CSI datasets and likely to benefit the evolution of deep learning models that need a large amount of training/testing data. The proposed method comprises two parts. (i) The first part applies data clustering and transformations such as principal component analysis (PCA)-based dimensionality reduction and probability integral transform (PIT) to convert the real data into a multivariate normal distribution of a smaller number of variables, and (ii) The second part synthesizes artificial data by learning from the multivariate normal distribution of the first part. The last step in the second part is to apply PIT and inverse PCA transformations to transform the artificial data into the same space as the input data. We compared the proposed method&#x2019;s performance with the well-known Kernel density estimation (KDE)-based methods that use Scott&#x2019;s rule and Silverman&#x2019;s rule to choose the bandwidth parameter value. The results show that the artificial samples generated by the proposed method exhibit very high similarity with the real ones as compared to the KDE-based methods",'Institute of Electrical and Electronics Engineers (IEEE)',A Machine Learning Method to Synthesize Channel State Information Data in Millimeter Wave Networks,,10.1109/ACCESS.2021.3087630,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
387280891,2021-09-29T00:00:00,"Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates",'Institute of Electrical and Electronics Engineers (IEEE)',"Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space",http://arxiv.org/abs/2011.09747,10.1109/JIOT.2021.3114102,,core
480199880,2021-08-01T00:00:00,"Education 4.0 is looking to prepare future scientists and engineers not only by granting them with knowledge and skills but also by giving them the ability to apply them to solve real life problems through the implementation of disruptive technologies. As a consequence, there is a growing demand for educational material that introduces science and engineering students to technologies, such as Artificial Intelligence (AI) and Brain–Computer Interfaces (BCI). Thus, our contribution towards the development of this material is to create a test bench for BCI given the basis and analysis on how they can be discriminated against. This is shown using different AI methods: Fisher Linear Discriminant Analysis (LDA), Support Vector Machines (SVM), Artificial Neural Networks (ANN), Restricted Boltzmann Machines (RBM) and Self-Organizing Maps (SOM), allowing students to see how input changes alter their performance. These tests were done against a two-class Motor Image database. First, using a large frequency band and no filtering eye movement. Secondly, the band was reduced and the eye movement was filtered. The accuracy was analyzed obtaining values around 70∼80% for all methods, excluding SVM and SOM mapping. Accuracy and mapping differentiability increased for some subjects for the second scenario 70∼85%, meaning either their band with the most significant information is on that limited space or the contamination because of eye movement was better mitigated by the regression method. This can be translated to saying that these methods work better under limited spaces. The outcome of this work is useful to show future scientists and engineers how BCI experiments are conducted while teaching them the basics of some AI techniques that can be used in this and other several experiments that can be carried on the framework of Education 4.0",'MDPI AG',Education 4.0: Teaching the Basis of Motor Imagery Classification Algorithms for Brain-Computer Interfaces,,10.3390/fi13080202,"[{'title': 'Future Internet', 'identifiers': ['issn:1999-5903', '1999-5903']}]",core
490928344,2021-11-26T00:00:00,"Deep learning algorithms have recently achieved promising deraining
performances on both the natural and synthetic rainy datasets. As an essential
low-level pre-processing stage, a deraining network should clear the rain
streaks and preserve the fine semantic details. However, most existing methods
only consider low-level image restoration. That limits their performances at
high-level tasks requiring precise semantic information. To address this issue,
in this paper, we present a segmentation-aware progressive network (SAPNet)
based upon contrastive learning for single image deraining. We start our method
with a lightweight derain network formed with progressive dilated units (PDU).
The PDU can significantly expand the receptive field and characterize
multi-scale rain streaks without the heavy computation on multi-scale images. A
fundamental aspect of this work is an unsupervised background segmentation
(UBS) network initialized with ImageNet and Gaussian weights. The UBS can
faithfully preserve an image's semantic information and improve the
generalization ability to unseen photos. Furthermore, we introduce a perceptual
contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL)
to regulate model learning. By exploiting the rainy image and groundtruth as
the negative and the positive sample in the VGG-16 latent space, we bridge the
fine semantic details between the derained image and the groundtruth in a fully
constrained manner. Comprehensive experiments on synthetic and real-world rainy
images show our model surpasses top-performing methods and aids object
detection and semantic segmentation with considerable efficacy. A Pytorch
Implementation is available at
https://github.com/ShenZheng2000/SAPNet-for-image-deraining.Comment: Accepted by WACV202",,"SAPNet: Segmentation-Aware Progressive Network for Perceptual
  Contrastive Deraining",http://arxiv.org/abs/2111.08892,,,core
481283852,2021-11-10T17:30:14,"This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) under Grant EP/K008730/1, PAMELA ProjectVisual understanding of 3D environments in real-time, at low power, is a huge computational challenge. Often referred to as SLAM (Simultaneous Localisation and Mapping), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are (1) tools and methodology for systematic quantitative evaluation of SLAM algorithms, (2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives, (3) end-to-end simulation tools to enable optimisation of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches, and (4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.Publisher PDFPeer reviewe",'Institute of Electrical and Electronics Engineers (IEEE)',Navigating the landscape for real-time localization and mapping for robotics and virtual and augmented reality,https://core.ac.uk/download/481283852.pdf,10.1109/JPROC.2018.2856739,"[{'title': 'Proceedings of the IEEE', 'identifiers': ['0018-9219', 'issn:0018-9219']}]",core
14690759,2045-05-23T00:00:00,"Gli uomini vivono in società. Un campo di analisi vasto, complesso, un universo di pianeti. Le modalità per studiarlo molteplici, intersecanti, interdisciplinari: una “rete”.
Gli uomini vivono in una società e interagiscono per ottenere dal “sistema società” il massimo profitto “personale”. Il tutto avviene in un determinato “spazio vitale reale o virtuale”, quello dove i nostri affari vengono svolti. Ed è qui che l’uomo cerca un equilibrio vantaggioso, o certamente positivo per ciò che “rappresenta”. Ma non può fare tutto da sé. Il mondo oggi è ormai ad un grado di complessità che è utile, efficace, efficiente collaborare per tendere a sviluppare l’eccellenza e non solo.
Collaborare diventa poi fondamentale in alcune situazioni, come quelle stra–ordinarie, dove “lo stato di non normalità”, diventa “emergenza”. La riflessione ci porterà attraverso una lettura della normalità su ciò che è possibile fare, quello che succede mentre comunichiamo e come meglio porci in dinamica, ricavando dalla sinergia interdisciplinare, o meglio transdisciplinare, lo stimolo a non disperdere energie.
Per far questo, proporrò dopo aver chiarito cosa s’intenda per emergenza e per collaborazione ed aver fatto un quadro generale della ricerca nel primo capitolo, un’analisi sull’organizzazione nell’emergenza: quali presupposti educativi macrosociali, quale specificità potenziale per gli addetti ai lavori nella nuova figura professionale del Disaster Manager, quale urgenza di una progettazione oltre la pianificazione attraverso ad esempio la tecnica logoterapica ed infine quale sviluppo fiduciario di comunità spinga le motivazioni.
Non solo. Nel capitolo terzo, mi misurerò con altre scoperte di ricerca sulla necessità di un gruppo guida e su come creare l’idea della partecipazione, quale mezzo per ricostruire un’intesa spesso dispersa, facendo accenno allo sviluppo ed all’efficacia del software Haria–2 sempre più innovativo e da noi implementato.
Infine, per completare la visione globale della ricerca mi sono sentito in dovere di accennare all’evoluzione di tecniche di gruppo, personalmente sperimentate, che centrano le competenze collaborative, proponendo così una formazione partecipata.
Si conosce sé stessi. Si sviluppa cioè la consapevolezza graduale di ciò di cui sentiamo di aver bisogno e anche di cosa possiamo mettere a servizio degli altri e per noi stessi. L’educazione deve essere vissuta, elaborata, come strada maestra per un’auto consapevolezza di cammino con…, ma curando il “peso” delle relazioni.
Per essere collaborativi, per diventare gruppo integrato e integrante in situazioni rischiose, questo “essere sé stessi” è immerso in un “essere sé stessi con gli altri” e va saputo gestire. Ecco la Maratona, lo psicodramma, lo stare in rete, l’educazione dei piccoli, ma anche tecniche relazionali come il Focus Group ed il T–Group.
INGLESE (vedi Indice Inglese)
People live in society. A wide, complex, field of analysis, a universe of planets . The ways to study it are various, intersecting, interdisciplinary: a “net”.
People live in society and they interact to obtain by the “society system” the greatest “personal” profit. Everything happens in a determined “vital real or virtual space”, the one in which our affairs are performed. Here people search for a favourable, or certainly positive balance, for his “role”. But they can’t act alone. Today the world is so complex that it is useful, efficacious, efficient, to collaborate to develop excellence and not only it.
Collaborating becomes then very important in several situations, as for example the extra–ordinary ones, where “the not–normality condition”, becomes “emergency”. The reflection will lead us through a reading of normality to what is possible to do, what happens while we are speaking and how to better relate, taking from the interdisciplinary synergy, or better transdisciplinary , the incentive not to dissipate energies.
For this purpose, after clearing what I mean by emergency and by collaboration and after a general view of the research in the first chapter, I will propose an analysis about the organization in emergency: what macrosocial educational assumptions, what potential specificity for people in charge of works in the new professional figure of the Disaster Manager, what urgency of a planning over the project through for example the ‘logotherapeutic’ technique and finally which trust development of community pushes motivations.
Not only. In the third chapter, I will compete with other discoveries of research about the necessity of a guide group and about how to create the idea of the participation, as a means to rebuild an often dispersed accord, referring to the development and the efficaciousness of the software Haria–2 more and more innovative and improved by us.
Finally, to complete the global vision of the research I felt bound to speak about the evolution of group techniques, personally experimented, that centre the competence of collaboration, so proposing a participated formation.
We know ourselves. That is we develop gradual awareness of what we feel we need and also what we can put at the service of the others and of ourselves. Education has to be lived, elaborated, like a main road for a self–awareness of way with…, but caring the “weight” of relationships.
To collaborate, to become an integrated group and integrating in dangerous situations, this “being ourselves” is immersed in a “being ourselves with the others” and we must be able to manage it. Here is the ‘Marathon’, the ‘psychodrama’, staying in the net, children education, but also connection techniques like the Focus Group and the T–Group",'Pisa University Press',"La collaborazione in ""emergenza"". Come sviluppare competenze collaborative da utilizzare in contesti anche ""fuori dai margini"".",https://core.ac.uk/download/14690759.pdf,,,core
482129734,2021-04-01T07:00:00,"Research into Internet of things (IoT) began January 21st, 2021, as part of the subaward of Kansas NASA Space Grant 1, regarding machine learning on the edge. The research is multifaceted and pertains microcontrollers capable of running neural networks, and deep learning through the assistance of real-time operating systems (RTOS). The study included a multistep process which involved an understanding of a microcontroller which is capable of RTOS and neural network functionality, such as the MSP432 microcontroller. An understanding of the advantages and limitations of microcontroller regarding on the edge applications is also necessary. The premise of edge machine learning pertains to the necessity of being able to train neural networks using powerful computing. After a neural network is trained, the neural network may be implemented on smaller, less powerful devices. This is beneficial where high latency and restricted bandwidth may impede access to cloud computing. Using the software Excel, PyCharm, and Google Colab, the research has progressed into a deeper understanding of forward propagation, weights, biases, multi-hidden layer networks, and activation functions. Small feedforward neural networks have been developed in Excel in order to better understand the mathematics of training neural networks. This information can later be used in larger neural networks. The research is currently moving into a deeper understanding of backpropagation and training of neural networks which includes an understanding of gradient descent and cost functions. These processes will likely include the application of developed libraries",Pittsburg State University Digital Commons,Machine Learning on the Edge,,,,core
421031031,2021-04-27T00:00:00,"I. PROJECT OVERVIEW A. Research Question In this project, the question was asked: ”Is there an easier way to extract vocals from music?” Many other works are able to extract vocals with Deep Neural Networks using Multitask Learning, which are large and take a long time to train. To rival this, we wish to present a method to identify vocals with a Convolutional U-Network (U-Net) for Semantic Segmentation of audio files. B. Project Description This project differs from other works by identifying vocal locations by converting audio files in Short Time Fourier Transforms(STFT), and treating them as images in the UNet. By treating these as images, the U-Net is able to identify the location of ”vocal features” the same way a U-Net would identify desired features within an image. The object detection is what sets this project apart from similar works. Many of these other works treat each song as an audio signal with real and imaginary components which means these algorithms treat the issue as a signal processing problem. However, by looking at the STFT of the song as a graph, we are instead able to approach this as an image processing problem instead, which offers more tools within the realm of Deep Learning–such as Semantic Segmentation. II. EXPERIMENTATION A. Materials and Methods All Materials used were a form of software. Firstly, the UNetwork was created and ran in python on the CCSE Cluster for High Efficiency. A U-Network is a Convoluted Neural Network that has the ability to output images by Convoluting the original image to allow only the prominent features to be shown and Deconvoluting the Output to display these features in the original image resolution to be used for further processing. This gives the U-net it’s ”u” shape when drawn out. Secondly, the data created for the project were music files converted into Short Time Fourier Transforms(STFT) and processed as image files, where the input into the U-Network was an entire song’s STFT and the labeled data was the vocal audio file STFT for that same song. A Short-Time Fourier Transform can be considered the heatmap of the amplitudes of the song across frequency and time. B. Results The initial Results from the U-Network show a high level of accuracy for vocal location predictions. As the output from a U-Network is an image, these images are the initial song’s STFT with a mask applied to show the location of Vocal Waves. These trials have an accuracy greater than 80% which is a very good result this early in the processing. The vocals have been identified and located in this study, however the next step is to pull the vocals out and convert them back into a song wave. III. MARKETABILITY For the last 20 or so years, large record labels have been attempting to ”Remaster” old music, which is the process of digitizing old analog tracks of songs, mixing them on a new sound board, and releasing the remastered work at a marked up price. As recording methods, pre-computers, relied on tape, often times tracks were record over each other to save space on the real. When the song has this issue, a computer program has to pull out all of the pieces of the song so that the engineer can remaster it. This project shows the initial steps to a simpler audio extraction, where handling this issue as an image processing problem instead of a signal processing problem, we are able to create a more efficient Neural Network.Advisors(s): Dr. AledhariTopic(s): Artificial IntelligenceCS 426",DigitalCommons@Kennesaw State University,UR-48 Using Semantic Segmentation in a Convoluted Neural Network for Vocal Localization in Music,https://core.ac.uk/download/421031031.pdf,,,core
395737675,2021-03-01T08:00:00,"In this dissertation, we are interested in improving the generalization of deep neural networks for biomedical data (e.g., electrocardiogram signal, x-ray images, etc). Although deep neural networks have attained state-of-the-art performance and, thus, deployment across a variety of domains, similar performance in the clinical setting remains challenging due to its ineptness to generalize across unseen data (e.g., new patient cohort).
We address this challenge of generalization in the deep neural network from two perspectives: 1) learning disentangled representations from the deep network, and 2) developing efficient semi-supervised learning (SSL) algorithms using the deep network.
In the former, we are interested in designing specific architectures and objective functions to learn representations, where variations in the data are well separated, i.e., disentangled. In the latter, we are interested in designing regularizers that encourage the underlying neural function\u27s behavior toward a common inductive bias to avoid over-fitting the function to small labeled data.
Our end goal is to improve the generalization of the deep network for the diagnostic model in both of these approaches. In disentangled representations, this translates to appropriately learning latent representations from the data, capturing the observed input\u27s underlying explanatory factors in an independent and interpretable way. With data\u27s expository factors well separated, such disentangled latent space can then be useful for a large variety of tasks and domains within data distribution even with a small amount of labeled data, thus improving generalization. In developing efficient semi-supervised algorithms, this translates to utilizing a large volume of the unlabelled dataset to assist the learning from the limited labeled dataset, commonly encountered situation in the biomedical domain.
By drawing ideas from different areas within deep learning like representation learning (e.g., autoencoder), variational inference (e.g., variational autoencoder), Bayesian nonparametric (e.g., beta-Bernoulli process), learning theory (e.g., analytical learning theory), function smoothing (Lipschitz Smoothness), etc., we propose several leaning algorithms to improve generalization in the associated task. We test our algorithms on real-world clinical data and show that our approach yields significant improvement over existing methods. Moreover, we demonstrate the efficacy of the proposed models in the benchmark data and simulated data to understand different aspects of the proposed learning methods.
We conclude by identifying some of the limitations of the proposed methods, areas of further improvement, and broader future directions for the successful adoption of AI models in the clinical environment",RIT Scholar Works,Learning with Limited Labeled Data in Biomedical Domain by Disentanglement and Semi-Supervised Learning,https://core.ac.uk/download/395737675.pdf,,,core
395145104,2024-03-26T00:00:00,"We implement a machine-learning inversion approach that uses a convolutional neural network (CNN) to solve the petrophysical seismic inversion. A discrete Cosine Transform (DCT) is used to compress both the input and output response of the network, and hence the network is trained to predict the nonlinear mapping between the DCT-transformed seismic data and the DCT-transformed petrophysical model. This transformation is used as an additional feature extraction technique that not only reduces the dimensionality of the input and output of the network but also guarantees the preservation of the assumed temporal and spatial continuity pattern in the estimated model. A theoretical rock-physics model (RPM) based on granular media models constitutes the link between the elastic and the petrophysical space, whereas the exact Zoeppritz equations map the elastic properties onto the seismic pre-stack domain. A direct sequential co-simulation with joint probability distribution generates the training and validation sets under the assumption of a stationary non-parametric prior and a Gaussian variogram model. We apply a Monte Carlo simulation strategy to propagate onto the final estimates both the uncertainties associated to the noise contamination in the data and the modeling error introduced by the network approximation. We discuss synthetic inversions to a realistic subsurface model that simulates a real gas-saturated reservoir hosted in a turbiditic sequence. We assess the robustness and stability of our trained CNN in case of erroneous assumptions about the noise statistics, errors in the calibrated RPM, and errors in the estimated source wavelet. The outcomes of the proposed approach are compared with those achieved by a more standard linearized inversion in which each seismic gather is inverted separately. Lastly, we demonstrate that transfer learning avoids retraining the network from scratch when the statistical properties of the training and test sets differ. Our experiments confirm that the implemented CNN inversion successfully solves the petrophysical seismic inversion and guarantees more stable and accurate predictions with respect to the standard inversion approach. In particular, once the network has been trained, the implemented inversion retrieves petrophysical properties and associated uncertainties in near real-time",'Pisa University Press',Hybrid machine learning - Monte Carlo approach to petrophysical seismic inversion,,,,core
390090541,2021-03-10T08:00:00,"Systems experiencing high-rate dynamic events, termed high-rate systems, typically undergo accelerations of amplitudes higher than 100 g-force in less than 10 ms. Examples include adaptive airbag deployment systems, hypersonic vehicles, and active blast mitigation systems. Given their critical functions, accurate and fast modeling tools are necessary for ensuring the target performance. However, the unique characteristics of these systems, which consist of (1) large uncertainties in the external loads, (2) high levels of non-stationarities and heavy disturbances, and (3) unmodeled dynamics generated from changes in system configurations, in combination with the fast-changing environments, limit the applicability of physical modeling tools. In this paper, a deep learning algorithm is used to model high-rate systems and predict their response measurements. It consists of an ensemble of short-sequence long short-term memory (LSTM) cells which are concurrently trained. To empower multi-step ahead predictions, a multi-rate sampler is designed to individually select the input space of each LSTM cell based on local dynamics extracted using the embedding theorem. The proposed algorithm is validated on experimental data obtained from a high-rate system. Results showed that the use of the multi-rate sampler yields better feature extraction from non-stationary time series compared with a more heuristic method, resulting in significant improvement in step ahead prediction accuracy and horizon. The lean and efficient architecture of the algorithm results in an average computing time of 25 μμs, which is below the maximum prediction horizon, therefore demonstrating the algorithm’s promise in real-time high-rate applications",Iowa State University Digital Repository,Multi-Time Resolution Ensemble LSTMs for Enhanced Feature Extraction in High-Rate Time Series,https://core.ac.uk/download/390090541.pdf,,,core
387291020,2021-04-02T00:00:00,"Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks: (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environments. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30% on camera pose accuracy, while
running comparably fast for evaluation",,"Robust Neural Routing Through Space Partitions for Camera Relocalization
  in Dynamic Indoor Environments",http://arxiv.org/abs/2012.04746,,,core
478614122,2021-10-08T00:00:00,"Federated learning (FL) allows multiple clients with (private) data to
collaboratively train a common machine learning model without sharing their
private training data. In-the-wild deployment of FL faces two major hurdles:
robustness to poisoning attacks and communication efficiency. To address these
concurrently, we propose Federated Supermask Learning (FSL). FSL server trains
a global subnetwork within a randomly initialized neural network by aggregating
local subnetworks of all collaborating clients. FSL clients share local
subnetworks in the form of rankings of network edges; more useful edges have
higher ranks. By sharing integer rankings, instead of float weights, FSL
restricts the space available to craft effective poisoning updates, and by
sharing subnetworks, FSL reduces the communication cost of training. We show
theoretically and empirically that FSL is robust by design and also
significantly communication efficient; all this without compromising clients'
privacy. Our experiments demonstrate the superiority of FSL in real-world FL
settings; in particular, (1) FSL achieves similar performances as
state-of-the-art FedAvg with significantly lower communication costs: for
CIFAR10, FSL achieves same performance as Federated Averaging while reducing
communication cost by ~35%. (2) FSL is substantially more robust to poisoning
attacks than state-of-the-art robust aggregation algorithms. We have released
the code for reproducibility",,FSL: Federated Supermask Learning,http://arxiv.org/abs/2110.04350,,,core
478911817,2021-10-15T00:00:00,"Plants are dynamic systems that are integral to our existence and survival.
Plants face environment changes and adapt over time to their surrounding
conditions. We argue that plant responses to an environmental stimulus are a
good example of a real-world problem that can be approached within a
reinforcement learning (RL)framework. With the objective of controlling a plant
by moving the light source, we propose GrowSpace, as a new RL benchmark. The
back-end of the simulator is implemented using the Space Colonisation
Algorithm, a plant growing model based on competition for space. Compared to
video game RL environments, this simulator addresses a real-world problem and
serves as a test bed to visualize plant growth and movement in a faster way
than physical experiments. GrowSpace is composed of a suite of challenges that
tackle several problems such as control, multi-stage learning,fairness and
multi-objective learning. We provide agent baselines alongside case studies to
demonstrate the difficulty of the proposed benchmark",,GrowSpace: Learning How to Shape Plants,http://arxiv.org/abs/2110.08307,,,core
386113006,2021-01-15T00:00:00,"In this study, we propose a technique for quantitative visual inspection that can quantify structural damage using extended reality (XR). The XR headset can display and overlay graphical information on the physical space and process the data from the built-in camera and depth sensor. Also, the device permits accessing and analyzing image and video stream in real-time and utilizing 3D meshes of the environment and camera pose information. By leveraging these features for the XR headset, we build a workflow and graphic interface to capture the images, segment damage regions, and evaluate the physical size of damage. A deep learning-based interactive segmentation algorithm called f-BRS was deployed to precisely segment damage regions through the XR headset. A ray-casting algorithm is implemented to obtain 3D locations corresponding to the pixel locations of the damage region on the image. The size of the damage region is computed from the 3D locations of its boundary. The performance of the proposed method is demonstrated through a field experiment at an in-service bridge where spalling damage is present at its abutment. The experiment shows that the proposed method provides sub-centimeter accuracy for the size estimation",'University of Waterloo',Real-time Quantitative Visual Inspection using Extended Reality,https://core.ac.uk/download/386113006.pdf,10.15353/jcvis.v6i1.3557,,core
481197080,2021-09-01T00:00:00,"Propose: The purpose of this study was to compare the performance of deep learning networks trained with complex-valued and magnitude images in suppressing the aliasing artifact for highly accelerated real-time cine MRI.Methods: Two 3D U-net models (Complex-Valued-Net and Magnitude-Net) were implemented to suppress aliasing artifacts in real-time cine images. ECG-segmented cine images (n = 503) generated from both complex k-space data and magnitude-only DICOM were used to synthetize radial real-time cine MRI. Complex-Valued-Net and Magnitude-Net were trained with fully sampled and synthetized radial real-time cine pairs generated from highly undersampled (12-fold) complex k-space and DICOM images, respectively. Real-time cine was prospectively acquired in 29 patients with 12-fold accelerated free-breathing tiny golden-angle radial sequence and reconstructed with both Complex-Valued-Net and Magnitude-Net. Cardiac function, left-ventricular (LV) structure, and subjective image quality [1(non-diagnostic)-5(excellent)] were calculated from Complex-Valued-Net– and Magnitude-Net–reconstructed real-time cine datasets and compared to those of ECG-segmented cine (reference).Results: Free-breathing real-time cine reconstructed by both networks had high correlation (all R2 &gt; 0.7) and good agreement (all p &gt; 0.05) with standard clinical ECG-segmented cine with respect to LV function and structural parameters. Real-time cine reconstructed by Complex-Valued-Net had superior image quality compared to images from Magnitude-Net in terms of myocardial edge sharpness (Complex-Valued-Net = 3.5 ± 0.5; Magnitude-Net = 2.6 ± 0.5), temporal fidelity (Complex-Valued-Net = 3.1 ± 0.4; Magnitude-Net = 2.1 ± 0.4), and artifact suppression (Complex-Valued-Net = 3.1 ± 0.5; Magnitude-Net = 2.0 ± 0.0), which were all inferior to those of ECG-segmented cine (4.1 ± 1.4, 3.9 ± 1.0, and 4.0 ± 1.1).Conclusion: Compared to Magnitude-Net, Complex-Valued-Net produced improved subjective image quality for reconstructed real-time cine images and did not show any difference in quantitative measures of LV function and structure",'Frontiers Media SA',Comparison of Complex k-Space Data and Magnitude-Only for Training of Deep Learning–Based Artifact Suppression for Real-Time Cine MRI,https://core.ac.uk/download/481197080.pdf,10.3389/fphy.2021.684184,"[{'title': 'Frontiers in Physics', 'identifiers': ['issn:2296-424X', '2296-424x']}]",core
482093119,2021-08-01T00:00:00,"Classifying fluctuating operating wireless environments can be crucial for successfully delivering authentic and confidential packets and for identifying legitimate signals. This study utilizes raw in-phase (I) and quadrature-phase (Q) samples, exclusively, to develop a low-order statistical feature set for wireless signal classification. Edge devices making decentralized decisions from I/Q sample analysis is beneficial. Implementing appropriate security and transmitting mechanisms, reducing retransmissions and increasing energy efficiency are examples. Wireless sensor networks (WSNs) and their Internet of Things (IoT) utilization emphasize the significance of this time series classification problem. Here, I/Q samples of typical WSN and industrial, scientific and medical band transmissions are collected in a live operating environment. Analog Pluto software-defined radios and Raspberry Pi devices are utilized to achieve a low-cost yet high-performance testbed. Features are extracted from Matlab-based statistical analysis of the I/Q samples across time, frequency (fast Fourier transform) and space (probability density function). Noise, ZigBee, continuous wave jamming, WiFi and Bluetooth signal data are examined. Supervised machine learning approaches, including support vector machines, Random Forest, XGBoost, k nearest neighbors and a deep neural network (DNN), evaluate the developed feature set. The optimal approach is determined as an XGBoost/SVM classifier. This classifier achieves similar accuracy and generalization results, on unseen data, to the DNN, but for a fraction of time and computation requirements. Compared to existing approaches, this study’s principal contribution is the developed low-order feature set that achieves signal classification without prior network knowledge or channel assumptions and is validated in a real-world wireless operating environment. The feature set can extend the development of resource-constrained edge devices as it is widely deployable due to only requiring received I/Q samples and these features are warranted as IoT devices become widely used in various modern applications",'MDPI AG',Developing a Low-Order Statistical Feature Set Based on Received Samples for Signal Classification in Wireless Sensor Networks and Edge Devices,,10.3390/iot2030023,"[{'title': 'IoT', 'identifiers': ['issn:2624-831X', '2624-831x']}]",core
421092900,2021-03-08T14:08:16,"Subduction zones are monitored using space geodesy with increasing resolution, with the aim of better capturing the deformation accompanying the seismic cycle. Here, we investigate data characteristics that maximize the performance of a machine learning binary classifier predicting slip‐event imminence. We overcome the scarcity of recorded instances from real subduction zones using data from a seismotectonic analog model monitored with a spatially dense, continuously recording onshore geodetic network. We show that a 70–85 km‐wide coastal swath recording interseismic deformation gives the most important information on slip imminence. Prediction performances are mainly influenced by the alarm duration (amount of time that we consider an event as imminent), with density of stations and record length playing a secondary role. The techniques developed in this study are most likely applicable in regions of slow earthquakes, where stick‐slip‐like failures occur at time intervals of months to years.Plain Language Summary:
Machine learning, a group of algorithms that produce predictions based on past “experience,” has been successfully used to predict various aspects of the earthquake process, including slip imminence. The accuracy of those algorithms depends on a variety of data characteristics, for example, the amount of data used for building the “experience” of the model. We focus on this point using a scaled representation of a seismic subduction zone and a monitoring technique similar to Global Navigation Satellite System. We identify the most useful surface regions to be monitored and the parameter that most strongly influences prediction accuracy for the timing of upcoming laboratory earthquakes. The routine implemented in this study could be used to predict the onset and extent of slow earthquakes.Key Points:


We investigate the performances of a binary classifier predicting slip‐event imminence in analog models of megathrust seismic cycling.
A 70–85 km‐wide coastal swath is the region producing the most important information for the imminence classification.
Length of time that we consider an event imminent plays a primary role in tuning the performances of a binary classifier predicting the imminence of analog earthquakes.DAAD‐Prim",'American Geophysical Union (AGU)',Predicting Imminence of Analog Megathrust Earthquakes With Machine Learning: Implications for Monitoring Subduction Zones,,10.1029/2019GL086615,,core
395623931,2021-01-28T00:00:00,"Ever more frequently network management tasks apply machine learning on network traffic. Both the accuracy of a machine learning model and its effectiveness in practice ultimately depend on the representation of raw network traffic as features. Often, the representation of the traffic is as important as the choice of the model itself; furthermore, the features that the model relies on will ultimately determine where (and even whether) the model can be deployed in practice. This paper develops a new framework and system that enables a joint evaluation of both the conventional notions of machine learning performance (e.g., model accuracy) and the systems-level costs of different representations of network traffic. We highlight these two dimensions for a practical network management task, video streaming quality inference, and show that the appropriate operating point for these two dimensions depends on the deployment scenario. We demonstrate the benefit of exploring a range of representations of network traffic and present Traffic Refinery, a proof-of-concept reference implementation that both monitors network traffic at 10 Gbps and transforms the traffic in real time to produce a variety of feature representations for machine learning models. Traffic Refinery both highlights this design space and makes it possible for network operators to easily explore different representations for learning, balancing systems costs related to feature extraction and model training against the resulting model performance",HAL CCSD,Traffic Refinery: Cost-Aware Traffic Representation for Machine Learning in Networks,,,,core
4829533,,"While many excellent induction algorithms are known for making predictions from databases in well-studied domains, learning systems still perform poorly in many difficult real-world domains, such as weather prediction or financial risk analysis. Two characteristics of real-world domains are inadequately addressed by current machine learning research. First, the difficulty in these domains is often caused by a low-level representation, which necessitates shifting to a higher-level representation. But the space of possible representations is very large, so we need intelligent methods for finding higher-level representations. Second, background knowledge is almost always available in real-world domains, which we would like to take advantage of to increase predictive accuracy. However, known roles for domain knowledge in machine learning are often inflexible, requiring the use of a specific induction algorithm or being sensitive to incorrectness or incompleteness in the knowledge.We propose a general framework for change-of-representation based on searching for alternative representations to improve the accuracy of an underlying induction algorithm. Representations are selected as candidates by querying a strategy component, which relies on domain knowledge to suggest which alternatives to search. An evaluation component then compares these representations by applying each representation to a set of examples and running the induction algorithm on the transformed examples to empirically determine the effect of the change on accuracy. This approach provides solutions to the two characteristic problems of learning in real-world domains. First, domain knowledge is used as a heuristic to guide the search for alternative representations, enabling more intelligent decisions during change-of-representation. Second, the framework provides a flexible role for knowledge that can be used with any learning algorithm and is tolerant of uncertainty. An implementation of this framework could be used as an interface between a human expert and a learning program in which: (1) the human uses background knowledge to generate and prioritize alternative representations, and (2) the system empirically evaluates these to discover the best change for improving accuracy.We apply our framework for change-of-representation to the difficult, real-world domain of protein tertiary (3D) structure prediction. The best computational method to date for determining the structure of a protein from its amino acid sequence is homology modeling, which is based on sequence alignments with a protein database. Homology modeling can fail in cases where the sequence similarity is low between proteins with similar structures. However, the physical and chemical properties of amino acids are believed to relevant to protein structure. Using an instantiation of our framework, we incorporate this domain knowledge to suggest ways to change the representation of amino acid sequences. Efficient search procedures are derived from the knowledge that lead to the discovery of representations that improve the ability to predict protein structures by homology modeling.U of I OnlyETDs are only available to UIUC Users without author permissio",,"Change of representation in machine learning, and an application to protein structure prediction",,,,core
479018341,2021-05-01T07:00:00,"Advances in artificial intelligence and machine learning have begun a revolution in the understanding and analysis of data across nearly every industry. AI and ML methods (particularly deep neural models) have been successfully scaled to fit the massive datasets available today, especially in image- and text-based tasks. However, in many settings, the application of these advanced methods is held back by underlying data issues that hamstring the models’ generalization performance.
In this work, two such challenges have been considered. The first is data-dependent uncertainty in ground-truth labels. This uncertainty can arise from ambiguity in the labeling process - e.g., whether a certain song should be labeled ‘folk’ or ‘country’ may be answered differently by different annotators - or low data quality that induces annotation mistakes. In this work, a new neighborhood-based scoring system is introduced to identify the data examples that may have suspect labels. A means for translating those uncertainty scores to sample weights is then provided so that the influence of label mistakes on a model’s decision boundary can be reduced.
The second challenge is uneven generalization performance across individuals leading to unfair deployed models. When a deep neural network is deployed to predict data from unseen users, some of those new users could experience poor performance if their data is not typical of that in the training set. To improve model fairness over individuals in a deep learning setting, we use mode connectivity, a technique from the study of neural network loss landscapes, to explore the region around a trained network in parameter space to identify a feasible set of weight configurations with similar overall performance but different distributions of performance over individuals. Multi-objective optimization over that feasible set can then be used to select the best model by observed fairness, a process we call Fairness Maximization via Mode Connectivity (FMMC).
These methods have been validated in real-world settings on time-distributed data, including two human activity recognition datasets and a music genre classification task. Our fairness approach is further validated on a Tamil handwriting classification dataset. Each is shown to surpass the performance of current baseline approaches",ScholarWorks at UMass Boston,Effects of Real-World Data Challenges on Generalization in Applied Machine Learning and Time Series Modeling,,,,core
402893381,2021-04-01T11:09:54,"Unsupervised representation learning on mixed data is highly challenging but rarely explored. It has to tackle significant challenges related to common issues in real-life mixed data, including sparsity, dynamics and heterogeneity of attributes and values. This work introduces an effective and efficient unsupervised deep representer called Mix2Vec to automatically learn a universal representation of dynamic mixed data with the above complex characteristics. Mix2Vec is empowered with three effective mechanisms: random shuffling prediction, prior distribution matching, and structural informativeness maximization, to tackle the aforementioned challenges. These mechanisms are implemented as an unsupervised deep neural representer Mix2Vec. Mix2Vec converts complex mixed data into vector space-based representations that are universal and comparable to all data objects and transparent and reusable for both unsupervised and supervised learning tasks. Extensive experiments on four large mixed datasets demonstrate that Mix2Vec performs significantly better than state-of-the-art deep representation methods. We also empirically verify the designed mechanisms in terms of representation quality, visualization and capability of enabling better performance of downstream tasks",'Institute of Electrical and Electronics Engineers (IEEE)',Mix2Vec: Unsupervised mixed data representation,http://hdl.handle.net/10453/147804,10.1109/DSAA49011.2020.00024,,core
422242006,2021-05-04T07:00:00,"Classification and segmentation of objects using machine learning algorithms have been widely used in a large variety of scientific domains in the past few decades. With the exponential growth in the number of ground-based, air-borne, and space-borne observatories, Heliophysics has been taking full advantage of such algorithms in many automated tasks, and obtained valuable knowledge by detecting solar events and analyzing the big-picture patterns. Despite the fact that in many cases, the strengths of the general-purpose algorithms seem to be transferable to problems of scientific domains where scientific events are of interest, in practice there are some critical issues which I address in this dissertation. First, I discuss the four main categories of such issues and then in the proceeding chapters I present real-world examples and the different approaches I take for tackling them. In Chapter II, I take a classical path for classification of three solar events; Active Regions, Coronal Holes, and Quiet Suns. I optimize a set of ten image parameters and improve the classification performance by up to 36%. In Chapter III, in contrast, I utilize an automated feature extraction algorithm, i.e., a deep neural network, for detection and segmentation of another solar event, namely solar Filaments. Using an off-the-shelf algorithm, I overcome several of the issues of the existing detection module, while facing an important challenge; lack of an appropriate evaluation metric for verification of the segmentations. In Chapter IV, I introduce a novel metric to provide a more accurate verification especially for salient objects with fine structures. This metric, called Multi-Scale Intersection over Union (MIoU), is a fusion of two concepts; fractal dimension from Geometry, and Intersection over Union (IoU) which is a popular metric for segmentation verification. Through several experiments I examine the advantages of using MIoU over IoU, and I conclude this chapter by a follow-through on the segmentation results of the previously implemented filament detection module",ScholarWorks @ Georgia State University,"Machine Learning of Scientific Events: Classification, Detection, and Verification",https://core.ac.uk/download/422242006.pdf,,,core
479163725,2091-02-19T00:00:00,"The dramatic increase in the number of satellites in orbit in recent years has brought progressive interest even from companies that did not operate in sectors directly related to space. To fully exploit this area, however, years of flight heritage are required, which not all companies can afford, because of that many rely on services offered by others. Deploying a new remote sensing service, however, is not simple, especially considering that the advancing interest does not keep pace with the advancement of artificial intelligence for space applications, which although on earth have had incredible improvements in recent times, on satellites had never been used by european missions. For this reason, several companies in collaboration with the European Space Agency and the University of Pisa have created the PhiSat-1 mission, demonstrating that artificial intelligence could be used on-board even for sensors never adopted in orbit, using a training dataset composed of images from the Sentinel-2 mission. Thanks to systems like the Intel Myriad Movidius Stick 2 it has been possible to reduce the time to market of neural networks for embedded satellite systems, allowing to meet the strict requirements of size and power consumption, still obtaining good performances. The use case examined in this thesis is the application of Artificial Intelligence to filter out images that are too cloudy to be usable, in order to send on-ground only those with a cloud presence considered acceptable. Given that the neural network deployed on PhiSat-1 had great limitations on its training set, as it was not very large in size and with a very unbalanced distribution in the cloudiness of the images, it has been used an architecture of Generative Adversarial Network partly customized to produce artificial images to join the real ones during a new training of the classifier network employed on PhiSat-1, both to increase the size of the dataset and to rebalance it. A custom Frechet Inception Distance has been defined to assess the quality of the produced samples. Several scenarios were defined to test in which situation the artificial images were useful. In particular, there was a statistical increase in the ability of the network to detect cloudy images when artificial samples were used to rebalance the cloudiness of the training set",'Pisa University Press',Exploiting Generative Adversarial Networks for data augmentation: the European Space Agency PhiSat-1 mission case study,,,,core
479313872,2021-07-01T00:00:00,"Abstract Rapid advancements of artificial intelligence of things (AIoT) technology pave the way for developing a digital‐twin‐based remote interactive system for advanced robotic‐enabled industrial automation and virtual shopping. The embedded multifunctional perception system is urged for better interaction and user experience. To realize such a system, a smart soft robotic manipulator is presented that consists of a triboelectric nanogenerator tactile (T‐TENG) and length (L‐TENG) sensor, as well as a poly(vinylidene fluoride) (PVDF) pyroelectric temperature sensor. With the aid of machine learning (ML) for data processing, the fusion of the T‐TENG and L‐TENG sensors can realize the automatic recognition of the grasped objects with the accuracy of 97.143% for 28 different shapes of objects, while the temperature distribution can also be obtained through the pyroelectric sensor. By leveraging the IoT and artificial intelligence (AI) analytics, a digital‐twin‐based virtual shop is successfully implemented to provide the users with real‐time feedback about the details of the product. In general, by offering a more immersive experience in human–machine interactions, the proposed remote interactive system shows the great potential of being the advanced human–machine interface for the applications of the unmanned working space",'Wiley',Artificial Intelligence of Things (AIoT) Enabled Virtual Shop Applications Using Self‐Powered Sensor Enhanced Soft Robotic Manipulator,,10.1002/advs.202100230,"[{'title': 'Advanced Science', 'identifiers': ['2198-3844', 'issn:2198-3844']}]",core
479298811,2021-06-01T00:00:00,"In this study, the machine vision and artificial intelligence algorithms were used to rapidly check the degree of cooking of foods and avoid the over-cooking of foods. Using a smart induction cooker for heating, the image processing program automatically recognizes the color of the food before and after cooking. The new cooking parameters were used to identify the cooking conditions of the food when it is undercooked, cooked, and overcooked. In the research, the camera was used in combination with the software for development, and the real-time image processing technology was used to obtain the information of the color of the food, and through calculation parameters, the cooking status of the food was monitored. In the second year, using the color space conversion, a novel algorithm, and artificial intelligence, the foreground segmentation was used to separate the vegetables from the background, and the cooking ripeness, cooking unevenness, oil glossiness, and sauce absorption were calculated. The image color difference and the distribution were used to judge the cooking conditions of the food, so that the cooking system can identify whether or not to adopt partial tumbling, or to end a cooking operation. A novel artificial intelligence algorithm is used in the relative field, and the error rate can be reduced to 3%. This work will significantly help researchers working in the advanced cooking devices",'MDPI AG',A Study of Automatic Judgment of Food Color and Cooking Conditions with Artificial Intelligence Technology,,10.3390/pr9071128,"[{'title': 'Processes', 'identifiers': ['2227-9717', 'issn:2227-9717']}]",core
387306127,2021-11-29T00:00:00,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics",,Technology Readiness Levels for Machine Learning Systems,http://arxiv.org/abs/2101.03989,,,core
464903810,2021-11-10T00:00:00,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.Comment: NeurIPS 2021 Datasets and Benchmarks Track. Code:
  https://github.com/pliang279/MultiBench and Website:
  https://cmu-multicomp-lab.github.io/multibench",,MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,http://arxiv.org/abs/2107.07502,,,core
417953203,2021-04-23T00:00:00,"As megaconstellations are launched and the space sector grows, space debris pollution is posing an increasing threat to operational spacecraft. Low Earth orbit is a junkyard of dead satellites, rocket bodies, shrapnels, and other debris that travel at very high speed in an uncontrolled manner. Collisions at orbital speeds can generate fragments and potentially trigger a cascade of more collisions endangering the whole population, a scenario known since the late 1970s as the Kessler syndrome. In this work we present Kessler: an open-source Python package for machine learning (ML) applied to collision avoidance. Kessler provides functionalities to import and export conjunction data messages (CDMs) in their standard format and predict the evolution of conjunction events based on explainable ML models. In Kessler we provide Bayesian recurrent neural networks that can be trained with existing collections of CDM data and then deployed in order to predict the contents of future CDMs in a given conjunction event, conditioned on all CDMs received up to now, with associated uncertainty estimates about all predictions. Furthermore Kessler includes a novel generative model of conjunction events and CDM sequences implemented using probabilistic programming, simulating the CDM generation process of the Combined Space Operations Center (CSpOC). The model allows Bayesian inference and also the generation of large datasets of realistic synthetic CDMs that we believe will be pivotal to enable further ML approaches given the sensitive nature and public unavailability of real CDM data",,Kessler : A machine learning library for spacecraft collision avoidance,,,,core
490561694,2021-10-01T00:00:00,"Scalable and sustainable AI-driven analytics are necessary to enable large-scale and heterogeneous service deployment in sixth-generation (6G) ultra-dense networks. This implies that the exchange of raw monitoring data should be minimized across the network by bringing the analysis functions closer to the data collection points. While federated learning (FL) is an efficient tool to implement such a decentralized strategy, real networks are generally characterized by time- and space-varying traffic patterns and channel conditions, making thereby the data collected in different points non independent and identically distributed (non-IID), which is challenging for FL. To sidestep this issue, we first introduce a new a priori metric that we call dataset entropy, whose role is to capture the distribution, the quantity of information, the unbalanced structure and the “non-IIDness” of a dataset independently of the models. This a priori entropy is calculated using a multi-dimensional spectral clustering scheme over both the features and the supervised output spaces, and is suitable for classification as well as regression tasks. The FL aggregation operations support system (OSS) server then uses the reported dataset entropies to devise 1) an entropy-based federated averaging scheme, and 2) a stochastic participant selection policy to significantly stabilize the training, minimize the convergence time, and reduce the corresponding computation cost. Numerical results are provided to show the superiority of these novel approaches",'Frontiers Media SA',Entropy-Driven Stochastic Federated Learning in Non-IID 6G Edge-RAN,https://core.ac.uk/download/490561694.pdf,10.3389/frcmn.2021.739414,"[{'title': None, 'identifiers': ['issn:2673-530X', '2673-530x']}]",core
14695018,2048-02-27T00:00:00,"La presente tesi propone un metodo sistematico per l'ottimizzazione di impronte di contatto in ingranaggi ipoidi e spiroconici: rispetto alla condizione non ottimizzata, si determinano le variazioni dei parametri macchina del pignone necessarie per ottenere un'impronta di contatto la cui distanza, misurata attraverso un'opportuna metrica, è minima rispetto a quella ottimale.

Queste sono le fasi in cui si sviluppa la procedura.
• Determinare le superfici B-Spline che interpolano i punti campionati sui denti di una ruota, ottenuti applicando l'approccio invariante della teoria dell'ingranamento al metodo face milling della Gleason®. La superficie del pignone è generata sovrapponendo alla superficie di base quella di spoglia, che viene modificata durante l'ottimizzazione.
• Effettuare l'analisi del contatto rigido (TCA) e quella del contatto esteso simulato (Simulated LTCA); quest'ultima consiste nell'approssimare l'impronta di contatto sotto carico come l'intersezione tra la superficie del pignone e quella della corona con sovrapposto uno strato di pasta.
• Individuare la superficie di spoglia ottima che minimizza la distanza tra l'impronta di contatto target e quella attuale attraverso il metodo di Nelder-Mead.
• Determinare i settaggi macchina che minimizzano la distanza tra la superficie di spoglia obiettivo e quella attuale, utilizzando un metodo non lineare ai minimi quadrati, quale l'algoritmo di Levenberg-Marquardt con strategia trust-region.

Il metodo è stato implementato in C++ e validato testando alcune coppie impiegate in campo aerospaziale; sono riportati la struttura del codice con i file di input e i risultati ottenuti. Sono inoltre stati discussi lo stato dell'arte, i possibili sviluppi futuri di questo lavoro e i riferimenti alla normativa vigente.

------------------------------------------------

This thesis work presents a systematic and robust methodology to optimize tooth contact patterns in hypoid and spiral bevel gears. The main goals are: (1) to find the shape of the ease off surface that minimizes the distance between the current contact pattern and the target one; (2) to identify the machine setting corrections required to obtain the previously identified ease off surface.

The proposed method consists in the following steps.
• The surfaces of the teeth, generated with the face milling process (Gleason®), are modeled using the invariant approach and then interpolated by B-Spline surfaces (the pinion surface is considered as the superposition of the base surface and the ease off surface along the local normals).
• The Tooth Contact Analysis and the Simulated Loaded Tooth Contact Analysis are carried out; in the latter, a marking compound layer is applied on the gear surface and the portion of the compound removed during the simulation of meshing is taken as a valid approximation of the tooth contact pattern.
• The best ease off minimizing the distance between the current contact pattern and the target one is identified employing the Nelder-Mead algorithm.
• The feasible setting machine variations, that allow to achieve the prescribed ease off surface, are detected employing the Levenberg-Marquardt algorithm (a nonlinear least square method) with a trust region strategy.

The method was implemented in C++ language and tested in the optimization of the contact pattern properties of real aerospace drives. The structure of the program along with the input files and the results obtained are also presented. The state of art, some possible future developments with reference to the current standards are discussed.<br",'Pisa University Press',Ottimizzazione sistematica di impronte di contatto in ingranaggi spiroconici,https://core.ac.uk/download/14695018.pdf,,,core
395143868,2021-02-01T00:00:00,"[EN] Wetlands play a key role in preserving biodiversity and preventing climate change. Their conservation poses an important and pressing challenge. In the Mediterranean region, one of the key threats to wetland survival is the lack of water due to competition for resources. The selection of the most sustainable water resources for wetland conservation is a complex elicitation problem. A novel Water Resources Sustainability Model (WRSM) focused on water quality has been developed to support the decision-making. This collaborative elicitation model is based on the analytical hierarchy process and uses the reference environmental status of the wetland. The model can be used to discriminate which water resources are more sustainable for the conservation of the wetland. The WRSM has been applied successfully to Las Tablas de Daimiel National Park. The framework enables establishing priorities when analyzing in terms of water quality any surface, recycled or underground water resources.Canto-Perello, J.; Benitez-Navio, A.; Martín Utrillas, MG.; Martinez-Leon, J.; Curiel Esparza, J. (2021). Water resources sustainability model for wetland conservation based on anonymous expert elicitation. Environmental Modelling & Software. 136:1-12. https://doi.org/10.1016/j.envsoft.2020.104952S112136Aguilera, H., & Merino, L. M. (2018). Data on chemical composition of soil and water in the semiarid wetland of Las Tablas de Damiel National Park (Spain) during a drought period. Data in Brief, 19, 2481-2486. doi:10.1016/j.dib.2018.04.085Aguilera, H., Moreno, L., Wesseling, J. G., Jiménez-Hernández, M. E., & Castaño, S. (2016). Soil moisture prediction to support management in semiarid wetlands during drying episodes. CATENA, 147, 709-724. doi:10.1016/j.catena.2016.08.007Alafifi, A. H., & Rosenberg, D. E. (2020). Systems modeling to improve river, riparian, and wetland habitat quality and area. Environmental Modelling & Software, 126, 104643. doi:10.1016/j.envsoft.2020.104643Alvarez Etxeberria, I., Garayar, A., & Calvo Sánchez, J. A. (2015). Development of sustainability reports for farming operations in the Basque Country using the Delphi method. Revista de Contabilidad, 18(1), 44-54. doi:10.1016/j.rcsar.2014.03.004Bilotta, G. S., & Brazier, R. E. (2008). Understanding the influence of suspended solids on water quality and aquatic biota. Water Research, 42(12), 2849-2861. doi:10.1016/j.watres.2008.03.018Bilotta, G. S., Burnside, N. G., Cheek, L., Dunbar, M. J., Grove, M. K., Harrison, C., … Davy-Bowker, J. (2012). Developing environment-specific water quality guidelines for suspended particulate matter. Water Research, 46(7), 2324-2332. doi:10.1016/j.watres.2012.01.055Blaas, H., & Kroeze, C. (2016). Excessive nitrogen and phosphorus in European rivers: 2000–2050. Ecological Indicators, 67, 328-337. doi:10.1016/j.ecolind.2016.03.004Caen, A., Latour, D., & Mathias, J. D. (2019). Dynamical effects of retention structures on the mitigation of lake eutrophication. Environmental Modelling & Software, 119, 309-326. doi:10.1016/j.envsoft.2019.06.012Camargo, J. A., & Alonso, Á. (2006). Ecological and toxicological effects of inorganic nitrogen pollution in aquatic ecosystems: A global assessment. Environment International, 32(6), 831-849. doi:10.1016/j.envint.2006.05.002Canto-Perello, J., Martinez-Leon, J., Curiel-Esparza, J., & Martin-Utrillas, M. (2017). Consensus in prioritizing river rehabilitation projects through the integration of social, economic and landscape indicators. Ecological Indicators, 72, 659-666. doi:10.1016/j.ecolind.2016.09.004Canto-Perello, J., Morera-Escrich, J. L., Martin-Utrillas, M., & Curiel-Esparza, J. (2018). Restoration prioritization framework for roadway high cut slopes to reverse land degradation and fragmentation. Land Use Policy, 71, 470-479. doi:10.1016/j.landusepol.2017.11.020Cirujano, S., Casado, C., Bernués, M., & Camargo, J. A. (1996). Ecological study of Las Tablas de Daimiel National Park (Ciudad Real, central Spain): Differences in water physico-chemistry and vegetation between 1974 and 1989. Biological Conservation, 75(3), 211-215. doi:10.1016/0006-3207(95)00079-8Curiel-Esparza, J., Gonzalez-Utrillas, N., Canto-Perello, J., & Martin-Utrillas, M. (2015). Integrating climate change criteria in reforestation projects using a hybrid decision-support system. Environmental Research Letters, 10(9), 094022. doi:10.1088/1748-9326/10/9/094022Curiel-Esparza, J., Mazario-Diez, J. L., Canto-Perello, J., & Martin-Utrillas, M. (2016). Prioritization by consensus of enhancements for sustainable mobility in urban areas. Environmental Science & Policy, 55, 248-257. doi:10.1016/j.envsci.2015.10.015Curiel-Esparza, J., Reyes-Medina, M., Martin-Utrillas, M., Martinez-Garcia, M. P., & Canto-Perello, J. (2019). Collaborative elicitation to select a sustainable biogas desulfurization technique for landfills. Journal of Cleaner Production, 212, 1334-1344. doi:10.1016/j.jclepro.2018.12.095Dong, Y., Zhang, G., Hong, W.-C., & Xu, Y. (2010). Consensus models for AHP group decision making under row geometric mean prioritization method. Decision Support Systems, 49(3), 281-289. doi:10.1016/j.dss.2010.03.003Forman, E., & Peniwati, K. (1998). Aggregating individual judgments and priorities with the analytic hierarchy process. European Journal of Operational Research, 108(1), 165-169. doi:10.1016/s0377-2217(97)00244-0Gu, S., Gruau, G., Dupas, R., Petitjean, P., Li, Q., & Pinay, G. (2019). Respective roles of Fe-oxyhydroxide dissolution, pH changes and sediment inputs in dissolved phosphorus release from wetland soils under anoxic conditions. Geoderma, 338, 365-374. doi:10.1016/j.geoderma.2018.12.034Haas, M. B., Guse, B., & Fohrer, N. (2017). Assessing the impacts of Best Management Practices on nitrate pollution in an agricultural dominated lowland catchment considering environmental protection versus economic development. Journal of Environmental Management, 196, 347-364. doi:10.1016/j.jenvman.2017.02.060Thi Minh Hanh, P., Sthiannopkao, S., The Ba, D., & Kim, K.-W. (2011). Development of Water Quality Indexes to Identify Pollutants in Vietnam’s Surface Water. Journal of Environmental Engineering, 137(4), 273-283. doi:10.1061/(asce)ee.1943-7870.0000314Hes, E. M., & van Dam, A. A. (2019). Modelling nitrogen and phosphorus cycling and retention in Cyperus papyrus dominated natural wetlands. Environmental Modelling & Software, 122, 104531. doi:10.1016/j.envsoft.2019.104531Juston, J. M., & Kadlec, R. H. (2019). Data-driven modeling of phosphorus (P) dynamics in low-P stormwater wetlands. Environmental Modelling & Software, 118, 226-240. doi:10.1016/j.envsoft.2019.05.002Juwana, I., Muttil, N., & Perera, B. J. C. (2012). Indicator-based water sustainability assessment — A review. Science of The Total Environment, 438, 357-371. doi:10.1016/j.scitotenv.2012.08.093Kløve, B., Allan, A., Bertrand, G., Druzynska, E., Ertürk, A., Goldscheider, N., … Schipper, P. (2011). Groundwater dependent ecosystems. Part II. Ecosystem services and management in Europe under risk of climate change and land use intensification. Environmental Science & Policy, 14(7), 782-793. doi:10.1016/j.envsci.2011.04.005Koskiaho, J., & Puustinen, M. (2019). Suspended solids and nutrient retention in two constructed wetlands as determined from continuous data recorded with sensors. Ecological Engineering, 137, 65-75. doi:10.1016/j.ecoleng.2019.04.006Lefebvre, G., Redmond, L., Germain, C., Palazzi, E., Terzago, S., Willm, L., & Poulin, B. (2019). Predicting the vulnerability of seasonally-flooded wetlands to climate change across the Mediterranean Basin. Science of The Total Environment, 692, 546-555. doi:10.1016/j.scitotenv.2019.07.263Zhuang, L.-L., Yang, T., Zhang, J., & Li, X. (2019). The configuration, purification effect and mechanism of intensified constructed wetland for wastewater treatment from the aspect of nitrogen removal: A review. Bioresource Technology, 293, 122086. doi:10.1016/j.biortech.2019.122086Liu, Z., Tai, P., Li, X., Kong, L., Matthews, T. G., Lester, R. E., & Mondon, J. A. (2019). Deriving site-specific water quality criteria for ammonia from national versus international toxicity data. Ecotoxicology and Environmental Safety, 171, 665-676. doi:10.1016/j.ecoenv.2018.12.078Lobanova, A., Liersch, S., Tàbara, J. D., Koch, H., Hattermann, F. F., & Krysanova, V. (2017). Harmonizing human-hydrological system under climate change: A scenario-based approach for the case of the headwaters of the Tagus River. Journal of Hydrology, 548, 436-447. doi:10.1016/j.jhydrol.2017.03.015Martin-Utrillas, M., Reyes-Medina, M., Curiel-Esparza, J., & Canto-Perello, J. (2014). Hybrid method for selection of the optimal process of leachate treatment in waste treatment and valorization plants or landfills. Clean Technologies and Environmental Policy, 17(4), 873-885. doi:10.1007/s10098-014-0834-4Man, Y., Hu, Y., & Ren, J. (2019). Forecasting COD load in municipal sewage based on ARMA and VAR algorithms. Resources, Conservation and Recycling, 144, 56-64. doi:10.1016/j.resconrec.2019.01.030Martinez-Martinez, E., Nejadhashemi, A. P., Woznicki, S. A., Adhikari, U., & Giri, S. (2015). Assessing the significance of wetland restoration scenarios on sediment mitigation plan. Ecological Engineering, 77, 103-113. doi:10.1016/j.ecoleng.2014.11.031Mayo, A. W., Muraza, M., & Norbert, J. (2018). Modelling nitrogen transformation and removal in mara river basin wetlands upstream of lake Victoria. Physics and Chemistry of the Earth, Parts A/B/C, 105, 136-146. doi:10.1016/j.pce.2018.03.005Moreno, L., Jiménez, M.-E., Aguilera, H., Jiménez, P., & de la Losa, A. (2010). The 2009 Smouldering Peat Fire in Las Tablas de Daimiel National Park (Spain). Fire Technology, 47(2), 519-538. doi:10.1007/s10694-010-0172-yNagisetty, R. M., Flynn, K. F., & Uecker, D. (2019). Dissolved oxygen modeling of effluent-dominated macrophyte-rich Silver Bow Creek. Ecological Modelling, 393, 85-97. doi:10.1016/j.ecolmodel.2018.12.009Navarro, V., García, B., Sánchez, D., & Asensio, L. (2011). An evaluation of the application of treated sewage effluents in Las Tablas de Daimiel National Park, Central Spain. Journal of Hydrology, 401(1-2), 53-64. doi:10.1016/j.jhydrol.2011.02.008Norouzian-Maleki, S., Bell, S., Hosseini, S.-B., & Faizi, M. (2015). Developing and testing a framework for the assessment of neighbourhood liveability in two contrasting countries: Iran and Estonia. Ecological Indicators, 48, 263-271. doi:10.1016/j.ecolind.2014.07.033Novakowski, N., & Wellar, B. (2008). Using the Delphi Technique in Normative Planning Research: Methodological Design Considerations. Environment and Planning A: Economy and Space, 40(6), 1485-1500. doi:10.1068/a39267Okoli, C., & Pawlowski, S. D. (2004). The Delphi method as a research tool: an example, design considerations and applications. Information & Management, 42(1), 15-29. doi:10.1016/j.im.2003.11.002O’Neil, G. L., Goodall, J. L., Behl, M., & Saby, L. (2020). Deep learning Using Physically-Informed Input Data for Wetland Identification. Environmental Modelling & Software, 126, 104665. doi:10.1016/j.envsoft.2020.104665Pérez-Martín, M. A., Estrela, T., & del-Amo, P. (2016). Measures required to reach the nitrate objectives in groundwater based on a long-term nitrate model for large river basins (Júcar, Spain). Science of The Total Environment, 566-567, 122-133. doi:10.1016/j.scitotenv.2016.04.206Pottinger, T. G. (2017). Modulation of the stress response in wild fish is associated with variation in dissolved nitrate and nitrite. Environmental Pollution, 225, 550-558. doi:10.1016/j.envpol.2017.03.021Prăvălie, R., Patriche, C., & Bandoc, G. (2017). Quantification of land degradation sensitivity areas in Southern and Central Southeastern Europe. New results based on improving DISMED methodology with new climate data. CATENA, 158, 309-320. doi:10.1016/j.catena.2017.07.006Restuccia, F., Huang, X., & Rein, G. (2017). Self-ignition of natural fuels: Can wildfires of carbon-rich soil start by self-heating? Fire Safety Journal, 91, 828-834. doi:10.1016/j.firesaf.2017.03.052Rivers-Moore, N. A., Dallas, H. F., & Morris, C. (2013). Towards setting environmental water temperature guidelines: A South African example. Journal of Environmental Management, 128, 380-392. doi:10.1016/j.jenvman.2013.04.059Rusydi, A. F. (2018). Correlation between conductivity and total dissolved solid in various type of water: A review. IOP Conference Series: Earth and Environmental Science, 118, 012019. doi:10.1088/1755-1315/118/1/012019Sánchez-Montoya, M. del M., Arce, M. I., Vidal-Abarca, M. R., Suárez, M. L., Prat, N., & Gómez, R. (2012). Establishing physico-chemical reference conditions in Mediterranean streams according to the European Water Framework Directive. Water Research, 46(7), 2257-2269. doi:10.1016/j.watres.2012.01.042Sanchez-Ramos, D., Sánchez-Emeterio, G., & Florín Beltrán, M. (2015). Changes in water quality of treated sewage effluents by their receiving environments in Tablas de Daimiel National Park, Spain. Environmental Science and Pollution Research, 23(7), 6082-6090. doi:10.1007/s11356-015-4660-ySapriza-Azuri, G., Jódar, J., Carrera, J., & Gupta, H. V. (2015). Toward a comprehensive assessment of the combined impacts of climate change and groundwater pumping on catchment dynamics. Journal of Hydrology, 529, 1701-1712. doi:10.1016/j.jhydrol.2015.08.015Singh, S., Ghosh, N. C., Krishan, G., Galkate, R., Thomas, T., & Jaiswal, R. K. (2015). Development of an Overall Water Quality Index (OWQI) for Surface Water in Indian Context. Current World Environment, 10(3), 813-822. doi:10.12944/cwe.10.3.12Singh, S., Ghosh, N. C., Gurjar, S., Krishan, G., Kumar, S., & Berwal, P. (2017). Index-based assessment of suitability of water quality for irrigation purpose under Indian conditions. Environmental Monitoring and Assessment, 190(1). doi:10.1007/s10661-017-6407-3Sperotto, A., Molina, J. L., Torresan, S., Critto, A., Pulido-Velazquez, M., & Marcomini, A. (2019). A Bayesian Networks approach for the assessment of climate change impacts on nutrients loading. Environmental Science & Policy, 100, 21-36. doi:10.1016/j.envsci.2019.06.004Sun, B., Tang, J., Yu, D., Song, Z., & Wang, P. (2019). Ecosystem health assessment: A PSR analysis combining AHP and FCE methods for Jiaozhou Bay, China1. Ocean & Coastal Management, 168, 41-50. doi:10.1016/j.ocecoaman.2018.10.026Sutadian, A. D., Muttil, N., Yilmaz, A. G., & Perera, B. J. C. (2015). Development of river water quality indices—a review. Environmental Monitoring and Assessment, 188(1). doi:10.1007/s10661-015-5050-0Sutadian, A. D., Muttil, N., Yilmaz, A. G., & Perera, B. J. C. (2017). Using the Analytic Hierarchy Process to identify parameter weights for developing a water quality index. Ecological Indicators, 75, 220-233. doi:10.1016/j.ecolind.2016.12.043Tooth, S. (2018). The geomorphology of wetlands in drylands: Resilience, nonresilience, or …? Geomorphology, 305, 33-48. doi:10.1016/j.geomorph.2017.10.017Tyagi, S., Sharma, B., Singh, P., & Dobhal, R. (2020). Water Quality Assessment in Terms of Water Quality Index. American Journal of Water Resources, 1(3), 34-38. doi:10.12691/ajwr-1-3-3Viaroli, S., Mastrorillo, L., Lotti, F., Paolucci, V., & Mazza, R. (2018). The groundwater budget: A tool for preliminary estimation of the hydraulic connection between neighboring aquifers. Journal of Hydrology, 556, 72-86. doi:10.1016/j.jhydrol.2017.10.066Wang, H.-J., Xiao, X.-C., Wang, H.-Z., Li, Y., Yu, Q., Liang, X.-M., … Jeppesen, E. (2017). Effects of high ammonia concentrations on three cyprinid fish: Acute and whole-ecosystem chronic tests. Science of The Total Environment, 598, 900-909. doi:10.1016/j.scitotenv.2017.04.070Xu, Y., Wang, Y., Li, S., Huang, G., & Dai, C. (2018). Stochastic optimization model for water allocation on a watershed scale considering wetland’s ecological water requirement. Ecological Indicators, 92, 330-341. doi:10.1016/j.ecolind.2017.02.019Yuan, L., Ge, Z., Fan, X., & Zhang, L. (2014). Ecosystem-based coastal zone management: A comprehensive assessment of coastal ecosystems in the Yangtze Estuary coastal zone. Ocean & Coastal Management, 95, 63-71. doi:10.1016/j.ocecoaman.2014.04.005Zhang, R., Zhang, X., Yang, J., & Yuan, H. (2013). Wetland ecosystem stability evaluation by using Analytical Hierarchy Process (AHP) approach in Yinchuan Plain, China. Mathematical and Computer Modelling, 57(3-4), 366-374. doi:10.1016/j.mcm.2012.06.014ZHANG, L. (2016). CALCULATION OF WETLANDS ECOLOGICAL WATER REQUIREMENT IN CHINA’S WESTERN JILIN PROVINCE BASED ON REGIONALIZATION AND GRADATION TECHNIQUES. Applied Ecology and Environmental Research, 14(3), 463-478. doi:10.15666/aeer/1403_463478Zhang, B., Zhao, D., Zhou, P., Qu, S., Liao, F., & Wang, G. (2020). Hydrochemical Characteristics of Groundwater and Dominant Water–Rock Interactions in the Delingha Area, Qaidam Basin, Northwest China. Water, 12(3), 836. doi:10.3390/w1203083",'Elsevier BV',Water resources sustainability model for wetland conservation based on anonymous expert elicitation,http://hdl.handle.net/10251/164063,10.1016/j.envsoft.2020.104952,,core
491191609,2021-06-15T00:00:00,"In this paper we present the recent developments in the AI-terity instrument. AI-terity is a deformable, non-rigid musical instrument that comprises a particular artificial intelligence (AI) method for generating audio samples for real-time audio synthesis. As an improvement, we developed the control interface structure with additional sensor hardware. In addition, we implemented a new hybrid deep learning architecture, GANSpaceSynth, in which we applied the GANSpace method on the GANSynth model. Following the deep learning model improvement, we developed new autonomous features for the instrument that aim at keeping the musician in an active and uncertain state of exploration. Through these new features, the instrument enables more accurate control on GAN latent space. Further, we intend to investigate the current developments through a musical composition that idiomatically reflects the new autonomous features of the AI-terity instrument. We argue that the present technology of AI is suitable for enabling alternative autonomous features in audio domain for the creative practices of musicians.Peer reviewe",'PubPub',AI-terity 2.0: An Autonomous NIME Featuring GANSpaceSynth Deep Learning Model,,10.21428/92fbeb44.3d0e9e12,,core
440717057,2021-01-01T00:00:00,"In this paper, we track the motion of multiple targets in sports videos by a machine learning algorithm and study its tracking technique in depth. In terms of moving target detection, the traditional detection algorithms are analysed theoretically as well as implemented algorithmically, based on which a fusion algorithm of four interframe difference method and background averaging method is proposed for the shortcomings of interframe difference method and background difference method. The fusion algorithm uses the learning rate to update the background in real time and combines morphological processing to correct the foreground, which can effectively cope with the slow change of the background. According to the requirements of real time, accuracy, and occupying less video memory space in intelligent video surveillance systems, this paper improves the streamlined version of the algorithm. The experimental results show that the improved multitarget tracking algorithm effectively improves the Kalman filter-based algorithm to meet the real-time and accuracy requirements in intelligent video surveillance scenarios",'Hindawi Limited',Machine Learning-Based Multitarget Tracking of Motion in Sports Video,,10.1155/2021/5533884,"[{'title': 'Complexity', 'identifiers': ['issn:1099-0526', 'issn:1076-2787', '1076-2787', '1099-0526']}]",core
395145102,2061-03-26T00:00:00,"This thesis assesses the applicability of a machine learning approach to solve the Electrical Resistivity Tomography (ERT). The ERT is a non-linear and ill-conditioned problem usually solved through gradient-based methods. These approaches ensure a rapid convergence toward a best-fitting model but suffer from the local linearization, and hence are prone to get trapped into local minima of the error function. Here we implement and apply an alternative approach in which a convolutional neural network is trained to learn the non-linear mapping between the pseudo resistivity (data) domain and the resistivity space. The computational cost of the training procedure is highly dependent on the dimensionality of the input and output of the network and for this reason, we also assess the applicability of the Discrete Cosine Transform (DCT) (an orthogonal transformation) to compress the number of data points and model parameters. The aim of the DCT is twofold: not only it reduces the dimensionality of the input and output of the network, but it also acts as a regularization operator in the model space that mitigates the ill-conditioning of the ERT inversion problem, while preserving the assumed spatial continuity pattern in the recovered solution. In addition to standard CNNs, we also apply a Residual Neural Network (RNN), a special kind of CNN that uses skip connections to avoid the so-called vanishing gradient problem that arises when training a deep network.
The implemented machine-learning ERT inversion includes four steps:
1) Generation phase: define an ensemble of 2D resistivity models and compute the associated pseudosections, which constitute the network output and input responses, respectively. The models are created according to a previously defined prior distribution and spatial variability pattern, while a finite-elements code constitutes the forward modeling engine.
2) Network Design: define a network architecture to approximate the non-linear mapping between the data and the model spaces.
3) Training phase: train the network by minimizing the differences between the predicted and desired output.
4) Prediction phase: once trained, use the network to project an observed pseudosection onto the associated resistivity values.
For what concerns the network design we analyze how different hyperparameter settings (i.e., number of layers, size of the convolutional filters, number of filters) affect the network performances that are evaluated on the training, validation, and test sets. We also assess the robustness and stability of the network predictions in case of erroneous assumptions about the noise and prior model statistics assumed during the learning stage. In this regard, we also demonstrate that transfer learning avoids retraining the network from scratch when the trained convolutional neural network is applied to a different scenario (i.e., a test model with different statistical properties). This technique is routinely employed in machine learning applications and consists of an additional training process with a small portion of target data, thereby allowing for a quick transfer of the learned features to a new task.
The implemented machine learning approaches for ERT inversion are first tested on synthetic data and then applied to real data measured along a river embankment. Our tests confirm the suitability of the proposed approach, opening the possibility to estimate the subsurface resistivity values in near real-time",'Pisa University Press',Applications of convolutional and residual neural networks to solve the electrical resistivity tomography inversion,,,,core
490990485,2021-12-08T00:00:00,"Mobile networks (MN) are anticipated to provide unprecedented opportunities
to enable a new world of connected experiences and radically shift the way
people interact with everything. MN are becoming more and more complex, driven
by ever-increasingly complicated configuration issues and blossoming new
service requirements. This complexity poses significant challenges in
deployment, management, operation, optimization, and maintenance, since they
require a complete understanding and cognition of MN. Artificial intelligence
(AI), which deals with the simulation of intelligent behavior in computers, has
demonstrated enormous success in many application domains, suggesting its
potential in cognizing the state of MN and making intelligent decisions. In
this paper, we first propose an AI-powered mobile network architecture and
discuss challenges in terms of cognition complexity, decisions with
high-dimensional action space, and self-adaption to system dynamics. Then,
potential solutions that are associated with AI are discussed. Finally, we
propose a deep learning approach that directly maps the state of MN to
perceived QoS, integrating cognition with the decision. Our proposed approach
helps operators in making more intelligent decisions to guarantee QoS.
Meanwhile, the effectiveness and advantages of our proposed approach are
demonstrated on a real-world dataset, involving $31261$ users over $77$
stations within $5$ days",,"Artificial Intelligence Powered Mobile Networks: From Cognition to
  Decision",http://arxiv.org/abs/2112.04263,,,core
479632752,2021-07-01T00:00:00,"The properties of the human eye retina, including space-variant resolution and gaze characters, provide many advantages for numerous applications that simultaneously require a large field of view, high resolution, and real-time performance. Therefore, retina-like mechanisms and sensors have received considerable attention in recent years. This paper provides a review of state-of-the-art retina-like imaging techniques and applications. First, we introduce the principle and implementing methods, including software and hardware, and describe the comparisons between them. Then, we present typical applications combined with retina-like imaging, including three-dimensional acquisition and reconstruction, target tracking, deep learning, and ghost imaging. Finally, the challenges and outlook are discussed to further study for practical use. The results are beneficial for better understanding retina-like imaging",'MDPI AG',Retina-like Imaging and Its Applications: A Brief Review,,10.3390/app11157058,"[{'title': 'Applied Sciences', 'identifiers': ['2076-3417', 'issn:2076-3417']}]",core
479996476,2021-05-01T00:00:00,"In recent years, we have assisted with an impressive advance in augmented reality systems and computer vision algorithms, based on image processing and artificial intelligence. Thanks to these technologies, mainstream smartphones are able to estimate their own motion in 3D space with high accuracy. In this paper, we exploit such technologies to support the autonomous mobility of people with visual disabilities, identifying pre-defined virtual paths and providing context information, reducing the distance between the digital and real worlds. In particular, we present ARIANNA+, an extension of ARIANNA, a system explicitly designed for visually impaired people for indoor and outdoor localization and navigation. While ARIANNA is based on the assumption that landmarks, such as QR codes, and physical paths (composed of colored tapes, painted lines, or tactile pavings) are deployed in the environment and recognized by the camera of a common smartphone, ARIANNA+ eliminates the need for any physical support thanks to the ARKit library, which we exploit to build a completely virtual path. Moreover, ARIANNA+ adds the possibility for the users to have enhanced interactions with the surrounding environment, through convolutional neural networks (CNNs) trained to recognize objects or buildings and enabling the possibility of accessing contents associated with them. By using a common smartphone as a mediation instrument with the environment, ARIANNA+ leverages augmented reality and machine learning for enhancing physical accessibility. The proposed system allows visually impaired people to easily navigate in indoor and outdoor scenarios simply by loading a previously recorded virtual path and providing automatic guidance along the route, through haptic, speech, and sound feedback",'MDPI AG',A Navigation and Augmented Reality System for Visually Impaired People,https://core.ac.uk/download/479996476.pdf,10.3390/s21093061,,core
481692910,2021-01-01T00:00:00,"Mobile Virtual Reality inspired by Google Cardboard (where a smartphone is placed in a cheap headset with no electronics) has lost its popularity after standalone VR headsets appeared on the market. The devices like Oculus Quest support full tracking of the real-world position and rotation of the headset and a pair of controllers (or even just the user's hands), all without any external sensors or computers, while Cardboard-like applications only track the rotation of the user's head. We tried to create a framework that could provide a similar experience to the standalone headsets - a mobile VR application capable of tracking itself and the user's hands in 6 degrees of freedom, all while only requiring a smartphone with a few cheap additions. We used Unity AR Foundation to achieve headset tracking and a variety of OpenCV algorithms to solve hand tracking - ArUco markers, Color Thresholding, Camshift, and deep learning approaches like OpenPose and YOLOv3. Our focus was on testing the hand tracking algorithms, and we discovered that while they are far from perfect, the concept is feasible, and, with some improvements, the framework could become a real competitor in the space of standalone VR headsets.Mobilní virtuální realita inspirovaná Google Cardboard (kde je smartphone umístěn v levné náhlavní soupravě bez elektroniky) ztratila popularitu poté, co se na trhu objevily samostatné náhlavní soupravy VR. Zařízení jako Oculus Quest podporují úplné sledování skutečné polohy a rotace náhlavní soupravy a dvojice ovladačů (nebo dokonce jen rukou uživatele), to vše bez externích senzorů nebo počítačů, zatímco aplikace podobné kartonu sledují pouze rotaci hlavy uživatele. Pokusili jsme se vytvořit rámec, který by mohl poskytnout podobný zážitek jako samostatné náhlavní soupravy - mobilní VR aplikace schopná sledovat sebe a ruce uživatele v 6 stupních volnosti, to vše vyžaduje pouze smartphone s několika levnými doplňky. Použili jsme Unity AR Foundation k dosažení sledování náhlavní soupravy a různých algoritmů OpenCV k řešení sledování rukou - značky ArUco, Color Thresholding, Camshift a přístupy hlubokého učení, jako jsou OpenPose a YOLOv3. Naše zaměření bylo na testování algoritmů pro sledování rukou a zjistili jsme, že i když zdaleka nejsou dokonalé, koncept je proveditelný a s některými vylepšeními by se rámec mohl stát skutečným konkurentem v prostoru samostatných VR sluchátek.Department of Software and Computer Science EducationKatedra softwaru a výuky informatikyMatematicko-fyzikální fakultaFaculty of Mathematics and Physic","Univerzita Karlova, Matematicko-fyzikální fakulta",Hand tracking pro mobilní virtuální realitu,,,,core
490728138,2021-10-01T00:00:00,"Benchmarking deep learning algorithms before deploying them in hardware-constrained execution environments, such as imaging satellites, is pivotal in real-life applications. Although a thorough and consistent benchmarking procedure can allow us to estimate the expected operational abilities of the underlying deep model, this topic remains under-researched. This paper tackles this issue and presents an end-to-end benchmarking approach for quantifying the abilities of deep learning algorithms in virtually any kind of on-board space applications. The experimental validation, performed over several state-of-the-art deep models and benchmark datasets, showed that different deep learning techniques may be effectively benchmarked using the standardized approach, which delivers quantifiable performance measures and is highly configurable. We believe that such benchmarking is crucial in delivering ready-to-use on-board artificial intelligence in emerging space applications and should become a standard tool in the deployment chain",'MDPI AG',Benchmarking Deep Learning for On-Board Space Applications,,10.3390/rs13193981,"[{'title': 'Remote Sensing', 'identifiers': ['2072-4292', 'issn:2072-4292']}]",core
480045276,2021-01-01T00:00:00,"A computationally efficient method for online joint state inference and dynamical model learning is presented. The dynamical model combines an a priori known, physically derived, state-space model with a radial basis function expansion representing unknown system dynamics and inherits properties from both physical and data-driven modeling. The method uses an extended Kalman filter approach to jointly estimate the state of the system and learn the unknown system dynamics, via the parameters of the basis function expansion. The key contribution is a computational complexity reduction compared to a similar approach with globally supported basis functions. By using compactly supported radial basis functions and an approximate Kalman gain, the computational complexity is considerably reduced and is essentially determined by the support of the basis functions. The approximation works well when the system dynamics exhibit limited correlation between points well separated in the state-space domain. The method is exemplified via two intelligent vehicle applications where it is shown to: (i) have competitive system dynamics estimation performance compared to the globally supported basis function method, and (ii) be real-time applicable to problems with a large-scale state-space.Funding: Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation</p",'Institute of Electrical and Electronics Engineers (IEEE)',Online Joint State Inference and Learning of Partially Unknown State-Space Models,,10.1109/TSP.2021.3095709,,core
479175222,2021-10-26T00:00:00,"To scale outlier detection (OD) to large-scale, high-dimensional datasets, we
propose TOD, a novel system that abstracts OD algorithms into basic tensor
operations for efficient GPU acceleration. To make TOD highly efficient in both
time and space, we leverage recent advances in deep learning infrastructure in
both hardware and software. To deploy large OD applications on GPUs with
limited memory, we introduce two key techniques. First, provable quantization
accelerates OD computation and reduces the memory requirement by performing
specific OD computations in lower precision while provably guaranteeing no
accuracy loss. Second, to exploit the aggregated compute resources and memory
capacity of multiple GPUs, we introduce automatic batching, which decomposes OD
computations into small batches that can be executed on multiple GPUs in
parallel.
  TOD supports a comprehensive set of OD algorithms and utility functions.
Extensive evaluation on both real and synthetic OD datasets shows that TOD is
on average 11.9X faster than the state-of-the-art comprehensive OD system PyOD,
and takes less than an hour to detect outliers within a million samples. TOD
enables straightforward integration for additional OD algorithms and provides a
unified framework for combining classical OD algorithms with deep learning
methods. These combinations result in an infinite number of OD methods, many of
which are novel and can be easily prototyped in TOD.Comment: Code available at https://github.com/yzhao062/pyto",,TOD: Tensor-based Outlier Detection,http://arxiv.org/abs/2110.14007,,,core
479058314,2021-10-25T00:00:00,"Data security is of the utmost concern of a communication system. Since the
early days, many developments have been made to improve the performance of the
system. PSNR of the received signal, secure transmission channel, quality of
encoding used, etc. are some of the key attributes of a good system. To ensure
security, the most commonly used technique is cryptography in which the message
is altered with respect to a key and using the same, the encoded message is
decoded at the receiver side. A complementary technique that is popularly used
to insure security is steganography. The advancements in Artificial
Intelligence(AI) have paved way for performing steganography in an intelligent,
tamper-proof manner. The recent discovery by researchers in the field of Deep
Learning(DL), an unsupervised learning network known as the Generative
Adversarial Networks(GAN) has improved the performance of this technique
exponentially. It has been demonstrated that deep neural networks are highly
sensitive to tiny perturbations of input data, giving rise to adversarial
examples. Though this property is usually considered a weakness of learned
models, it could be beneficial if used appropriately. The work that has been
accomplished by MIT for this purpose, a deep-neural model by the name of
SteganoGAN, has shown obligation for using this technique for steganography. In
this work, we have proposed a novel approach to improve the performance of the
existing system using latent space compression on the encoded data. This
theoretically would improve the performance exponentially. Thus, the algorithms
used to improve the system's performance and the results obtained have been
enunciated in this work. The results indicate the level of dominance this
system could achieve to be able to diminish the difficulties in solving
real-time problems in terms of security, deployment and database management.Comment: Presented at the 6 th National Conference on Information and
  Communication Technologies (NCICT 2020), June 12, 202",,GANash -- A GAN approach to steganography,http://arxiv.org/abs/2110.13650,,,core
490750518,2021-01-01T00:00:00,"The purpose of this article is to identify the risks, threats, and challenges associated with possible social changes in the processes of digitalization of society and transformations of traditional communication practices, which is associated with the emergence of new digital subjects of mass public communication that form the pseudo structure of digital interaction of people. The primary tasks of the work were to identify the potential of artificial intelligence technologies and neural networks in the field of social and political communications, as well as to analyze the features of “smart” communications in terms of their subjectness. As a methodological optics, the work used the method of discourse analysis of scientific research devoted to the implementation and application of artificial intelligence technologies and self-learning neural networks in the processes of social and political digitalization, as well as the method of critical analysis of current communication practices in the socio-political sphere. At the same time, when analyzing the current digitalization practices, the case study method was used. The authors substantiate the thesis that introducing technological solutions based on artificial intelligence algorithms and self-learning neural networks into contemporary processes of socio-political communication creates the potential for a wide range of challenges, threats, and risks, the key of which is the problem of identifying the actual subjects of digital communication acts. The article also discusses the problem of increasing the manipulative potential of “smart” communications, for which the authors used the concepts of cyber simulacrum and information capsule developed by them. The paper shows that artificial intelligence and self-learning neural network algorithms, being increasingly widely introduced into the current practice of contemporary digital communications, form a high potential for information and communication impact on the mass consciousness from technological solutions that no longer require control by operators – humans. As a result, conditions arise to form a hybrid socio-technical reality – a communication reality of a new type with mixed subjectness. The paper also concludes that in the current practices of social interactions in the digital space, a person faces a new phenomenon – interfaceization, within which self-communication stimulates the universalization and standardization of digital behavior, creating, disseminating, strengthening, and imposing special digital rituals. In the article, the authors suggest that digital rituals blur the line between the activity of digital avatars based on artificial intelligence and the activity of actual people, resulting in the potential for a person to lose his own subjectness in the digital communications space.Celem niniejszego artykułu jest identyfikacja ryzyk, zagrożeń i wyzwań związanych z możliwymi zmianami społecznymi w procesach cyfryzacji społeczeństwa oraz przekształceniami tradycyjnych praktyk komunikacyjnych, co wiąże się z pojawieniem się nowych cyfrowych podmiotów masowej komunikacji publicznej tworzących pseudostrukturę cyfrowej interakcji pomiędzy ludźmi. Podstawowymi zadaniami pracy była identyfikacja potencjału technologii sztucznej inteligencji i sieci neuronowych w obszarze komunikacji społecznej i politycznej, a także analiza cech komunikacji „inteligentnej” pod kątem jej podmiotowości. Jako optykę metodologiczną w pracy wykorzystano metodę analizy dyskursu badań naukowych poświęconych wdrożeniu i zastosowaniu technologii sztucznej inteligencji oraz samouczących się sieci neuronowych w procesach cyfryzacji społecznej i politycznej, a także metodę krytycznej analizy aktualnych praktyk komunikacyjnych w sferze społeczno-politycznej. Jednocześnie przy analizie aktualnych praktyk digitalizacyjnych zastosowano metodę studium przypadku. Autorzy uzasadniają tezę, że wprowadzenie do współczesnych procesów komunikacji społeczno-politycznej rozwiązań technologicznych opartych na algorytmach sztucznej inteligencji i samouczących się sieciach neuronowych stwarza potencjał dla szerokiego wachlarza wyzwań, zagrożeń i ryzyka, których kluczem jest problem identyfikacji rzeczywistych podmiotów aktów komunikacji cyfrowej. W artykule omówiono również problem zwiększenia potencjału manipulacyjnego „inteligentnej” komunikacji, do czego autorzy wykorzystali opracowane przez siebie koncepcje cyber simulacrum i kapsuły informacyjnej. Artykuł pokazuje, że sztuczna inteligencja i samouczące się algorytmy sieci neuronowych, coraz szerzej wprowadzane do obecnej praktyki współczesnej komunikacji cyfrowej, stwarzają duży potencjał oddziaływania informacyjno-komunikacyjnego na świadomość masową z rozwiązań technologicznych, które nie wymagają już kontroli przez operatorów – ludzi. W efekcie powstają warunki do uformowania hybrydowej rzeczywistości społeczno-technicznej – rzeczywistości komunikacyjnej nowego typu o mieszanej podmiotowości. W artykule stwierdzono również, że w obecnych praktykach interakcji społecznych w przestrzeni cyfrowej człowiek staje przed nowym zjawiskiem – interfaceization, w ramach którego autokomunikacja stymuluje uniwersalizację i standaryzację zachowań cyfrowych, tworzenie, rozpowszechnianie, wzmacnianie i narzucanie szczególnego cyfrowego rytuału. W artykule autorzy sugerują, że cyfrowe rytuały zacierają granicę między aktywnością cyfrowych awatarów opartych na sztucznej inteligencji a aktywnością rzeczywistych ludzi, co skutkuje możliwością utraty przez człowieka własnej podmiotowości w cyfrowej przestrzeni komunikacyjnej",'Adam Mickiewicz University Poznan',"Tematyka komunikacji cyfrowej w kontekście ewolucji technologicznej współczesnego społeczeństwa: zagrożenia, wyzwania, ryzyka",,10.14746/ps.2021.1.25,"[{'title': 'Przegląd Strategiczny', 'identifiers': ['issn:2084-6991', '2084-6991']}]",core
491329212,2021-05-31T09:22:28,"With the introduction of edge analytics, IoT devices are becoming smart and ready for AI applications. A few modern ML frameworks are focusing on the generation of small-size ML models (often in kBs) that can directly be flashed and executed on tiny IoT devices, particularly the embedded systems. Edge analytics eliminates expensive device-to-cloud communications, thereby producing intelligent devices that can perform energy-efficient real-time offline analytics. Any increase in the training data results in a linear increase in the size and space complexity of the trained ML models, making them unable to be deployed on IoT devices with limited memory. To alleviate the memory issue, a few studies have focused on optimizing and fine-tuning existing ML algorithms to reduce their complexity and size. However, such optimization is usually dependent on the nature of IoT data being trained. In this paper, we presented an approach that protects model quality without requiring any alteration to the existing ML algorithms. We propose SRAM-optimized implementation and efficient deployment of widely used standard/stable ML-frameworks classifier versions (e.g., from Python scikit-learn). Our initial evaluation results have demonstrated that ours is the most resource-friendly approach, having a very limited memory footprint while executing large and complex ML models on MCU-based IoT devices, and can perform ultra-fast classifications while consuming 0 bytes of SRAM. When we tested our approach by executing it on a variety of MCU-based devices, the majority of models ported and executed produced 1-4x times faster inference results in comparison with the models ported by the sklearn-porter, m2cgen, and emlearn libraries.This publication has emanated from research supported in
part by a research grant from Science Foundation Ireland (SFI)
under Grant Number SFI/16/RC/3918 (Confirm) and also by
a research grant from Science Foundation Ireland (SFI) under
Grant Number SFI/12/RC/2289_P2 (Insight), with both grants
co-funded by the European Regional Development Fund.peer-reviewe",'Institute of Electrical and Electronics Engineers (IEEE)',Ultra-fast machine learning classifier execution on IoT devices without SRAM consumption,https://core.ac.uk/download/491329212.pdf,10.1109/PerComWorkshops51409.2021.9431061,,core
490928511,2021-11-23T00:00:00,"A wide range of analysis and testing techniques targeting modern web apps
rely on the automated exploration of their state space by firing events that
mimic user interactions. However, finding out which elements are actionable in
web apps is not a trivial task. To improve the efficacy of exploring the event
space of web apps, we propose a browser-independent, instrumentation-free
approach based on structural and visual stylistic cues. Our approach,
implemented in a tool called StyleX, employs machine learning models, trained
on 700,000 web elements from 1,000 real-world websites, to predict actionable
elements on a webpage a priori. In addition, our approach uses stylistic cues
for ranking these actionable elements while exploring the app. Our actionable
predictor models achieve 90.14\% precision and 87.76\% recall when considering
the click event listener, and on average, 75.42\% precision and 77.76\% recall
when considering the five most-frequent event types. Our evaluations show that
StyleX can improve the JavaScript code coverage achieved by a general-purpose
crawler by up to 23\%",,Style-Guided Web Application Exploration,http://arxiv.org/abs/2111.12184,,,core
477671455,2021-09-01T00:00:00,"This research is concerned with the novel application and investigation of ‘Soft Actor Critic’ based deep reinforcement learning to control the cooling setpoint (and hence cooling loads) of a large commercial building to harness energy flexibility. The research is motivated by the challenge associated with the development and application of conventional model-based control approaches at scale to the wider building stock. Soft Actor Critic is a model-free deep reinforcement learning technique that is able to handle continuous action spaces and which has seen limited application to real-life or high-fidelity simulation implementations in the context of automated and intelligent control of building energy systems. Such control techniques are seen as one possible solution to supporting the operation of a smart, sustainable and future electrical grid. This research tests the suitability of the technique through training and deployment of the agent on an EnergyPlus based environment of the office building. The agent was found to learn an optimal control policy that was able to minimise energy costs by 9.7% compared to the default rule-based control scheme and was able to improve or maintain thermal comfort limits over a test period of one week. The algorithm was shown to be robust to the different hyperparameters and this optimal control policy was learnt through the use of a minimal state space consisting of readily available variables. The robustness of the algorithm was tested through investigation of the speed of learning and ability to deploy to different seasons and climates. It was found that the agent requires minimal training sample points and outperforms the baseline after three months of operation and also without disruption to thermal comfort during this period. The agent is transferable to other climates and seasons although further retraining or hyperparameter tuning is recommended",'Elsevier BV',Development of a Soft Actor Critic deep reinforcement learning approach for harnessing energy flexibility in a Large Office building,,10.1016/j.egyai.2021.100101,"[{'title': 'Energy and AI', 'identifiers': ['issn:2666-5468', '2666-5468']}]",core
388360176,2021-03-05T00:00:00,"Thanks to its capability of classifying complex phenomena without explicit
modeling, deep learning (DL) has been demonstrated to be a key enabler of
Wireless Signal Classification (WSC). Although DL can achieve a very high
accuracy under certain conditions, recent research has unveiled that the
wireless channel can disrupt the features learned by the DL model during
training, thus drastically reducing the classification performance in
real-world live settings. Since retraining classifiers is cumbersome after
deployment, existing work has leveraged the usage of carefully-tailored Finite
Impulse Response (FIR) filters that, when applied at the transmitter's side,
can restore the features that are lost because of the the channel actions,
i.e., waveform synthesis. However, these approaches compute FIRs using offline
optimization strategies, which limits their efficacy in highly-dynamic channel
settings. In this paper, we improve the state of the art by proposing Chares, a
Deep Reinforcement Learning (DRL)-based framework for channel-resilient
adaptive waveform synthesis. Chares adapts to new and unseen channel conditions
by optimally computing through DRL the FIRs in real-time. Chares is a DRL agent
whose architecture is-based upon the Twin Delayed Deep Deterministic Policy
Gradients (TD3), which requires minimal feedback from the receiver and explores
a continuous action space. Chares has been extensively evaluated on two
well-known datasets. We have also evaluated the real-time latency of Chares
with an implementation on field-programmable gate array (FPGA). Results show
that Chares increases the accuracy up to 4.1x when no waveform synthesis is
performed, by 1.9x with respect to existing work, and can compute new actions
within 41us",,"Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for
  Resilient Wireless Signal Classification",http://arxiv.org/abs/2103.03745,,,core
327123147,2021-06-01T00:00:00,"High-order interactive features capture the correlation between different
columns and thus are promising to enhance various learning tasks on ubiquitous
tabular data. To automate the generation of interactive features, existing
works either explicitly traverse the feature space or implicitly express the
interactions via intermediate activations of some designed models. These two
kinds of methods show that there is essentially a trade-off between feature
interpretability and search efficiency. To possess both of their merits, we
propose a novel method named Feature Interaction Via Edge Search (FIVES), which
formulates the task of interactive feature generation as searching for edges on
the defined feature graph. Specifically, we first present our theoretical
evidence that motivates us to search for useful interactive features with
increasing order. Then we instantiate this search strategy by optimizing both a
dedicated graph neural network (GNN) and the adjacency tensor associated with
the defined feature graph. In this way, the proposed FIVES method simplifies
the time-consuming traversal as a typical training course of GNN and enables
explicit feature generation according to the learned adjacency tensor.
Experimental results on both benchmark and real-world datasets show the
advantages of FIVES over several state-of-the-art methods. Moreover, the
interactive features identified by FIVES are deployed on the recommender system
of Taobao, a worldwide leading e-commerce platform. Results of an online A/B
testing further verify the effectiveness of the proposed method FIVES, and we
further provide FIVES as AI utilities for the customers of Alibaba Cloud.Comment: Accepted by KDD-2",,FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data,http://arxiv.org/abs/2007.14573,,,core
440629292,2021-01-01T00:00:00,"With the rapid development of deep learning techniques, the popularity of voice services implemented on various Internet of Things (IoT) devices is ever increasing. In this paper, we examine user-level membership inference in the problem space of voice services, by designing an audio auditor to verify whether a specific user had unwillingly contributed audio used to train an automatic speech recognition (ASR) model under strict black-box access. With user representation of the input audio data and their corresponding translated text, our trained auditor is effective in user-level audit. We also observe that the auditor trained on specific data can be generalized well regardless of the ASR model architecture. We validate the auditor on ASR models trained with LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid ASR system and the end-to-end ASR system. Finally, we conduct a real-world trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding 80%. We hope the methodology developed in this paper and findings can inform privacy advocates to overhaul IoT privacy",'Walter de Gruyter GmbH',The Audio Auditor: User-Level Membership Inference in Internet of Things Voice Services,,10.2478/popets-2021-0012,"[{'title': 'Proceedings on Privacy Enhancing Technologies', 'identifiers': ['2299-0984', 'issn:2299-0984']}]",core
479489791,2021-11-02T00:00:00,"Offline reinforcement learning leverages large datasets to train policies
without interactions with the environment. The learned policies may then be
deployed in real-world settings where interactions are costly or dangerous.
Current algorithms over-fit to the training dataset and as a consequence
perform poorly when deployed to out-of-distribution generalizations of the
environment. We aim to address these limitations by learning a Koopman latent
representation which allows us to infer symmetries of the system's underlying
dynamic. The latter is then utilized to extend the otherwise static offline
dataset during training; this constitutes a novel data augmentation framework
which reflects the system's dynamic and is thus to be interpreted as an
exploration of the environments phase space. To obtain the symmetries we employ
Koopman theory in which nonlinear dynamics are represented in terms of a linear
operator acting on the space of measurement functions of the system and thus
symmetries of the dynamics may be inferred directly. We provide novel
theoretical results on the existence and nature of symmetries relevant for
control systems such as reinforcement learning settings. Moreover, we
empirically evaluate our method on several benchmark offline reinforcement
learning tasks and datasets including D4RL, Metaworld and Robosuite and find
that by using our framework we consistently improve the state-of-the-art for
Q-learning methods",,"Koopman Q-learning: Offline Reinforcement Learning via Symmetries of
  Dynamics",http://arxiv.org/abs/2111.01365,,,core
421030976,2021-04-27T00:00:00,"I. PROJECT OVERVIEW A.	Research Question In this project, the question was asked: ”Is there an easier way to extract vocals from music?” Many other works are able to extract vocals with Deep Neural Networks using Multitask Learning, which are large and take a long time to train. To rival this, we wish to present a method to identify vocals with a Convolutional U-Network (U-Net) for Semantic Segmentation of audio files. B.	Project Description This project differs from other works by identifying vocal locations by converting audio files in Short Time Fourier Transforms(STFT), and treating them as images in the UNet. By treating these as images, the U-Net is able to identify the location of ”vocal features” the same way a U-Net would identify desired features within an image. The object detection is what sets this project apart from similar works. Many of these other works treat each song as an audio signal with real and imaginary components which means these algorithms treat the issue as a signal processing problem. However, by looking at the STFT of the song as a graph, we are instead able to approach this as an image processing problem instead, which offers more tools within the realm of Deep Learning–such as Semantic Segmentation. II. EXPERIMENTATION A.	Materials and Methods All Materials used were a form of software. Firstly, the UNetwork was created and ran in python on the CCSE Cluster for High Efficiency. A U-Network is a Convoluted Neural Network that has the ability to output images by Convoluting the original image to allow only the prominent features to be shown and Deconvoluting the Output to display these features in the original image resolution to be used for further processing. This gives the U-net it’s ”u” shape when drawn out. Secondly, the data created for the project were music files converted into Short Time Fourier Transforms(STFT) and processed as image files, where the input into the U-Network was an entire song’s STFT and the labeled data was the vocal audio file STFT for that same song. A Short-Time Fourier Transform can be considered the heatmap of the amplitudes of the song across frequency and time. B.	Results The initial Results from the U-Network show a high level of accuracy for vocal location predictions. As the output from a U-Network is an image, these images are the initial song’s STFT with a mask applied to show the location of Vocal Waves. These trials have an accuracy greater than 80% which is a very good result this early in the processing. The vocals have been identified and located in this study, however the next step is to pull the vocals out and convert them back into a song wave. III. MARKETABILITY For the last 20 or so years, large record labels have been attempting to ”Remaster” old music, which is the process of digitizing old analog tracks of songs, mixing them on a new sound board, and releasing the remastered work at a marked up price. As recording methods, pre-computers, relied on tape, often times tracks were record over each other to save space on the real. When the song has this issue, a computer program has to pull out all of the pieces of the song so that the engineer can remaster it. This project shows the initial steps to a simpler audio extraction, where handling this issue as an image processing problem instead of a signal processing problem, we are able to create a more efficient Neural Network. Advisors(s): Dr. AledhariTopic(s): Artificial IntelligenceCS 426",DigitalCommons@Kennesaw State University,UR-48 Using Semantic Segmentation in a Convoluted Neural Network for Vocal Localization in Music,,,,core
478868057,2021-09-15T15:40:54,"The objective of the proposed research is to develop methodologies, support algorithms and software-hardware infrastructure for detection, diagnosis, and correction of failures for actuators, sensors and control software in linear and nonlinear state variable systems with the help of multiple checks employed in the system. This objective is motivated by the proliferation of autonomous sense-and-control real-time systems, such as intelligent robots and self-driven cars which must maintain a minimum level of performance in the presence of electro-mechanical degradation of system-level components in the field as well as external attacks in the form of transient errors. A key focus is on rapid recovery from the effects of such anomalies and impairments with minimal impact on system performance while maintaining low implementation overhead as opposed to traditional schemes for recovery that rely on duplication or triplication. On-line detection, diagnosis and correction techniques are investigated and rely on analysis of system under test response signatures to real-time stimulus. For on-line error detection and diagnosis, linear and nonlinear state space encodings of the system under test are used and specific properties of the codes, as well as machine learning model based approaches were used are analyzed in real-time. Recovery is initiated by copying check model values to correct error for sensor and control software malfunction, and by redesigning the controller parameter on-the-fly for actuators to restore system performance. Future challenges that need to be addressed include viability studies of the proposed techniques on mobile autonomous system in distributed setting as well as application to systems with soft as well as hard real-time performance constraints.Ph.D",Georgia Institute of Technology,REAL-TIME ERROR DETECTION AND CORRECTION FOR ROBUST OPERATION OF AUTONOMOUS SYSTEMS USING ENCODED STATE CHECKS,https://core.ac.uk/download/478868057.pdf,,,core
479151683,2021-06-13T00:00:00,"The ongoing deployment of Distributed Energy Resources (DERs), while bringing benefits, introduces significant challenges to the electric utility industry, especially in the distribution grid. These challenges call for closer monitoring through state estimation, where real-time topology recovery is the basis for accurate modeling. With the dramatic increase of the residential photovoltaic (PV) systems (i.e., DER), utilities need to know the locations of these new assets to manage the unconventional two-way power flow for sustainable management of distribution grids. Previous methods to maintain the system connectivity are either based on outdated maps or an ideal assumption of an isolated sub-network for topology recovery, e.g., within one transformer. This requires field engineers to identify the association, which is costly and may contain errors. As it has been shown that, historical records are not always up-to-date. 



To solve these problems, a density-based clustering method is proposed that leverage both voltage domain data from the Advanced Measurement Infrastructure (AMI) and the geographical space information. The goal of such a method is to efficiently segment data sets from a large utility customer pool, after which other topology reconstruction methods can carry over. Specifically, it is shown how to use the voltage data and GIS information to refine the connectivity within one transformer. To give a guarantee, a theoretic bound for the proposed clustering method is shown, providing the ability to explain the performance of the machine learning method. Numerical results on both IEEE test systems and utility networks show the outstanding performance of the new method. An implementation is also demonstrated in the field.

In this dissertation, we consider the rich potential of large utility datasets, in which physical laws are inherently embedded, to identify system information and utilization by using machine learning algorithms. In order to provide situational awareness and tackle practical issues such as limited measurements and un-scalability, we start with proposing a customized data-driven approach to provide an accurate model for distribution grid control and planning",,Utilizing AMI Interval Data and Machine Learning Algorithms to IdentifyDistribution System Topology and DER Connectivity,,,,core
388365302,2021-03-10T00:00:00,"AI-manipulated videos, commonly known as deepfakes, are an emerging problem.
Recently, researchers in academia and industry have contributed several
(self-created) benchmark deepfake datasets, and deepfake detection algorithms.
However, little effort has gone towards understanding deepfake videos in the
wild, leading to a limited understanding of the real-world applicability of
research contributions in this space. Even if detection schemes are shown to
perform well on existing datasets, it is unclear how well the methods
generalize to real-world deepfakes. To bridge this gap in knowledge, we make
the following contributions: First, we collect and present the largest dataset
of deepfake videos in the wild, containing 1,869 videos from YouTube and
Bilibili, and extract over 4.8M frames of content. Second, we present a
comprehensive analysis of the growth patterns, popularity, creators,
manipulation strategies, and production methods of deepfake content in the
real-world. Third, we systematically evaluate existing defenses using our new
dataset, and observe that they are not ready for deployment in the real-world.
Fourth, we explore the potential for transfer learning schemes and
competition-winning techniques to improve defenses.Comment: Accepted to The Web Conference 2021; First two authors contributed
  equally to this work; 12 pages, 6 table",,Deepfake Videos in the Wild: Analysis and Detection,http://arxiv.org/abs/2103.04263,,,core
326904216,2021-06-30T00:00:00,"Disparate treatment occurs when a machine learning model yields different
decisions for individuals based on a sensitive attribute (e.g., age, sex). In
domains where prediction accuracy is paramount, it could potentially be
acceptable to fit a model which exhibits disparate treatment. To evaluate the
effect of disparate treatment, we compare the performance of split classifiers
(i.e., classifiers trained and deployed separately on each group) with
group-blind classifiers (i.e., classifiers which do not use a sensitive
attribute). We introduce the benefit-of-splitting for quantifying the
performance improvement by splitting classifiers. Computing the
benefit-of-splitting directly from its definition could be intractable since it
involves solving optimization problems over an infinite-dimensional functional
space. Under different performance measures, we (i) prove an equivalent
expression for the benefit-of-splitting which can be efficiently computed by
solving small-scale convex programs; (ii) provide sharp upper and lower bounds
for the benefit-of-splitting which reveal precise conditions where a
group-blind classifier will always suffer from a non-trivial performance gap
from the split classifiers. In the finite sample regime, splitting is not
necessarily beneficial and we provide data-dependent bounds to understand this
effect. Finally, we validate our theoretical results through numerical
experiments on both synthetic and real-world datasets",,"To Split or Not to Split: The Impact of Disparate Treatment in
  Classification",http://arxiv.org/abs/2002.04788,,,core
388710373,2021-07-04T00:00:00,"Offline reinforcement learning proposes to learn policies from large
collected datasets without interacting with the physical environment. These
algorithms have made it possible to learn useful skills from data that can then
be deployed in the environment in real-world settings where interactions may be
costly or dangerous, such as autonomous driving or factories. However, current
algorithms overfit to the dataset they are trained on and exhibit poor
out-of-distribution generalization to the environment when deployed. In this
paper, we study the effectiveness of performing data augmentations on the state
space, and study 7 different augmentation schemes and how they behave with
existing offline RL algorithms. We then combine the best data performing
augmentation scheme with a state-of-the-art Q-learning technique, and improve
the function approximation of the Q-networks by smoothening out the learned
state-action space. We experimentally show that using this Surprisingly Simple
Self-Supervision technique in RL (S4RL), we significantly improve over the
current state-of-the-art algorithms on offline robot learning environments such
as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4]",,"S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement
  Learning",http://arxiv.org/abs/2103.06326,,,core
459158616,2021-04-25T00:00:00,"This research is concerned with the novel application and investigation of
`Soft Actor Critic' (SAC) based Deep Reinforcement Learning (DRL) to control
the cooling setpoint (and hence cooling loads) of a large commercial building
to harness energy flexibility. The research is motivated by the challenge
associated with the development and application of conventional model-based
control approaches at scale to the wider building stock. SAC is a model-free
DRL technique that is able to handle continuous action spaces and which has
seen limited application to real-life or high-fidelity simulation
implementations in the context of automated and intelligent control of building
energy systems. Such control techniques are seen as one possible solution to
supporting the operation of a smart, sustainable and future electrical grid.
This research tests the suitability of the SAC DRL technique through training
and deployment of the agent on an EnergyPlus based environment of the office
building. The SAC DRL was found to learn an optimal control policy that was
able to minimise energy costs by 9.7% compared to the default rule-based
control (RBC) scheme and was able to improve or maintain thermal comfort limits
over a test period of one week. The algorithm was shown to be robust to the
different hyperparameters and this optimal control policy was learnt through
the use of a minimal state space consisting of readily available variables. The
robustness of the algorithm was tested through investigation of the speed of
learning and ability to deploy to different seasons and climates. It was found
that the SAC DRL requires minimal training sample points and outperforms the
RBC after three months of operation and also without disruption to thermal
comfort during this period. The agent is transferable to other climates and
seasons although further retraining or hyperparameter tuning is recommended.Comment: submitted to Energy and A",'Elsevier BV',"Development of a Soft Actor Critic Deep Reinforcement Learning Approach
  for Harnessing Energy Flexibility in a Large Office Building",http://arxiv.org/abs/2104.12125,10.1016/j.egyai.2021.100101,,core
478995139,2021-10-21T00:00:00,"Deep convolutional neural networks are shown to be overkill with high
parametric and computational redundancy in many application scenarios, and an
increasing number of works have explored model pruning to obtain lightweight
and efficient networks. However, most existing pruning approaches are driven by
empirical heuristics and rarely consider the joint impact of channels, leading
to unguaranteed and suboptimal performance. In this paper, we propose a novel
channel pruning method via class-aware trace ratio optimization (CATRO) to
reduce the computational burden and accelerate the model inference. Utilizing
class information from a few samples, CATRO measures the joint impact of
multiple channels by feature space discriminations and consolidates the
layer-wise impact of preserved channels. By formulating channel pruning as a
submodular set function maximization problem, CATRO solves it efficiently via a
two-stage greedy iterative optimization procedure. More importantly, we present
theoretical justifications on convergence and performance of CATRO.
Experimental results demonstrate that CATRO achieves higher accuracy with
similar computation cost or lower computation cost with similar accuracy than
other state-of-the-art channel pruning algorithms. In addition, because of its
class-aware property, CATRO is suitable to prune efficient networks adaptively
for various classification subtasks, enhancing handy deployment and usage of
deep networks in real-world applications",,CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization,http://arxiv.org/abs/2110.10921,,,core
480004268,2021-01-01T00:00:00,"Satellite communications are gaining attention in 5G networks, as enablers for the wide coverage of remote areas, efficient transmission of broadcasting and multicasting contents and alternative backhauling solutions to guarantee service continuity and dynamic traffic offloading. The recent advancements in dynamic management of satellite resources and virtualization of satellite functions are paving the way towards the full integration of satellite networks in 5G infrastructures. A key aspect is the orchestration of the satellite network as an integrated part of 5G management, which exploits the concepts of Software Defined Networking and Network Function Virtualization towards the flexible delivery of end-to-end network slices, seamlessly instantiated across hybrid satellite-terrestrial networks. The European Space Agency (ESA) ANChOR project is implementing a data-driven solution for a Network Controller and Orchestrator that exploits Artificial Intelligence and Machine Learning techniques to automate the real-time provisioning and lifecycle management of eMBB and mIoT network slices over integrated 5G Satellite-Terrestrial networks. This paper presents the ANChOR use cases and the architecture of the ANChOR orchestration solution",,"Data-driven, real-time automation of network slice management in 5G infrastructures integrating satellite networks",,,,core
158304924,,"200 p.Thesis (Ph.D.)--University of Illinois at Urbana-Champaign, 1997.In the past, AI systems have strived to automate problem-solving processes completely. However, in recent years researchers have come to realize that it is not always possible or desirable to aim for total automation. Researchers are realizing the importance of human-computer collaborative systems in which the human and the computer work as a team in solving problems. This approach raises the question of how to design systems that support effective collaborative problem-solving between humans and computers. The primary contribution of this thesis is an architecture for coordination of collaborative problem-solving (CO-SOLVE) for an important class of human-computer collaborative systems called associate systems. Associate systems are knowledge-based systems which share the cognitive workload with their human partners. Designers of associate systems must deal with the complexities of integrating mixed-initiative (i.e. human and computer) control with the general issues of problem-solving control faced by traditional AI systems. CO-SOLVE provides mechanisms for attention synchronization and collaborative alternatives exploration. The Attention Synchronization Model (ASM) allows the system to track (rather than direct) the user's activities in order to provide advice relevant to the current user activities. The Collaborative Alternatives Exploration Model (CAEM) is a mixed-initiative approach to exploring large, complex solution spaces. In this collaborative framework the user serves as solution evaluator and system controller and the computer as solution alternative generator. System developers using CAEM explicitly lay out steps in the problem-solving process for a given task and define points of human-computer interaction within the sequence of process steps. The other contribution of this thesis is a proof-of-concept prototype of CO-SOLVE, called SEDAR, which is implemented for a real-world, complex domain (flat and low-slope roof design). Two evaluations were conducted on SEDAR. The first assessed the effectiveness and usability of the ASM and its critiquing strategies as implemented in SEDAR. The evaluation showed that SEDAR reduced the error rate of experienced architects and also which advising strategies they preferred. The second evaluation assessed the effectiveness of the CAEM as implemented in SEDAR and showed that SEDAR (1) helped experienced architects reduce the amount of time spent developing solutions, and (2) increased the number of alternatives searched in the solution space.U of I OnlyRestricted to the U of I community idenfinitely during batch ingest of legacy ETD",,An Architecture for Collaborative Problem-Solving Control in Associate Systems,,,,core
324164938,,"Convolutional neural networks (CNNs) have emerged as a crucial part in many applications ranging
from self-driving cars to voice-activated assistants. Numerous cloud computing providers, such as
Amazon (AWS), IBM (SoftLayer), and Microsoft (Azure), choose to use heterogeneous computing
systems to off-load the CNN computations from the CPU to a dedicated hard-ware since such
hardware provides significant improvements in both computing throughput and energy savings. In
this senior thesis, the author presents a weight-stationary systolic convolution kernel design for
a field-programmable gate array (FPGA) and its implementation targeting Nallantech 250s+card
that is enabled for the coherent accelerator processor interface (CAPI). CAPI is an interface for
heterogeneous systems that allows accelerators to access I/O devices as CPU peers. Systolic
array architecture has shown advantages for accelerators in tasks that involve vector dot-product
calculations, such as matrix multiplication and convolution. The proposed hardware is synthesized
by the High-Level Synthesis tool targeting Kintex UltraScale+XCKU15P FPGA and can provide high
throughput (4.6×CPU) computations for real-time applications. In the future, the author plans to
extend this kernel to a complete CNN by supporting CPU-FPGA task partitioning using the coherent
memory space enabled by CAPI.U of I OnlyUndergraduate senior thesis not recommended for open acces",,Accelerating convolution in deep neural networks on a CAPI-based FPGA,,,,core
491233905,2022-01-01T00:00:00,"The increasing accessibility to dynamic data collected from low-cost sensing and crowd-sourced technologies, and geo-localized mobile and social media networks, are generating new types of data analysis practices. Such practices are opening new possibilities to rethink urban planning processes to address pressing urban contemporary challenges such as urban health and comfort. The study of dynamic data can enable the development of adaptable urban environmental and mobility planning strategies; however, the implementation of the data analysis protocols in urban design and planning strategies still remains to be further discussed. This paper presents several research case studies focused on the utilization of dynamic data sources and machine learning technologies, for the study and prediction of urban environmental gradients and mobility patterns. The paper argues that the presented research workflows can enable a high spatiotemporal resolution urban field exploration which was not possible with the traditional sensing and traffic monitoring platforms. Furthermore, the paper argues that dynamic data-driven methodological approaches can inform real-time urban planning strategies such as the adjustment of transportation, bike and micro mobility routes, or public space configuration strategies to minimize the exposure to air pollution or heat stress",Mitra Kanaaani,New data sources and analysis practices in urban mobility and environmental studies,,,,core
323787593,2020-05-20T00:00:00,"Well known to the machine learning community, the random feature model, originally introduced by Rahimi and Recht in 2008, is a parametric approximation to kernel interpolation or regression methods. It is typically used to approximate functions mapping a finite-dimensional input space to the real line. In this paper, we instead propose a methodology for use of the random feature model as a data-driven surrogate for operators that map an input Banach space to an output Banach space. Although the methodology is quite general, we consider operators defined by partial differential equations (PDEs); here, the inputs and outputs are themselves functions, with the input parameters being functions required to specify the problem, such as initial data or coefficients, and the outputs being solutions of the problem. Upon discretization, the model inherits several desirable attributes from this infinite-dimensional, function space viewpoint, including mesh-invariant approximation error with respect to the true PDE solution map and the capability to be trained at one mesh resolution and then deployed at different mesh resolutions. We view the random feature model as a non-intrusive data-driven emulator, provide a mathematical framework for its interpretation, and demonstrate its ability to efficiently and accurately approximate the nonlinear parameter-to-solution maps of two prototypical PDEs arising in physical science and engineering applications: viscous Burgers' equation and a variable coefficient elliptic equation",The Random Feature Model for Input-Output Maps between Banach Spaces,https://core.ac.uk/download/323787593.pdf,,,,core
388635383,2020-01-01T00:00:00,"Recurrent neural networks (RNNs) are a powerful approach for time series prediction. However, their performance is strongly affected by their architecture and hyperparameter settings. The architecture optimization of RNNs is a time-consuming task, where the search space is typically a mixture of real, integer and categorical values. To allow for shrinking and expanding the size of the network, the representation of architectures often has a variable length. In this paper, we propose to tackle the architecture optimization problem with a variant of the Bayesian Optimization (BO) algorithm. To reduce the evaluation time of candidate architectures the Mean Absolute Error Random Sampling (MRS), a training-free method to estimate the network performance, is adopted as the objective function for BO. Also, we propose three fixed-length encoding schemes to cope with the variable-length architecture representation. The result is a new perspective on accurate and efficient design of RNNs, that we validate on three problems. Our findings show that 1) the BO algorithm can explore different network architectures using the proposed encoding schemes and successfully designs well-performing architectures, and 2) the optimization time is significantly reduced by using MRS, without compromising the performance as compared to the architectures obtained from the actual training procedure.Algorithms and the Foundations of Software technolog",Bayesian Neural Architecture Search using A Training-Free Performance Metric,,,,,core
237183624,2020-01-01T00:00:00,"Parking vehicle is a daunting task and a common problem in many cities around the globe. The search for parking space leads to congestion, frustration and increased air pollution. Information of a vacant parking space would facilitate to reduce congestion and subsequent air pollution. Therefore, aim of the paper is to acquire vehicle occupancy in an open parking lot using deep learning. Thermal camera was used to collect the data during varying environmental conditions such as; sunny, dusk, dawn, dark and snowy conditions. Vehicle detection with deep learning was implemented where image classification and object localization were performed for multi object detection. The dataset consists of 527 images which were manually labelled as there were no pre-labelled thermal images available. Multiple deep learning networks such as Yolo, ReNet18, ResNet50 and GoogleNet with varying layers and architectures were evaluated on vehicle detection. Yolo, GoogleNet and ResNet18 are computationally efficient detectors which took less processing time while Resnet50 produced better detection results compared to other detectors. However, ResNet18 also produced minimal miss rates and is suitable for real time vehicle detection. The detected results were compared with a template of parking spaces and IoU value is used to identify vehicle occupancy information",Deep learning-based vehicle occupancy detection in an open parking lot using thermal camera,,'Institution of Engineering and Technology (IET)',10.1049/iet-its.2019.0468,,core
370074669,2020-12-11T00:00:00,"References
Bowker, L. & J. Buitrago Ciro. 2019. Machine Translation and Global Research: Towards Improved Machine Translation Literacy in the Scholarly Community. Bingley: Emerald Publishing.
Earley, P. C. & S. Ang. 2003. Cultural intelligence: Individual interactions across cultures. Stanford, CA: Stanford Business Books. 
Federici, F. M. & C. Declercq, eds. 2019. Intercultural Crisis Communication. London: Bloomsbury Press.
Forcada, M. L. 2017. Making sense of neural machine translation. Translation Spaces 6 (2): 291–309.
Hassan, H., A. Aue, C. Chen, V. Chowdhary, J. Clark, C. Federmann, X. Huang, M. Junczys-Dowmunt, W. Lewis, M. Li, S. Liu, T-Y. Liu, R. Luo, A., Menezes, T., Qin, F., Seide, X., Tan, F., Tian, L., Wu, S., Wu, Y., Xia, D., Zhang, Z., & Zhou, M. 2018. Achieving human parity on automatic Chinese to English news translation. arXiv:1803.05567.
Martindale, M. J. & M. Carpuat. 2018. Fluency over accuracy: A pilot study in measuring user trust in imperfect MT. Proceedings of AMTA 2018 1: 13-25. http://aclweb.org/anthology/W18-1803
Massey, G. & M. Ehrensberger-Dow. 2017. Machine learning – implications for translator education. Lebende Sprache 62 (2): 300-312.The recent advances in artificial intelligence, natural language processing, and ready access to freely available online tools are raising people's expectations that quality translation is only a click away, with media dramatically citing research reports claiming human parity for neural machine translation (NMT; Hassan et al. 2018). Neglected in this discourse is the sobering reality of the risks and cultural inappropriateness associated with the misleadingly fluent output of some of these systems (cf. Martindale & Carpuat 2018). Within translation studies and among the professional translation community, interest in digital literacy with respect to recent advances in NMT has been growing (e.g. Forcada 2017). It now seems generally accepted that translation students should develop the capacity to decide on the deployment of language technologies by learning about the capabilities and limitations of the machines and tools with which they are and will be working (cf. Massey & Ehrensberger-Dow 2017). This type of knowledge has been referred to as MT literacy by Bowker and Buitrago Ciro (2019), whose list of component competences probably seems familiar to most institutions currently involved in translator training (e.g. comprehend the basics of MT systems; appreciate the wider implications associated with the use of MT; evaluate how MT-friendly texts are; create or modify a text so that it could be translated; modify MT output to improve its accuracy and readability). MT literacy can inform judgements about the appropriate genres, quality expectations, risks, and limitations that call for intervention by human translators. Although rarely mentioned in this context, those judgements also necessarily draw on the rich intercultural awareness that translators bring to their work. As intercultural mediators, they have been trained to recognize and deal with cultural differences, potential ambiguity, terminological inconsistencies as well as conceptual and lexical gaps as they transfer meaning from one language to another (e.g. Federici & Declercq 2019). This is very much in line with Earley and Ang’s (2003: 9) multidimensional concept of cultural intelligence, which they define as “a person’s capability for successful adaptation to new cultural settings”. The extension of this concept to “intercultural intelligence” captures the reality of translators shifting back and forth between the context of the source text and that of the target text. Conceptualising MT literacy as being at the interface between language-related artificial intelligence and intercultural intelligence allows for the integration of apparently opposite poles of the human-machine spectrum. Such a conceptualisation can provide the space to encourage the development of expertise in language technology and at the same time foster the uniquely human dimensions of intercultural mediation, intuition, creativity, and ethical judgement",NMT literacy at the interface of AI and intercultural intelligence,,,,,core
334990296,2020-08-01T07:00:00,"The increasing number of spacefaring nations and agendas, miniaturization of subsystems, and trend toward integrated systems are no doubt influencing the evolution of space systems. The diversification of space architectures has surged at an unprecedented rate in recent history with initial deployments of planned mega-constellations. This paper explores HIVE-a reconfigurable small satellite system primed to revolutionize the concept of modular space systems and future space architectures.
Based on a mass producible functioning unit consisting of nested rings, HIVE is a comprehensive satellite design harnessing advancement in robotics, software and machine learning, precision scale manufacturing, and novel materials with multifunctional properties. HIVE is addressing solutions for detailed design of interconnected hardware, engineering analysis for multi-payload applications, and policy to accomplish modularized, in-space deployment and reconfiguration.
The HIVE unit design lends itself to the “infinite possibilities” of space mission architectures and presents a revolutionary way to design, integrate, and operate missions from space. This paper provides and overview of the HIVE concept development and provides examples of applications for HIVE to showcase the range of possible systems and architectural advantages; such as space domain awareness, large service structure, and planetary surface infrastructure. Finally, we will discuss technology transfer and possible pathways to making a resilient, adaptable, and continually upgradable space infrastructure a reality",HIVE: A Space Architecture Concept,https://core.ac.uk/download/334990296.pdf,DigitalCommons@USU,,,core
329133439,2020-09-02T00:00:00,"High-level synthesis (HLS) enables designers to customize hardware designs
efficiently. However, it is still challenging to foresee the correlation
between power consumption and HLS-based applications at an early design stage.
To overcome this problem, we introduce HL-Pow, a power modeling framework for
FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow
incorporates an automated feature construction flow to efficiently identify and
extract features that exert a major influence on power consumption, simply
based upon HLS results, and a modeling flow that can build an accurate and
generic power model applicable to a variety of designs with HLS. By using
HL-Pow, the power evaluation process for FPGA designs can be significantly
expedited because the power inference of HL-Pow is established on HLS instead
of the time-consuming register-transfer level (RTL) implementation flow.
Experimental results demonstrate that HL-Pow can achieve accurate power
modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To
further facilitate power-oriented optimizations, we describe a novel design
space exploration (DSE) algorithm built on top of HL-Pow to trade off between
latency and power consumption. This algorithm can reach a close approximation
of the real Pareto frontier while only requiring running HLS flow for 20% of
design points in the entire design space.Comment: published as a conference paper in ASP-DAC 202","HL-Pow: A Learning-Based Power Modeling Framework for High-Level
  Synthesis",http://arxiv.org/abs/2009.00871,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ASP-DAC47756.2020.9045442,,core
357337238,2020-04-03T00:00:00,"This paper investigates the use of an automatic system for preparation of gas mixtures in a multivariate calibration problem involving near-infrared (NIR) spectrometric analysis of natural gas. The automatic system is used to prepare calibration mixtures according to a Brereton experimental design, in order to exploit a suitable range of gas concentrations and thus avoid extrapolations in the predictions. These mixtures were employed to build partial-least-squares models for NIR determination of methane, ethane and propane, which are the major components of natural gas. Prediction performance was evaluated by using a separate set of prepared mixtures and natural gas samples with composition analyzed by gas chromatography, as well as a group of certified mixtures. The resulting root-mean-square errors of prediction (RMSEP) values for methane, ethane and propane (3.0, 0.9 and 1.2% mol mol -1 , respectively) were approximately 10 times smaller than the corresponding calibration ranges, with correlations of 0.91, 0.96 and 0.86 between the predicted and reference values. Keywords: natural gas analysis, automatic system for preparation of gas mixtures, multivariate calibration, NIR spectrometry, gas chromatography Introduction The analysis of chemical composition of gas samples is usually carried out by using gas chromatography (GC), which allows accurate determinations of individual gas components even in complex matrices. The widespread use of GC is motivated by the minimization of interference effects as the result of the separation in the chromatographic column. 1 However, the operational costs related to the use of consumables and the low sample throughput associated to the time required by the separation process are inconveniences that should be taken into account. In this context, spectrometric techniques have been proposed as a faster and less costly alternative for gas analysis, 2-4 provided that multivariate calibration is used to compensate for the absence of a separation process. 5 Multivariate calibration methods are aimed at obtaining a mathematical model that relates the instrumental measurements with the chemical composition of the sample. For this purpose, the analyst must gather a representative set of calibration samples with known composition. In the case of gas analysis, calibration mixtures with certified composition can be acquired from specialized suppliers. However, the acquisition of these mixtures can be expensive, which escapes the purpose of using a less costly alternative to GC. Alternatively, real samples with composition determined by GC can be used to build the multivariate calibration model, but the variability in the composition of these samples may not be large enough to build an appropriate model.  In this context, the present work investigates the use of an automatic system for accurate preparation of gas mixtures, which was proposed in a recent paper 7 as an improvement on a simpler architecture which had been developed for non-quantitative screening applications.  Use of an Automatic System in the Preparation of Gas Mixtures for Multivariate Calibration J. Braz. Chem. Soc.  2030    system comprises a set of gas admission valves which are controlled in an automatic manner to achieve the desired partial pressures for each component of the mixture. A piston-driven diaphragm pump is used to circulate the mixture within the system in order to obtain an appropriate homogenization. In Dantas et al.,   7 the operation of the system was validated by preparing binary mixtures of nitrogen with methane, ethane or propane. As a result, the programmed molar fractions of the component gases in the prepared mixtures were found to be in good agreement with the results of GC analysis. However, the system was not tested in an actual application involving the preparation of gas mixtures for multivariate calibration. Within this scope, the present investigation is aimed at demonstrating the applicability of this automatic system in an actual analytical problem involving the simultaneous determination of the major components in natural gas samples by using nearinfrared (NIR) spectrometry and multivariate calibration. Natural gas (NG) is mainly composed by methane (CH 4 ) and heavier hydrocarbons, especially ethane (C 2 H 6 ) and propane (C 3 H 8 ). 9 The development of analytical methods for quality control of this fuel has become an important issue, 10 in view of the growing demand for domestic, commercial, industrial, utility and vehicular use of NG, motivated by both economic gains and environmental impact. 11,12 Within this scope, NIR spectrometry has been proposed as an attractive alternative to the use of GC, with advantages including reduced analysis time and little sample preparation 2,13 in addition to the possibility of deploying portable field instruments. 5 More specifically, the use of NIR spectrometry has been reported for screening analysis 8 and determination of the calorific value of NG. 14 In a broader scope, applications have also been reported in the context of screening analysis of liquefied petroleum gas 15    and quantitative analysis of gases in hydrocarbon mixtures.  18 The prediction performance of the resulting model was evaluated by using a separate set of prepared mixtures, as well as three gas mixtures with certified composition and eight actual NG samples for vehicular use. Experimental Samples Methane (99.9%), ethane (99.0%), propane (99.5%), nitrogen (99.9%) and three mixtures of these gases, with certified composition, were acquired from Linde Gas. The certified mixtures were designed in order to simulate the composition of natural gas samples. All gas contents indicated herein are expressed in % mol mol -1 . In addition, eight real NG samples were acquired at 220 bar from vehicle fuelling stations in the city of João Pessoa (Paraíba, Brazil). These samples were collected by using a lab-made sampling cylinder described elsewhere.  After the NIR spectra of the 67 prepared mixtures were recorded, the Kennard-Stone algorithm 21 was employed to select 45 of these mixtures for use in the calibration of the PLS model. This algorithm is aimed at choosing a representative subset of samples in a near-uniform manner in the space of spectral variables, by avoiding the selection of samples with similar spectra. The remaining 22 mixtures were used as a separate prediction set, together with the 3 mixtures of certified composition and the 8 real NG samples. The composition of these 33 prediction samples was analyzed by GC, in order to evaluate the predictive ability of the PLS model.  Apparatus 8 In addition, the system was connected to a gas chromatograph for the analysis of the prediction samples. The NIR spectra of the samples were acquired by using an FTIR Analyzer (AIT, Analect Diamond 20) in the range 4,000-12,000 cm -1 as the average of 16 scans with a resolution of 2.0 cm -1 . The samples were introduced in Barbosa et al.   2031  Vol. 26, No. 10, 2015 the NIR flow cell at a pressure of 1.5 bar. The overall time required by the NIR analysis was one minute per sample. The experimental procedures were carried out in a laboratory environment with air conditioning (split configuration) and dehumidifier units for temperature and humidity control. The temperature and relative humidity were controlled during the analyses in order to remain within the ranges of 23 ± 1 °C and 55 ± 1%, respectively. The gas mixing system is not fitted with internal temperature sensors. However, the internal pressure is controlled by using a digital manometer with precision of ± 0.001 bar. The pressure measurements provided by the digital manometer are employed by the system software to control the admission of the components of the gas mixture, in order to achieve partial pressures corresponding to the desired molar fractions (% mol mol -1 ). Changes in the internal temperature of the system will not affect the results in a significant manner, because the preparation of the gas mixtures is based on the actual pressure values. The GC analyses were carried out by using a gas chromatograph (GC-2014, Shimadzu) using a 30-meter capillary column (GC-GASPRO) with internal diameter of 0.32 mm. The GC injections were performed in split mode (1:100) at a temperature of 240 °C by using a sampling valve (Valco E60) with a 25 microliter loop. Helium was used as carrier gas with a flow rate of 1.4 mL min -1 . All analyses were carried out in isothermal mode with the column temperature at 90 °C. A flame ionization detector (FID) was employed with temperature set at 250 °C. The total analysis time per run was 10 min. Software Spectral preprocessing, principal component analysis and PLS modelling were carried out by using The Unscrambler 9.7 (CAMO S.A.). The optimal number of factors for each PLS model was determined by using cross-validation with the default settings of the software package. The Kennard-Stone algorithm was implemented in Matlab R2010b. Results and Discussion After a preliminary inspection of the NIR spectra, the range 4,000-6,500 cm -1 was selected in view of its large signal-to-noise ratio compared to other spectral regions. The intervals 4,000-4,600 cm -1 and 5,500-6,500 cm -1 correspond to combination bands and first overtones of CH, CH 2 , CH 3 related to the main hydrocarbons (methane, ethane, propane) of the gas samples. 22,23 Figure 2a presents the NIR spectra of three mixtures prepared in this study. In order to remove the baseline features, first-derivative spectra were obtained by using the Savitzky-Golay method with a 2 nd order polynomial and a 3-point window.  An exploratory analysis of the spectral data was carried out by using principal component analysis (PCA). As can be seen in  The PLS models for methane, ethane and propane were built by using 1, 4 and 5 factors, respectively. The three elliptical joint confidence regions (EJCRs) (obtained on the basis of a linear regression between the reference and predicted gas concentrations) are presented in  As can be seen in  Conclusions This paper investigated the use of an automatic system for preparation of gas mixtures in a multivariate calibration problem involving NIR spectrometric analysis of natural gas. The use of prepared calibration mixtures is of value to form an adequate envelope around the samples to be analyzed, which is convenient to avoid extrapolations in the model predictions. For this purpose, the automatic system is convenient to reduce the manual workload in the preparation of the mixtures and to minimize the possibility of human errors. The NIR spectra of 45 prepared mixtures in the range 4,000-6,500 cm -1 was employed to build PLS models for determination of methane, ethane and propane, which are the major components of natural gas. The prediction performance of the resulting models was evaluated by using a separate set of 22 prepared mixtures and 8 natural gas samples, with composition analyzed by gas chromatography, as well as 3 certified mixtures. Only the results associated to reference values larger than the limit of quantification were considered.The resulting RMSEP values for methane, ethane and propane (3.0, 0.9 and 1.2% mol mol -1 , respectively) were approximately 10 times smaller than the corresponding calibration ranges, with correlations of 0.91, 0.96 and 0.86 between the predicted and reference values. No systematic error was observed. In addition, the prediction errors for the certified mixtures and real NG samples were comparable to the errors obtained for the prepared mixtures. The results of this investigation reveal that the automatic system for preparation of gas mixtures is indeed of value for use in multivariate calibration applications. Supplementary Information Supplementary data (tables of concentrations of the components in the mixtures) are available free of charge at http://jbcs.sbq.org.br as PDF file",Use of an Automatic System in the Preparation of Gas Mixtures for Multivariate Calibration: A Case Study Involving NIR Analysis of Natural Gas,,,,,core
323253814,2020-09-11T00:00:00,"Fuzzing is a widely used technique for detecting software bugs and
vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary
search to maximize code coverage. Essentially, these fuzzers start with a set
of seed inputs, mutate them to generate new inputs, and identify the promising
inputs using an evolutionary fitness function for further mutation. Despite
their success, evolutionary fuzzers tend to get stuck in long sequences of
unproductive mutations. In recent years, machine learning (ML) based mutation
strategies have reported promising results. However, the existing ML-based
fuzzers are limited by the lack of quality and diversity of the training data.
As the input space of the target programs is high dimensional and sparse, it is
prohibitively expensive to collect many diverse samples demonstrating
successful and unsuccessful mutations to train the model. In this paper, we
address these issues by using a Multi-Task Neural Network that can learn a
compact embedding of the input space based on diverse training samples for
multiple related tasks (i.e., predicting for different types of coverage). The
compact embedding can guide the mutation process by focusing most of the
mutations on the parts of the embedding where the gradient is high. \tool
uncovers $11$ previously unseen bugs and achieves an average of $2\times$ more
edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world
programs.Comment: ACM Joint European Software Engineering Conference and Symposium on
  the Foundations of Software Engineering (ESEC/FSE) 202",MTFuzz: Fuzzing with a Multi-Task Neural Network,http://arxiv.org/abs/2005.12392,'Association for Computing Machinery (ACM)',10.1145/3368089.3409723,,core
389904876,2020-07-09T00:00:00,"The article presents a new concept of “Management of knowledge about virtual promotion” on the Internet. Usually a real produ ct or service is being divided into four components (product, price, promotion and place) in accordance with the theory of marketing. One of the components is a product promotion. But now this element is becoming a fully virtual tool. It is necessary to consider product promotion as an image or a copy of a real product in a virtual space that lives in parallel on the network. Therefore, the objective of the paper is the presentation of a new object of research based on the experience of more than thirty real projects performed in Ukraine, USA, Europe and Canada. We regard the promotion as a software product, which works according to principles of knowledge management and machine learning. It is proposed that virtual promotion is characterized by four views: customer or user, data, technology and marketing. Thus, the structure of virtual promotion business process was presented. It includes four steps: selection of hypertext sources, knowledge representation and extraction, semantic kernel building and quality criterion evaluation to stop the process. Based on the process structure the research tasks were identified. The central task is semantic kernel forming. Then the software architecture was developed. IT solution contains CRM system as accounting tool and Web site as an image of virtual promotion. CRM plays main role as a commander center. Here we form semantic kernel and then send it via marketing channels such as Web site, telegram or viber accounts. Another part of IT solution is Web service such as Bing API or Google API. They help us to build the kernel. Also the paper demonstrates the list of future tasks that should be solved and the example of real project of proposed approach.В статье представлена новая технология «управление знаниями о виртуальном продвижении» в среде Интернет. Обычно реальный продукт или услуга характеризуются четырьмя компонентами (продукт, цена, продвижение и место) в соответствии с теорией маркетинга. Одним из компонентов является продвижение товара. Однако сейчас этот элемент становится полностью виртуальным инструментом. Необходимо рассматривать продвижение продукта как отображение или копию реального продукта в виртуальном пространстве. Это отражение существует параллельно в сети и непосредственно влияет на реальный продукт или услугу. Поэтому целью статьи является презентация нового объекта исследования, основанного на опыте более тридцати реальных проектов, выполненных в Украине, США, Европе и Канаде. Мы рассматриваем продвижение как программный продукт, который работает в соответствии с принципами управления знаниями и машинного обучения. Предполагается, что виртуальное продвижение характеризуется четырьмя репрезентациями: клиент или пользова тель, данные, технологии и маркетинг. Далее была представлена структура бизнес–процесса виртуального продвижения. Он включает четыре этапа: выбор источников гипертекста, представление и извлечение знаний, построение семантического ядра и оценка критерия качества для остановки процесса. На основе структуры процесса были определены задачи исследования. Центральная задача – формирование семантического ядра. Затем была разработана архитектура программного обеспечения. ИТ решение содержит CRM систему в качестве инструмента учета и Веб сайт как образ виртуального продвижения. CRM играет главную роль командного центра. Здесь мы формируем семантическое ядро и затем отправляем его через маркетинговые каналы, такие как Веб сайт, Телеграмм каналы или профили в Вайбере. Другая часть ИТ решения – это Веб сервис, такой как Bing API или Google API. Они помогают нам построить ядро. Также в статье приведен список будущих задач, которые необходимо решить, и пример реальных проектов в рамках предложенного подхода.У статті представлена нова технологія «управління знаннями про віртуальне просування» в мережі Інтернет. Звичайно реальний продукт або послуга характеризуються наступними компонентами: продукт, ціна, просування і місце, згідно з теорією маркетингу. Одним з компонентів є просування товару. Однак зараз цей елемент стає повністю віртуальним інструментом. Необхідно розглядати просування продукту як відображення або копію реального продукту у віртуальному просторі. Це відображення існує паралельно у мережі та безпосередньо впливає на реальний продукт чи послугу. Тому ціллю статті є презентація нового об’єкта дослідження, поява якого основана на досвіді виконання більш ніж тридцяти реальних проектів в Україні, США, Європі та Канаді. Ми працюємо відповідно до принципів управління знаннями і машинного навчання. Передбачається, що віртуальне просування характеризується чотирма репрезентаціями: клієнт або користувач, дані, технологія та маркетинг. Далі була представлена структура бізнес–процесу віртуального просування. Він включає чотири етапи: вибір джерел гіпертексту, подання та витяг знань, побудова семантичного ядра і оцінка критерію якості для зупинки процесу. На основі структури процесу були визначені задачі дослідження. Центральна задача – формування семантичного ядра. Потім була розроблена архітектура програмного забезпечення. ІТ рішення містить CRM систему в якості інструменту обліку та Веб сайт як образ віртуального просування. CRM грає роль командного центру. Тут формується семантичне ядро і потім відправляється через маркетингові канали, такі як Веб сайт, Телеграм канали або профілі в Вайбері. Інша частина ІТ рішення – це Веб сервіс, такий як Bing API або Google API. Вони допомагають нам побудувати ядро. Також в статті наведено список майбутніх завдань, які необхідно вирішити, і приклад реальних проектів в рамках запропонованого підходу",ТЕХНОЛОГІЯ УПРАВЛІННЯ ЗНАННЯМИ ПРО ВІРТУАЛЬНЕ ПРОСУВАННЯ,,"NTU ""KhPI""",,,core
357559749,2020-04-24T00:00:00,"ABSTRACT Ground is an open-source data context service, a system to manage all the information that informs the use of data. Data usage has changed both philosophically and practically in the last decade, creating an opportunity for new data context services to foster further innovation. In this paper we frame the challenges of managing data context with basic ABCs: Applications, Behavior, and Change. We provide motivation and design guidelines, present our initial design of a common metamodel and API, and explore the current state of the storage solutions that could serve the needs of a data context service. Along the way we highlight opportunities for new research and engineering solutions. FROM CRISIS TO OPPORTUNITY Traditional database management systems were developed in an era of risk-averse design. The technology itself was expensive, as was the on-site cost of managing it. Expertise was scarce and concentrated in a handful of computing and consulting firms. Two conservative design patterns emerged that lasted many decades. First, the accepted best practices for deploying databases revolved around tight control of schemas and data ingest in support of general-purpose accounting and compliance use cases. Typical advice from data warehousing leaders held that &quot;There is no point in bringing data . . . into the data warehouse environment without integrating it&quot;  As computing and data have become orders of magnitude more efficient, changes have emerged for both of these patterns. Usage is changing profoundly, as expertise and control shifts from the central accountancy of an IT department to the domain expertise of &quot;business units&quot; tasked with extracting value from data  Crisis: Big Metadata An unfortunate consequence of the disaggregated nature of contemporary data systems is the lack of a standard mechanism to assemble a collective understanding of the origin, scope, and usage of the data they manage. In the absence of a better solution to this pressing need, the Hive Metastore is sometimes used, but it only serves simple relational schemas-a dead end for representing a Variety of data. As a result, data lake projects typically lack even the most rudimentary information about the data they contain or how it is being used. For emerging Big Data customers and vendors, this Big Metadata problem is hitting a crisis point. Two significant classes of end-user problems follow directly from the absence of shared metadata services. The first is poor productivity. Analysts are often unable to discover what data exists, much less how it has been previously used by peers. Valuable data is left unused and human effort is routinely duplicated-particularly in a schema-on-use world with raw data that requires preparation. &quot;Tribal knowledge&quot; is a common description for how organizations manage this productivity problem. This is clearly not a systematic solution, and scales very poorly as organizations grow. The second problem stemming from the absence of a system to track metadata is governance risk. Data management necessarily entails tracking or controlling who accesses data, what they do with it, where they put it, and how it gets consumed downstream. In the absence of a standard place to store metadata and answer these questions, it is impossible to enforce policies and/or audit behavior. As a result, many administrators marginalize their Big Data stack as a playpen for non-critical data, and thereby inhibit both the adoption and the potential of new technologies. In our experiences deploying and managing systems in production, we have seen the need for a common service layer to support the capture, publishing and sharing of metadata information in a flexible way. The effort in this paper began by addressing that need. Opportunity: Data Context The lack of metadata services in the Big Data stack can be viewed as an opportunity: a clean slate to rethink how we track and leverage modern usage of data. Storage economics and schema-on-use agility suggest that the Data Lake movement could go much farther than Data Warehousing in enabling diverse, widely-used central repositories of data that can adapt to new data formats and rapidly changing organizations. In that spirit, we advocate rethinking traditional metadata in a far more comprehensive sense. More generally, what we should strive to capture is the full context of data. To emphasize the conceptual shifts of this data context, and as a complement to the &quot;three Vs&quot; of Big Data, we introduce three key sources of information-the ABCs of Data Context. Each represents a major change from the simple metadata of traditional enterprise data management. Applications: Application context is the core information that describes how raw bits get interpreted for use. In modern agile scenarios, application context is often relativistic (many schemas for the same data) and complex (with custom code for data interpretation). Application context ranges from basic data descriptions (encodings, schemas, ontologies, tags), to statistical models and parameters, to user annotations. All of the artifacts involved-wrangling scripts, view definitions, model parameters, training sets, etc.-are critical aspects of application context. Behavior: This is information about how data was created and used over time. In decoupled systems, behavioral context spans multiple services, applications and formats and often originates from highvolume sources (e.g., machine-generated usage logs). Not only must we track upstream lineage-the data sets and code that led to the creation of a data object-we must also track the downstream lineage, including data products derived from this data object. Aside from data lineage, behavioral context includes logs of usage: the &quot;digital exhaust&quot; left behind by computations on the data. As a result, behavioral context metadata can often be larger than the data itself. Change: This is information about the version history of data, code and associated information, including changes over time to both structure and content. Traditional metadata focused on the present, but historical context is increasingly useful in agile organizations. This context can be a linear sequence of versions, or it can encompass branching and concurrent evolution, along with interactions between co-evolving versions. By tracking the version history of all objects spanning code, data, and entire analytics pipelines, we can simplify debugging and enable auditing and counterfactual analysis. Data context services represent an opportunity for database technology innovation, and an urgent requirement for the field. We are building an open-source data context service we call Ground, to serve as a central model, API and repository for capturing the broad context in which data gets used. Our goal is to address practical problems for the Big Data community in the short term and to open up opportunities for long-term research and innovation. In the remainder of the paper we illustrate the opportunities in this space, design requirements for solutions, and our initial efforts to tackle these challenges in open source. DIVERSE USE CASES To illustrate the potential of the Ground data context service, we describe two concrete scenarios in which Ground can aid in data discovery, facilitate better collaboration, protect confidentiality, help diagnose problems, and ultimately enable new value to be captured from existing data. After presenting these scenarios, we explore the design requirements for a data context service. Scenario: Context-Enabled Analytics This scenario represents the kind of usage we see in relatively technical organizations making aggressive use of data for machinelearning driven applications like customer targeting. In these organizations, data analysts make extensive use of flexible tools for data preparation and visualization and often have some SQL skills, while data scientists actively prototype and develop custom software for machine learning applications. Janet is an analyst in the Customer Satisfaction department at a large bank. She suspects that the social network behavior of customers can predict if they are likely to close their accounts (customer churn). Janet has access to a rich context-service-enabled data lake and a wide range of tools that she can use to assess her hypothesis. Janet begins by downloading a free sample of a social media feed. She uses an advanced data catalog application (we&apos;ll call it &quot;Catly&quot;) which connects to Ground, recognizes the content of her sample, and notifies her that the bank&apos;s data lake has a complete feed from the previous month. She then begins using Catly to search the lake for data on customer retention: what is available, and who has access to it? As Janet explores candidate schemas and data samples, Catly retrieves usage data from Ground and notifies her that Sue, from the data-science team, had previously used a database table called cust_roster as input to a Python library called cust_churn. Examining a sample from cust_roster and knowing of Sue&apos;s domain expertise, Janet decides to work with that table in her own churn analysis. Having collected the necessary data, Janet turns to a data preparation application (&quot;Preply&quot;) to clean and transform the data. The social media data is a JSON document; Preply searches Ground for relevant wrangling scripts and suggests unnesting attributes and pivoting them into tables. Based on security information in Ground, Preply warns Janet that certain customer attributes in her table are protected and may not be used for customer retention analysis. Finally, to join the social media names against the customer names, Preply uses previous wrangling scripts registered with Ground by other analysts to extract standardized keys and suggest join conditions to Janet. Having prepared the data, Janet loads it into her BI charting tool and discovers a strong correlation between customer churn and social sentiment. Janet uses the &quot;share&quot; feature of the BI tool to send it to Sue; the tool records the share in Ground. Sue has been working on a machine learning pipeline for automated discount targeting. Janet&apos;s chart has useful features, so Sue consults Ground to find the input data. Sue joins Janet&apos;s dataset into her existing training data but discovers that her pipeline&apos;s prediction accuracy decreases. Examining Ground&apos;s schema for Janet&apos;s dataset, Sue realizes that the sentiment column is categorical and needs to be pivoted into indicator columns isPositive, isNegative, and isNeutral. Sue writes a Python script to transform Janet&apos;s data into a new file in the required format. She trains a new version of the targeting model and deploys it to send discount offers to customers at risk of leaving. Sue registers her training pipeline including Janet&apos;s social media feeds in the daily build; Ground is informed of the new code versions and service registration. After several weeks of improved predictions, Sue receives an alert from Ground about changes in Janet&apos;s script; she also sees a notable drop in prediction accuracy of her pipeline. Sue discovers that some of the new social media messages are missing sentiment scores. She queries Ground for the version of the data and pipeline code when sentiment scores first went missing. Upon examination, she sees that the upgrade to the sentiment analysis code produced new categories for which she doesn&apos;t have columns (e.g., isAngry, isSad, . . . ). Sue uses Ground to roll back the sentiment analysis code in Janet&apos;s pipeline and re-run her pipeline for the past month. This fixes Sue&apos;s problem, but Sue wonders if she can simply roll back Janet&apos;s scripts in production. Consulting Ground, Sue discovers that other pipelines now depend upon the new version of Janet&apos;s scripts. Sue calls a meeting with the relevant stakeholders to untangle the situation. Throughout our scenario, the users and their applications benefited from global data context. Applications like Catly and Preply were able to provide innovative features by mining the &quot;tribal knowledge&quot; captured in Ground: recommending datasets and code, identifying experts, flagging security concerns, notifying developers of changes, etc. The users were provided contextual awareness of both technical and organizational issues and able to interrogate global context to understand root causes. Many of these features exist in isolated applications today, but would work far better with global context. Data context services make this possible, opening up opportunities for innovation, efficiency and better governance. Scenario: Big Data in Enterprise IT Many organizations are not as technical as the one in our previous scenario. We received feedback on an early draft of this paper from an IT executive at a global financial services firm (not affiliated with the authors), who characterized both Janet and Sue as &quot;developers&quot; not analysts. (&quot;If she knows what JSON is, she&apos;s a developer!&quot;) In his organization, such developers represent less than 10% of the data users. The remaining 90% interact solely with graphical interfaces. However, he sees data context offering enormous benefits to his organization. Here we present an illustrative enterprise IT scenario. Mark is an Data Governance manager working in the IT department of a global bank. He is responsible for a central data warehouse, and the legacy systems that support it, including ExtractTransform-Load (ETL) mappings for loading operational databases into the warehouse, and Master Data Management (MDM) systems for governing the &quot;golden master&quot; of various reference data sets (customers, partner organizations, and so on.) Recently, the bank decided to migrate off of these systems and onto a Big Data stack, to accomodate larger data volumes and greater variety of data. In so doing, they rewrote many of their workflows; the new workflows register their context in Ground. Sara is an analyst in the bank&apos;s European Compliance office; she uses Preply to prepare monthly reports for various national governments demonstrating the firm&apos;s compliance with regulations like Basel III [33]. As Sara runs this month&apos;s AssetAllocation report, she sees that a field called IPRE_AUSNZ came back with a very small value relative to other fields prefixed with IPRE. She submits a request to the IT department&apos;s trouble ticket system (&quot;Helply&quot;) referencing the report she ran, asking &quot;What is this field? What are the standard values? If it is unusual, can you help me understand why?&quot; Mark receives the ticket in his email, and Helply stores an association in Ground between Sara and AssetAllocation. Mark looks in Ground at summary statistics for the report fields over time, and confirms that the value in that field is historically low by an order of magnitude. Mark then looks at a &quot;data dictionary&quot; of reference data in Ground and sees that IPRE was documented as &quot;Income-Producing Real Estate&quot;. He looks at lineage data in Ground and finds that the IPRE_AUSNZ field in the report is calculated by a SQL view aggregating data from both Australia and New Zealand. He also looks at version information for the view behind AssetAllocation, and finds that the view was modified on the second day of the month to compute two new fields, IPRE_AUS and IPRE_NZ that separate the reporting across those geographies. Mark submits a response in Helply that explains this to Sara. Armed with that information, Sara uses the Preply UI to sum all three fields into a single cell representing the IPRE calculation for the pair of countries over the course of the full month. Based on the Helply association, Sara is subscribed automatically to an RSS feed associated with AssetAllocation. In future, Sara will automatically learn about changes that affect the report, thanks to the the new workloads from Mark&apos;s team that autogenerate data lineage in Ground. Mark&apos;s team takes responsibility for upstream reporting of version changes to data sources (e.g. reference data) and code (ETL scripts, warehouse queries, etc), as well as the data lineage implicit in that code. Using that data lineage, a script written by Mark&apos;s team auto-computes downstream Helply alerts for all data products that depend transitively on a change to upstream data and scripts. In this scenario, both the IT and business users benefit from various kinds of context stored in Ground, including statistical data profiles, data dictionaries, field-level data lineage, code version history, and (transitive) associations between people, data, code and their versions. Our previous data science use cases largely exploited statistical and probabilistic aspects of context (correlations, recommendations); in this scenario, the initial motivation was quantitative, but the context was largely used in more deterministic and discrete ways (dependencies, definitions, alerts). Over time time, we believe organizations will leverage data context using both deterministic and probabilistic approaches. DESIGN AND ARCHITECTURE In a decoupled architecture of multiple applications and backend services, context serves as a &quot;narrow waist&quot;-a single point of access for the basic information about data and its usage. It is hard to anticipate the breadth of applications that could emerge. Hence we were keen in designing Ground to focus on initial decisions that could enable new services and applications in future. Design Requirements In our design, we were guided by Postel&apos;s Law of Robustness from Internet architecture: &quot;Be conservative in what you do, be liberal in what you accept from others.&quot; Guided by this philosophy, we identified four central design requirements for a successful data context service. Model-Agnostic. For a data context service to be broadly adopted, it cannot impose opinions on metadata modeling. Data models evolve and persist over time: modern organizations have to manage everything from COBOL data layouts to RDBMS dumps to XML, JSON, Apache logs and free text. As a result, the context service cannot prescribe how metadata is modeled-each dataset may have different metadata to manage. This is a challenge in legacy &quot;master data&quot; systems, and a weakness in the Big Data stack today: Hive Metastore captures fixed features of relational schemas; HDFS captures fixed features of files. A key challenge in Ground is to design a core metamodel that captures generic information that applies to all data, as well as custom information for different data models, applications, and usage. We explore this issue in Section 3.3. Immutable. Data context must be immutable; updating stored context is tantamount to erasing history. There are multiple reasons why history is critical. The latest context may not always be the most relevant: we may want to replay scenarios from the past for what-if analysis or debugging, or we may want to study how context information (e.g., success rate of a statistical model) changes over time. Prior context may also be important for governance and veracity purposes: we may be asked to audit historical behavior and metadata, or reproduce experimental results published in the past. This simplifies record-keeping, but of course it raises significant engineering challenges. We explore this issue in Section 4. Scalable. It is a frequent misconception that metadata is small. In fact, metadata scaling was already a challenge in previousgeneration ETL technology. In many Big Data settings, it is reasonable to envision the data context being far larger than the data 3) is at the center, supported by a set of swappable underground services. The system is intended to support a growing set of aboveground applications, examples of which are shown. Ground is decoupled from applications and services via asynchronous messaging services. Our initial concrete instantiation of this architecture, Ground 0, is described in Section 4. itself. Usage information is one culprit: logs from a service can often outstrip the data managed by the service. Another is data lineage, which can grow to be extremely large depending on the kind of lineage desired  Politically Neutral. Common narrow-waist service like data context must interoperate with a wide range of other services and systems designed and marketed by often competing vendors. Customers will only adopt and support a central data context service if they feel no fear of lock-in; application writers will prioritize support for widely-used APIs to maximize the benefit of their efforts. It is important to note here that open source is not equivalent to political neutrality; customers and developers have to believe that the project leadership has strong incentives to behave in the common interest. Based on the requirements above, the Ground architecture is informed by Postel&apos;s Law of Robustness and the design pattern of decoupled components. At its heart is a foundational metamodel called Common Ground with an associated aboveground API for data management applications like the catalog and wrangling examples above. The core functions underneath Ground are provided by swappable component services that plug in via the underground API. A sketch of the architecture of Ground is provided in  Key Services Ground&apos;s functionality is backed by five decoupled subservices, connected via direct REST APIs and a message bus. For agility, we are starting the project using existing open source solutions for each service. We anticipate that some of these will require additional featu",Ground: A Data Context Service,,,,,core
346584168,2020-01-01T00:00:00,"Optics is a promising platform in which to help realize the next generation of fast, parallel, and energy-efficient computation. We demonstrate a reconfigurable free-space optical multiplier that is capable of over 3000 computations in parallel, using spatial light modulators with a pixel resolution of only 340×340. This enables vector–matrix multiplication and parallel vector–vector multiplication with vector size of up to 56. Our design is, to the best of our knowledge, the first to simultaneously support optical implementation of reconfigurable, large-sized, and real-valued linear algebraic operations. Such an optical multiplier can serve as a building block of special-purpose optical processors such as optical neural networks and optical Ising machines",Fully reconfigurable coherent optical vector–matrix multiplication,,Optical Society of America,,,core
288813933,2020-03-27T00:00:00,"Η εκτίμηση του Κέντρου Μάζας (CoM) διαδραματίζει κρίσιμο ρόλο στη ρομποτική βάδιση. Οι περισσότεροι σχεδιαστές κίνησης και ελεγκτές βάδισης πραγματικού χρό¬νου υποθέτουν ότι η θέση και η ταχύτητα του CoM είναι διαθέσιμες για ανατροφοδό¬τηση ανά πάσα στιγμή. Σε αυτή τη διατριβή παρουσιάζουμε έναν από τους πρώτους τρισδιάστατους εκτιμητές κατάστασης CoM για το περπάτημα των ανθρωποειδών ρομπότ. Ο προτεινόμενος εκτιμητής συνδυάζει αποτελεσματικά τις μετρήσεις από αισθητήρες πίεσης στα πόδια, κωδικοποιητές στις αρθρώσεις και αδρανειακής μο¬νάδας (IMU) στο σώμα με ένα Εκτεταμένο Φίλτρο Κάλμαν (EKF) για την ακριβή εκτίμηση τόσο της θέσης και της ταχύτητας του CoM αλλά και των εξωτερικών δυ¬νάμεων που δρουν πάνω σε αυτό. Επιπλέον, λαμβάνει υπόψιν την ανωμαλότητα του εδάφους και την στροφορμή του σώματος με αποτέλεσμα να συνδυάζει το μετωπικό με το πλευρικό επίπεδο κίνησης, χωρίς να βασίζεται σε αισθητήρες δύναμης / ροπής (F/T) στα πόδια.
Ωστόσο, είναι κοινή πρακτική να επιχειρείται η μετατροπή των μετρήσεων σε ένα αδρανειακό σύστημα αναφοράς ώστε η εκτίμηση του CoM να γίνεται σε σχέση με αυτό. Κατά συνέπεια, για την επίτευξη του παραπάνω είναι υποχρεωτικό να συνε¬κτιμηθούν η βάση και το πόδι στήριξης του ρομπότ. Για το σκοπό αυτό, επεκτείνουμε έναν καθιερωμένο στη βιβλιογραφία εκτιμητή αιωρούμενης μάζας με τη δυναμική του ποδιού στήριξης χρησιμοποιώντας μετρήσεις κινηματικής και αδρανειακής μονάδας με το Φίλτρο Κάλμαν Σφάλματος Κατάστασης (ESKF) για την κατάλληλη διαχείριση της υπερ-παραμετροποίησης των περιστροφών. Με αυτό το τρόπο,δημιουργείται ένα σύστημα σειριακής εκτίμησης κατάστασης που αποτελείται από έναν εκτιμητή βάσης και έναν εκτιμητή CoM το οποίο ονομάζουμε State Estimation RObot Walking (SEROW). Επιπλέον, για να διορθώσουμε την κινηματική απόκλιση που προκαλείται από την ολίσθηση των ποδιών κατά το περπάτημα, χρησιμοποιούμε μετρήσεις Οπτι¬κής Οδομετρίας (VO) και/ή Οδομετρίας LIDAR (LO). Δυστυχώς, τέτοιες μετρήσεις υποφέρουν από ακραίες τιμές σε ένα δυναμικό περιβάλλον, αφού κατά τον υπολογι¬σμό τους χρησιμοποιείται η υπόθεση ότι μόνο το ρομπότ βρίσκεται σε κίνηση και ο κόσμος γύρω του είναι στατικός. Για αυτό το λόγο, εισάγουμε το Σθεναρό Γκαουσια-νό Φίλτρο Κάλμαν Σφάλματος Κατάστασης (RGESKF) για την αυτόματη ανίχνευση και απόρριψη των ακραίων μετρήσεων. Το προτεινόμενο φίλτρο δεν βασίζεται σε πρότερη γνώση σχετικά με τις κατανομές των μετρήσεων και δεν χρησιμοποιεί ειδι¬κά ρυθμισμένα κατώφλια. Ως εκ τούτου,το SEROW γίνεται ένα σθεναρό σύστημα εκτίμησης κατάστασης, κατάλληλο για δυναμικά ανθρώπινα περιβάλλοντα. Προ¬κειμένου να ενισχυθούν περαιτέρω οι ερευνητικές προσπάθειες, οο SEROW δίνεται ελεύθερα στη ρομποτική κοινότητα ως ένα πακέτο ROS/C++ ανοικτού κώδικα.
Τα σύγχρονα συστήματα ελέγχου και εκτίμησης κατάστασης ανθρωποειδών ρο¬μπότ υποθέτουν ότι η κατάσταση επαφής ποδιών-εδάφους είναι γνωστή εκ των προ¬τέρων. Η ανίχνευση τέτοιων επαφών είναι ένα σημαντικό και σε μεγάλο βαθμό ανεξερεύνητο θέμα στη σύγχρονη ρομποτική έρευνα. Σε αυτή τη διατριβή, διατυ¬πώνουμε μια ευρύτερη ερώτηση: σε ποια φάση βάδισης βρίσκεται το ρομπότ; Ποςς το σκοπό αυτό, προτείνουμε ένα ολιστικό πλαίσιο βασισμένο σε μη-επιβλεπόμενη μάθηση από δεδομένα ιδιοδεκτικής αίσθησης που αντιμετωπίζει με ακρίβεια και α¬ποτελεσματικότητα αυτό το πρόβλημα. Συγκεκριμένα, ανιχνεύουμε με ακρίβεια μια από τις τρεις φάσεις βάδισης, την Αριστερή Υποστήριξη (LSS), την Διπλή Υποστήριξη (DS) και τη Δεξιά Υποστήριξη (RSS), χρησιμοποιώντας μετρήσεις από κωδικοποιητές, IMU και F/T. Αρχικά, πραγματοποιείται μείωση των διαστάσεων με Ανάλυση Κύριων Στοιχείων (PCA) ή με αυτόματους κωδικοποιητές ώστε να εξαχθούν χρήσιμα χαρα¬κτηριστικά, μια συμπαγής αναπαράσταση και να μειωθεί ο θόρυβος στα δεδομένα. Στη συνέχεια, πραγματοποιείται μια ομαδοποίηση στον χώρο χαμηλών διαστάσεων με Γκαουσιανά Μοντέλα Μίγματος (GMMs). Ως αποτέλεσμα λαμβάνονται τρία πυ¬κνά συμπλέγματα που αντιστοιχούν στις φάσεις της βάδισης. Αυτό σημαίνει ότι η δυναμική της φάσης του βαδίσματος είναι χαμηλής διάστασης το οποίο λειτουργεί ως άλλη μια ένδειξη στο ότι ολόκληρη η διαδικασία της βάδισης είναι χαμηλής διά¬στασης. Επιπλέον, δεδομένου ότι το προτεινόμενο πλαίσιο χρησιμοποιεί μετρήσεις από αισθητήρες που είναι συνήθως διαθέσιμοι στα σημερινά ανθρωποειδή ρομπότ, προσφέρουμε στη ρομποτική κοινότητα το Gait-Phase Estimation Module (GEM), μια ανοικτού κώδικα εφαρμογή σε ROS/Python.
Το SEROW και το GEM έχουν αξιολογηθεί ποσοτικά και ποιοτικά αφορικά με την ακρίβεια και την αποδοτικότητα τους τόσο σε προσομοίωση όσο και σε πραγματικές συνθήκες.Αρχικά , χρησιμοποιήθηκε ένα προσομοιωμένο ρομπότ στο MATLAB και το ανθρωποειδές ρομπότ Valkyrie της NASA στο ROS/Gazebo για να τεκμηριωθούν τα προτεινόμενα σχήματα στο βάδισμα πάνω σε ανομοιόμορφο/ανώμαλο έδαφος. τηη συνέχεια, τα προτεινόμενα σχήματα ενσωματώθηκαν στο α) μικρού μεγέθους ανθρω-ποειδές ρομπότ NAO v4.0 και β) στο πλήρους μεγέθους ανθρωποειδές WALK-MAN v2.0 για περεταίρω πειραματική επικύρωση. Με το NAO, οο SEROW εφαρμόστηκε στο ρομπότ για να παράσχει την απαραίτητη ανατροφοδότηση στον σχεδιασμό της κίνησης και τη σταθεροποίηση του βηματισμού σε πραγματικό χρόνο. Με αυτό το τρόπο επιτεύχθηκε πολυκατευθυντική βάδιση ακόμη και σε εξωτερικά/ανομοιογενή εδάφη. Επιπλέον,το SEROW χρησιμοποιήθηκε στον σχεδιασμό βημάτων για την πλοήγηση και επίσης στο Visual SLAM με το ίδιο ρομπότ. Όσον αφορά το WALK¬MAN v2.0, το SEROW εφαρμόστηκε με δεδομένα κινηματικής, αδρανειακής μονάδας και F/T για να παρέχει ανατροφοδότηση βάσης και CoM σε πραγματικό χρόνο. Στην εκτίμηση λήφθηκε υπόψη και το VO για την διόρθωση της κινηματικής απόκλισης κατά το περπάτημα. Με αυτό το τρόπο διευκολύνεται σημαντικά ο πιθανός σχεδια¬σμός βημάτων. Τλοςς, το GEM χρησιμοποιήθηκε επίσης για την εκτίμηση της φάσης της βάδισης στο δυναμικό περπάτημα του WALK-MAN.
Συνοψίζοντας, σε αυτή τη διατριβή προτείνεται ένας σθεναρός μη-γραμμικός ε¬κτιμητής κατάστασης για το βάδισμα ανθρωποειδών ρομπότ. Παρόλα αυτά, το προ¬τεινόμενο σύστημα μπορεί εύκολα να επεκταθεί και σε άλλους τύπους ρομπότ με πόδια, όπως τα τετράποδα, μιας και διαθέτουν τις ίδιες βασικές αρχές κίνησης.Center of Mass (CoM) estimation realizes a crucial role in legged locomotion. Most walking
pattern generators and real-time gait stabilizers commonly assume that the CoM position
and velocity are available for feedback. In this thesis we present one of the first
3D-CoM state estimators for humanoid robot walking. The proposed estimation scheme
fuses effectively joint encoder, inertial, and feet pressure measurements with an Extended
Kalman Filter (EKF) to accurately estimate the 3D-CoM position, velocity, and external
forces acting on the CoM. Furthermore, it directly considers the presence of uneven terrain
and the body’s angular momentum rate and thus effectively couples the frontal with
the lateral plane dynamics, without relying on feet Force/Torque (F/T) sensing.
Nevertheless, it is common practice to transform the measurements to a world frame
of reference and estimate the CoM with respect to the world frame. Consequently, the
robot’s base and support foot pose are mandatory and need to be co-estimated. To this
end, we extend a well-established in literature floating mass estimator to account for the
support foot dynamics and fuse kinematic-inertial measurements with the Error State
Kalman Filter (ESKF) to appropriately handle the overparametrization of rotations. In
such a way, a cascade state estimation scheme consisting of a base and a CoM estimator
is formed and coined State Estimation RObot Walking (SEROW). Additionally, we employ
Visual Odometry (VO) and/or LIDAR Odometry (LO) measurements to correct the kinematic
drift caused by slippage during walking. Unfortunately, such measurements suffer
from outliers in a dynamic environment, since frequently it is assumed that only the
robot is inmotion and the world around is static. Thus, we introduce the Robust Gaussian
ESKF (RGESKF) to automatically detect and reject outliers without relying on any prior
knowledge on measurement distributions or finely tuned thresholds. Therefore, SEROW
is robustified and is suitable for dynamic human environments. In order to reinforce further
research endeavors, SEROW is released to the robotic community as an open-source
ROS/C++ package.
Up to date control and state estimation schemes readily assume that feet contact status
is known a priori. Contact detection is an important and largely unexplored topic in
contemporary humanoid robotics research. In this thesis, we elaborate on a broader question:
in which gait phase is the robot currently in? To this end, we propose a holistic framework
based on unsupervised learning from proprioceptive sensing that accurately and efficiently
addresses this problem. More specifically, we robustly detect one of the three gaitphases,
namely Left Single Support (LSS), Double Support (DS), and Right Single Support (RSS) utilizing joint encoder, IMU, and F/T measurements. Initially, dimensionality reduction
with Principal Components Analysis (PCA) or autoencoders is performed to extract
useful features, obtain a compact representation, and reduce the noise. Next, clustering
is performed on the low-dimensional latent space with GaussianMixtureModels (GMMs)
and three dense clusters corresponding to the gait-phases are obtained. Interestingly, it is
demonstrated that the gait phase dynamics are low-dimensional which is another indication
pointing towards locomotion being a low dimensional skill. Accordingly, given that
the proposed framework utilizes measurements fromsensors that are commonly available
on humanoids nowadays, we offer the Gait-phase Estimation Module (GEM), an opensource
ROS/Python implementation to the robotic community.
SEROW and GEM have been quantitatively and qualitatively assessed in terms of accuracy
and efficiency both in simulation and under real-world conditions. Initially, a simulated
robot in MATLAB and NASA’s Valkyrie humanoid robot in ROS/Gazebo were employed
to establish the proposed schemes with uneven/rough terrain gaits. Subsequently,
the proposed schemes were integrated on a) the small size NAO humanoid robot v4.0 and
b) the adult size WALK-MAN v2.0 for experimental validation. With NAO, SEROW was implemented
on the robot to provide the necessary feedback for motion planning and realtime
gait stabilization to achieve omni-directional locomotion even on outdoor/uneven
terrains. Additionally, SEROW was used in footstep planning and also in Visual SLAM
with the same robot. Regarding WALK-MAN v2.0, SEROW was executed onboard with
kinematic-inertial and F/T data to provide base and CoM feedback in real-time. Furthermore,
VO has also been considered to correct the kinematic drift while walking and facilitate
possible footstep planning. GEM was also employed to estimate the gait phase in
WALK-MAN’s dynamic gaits.
Summarizing, a robust nonlinear state estimator is proposed for humanoid robot walking.
Nevertheless, this scheme can be readily extended to other type of legged robots such as quadrupeds, since they share the same fundamental principles",Σθεναρή μη γραμμική εκτίμηση κατάστασης ανθρωποειδών ρομπότ,,,,,core
395634719,2020-08-03T00:00:00,"International audienceWith the rapid advancement of power electronic technologies and the reduction of photovoltaic cell price, the share of solar energy in the total power production has been booming recently. On the one hand, the increase in the amount of power delivered by solar energy can be beneficial in many economic and environmental aspects. On the other hand, this can cause various technical challenges to network operators. One of these issues is related to classifying faults located in distribution networks with high penetration of photovoltaic systems. Although many studies have paid significant attention to developing new algorithms applicable for a more active today distribution networks, there is still space for other improvements. Hence, after reviewing stateof-the-art researches, this paper was intended to develop a fault classification that is based on artificial neural networks. In particular, a technique so-called Multiplayer Perceptron Classifier was selected for the proposed algorithm. First, the authors generated a data set for the study by modeling and simulating a real distribution network with practical parameters provided by a local utility in the environment software PowerFactory/DigSILENT. Multiple fault scenarios were simulated. Second, a part of the generated data collection was used for network learning. Finally, the performance of the proposed methodology was demonstrated via testing on the remaining number of generated data",A Fault Classification Method for Medium Voltage Networks with a high Penetration of Photovoltaic Systems using Artificial Neural Networks,,HAL CCSD,,,core
480384343,2020-01-01T00:00:00,"Object detection is a fundamental problem in computer vision and is an essential building block for many applications such as autonomous driving, visual search, and object tracking. Given its large-scale and real-time applications, scalable training and fast inference are critical. Deep neural networks, although powerful in visual recognition, can be computationally expensive. Besides, they introduce shortcomings such as lack of scale-invariance and inaccurate predictions in crowded scenes that can affect detection. This dissertation studies the intrinsic problems which emerge when deep convolutional neural networks are used for object and face detection. We introduce methods to overcome these issues which are not only accurate but also efficient.

First, we focus on the problem of lack of scale-invariance. Performing inference on a multi-scale image pyramid, although effective, increases computation noticeably. Moreover, multi-scale inference really blooms when the model is also trained using expensive multi-scale approaches. As a result, we start by introducing an efficient multi-scale training algorithm called ""SNIPER"" (Scale Normalization for Image Pyramids with Efficient Re-sampling). Based on the ground-truth annotations, SNIPER sparsely samples high-resolution image regions wherever needed. In contrast to training, at inference, there is no ground-truth information to guide region sampling. Thus, we propose ""AutoFocus"". AutoFocus predicts regions to be zoomed-in from low resolutions at inference time, making it possible to skip a large portion of the input pyramid. While being as efficient as single-scale detectors, these methods boost performance noticeably.

Second, we study the problem of efficient face detection. Compared to generic objects, faces are rigid and crowded scenes containing hundreds of faces with extreme scales are more common. In this dissertation, we present ""SSH"" (Single Stage Headless Face Detector). A method that unlike two-stage localization/classification detectors, performs both tasks in a single stage, efficiently models scale variation by design, and removes most of the parameters from its underlying network, but still achieves state-of-the-art results on challenging benchmarks. Furthermore, for the two-stage detection paradigm, we introduce ""FA-RPN"" (Floating Anchor Region Proposal Network). FA-RPN takes the spatial structure of faces into account and allows modification of the prediction density during inference to efficiently deal with crowded scenes.

Finally, we turn our attention to the first step in two-stage localization/classification detectors. While neural networks were deployed for classification, localization was previously solved using classic algorithms which became the bottleneck. To remedy, we propose ""G-CNN"" which models localization as a search in the space of all possible bounding boxes and deploys the same neural network used for classification. Furthermore, for tasks such as saliency detection, where the number of predictions is typically small, we develop an alternative approach that runs at speeds close to 120 frames/second",Efficient Detection of Objects and Faces with Deep Learning,https://core.ac.uk/download/480384343.pdf,'Wiley',10.13016/g6qg-dkfy,,core
294759563,2020-03-31T00:00:00,"У статті досліджено характерні особливості та розкрито зміст проривних технологій на основі реалізації моделі “інвестиції-вплив”. Проаналізовано показники України за світовим рейтингом цифрової конкурентоспроможності станом на 01.01.2016 р. та 01.01.2019 р.. В структурі показника знання простежується погіршення за інститутом навчання, освіта та наукова концентрація; за показником готовності до майбутнього, ситуація незмінна; показник технології, що розкривається через критерії нормативної бази, капіталу та технологічної основи, те ж є сталим.

Вказано компетенції, якими оволодіває індивідуум в ході навчання і роботи на цифровій платформі інноваційно-підприємницького університету, маючи такі фундаментальні знання: критичне мислення, кмітливість, вибір пріоритетів, фільтрація інформації, робота в команді, системне мислення, спілкування.

Аргументовано, що формування ефективно працюючої цифрової економіки можливе за умов напрацювання урядом інструментів наступних інноваційних досягнень: ріст використання Коботів, входження в промисловість штучного інтелекту, автономних речей, входження у промисловість технологій блокчейн.

Запропоновано авторське осмислення цифрової трансформації економіки України крізь зріз віртуально-реального кубічного простору. Визначено важливу роль становлення цифрової економіки, що формує принципово нові бізнес-моделі та постійно удосконалюється, впроваджуючи хмарні технології, штучний інтелект, нову віртуальну реальність, накопичує величезні обсяги даних (Big Data), які при досягненні критичної маси стають важливим її капіталом.

Авторами висловлено думку про те, що пріоритетними напрямами у розвитку цифрової економіки та інноваційно-підприємницьких університетів в ході становлення Індустрії 4.0 і надалі залишаються роботизація виробничих процесів, штучний інтелект, відцифрування інституту освіти та науки, запровадження цифрових технологій на всіх рівнях економічної агрегації, впровадження у промисловість технології блокчейн.The article investigates features and content of breakthrough technologies based on the implementation of the investment-impact model. Indicators of Ukraine according to global digital competitiveness rating as of 01.01.2016 and 01.01.2019 are analyzed. In the structure of knowledge index, the deterioration of the Institute of Education, Education and Scientific Concentration is traced; in terms of readiness for future, the situation is unchanged; the indicator of technology disclosed through the criteria of regulatory framework, capital and technological basis is same.

The competencies that an individual possesses in the course of training and work on digital platform of innovation-entrepreneurial university are indicated, having the following basic knowledge: critical thinking, ingenuity, choice of priorities, filtering information, teamwork, systemic thinking, communication.

It is argued that formation of an effective digital economy is possible under the conditions of government’s development of instruments of the following innovative achievements: growth of the use of Cobots, entry into the artificial intelligence industry, autonomous things, entry into blockchain technology industry.

Author’s understanding of digital transformation of Ukrainian economy through the section of a virtual-real cubic space is offered. The important role of becoming digital economy is defined, which forms fundamentally new business models and is constantly being improved by introducing cloud technologies, artificial intelligence, new virtual reality, and accumulating huge amounts of data (Big Data), which become critical capital when it reaches critical mass.

Authors argue that the priority areas in the development of digital economy and innovation-entrepreneurial universities in the process of becoming Industry 4.0 continue to be the robotization of production processes, artificial intelligence, digitization of the Institute of Education and Science, the introduction of digital technologies at all levels of economic aggregation, blockchain technology industry",Digital economy and innovation-entrepreneurial university in the light of competitiveness,https://core.ac.uk/download/294759563.pdf,Дніпропетровський державний аграрно-економічний університет,,,core
351022005,2020-08-10T07:00:00,"Even before COVID-19, online education is already experiencing high growth and adoption (Erickson & Siau, 2003). Whether it is language application, virtual tutoring, video conferencing tool, or online learning software, there has been a significant surge in usage since COVID-19. In this unprecedented and uncertain time, most people are encouraged to study at home and work from home. On one hand, there are many challenges to online education, especially with the sudden transition. Many instructors and students have little or no training in online education. Other issues such as insufficient bandwidth and missing hardware and software are common. On the other hand, this presents an unforeseen and golden opportunity for a wider student population to experience online education. This will likely change the perception of students on online education and may trigger a wider online education adoption after the pandemic. There are many nice features with online education: (i) Removing the limitation of learning space and time. Online education is open to all people wherever they are and whenever they want to study; (ii) Synchronous teaching provides more opportunities for online students to participate in real-time interaction and to communicate with off-online students. Asynchronous education provides course scheduling flexibility and allows students to progress according to their understanding, mastery of course materials, and internalization of knowledge; (iii) Providing more people with access to education and promoting educational equity. Students from poor areas and developing countries have the opportunity to access high-quality education resources in their countries or any parts of the world at an affordable price (Siau, 2018); (iv) Big data and artificial intelligence can analyze the outcomes of pedagogical activities (Wang & Siau, 2019). This improves the quality of teaching; (v) Students have access to more materials when learning online, such as electronic resource databases at the school/national/public library and Google Scholar; (vi) Better experience with online education because of the availability of comprehensive course materials. For example, arts and science education courses on how to live a healthy life and how to manage time. Online education has its shortcomings as well: (i) Difficult to establish a sense of belonging in an online class which may not be designed to cultivate collective consciousness by emphasizing individual activities; (ii) The learning process requires students to have high self-discipline because they can be easily distracted and attracted by social chats, news, and games; (iii) Lack of emotional engagement between teachers and students, and between students. For those who have access to the right technologies, they can capitalize on the advantages of online education to improve learning efficiency. Although online education is a trend, it is not the only form of future education. With the COVID-19 pandemic, online education is thrust into the limelight and serves a key role during the pandemic. Undoubtedly, online education will become an integral component of education after the pandemic",Online Education During and After COVID-19 Pandemic,https://core.ac.uk/download/351022005.pdf,AIS Electronic Library (AISeL),,,core
322450090,2020-05-01T00:00:00,"Geomagnetic storms resulting from high-speed streams can have significant
negative impacts on modern infrastructure due to complex interactions between
the solar wind and geomagnetic field. One measure of the extent of this effect
is the Kyoto $Dst$ index. We present a method to predict $Dst$ from data
measured at the Lagrange 5 (L5) point, which allows for forecasts of solar wind
development 4.5 days in advance of the stream reaching the Earth. Using the
STEREO-B satellite as a proxy, we map data measured near L5 to the near-Earth
environment and make a prediction of the $Dst$ from this point using the
Temerin-Li $Dst$ model enhanced from the original using a machine learning
approach. We evaluate the method accuracy with both traditional point-to-point
error measures and an event-based validation approach. The results show that
predictions using L5 data outperform a 27-day solar wind persistence model in
all validation measures but do not achieve a level similar to an L1 monitor.
Offsets in timing and the rapidly-changing development of $B_z$ in comparison
to $B_x$ and $B_y$ reduce the accuracy. Predictions of $Dst$ from L5 have an
RMSE of $9$ nT, which is double the error of $4$ nT using measurements
conducted near the Earth. The most useful application of L5 measurements is
shown to be in predicting the minimum $Dst$ for the next four days. This method
is being implemented in a real-time forecast setting using STEREO-A as an L5
proxy, and has implications for the usefulness of future L5 missions.Comment: 20 pages, 6 figures, in press at AGU Space Weathe",Prediction of Dst during solar minimum using in situ measurements at L5,http://arxiv.org/abs/2005.00249,'American Geophysical Union (AGU)',10.1029/2019SW002424,,core
475246404,2020-12-24T00:00:00,"In a cooperative automated driving scenario like platooning, the ego vehicle needs reliable and accurate perception capabilities to autonomously follow the lead vehicle. This paper presents the architecture design and development of an on-board stereo vision system for cooperative automated vehicles. The input to the proposed system is stereo image pairs. It uses three deep neural networks to detect and classify objects, lane markings, and free space boundary simultaneously in front of the ego vehicle. The rectified left and right image frames of the stereo camera are used to compute a disparity map to estimate the detected object’s depth and radial distance. It also estimates the object’s relative velocity, azimuth, and elevation angle with respect to the ego vehicle. It sends the perceived information to the vehicle control system and displays the perceived information in a meaningful way on the human-machine interface. The system runs on both PC (x86_64 architecture) with Nvidia GPU, and the Nvidia Drive PX 2 (aarch64 architecture) automotive-grade compute platform. It is deployed and evaluated on Renault Twizy cooperative automated driving research platform. The presented results show that the stereo vision system works in real-time and is useful for cooperative automated vehicles",Architecture Design and Development of an On-board Stereo Vision System for Cooperative Automated Vehicles,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/itsc45102.2020.9294435,,core
200839244,2020-09-18T00:00:00,"With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy.Comment: Accepted by PoPETs 2021.","The Audio Auditor: User-Level Membership Inference in Internet of Things
  Voice Services",http://arxiv.org/abs/1905.07082,,,,core
333567050,2020-01-01T00:00:00,"In recent year, embedded systems architectures and applications have gained a lot of interest, especially the possibility to add on-bard intelligence has fostered research in several directions, including not only smart IoT and cyber physical systems, but also hot topics such as accelerating deep learning. This special issue contains four papers dealing with architectures and design methodologies to support embedded intelligence, but also providing best practices and software support.



The paper “A Technologically Agnostic Framework for Cyber-Physical and IoT Processing-in-Memory-based Systems Simulation”, by Santos et al., aims to focus on Processing-In-Memory (PIM) as a solution for efficiently processing big data. In particular, this work presents a framework to simulate and automatically generate code for IoT PIM-based systems. Moreover, it proposes an high speed and energy efficient architecture for an IoT PIM system, able to compute a real image recognition application.



The paper “Recommender system implementations for embedded collaborative filtering applications”, by Pajuelo-Holguera et al., aims to propose a complete recommender system implemented on reconfigurable hardware with the purpose of testing on-chip, low-energy embedded collaborative filtering applications. The proposed approach solves any prediction problem based on collaborative filtering by using an off-line, highly-portable light computing environment. Moreover, this work exploits a custom, fine-grained parallel circuit for quick matrix multiplication with floating-point numbers.



The paper “SystemC-based Electronic System-Level Design Space Exploration Environment for Dedicated Heterogeneous Multi-Processor Systems”, by Pomante et al., faces the problem of the Electronic System-Level (ESL) HW/SW co-design of dedicated electronic digital systems based on heterogeneous multiprocessor architectures. In particular, the work presents a prototype SystemC -based environment that exploits a Design Space Exploration (DSE) approach able to suggest an HW/SW partitioning of the system specification and a mapping onto an automatically defined architecture.



The paper “A Fast and Scalable Architecture to Run Convolutional Neural Networks in Low Density FPGAs”, by Véstias et al., deals with efficient configurable architectures for Convolutional Neural Networks (CNN) inference targeting any density FPGAs. The architecture exploits fixed-point arithmetic and image batch to reduce computational, memory and memory bandwidth requirements without compromising network accuracy.



In conclusion, this special issue offers some timely contributions to advance the research of intelligent embedded systems by analyzing both architectures and applications. All of four papers are worth reading and will inspire more interesting ideas and research topics.



We sincerely express our gratitude to the Editor-in-Chief of the journal, Prof. Lech Jozwiak for all the valuable advice and constructive comments. We would also like to thank all the reviewers for their hard work on reviewing the papers. Last but not least, we appreciate all the authors who spent time and effort to respond to this call-for-papers. We truly hope that the readers will enjoy and benefit from this special issue",Guest Editorial: Special Issue on Intelligent Embedded Systems Architectures and Applications (INTESA),,'Elsevier BV',10.1016/j.micpro.2020.103187,,core
357385235,2020-04-11T00:00:00,"ABSTRACT Local injection of glucocorticosteroid (GCS) is an effective treatment of painful conditions in the temporomandibular joint (TMJ). GCS can be administered using anatomical landmarks for orientation or by the use of simultaneous radiographic imaging. In the image guided technique the corticosteroid is mixed with a contrast medium and the injection visualized using radiography. The aim of this prospective pilot study was to compare the treatment effect of intraarticular GCS injection in the TMJ with-and without the use of simultaneous radiographic imaging. 13 patients (9 women and 4 men) with TMJ arthralgia received injection either with or without simultaneous radiographic imaging. Treatment effect was evaluated based on changes in clinical signs and symptoms before and 4-6 weeks after treatment. The symptoms included pain at rest and at jaw function, joint locking, pain index and global improvement. Clinical observations involved TMJ pain to palpation, maximal mouth opening, pain at maximal opening and joint sounds. The main findings were significant decreases in pain index and relief of familiar pain before and after treatment as well as a positive effect on global improvement regardless of administration technique. There were no significant differences between the two methods in treatment outcome. The results suggest that both administration techniques are comparable in treatment effect and should therefore rather be evaluated based on cost-effectiveness and radiation dose. It may be reasonable to apply the image-guided technique mainly when further diagnostic information is needed. 3 INTRODUCTION Terminology and prevalence of temporomandibular disorders Temporomandibular disorder (TMD) is a term used for pain and dysfunction in the temporomandibular joint (TMJ) and jaw muscles. TMD is a common condition, third in prevalence after headache and back pain  According to the guidelines of the National Board of Health and Welfare (NBHW) the treatment need is estimated to be about 5-15% among adults  Etiology of TMD The etiology of TMD is considered to be multifactorial (Carlsson, 1990; Greene, 2001)    and the outcome impairs quality of life. Contributing factors can be predisposing, initiating or maintaining factors. Predisposing factors may increase the risk of developing a condition, such as systemic disease and anatomical factors. Initiating factors may cause onset of a condition, such as trauma. Maintaining factors may contribute to the maintenance of a condition e.g. overload by parafunctional activities or stress, distress and pain  Diagnosis of TMJ pain The Research Diagnostic Criteria for TMD is a widely used diagnostic system (LeResche, 1992). This original system has recently been revised and a new evidence-4 based diagnostic algorithm, Diagnostic Criteria for TMD (DC/ TMD), has been recommended (Shiffaman et al., 2014). Here follows a description of the diagnosis TMJ arthralgia, arthritis and symptomatic arthrosis based on the newly recommended diagnostic criteria. TMJ arthralgia TMJ arthralgia is a painful condition in the TMJ area. It is characterised by pain in the jaw, temple, ear, or in front of the ear, which can be associated with jaw movement, function, or parafunction. Report of familiar pain in the TMJ by palpation of the lateral pole, around the lateral pole, with maximum assisted/ unassisted opening or lateral/ protrusive movement confirms the diagnosis (Shiffaman et al., 2014). TMJ arthritis TMJ arthritis is an inflammatory condition, which involves pain from jaw movements and familiar pain to TMJ palpation. The level of inflammatory mediators in the synovial fluid indicates the degree of inflammation (Kopp et al., 2002). The radiographic image of arthritis is erosive. TMJ arthritis is often associated with a systemic inflammatory joint disease (Tegelberg et al., 1987). Symptomatic TMJ arthrosis TMJ arthrosis is a degenerative disorder characterized by degradation of articular tissue with concomitant osseous changes in the condyle and/ or articular eminence (Shiffaman   et al., 2014)  causing movement related crepitus. The diagnosis can be confirmed by radiological findings such as flattening of the articular surface, sclerosis and osteophytes  Treatment of inflammatory and pain conditions in the TMJ The aim of treatment is to minimize the degree of inflammation and pain, improve TMJ function and to minimize the risk for future onset. Occlusal splint and physical therapy including active or passive jaw movements and relaxation techniques can be used to reduce loading and improve function (NBHW, 2006). Non-steroidal anti-inflammatory 5 agents (NSAIDs) are also a common therapy to reduce pain and inflammation. Systemic disease modifying anti-rheumatic drugs (DMARD) and biologics can be beneficial if the condition is associated with a chronic inflammatory joint disease  Treatment recommendations The NBHW have graded intra-articular corticosteroid injection in the TMJ in relation to the severeness of different diagnosis. The ranking system is also based on scientific evidence of treatment effect and cost-effectiveness. The grade of recommendation for GCS injection in TMJ arthralgia is five where the highest possible rank is three. For TMJ arthritis associated with inflammatory disease the grade of recommendation is three and the highest possible ranking is one. As for symptomatic arthrosis in the TMJ the grade of recommendation is seven and the highest possible ranking is five. The evaluation of the NBHW states that TMJ arthralgia strongly influences oral health. It is associated with severe pain and discomfort and moderately effects jaw function. It may also cause psychosocial impairment. GCS injection is believed to have a moderate effect on pain and maximal opening according to expert group. A two years follow-up study showed that GCS injection in patients with TMJ arthralgia reduced the severity of symptoms and decreased dysfunction index. It also had a positive effect on global improvement (Kopp and Wenneberg, 1981). TMJ arthritis associated with chronic inflammatory joint disease is considered to influence oral health to a large degree and cause severe pain and dysfunction. The condition may have a negative psychosocial effect. There is also a high risk of tissue damage. GCS injection is thought to have a moderate effect on familiar pain, pain at rest and at function, tenderness to palpation and some positive effect on jaw opening and global improvement (Kopp et al., 1991). . 6 Symptomatic TMJ arthrosis is believed to have a mild effect on oral health. The condition is associated to pain, discomfort and impairment of normal jaw function. Locally administered GCS injection is believed to have a moderate effect on pain and a small effect on jaw opening. One relevant clinical trial including patients with arthrosis who received conservative treatment prior to injection shows a reduction in pain intensity by 58% and a 10% increase in maximal opening  Administration of GCS in TMJ Injections can be performed using anatomical landmarks for orientation. GCS is injected in the superior joint space. There is also an image guided technique where the corticosteroid is mixed with contrast medium and the TMJ is visualized using a continuous X-ray beam to generate real time images  Using this technique the corticosteroid is injected in both the superior and inferior joint space. Earlier studies have concluded that the use of fluoroscopy is safe and efficient. It also offers greater control over the procedure, more diagnostic information and is less time consuming  Aim According to a systemic review earlier studies show a wide variation in the outcome of  MATERIALS AND METHODS Study design In this prospective pilot study, the study population comprised patients from two clinics, Clinical oral physiology, Specialist clinic, County of Västerbotten, Umeå and Specialist clinic, Centre of dental competence, County of Norrbotten, Luleå. All patients received treatment with GCS locally injected in the TMJ. In Umeå, the injections were administrated at the Department of Oral Maxillofacial Radiology, guided by fluoroscopy, while the clinic in Luleå used anatomical structures for orientation. Both methods are described in detail below. Data was collected before the injections were made (baseline) and 4-6 weeks after the injection (after). Inclusion criteria were any of the diagnosis TMJ arthralgia, arthritis or symptomatic arthrosis, where corticosteroid injection was the recommended treatment alternative according to the guidelines of the NBHW, as well as pain on the numeric rating scale (NRS 0-10) of 4 or higher (NRS ≥ 4). Study Population The study included a total of 14 treated joints during a time period between 1/9 2013 -3/4 2014 assessed by dental specialists either in Umeå (group A) or Luleå (group B). Group A included eight patients of whom two were excluded due to insufficient data. Six patients remained (4 women and 2 men) where one patient received injection in both TMJs and was therefore counted for two separate joints. Group B included seven patients (5 women and 2 men) apart from one who discontinued the study. The ages ranged between 20-70 years (median age: 52yrs in group A and 54yrs sin group B). A total of three patients had received an injection in the same joint earlier. Two patients in 8 each group had rheumatoid arthritis (RA). In group A four patients were diagnosed with arthrosis and three with arthritis. In group B three patients were diagnosed with arthrosis and four patients with arthralgia. Protocol for data collection Examination protocols were provided to each clinic and the dental specialists were asked to fill in two separate protocols before the injection and at the re-examination 4-6 weeks (according to regular routine) after the injection. The protocols focused on patient subjective symptoms and clinical signs described below. Subjective symptoms All patients were asked if they experienced any of the following (yes/no): They were also asked if they had a chronic inflammatory joint disease and if they had received an injection earlier in the same TMJ. The patients were requested to grade the TMJ pain intensity on numeric rating scale (NRS 0-10). The frequency of the jaw pain was estimated to a value between 0-5 where 0= never, 1= on few occasions, 2= a few times a month, 3= once a week, 4= a few times a week, 5= daily and PI was calculated by multiplying pain intensity and frequency (minimum 0, maximum 50). In addition, the follow-up protocol also inquired the patients to estimate global improvement e.g. the difference before and after treatment as unchanged, slightly better, better, much better or slightly worse, worse or much worse. Clinical signs The jaw function was evaluated based on: The pre-auricular area in front or the ear was cleaned with alcohol.  The injection needle was inserted perpendicular to the skin surface between the posterior margin of the caput mandibulae, anterior to the auricular meatus and inferior to zygomatical arch.  The needle direction was then angled superior and anterior to penetrate the superior articular cavity. Aspiration was performed to ensure correct positioning and 1ml of Depo-Medrol 40mg/ml cum Lidocain 10mg/ml was injected.  After the injection the patient was recommended rest and soft diet. Image guided method for GCS injection in the TMJ (Umeå)  The patient was placed in a side-lying position and the head resting with the TMJ to be injected facing upwards.  The x-ray apparatus was directed toward the TMJ, targeted for optimal visualization.  The patient was covered with a surgical cloth exposing only the area of the TMJ, which was cleaned with alcohol. 10  Local anaesthetic with 1-2 ml Xylocain 10mg/mL was injected in the area posterior to collum mandibuale for nerve block of n. auriculotemporalis.  A solution of 1ml Depo-Medrol 40mg/ml mixed with an equal amount of contrast medium Omnipaque 300mg/ml was injected first in the superior and then the inferior joint cavity.  The total amount of solution injected into the joint does usually not exceed 1.6ml since approximately 0.4ml is left in the extension tube. The clinician can easily monitor the placement of the needle and the distribution of the drug on a screen in real time.  After the injection the patient was recommended rest and soft diet. Literature search Articles were found on PubMed by using the MeSH terms and Boolean operators &quot;corticosteroid injection TMJ&quot; and &quot;TMJ osteoarthritis etiology&quot;. To sort out relevant articles, the search was limited to osteoarthritis and chronic inflammatory joint disease in the TMJ in an adult population and included the prevalence, etiology, epidemiology and pathology of the condition as well as the diagnostic criteria, treatment modalities and their effect. To complement the search studies were found in the Cochrane Library using the terms &quot;TMJ osteoarthritis&quot;. After reading the 16 abstracts the number of articles was narrowed down to 11 and the articles were acquired and read in full texts. Other relevant sources were &quot;Management of temporomandibular disorders and occlusion&quot; by  Ethical considerations This study does not affect the clinician&apos;s choice of treatment. Corticosteroid injections in the TMJ are evaluated and recommended by the NBHW. Although the form of treatment is invasive and can cause pain and discomfort, the purpose of the treatment is to relieve the original pain condition. An important disadvantage with the image guided method for GCS injection in the TMJ is the radiation exposure. Furthermore there is the aspect that either the patient or the clinician may choose a less expensive and perhaps 11 also less effective treatment due to the economic status of the patient. All patients included in the study were informed and have given written consent for participation. The Ethics Forum at the Department of Odontology finds that appropriate ethics considerations have been integrated into this degree project. Statistical analysis The statistical analysis was performed using the SPSS software version 22. The level of significance was set as ≤ 0.05. Binary data e.g. pain at rest, pain at function and pain at maximal opening as well as joint sounds and locking was tested for statistical difference before and after treatment using the McNemar Chi-square test. A paired t-test was used to analyse difference in PI and maximal jaw opening before and after treatment. The same variables were tested for difference between groups using a Mann-Whitney U-test. RESULTS Subjective symptoms The result showed statistically significant decrease in median PI for both groups A and B, respectively, when compared before and after treatment (median=50%, p=0.034 and median=74%, p=0.027) (table 1). There was no statistically significant difference in pain at rest (29%, p=0.500 and 43%, p=0.250), pain at function (14%, p=1 and 57%, p=0.125) or joint locking (75%, p=0.250 and 50%, p= 0.625) before and after treatment  Clinical signs There was a statistically significant change in familiar pain for both groups group A and B, respectively (86%, p= 0.016 and 71%, p=0.023) (table 2). There was no significant reduction in either lateral (75%, p=0.500 and 100% p=0.125) or lateral and posterior pain to palpation (33%, p= 1 and 25%, p=1) when compared before and after treatment  No statistically significant difference was found when comparing the two groups in maximal jaw opening (p=0.942). Global improvement The global improvement showed that the vast majority of the patients (79%) experienced an overall positive treatment outcome effect, of which 57% reported that they felt better or much better after the injection  DISCUSSION One of the main findings in this pilot study was that treatment with intra-articular corticosteroid for patients with TMJ arthralgia resulted in relieved subjective symptoms, as measured by PI and global improvement. There was also a statistically significant reduction of familiar pain, which may indicate that the treatment has a high specificity. There was also a subtle increase in maximum opening, although not statistically significant in both groups. This suggests that regardless of administration technique GCS injection in the TMJ has a clinically verifiable positive treatment effect. When comparing the groups, there was no statistically significant difference in either PI or maximal jaw opening. There was however a slightly higher proportion of patients with reduced pain during rest and function in group B, indicating that this method is at least equally effective as the injection performed with simultaneous radiographic imaging. It must be taken into consideration that the anatomically oriented injection may not always accurately reach the superior cavity. The steroid may rather be injected in the periarticular tissues. This could theoretically be advantageous if the inflammation is spread to the surrounding tissues. However, one systematic review (Li et al., 2012)    concluded that injection in both the upper and lower joint space has a better effect on 13 maximum opening and jaw pain compared with injection in the superior joint space alone. Group A showed a slightly greater tendency in reducing joint locking. One theory is that locking due to adherences in the inferior joint cavity may loosen during the intra articular administration. On the other hand, mixing the steroids with contrast medium leaves less room for the steroids since the joint space is limited. Therefore it can be assumed that more active substance is injected using the anatomically oriented technique. Little is known about the clinical importance of this aspect, which could neither be concluded from this study. It is also questionable if the solution of GCS, contrast medium and local anaesthetics is stable. One study (Shah et al., 2008)  where the mix was analysed for stability at different points of time during 24 hours confirms the stability of the solution, which implies safe clinical use. Another concern is the possibility that the contrast medium might impair the anti-inflammatory properties of corticosteroid or cause tissue damage but according to FASS, there is no known adverse effect of the contrast medium Omnipaque on joint tissues. The image-guided technique is not only more precise but could also provide diagnostic information. Studying the distribution of the agent during the injection can aid in detecting disc perforation for example  Furthermore, the radiation risk for both the patient and the dental staff is to be considered. The exposure time may vary between patients. For an ordinary procedure the effective dose during 4 minutes of exposure is 0,13mSv (calculated by JS Andersson, from the Department of Radiation Science, Umeå University). When compared to thresholds for different proximate radiation sensitive structures (e.g. the lens of the eye 20mSv/ year, the thyroid 25mSv/ year and 50mSv/ year for the skin) it can be concluded that it is fairly unlikely to reach potentially harmful doses (NAS,   2006). 14 The outcome of this study may have been affected by several factors, most importantly previous or simultaneous interventions with analgesics and anti-inflammatory drugs, unloading with occlusal splint or jaw movement training and stretching. Patients whom received a more comprehensive conservative treatment prior to or in combination with the injection may have responded better. The treatment effect may also be related to different diagnosis. Patients in group A had either the diagnosis arthritis or arthralgia whereas the patients in group B were diagnosed with symptomatic arthrosis or arthralgia. Patients with more severe forms of TMD or with associated inflammatory disease would naturally pose a greater therapeutic challenge. This has not been further investigated due to a small sample. Neither has the psychosocial status of the patient been accounted for as a contributing etiologic factor that may be associated to increased pain sensitivity  As with most pilot studies, it is not possible to provide any firm conclusions from our findings and therefore we cannot confirm or reject our hypothesis. A larger sample would have helped to improve the statistical analysis and allowed for more conclusive findings. Conclusion The results indicate that regardless the use of administration method, intra-articula",Uppsatsmall,,,,,core
357291093,2020-04-02T00:00:00,". Abstract Design for manufacturing is often difficult for mechanical parts since significant manufacturing knowledge is required to adjust part designs for manufacturability. The traditional trial and error approach usually leads to expensive iterations and compromises the quality of the final design. The authors believe the appropriate way to handle product design for manufacturing problems is not to formulate a large design problem that exhaustively incorporates design and manufacturing issues, but to separate the design and manufacturing activities and provide support for collaboration between engineering teams. In this paper, the Collaborative Multidisciplinary Decision-making Methodology (CMDM) is used to solve a product design and manufacturing problem. First, the compromise Decision Support Problem is used as a mathematical model of each engineering teams&apos; design decisions and as a medium for information exchange. Second, game theoretic principles are employed to resolve couplings or interactions between the teams&apos; decisions. Third, design capability indices are used to maintain design freedom at the early stages of product realization in order to accommodate unexpected downstream design changes. A plastic robot arm design and manufacturing scenario is presented to demonstrate the application of this methodology and its effectiveness for solving a complex design for manufacturing problem in a streamlined manner, with minimal expensive iterations. Keywords: Collaborative Design, Design for Manufacturing, Game Theory, and Multidisciplinary Decision Making 2 Design for manufacturing Concurrent engineering involves separating product realization activities so that design activities can be executed independently while simultaneously incorporating relevant information from downstream domains such as manufacturing, assembly, or recycling  Alternatively, a manufacturing team that understands the purpose of a design and its functional requirements may be more capable of adjusting it to facilitate manufacturing. The authors believe the appropriate way to handle complex product design problems such as DfM is not to formulate large design problems but to support cooperation and collaboration between multidisciplinary engineering teams. Towards this end, the Collaborative Multidisciplinary Decision-making Methodology (CMDM) is established (Xiao et al. 2005)  to enable collaborative decision making between design and manufacturing teams through &apos;collaboration by separation&apos;. Separation signifies that the responsibility for DfM is transferred from the design team to the downstream manufacturing team; whereas, collaboration signifies that satisfactory systems-level solutions are coordinated with minimal information exchange and iteration. Unlike many mathematical multidisciplinary optimization (MDO) approaches (Balling and SobieszczanskiSobieski 1996 1. Exchanging Information. The information required for decision-making in an activity must be transferred completely from one team to another, and the recipient teams should be able to understand the team&apos;s intentions without requiring additional information flows or causing iterations. The compromise Decision Support Problem (DSP)  2. Accommodating interactions between activities. Some activities in a DfM process may be coupled, such that each design team makes decisions that affect the decisions of other teams. Game theory is used in the CMDM to model different degrees of collaboration and manage interactions between engineering teams, with little or no expensive, systems-level iterations. 3. Maintaining feasible and satisfactory overall designs. When design activities are separated, design teams must make decisions without full knowledge of their impact on downstream activities. If single point solutions are exchanged, downstream designers are prevented from adjusting designs for feasibility or satisfactory local performance, and iterations often ensue. With set-based approaches, however, ranges or sets of solutions are shared and gradually narrowed during the design process, thereby reducing or eliminating the need for global, systems-level iterations  In a product realization problem, the dependencies between any two activities, such as designmanufacturing, design-design, or manufacturing-manufacturing, may be interactive or sequential. Game theory is used to resolve the interactive couplings and design capability indices are used to handle the sequential relationships. The authors believe the sequential relationships are more significant in DfM problems, mostly due to the upstream/downstream nature of the designmanufacturing relationship. As shown in  &lt;Figure 1 goes about here&gt; Collaborative multidisciplinary decision making methodology The CMDM is implemented in three steps: Step 1 Representing decision making information in a compromise DSP which serves as an information medium to eliminate iterations caused by information exchange and communication; Step 2 Representing cooperation styles among engineering teams with game theoretic protocols to eliminate iterations caused by interdisciplinary interactions; and Step 3 Reformulating the compromise DSPs using design capability indices for finding superior ranged set of solutions that eliminate or reduce costly iterations caused by unexpected downstream requirements and constraints. Step 1, modeling product realization activities using compromise DSPs In order to resolve the first challenge, that of exchanging information, Section 1, a compromise Decision Support Problem, DSP, is used. A compromise DSP is a multi-objective decision modela hybrid formulation based on mathematical programming and goal programming -that is used to find the values of design variables that satisfy a set of constraints and achieve a set of conflicting goals as closely as possible  For a given product realization activity, a compromise DSP is capable of representing a team&apos;s decision-making knowledge, as well as the design rationale underlying its decision. A team&apos;s decision is represented with a feasible design space, a set of design objectives, and a tradeoff strategy between these design objectives. As shown in  The compromise DSP resolves the first challenge of exchanging information, but it does not address the second challenge of enabling the separation of activities. There are three possible relationships between any two compromise DSPs; they may be solved concurrently, sequentially, or as coupled problems. Given the disk brakes in a passenger vehicle as an example, there is no direct information exchange between the brake design and exhaust system design. From a decisionmaking perspective, the two compromise DSPs (brake and exhaust system) do not share any unknown variables. They can be solved concurrently, and the solution remains the same regardless of the teams&apos; cooperation styles. Meanwhile, the brake pad cannot be designed without knowledge of the rotor design team&apos;s results, whereas the rotor has to be designed with knowledge of the geometric shape, surface finish, and other details of the brake pad. This situation is reflected as shared variables between the rotor and brake pad design compromise DSPs. Neither compromise 6 DSP can be solved independently, and the result is always affected by the two design teams&apos; cooperation styles, namely, which team solves its compromise DSP first. Game theoretic protocols are used to address the second challenge of separating the coupled activities. In addition, a manufacturing team must design the fixtures, determining the processing parameters based on the final rotor and brake pad designs. The manufacturing compromise DSP includes variables that are determined only by solving the design compromise DSPs. This is a sequential process and the teams&apos; cooperation styles do not affect the solution. However, the downstream manufacturing team may need to modify the design, causing potential iterations. Design capability indices are used to address this third challenge. Since design and manufacturing activities are separated and the responsibility for DfM is transferred from the upstream design team to the downstream manufacturing team, the third challenge becomes more significant in this study. Step 2, representing cooperation styles using game theory The second challenge is resolving couplings between activities. Traditionally, a trial and error approach is used to solve coupled compromise DSPs. Since a team has to make assumptions about another team&apos;s decisions to initiate the trial and error process, this traditional approach may not guarantee consensus (convergence) and usually fails to achieve superior results. Game theory facilitates interaction among multiple engineers without integrating a product realization process into a single large optimization problem or causing iterations. There are three game protocols representing different types of interactions between teams (or players in game theory terminology): cooperative, noncooperative, and leader/follower. Rao and colleagues  In a leader/follower game, the leader makes a set of rational decisions by predicting the A is a subset of X A which must be determined using information from team B, and x B B must be determined using  The feasibility of the solution is ensured by using a BRC to predict the follower&apos;s behavior. Generally, a leader-follower game protocol facilitates collaborative decision making without requiring iteration, hence the coupled activities can be accomplished separately. This solves the second challenge. Step 3, maintaining design freedom using design capability indices The third challenge is to eliminate, or at least reduce, costly iterations between upstream and downstream activities by having the upstream team identify ranges of design variables, rather than single point values, that are as broad as possible without deviating from a desired range of 8 performance, as shown in  &lt;Figure 3 goes about here&gt; As presented in  In the design variable set, X, if any design variable is discrete, say x j , the location and deviation of the performance measures have to be conservatively estimated using: In many cases, calculating the min/max values of a performance measure requires exhaustive search, but the performance range estimated in this manner will cover all the possible values even though they are not continuous. If a performance variable is discrete, the design capability indices are not applicable. Given that all the performance variables are continuous, design capability indices are embedded into the compromise DSP by formulating the design goals using C dk , adding constraints C dk ≥1, and formulating the deviation function to maximize the overachievements of C dk . Moreover, the constraints are re-formulated using Equation  Clearly, constraint g k (X) must be differentiable. If any design variable is discrete, the constraints can be calculated using Equation (4). The bounds of design variables are still formulated using constant values. The resulting compromise DSP is shown in  &lt;Figure 4 goes about here&gt; In  Step 1, the challenge is to provide a method for exchanging information, and it is solved by representing the decision making information in compromise DSPs. In Step 2, the challenge is accommodating interactions between activities, and it is addressed with game theory. In Step 3, the challenge is to maintain feasible and satisfactory overall designs, and it is addressed by reformulating the compromise DSPs using design capability indices. The CMDM provides a normative framework that facilitates collaborative product realization by separating the decision making activities. A robot arm design and manufacturing scenario The authors have developed a distributed product realization environment called the Rapid Tooling  &lt;Figure 5 goes about here&gt; A design team, a rapid tooling team, and an injection molding team are assigned to this task. Correspondingly, the product realization process is partitioned into three activities, as shown in  In this scenario, DfM includes not only adjusting the geometric shape, but also the entire rapid tooling activity. Designing the mold pairs requires knowledge about rapid tooling; hence, it is very difficult for the design team. Since the robot arm is designed without knowledge of the downstream manufacturing process, the design team will have to modify its design based on feedback from manufacturing experts. For a simple product realization process like this one, it is possible to collect all the manufacturing related information and formulate a large design problem to solve all the design variables, i.e., robot arm geometry shape, mold geometry shape, and some manufacturing parameters, like that in Concurrent Engineering. For complex real-world problems, this implies a design problem containing large numbers of design variables and complex analyses; therefore it is not practical to solve as a single problem. The traditional approach to solving a DfM problem is a trial and error approach, which will cause extensive information exchange and iteration. In this example, all three of the challenges addressed by the CMDM exist. Information exchange is required between these activities (challenge 1). Rapid tooling and injection molding are coupled activities; thus iterations exist between them (challenge 2). The geometric shape of the 12 robot arm may have to be modified in order to fabricate the batch with given time and cost; this forces the upstream design team to redesign the geometry (challenge 3). Engineering teams&apos; compromise DSPs The first step of the CMDM is modeling each activity as a standard compromise DSP as shown in  Three design goals are determined based on the customer requirements: (i) the maximum deformation under working load should be as close to 0.5mm as possible, (ii) the maximum von Mises stress under working load should be as close to 6MPa as possible, and (iii) the weight of the robot arm should be as close to 3.5g as possible. The design compromise DSP is shown in  Please note at this step, no design capability indices or game protocol is yet involved. Mathematical equations for deformation, stress, and weight can be found in (Xiao 2003). &lt;Figure 6 goes about here&gt; The rapid tooling team designs the injection mold halves,  On the other hand, the mold life determines the number of mold halves, N m , that must be built in the rapid tooling activity, which thus affects all of the process parameters in this activity. Therefore, injection molding and rapid tooling are coupled activities. As shown in  If we combine all compromise DSPs into one problem with a variable set that includes all of the variables of design, rapid tooling, and injection molding and an objective set that includes all of the objectives with the same weights, the results are shown in  = 9.25mm, t = 3.10mm. In this case, all design goals achieve their target values and the overall deviation is 0. Then, the rapid tooling and injection molding teams make decisions based on this result. Since these two activities are coupled, the rapid tooling team assumes a value of ML, and expects to acquire converged results after several iterations. Unfortunately in this case, rapid tooling and injection molding teams&apos; solutions do not converge. The reason is the design team makes decisions only considering its own design goals; hence the thickness of robot arm t is too large and the mold life becomes so short that the rapid tooling team must build 10 pairs of molds. This violates the constraints of time and cost. Therefore, the product design must be modified. For simplicity, the intermediate results are not listed here. After several rounds of iterations, the converged results from the traditional approach are as shown in  In the trial and error process, iterations happen not only between the coupled rapid tooling and injection molding activities, but also with the upstream design activity. Furthermore, the number and styles of iterations are affected by some unpredictable or uncontrollable factors, such as the teams&apos; experience, and the initial values the teams choose to start the iterative process. So far, this case has demonstrated the difficulties of DfM, and why the traditional approach cannot guarantee the superiority of the final result. By using the CMDM to separate the activities, we expect to change the process shown in  &lt;Table 1 goes about here&gt; &lt;Table 2 goes about here&gt; Resolving couplings using Leader/Follower protocol Since the design team&apos;s decision is not coupled with the decisions of either of the manufacturing teams, as shown in  when LT = 2 mils (7) ML=1696. 10-135.46d-311.63t+423.03Θ +61.17(d-8.11  et al. 1996), which is fundamentally different from using them to approximate the BRCs. Beyond predicting a player&apos;s behavior using its BRC, game protocols also govern issues such as the sequence of the players&apos; decision making activities and control over specific variables. All of these factors are determined by the players&apos; cooperation styles. If the injection molding team is selected as the leader, the BRC T is: When 150&gt; ML ≥75, two pairs of molds have to be build, thus LT = 8 mils has to be selected in order to meet the time constraint. Here, we do not consider the situation that several pairs of molds can be built simultaneously in an SLA3500 machine. It can also be observed from BRC T that Θ remains 0 because the rapid tooling team does not know how the draft angle will affect mold life, and strives to reduce surface roughness, SF, of the robot arm which is achieved with Θ = 0. Obviously, the injection molding team will be unable to eject the parts with a zero draft angle. This is the main reason why the traditional trial and error approach does not converge between the rapid tooling and injection molding teams. Compromise DSP for a ranged set of decisions As described in Section 1, the third step is reformulating the compromise DSPs using design capability indices. In the design activity, all the design variables are continuous; therefore the 16 locations and deviations of the performance variables are calculated using Equation (2). The design team&apos;s compromise DSP for a ranged set of decisions is shown in  &lt;Table 5 goes about here&gt; &lt;Figure 14 goes about here&gt; In  The CMDM is especially useful in the early stages of product design, when little is known about the product and approximating a set of correct designs is much more efficient than conducting rounds and rounds of guessing and correcting. The advantage of the CMDM in DfM is that design and manufacturing activities are separated systematically; hence the product realization activities are accomplished in a more streamlined process as shown in  18 Design freedom in the process The reason the CMDM results in more superior results than the traditional trial and error approach is the design freedom in the product realization process. A design freedom metric is presented in  where n is the number of performance measures of the system. For the i th performance measure, TR i is the target range, PR i is the feasible performance range, and PR i,initial is the initial feasible performance range. In  At the second row, the performance ranges and design freedom at the initial state of this product realization process are listed. The initial design freedom is 0.734, which is smaller than 1 due to the natural limitation of this process and the couplings between activities. In the trial and error process, when the design team makes a specific decision, design freedom is quantified as 0.368, shown at the third row of  Step 3. The injection molding team finally makes its decision at Step 4. It is observed that when the design team makes a ranged set of decisions, design freedom is 0.504 at Step 2. At this moment of the product realization process, the rapid tooling and injection molding teams can  Closure In this paper, the idea of collaboration by separation is tested in product DfM problems. The CMDM is used to enable the separation without causing costly information exchange and iterations. Generally, the compromise DSP is used as an information medium to separate the activities at the information communication level, game theoretical principles separate the coupled activities, and design capability indices separate upstream and downstream activities. The robot arm design and manufacturing process demonstrates that by using the CMDM, a complex product realization process is implemented in a streamlined manner, with each engineering team focusing on its areas of expertise. With the CMDM, final results are obtained with fewer iterations between design teams and significantly less deviation from target performance, relative to using the traditional trial and error approach",Sobieszczanski-Sobieski and Haftka,,,,,core
429121488,2020-12-01T00:00:00,"[EN] Due to transistor shrinking, intermittent faults are a major concern in current digital systems. This work presents an adaptive fault tolerance mechanism based on error correction codes (ECC), able to modify its behavior when the error conditions change without increasing the redundancy. As a case example, we have designed a mechanism that can detect intermittent faults and swap from an initial generic ECC to a specific ECC capable of tolerating one intermittent fault. We have inserted the mechanism in the memory system of a 32-bit RISC processor and validated it by using VHDL simulation-based fault injection. We have used two (39, 32) codes: a single error correction-double error detection (SEC-DED) and a code developed by our research group, called EPB3932, capable of correcting single errors and double and triple adjacent errors that include a bit previously tagged as error-prone. The results of injecting transient, intermittent, and combinations of intermittent and transient faults show that the proposed mechanism works properly. As an example, the percentage of failures and latent errors is 0% when injecting a triple adjacent fault after an intermittent stuck-at fault. We have synthesized the adaptive fault tolerance mechanism proposed in two types of FPGAs: non-reconfigurable and partially reconfigurable. In both cases, the overhead introduced is affordable in terms of hardware, time and power consumption.This research was supported in part by the Spanish Government, project TIN2016-81,075-R, and by Primeros Proyectos de Investigacion (PAID-06-18), Vicerrectorado de Investigacion, Innovacion y Transferencia de la Universitat Politecnica de Valencia (UPV), project 20190032.Baraza Calvo, JC.; Gracia-Morán, J.; Saiz-Adalid, L.; Gil Tomás, DA.; Gil, P. (2020). Proposal of an Adaptive Fault Tolerance Mechanism to Tolerate Intermittent Faults in RAM. Electronics. 9(12):1-30. https://doi.org/10.3390/electronics9122074S130912International Technology Roadmap for Semiconductors (ITRS)http://www.itrs2.net/2013-itrs.htmlJeng, S.-L., Lu, J.-C., & Wang, K. (2007). A Review of Reliability Research on Nanotechnology. IEEE Transactions on Reliability, 56(3), 401-410. doi:10.1109/tr.2007.903188Ibe, E., Taniguchi, H., Yahagi, Y., Shimbo, K., & Toba, T. (2010). Impact of Scaling on Neutron-Induced Soft Error in SRAMs From a 250 nm to a 22 nm Design Rule. IEEE Transactions on Electron Devices, 57(7), 1527-1538. doi:10.1109/ted.2010.2047907Boussif, A., Ghazel, M., & Basilio, J. C. (2020). Intermittent fault diagnosability of discrete event systems: an overview of automaton-based approaches. Discrete Event Dynamic Systems, 31(1), 59-102. doi:10.1007/s10626-020-00324-yConstantinescu, C. (2003). Trends and challenges in VLSI circuit reliability. IEEE Micro, 23(4), 14-19. doi:10.1109/mm.2003.1225959Bondavalli, A., Chiaradonna, S., Di Giandomenico, F., & Grandoni, F. (2000). Threshold-based mechanisms to discriminate transient from intermittent faults. IEEE Transactions on Computers, 49(3), 230-245. doi:10.1109/12.841127Contant, O., Lafortune, S., & Teneketzis, D. (2004). Diagnosis of Intermittent Faults. Discrete Event Dynamic Systems, 14(2), 171-202. doi:10.1023/b:disc.0000018570.20941.d2Sorensen, B. A., Kelly, G., Sajecki, A., & Sorensen, P. W. (s. f.). An analyzer for detecting intermittent faults in electronic devices. Proceedings of AUTOTESTCON  ’94. doi:10.1109/autest.1994.381590Gracia-Moran, J., Gil-Tomas, D., Saiz-Adalid, L. J., Baraza, J. C., & Gil-Vicente, P. J. (2010). Experimental validation of a fault tolerant microcomputer system against intermittent faults. 2010 IEEE/IFIP International Conference on Dependable Systems & Networks (DSN). doi:10.1109/dsn.2010.5544288Fujiwara, E. (2005). Code Design for Dependable Systems. doi:10.1002/0471792748Hamming, R. W. (1950). Error Detecting and Error Correcting Codes. Bell System Technical Journal, 29(2), 147-160. doi:10.1002/j.1538-7305.1950.tb00463.xSaiz-Adalid, L.-J., Gil-Vicente, P.-J., Ruiz-García, J.-C., Gil-Tomás, D., Baraza, J.-C., & Gracia-Morán, J. (2013). Flexible Unequal Error Control Codes with Selectable Error Detection and Correction Levels. Computer Safety, Reliability, and Security, 178-189. doi:10.1007/978-3-642-40793-2_17Frei, R., McWilliam, R., Derrick, B., Purvis, A., Tiwari, A., & Di Marzo Serugendo, G. (2013). Self-healing and self-repairing technologies. The International Journal of Advanced Manufacturing Technology, 69(5-8), 1033-1061. doi:10.1007/s00170-013-5070-2Maiz, J., Hareland, S., Zhang, K., & Armstrong, P. (s. f.). Characterization of multi-bit soft error events in advanced SRAMs. IEEE International Electron Devices Meeting 2003. doi:10.1109/iedm.2003.1269335Schroeder, B., Pinheiro, E., & Weber, W.-D. (2011). DRAM errors in the wild. Communications of the ACM, 54(2), 100-107. doi:10.1145/1897816.1897844BanaiyanMofrad, A., Ebrahimi, M., Oboril, F., Tahoori, M. B., & Dutt, N. (2015). Protecting caches against multi-bit errors using embedded erasure coding. 2015 20th IEEE European Test Symposium (ETS). doi:10.1109/ets.2015.7138735Kim, J., Sullivan, M., Lym, S., & Erez, M. (2016). All-Inclusive ECC: Thorough End-to-End Protection for Reliable Computer Memory. 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA). doi:10.1109/isca.2016.60Hwang, A. A., Stefanovici, I. A., & Schroeder, B. (2012). Cosmic rays don’t strike twice. ACM SIGPLAN Notices, 47(4), 111-122. doi:10.1145/2248487.2150989Gil-Tomás, D., Gracia-Morán, J., Baraza-Calvo, J.-C., Saiz-Adalid, L.-J., & Gil-Vicente, P.-J. (2012). Studying the effects of intermittent faults on a microcontroller. Microelectronics Reliability, 52(11), 2837-2846. doi:10.1016/j.microrel.2012.06.004Plasma CPU Modelhttps://opencores.org/projects/plasmaArlat, J., Aguera, M., Amat, L., Crouzet, Y., Fabre, J.-C., Laprie, J.-C., … Powell, D. (1990). Fault injection for dependability validation: a methodology and some applications. IEEE Transactions on Software Engineering, 16(2), 166-182. doi:10.1109/32.44380Gil-Tomas, D., Gracia-Moran, J., Baraza-Calvo, J.-C., Saiz-Adalid, L.-J., & Gil-Vicente, P.-J. (2012). Analyzing the Impact of Intermittent Faults on Microprocessors Applying Fault Injection. IEEE Design & Test of Computers, 29(6), 66-73. doi:10.1109/mdt.2011.2179514Rashid, L., Pattabiraman, K., & Gopalakrishnan, S. (2010). Modeling the Propagation of Intermittent Hardware Faults in Programs. 2010 IEEE 16th Pacific Rim International Symposium on Dependable Computing. doi:10.1109/prdc.2010.52Amiri, M., Siddiqui, F. M., Kelly, C., Woods, R., Rafferty, K., & Bardak, B. (2016). FPGA-Based Soft-Core Processors for Image Processing Applications. Journal of Signal Processing Systems, 87(1), 139-156. doi:10.1007/s11265-016-1185-7Hailesellasie, M., Hasan, S. R., & Mohamed, O. A. (2019). MulMapper: Towards an Automated FPGA-Based CNN Processor Generator Based on a Dynamic Design Space Exploration. 2019 IEEE International Symposium on Circuits and Systems (ISCAS). doi:10.1109/iscas.2019.8702589Mittal, S. (2018). A survey of FPGA-based accelerators for convolutional neural networks. Neural Computing and Applications, 32(4), 1109-1139. doi:10.1007/s00521-018-3761-1Intel Completes Acquisition of Alterahttps://newsroom.intel.com/news-releases/intel-completes-acquisition-of-altera/#gs.mi6ujuAMD to Acquire Xilinx, Creating the Industry’s High Performance Computing Leaderhttps://www.amd.com/en/press-releases/2020-10-27-amd-to-acquire-xilinx-creating-the-industry-s-high-performance-computingKim, K. H., & Lawrence, T. F. (s. f.). Adaptive fault tolerance: issues and approaches. [1990] Proceedings. Second IEEE Workshop on Future Trends of Distributed Computing Systems. doi:10.1109/ftdcs.1990.138292Gonzalez, O., Shrikumar, H., Stankovic, J. A., & Ramamritham, K. (s. f.). Adaptive fault tolerance and graceful degradation under dynamic hard real-time scheduling. Proceedings Real-Time Systems Symposium. doi:10.1109/real.1997.641271Jacobs, A., George, A. D., & Cieslewski, G. (2009). Reconfigurable fault tolerance: A framework for environmentally adaptive fault mitigation in space. 2009 International Conference on Field Programmable Logic and Applications. doi:10.1109/fpl.2009.5272313Shin, D., Park, J., Park, J., Paul, S., & Bhunia, S. (2017). Adaptive ECC for Tailored Protection of Nanoscale Memory. IEEE Design & Test, 34(6), 84-93. doi:10.1109/mdat.2016.2615844Silva, F., Muniz, A., Silveira, J., & Marcon, C. (2020). CLC-A: An Adaptive Implementation of the Column Line Code (CLC) ECC. 2020 33rd Symposium on Integrated Circuits and Systems Design (SBCCI). doi:10.1109/sbcci50935.2020.9189901Mukherjee, S. S., Emer, J., Fossum, T., & Reinhardt, S. K. (s. f.). Cache scrubbing in microprocessors: myth or necessity? 10th IEEE Pacific Rim International Symposium on Dependable Computing, 2004. Proceedings. doi:10.1109/prdc.2004.1276550Saleh, A. M., Serrano, J. J., & Patel, J. H. (1990). Reliability of scrubbing recovery-techniques for memory systems. IEEE Transactions on Reliability, 39(1), 114-122. doi:10.1109/24.52622X9SRA User’s Manual (Rev. 1.1)https://www.manualshelf.com/manual/supermicro/x9sra/user-s-manual-1-1.htmlChishti, Z., Alameldeen, A. R., Wilkerson, C., Wu, W., & Lu, S.-L. (2009). Improving cache lifetime reliability at ultra-low voltages. Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture - Micro-42. doi:10.1145/1669112.1669126Datta, R., & Touba, N. A. (2011). Designing a fast and adaptive error correction scheme for increasing the lifetime of phase change memories. 29th VLSI Test Symposium. doi:10.1109/vts.2011.5783773Kim, J., Lim, J., Cho, W., Shin, K.-S., Kim, H., & Lee, H.-J. (2016). Adaptive Memory Controller for High-performance Multi-channel Memory. JSTS:Journal of Semiconductor Technology and Science, 16(6), 808-816. doi:10.5573/jsts.2016.16.6.808Yuan, L., Liu, H., Jia, P., & Yang, Y. (2015). Reliability-Based ECC System for Adaptive Protection of NAND Flash Memories. 2015 Fifth International Conference on Communication Systems and Network Technologies. doi:10.1109/csnt.2015.23Zhou, Y., Wu, F., Lu, Z., He, X., Huang, P., & Xie, C. (2019). SCORE. ACM Transactions on Architecture and Code Optimization, 15(4), 1-25. doi:10.1145/3291052Lu, S.-K., Li, H.-P., & Miyase, K. (2018). Adaptive ECC Techniques for Reliability and Yield Enhancement of Phase Change Memory. 2018 IEEE 24th International Symposium on On-Line Testing And Robust System Design (IOLTS). doi:10.1109/iolts.2018.8474118Chen, J., Andjelkovic, M., Simevski, A., Li, Y., Skoncej, P., & Krstic, M. (2019). Design of SRAM-Based Low-Cost SEU Monitor for Self-Adaptive Multiprocessing Systems. 2019 22nd Euromicro Conference on Digital System Design (DSD). doi:10.1109/dsd.2019.00080Wang, X., Jiang, L., & Chakrabarty, K. (2020). LSTM-based Analysis of Temporally- and Spatially-Correlated Signatures for Intermittent Fault Detection. 2020 IEEE 38th VLSI Test Symposium (VTS). doi:10.1109/vts48691.2020.9107600Ebrahimi, H., & G. Kerkhoff, H. (2018). Intermittent Resistance Fault Detection at Board Level. 2018 IEEE 21st International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS). doi:10.1109/ddecs.2018.00031Ebrahimi, H., & Kerkhoff, H. G. (2020). A New Monitor Insertion Algorithm for Intermittent Fault Detection. 2020 IEEE European Test Symposium (ETS). doi:10.1109/ets48528.2020.9131563Hsiao, M. Y. (1970). A Class of Optimal Minimum Odd-weight-column SEC-DED Codes. IBM Journal of Research and Development, 14(4), 395-401. doi:10.1147/rd.144.0395Benso, A., & Prinetto, P. (Eds.). (2004). Fault Injection Techniques and Tools for Embedded Systems Reliability Evaluation. Frontiers in Electronic Testing. doi:10.1007/b105828Gracia, J., Saiz, L. J., Baraza, J. C., Gil, D., & Gil, P. J. (2008). Analysis of the influence of intermittent faults in a microcontroller. 2008 11th IEEE Workshop on Design and Diagnostics of Electronic Circuits and Systems. doi:10.1109/ddecs.2008.4538761ZC702 Evaluation Board for the Zynq-7000 XC7Z020 SoChttps://www.xilinx.com/support/documentation/boards_and_kits/zc702_zvik/ug850-zc702-eval-bd.pd",Proposal of an Adaptive Fault Tolerance Mechanism to Tolerate Intermittent Faults in RAM,https://riunet.upv.es/bitstream/10251/165770/1/Baraza%3bGraca-Mor%c3%a1n%3bSaz-Adald%20-%20Proposal%20of%20an%20Adaptve%20Fault%20Tolerance%20Mechansm%20to%20Tolerate%20Interm....pdf,'MDPI AG',10.3390/electronics9122074,,core
441015481,2020-11-01T00:00:00,"Today's Internet marketing ecosystems are very complex, with many competing players, transactions concluded within milliseconds, and hundreds of different parameters to be analyzed in the decision-making process. In addition, both sellers and buyers operate under uncertainty, without full information about auction results, purchasing preferences, and strategies of their competitors or suppliers. As a result, most market participants strive to optimize their trading strategies using advanced machine learning algorithms. In this publication, we propose a new approach to determining reserve-price strategies for publishers, focusing not only on the profits from individual ad impressions, but also on maximum coverage of advertising space. This strategy combines the heuristics developed by experienced RTB consultants with machine learning forecasting algorithms like ARIMA, SARIMA, Exponential Smoothing, and Facebook Prophet. The paper analyses the effectiveness of these algorithms, recommends the best one, and presents its implementation in real environment. As such, its results may form a basis for a competitive advantage for publishers on very demanding online advertising markets",The Reserve Price Optimization for Publishers on Real-Time Bidding on-Line Marketplaces with Time-Series Forecasting,,'Walter de Gruyter GmbH',10.2478/fman-2020-0013,"[{'title': 'Foundations of Management', 'identifiers': ['2300-5661', 'issn:2300-5661']}]",core
344910854,2020-01-01T00:00:00,"Abstract

Cloud/software-based wireless resource controllers have been recently proposed to exploit radio frequency (RF) data analytics for a network control, configuration and management. For efficient resource controller design, tracking the right metrics in real-time (analytics) and making realistic predictions (deep learning) will play an important role to increase its efficiency. This factor becomes particularly critical as radio environments are generally dynamic, and the data sets collected may exhibit shift in distribution over time and/or space. When a trained model is deployed at the controller without taking into account dataset shift, a large amount of prediction errors may take place. This paper quantifies dataset shift in real wireless physical layer data by using a statistical distance method called earth mover’s distance (EMD). It utilizes an FPGA to process in real-time the inphase and quadrature (IQ) samples to obtain useful information, such as histograms of wireless channel utilization (CU). We have prototyped the data processing modules on a Xilinx System on Chip (SoC) board using Vivado, Vivado HLS, SDK and MATLAB tools. The histograms are sent as low-overhead analytics to the resource controller server where they are processed to evaluate dataset shift. The presented results provide insight into dataset shift in real wireless CU data collected over multiple weeks in the University of Oulu using the implemented modules on SoC devices. The results can be used to design approaches that can prevent failures due to datashift in deep learning models for wireless networks",Histograms to quantify dataset shift for spectrum data analytics:a SoC based device perspective,,,,,core
351125143,2020-11-04T08:00:00,"Deep neural networks (DNNs) have achieved significant success in many applications, such as computer vision, natural language processing, robots, and self-driving cars. With the growing demand for more complex real-world applications, more complicated neural networks have been proposed. However, high capacity models result in two major problems: long training times and high inference delays, making the neural networks hard to train and infeasible to deploy for time-intensive applications or resource-limited devices. In this work, we propose multiple techniques to accelerate the training and inference speed as well as model performance
The first technique we study is model parallelization on generative adversarial networks (GANs). Multiple orthogonal generators with shared memory are employed to capture the whole data distribution space. This method can not only improve the model performance but also alleviate the mode collapse problem that is common in GANs. The second technique we investigate is the automatic network pruning. To reduce the floating-point operations (FLOPs) to a proper level without compromising accuracy, we propose a better generalized and easy-to-use pruning method, which prunes the network through optimizing a set of trainable auxiliary parameters instead of original weights. Weakly coupled gradient update rules are proposed to keep consistency with pruning tasks. The third technique is to remove the redundancy of the complicated model based on the need of applications. We treat the chemical reaction prediction as a translation problem and apply a low capacity neuron translation model to this problem. The fourth technique is to combine distillation with Differentiable Architecture Search to stabilize and improve the searching procedure. Intermediate results as well as the output logits are transferred from the teacher network to the student network. For the application of the speedup technique, we introduce neural network pruning into Materials Genomics. We propose attention based AutoPrune for the kernel pruning of a continuous filtering neural network for molecular property prediction and achieves better performance and more compact size",Speedup Techniques for Deep Neural Networks,https://core.ac.uk/download/351125143.pdf,OpenCommons@UConn,,,core
288570501,2020-01-01T00:00:00,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm – II, and ~23–29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by ~10–95% as compared to those obtained using other standard uncertainty sets",Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,,'Elsevier BV',10.1016/j.ress.2020.106821,,core
362647068,2020-11-02T00:00:00,"Space systems are complex and consist of multiple subsystems. Research and development teams of such complex systems are usually distributed among various institutions and space agencies. This affects the quality of the On-board Software (OBSW) since testing it without having all required subsystems at the software development site can be troublesome. In this paper, we present a data-driven method which can be used to synthesize parts of a system or even an entire system as a black-box model. We exploit the data collected from the real hardware to derive a model using a Machine Learning (ML) algorithm. The proposed model can easily be distributed among development teams and is dedicated to emulate the system for testing the OBSW",Modeling and Simulation of a Spacecraft Payload Hardware Using Machine Learning Techniques,https://core.ac.uk/download/362647068.pdf,'American Institute of Aeronautics and Astronautics (AIAA)',10.2514/6.2020-4219,,core
327934217,2020-06-28T00:00:00,"The space of synthesizable molecules is greater than $10^{60}$, meaning only
a vanishingly small fraction of these molecules have ever been realized in the
lab. In order to prioritize which regions of this space to explore next,
synthetic chemists need access to accurate molecular property predictions.
While great advances in molecular machine learning have been made, there is a
dearth of benchmarks featuring properties that are useful for the synthetic
chemist. Focussing directly on the needs of the synthetic chemist, we introduce
the Photoswitch Dataset, a new benchmark for molecular machine learning where
improvements in model performance can be immediately observed in the throughput
of promising molecules synthesized in the lab. Photoswitches are a versatile
class of molecule for medical and renewable energy applications where a
molecule's efficacy is governed by its electronic transition wavelengths. We
demonstrate superior performance in predicting these wavelengths compared to
both time-dependent density functional theory (TD-DFT), the incumbent first
principles quantum mechanical approach, as well as a panel of human experts.
Our baseline models are currently being deployed in the lab as part of the
decision process for candidate synthesis. It is our hope that this benchmark
can drive real discoveries in photoswitch chemistry and that future benchmarks
can be introduced to pivot learning algorithm development to benefit more
expansive areas of synthetic chemistry.Comment: Prior version accepted to the 2020 ICLR Workshop on Fundamental
  Science in the Era of AI. First two authors contributed equall","The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the
  Advancement of Synthetic Chemistry",http://arxiv.org/abs/2008.03226,,,,core
334913045,2020-02-16T00:00:00,"Deep learning speech separation algorithms have achieved great success in
improving the quality and intelligibility of separated speech from mixed audio.
Most previous methods focused on generating a single-channel output for each of
the target speakers, hence discarding the spatial cues needed for the
localization of sound sources in space. However, preserving the spatial
information is important in many applications that aim to accurately render the
acoustic scene such as in hearing aids and augmented reality (AR). Here, we
propose a speech separation algorithm that preserves the interaural cues of
separated sound sources and can be implemented with low latency and high
fidelity, therefore enabling a real-time modification of the acoustic scene.
Based on the time-domain audio separation network (TasNet), a single-channel
time-domain speech separation system that can be implemented in real-time, we
propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that
takes binaural mixed audio as input and simultaneously separates target
speakers in both channels. Experimental results show that the proposed
end-to-end MIMO system is able to significantly improve the separation
performance and keep the perceived location of the modified sources intact in
various acoustic scenes.Comment: To appear in ICASSP 202",Real-time binaural speech separation with preserved spatial cues,http://arxiv.org/abs/2002.06637,,,,core
337302300,2020-12-08T00:00:00,"Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time","Graph-based Reinforcement Learning for Active Learning in Real Time: An
  Application in Modeling River Networks",http://arxiv.org/abs/2010.14000,,,,core
334934015,2020-04-19T00:00:00,"The widely-adopted practice is to train deep learning models with specialized
hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on
linear algebra operations. However, this strategy does not employ effectively
the extensive CPU and memory resources -- which are used only for
preprocessing, data transfer, and scheduling -- available by default on the
accelerated servers. In this paper, we study training algorithms for deep
learning on heterogeneous CPU+GPU architectures. Our two-fold objective --
maximize convergence rate and resource utilization simultaneously -- makes the
problem challenging. In order to allow for a principled exploration of the
design space, we first introduce a generic deep learning framework that
exploits the difference in computational power and memory hierarchy between CPU
and GPU through asynchronous message passing. Based on insights gained through
experimentation with the framework, we design two heterogeneous asynchronous
stochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU
Hogbatch -- combines small batches on CPU with large batches on GPU in order to
maximize the utilization of both resources. However, this generates an
unbalanced model update distribution which hinders the statistical convergence.
The second algorithm -- Adaptive Hogbatch -- assigns batches with continuously
evolving size based on the relative speed of CPU and GPU. This balances the
model updates ratio at the expense of a customizable decrease in utilization.
We show that the implementation of these algorithms in the proposed CPU+GPU
framework achieves both faster convergence and higher resource utilization than
TensorFlow on several real datasets and on two computing architectures -- an
on-premises server and a cloud instance",Heterogeneous CPU+GPU Stochastic Gradient Descent Algorithms,http://arxiv.org/abs/2004.08771,,,,core
334934584,2020-04-22T00:00:00,"Clustering points in a vector space or nodes in a graph is a ubiquitous
primitive in statistical data analysis, and it is commonly used for exploratory
data analysis. In practice, it is often of interest to ""refine"" or ""improve"" a
given cluster that has been obtained by some other method. In this survey, we
focus on principled algorithms for this cluster improvement problem. Many such
cluster improvement algorithms are flow-based methods, by which we mean that
operationally they require the solution of a sequence of maximum flow problems
on a (typically implicitly) modified data graph. These cluster improvement
algorithms are powerful, both in theory and in practice, but they have not been
widely adopted for problems such as community detection, local graph
clustering, semi-supervised learning, etc. Possible reasons for this are: the
steep learning curve for these algorithms; the lack of efficient and easy to
use software; and the lack of detailed numerical experiments on real-world data
that demonstrate their usefulness. Our objective here is to address these
issues. To do so, we guide the reader through the whole process of
understanding how to implement and apply these powerful algorithms. We present
a unifying fractional programming optimization framework that permits us to
distill out in a simple way the crucial components of all these algorithms. It
also makes apparent similarities and differences between related methods.
Viewing these cluster improvement algorithms via a fractional programming
framework suggests directions for future algorithm development. Finally, we
develop efficient implementations of these algorithms in our
LocalGraphClustering python package, and we perform extensive numerical
experiments to demonstrate the performance of these methods on social networks
and image-based data graphs.Comment: 71 Pages, 21 Figure","Flow-based Algorithms for Improving Clusters: A Unifying Framework,
  Software, and Performance",http://arxiv.org/abs/2004.09608,,,,core
478192963,2021-01-01T00:00:00,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.ISSN:1552-3098ISSN:1042-296XISSN:1941-046",Cat-Like Jumping and Landing of Legged Robots in Low Gravity Using Deep Reinforcement Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/tro.2021.3084374,,core
479813685,2021-01-01T00:00:00,"Scanning probe microscopies allow investigating surfaces at the nanoscale, in real space and with unparalleled signal-to-noise ratio. However, these microscopies are not used as much as it would be expected considering their potential. The main limitations preventing a broader use are the need of experienced users, the difficulty in data analysis and the time-consuming nature of experiments that require continuous user supervision. In this work, we addressed the latter and developed an algorithm that controlled the operation of an Atomic Force Microscope (AFM) that, without the need of user intervention, allowed acquiring multiple high-resolution images of different molecules. We used DNA on mica as a model sample to test our control algorithm, which made use of two deep learning techniques that so far have not been used for real time SPM automation. One was an object detector, YOLOv3, which provided the location of molecules in the captured images. The second was a Siamese network that could identify the same molecule in different images. This allowed both performing a series of images on selected molecules while incrementing the resolution, as well as keeping track of molecules already imaged at high resolution, avoiding loops where the same molecule would be imaged an unlimited number of times. Overall, our implementation of deep learning techniques brings SPM a step closer to full autonomous operation",Enabling autonomous scanning probe microscopy imaging of single molecules with deep learning,,'Royal Society of Chemistry (RSC)',10.1039/d1nr01109j,,core
483715342,2021-01-01T00:00:00,"The use of fixed or scheduled setpoints combined with varying occupancy patterns in buildings could lead to spaces being over or under-conditioned, which may lead to significant waste in energy consumption. The present study aims to develop a vision-based deep learning method for real-time occupancy activity detection and recognition. The method enables predicting and generating real-time heat gain data, which can inform building energy management systems and heating, ventilation, and air-conditioning (HVAC) controls. A faster region-based convolutional neural network was developed, trained and deployed to an artificial intelligence-powered camera. For the initial analysis, an experimental test was performed within a selected case study building\u27s office space. Average detection accuracy of 92.2% was achieved for all activities. Using building energy simulation, the case study building was simulated with both ‘static’ and deep learning influenced profiles to assess the potential energy savings that can be achieved. The work has shown that the proposed approach can better estimate the occupancy internal heat gains for optimising the operations of building HVAC systems",Occupancy Heat Gain Detection and Prediction Using Deep Learning Approach for Reducing Building Energy Demand,https://core.ac.uk/download/483715342.pdf,'SDEWES Centre',10.13044/j.sdewes.d8.0378,,core
326904721,2021-04-12T00:00:00,"Gravitational waves are ripples in the space time fabric when high energy
events such as black hole mergers or neutron star collisions take place. The
first Gravitational Wave (GW) detection (GW150914) was made by the Laser
Interferometer Gravitational-wave Observatory (LIGO) and Virgo Collaboration on
September 14, 2015. Furthermore, the proof of the existence of GWs had
countless implications from Stellar Evolution to General Relativity.
Gravitational waves detection requires multiple filters and the filtered data
has to be studied intensively to come to conclusions on whether the data is a
just a glitch or an actual gravitational wave detection. However, with the use
of Deep Learning the process is simplified heavily, as it reduces the level of
filtering greatly, and the output is more definitive, even though the model
produces a probabilistic result. Our technique, Deep Learning, utilizes a
different implementation of a one-dimensional convolutional neural network
(CNN). The model is trained by a composite of real LIGO noise, and injections
of GW waveform templates. The CNN effectively uses classification to
differentiate weak GW time series from non-gaussian noise from glitches in the
LIGO data stream. In addition, we are the first study to utilize fine-tuning as
a means to train the model with a second pass of data, while maintaining all
the learned features from the initial training iteration. This enables our
model to have a sensitivity of 100%, higher than all prior studies in this
field, when making real-time detections of GWs at an extremely low
Signal-to-noise ratios (SNR), while still being less computationally expensive.
This sensitivity, in part, is also achieved through the use of deep signal
manifolds from both the Hanford and Livingston detectors, which enable the
neural network to be responsive to false positives.Comment: 9 pages, 4 figure","Deep Learning Techniques to make Gravitational Wave Detections from Weak
  Time-Series Data",http://arxiv.org/abs/2007.05889,,,,core
337302905,2021-06-07T00:00:00,"Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy","Traffic Refinery: Cost-Aware Data Representation for Machine Learning on
  Network Traffic",http://arxiv.org/abs/2010.14605,,,,core
390285317,2021-01-01T00:00:00,"International audienceIn the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis. This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced Blanchard et al., 2011. We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets",Domain Generalization by Marginal Transfer Learning,,Microtome Publishing,,,core
478072019,2021-09-22T10:48:31,"Since SARS-CoV-2 is likely to become endemic in many countries, it will require not only short-term
support but also long-term support, as social distancing policies cannot be extended for long. Therefore, a technological
platform for epidemiological surveillance can represent a fundamental tool. The impact of the project is essential for public
health actors to design and evaluate policies aimed at the safe reactivation of social activities after social distancing policies
are suspended. We also consider this software service as a basic piece in the digital Transformation strategy, since it allows
us to anticipate the behaviors and necessary resources that adapt the needs with the provision in a dynamic way, but
adjusted to reality. This anticipation approach becomes a pillar in the digital strategy of any company, Administration and
education center. The tool includes a mechanism based on Artificial Intelligence for data analysis in order to have a
dynamic understanding of symptoms, evolution, social space-time data and the relationships between them, which will
allow the relevant entities to optimize resources such as virus detection tests and positive test controls.dado que resulta probable que el SARS-CoV-2 se vuelva endémico en muchos países, requerirá no sólo
apoyo a corto plazo sino también a largo plazo, ya que las políticas de distanciamiento social no pueden extenderse por
mucho tiempo. Por lo tanto, una plataforma tecnológica de vigilancia epidemiológica puede representar una herramienta
fundamental. El impacto del proyecto resulta esencial para que los actores relacionados con la salud pública diseñen y
evalúen políticas destinadas a la reactivación segura de las actividades sociales después de que se suspendan las políticas
de distanciamiento social. Consideramos también este servicio software como una pieza básica en la estrategia de
Transformación digital, ya que permite anticipar comportamientos y recursos necesarios que amolden las necesidades con
la provisión de manera dinámica, pero ajustada a la realidad. Este enfoque de anticipación se vuelve un pilar en la
estrategia digital de cualquier empresa, Administración y centro de educación. La herramienta incluye un mecanismo
basado en Inteligencia Artificial para el análisis de datos con el fin de tener una comprensión dinámica de los síntomas, la
evolución, los datos espacio-temporales sociales y las relaciones entre ellos, lo que permitirá a las entidades relevantes
optimizar recursos como las pruebas de detección de virus y controles de prueba positivo",Conceptual architecture of the epidemiological surveillance technology platform for COVID-19,,Campus Virtuales,,"[{'title': None, 'identifiers': ['2255-1514', 'issn:2255-1514']}]",core
402209797,2021-02-01T00:00:00,"COVID-19 has severely impacted mental health in vulnerable demographics, in particular older adults, who face unprecedented isolation. Consequences, while globally severe, are acutely pronounced in low- and middle-income countries (LMICs) confronting pronounced gaps in resources and clinician accessibility. Social robots are well-recognized for their potential to support mental health, yet user compliance (i.e., trust) demands seamless affective human-robot interactions; natural ‘human-like’ conversations are required in simple, inexpensive, deployable platforms. We present the design, development, and pilot testing of a multimodal robotic framework fusing verbal (contextual speech) and nonverbal (facial expressions) social cues, aimed to improve engagement in human-robot interaction and ultimately facilitate mental health telemedicine during and beyond the COVID-19 pandemic. We report the design optimization of a hybrid face robot, which combines digital facial expressions based on mathematical affect space mapping with static 3D facial features. We further introduce a contextual virtual assistant with integrated cloud-based AI coupled to the robot’s facial representation of emotions, such that the robot adapts its emotional response to users’ speech in real-time. Experiments with healthy participants demonstrate emotion recognition exceeding 90% for happy, tired, sad, angry, surprised and stern/disgusted robotic emotions. When separated, stern and disgusted are occasionally transposed (70%+ accuracy overall) but are easily distinguishable from other emotions. A qualitative user experience analysis indicates overall enthusiastic and engaging reception to human-robot multimodal interaction with the new framework. The robot has been modified to enable clinical telemedicine for cognitive engagement with older adults and people with dementia (PwD) in LMICs. The mechanically simple and low-cost social robot has been deployed in pilot tests to support older individuals and PwD at the Schizophrenia Research Foundation (SCARF) in Chennai, India. A procedure for deployment addressing challenges in cultural acceptance, end-user acclimatization and resource allocation is further introduced. Results indicate strong promise to stimulate human-robot psychosocial interaction through the hybrid-face robotic system. Future work is targeting deployment for telemedicine to mitigate the mental health impact of COVID-19 on older adults and PwD in both LMICs and higher income regions",Robotic telemedicine for mental health: a multimodal approach to improve human-robot engagement,,'Frontiers Media SA',10.3389/frobt.2021.618866,"[{'title': 'Frontiers in Robotics and AI', 'identifiers': ['issn:2296-9144', '2296-9144']}]",core
387245336,2021-01-13T08:00:00,"As machine learning and deep learning systems continue to find applications in science and engineering, the problem of providing these systems with high-quality data continues to increase in importance. Many of these systems utilize machine vision as their primary source of information, and in order to maximally leverage their abilities it is important to be able to provide them with high quality, accurate data. Unfortunately, many sets of tracking data extracted from video suffer from the problem of missing frames, which can arise from a multitude of causes depending on the system. These missing frames can result in confusion between object-track links, or even to the loss of tracking altogether.
This work is motivated by the specific problem of repairing flight tracking information in a pre-existing dataset involving large swarms of birds. These swarms can contain very large numbers of small birds, and the tracking information is derived from image segmentation of video, thus confusion between several objects in the data is a common and pervasive issue. The amount of data and the large number of tracked individuals necessitates efficient and trustworthy algorithms for interpolating between endpoints of known tracks.
We perform a literature review that discusses several approaches to track reconstruction and highlights the need for effective and accurate interpolation. We evaluate the effectiveness of a Kalman filter implementation on several tracks from such a dataset as well as synthetic tracks designed to simulate the flocking behavior of another species of interest. We also propose a novel interpolation algorithm, Stochastic Straight-Line Perturbation (SSLP), suitable for use on its own or as a pre-processing step for other algorithms. Using complete tracks, real and synthetic, with synthetically created gaps, we compare the accuracy of the Kalman filter to that of SSLP, as well as that of the Kalman filter operating on the same data pre-processed by SSLP. We find empirically that not only does SSLP outperform the Kalman filter on gap reconstruction in a number of scenarios, but it can also improve the performance of a Kalman filter, or potentially another tracking algorithm, when used as a pre-processing step. We also discuss performance trade-offs and potential limitations for its application.
For future work, we propose an alternate state-space model that may have some potential to improve upon the performance of the Kalman filter versus the model employed. We also discuss several possible strategies for further improving upon the performance of the SSLP algorithm both in terms of prediction accuracy and computational load, as well as several possible future applications",Methods for Object Tracking With Machine Vision,,PDXScholar,,,core
426963251,2021-04-21T10:05:33,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search",Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,,'Institute of Electrical and Electronics Engineers (IEEE)',10.23919/DATE.2019.8714959,,core
491167396,2021-07-03T00:00:00,"Reinforcement Learning (RL) is emerging as tool for tackling complex control and decision-making problems. However, in high-risk environments such as healthcare, manufacturing, automotive or aerospace, it is often challenging to bridge the gap between an apparently optimal policy learned by an agent and its real-world deployment, due to the uncertainties and risk associated with it. Broadly speaking RL agents face two kinds of uncertainty, 1. aleatoric uncertainty, which reflects randomness or noise in the dynamics of the world, and 2. epistemic uncertainty, which reflects the bounded knowledge of the agent due to model limitations and finite amount of information/data the agent has acquired about the world. These two types of uncertainty carry fundamentally different implications for the evaluation of performance and the level of risk or trust. Yet these aleatoric and epistemic uncertainties are generally confounded as standard and even distributional RL is agnostic to this difference. Here we propose how a distributional approach (UA-DQN) can be recast to render uncertainties by decomposing the net effects of each uncertainty . We demonstrate the operation of this method in grid world examples to build intuition and then show a proof of concept application for an RL agent operating as a clinical decision support system in critical care",Enabling risk-aware Reinforcement Learning for medical interventions through uncertainty decomposition,,,,,core
482000565,2021-01-01T00:00:00,"The use of fixed or scheduled setpoints combined with varying occupancy patterns in buildings could lead to spaces being over or under-conditioned, which may lead to significant waste in energy consumption. The present study aims to develop a vision-based deep learning method for real-time occupancy activity detection and recognition. The method enables predicting and generating real-time heat gain data, which can inform building energy management systems and heating, ventilation, and air-conditioning (HVAC) controls. A faster region-based convolutional neural network was developed, trained and deployed to an artificial intelligence-powered camera. For the initial analysis, an experimental test was performed within a selected case study building\u27s office space. Average detection accuracy of 92.2% was achieved for all activities. Using building energy simulation, the case study building was simulated with both ‘static’ and deep learning influenced profiles to assess the potential energy savings that can be achieved. The work has shown that the proposed approach can better estimate the occupancy internal heat gains for optimising the operations of building HVAC systems",Occupancy Heat Gain Detection and Prediction Using Deep Learning Approach for Reducing Building Energy Demand,https://core.ac.uk/download/482000565.pdf,'SDEWES Centre',10.13044/j.sdewes.d8.0378,,core
477814430,2021-08-01T07:00:00,"This is the second volume of the Advances in Global Services and Retail Management Book Series. This volume has the following parts:   Part 1: Hospitality and Tourism Part 2: Marketing, E-marketing, and Consumer Behavior Part 3: Management Part 4: Human Resources Management Part 5: Retail Management Part 6: Economics Part 7: Accounting and Finance Part 8: Sustainability and Environmental Issues Part 9: Information Technology 
ISBN: 978-1-955833-03-5  Hospitality and Tourism  Significance of VR in the spa: A spatial analysis  Irini Lai Fun Tang, Schultz Zhi Bin Xu, and Eric Chan   Social media marketing in rural hospitality and tourism destination research  Samuel Adeyinka-Ojo and Shamsul Kamariah Abdullah   All aboard! Is space tourism still a fantasy or a reality: An investigation on Turkish market  Emrah Tasarer, Vahit Oguz Kiper, Orhan Batman, and Oguz Turkay   Strategic consciousness and business performance relationship of open innovation strategies in food and beverage businesses  Muhsin Halis, Kazim Ozan Ozer, Hasan Cinnioglu, and Zafer Camlibel   The effects of COVID-19 epidemic on guided tours and alternative tour samples from Turkey  Bayram Akay   The effect of COVID-19 phobia on holiday intention  Halil Akmese and Ali Ilgaz    The effect of the usage of virtual reality in tourism education on learning motivation  Sarp Tahsin Kumlu and Emrah Ozkul   The impact of effective implementation of customer relationship management to the success of hotels in Afikpo North local government of Ebonyi State, Nigeria  Ogboagha Callister and Managwu Lilian   The influence of study travel on quality-oriented education: The case of Handan, China  Wang Jingya and Alaa Nimer Abukhalifeh   The impact of U.S. Cuba policies on Cuban tourism industry: Focus on the Obama and Trump Administration  Jukka M. Laitamaki, Antonio Diaz Medina, and Lisandra Torres Hechavarria   Determination of students’ characteristics and perspectives about social entrepreneurship: A case of Anadolu University  Muhammed Kavak, Ipek Itir Can, and Emre Ozan Aksoz   The place of Kazakhstan tourism sector in the countries of the region in terms of transportation infrastructure  Maiya Myrzabekova, Muhsin Halis, and Zafer Camlibel   What are tour guides most praised for? A sharing economy perspective  Derya Demirdelen-Alrawadieh and Ibrahim Cifci   An examination of representations for USA in tourism brochures for Chinese market  Yasong Wang   An exploratory study on cognitive internship perception of tourism students  Ozge Buyuk and Gulsah Akkus   Are you afraid to travel during COVID-19?  Gulsum Tabak, Sibel Canik, and Ebru Guneren   Destination management during the health emergency: A bibliometric analysis  Valentina Della Corte, Giovanna Del Gaudio, Giuliana Nevola, Enrico Di Taranto, and Simone Luongo   Determination of food neophobia levels of International Mersin Citrus Festival participants  Sevda Sahilli Birdir, Nurhayat Iflazoglu, and Kemal Birdir   Analysis of effectiveness of industrial exposure training undertaken by students of hospitality management in star hotels  G. Saravana Kumar   Conceptualization of ecotourism service experiences framework from the dimensions of motivation and quality of experiences: Four realms of experience approach  Jennifer Kim Lian Chan   Does Coronavirus (COVID-19) transform travel and tourism to automation (robots)?  M. Omar Parvez, Ali Ozturen, and Cihan Cobanoglu   Efficiency of internal control systems and the effect of organizational structure and culture on internal control systems in accommodation industry  Kadriye Alev Akmese and Ali Ilgaz   Ethical perceptions of housekeeping department employees: A study in Izmir Province  Tuba Turkmendag and Bayram Sahin   Factors that prevent participation of tourists in online co-creation activities  Resat Arica, Feridun Duman, and Abdulkadir Corbaci   Health sector after COVID-19: Salt thermal facilities example  Azize Serap Tuncer and Sinan Bulut   PRISMA statement and thematic analysis framework in hospitality and tourism research  Samuel Adeyinka-Ojo   Evaluation of Turkish nights as a tourism product: The case of Cappadocia  Meral Buyukkuru, Eda Ozgul Katlav, and Firdevs Yonet Eren   Customer perceptions against COVID-19 precautionary measures of the restaurants: The case of Istanbul-Turkey  Elif Kaymaz and Sevki Ulema   Analysis of e-complaints regarding hotel restaurants during COVID-19 process: The case of Antalya  Sevim Usta and Serkan Sengul    Marketing, E-marketing, and Consumer Behavior  Materialistic social consumption amidst COVID-19 pandemic: Terror management theory in the Malaysia context  Seong-Yuen Toh and Siew-Wai Yuan   A conceptual framework for the mediating role of the flow experience between destination brand experience and destination loyalty  Ipek Kazancoglu and Taskin Dirsehan   Investigating drivers influencing choice behaviour of Islamic investment products  Hanudin Amin   Local food festivals within the scope of destination branding  Hatice Akturk and Atilla Akbaba   Marketing a destination on social media: Case of three municipalities of Izmir  Huseyin Ozan Altin and Ige Pirnar   Perceived usefulness, ease of use, online trust and online purchase intention: Mediating role of attitude towards online purchase  Muhammed Yazeed, Mohammed Aliyu Dantsoho, and Adamu Ado Abubakar    Social media framework for businesses  Nawel Amrouche   Social media marketing the African door of return experience in Badagry-Nigeria  Huseyin Arasli, Maryam Abdullahi, and Tugrul Gunay   The effect of corporate social responsibility on consumer-based brand equity: A research on automobile brands  Ali Koroglu and Ibrahim Avci   The effect of superstitions on consumer luck, horoscope and evil eye-oriented purchasing behavior: A study in Turkey  Ibrahim Avci and Salih Yildiz   The evaluation of S-D orientation on service innovation and performance of airline  Inci Polat and Ozlem Atalik   Brand new leisure constraint: COVID-19  Guliz Coskun    The impact of consumers price level perception on emotions towards supermarkets  Abdulcelil Cakici and Sena Tekeli   The impact of TikTok’s plastic surgery content on adolescents’ self-perception and purchase intention  Markus Rach   Accelerated modernity: What are the social media stories undergraduate students engage with?  Pericles Asher Rospigliosi and Sebastian Raza-Mejia   Virtual influencer as celebrity endorsers  Fanny Cheung and Wing-Fai Leung   Does millennial shopping orientation using augmented reality enabled mobile applications really impact product purchase intention?  Anil Kumar   Exposure to e-cigarette marketing and product use among highly educated adults  Onur Sahin   Extending the theory of planned behavior to explain intention to use online food delivery services in the context of COVID -19 pandemic  Ahmed Chemseddine Bouarar, Smail Mouloudj, and Kamel Mouloudj   Factors affecting investors’ buying decision in real estate market in Northern Cyprus  Gurkan Arslan and Karen Howells   From home to the store: Combined effect of music and traffic on consumers shopping behaviour  Luigi Piper, Lucrezia Maria de Cosmo, Maria Irene Prete, and Gianluigi Guido   Market expansion and business growth from the perspective of resources and capabilities: The case of a micro-enterprise  Jose G. Vargas-Hernandez and Omar C. Vargas-Gonzalez   How learning style interacts with voice-assisted technology (VAT) in consumer task evaluation  Bonnie Canziani and Sara MacSween     Effect of brand credibility and innovation on customer based brand equity and overall brand equity in Turkey: An investigation of GSM operators  Suphan Nasir and Ozge Guvendik   Value chain for a B school in India  Vimal Chandra Verma and Devashish Das Gupta    Management  AI as a boost for startups companies: Evidence from Italy  Irene Di Bernardo, Marco Tregua, Greco Fabio, and Ruggiero Andrea   The role of quality management applications for corporate reputations  Ibrahim Sapaloglu and Isik Cicek   Toxicity in organizations: A sample study on the perceived toxicity in Turkish academicians  Mustafa Hakan Atasoy and Muhsin Halis    Which resources are matter to healthcare performance? A case study on Bahrain  Mahmood Asad Ali and Mohamed Sayed Abou Elseoud   Case study: HereWay Inc. European expansion: A facility location problem  Mikhail M. Sher, Michael T. Paz, and Donald R. (Bob) Smith   In search of the effective mission statement: Structural support of the firm’s culture to augment financial performance  Seong-Yuen Toh   Innovation labs to support tourism organization in transforming crisis into opportunities: Insight from a case study  Francesco Santarsiero, Daniela Carlucci, and Giovanni Schiuma   Novelty and success of healthcare service innovation: A comparison between China and the Netherlands  Yu Mu, Rujun Wang and Ying Huang   Public private partnership in selected countries: A comparative analysis   Bekir Parlak and Abdullahi Suleiman Hashi   Strategic orientation of service enterprises towards customers  Korhan Arun and Saniye Yildirim Ozmutlu   The effects of organizational culture on information sharing attitude  Mohammadi Lanbaran Nasrin and Cicek Isik   The impact of industry 4.0 strategy on the work-life balance of employees  Ali Sukru Cetinkaya   The mediating effect of psychological empowerment on inclusive leadership and innovative work behaviour: A research in hotels  Emete Toros, Ahmet Maslakci, and Lutfi Surucu   Assessment of industry 4.0 on manufacturing enterprises: Demographic perspective  Ali Sukru Cetinkaya and M. Kemal Unsacar    Human Resources Management  Affective commitment in new hires’ onboarding? The role of organizational socialization in the fashion retail industry  Pui Sze Chan, Ho Ching Ching, Pui Yi Ng, and Annie Ko   Do burnout perception levels of nurses working in the health sector differ according to demographic characteristics?  Irfan Akkoc and Korhan Arun   Examining a moderating effect of employee turnover between recruitment and selection practice and organizational performance in Maldives civil service sector  Fathmath Muna, Azam S. M. Ferdous, and Ahmad Albattat   Personnel relationships in the workplace  Ali Sukru Cetinkaya, Shafiq Habibi, and Umut Yavuz   The evolution of human resources empowerment theory: A literature review (1970–2020)  Theodoros Stavrinoudis and Moschos Psimoulis   Teamwork, satisfaction and mediating effect of affective, continuance and normative commitments on employee’s loyalty  Thalita Aparecida Costa Nicolleti, Eduardo Roque Mangini, Leonardo Aureliano-Silva, Cristiane Sales Pires, and Carolina Aparecida de Freitas Dias   Perceptions of teachers in educational institutions regarding the principles of teaching professional ethics  Gulsah Aki, Nejat Ira, and Hasan Arslan   Influence of psychological empowerment on employee competence in Nigerian universal basic education system: The mediating role of work engagement  Isah Sani, Rashidah Binti Mohammad Ibrahim, and Fazida Karim    Retail Management  Artificial intelligence in retailing  Ibrahim Kircova, Munise Hayrun Saglam, and Sirin Gizem Kose    Customer value in retailing (2000-2020): A narrative review and future research directions  Rajat Gera and Ashish Pruthi   Effect of social media marketing on online retail performance of Konga Nigeria LTD  Abubakar Ado Adamu, Muhammed Yazeed, Mohammed Aliyu Dantsoho, Jamilu Abdulkadir, and Aliyu Audu Gemu   Employment of blue-collar workers in organized retail sector: The case of Turkey  Inci Kayhan-Kuzgun   Saving grace: Digitization to stay or address crisis?   Smitha Vasudevan   Inclusion of disabled consumers in online retail landscape: Web accessibility conformance of Turkish organized food retailers’ web sites  Asiye Ayben Celik   A customer segmentation model proposal for retailers: RFM-V  Pinar Ozkan and Ipek Deveci Kocakoc    Economics  Nigeria’s economic management: Reflections through monthly interest rate movement from 1996 to 2020 and beyond  Job Nmadu, Halima Sallawu, and Yebosoko Nmadu   A qualitative study of perceptions of the residents of Sidon, Lebanon regarding the economic effect on Sidon with reference to repatriation of the Palestinian refugees  Raja El Majzoub and Karen Howells   Three keys of development: Knowledge, efficiency and innovative entrepreneurship  Irfan Kalayci, Ali Soylu, and Baris Aytekin   Tourism and women empowerment: Empirical findings from past experience and predictions for the post-COVID era  Burcu Turkcan   COVID-19 effect on FDI motivation and their impact on service sector: Case of Georgia  Vakhtang Charaia and Mariam Lashkhi   Economic cooperation between Central Caucasus, China, and EU, under COVID-19 challenges  Vakhtang Charaia and Mariam Lashkhi   Effect of real exchange rate and income on international tourist arrivals for Turkey  Erhan Aslanoglu, Oral Erdogan, and Yasin Enes Aksu   Innovative entrepreneurship in Turkey: Micro and macro perspectives  Irfan Kalayci, Baris Aytekin, and Ali Soylu   Optimal fiscal and price stability in Germany: Autoregressive distributed lags (ARDL) cointegration relationship  Ergin Akalpler and Dahiru Alhaji Birnintsabas   Struggle with COVID-19 crisis within the scope of financial national security: The example of the Republic of Turkey  Silacan Karakus   The nexus between fiscal freedom and investment freedom: The case of E7 countries  Mehmet Bolukbas   To be or not to be a female entrepreneur in the Mexicali Valley  Roberto Burgueno Romero and Jose David Ledezma Torrez    Accounting and Finance  Comparative measurement of working capital efficiency for Borsa Istanbul restaurants and hotels for the COVID-19 period and previous quarters  Fatih Gunay and Gary Cokins   Relationship between business confidence index and non-financial firms foreign exchange assets and liabilities: Evidence from ARDL bound approach  Ilkut Elif Kandil-Goker   The impact of RTGS on internal control - A comparative study between some Iraqi banks  Salowan H. Al Taee and Noor A. Radhi   The impact of working capital on cash management under IAS 7 framework: An examination of tourism listed companies in Indonesia and Turkey  Tri Damayanti and Tuba Derya Baskan     A nexus between mergers & acquisitions and financial performance of firms: A study of industrial sector of Pakistan  Fiza Quareshi, Mukhtiar Ali, and Salar Hussain   Decentralized approach to deep-learning based asset allocation  Sarthak Sengupta, Priyanshu Priyam, and Anurika Vaish    Sustainability and Environmental Issues  Blockchain technology applied to the Consortium Etna DOC to avoid counterfeiting  Matarazzo Agata, Edoardo Carmelo Spampinato, Sergio Arfo, Ugo Sinigaglia, Antonino Bajeli, and Salvino Benanti   Eco-label certification, hotel performance and customer satisfaction: Analysis of a case study and future developments  Michele Preziosi, Alessia Acampora, Roberto Merli, and Maria Claudia Lucchetti   The integration of circular economy in the tourism industry: A framework for the implementation of circular hotels  Martina Sgambati, Alessia Acampora, Olimpia Martucci, and Maria Claudia Lucchetti   Using the theory of planned behavior to explore green food purchase intentions  Katrina Anna Auza and Kamel Mouloudj   Survey on purchasing methods of food products in Tarragona and Catania  Matarazzo Agata, Vazzano Tommaso Alberto, and Squillaci Carmelo    Information Technology  Comparative analysis of tools for matching work-related skill profiles with CV data and other unstructured data  Florian Beuttiker, Stefan Roth, Tobias Steinacher, and Thomas Hanne   State-of-the-art next generation open innovation platforms  Murielle De Roche, Monika Blaser, Patrick Hollinger, and Thomas Hanne   The coverage of AIOT based functional service: Case study of Asian futuristic hotel  Gege Wang, Irini Lai Fun Tang, Eric Chan, and Wai Hung Wilco Chan   The effect of the blockchain technology on service companies and food retailers: An overview of the blockchain use cases and applications  Gokhan Kirbac and Erkut Ergenc   The regulation problem of cryptocurrencies  Lamiha Ozturk and Ece Sulungur   Understanding information technology acceptance by physicians: Testing technology acceptance model  Anuruddha Indika Jagod",Advances in global services and retail management: Volume 2,,Digital Commons @ University of South Florida,,,core
489413429,2021-01-01T00:00:00,"Reinforcement Learning (RL) has the potential of solving complex continuous control tasks, with direct applications to robotics. Nevertheless, current state-of-the-art methods are generally unsafe to learn directly on a physical robot as exploration by trial-and-error can cause harm to the real world systems. In this paper, we leverage a framework for learning latent action spaces for RL agents from demonstrated trajectories. We extend this framework by connecting it to a variable impedance Cartesian space controller, allowing us to learn contact-rich tasks safely and efficiently. Our method learns from trajectories that incorporate both positional, but also crucially impedance-space information. We evaluate our method on a number of peg-in-hole task variants with a Franka Panda arm and demonstrate that learning variable impedance actions for RL in Cartesian space can be safely deployed on the real robot directly, without resorting to learning in simulation and a subsequent policy transfer",Learning Impedance Actions for Safe Reinforcement Learning in Contact-Rich Tasks,,"Department of Computer Science, Lund University, Sweden",,,core
491073597,2021-12-15T00:00:00,"A major impediment to successful drug development is the complexity, cost,
and scale of clinical trials. The detailed internal structure of clinical trial
data can make conventional optimization difficult to achieve. Recent advances
in machine learning, specifically graph-structured data analysis, have the
potential to enable significant progress in improving the clinical trial
design. TrialGraph seeks to apply these methodologies to produce a
proof-of-concept framework for developing models which can aid drug development
and benefit patients. In this work, we first introduce a curated clinical trial
data set compiled from the CT.gov, AACT and TrialTrove databases (n=1191
trials; representing one million patients) and describe the conversion of this
data to graph-structured formats. We then detail the mathematical basis and
implementation of a selection of graph machine learning algorithms, which
typically use standard machine classifiers on graph data embedded in a
low-dimensional feature space. We trained these models to predict side effect
information for a clinical trial given information on the disease, existing
medical conditions, and treatment. The MetaPath2Vec algorithm performed
exceptionally well, with standard Logistic Regression, Decision Tree, Random
Forest, Support Vector, and Neural Network classifiers exhibiting typical
ROC-AUC scores of 0.85, 0.68, 0.86, 0.80, and 0.77, respectively. Remarkably,
the best performing classifiers could only produce typical ROC-AUC scores of
0.70 when trained on equivalent array-structured data. Our work demonstrates
that graph modelling can significantly improve prediction accuracy on
appropriate datasets. Successive versions of the project that refine modelling
assumptions and incorporate more data types can produce excellent predictors
with real-world applications in drug development.Comment: 17 pages (Manuscript); 3 pages (Supplemental Data); 9 figure","TrialGraph: Machine Intelligence Enabled Insight from Graph Modelling of
  Clinical Trials",http://arxiv.org/abs/2112.08211,,,,core
337295329,2021-02-27T00:00:00,"Space agencies and private companies prepare the beginning of human space
exploration for the 2030s with missions to put the first human on the Mars
surface. The absence of gravity and radiation, along with distance, isolation
and hostile environments, are expected to increase medical events where
previously unseen manifestations may arise. The current healthcare strategy
based on telemedicine and the possibility to stabilize and transport the
injured crewmember to a terrestrial definitive medical facility is not
applicable in exploration class missions. Therefore, the need for deploying the
full autonomous capability to solve medical emergencies may guide the design of
future onboard healthcare systems. We present ten basic principles and concept
design of a software suite to bring onboard decision support to help the crew
dealing with medical emergencies taking into consideration physiological
disturbances in space and spaceflight restrictions. 1) give real-time support
for emergency medical decision making, 2) give patient-specific advice for
executive problem-solving, 3) take into account available information from life
support and monitoring of crewmembers, 4) be fully autonomous from remote
facilities, 5) continuously adapt predictions to physiological disturbance and
changing conditions, 6) optimize emergency medical decision making in terms of
mission fundamental priorities, 7) take into account medical supplies and
equipment on board, 8) apply health standards for the level of care V, 9)
implement ethics responsibilities for spaceflights, and 10) apply ethical
standards for artificial intelligence. Based on these principles, we propose an
autonomous clinical decision support system (CDSS) to provide real-time advice
for emergency medical interventions on board of space exploration missions.Comment: 35 pages, 1 figure, 3 tables, 60 reference","Basic principles and concept design of a real-time clinical decision
  support system for managing medical emergencies on missions to Mars",http://arxiv.org/abs/2010.07029,,,,core
475043984,2021-01-01T00:00:00,"Digital agriculture, with the incorporation of Internet-of-Things (IoT)-based technologies, presents the ability to control a system at multiple levels (individual, local, regional, and global) and generates tools that allow for improved decision making and higher productivity. Recent advances in IoT hardware, e.g., networks of heterogeneous embedded devices, and software, e.g., lightweight computer vision algorithms and cloud optimization solutions, make it possible to efficiently process data from diverse sources in a connected (smart) farm. By interconnecting these IoT devices, often across large geographical distances, it is possible to collect data at different time scales, including in near real-time (i.e., with delays of only a few tens of seconds). This data can then be used for actionable insights, e.g., precise applications of soil supplements and reduced environmental footprint. Through LATTICE, we present an integrated vision for IoT solutions, data processing, and actionable analytics for digital agriculture. We couple this with discussion of economics and policy considerations that will underlie adoption of such IoT and ML technologies. Our paper starts off with the types of datasets in typical field operations, followed by the lifecycle for the data and storage, cloud and edge analytics, and fast information-retrieval solutions. We discuss what algorithms are proving to be most impactful in this space, e.g., approximate data analytics and on-device/in-network processing. We conclude by discussing analytics for alternative agriculture for generation of biofuels and policy challenges in the implementation of digital agriculture in the wild","<sc>Lattice</sc>: A Vision for Machine Learning, Data Engineering, and Policy Considerations for Digital Agriculture at Scale",,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/OJCS.2021.3085846,"[{'title': 'IEEE Open Journal of the Computer Society', 'identifiers': ['issn:2644-1268', '2644-1268']}]",core
490955230,2021-11-29T00:00:00,"The nonuniform quantization strategy for compressing neural networks usually
achieves better performance than its counterpart, i.e., uniform strategy, due
to its superior representational capacity. However, many nonuniform
quantization methods overlook the complicated projection process in
implementing the nonuniformly quantized weights/activations, which incurs
non-negligible time and space overhead in hardware deployment. In this study,
we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can
maintain the strong representation ability of nonuniform methods while being
hardware-friendly and efficient as the uniform quantization for model
inference. We achieve this through learning the flexible in-equidistant input
thresholds to better fit the underlying distribution while quantizing these
real-valued inputs into equidistant output levels. To train the quantized
network with learnable input thresholds, we introduce a generalized
straight-through estimator (G-STE) for intractable backward derivative
calculation w.r.t. threshold parameters. Additionally, we consider entropy
preserving regularization to further reduce information loss in weight
quantization. Even under this adverse constraint of imposing uniformly
quantized weights and activations, our N2UQ outperforms state-of-the-art
nonuniform quantization methods by 0.7~1.8% on ImageNet, demonstrating the
contribution of N2UQ design. Code will be made publicly available.Comment: Technical repor","Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via
  Generalized Straight-Through Estimation",http://arxiv.org/abs/2111.14826,,,,core
491029920,2021-12-11T00:00:00,"Over the past few years, deep neural networks (DNNs) have achieved tremendous
success and have been continuously applied in many application domains.
However, during the practical deployment in the industrial tasks, DNNs are
found to be erroneous-prone due to various reasons such as overfitting, lacking
robustness to real-world corruptions during practical usage. To address these
challenges, many recent attempts have been made to repair DNNs for version
updates under practical operational contexts by updating weights (i.e., network
parameters) through retraining, fine-tuning, or direct weight fixing at a
neural level. In this work, as the first attempt, we initiate to repair DNNs by
jointly optimizing the architecture and weights at a higher (i.e., block)
level.
  We first perform empirical studies to investigate the limitation of whole
network-level and layer-level repairing, which motivates us to explore a novel
repairing direction for DNN repair at the block level. To this end, we first
propose adversarial-aware spectrum analysis for vulnerable block localization
that considers the neurons' status and weights' gradients in blocks during the
forward and backward processes, which enables more accurate candidate block
localization for repairing even under a few examples. Then, we further propose
the architecture-oriented search-based repairing that relaxes the targeted
block to a continuous repairing search space at higher deep feature levels. By
jointly optimizing the architecture and weights in that space, we can identify
a much better block architecture. We implement our proposed repairing
techniques as a tool, named ArchRepair, and conduct extensive experiments to
validate the proposed method. The results show that our method can not only
repair but also enhance accuracy & robustness, outperforming the
state-of-the-art DNN repair techniques.Comment: 33 pages, 7 figure","ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural
  Networks",http://arxiv.org/abs/2111.13330,,,,core
479191725,2021-05-01T00:00:00,"Abstract This paper presents skin color enhancement based on favorite skin color to agree with user‐defined favorite skin color using improved histogram equalization with variable enhancement degree (IHEwVED) and machine learning methods. The skin color to be adjusted in the input image is shifted to favorite skin color by using novel control parameters of the proposed IHEwVED method. Three different novel display device‐dependent color image processing methods are introduced based on hsv and yiq color space to obtain the desired enhanced output images. A reduced convolutional neural network and the novel ensemble extreme learning machine (EELM) architectures are developed and implemented in a field‐programmable gate array to test, synthesize, and validate the recognition capability of the user‐defined favorite skin color. The less computational complex proposed IHEwVED‐EELM method recognizes 45 to 50 favorite skin color per second of test images by consuming 0.035 second training time with training root mean square error (RMSE) of 0.0048 and testing RMSE of 0.01208. Finally, a stand‐alone favorite skin color restoration system is developed using the high‐speed video processor NI‐PXI‐1031 based on the IHEwVED‐EELM method in the Python‐OpenCV environment. The laboratory experimental performances ascertain the real‐time ability of the proposed favorite skin color restoration method",FPGA‐based favourite skin colour restoration using improved histogram equalization with variable enhancement degree and ensemble extreme learning machine,,'Institution of Engineering and Technology (IET)',10.1049/ipr2.12101,"[{'title': 'IET Image Processing', 'identifiers': ['issn:1751-9659', '1751-9659', 'issn:1751-9667', '1751-9667']}]",core
390059886,2021-06-16T00:00:00,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model",Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,,'Elsevier BV',10.1016/j.ymssp.2020.107510,,core
490553146,2021-09-01T00:00:00,"Support vector machine (SVM) is one of the most powerful technologies of machine learning, which has been widely concerned because of its remarkable performance. However, when dealing with the classification problem of large-scale datasets, the high complexity of SVM model leads to low efficiency and become impractical. Due to the sparsity of SVM in the sample space, this paper presents a new parallel data geometry analysis (PDGA) algorithm to reduce the training set of SVM, which helps to improve the efficiency of SVM training. The PDGA introduce Mahalanobis distance to measure the distance from each sample to its centroid. And based on this, proposes a method that can identify non support vectors and outliers at the same time to help remove redundant data. When the training set is further reduced, cosine angle distance analysis method is proposed to determine whether the samples are redundant data, ensure that the valuable data are not removed. Different from the previous data geometry analysis methods, the PDGA algorithm is implemented in parallel, which greatly saving the computational cost. Experimental results on artificial dataset and 6 real datasets show that the algorithm can adapt to different sample distributions. Which significantly reduce the training time and memory requirements without sacrificing the classification accuracy, and its performance is obviously better than the other five competitive algorithms",A new parallel data geometry analysis algorithm to select training data for support vector machine,,'American Institute of Mathematical Sciences (AIMS)',10.3934/math.2021806,"[{'title': 'AIMS Mathematics', 'identifiers': ['issn:2473-6988', '2473-6988']}]",core
480168048,2021-01-01T00:00:00,"Shared e-mobility services have been widely tested and piloted in cities across the globe, and already woven into the fabric of modern urban planning. This paper studies a practical yet important problem in those systems: how to deploy and manage their infrastructure across space and time, so that the services are ubiquitous to the users while sustainable in profitability. However, in real-world systems evaluating the performance of different deployment strategies and then finding the optimal plan is prohibitively expensive, as it is often infeasible to conduct many iterations of trial-and-error. We tackle this by designing a high-fidelity simulation environment, which abstracts the key operation details of the shared e-mobility systems at fine-granularity, and is calibrated using data collected from the real-world. This allows us to try out arbitrary deployment plans to learn the optimal given specific context, before actually implementing any in the real-world systems. In particular, we propose a novel multi-agent neural search approach, in which we design a hierarchical controller to produce tentative deployment plans. The generated deployment plans are then tested using a multi-simulation paradigm, i.e., evaluated in parallel, where the results are used to train the controller with deep reinforcement learning. With this closed loop, the controller can be steered to have higher probability of generating better deployment plans in future iterations. The proposed approach has been evaluated extensively in our simulation environment, and experimental results show that it outperforms baselines e.g., human knowledge, and state-of-the-art heuristic-based optimization approaches in both service coverage and net revenue",Deployment optimization for shared e-mobility systems with multi-agent deep neural search,https://core.ac.uk/download/480168048.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
475698943,2021-06-03T12:55:40,"With the introduction of edge analytics, IoT devices are becoming smart and ready for AI applications. A few modern ML frameworks are focusing on the generation of small-size ML models (often in kBs) that can directly be flashed and executed on tiny IoT devices, particularly the embedded systems. Edge analytics eliminates expensive device-to-cloud communications, thereby producing intelligent devices that can perform energy-efficient real-time offline analytics. Any increase in the training data results in a linear increase in the size and space complexity of the trained ML models, making them unable to be deployed on IoT devices with limited memory. To alleviate the memory issue, a few studies have focused on optimizing and fine-tuning existing ML algorithms to reduce their complexity and size. However, such optimization is usually dependent on the nature of IoT data being trained. In this paper, we presented an approach that protects model quality without requiring any alteration to the existing ML algorithms. We propose SRAM-optimized implementation and efficient deployment of widely used standard/stable ML-frameworks classifier versions (e.g., from Python scikit-learn). Our initial evaluation results have demonstrated that ours is the most resource-friendly approach, having a very limited memory footprint while executing large and complex ML models on MCU-based IoT devices, and can perform ultra-fast classifications while consuming 0 bytes of SRAM. When we tested our approach by executing it on a variety of MCU-based devices, the majority of models ported and executed produced 1-4x times faster inference results in comparison with the models ported by the sklearn-porter, m2cgen, and emlearn libraries.This publication has emanated from research supported in
part by a research grant from Science Foundation Ireland (SFI)
under Grant Number SFI/16/RC/3918 (Confirm) and also by
a research grant from Science Foundation Ireland (SFI) under
Grant Number SFI/12/RC/2289_P2 (Insight), with both grants
co-funded by the European Regional Development Fund",Ultra-fast machine learning classifier execution on IoT devices without SRAM consumption,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
328270272,2021-06-03T00:00:00,"Over the past decade, the proliferating accessibility of digital technologies has significantly broadened the scope of museums and heritage institutions to increase the scale of their visitor engagement and collections management. Interactive digital aesthetics and maturing data analytics methodologies have become a staple informing exhibition design and database handling the world over. With their powerful information processing capabilities, emerging technologies are shifting the dominant agenda of carefully curated information access toward self-directed, i.e. user-led exploration of holdings. Instances of experimentation with dynamic real-time data assembly as a key structuring principle of exhibition experience are rife, with Refik Anadol’s installation Archive Dreaming (2017; SALT Galata, Istanbul, Turkey) figuring as one compelling example. Here, visitors enter a circular projection space and find themselves enveloped in an immersive media space where they can interactively engage with 1.7M archival documents through a tablet interface, with their assembly supported by artificial intelligence. While visitors are presented with the means to explore a multitude of links between collection items in a powerful setting that invites embodied engagement with data, the interaction design falls short of fully unlocking the capabilities of extended reality (XR) affordances: Without the ability to store user interactions on the system’s database, each transaction remains a stand-alone encounter that does not evolve the collection as such, nor does it allow the user to carry forth the insights gained in the encounter beyond the confines of the projection space as such, missing an opportunity to closely integrate the installation with its broader environment.In light of such considerations, the presentation critically reflects the limitations of extant interactive methodologies for exhibition design and collections management and propose a rationale for their expansion. The authors contextualise this discussion through a survey of large-scale digital museological projects undertaken across the past 10 years at the iCinema Research Center (Sydney, Australia), which has included work by Sarah Kenderdine, Jeffrey Shaw and Dennis Del Favero (e.g. Pure Land, mARCHIVE), reflecting strengths and limitations encountered. Key focus of the paper will however rest on the authors’ new netARCHIVE project currently underway in collaboration with Sydney’s Museum of Applied Arts & Sciences (ARC LP180100126), which develops a networked narrative framework for the integration of virtual and physical assets across the museum’s dispersed collections. Deploying a number of platforms (ranging from mobile phones to fully immersive CAVEs), visitors will be able –in concert with an AI– to establish, explore, save and share links between virtual and physical assets, following customised trajectories across three museum sites as well as extending into their surrounding environs. Embedded into strict protocols, the generated data will be made available to collections management, informing new curatorial pipelines and decision making. Special consideration is thereby afforded to the appropriate and sensitive handling of assets linked to Indigenous Australian cultures. The presentation will outline the conceptual and methodological underpinnings of this project, how it responds to limitations of current design and curatorial approaches, sketch implementation plans and anticipate new challenges and trajectories born from this research.ReferencesAnadol, R. (2019, November 28). Archive Dreaming. Retrieved from http://refikanadol.com/works/archive-dreaming/ iCinema Research Centre (2019, November 28). mARCHIVE. Retrieved from http://www.icinema.unsw.edu.au/projects/marchive/project-overview/  iCinema Research Centre (2019, November 28). Pure Land. Retrieved from http://www.icinema.unsw.edu.au/projects/all-projects",Networked Narrative + XR: A Framework for Distributed Collections,,,,,core
327065141,2021-07-10T00:00:00,"Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public",Fast Neural Models for Symbolic Regression at Scale,http://arxiv.org/abs/2007.10784,,,,core
387304918,2021-01-07T00:00:00,"Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.Comment: This article has been accepted in IEEE Transactions on Emerging
  Topics in Computing. 17 pages, 12 figures, IEEE copyrigh","SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning",http://arxiv.org/abs/2101.02780,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TETC.2021.3050733,,core
481663173,2021-08-01T00:00:00,"RÉSUMÉ : L’évolution et la réussite des entreprises sont de plus en plus liées à leur intégration aux écosystèmes qui les entourent. La prise en compte de ces écosystèmes est donc primordiale pour chaque entreprise qui souhaite déterminer son positionnement stratégique. L’objectif de ce mémoire de maîtrise est de proposer une méthode et de fournir les informations nécessaires à un positionnement stratégique prenant en compte l’écosystème de l’entre-prise. L’étude de cas porte sur une entreprise fournissant des solutions pour aider d’autres acteurs à réaliser leurs transformations numériques. Cette entreprise sera appelée l’entreprise θ. Elle appartient à un grand groupe du domaine de la défense et de l’aérospatiale. Le cas d’étude permet de tester les méthodes proposées. La question de recherche est la suivante : « Comment intégrer la prise en compte de l’écosystème à une méthode de positionnement stratégique ? ». Afin de répondre à cette question, des cartographies de l’écosystème de défense et d’aérospatiale canadien et de l’écosystème interne au groupe d’appartenance de l’entreprise étudiée ont d’abord été réalisées. Un questionnaire visant à évaluer la maturité d’une entité à adopter l’intelligence artificielle (IA) a ensuite été créé. Des entrevues et une enquête utilisant ce questionnaire ont ensuite été réalisées. Les entrevues contenaient des questions ouvertes visant notamment à comprendre la perception de l’écosystème par le répondant, ses ambitions en termes d’IA et les principaux bloquants qui l’empêchent d’implémenter les technologies de l’IA. On remarque par exemple que l’accès à des données fiables et en quantité pose encore problème pour de nombreux répondants. Le questionnaire vise quant à lui à permettre de calculer un indicateur de mesure de la maturité d’une entreprise à utiliser l’IA en se basant sur 4 piliers : technologique, financier, culturel et organisationnel. Enfin, une adaptation de la méthode de conception de feuilles de route « S-Plan » a été proposée et réalisée avec l’entreprise  à partir des informations obtenues par les différentes cartographies, les résultats des entrevues et du questionnaire. Ces différentes informations ont servi de point de départ aux ateliers de conception de feuilles de route. Le premier atelier a permis de définir une feuille de route générale et d’identifier des sujets d’intérêt par une approche innovation par l’o˙re/innovation par la demande (techno push/market pull). Des ateliers spécialisés ont ensuite été organisés afin de définir une feuille de route centrée sur chacun de ces sujets. Le modèle de conception de feuilles de route précédé d’une cartographie d’écosystème proposé dans ce mémoire et testé avec l’entreprise  a permis de répondre aux objectifs de recherche. La méthode permet en effet de définir un positionnement stratégique prenant en compte l’écosystème pour une entreprise technologique fournissant des solutions en IA. L’adoption de l’IA nécessitant une certaine maturité technologique avant de pouvoir être implémenté, l’analyse de données secondaires a d’abord permis de repérer les entités les plus prêtes à utiliser l’IA, le recueil de données par une méthode mixte (entrevues et questionnaires) a ensuite aidé à bien comprendre les besoins et bloquants des différents acteurs de l’écosystème vis-à-vis de l’IA. Ces données ont pu être intégrées au processus de conception de feuilles de route en tant qu’informations sur les besoins de l’écosystème auxquels l’entreprise pourrait répondre. L’approche innovation par l’o˙re/innovation par la demande mise en place a ensuite permis de confronter ces besoins aux capacités technologiques existantes de l’entreprise afin de déterminer les solutions technologiques à développer se basant sur les capacités technologiques de l’entreprise et permettant de répondre à de réels besoins identifiés au sein de l’écosystème.----------ABSTRACT : The evolution and success of companies is increasingly linked to their integration into the ecosystems that surround them. Taking these ecosystems into account is therefore essential for each company that wishes to determine its strategic positioning. The objective of this master’s thesis is to propose a method and to provide the necessary information for a strategic positioning taking into account the company’s ecosystem. The case study focuses on a company that provides solutions to help other players achieve their digital transformations. This company will be referred to as the company. It belongs to a large group in the field of defense and aerospace. The case study allows us to test the proposed methods. The research question is the following: ""How to integrate the consideration of the ecosystem into a strategic positioning method?"". In order to answer this question, mappings of the Canadian defense and aerospace ecosystem and of the ecosystem internal to the company’s group were first produced. A questionnaire to assess an entity’s readiness to adopt artificial intelligence (AI) was then created. Inter-views and a survey using this questionnaire were then conducted. The interviews contained open questions aimed at understanding the respondent’s perception of the ecosystem, their ambitions in terms of AI and the main obstacles that prevent them from implementing AI. For example, access to reliable data in quantity is still a problem for many respondents. The questionnaire aims to calculate an indicator to measure the maturity of a company to use AI based on 4 pillars: technological, financial, cultural and organizational. Finally, an adaptation of the ""S-Plan"" roadmap method was proposed and carried out with the com-pany based on the information obtained from the various maps, the results of the interviews and the questionnaire. This information served as a starting point for the roadmap design workshops. The first workshop defined a general roadmap and identified topics of interest through a techno push/market pull approach. Specialized workshops were then organized to define a roadmap focused on each of these topics. The roadmap design model preceded by an ecosystem mapping proposed in this dissertation and tested with the company allowed us to meet the research objectives. The method allows to define a strategic positioning taking into account the ecosystem for a technology company providing AI solutions. Since the adoption of AI requires a certain technological maturity before it can be implemented, the analysis of secondary data first made it possible to identify the entities most ready to use AI. The collection of data using a mixed method (interviews and questionnaires) then helped to understand the needs and blocking factors of the various actors in the ecosystem in relation to AI. This data could be integrated into the roadmap design process as information on the ecosystem needs that the company could address. The market pull/techno push approach then allowed us to compare these needs with the company’s existing technological capabilities in order to determine the technological solutions to be developed, based on the company’s technological capabilities and enabling to meet the real needs identified within the ecosystem",L’intégration de l’évaluation de l’écosystème d’innovation à la prise de décision en entreprise : une étude de cas de transformation numérique dans le secteur aérospatial,,,,,core
227186025,,"In this paper we present a new methodology for robot learning that combines ideas from statistical generalization and reinforcement learning. First we apply statistical generalization to compute an approximation for the optimal control policy as defined by training movements that solve the given task in a number of specific situations. This way we obtain a manifold of movements, which dimensionality is usually much smaller than the dimensionality of a full space of movement primitives. Next we refine the policy by means of reinforcement learning on the approximating manifold, which results in a learning problem constrained to the low dimensional manifold. We show that in some situations, learning on the low dimensional manifold can be implemented as an error learning algorithm. We apply golden section search to refine the control policy. Furthermore, we propose a reinforcement learning algorithm with an extended parameter set, which combines learning in constrained domain with learning in full space of parametric movement primitives, which makes it possible to explore actions outside of the initial approximating manifold. The proposed approach was tested for learning of pouring action both in simulation and on a real robotSistemų analizės katedraVytauto Didžiojo universiteta",Applying statistical generalization to determine search direction for reinforcement learning of movement primitives,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/HUMANOIDS.2012.6651500,,core
491096314,2021-11-01T00:00:00,"Proprioception, the ability to perceive one's own configuration and movement in space, enables organisms to safely and accurately interact with their environment and each other. The underlying sensory nerves that make this possible are highly dense and use sophisticated communication pathways to propagate signals from nerves in muscle, skin, and joints to the central nervous system wherein the organism can process and react to stimuli. In a step forward to realize robots with such perceptive capability, a flexible sensor framework that incorporates a novel modeling strategy, taking advantage of computational mechanics and machine learning, is proposed. The sensor framework on a large flexible sensor that transforms sparsely distributed strains into continuous surface is implemented. Finite element (FE) analysis is utilized to determine design parameters, while an FE model is built to enrich the morphological data used in the supervised training to achieve continuous surface reconstruction. A mapping between the local strain data and the enriched surface data is subsequently trained using ensemble learning. This hybrid approach enables real time, robust, and high‐order surface reconstruction. The sensing performance is evaluated in terms of accuracy, repeatability, and feasibility with numerous scenarios, which has not been demonstrated on such a large‐scale sensor before",Large‐Scale Surface Shape Sensing with Learning‐Based Computational Mechanics,,'Wiley',10.1002/aisy.202100089,"[{'title': 'Advanced Intelligent Systems', 'identifiers': ['2640-4567', 'issn:2640-4567']}]",core
478618446,2021-09-21T00:00:00,"In support of crisis management new technologies are continuously being developed. The remaining challenge for practitioner organizations is not only to identify suitable solutions to close their gaps, but also to test and evaluate its benefit before the disaster strikes. The EU-funded project DRIVER+ (Driving Innovation in Crisis Management for European Resilience) has therefore designed a methodical and technical environment to assess innovation in crisis management in a realistic but non-operational setup through trials. 

To bridge theoretical potential and practical implementation, the Center for Satellite Based Crisis Information (ZKI) at the German Aerospace Center (DLR) has been working closely with its users for years to apply new methods from remote sensing research to generate up-to-date situation information for civil security applications and disaster response products. In DRIVER+, ZKI interdisciplinary teamed up with colleagues at DLR to demonstrate near real-time contextual routing for crisis response. Experienced practitioners assessed its added value during a trial focusing on major urban flooding events. Systematically collected data through the DRIVER+ Test-bed approved that DLR’s system could improve transport planning and inter-agency situational awareness. Together with the Bavarian Red Cross, ZKI further tested the application of 3D situational awareness in an earthquake scenario during the EU civil protection exercise IRONORE2019. We will present the results of the two exercises and how user feedback is driving the further development of ZKI products. In addition, we will discuss how new developments as 3D mapping, AI and web data fusion will provide further opportunities for remote sensing in future to support emergency responders even more effectively in complex crisis scenarios",New Perspectives for Emergency Response: Lessons Learned on Crisis Mapping from Trials and Exercises,,,,,core
478906382,2021-08-11T21:00:00,"Large constellations are quickly becoming the norm in small satellite missions. These constellations are being designed and developed faster than ever before through the utilization of smaller, heterogeneous spacecraft. Often, these constellations provide increased resiliency and capabilities over their heritage, highly tailored counterparts. The ability to replace on-orbit assets quickly and with lower costs is an advantageous feature of these large smallsat constellations. With the advent of these new architectures, though, come increased complexity in mission operations. The management and monitoring of potentially hundreds of heterogeneous space assets can be extremely challenging and negate much of the cost savings using current operational approaches. Additional complexity is added with the loss expectancy of some number of assets inherent to the design within these constellations. Rather than tasking individual assets to complete missions on behalf of the system, ideal operation would be conducted through tasking of the constellation as a whole. This approach requires tasking of the individual assets by the constellation using machine-to-machine (M2M) data sharing and on-orbit autonomous decision making. Recent advances in machine learning (ML) and artificial intelligence (AI) have now set the stage for the state of the possible in this regard.
The authors of this paper are part of a research and development team aiming to develop solutions and tools to support this operational approach. The ideas presented involve a procedural and technical implementation of using forecasted operational effects developed by a combination of state machines and ML tools. First, the system’s state is gathered, time-synced, and produced into a “Dynamic Relative Telemetry Calculator.” This is presented as an NxN matrix documenting each node’s state relative to all other nodes in the system. Next, a desired operational command can be loaded into the system. Multiple possible operational scenarios and effects can be propagated. For each propagation, each asset must be capable of reporting the “cost” of performing a certain task within a certain operational scenario. By itself, this still requires a human in the loop to analyze the results and determine a command decision. However, the secondary and tertiary effects of these decisions are still unknown. To this front, the authors are developing a method of wrapping ML capability around the system\u27s state machine and propagators to create a forecaster capable of autonomously determining optimal decisions within a system. The forecaster operates in real time, improving its predictions as more data is produced by each subsystem. Generated operational forecasts, and their effects, are validated with log data from a simulation. This data is being applied to proprietary mission scenarios, but could also be applied to historical open/mission data for validation or operational lessons learned. Over time, this forecasting tool could optimize large constellation management by reserving human in the loop for only the most severe/impactful decision thresholds. This paper will present current progress of the integrated solution, next steps in the research and development roadmap, and, most importantly, the current technical hurdles still to overcome to achieve true spaceflight autonomy",Constellation Forecasting Tools for Autonomous Operations,https://core.ac.uk/download/478906382.pdf,DigitalCommons@USU,,,core
387325911,2021-02-18T00:00:00,"This paper provides a new avenue for exploiting deep neural networks to
improve physics-based simulation. Specifically, we integrate the classic
Lagrangian mechanics with a deep autoencoder to accelerate elastic simulation
of deformable solids. Due to the inertia effect, the dynamic equilibrium cannot
be established without evaluating the second-order derivatives of the deep
autoencoder network. This is beyond the capability of off-the-shelf automatic
differentiation packages and algorithms, which mainly focus on the gradient
evaluation. Solving the nonlinear force equilibrium is even more challenging if
the standard Newton's method is to be used. This is because we need to compute
a third-order derivative of the network to obtain the variational Hessian. We
attack those difficulties by exploiting complex-step finite difference, coupled
with reverse automatic differentiation. This strategy allows us to enjoy the
convenience and accuracy of complex-step finite difference and in the meantime,
to deploy complex-value perturbations as collectively as possible to save
excessive network passes. With a GPU-based implementation, we are able to wield
deep autoencoders (e.g., $10+$ layers) with a relatively high-dimension latent
space in real-time. Along this pipeline, we also design a sampling network and
a weighting network to enable \emph{weight-varying} Cubature integration in
order to incorporate nonlinearity in the model reduction. We believe this work
will inspire and benefit future research efforts in nonlinearly reduced
physical simulation problems",High-order Differentiable Autoencoder for Nonlinear Model Reduction,http://arxiv.org/abs/2102.11026,,,,core
432884549,2021-06-17T00:00:00,"In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.Comment: Published in IEEE Transactions on Robotics:
  https://ieeexplore.ieee.org/document/9453856 Video:
  https://youtu.be/KQhlZa42fe","Cat-like Jumping and Landing of Legged Robots in Low-gravity Using Deep
  Reinforcement Learning",http://arxiv.org/abs/2106.09357,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TRO.2021.3084374,,core
479727169,2021-06-01T00:00:00,"The article attempts to present a number of key technologies that determine the new quality of life of people. The following content is specified and disclosed: autonomous artificial intelligence in a smartphone, professional robot assistants, available satellite intelligence, podcasts, digital urban planning tools. In the article, the authors hypothesize that Industry X.0 is by far the highest stage of digitalization and represents a concept of innovative and digital production, the components of which are «smart assets», «smart services», «smart business», and «smart government». Structural elements of the authors’ concept of X.0 Industry are indicated, its visual cut in virtual reality conditions is provided and the functioning of this Industry exclusively within the framework of the 7th technological mode is characterized. The authors have developed and presented the protocol of formation of the X.0 Industry through the prism of innovations, technologies in both the industry sector and business management. 4 stages of implementation of this protocol are defined, namely: determination of the innovative landscape of «technological breakthrough» in a particular industry within the formation of industry X.0; assessment of threats; determining the course of further development and the action plan (four main approaches to which organizations can apply: protection, adoption of innovations, initiation of subversive innovations, retreat); implementation of structural changes at the DNA level of the organization. The authors on the basis of a number of factors bring forward the argument that today’s realities of the digital space require the development of a new logic of running a platform business in terms of its digitization. It is concluded that in practice it is necessary to form a broad coalition of educators, government officials, analysts, high-tech specialists, economists, industrialists, scientists who will join the formation of the X.0 Industry on the basis of digitalization and innovatizing. The authors concluded that Industry X.0 is a new approach to the organization of production in the context of virtual reality, which is based on highly intelligent integrated new products and digital ecosystems that form an innovative digital value chain, add new competencies and implement deep cultural changes in the direction of the formation of a new virtual reality",Formation of Industry X.0 on the Basis of Innovative-Digital Entrepreneurship and Virtual Mobility,https://core.ac.uk/download/479727169.pdf,'Research Centre of Industrial Problems of Development of NAS of Ukraine',10.32983/2222-4459-2021-6-50-58,"[{'title': 'Business Inform', 'identifiers': ['issn:2222-4459', '2222-4459', '2311-116x', 'issn:2311-116X']}]",core
405616918,2021-01-01T00:00:00,"In recent years, we have assisted to an impressive advance of augmented reality systems
and computer vision algorithms, based on image processing and artificial intelligence. Thanks to
these technologies, mainstream smartphones are able to estimate their own motion in 3D space
with high accuracy. In this paper, we exploit such technologies to support autonomous mobility
of people with visual disabilities, identifying pre-defined virtual paths and providing context
information, reducing the distance between digital and real world. In particular, we present
ARIANNA+, an extension of ARIANNA, a system explicitly designed for visually impaired
people for indoor and outdoor localization and navigation. While ARIANNA is based on the
assumption that landmarks, such as QR codes, and physical paths (composed of colored tapes,
painted lines, or tactile pavings) are deployed in the environment and recognized by the camera
of a common smartphone, ARIANNA+ eliminates the need of any physical support thanks to the
ARKit library which we exploit to build a completely virtual path. Moreover, ARIANNA+ adds
the possibility for the users to have enhanced interactions with the surrounding environment,
through Convolutional Neural Networks (CNNs) trained to recognize objects or buildings and
enabling the possibility of accessing contents associated to them. By using a common smartphone
as a mediation instrument with the environment, ARIANNA+ leverages augmented reality and
machine learning for enhancing physical accessibility. The proposed system allows visually
impaired people to easily navigate in indoor and outdoor scenarios simply by loading a previously recorded virtual path and providing automatic guidance along the route, through haptic, speech,
and sound feedback",A navigation and augmented reality system for visually impaired people,https://core.ac.uk/download/405616918.pdf,'MDPI AG',10.3390/s21093061,,core
481543806,2021-08-01T07:00:00,"This is the second volume of the Advances in Global Services and Retail Management Book Series. This volume has the following parts:   Part 1: Hospitality and Tourism Part 2: Marketing, E-marketing, and Consumer Behavior Part 3: Management Part 4: Human Resources Management Part 5: Retail Management Part 6: Economics Part 7: Accounting and Finance Part 8: Sustainability and Environmental Issues Part 9: Information Technology 
ISBN: 978-1-955833-03-5  Hospitality and Tourism  Significance of VR in the spa: A spatial analysis  Irini Lai Fun Tang, Schultz Zhi Bin Xu, and Eric Chan   Social media marketing in rural hospitality and tourism destination research  Samuel Adeyinka-Ojo and Shamsul Kamariah Abdullah   All aboard! Is space tourism still a fantasy or a reality: An investigation on Turkish market  Emrah Tasarer, Vahit Oguz Kiper, Orhan Batman, and Oguz Turkay   Strategic consciousness and business performance relationship of open innovation strategies in food and beverage businesses  Muhsin Halis, Kazim Ozan Ozer, Hasan Cinnioglu, and Zafer Camlibel   The effects of COVID-19 epidemic on guided tours and alternative tour samples from Turkey  Bayram Akay   The effect of COVID-19 phobia on holiday intention  Halil Akmese and Ali Ilgaz    The effect of the usage of virtual reality in tourism education on learning motivation  Sarp Tahsin Kumlu and Emrah Ozkul   The impact of effective implementation of customer relationship management to the success of hotels in Afikpo North local government of Ebonyi State, Nigeria  Ogboagha Callister and Managwu Lilian   The influence of study travel on quality-oriented education: The case of Handan, China  Wang Jingya and Alaa Nimer Abukhalifeh   The impact of U.S. Cuba policies on Cuban tourism industry: Focus on the Obama and Trump Administration  Jukka M. Laitamaki, Antonio Diaz Medina, and Lisandra Torres Hechavarria   Determination of students’ characteristics and perspectives about social entrepreneurship: A case of Anadolu University  Muhammed Kavak, Ipek Itir Can, and Emre Ozan Aksoz   The place of Kazakhstan tourism sector in the countries of the region in terms of transportation infrastructure  Maiya Myrzabekova, Muhsin Halis, and Zafer Camlibel   What are tour guides most praised for? A sharing economy perspective  Derya Demirdelen-Alrawadieh and Ibrahim Cifci   An examination of representations for USA in tourism brochures for Chinese market  Yasong Wang   An exploratory study on cognitive internship perception of tourism students  Ozge Buyuk and Gulsah Akkus   Are you afraid to travel during COVID-19?  Gulsum Tabak, Sibel Canik, and Ebru Guneren   Destination management during the health emergency: A bibliometric analysis  Valentina Della Corte, Giovanna Del Gaudio, Giuliana Nevola, Enrico Di Taranto, and Simone Luongo   Determination of food neophobia levels of International Mersin Citrus Festival participants  Sevda Sahilli Birdir, Nurhayat Iflazoglu, and Kemal Birdir   Analysis of effectiveness of industrial exposure training undertaken by students of hospitality management in star hotels  G. Saravana Kumar   Conceptualization of ecotourism service experiences framework from the dimensions of motivation and quality of experiences: Four realms of experience approach  Jennifer Kim Lian Chan   Does Coronavirus (COVID-19) transform travel and tourism to automation (robots)?  M. Omar Parvez, Ali Ozturen, and Cihan Cobanoglu   Efficiency of internal control systems and the effect of organizational structure and culture on internal control systems in accommodation industry  Kadriye Alev Akmese and Ali Ilgaz   Ethical perceptions of housekeeping department employees: A study in Izmir Province  Tuba Turkmendag and Bayram Sahin   Factors that prevent participation of tourists in online co-creation activities  Resat Arica, Feridun Duman, and Abdulkadir Corbaci   Health sector after COVID-19: Salt thermal facilities example  Azize Serap Tuncer and Sinan Bulut   PRISMA statement and thematic analysis framework in hospitality and tourism research  Samuel Adeyinka-Ojo   Evaluation of Turkish nights as a tourism product: The case of Cappadocia  Meral Buyukkuru, Eda Ozgul Katlav, and Firdevs Yonet Eren   Customer perceptions against COVID-19 precautionary measures of the restaurants: The case of Istanbul-Turkey  Elif Kaymaz and Sevki Ulema   Analysis of e-complaints regarding hotel restaurants during COVID-19 process: The case of Antalya  Sevim Usta and Serkan Sengul    Marketing, E-marketing, and Consumer Behavior  Materialistic social consumption amidst COVID-19 pandemic: Terror management theory in the Malaysia context  Seong-Yuen Toh and Siew-Wai Yuan   A conceptual framework for the mediating role of the flow experience between destination brand experience and destination loyalty  Ipek Kazancoglu and Taskin Dirsehan   Investigating drivers influencing choice behaviour of Islamic investment products  Hanudin Amin   Local food festivals within the scope of destination branding  Hatice Akturk and Atilla Akbaba   Marketing a destination on social media: Case of three municipalities of Izmir  Huseyin Ozan Altin and Ige Pirnar   Perceived usefulness, ease of use, online trust and online purchase intention: Mediating role of attitude towards online purchase  Muhammed Yazeed, Mohammed Aliyu Dantsoho, and Adamu Ado Abubakar    Social media framework for businesses  Nawel Amrouche   Social media marketing the African door of return experience in Badagry-Nigeria  Huseyin Arasli, Maryam Abdullahi, and Tugrul Gunay   The effect of corporate social responsibility on consumer-based brand equity: A research on automobile brands  Ali Koroglu and Ibrahim Avci   The effect of superstitions on consumer luck, horoscope and evil eye-oriented purchasing behavior: A study in Turkey  Ibrahim Avci and Salih Yildiz   The evaluation of S-D orientation on service innovation and performance of airline  Inci Polat and Ozlem Atalik   Brand new leisure constraint: COVID-19  Guliz Coskun    The impact of consumers price level perception on emotions towards supermarkets  Abdulcelil Cakici and Sena Tekeli   The impact of TikTok’s plastic surgery content on adolescents’ self-perception and purchase intention  Markus Rach   Accelerated modernity: What are the social media stories undergraduate students engage with?  Pericles Asher Rospigliosi and Sebastian Raza-Mejia   Virtual influencer as celebrity endorsers  Fanny Cheung and Wing-Fai Leung   Does millennial shopping orientation using augmented reality enabled mobile applications really impact product purchase intention?  Anil Kumar   Exposure to e-cigarette marketing and product use among highly educated adults  Onur Sahin   Extending the theory of planned behavior to explain intention to use online food delivery services in the context of COVID -19 pandemic  Ahmed Chemseddine Bouarar, Smail Mouloudj, and Kamel Mouloudj   Factors affecting investors’ buying decision in real estate market in Northern Cyprus  Gurkan Arslan and Karen Howells   From home to the store: Combined effect of music and traffic on consumers shopping behaviour  Luigi Piper, Lucrezia Maria de Cosmo, Maria Irene Prete, and Gianluigi Guido   Market expansion and business growth from the perspective of resources and capabilities: The case of a micro-enterprise  Jose G. Vargas-Hernandez and Omar C. Vargas-Gonzalez   How learning style interacts with voice-assisted technology (VAT) in consumer task evaluation  Bonnie Canziani and Sara MacSween     Effect of brand credibility and innovation on customer based brand equity and overall brand equity in Turkey: An investigation of GSM operators  Suphan Nasir and Ozge Guvendik   Value chain for a B school in India  Vimal Chandra Verma and Devashish Das Gupta    Management  AI as a boost for startups companies: Evidence from Italy  Irene Di Bernardo, Marco Tregua, Greco Fabio, and Ruggiero Andrea   The role of quality management applications for corporate reputations  Ibrahim Sapaloglu and Isik Cicek   Toxicity in organizations: A sample study on the perceived toxicity in Turkish academicians  Mustafa Hakan Atasoy and Muhsin Halis    Which resources are matter to healthcare performance? A case study on Bahrain  Mahmood Asad Ali and Mohamed Sayed Abou Elseoud   Case study: HereWay Inc. European expansion: A facility location problem  Mikhail M. Sher, Michael T. Paz, and Donald R. (Bob) Smith   In search of the effective mission statement: Structural support of the firm’s culture to augment financial performance  Seong-Yuen Toh   Innovation labs to support tourism organization in transforming crisis into opportunities: Insight from a case study  Francesco Santarsiero, Daniela Carlucci, and Giovanni Schiuma   Novelty and success of healthcare service innovation: A comparison between China and the Netherlands  Yu Mu, Rujun Wang and Ying Huang   Public private partnership in selected countries: A comparative analysis   Bekir Parlak and Abdullahi Suleiman Hashi   Strategic orientation of service enterprises towards customers  Korhan Arun and Saniye Yildirim Ozmutlu   The effects of organizational culture on information sharing attitude  Mohammadi Lanbaran Nasrin and Cicek Isik   The impact of industry 4.0 strategy on the work-life balance of employees  Ali Sukru Cetinkaya   The mediating effect of psychological empowerment on inclusive leadership and innovative work behaviour: A research in hotels  Emete Toros, Ahmet Maslakci, and Lutfi Surucu   Assessment of industry 4.0 on manufacturing enterprises: Demographic perspective  Ali Sukru Cetinkaya and M. Kemal Unsacar    Human Resources Management  Affective commitment in new hires’ onboarding? The role of organizational socialization in the fashion retail industry  Pui Sze Chan, Ho Ching Ching, Pui Yi Ng, and Annie Ko   Do burnout perception levels of nurses working in the health sector differ according to demographic characteristics?  Irfan Akkoc and Korhan Arun   Examining a moderating effect of employee turnover between recruitment and selection practice and organizational performance in Maldives civil service sector  Fathmath Muna, Azam S. M. Ferdous, and Ahmad Albattat   Personnel relationships in the workplace  Ali Sukru Cetinkaya, Shafiq Habibi, and Umut Yavuz   The evolution of human resources empowerment theory: A literature review (1970–2020)  Theodoros Stavrinoudis and Moschos Psimoulis   Teamwork, satisfaction and mediating effect of affective, continuance and normative commitments on employee’s loyalty  Thalita Aparecida Costa Nicolleti, Eduardo Roque Mangini, Leonardo Aureliano-Silva, Cristiane Sales Pires, and Carolina Aparecida de Freitas Dias   Perceptions of teachers in educational institutions regarding the principles of teaching professional ethics  Gulsah Aki, Nejat Ira, and Hasan Arslan   Influence of psychological empowerment on employee competence in Nigerian universal basic education system: The mediating role of work engagement  Isah Sani, Rashidah Binti Mohammad Ibrahim, and Fazida Karim    Retail Management  Artificial intelligence in retailing  Ibrahim Kircova, Munise Hayrun Saglam, and Sirin Gizem Kose    Customer value in retailing (2000-2020): A narrative review and future research directions  Rajat Gera and Ashish Pruthi   Effect of social media marketing on online retail performance of Konga Nigeria LTD  Abubakar Ado Adamu, Muhammed Yazeed, Mohammed Aliyu Dantsoho, Jamilu Abdulkadir, and Aliyu Audu Gemu   Employment of blue-collar workers in organized retail sector: The case of Turkey  Inci Kayhan-Kuzgun   Saving grace: Digitization to stay or address crisis?   Smitha Vasudevan   Inclusion of disabled consumers in online retail landscape: Web accessibility conformance of Turkish organized food retailers’ web sites  Asiye Ayben Celik   A customer segmentation model proposal for retailers: RFM-V  Pinar Ozkan and Ipek Deveci Kocakoc    Economics  Nigeria’s economic management: Reflections through monthly interest rate movement from 1996 to 2020 and beyond  Job Nmadu, Halima Sallawu, and Yebosoko Nmadu   A qualitative study of perceptions of the residents of Sidon, Lebanon regarding the economic effect on Sidon with reference to repatriation of the Palestinian refugees  Raja El Majzoub and Karen Howells   Three keys of development: Knowledge, efficiency and innovative entrepreneurship  Irfan Kalayci, Ali Soylu, and Baris Aytekin   Tourism and women empowerment: Empirical findings from past experience and predictions for the post-COVID era  Burcu Turkcan   COVID-19 effect on FDI motivation and their impact on service sector: Case of Georgia  Vakhtang Charaia and Mariam Lashkhi   Economic cooperation between Central Caucasus, China, and EU, under COVID-19 challenges  Vakhtang Charaia and Mariam Lashkhi   Effect of real exchange rate and income on international tourist arrivals for Turkey  Erhan Aslanoglu, Oral Erdogan, and Yasin Enes Aksu   Innovative entrepreneurship in Turkey: Micro and macro perspectives  Irfan Kalayci, Baris Aytekin, and Ali Soylu   Optimal fiscal and price stability in Germany: Autoregressive distributed lags (ARDL) cointegration relationship  Ergin Akalpler and Dahiru Alhaji Birnintsabas   Struggle with COVID-19 crisis within the scope of financial national security: The example of the Republic of Turkey  Silacan Karakus   The nexus between fiscal freedom and investment freedom: The case of E7 countries  Mehmet Bolukbas   To be or not to be a female entrepreneur in the Mexicali Valley  Roberto Burgueno Romero and Jose David Ledezma Torrez    Accounting and Finance  Comparative measurement of working capital efficiency for Borsa Istanbul restaurants and hotels for the COVID-19 period and previous quarters  Fatih Gunay and Gary Cokins   Relationship between business confidence index and non-financial firms foreign exchange assets and liabilities: Evidence from ARDL bound approach  Ilkut Elif Kandil-Goker   The impact of RTGS on internal control - A comparative study between some Iraqi banks  Salowan H. Al Taee and Noor A. Radhi   The impact of working capital on cash management under IAS 7 framework: An examination of tourism listed companies in Indonesia and Turkey  Tri Damayanti and Tuba Derya Baskan     A nexus between mergers & acquisitions and financial performance of firms: A study of industrial sector of Pakistan  Fiza Quareshi, Mukhtiar Ali, and Salar Hussain   Decentralized approach to deep-learning based asset allocation  Sarthak Sengupta, Priyanshu Priyam, and Anurika Vaish    Sustainability and Environmental Issues  Blockchain technology applied to the Consortium Etna DOC to avoid counterfeiting  Matarazzo Agata, Edoardo Carmelo Spampinato, Sergio Arfo, Ugo Sinigaglia, Antonino Bajeli, and Salvino Benanti   Eco-label certification, hotel performance and customer satisfaction: Analysis of a case study and future developments  Michele Preziosi, Alessia Acampora, Roberto Merli, and Maria Claudia Lucchetti   The integration of circular economy in the tourism industry: A framework for the implementation of circular hotels  Martina Sgambati, Alessia Acampora, Olimpia Martucci, and Maria Claudia Lucchetti   Using the theory of planned behavior to explore green food purchase intentions  Katrina Anna Auza and Kamel Mouloudj   Survey on purchasing methods of food products in Tarragona and Catania  Matarazzo Agata, Vazzano Tommaso Alberto, and Squillaci Carmelo    Information Technology  Comparative analysis of tools for matching work-related skill profiles with CV data and other unstructured data  Florian Beuttiker, Stefan Roth, Tobias Steinacher, and Thomas Hanne   State-of-the-art next generation open innovation platforms  Murielle De Roche, Monika Blaser, Patrick Hollinger, and Thomas Hanne   The coverage of AIOT based functional service: Case study of Asian futuristic hotel  Gege Wang, Irini Lai Fun Tang, Eric Chan, and Wai Hung Wilco Chan   The effect of the blockchain technology on service companies and food retailers: An overview of the blockchain use cases and applications  Gokhan Kirbac and Erkut Ergenc   The regulation problem of cryptocurrencies  Lamiha Ozturk and Ece Sulungur   Understanding information technology acceptance by physicians: Testing technology acceptance model  Anuruddha Indika Jagod",Advances in global services and retail management: Volume 2,,Digital Commons @ University of South Florida,,,core
478617377,2021-01-01T00:00:00,"Autonomy plays a key role for more scalable, precise and economic future robotic space missions. Teleoperated space robotic tasks are affected by the communication delay between the spacecraft and the ground station. In the context of robotics in-space assembly and the PULSAR project, a technical demonstrator of the autonomous assembly of a telescope's primary mirror, a learning-based method for such operation is proposed in this work. Conventional robotics assembly methods usually rely on pre-defined motions and strategies, and are engineered for a single use case. Learning-based approaches allow to use the same method for different geometries with little efforts. In this work, a reinforcement learning environment where an industrial robotic arm performs the assembly operation is modelled. Then, open source software components are used to implement the proposed design and validate it in a simulated environment with a precise physics engine. The experiments in the simulator show that the training converges and the trained reinforcement learning agents are able to successfully perform the assemblies of different peg-in-hole geometries and the parts designed for the PULSAR project. These results make reinforcement learning methods worth considering for future real-world experiments and potential in-space assembly missions",Learning Robust Strategies For In-Space Autonomous Assembly,,,,,core
491167517,2021-12-20T00:00:00,"Complete streets scheme makes seminal contributions to securing the basic public right-of-way (ROW), improving road safety, and maintaining high traffic efficiency for all modes of commute. However, such a popular street design paradigm also faces endogenous pressures like the appeal to a more balanced ROW for non-vehicular users. In addition, the deployment of Autonomous Vehicle (AV) mobility is likely to challenge the conventional use of the street space as well as this scheme. Previous studies have invented automated control techniques for specific road management issues, such as traffic light control and lane management. Whereas models and algorithms that dynamically calibrate the ROW of road space corresponding to travel demands and place-making requirements still represent a research gap. This study proposes a novel optimal control method that decides the ROW of road space assigned to driveways and sidewalks in real-time. To solve this optimal control task, a reinforcement learning method is introduced that employs a microscopic traffic simulator, namely SUMO, as its environment. The model was trained for 150 episodes using a four-legged intersection and joint AVs-pedestrian travel demands of a day. Results evidenced the effectiveness of the model in both symmetric and asymmetric road settings. After being trained by 150 episodes, our proposed model significantly increased its comprehensive reward of both pedestrians and vehicular traffic efficiency and sidewalk ratio by 10.39%. Decisions on the balanced ROW are optimised as 90.16% of the edges decrease the driveways supply and raise sidewalk shares by approximately 9%. Moreover, during 18.22% of the tested time slots, a lane-width equivalent space is shifted from driveways to sidewalks, minimising the travel costs for both an AV fleet and pedestrians. Our study primarily contributes to the modelling architecture and algorithms concerning centralised and real-time ROW management. Prospective applications out of this method are likely to facilitate AV mobility-oriented road management and pedestrian-friendly street space design in the near future",Spatial-temporal flows-adaptive street layout control using reinforcement learning,https://core.ac.uk/download/491167517.pdf,'MDPI AG',10.3390/su14010107,"[{'title': 'Sustainability', 'identifiers': ['2071-1050', 'issn:2071-1050']}]",core
483446530,2021-06-14T00:00:00,"Satellite-based applications produce ever-increasing quantities of data, challenging the capabilities of existing telemetry and on-board processing systems, especially when results must be transmitted quickly to ground.

The Scalable On-Board Computing for Space Avionics (ScOSA) platform contributes the processing capability necessary to perform such computationally intensive analysis on-board. This platform offers a high-performance on-board computer by combining multiple commercial off-the-shelf processors and space-grade processors into a distributed computer. Middleware ensures reliability by detecting and mitigating faults, while allowing applications to effectively use multiple, distributed processors.

The current work aims to demonstrate the use and advantages of utilizing the data-flow programming paradigm supported by the ScOSA platform to provide high-throughput on-board analysis. This enables rapid analysis even for applications requiring high frame rates, high resolutions, multi-spectral imaging or in-depth processing.

The On-Board Data Analysis and Real-Time Information System (ODARIS) is used to demonstrate this method. ODARIS is a system for providing low-latency access to satellite-based observations, even when large quantities of sensor data are involved. By performing on-board processing of the data from the satellite-borne instruments, the amount of data which must be sent to ground is drastically reduced. This allows the use of low-latency telecommunication-satellite constellations for communicating with ground to achieve query-response times of only a few minutes. The current application combines an Earth-observation camera with AI-based image processing to provide real-time object detection.

In the data-flow driven implementation of ODARIS on the ScOSA platform, images are captured by a camera and sent to any of several processors for the computationally intensive image processing. This allows multiple images to be processed in parallel by as many processors as are available, while avoiding the need to divide each image across several processors. The results are transferred to an on-board database from which queries can be served asynchronously.

The system will be tested in configurations with one, two and three processors and the resulting image throughput presented. Testing is performed on a ground-based prototype system using pre-recorded images.

This paper presents the necessary details of the underlying ScOSA and ODARIS systems as well as the implementation of the objection-detection algorithm using a parallelized, data-flow model. The results of executing the system using a variable number of processors are presented to demonstrate the improvement in image throughput and its potential application to other computationally-intensive tasks",Parallelizing On-Board Data Analysis Applications for a Distributed Processing Architecture,https://core.ac.uk/download/483446530.pdf,,10.5281/zenodo.5521593,,core
388908684,2021-03-11T00:00:00,"UI design is an integral part of software development. For many developers
who do not have much UI design experience, exposing them to a large database of
real-application UI designs can help them quickly build up a realistic
understanding of the design space for a software feature and get design
inspirations from existing applications. However, existing keyword-based,
image-similarity-based, and component-matching-based methods cannot reliably
find relevant high-fidelity UI designs in a large database alike to the UI
wireframe that the developers sketch, in face of the great variations in UI
designs. In this article, we propose a deep-learning-based UI design search
engine to fill in the gap. The key innovation of our search engine is to train
a wireframe image autoencoder using a large database of real-application UI
designs, without the need for labeling relevant UI designs. We implement our
approach for Android UI design search, and conduct extensive experiments with
artificially created relevant UI designs and human evaluation of UI design
search results. Our experiments confirm the superior performance of our search
engine over existing image-similarity or component-matching-based methods and
demonstrate the usefulness of our search engine in real-world UI design tasks.Comment: Accepted to TOSEM 202",Wireframe-Based UI Design Search Through Image Autoencoder,http://arxiv.org/abs/2103.07085,'Association for Computing Machinery (ACM)',10.1145/3391613,,core
390193721,2021-01-01T00:00:00,"International audienceIn the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis. This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced Blanchard et al., 2011. We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets",Domain Generalization by Marginal Transfer Learning,,Microtome Publishing,,,core
337298546,2021-03-16T00:00:00,"With the ever-increasing adoption of machine learning for data analytics,
maintaining a machine learning pipeline is becoming more complex as both the
datasets and trained models evolve with time. In a collaborative environment,
the changes and updates due to pipeline evolution often cause cumbersome
coordination and maintenance work, raising the costs and making it hard to use.
Existing solutions, unfortunately, do not address the version evolution
problem, especially in a collaborative environment where non-linear version
control semantics are necessary to isolate operations made by different user
roles. The lack of version control semantics also incurs unnecessary storage
consumption and lowers efficiency due to data duplication and repeated data
pre-processing, which are avoidable. In this paper, we identify two main
challenges that arise during the deployment of machine learning pipelines, and
address them with the design of versioning for an end-to-end analytics system
MLCask. The system supports multiple user roles with the ability to perform
Git-like branching and merging operations in the context of the machine
learning pipelines. We define and accelerate the metric-driven merge operation
by pruning the pipeline search tree using reusable history records and pipeline
compatibility information. Further, we design and implement the prioritized
pipeline search, which gives preference to the pipelines that probably yield
better performance. The effectiveness of MLCask is evaluated through an
extensive study over several real-world deployment cases. The performance
evaluation shows that the proposed merge operation is up to 7.8x faster and
saves up to 11.9x storage space than the baseline method that does not utilize
history records.Comment: 13 pages; added new baselines, i.e., MLflow and ModelDB, in Section
  VII-C; added experience on the system deployment in Section VIII; added Table
  I to clarify the correctness of the prioritized pipeline search in Section
  VII-","MLCask: Efficient Management of Component Evolution in Collaborative
  Data Analytics Pipelines",http://arxiv.org/abs/2010.10246,,,,core
444081938,2021-07-05T00:00:00,"Relational databases are the de facto standard for storing and querying
structured data, and extracting insights from structured data requires advanced
analytics. Deep neural networks (DNNs) have achieved super-human prediction
performance in particular data types, e.g., images. However, existing DNNs may
not produce meaningful results when applied to structured data. The reason is
that there are correlations and dependencies across combinations of attribute
values in a table, and these do not follow simple additive patterns that can be
easily mimicked by a DNN. The number of possible such cross features is
combinatorial, making them computationally prohibitive to model. Furthermore,
the deployment of learning models in real-world applications has also
highlighted the need for interpretability, especially for high-stakes
applications, which remains another issue of concern to DNNs.
  In this paper, we present ARM-Net, an adaptive relation modeling network
tailored for structured data, and a lightweight framework ARMOR based on
ARM-Net for relational data analytics. The key idea is to model feature
interactions with cross features selectively and dynamically, by first
transforming the input features into exponential space, and then determining
the interaction order and interaction weights adaptively for each cross
feature. We propose a novel sparse attention mechanism to dynamically generate
the interaction weights given the input tuple, so that we can explicitly model
cross features of arbitrary orders with noisy features filtered selectively.
Then during model inference, ARM-Net can specify the cross features being used
for each prediction for higher accuracy and better interpretability. Our
extensive experiments on real-world datasets demonstrate that ARM-Net
consistently outperforms existing models and provides more interpretable
predictions for data-driven decision making.Comment: 14 pages, 11 figures, 5 tables, published as a conference paper in
  ACM SIGMOD 202",ARM-Net: Adaptive Relation Modeling Network for Structured Data,http://arxiv.org/abs/2107.01830,'Association for Computing Machinery (ACM)',10.1145/3448016.3457321,,core
479312419,2021-01-01T00:00:00,"The rise of Artificial Intelligence (AI) enables enterprises to manage large amounts of data in order to derive predictions about future performance and to gain meaningful insights. In this context, descriptive and predictive analytics has gained a significant research attention; however, prescriptive analytics has just started to emerge as the next step towards increasing data analytics maturity and leading to optimized decision making ahead of time. Although machine learning for decision making has been identified as one of the most important applications of AI, up to now, prescriptive analytics is mainly addressed with domain-specific optimization models. On the other hand, existing literature lacks generalized prescriptive analytics models capable of being dynamically adapted according to the human preferences. Reinforcement Learning, as the third machine learning paradigm alongside supervised learning and unsupervised learning, has the potential to deal with the dynamic, uncertain and time-variant environments, the huge states space of sequential decision making processes, as well as the incomplete knowledge. In this paper, we propose a human-augmented prescriptive analytics approach using Interactive Multi-Objective Reinforcement Learning (IMORL) in order to cope with the complexity of real-life environments and the need for optimized human-machine collaboration. The decision making process is modelled in a generalized way in order to assure scalability and applicability in a wide range of problems and applications. We deployed the proposed approach in a stock market case study in order to evaluate the proactive trading decisions that will lead to the maximum return and the minimum risk that the user&#x2019;s experience and the available data can yield in combination",Human-Augmented Prescriptive Analytics With Interactive Multi-Objective Reinforcement Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2021.3096662,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
482101542,2021-09-01T00:00:00,"The energy efficiency and flight endurance of small unmanned aerial vehicles (SUAVs) can be improved through the implementation of autonomous soaring strategies. Biologically inspired flight techniques such as dynamic and thermal soaring offer significant energy savings through the exploitation of naturally occurring wind phenomena for thrustless flight. Recent interest in the application of artificial intelligence algorithms for autonomous soaring has been motivated by the pursuit of instilling generalized behavior in control systems, centered around the use of neural networks. However, the topology of such networks is usually predetermined, restricting the search space of potential solutions, while often resulting in complex neural networks that can pose implementation challenges for the limited hardware onboard small-scale autonomous vehicles. In exploring a novel method of generating neurocontrollers, this paper presents a neural network-based soaring strategy to extend flight times and advance the potential operational capability of SUAVs. In this study, the Neuroevolution of Augmenting Topologies (NEAT) algorithm is used to train efficient and effective neurocontrollers that can control a simulated aircraft along sustained dynamic and thermal soaring trajectories. The proposed approach evolves interpretable neural networks in a way that preserves simplicity while maximizing performance without requiring extensive training datasets. As a result, the combined trajectory planning and aircraft control strategy is suitable for real-time implementation on SUAV platforms",Neuroevolutionary Control for Autonomous Soaring,,'MDPI AG',10.3390/aerospace8090267,"[{'title': 'Aerospace', 'identifiers': ['issn:2226-4310', '2226-4310']}]",core
478156438,2021-01-01T00:00:00,"This doctoral dissertation explores intelligent systems and services for image and video analysis. In view of scientific challenges for developing innovative solutions with a broad social impact, it investigates applications in biomedicine and computer-assisted navigation of visually impaired individuals. In this context, it focuses on machine learning, particularly the investigation of methods to improve the efficiency and the effectiveness of deep artificial neural network architectures, such as the Convolutional Neural Networks (CNNs).In Convolutional Neural Networks (CNNs) the input data can contain uncertainties, such as noise, color and geometric ubiquities, that is naturally propagated from the input layer to the convolution layers of the network affecting the quality of the extracted features. To cope with this problem, a novel pooling operation based on (type-1) fuzzy sets is proposed, named Fuzzy Pooling, which can be used as a drop-in replacement of the current, crisp, pooling layers of CNN architectures. Several experiments using publicly available datasets show that the proposed approach can enhance the classification performance of a CNN.Aiming to improve the effectiveness of CNNs, especially in the context of medical image analysis, a novel architecture named Look Behind Fully Convolutional Neural Network (LB-FCN) is proposed. The architecture is capable of extracting multi-scale image features by using blocks of parallel convolutional layers with different filter sizes. These blocks are connected by look-behind connections, so that the features they produce are combined with features extracted from behind layers, thus preserving the respective information. Furthermore, it has a smaller number of free parameters than conventional CNN architectures, which makes it suitable for training with smaller datasets. This is particularly useful in medical image analysis, since data availability is usually limited, due to ethicolegal constraints. Experiments on publicly available gastrointestinal image datasets show higher classification performance compared to state-of-the-art machine and deep learning methodologies. The architecture is capable of generalizing well even when the training dataset is different than the one on which it is tested. To investigate that, a novel cross-dataset experimental study was performed on various publicly available gastrointestinal tract image datasets, containing images from different modalities, including Wireless Capsule Endoscopy (WCE) and flexible endoscopy. The number of training samples in CNN training is directly linked to their generalization performance. When the training samples are limited, such as in the case of medical images, the generalization performance is negatively affected. A typical approach to mediate this problem is to use data augmentation techniques, which image rotation and translation. While effective, this technique still requires a substantial amount of training samples to be available. To battle this problem, in the context of inflammatory conditions detection in WCE images, a novel approach is presented that uses Generative Adversarial Networks (GANs) to generate artificial images. More specifically the study trained two GANs, one to generate healthy small bowel images and another, images with inflammatory conditions. The images are then used to train a CNN architecture and validate its performance in real images. The results from this study show that the substitution of real with artificially generated endoscopic images for CNN training can be a viable option.While CNNs have a remarkable performance in computer vision problems, usually, they are computationally expensive. This limits their usage in high-end expensive devices with multiple graphical processing units (GPUs). To mediate the problem, a typical approach is to reduce the number of floating-point operations (FLOPs) required for inference, at the expense of generalization performance. In this context, a novel LB-FCN inspired CNN architecture was proposed, named LB-FCN light. The architecture features a relatively low number of free parameters and FLOPs, while managing to maintain high generalization performance. The performance of the network is validated in the problem of staircase detection in indoor and outdoor environments, with application on assisted navigation of visually impaired individuals. The results from the experimental evaluation of LB-FCN light indicate its advantageous performance over the relevant state-of-the-art architectures.The development of easy-to-use machine learning (ML) application frameworks has enabled the development of advanced artificial intelligence (AI) applications with only a few lines of self-explanatory code. However, the deployment of ML algorithms as a service for remote high throughput ML task execution, involving complex data-processing pipelines can still be challenging, especially with respect to production ML use cases. To cope with this issue, a novel system architecture is presented, which enables Algorithm-agnostic, Scalable ML (ASML) task execution for high throughput applications. It aims to provide an answer to the research question of how to design and implement an abstraction framework, suitable for the deployment of end-to-end ML pipelines in a generic and standard way. The architecture manages horizontal scaling, task scheduling, reporting, monitoring and execution of multi-client ML tasks using modular, extensible components that abstract the execution details of the underlying algorithms. Applications of ASML are investigated for the analysis of image streams in the context of medical image analysis and assisted navigation of visually impaired individuals. The results of the experiments performed demonstrate its capacity for parallel, mission critical, task execution. Assistive navigation systems require the development, assessment, and optimization of different algorithms for obstacle detection, recognition, and avoidance, as well as path planning. This is a painstaking and costly process that requires repetitive measurements under stable conditions, which is usually difficult to achieve. To this end, a novel digital twin framework for the simulation and evaluation of assistive navigation systems is presented. The framework can replicate relevant real-life situations, enabling the evaluation and optimization of algorithms through adjustable and cost-effective simulations. The utility and the effectiveness of the framework are demonstrated with an indicative simulation study in the context of a camera-based wearable system for the navigation of visually impaired individuals in an outdoor cultural space.The work presented in this dissertation includes methods with both theoretical and practical impact, that can be used as the basis for further research, and the applications presented can be used as paradigms for applications on different domains, such as telemedicine, robotics, and intelligent transportation systems.Η παρούσα διδακτορική διατριβή διερευνά πρωτότυπα έξυπνα συστήματα και υπηρεσίες ανάλυσης εικόνας και βίντεο. Λαμβάνοντας υπόψη τις επιστημονικές προκλήσεις για την ανάπτυξη καινοτόμων λύσεων με ευρύ κοινωνικό αντίκτυπο, διερευνά εφαρμογές στη βιοϊατρική και την καθοδήγηση ατόμων με προβλήματα όρασης. Σε αυτό το πλαίσιο, επικεντρώνεται στη μηχανική μάθηση, εστιάζοντας στη διερεύνηση μεθόδων για τη βελτίωση της αποδοτικότητας και αποτελεσματικότητας των αρχιτεκτονικών βαθέων τεχνητών νευρικών δικτύων, όπως τα Συνελικτικά Νευρωνικά Δίκτυα (Convolutional Neural Networks, CNN).Τα δεδομένα εισόδου των CNN μπορούν να περιέχουν αβεβαιότητες, όπως θόρυβος, χρώμα και γεωμετρική απροσδιοριστία, που μεταδίδονται από το επίπεδο εισόδου στα συνελικτικά επίπεδα του δικτύου επηρεάζοντας την ποιότητα των εξαγόμενων χαρακτηριστικών. Προκειμένου να αντιμετωπιστεί αυτό το πρόβλημα, προτείνεται μια νέα λειτουργία συγκέντρωσης (pooling) βασισμένη σε ασαφή σύνολα (τύπου-1), με όνομα Fuzzy Pooling, η οποία μπορεί να χρησιμοποιηθεί για την αντικατάσταση των υπαρχόντων επιπέδων pooling των CNN αρχιτεκτονικών. Πειράματα σε δημοσίως διαθέσιμα δεδομένα έδειξαν ότι η χρήση της  προτεινόμενη προσέγγισης μπορεί να χρησιμοποιηθεί για την βελτίωση της απόδοσης ταξινόμησης των CNN.Με στόχο τη βελτίωση της αποτελεσματικότητας των CNN, και ειδικότερα στο πλαίσιο της ανάλυσης ιατρικών εικόνων, προτάθηκε μια νέα αρχιτεκτονική CNN που ονομάζεται Look Behind Fully Convolutional Neural Network (LB-FCN). Η αρχιτεκτονική είναι ικανή να εξαγάγει χαρακτηριστικά πολλαπλών κλιμάκων χρησιμοποιώντας σύνολα (μπλοκ) παράλληλων συνελικτικών στρωμάτων με διαφορετικά μεγέθη φίλτρου. Τα σύνολα αυτά, συνδέονται με οπίσθιες συνδέσεις, με στόχο τον συνδυασμό των παραγόμενων χαρακτηριστικών με τα χαρακτηριστικά  εισόδου, διατηρώντας έτσι τις αντίστοιχες πληροφορίες. Επιπλέον, η αρχιτεκτονική έχει μικρότερο πλήθος ελεύθερων παραμέτρων σε σχέση με συμβατικές αρχιτεκτονικές CNN, γεγονός που επιτρέπει την εκπαίδευσή της με μικρό πλήθος δεδομένων εκπαίδευσης. Αυτό είναι ιδιαίτερα χρήσιμο στην ανάλυση ιατρικών εικόνας, δεδομένου ότι η διαθεσιμότητα δεδομένων εκπαίδευσης είναι συνήθως περιορισμένη, λόγω βιοηθικών και νομικών περιορισμών. Πειράματα σε δημοσίως διαθέσιμα δεδομένα εικόνων του γαστρεντερικού συστήματος, παρουσιάζουν υψηλή απόδοση ταξινόμησης σε σύγκριση με άλλες σύγχρονες προσεγγίσεις. Η αρχιτεκτονική είναι ικανή να γενικεύει καλά ακόμη και όταν το δεδομένα εκπαίδευσης προέρχονται από διαφορετικά σύνολα δεδομένων από αυτά στα οποίο δοκιμάζεται. Σε αυτό το πλαίσιο, πραγματοποιήθηκε πειραματική μελέτη σε πληθώρα δημοσίων διαθέσιμων συνόλων δεδομένων γαστρεντερικού συστήματος, απαρτιζόμενα από εικόνες που έχουν ληφθεί κάνοντας χρήση διαφορετικών ιατρικών οργάνων, όπως ενδοσκοπικής κάψουλας και εύκαμπτου ενδοσκοπίου. Η δυνατότητα γενίκευσης των CNN συνδέεται άμεσα με το διαθέσιμο πλήθος δειγμάτων εκπαίδευσης. Όταν τα δείγματα εκπαίδευσης είναι περιορισμένα, όπως στην περίπτωση ιατρικών εικόνων, η δυνατότητα γενίκευσης επηρεάζεται αρνητικά. Μια τυπική προσέγγιση για τον περιορισμό αυτού του προβλήματος είναι η χρήση τεχνικών επαύξησης δεδομένων, τροποποιώντας τα υπάρχοντα δεδομένα. Αν και η τεχνική αυτή είναι αποτελεσματική και πάλι απαιτείται σημαντικό πλήθος δεδομένων εκπαίδευσης. Για την καταπολέμηση αυτού του προβλήματος, στο πλαίσιο της ανίχνευσης φλεγμονών σε εικόνες που προέρχονται από ενδοσκοπική κάψουλα, παρουσιάζεται μια προσέγγιση που χρησιμοποιεί Παραγωγικά Αντιπαραθετικά Δίκτυα (Generative Adversarial Networks, GAN) για τη δημιουργία συνθετικών εικόνων. Πιο συγκεκριμένα, η μελέτη βασίζεται στην εκπαίδευση δύο GAN, ένα για να την παραγωγή υγιών εικόνων του λεπτού εντέρου και ένα άλλο, για την παραγωγή εικόνων με φλεγμονές. Οι παραγόμενες εικόνες στη συνέχεια χρησιμοποιούνται για την εκπαίδευση ενός CNN με στόχο την αξιολόγηση της αποδοτικότητάς του σε πραγματικές εικόνες. Τα αποτελέσματα αυτής της μελέτης δείχνουν ότι η αντικατάσταση πραγματικών με τεχνητά παραγόμενων ενδοσκοπικών εικόνων για εκπαίδευση στο CNN μπορεί να είναι μια βιώσιμη επιλογή.Η αξιοσημείωτη απόδοση των CNN στον τομέα της υπολογιστικής όρασης, συνήθως, συνοδεύεται από αυξημένο υπολογιστικό κόστος. Αυτό περιορίζει τη χρήση τους σε συσκευές υψηλών υπολογιστικών προδιαγραφών εξοπλισμένες με πολλαπλές κάρτες γραφικών. Για την αντιμετώπιση αυτού του προβλήματος, μια τυπική προσέγγιση είναι η μείωση των απαιτούμενων αριθμητικών πράξεων, σε βάρος της απόδοσης γενίκευσης. Σε αυτό το πλαίσιο, προτάθηκε μια νέα αρχιτεκτονική CNN, εμπνευσμένη από την LB-FCN, με όνομα LB-FCN light. Η αρχιτεκτονική διαθέτει χαμηλό αριθμό ελεύθερων παραμέτρων και πράξεων, ενώ παράλληλα διατηρεί υψηλή απόδοση γενίκευσης. Η απόδοση του δικτύου διερευνήθηκε στο πρόβλημα της ανίχνευσης σκαλών σε εσωτερικούς και εξωτερικούς χώρους, με εφαρμογές στην υποβοηθούμενη πλοήγηση ατόμων με προβλήματα όρασης. Τα αποτελέσματα από την πειραματική αξιολόγηση του LB-FCN light δείχνουν πως απόδοσή του είναι υψηλότερη σε σύγκριση με άλλες, σύγχρονες αρχιτεκτονικές CNNs. Η ανάπτυξη εύχρηστων πλαισίων εφαρμογών μηχανικής μάθησης, δίνει την δυνατότητα ανάπτυξης προηγμένων εφαρμογών τεχνητής νοημοσύνης με μόνο λίγες γραμμές κώδικα. Ωστόσο, η εγκατάσταση αλγορίθμων μηχανικής μάθησης σε απομακρυσμένο περιβάλλον υψηλής απόδοσης, που περιλαμβάνει περίπλοκα επίπεδα επεξεργασίας δεδομένων, εξακολουθεί να είναι δύσκολη, ειδικά όταν τα περιβάλλοντα αυτά προορίζονται για χρήση από επιχειρήσεις. Για την αντιμετώπιση αυτού του προβλήματος, παρουσιάζεται μια νέα αρχιτεκτονική συστήματος, η οποία επιτρέπει την εκτέλεση εργασιών μηχανικής μάθησης για εφαρμογές υψηλής απόδοσης, με όνομα Algorithm-agnostic, Scalable Machine Learning (ASML). Στόχος της αρχιτεκτονικής είναι να δώσει μια απάντηση στο ερευνητικό πρόβλημα της σχεδίας και ανάπτυξης πλαισίου εφαρμογής, κατάλληλο για την ανάπτυξη διεργασιών μηχανικής μάθησης με γενικό και τυποποιημένο τρόπο, ανεξάρτητο του αλγορίθμου μηχανικής μάθησης. Η αρχιτεκτονική διαχειρίζεται την οριζόντια κλιμάκωση, τον προγραμματισμό εργασιών, την αναφορά, την παρακολούθηση και την εκτέλεση εργασιών μηχανικής μάθησης, με δυνατότητα χρήσης από πολλαπλούς χρήστες, χρησιμοποιώντας ανεξάρτητα και επεκτάσιμα στοιχεία που αποκρύπτουν τις λεπτομέρειες εκτέλεσης των υποκείμενων αλγορίθμων. Η δυνατότητες της αρχιτεκτονικής διερευνήθηκαν σε εφαρμογές ανάλυσης ροών εικόνων από ιατρικά δεδομένα και στα πλαίσια της υποβοηθούμενης πλοήγηση ατόμων με προβλήματα όρασης. Τα αποτελέσματα των πειραμάτων που πραγματοποιήθηκαν δείχνουν ότι η αρχιτεκτονική είναι κατάλληλη για παράλληλη χρήση και σε κρίσιμα συστήματα.Τα συστήματα υποβοηθούμενης πλοήγησης απαιτούν την ανάπτυξη, αξιολόγηση και βελτιστοποίηση διαφορετικών αλγορίθμων για την ανίχνευση εμποδίων, την αναγνώριση και την αποφυγή τους, καθώς και τον σχεδιασμό διαδρομών. Η διαδικασία αυτή είναι ιδιαιτέρως επίπονη και δαπανηρή και απαιτεί επαναλαμβανόμενες μετρήσεις υπό σταθερές συνθήκες, κάτι που συνήθως είναι δύσκολο να επιτευχθεί. Για το σκοπό αυτό, παρουσιάζεται ένα πρωτότυπο πλαίσιο εφαρμογής για την προσομοίωση και την αξιολόγηση συστημάτων υποβοήθησης πλοήγησης. Το πλαίσιο αυτό μπορεί να αναπαράγει πραγματικές καταστάσεις, επιτρέποντας την αξιολόγηση και βελτιστοποίηση αλγορίθμων μέσω ρυθμιζόμενων και οικονομικά αποδοτικών προσομοιώσεων. Η χρησιμότητα και η αποτελεσματικότητα του πλαισίου αποδεικνύονται με μια ενδεικτική μελέτη προσομοίωσης στο πλαίσιο ενός φορητού συστήματος που βασίζεται σε κάμερα για την πλοήγηση ατόμων με προβλήματα όρασης σε έναν υπαίθριο χώρο πολιτιστικού ενδιαφέροντος.Το έργο που παρουσιάστηκε στην παρούσα διατριβή περιλαμβάνει μεθόδους με θεωρητικό και πρακτικό αντίκτυπο, οι οποίες μπορούν να χρησιμοποιηθούν ως βάση για περαιτέρω έρευνα. Οι εφαρμογές που παρουσιάζονται μπορούν να χρησιμοποιηθούν ως πρότυπα για εφαρμογές σε διαφορετικούς τομείς, όπως τηλεϊατρική, ρομποτική και έξυπνα συστήματα μετακίνησης",Ευφυή συστήματα και υπηρεσίες για την ανάλυση εικόνων και βίντεο,,'National Documentation Centre (EKT)',10.12681/eadd/49853,,core
401600257,2021-11-17T00:00:00,"StarCraft II (SC2) is a real-time strategy game in which players produce and
control multiple units to fight against opponent's units. Due to its
difficulties, such as huge state space, various action space, a long time
horizon, and imperfect information, SC2 has been a research hotspot in
reinforcement learning. Recently, an agent called AlphaStar (AS) has been
proposed, which shows good performance, obtaining a high win rate of 99.8%
against human players. We implemented a mini-scaled version of it called
mini-AlphaStar (mAS) based on AS's paper and pseudocode. The difference between
AS and mAS is that we substituted the hyper-parameters of AS with smaller ones
for mini-scale training. Codes of mAS are all open-sourced
(https://github.com/liuruoze/mini-AlphaStar) for future research.Comment: 11 pages, 2 figure",An Introduction of mini-AlphaStar,http://arxiv.org/abs/2104.06890,,,,core
491073013,2021-12-14T00:00:00,"Money laundering is a global problem that concerns legitimizing proceeds from
serious felonies (1.7-4 trillion euros annually) such as drug dealing, human
trafficking, or corruption. The anti-money laundering systems deployed by
financial institutions typically comprise rules aligned with regulatory
frameworks. Human investigators review the alerts and report suspicious cases.
Such systems suffer from high false-positive rates, undermining their
effectiveness and resulting in high operational costs. We propose a machine
learning triage model, which complements the rule-based system and learns to
predict the risk of an alert accurately. Our model uses both entity-centric
engineered features and attributes characterizing inter-entity relations in the
form of graph-based features. We leverage time windows to construct the dynamic
graph, optimizing for time and space efficiency. We validate our model on a
real-world banking dataset and show how the triage model can reduce the number
of false positives by 80% while detecting over 90% of true positives. In this
way, our model can significantly improve anti-money laundering operations.Comment: 8 pages, 5 figure","Anti-Money Laundering Alert Optimization Using Machine Learning with
  Graphs",http://arxiv.org/abs/2112.07508,,,,core
401623799,2021-03-29T00:00:00,"Purpose: This paper discusses the impact of COVID-19 on the Visual Arts industry in Malaysia. In general, this pandemic has affected various forms of artistic activities and the income of visual arts artists and galleries. The cancellation of art projects and exhibitions has greatly affected the artist's source of income as well as disrupted the sale of works and forms of art appreciation. The crisis has also opened up a new form to the visual arts industry by looking at alternative approaches to the continuity of the arts field by switching to virtual or online methods. This emerging crisis of COVID-19 might be the starting point for all art practitioners including artists, art critics, galleries/museums, collectors, and curators in using the online space to continue to capitalize on and expand the Visual Arts industry.
Design/methodology/approach: Review approach.
Findings: The COVID-19 pandemic has made a huge impact on the country's Visual Arts industry where a wide range of art activities cannot be implemented and opened up opportunities for online activities
Practical implications: Exhibition and sale of works through online approach has become one of the main methods that support the Visual Arts industry with the application of a combination of the latest technologies such as VR and AI that enable the representation of real experiences in the context of art appreciation.
Originality/value: This paper is original.
Paper type: This paper can be categorized as a viewpoin",Covid -19: The Impact On Malaysian Visual Arts Scene,https://core.ac.uk/download/401623799.pdf,'Narotama University',10.29138/ijebd.v4i2.1117,,core
401696252,2021-01-01T00:00:00,"With the rapid development of deep learning techniques, the popularity of voice services implemented on various Internet of Things (IoT) devices is ever increasing. In this paper, we examine user-level membership inference in the problem space of voice services, by designing an audio auditor to verify whether a specific user had unwillingly contributed audio used to train an automatic speech recognition (ASR) model under strict black-box access. With user representation of the input audio data and their corresponding translated text, our trained auditor is effective in user-level audit. We also observe that the auditor trained on specific data can be generalized well regardless of the ASR model architecture. We validate the auditor on ASR models trained with LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid ASR system and the end-to-end ASR system. Finally, we conduct a real-world trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding 80%. We hope the methodology developed in this paper and findings can inform privacy advocates to overhaul IoT privacy",The audio auditor: user-level membership inference in Internet of Things voice services,http://hdl.handle.net/10536/DRO/DU:30147977,'Walter de Gruyter GmbH',10.2478/popets-2021-0012,,core
475069406,2021-06-01T00:00:00,"Mounting interest in ambitious clean energy goals is exposing critical gaps in our understanding of onshore wind power potential. Conventional approaches to evaluating wind power technical potential at the national scale rely on coarse geographic representations of land area requirements for wind power. These methods overlook sizable spatial variation in real-world capacity densities (i.e., nameplate power capacity per unit area) and assume that potential installation densities are uniform across space. Here, we propose a data-driven approach to overcome persistent challenges in characterizing localized deployment potentials over broad extents. We use machine learning to develop predictive relationships between observed capacity densities and geospatial variables. The model is validated against a comprehensive data set of United States (U.S.) wind facilities and subjected to interrogation techniques to reveal that key explanatory features behind geographic variation of capacity density are related to wind resource as well as urban accessibility and forest cover. We demonstrate application of the model by producing a high-resolution (2 km × 2 km) national map of capacity density for use in technical potential assessments for the United States. Our findings illustrate that this methodology offers meaningful improvements in the characterization of spatial aspects of technical potential, which are increasingly critical to draw reliable and actionable planning and research insights from renewable energy scenarios",Spatially-Explicit Prediction of Capacity Density Advances Geographic Characterization of Wind Power Technical Potential,,'MDPI AG',10.3390/en14123609,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core
429357144,2021-05-01T00:00:00,"Abstract We explore the potential of feed‐forward deep neural networks (DNNs) for emulating cloud superparameterization in realistic geography, using offline fits to data from the superparameterized community atmospheric model. To identify the network architecture of greatest skill, we formally optimize hyperparameters using ∼250 trials. Our DNN explains over 70% of the temporal variance at the 15‐min sampling scale throughout the mid‐to‐upper troposphere. Autocorrelation timescale analysis compared against DNN skill suggests the less good fit in the tropical, marine boundary layer is driven by neural network difficulty emulating fast, stochastic signals in convection. However, spectral analysis in the temporal domain indicates skillful emulation of signals on diurnal to synoptic scales. A closer look at the diurnal cycle reveals correct emulation of land‐sea contrasts and vertical structure in the heating and moistening fields, but some distortion of precipitation. Sensitivity tests targeting precipitation skill reveal complementary effects of adding positive constraints versus hyperparameter tuning, motivating the use of both in the future. A first attempt to force an offline land model with DNN emulated atmospheric fields produces reassuring results further supporting neural network emulation viability in real‐geography settings. Overall, the fit skill is competitive with recent attempts by sophisticated Residual and Convolutional Neural Network architectures trained on added information, including memory of past states. Our results confirm the parameterizability of superparameterized convection with continents through machine learning and we highlight the advantages of casting this problem locally in space and time for accurate emulation and hopefully quick implementation of hybrid climate models",Assessing the Potential of Deep Learning for Emulating Cloud Superparameterization in Climate Models With Real‐Geography Boundary Conditions,,'American Geophysical Union (AGU)',10.1029/2020MS002385,"[{'title': 'Journal of Advances in Modeling Earth Systems', 'identifiers': ['issn:1942-2466', '1942-2466']}]",core
479626782,2021-01-01T00:00:00,"A computationally efficient method for online joint state inference and dynamical model learning is presented. The dynamical model combines an a priori known, physically derived, state-space model with a radial basis function expansion representing unknown system dynamics and inherits properties from both physical and data-driven modeling. The method uses an extended Kalman filter approach to jointly estimate the state of the system and learn the unknown system dynamics, via the parameters of the basis function expansion. The key contribution is a computational complexity reduction compared to a similar approach with globally supported basis functions. By using compactly supported radial basis functions and an approximate Kalman gain, the computational complexity is considerably reduced and is essentially determined by the support of the basis functions. The approximation works well when the system dynamics exhibit limited correlation between points well separated in the state-space domain. The method is exemplified via two intelligent vehicle applications where it is shown to: (i) have competitive system dynamics estimation performance compared to the globally supported basis function method, and (ii) be real-time applicable to problems with a large-scale state-space.Funding: Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation</p",Online Joint State Inference and Learning of Partially Unknown State-Space Models,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TSP.2021.3095709,,core
487582066,2021-12-14T00:00:00,"International audienceNetwork management often relies on machine learning to make predictions about performance and security from network traffic. Often, the representation of the traffic is as important as the choice of the model. The features that the model relies on, and the representation of those features, ultimately determine model accuracy, as well as where and whether the model can be deployed in practice. Thus, the design and evaluation of these models ultimately requires understanding not only model accuracy but also the systems costs associated with deploying the model in an operational network. Towards this goal, this paper develops a new framework and system that enables a joint evaluation of both the conventional notions of machine learning performance (e.g., model accuracy) and the systems-level costs of different representations of network traffic. We highlight these two dimensions for two practical network management tasks, video streaming quality inference and malware detection, to demonstrate the importance of exploring different representations to find the appropriate operating point. We demonstrate the benefit of exploring a range of representations of network traffic and present Traffic Refinery, a proof-of-concept implementation that both monitors network traffic at 10~Gbps and transforms traffic in real time to produce a variety of feature representations for machine learning. Traffic Refinery both highlights this design space and makes it possible to explore different representations for learning, balancing systems costs related to feature extraction and model training against model accuracy",Traffic Refinery: Cost-Aware Data Representation for Machine Learning on Network Traffic,,'Association for Computing Machinery (ACM)',10.1145/3491052,,core
478906343,2021-08-11T18:00:00,"The EO-ALERT European Commission H2020 project proposes the definition, development, and verification and validation through ground hardware testing, of a next-generation Earth Observation (EO) data processing chain. The proposed data processing chain is based on a novel flight segment architecture that moves EO data processing elements traditionally executed in the ground segment to on-board the satellite, with the aim of delivering EO products to the end user with very low latency. EO-ALERT achieves, globally, latencies below five minutes for EO products delivery, and below one minute in realistic scenarios.
The proposed EO-ALERT architecture is enabled by on-board processing, recent improvements in processing hardware using Commercial Off-The-Shelf (COTS) components, and persistent space-to-ground communications links. EO-ALERT combines innovations in the on-board elements of the data chain and the communications, namely: on-board reconfigurable data handling, on-board image generation and processing for the generation of alerts (EO products) using Machine Learning (ML) and Artificial Intelligence (AI), on-board AI-based data compression and encryption, high-speed on-board avionics, and reconfigurable high data rate communication links to ground, including a separate chain for alerts with minimum latency and global coverage.
This paper presents the proposed architecture, its hardware realization for the ground testing in a representative environment and its performance. The architecture’s performance is evaluated considering two different user scenarios where very low latency (almost-real-time) EO product delivery is required: ship detection and extreme weather monitoring/nowcasting. The hardware testing results show that, when implemented using COTS components and available communication links, the proposed architecture can deliver alerts to the end user with a latency below five minutes, for both SAR and Optical missions, demonstrating the viability of the EO-ALERT architecture. In particular, in several test scenarios, for both the TerraSAR-X SAR and DEIMOS-2 Optical Very High Resolution (VHR) missions, hardware testing of the proposed architecture has shown it can deliver EO products and alerts to the end user globally, with latency lower than one-point-five minutes",A Novel Satellite Architecture for the Next Generation of Earth Observation Satellites Supporting Rapid Alerts,https://core.ac.uk/download/478906343.pdf,DigitalCommons@USU,,,core
387313093,2021-11-14T00:00:00,"Blind or no-reference video quality assessment of user-generated content
(UGC) has become a trending, challenging, heretofore unsolved problem. Accurate
and efficient video quality predictors suitable for this content are thus in
great demand to achieve more intelligent analysis and processing of UGC videos.
Previous studies have shown that natural scene statistics and deep learning
features are both sufficient to capture spatial distortions, which contribute
to a significant aspect of UGC video quality issues. However, these models are
either incapable or inefficient for predicting the quality of complex and
diverse UGC videos in practical applications. Here we introduce an effective
and efficient video quality model for UGC content, which we dub the Rapid and
Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably
to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.
RAPIQUE combines and leverages the advantages of both quality-aware scene
statistics features and semantics-aware deep convolutional features, allowing
us to design the first general and efficient spatial and temporal (space-time)
bandpass statistics model for video quality modeling. Our experimental results
on recent large-scale UGC video quality databases show that RAPIQUE delivers
top performances on all the datasets at a considerably lower computational
expense. We hope this work promotes and inspires further efforts towards
practical modeling of video quality problems for potential real-time and
low-latency applications. To promote public usage, an implementation of RAPIQUE
has been made freely available online: \url{https://github.com/vztu/RAPIQUE}.Comment: IEEE Open Journal of Signal Processing 202","RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated
  Content",http://arxiv.org/abs/2101.10955,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/OJSP.2021.3090333,,core
396684458,2021-08-01T00:00:00,"This paper provides a new avenue for exploiting deep neural networks to improve physics-based simulation. Specifically, we integrate the classic Lagrangian mechanics with a deep autoencoder to accelerate elastic simulation of deformable solids. Due to the inertia effect, the dynamic equilibrium cannot be established without evaluating the second-order derivatives of the deep autoencoder network. This is beyond the capability of off-the-shelf automatic differentiation packages and algorithms, which mainly focus on the gradient evaluation. Solving the nonlinear force equilibrium is even more challenging if the standard Newton's method is to be used. This is because we need to compute a third-order derivative of the network to obtain the variational Hessian. We attack those difficulties by exploiting complex-step finite difference, coupled with reverse automatic differentiation. This strategy allows us to enjoy the convenience and accuracy of complex-step finite difference and in the meantime, to deploy complex-value perturbations as collectively as possible to save excessive network passes. With a GPU-based implementation, we are able to wield deep autoencoders (e.g., 10+ layers) with a relatively high-dimension latent space in real-time. Along this pipeline, we also design a sampling network and a weighting network to enable weight-varying Cubature integration in order to incorporate nonlinearity in the model reduction. We believe this work will inspire and benefit future research efforts in nonlinearly reduced physical simulation problems",High-order Differentiable Autoencoder for Nonlinear Model Reduction,https://core.ac.uk/download/396684458.pdf,'Association for Computing Machinery (ACM)',10.1145/3450626.3459754,,core
491030856,2021-12-10T00:00:00,"Graph representation learning received increasing attentions in recent years.
Most of existing methods ignore the complexity of the graph structures and
restrict graphs in a single constant-curvature representation space, which is
only suitable to particular kinds of graph structure indeed. Additionally,
these methods follow the supervised or semi-supervised learning paradigm, and
thereby notably limit their deployment on the unlabeled graphs in real
applications. To address these aforementioned limitations, we take the first
attempt to study the self-supervised graph representation learning in the
mixed-curvature spaces. In this paper, we present a novel Self-supervised
Mixed-curvature Graph Neural Network (SelfMGNN). Instead of working on one
single constant-curvature space, we construct a mixed-curvature space via the
Cartesian product of multiple Riemannian component spaces and design
hierarchical attention mechanisms for learning and fusing the representations
across these component spaces. To enable the self-supervisd learning, we
propose a novel dual contrastive approach. The mixed-curvature Riemannian space
actually provides multiple Riemannian views for the contrastive learning. We
introduce a Riemannian projector to reveal these views, and utilize a
well-designed Riemannian discriminator for the single-view and cross-view
contrastive learning within and across the Riemannian views. Finally, extensive
experiments show that SelfMGNN captures the complicated graph structures in
reality and outperforms state-of-the-art baselines.Comment: Accepted by AAAI 2022, 11 page",A Self-supervised Mixed-curvature Graph Neural Network,http://arxiv.org/abs/2112.05393,,,,core
481990634,2021-01-01T00:00:00,"Hard time constraints in space missions bring in the problem of fast video processing for numerous autonomous tasks. Video processing involves the separation of distinct image frames, fetching image descriptors, applying different machine learning algorithms for object detection, obstacle avoidance, and many more tasks involved in the automatic maneuvering of a spacecraft. These tasks require the most informative descriptions of an image within the time constraints. Tracking these informative points from consecutive image frames is needed in flow estimation applications. Classical algorithms like SIFT and SURF are the milestones in the feature description development. But computational complexity and high time requirements force the critical missions to avoid these techniques to get adopted in real-time processing. Hence a time conservative and less complex pre-trained Convolutional Neural Network (CNN) model is chosen in this paper as a feature descriptor. 7-layer CNN model is designed and implemented with pre-trained VGG model parameters and then these CNN features are used to match the points of interests from consecutive image frames of a lunar descent video. The performance of the system is evaluated based on visual and empirical keypoints matching. The scores of matches between two consecutive images from the video using CNN features are then compared with state-of-the-art algorithms like SIFT and SURF. The results show that CNN features are more reliable and robust in case of time-critical video processing tasks for keypoint tracking applications of space missions",Tracking Keypoints from Consecutive Video Frames Using CNN Features for Space Applications,https://core.ac.uk/download/481990634.pdf,'University North',10.31803/tg-20210204161210,,core
478578273,2021-08-01T07:00:00,"Continuous security assessment of a power system is necessary to insure a reliable, stable, and continuous supply of electrical power to customers. To this end, this dissertation identifies and explores some of the various challenges encountered in the field of power system security assessment. Accordingly, several model-based and/or model-free approaches were developed to overcome these challenges.
First, a voltage stability index, named TAVSI, is proposed. This index has three important features: TAVSI applies to general load models including ZIP, exponential, and induction motor loads; TAVSI can be used for both measurement-based and model-based voltage stability assessment; and finally, TAVSI is calculated based on normalized sensitivities which enables identification of weak buses and the definition of a global instability threshold. TAVSI was tested on both the IEEE 14-bus and the 181-bus WECC systems. Results show that TAVSI gives a reliable assessment of system stability.
Second, a data-driven and model-based hybrid reinforcement learning approach is proposed for training a control agent to re-dispatch generators’ output power in order to relieve stressed branches. For large power systems, the agent’s action space is highly dimensioned which challenges the successful training of data-driven agents. Therefore, we propose a hybrid approach where model-based actions are utilized to help the agent learn an optimal control policy. The proposed approach was tested and compared to the generic data-driven DDPG-based approach on the IEEE 118-bus system and a larger 2749-bus real-world system. Results show that the hybrid approach performs well for large power systems and that it is superior to the DDPG-based approach.
Finally, a Convolutional Neural Network (CNN) based approach is proposed as a faster alternative to the classical AC power flow-based contingency screening. The proposed approach is investigated on both the IEEE 118-bus system and the Texas 2000-bus synthetic system. For such large systems, the implementation of the proposed approach came with several challenges, such as computational burden, learning from imbalanced datasets, and performance evaluation of trained models. Accordingly, this work contributes a set of novel techniques and best practices that enables both efficient and successful implementation of CNN-based multi-contingency classifiers for large power systems",Model-based and Model-free Approaches for Power System Security Assessment,https://core.ac.uk/download/478578273.pdf,TRACE: Tennessee Research and Creative Exchange,,,core
480455399,2021-06-01T00:00:00,"The widely-adopted practice is to train deep learning models with specialized hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on linear algebra operations. However, this strategy does not employ effectively the extensive CPU and memory resources - which are used only for preprocessing, data transfer, and scheduling - available by default on the accelerated servers. In this paper, we study training algorithms for deep learning on heterogeneous CPU+GPU architectures. Our two-fold objective - maximize convergence rate and resource utilization simultaneously - makes the problem challenging. In order to allow for a principled exploration of the design space, we first introduce a generic deep learning framework that exploits the difference in computational power and memory hierarchy between CPU and GPU through asynchronous message passing. Based on insights gained through experimentation with the framework, we design two heterogeneous asynchronous stochastic gradient descent (SGD) algorithms. The first algorithm - CPU+GPU Hogbatch - combines small batches on CPU with large batches on GPU in order to maximize the utilization of both resources. However, this generates an unbalanced model update distribution which hinders the statistical convergence. The second algorithm - Adaptive Hogbatch - assigns batches with continuously evolving size based on the relative speed of CPU and GPU. This balances the model updates ratio at the expense of a customizable decrease in utilization. We show that the implementation of these algorithms in the proposed CPU+GPU framework achieves both faster convergence and higher resource utilization than TensorFlow on several real datasets",Adaptive Stochastic Gradient Descent for Deep Learning on Heterogeneous CPU+GPU Architectures,,"eScholarship, University of California",,,core
351177996,2021-01-01T00:00:00,"With the rapid development of deep learning techniques, the popularity of voice services implemented on various Internet of Things (IoT) devices is ever increasing. In this paper, we examine user-level membership inference in the problem space of voice services, by designing an audio auditor to verify whether a specific user had unwillingly contributed audio used to train an automatic speech recognition (ASR) model under strict black-box access. With user representation of the input audio data and their corresponding translated text, our trained auditor is effective in user-level audit. We also observe that the auditor trained on specific data can be generalized well regardless of the ASR model architecture. We validate the auditor on ASR models trained with LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid ASR system and the end-to-end ASR system. Finally, we conduct a real-world trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding 80%. We hope the methodology developed in this paper and findings can inform privacy advocates to overhaul IoT privacy",The audio auditor: user-level membership inference in Internet of Things voice services,https://core.ac.uk/download/351177996.pdf,'Walter de Gruyter GmbH',10.2478/popets-2021-0012,,core
426887297,2021-01-01T00:00:00,"In the field of Artificial Intelligence, Bayesian Networks (BN) are a well-known framework for reasoning under uncertain knowledge. BN have been applied in a wide range of real-world domains, such as medical diagnosis, forensic analysis, dependability assessment, risk management, etc. With respect to other types of models, BN provide relevant advantages: at the modelling level, the compact representation of the joint distribution of the system variables leads to the factorization of the set of possible states, avoiding the generation of the complete state space of the system; at the analysis level, inference algorithms can compute the probability distribution of any variable, possibly conditioned on the observation of the value (state) of other variables, so that predictive and diagnostic measures can be easily evaluated. During the years, BN have been extended in order to increase their modelling and analysis power; for instance, Dynamic Bayesian Networks and Continuous-Time Bayesian Networks take time into account, Hybrid Bayesian Networks deal with both discrete and continuous variables, Decision Networks contain decision nodes and value nodes. The aim of this Special Issue is to collect recent developments about inference algorithms, their applications to real-case studies, and their implementation in software tools","Editorial for the Special Issue on \u201cBayesian Networks: Inference Algorithms, Applications, and Software Tools\u201d",,'MDPI AG',10.3390/a14050138,,core
467103270,2021-07-07T07:00:00,"The advanced development in autonomous agents like self-driving cars can be attributed to computer vision, a branch of artificial intelligence that enables software to understand the content of image and video. These autonomous agents require a three-dimensional modelling of its surrounding in order to operate reliably in the real-world. Despite the significant progress of 2D object detectors, they have a critical limitation in location sensitive applications as they do not provide accurate physical information of objects in 3D space. 3D object detection is a promising topic that can provide relevant solutions which could improve existing 2D based applications. Due to the advancements in deep learning methods and relevant datasets, the task of 3D scene understanding has evolved greatly in the past few years. 3D object detection and localization are crucial in autonomous driving tasks such as obstacle avoidance, path planning and motion control. Traditionally, there have been successful methods towards 3D object detection but they rely on highly expensive 3D LiDAR sensors for accurate depth information. On the other hand, 3D object detection from single monocular images is inexpensive but lacks in accuracy. The primary reason for such a disparity in performance is that the monocular image-based methods attempt at inferring 3D information from 2D images. In this work, we try to bridge the performance gap observed in single image input by introducing different mapping strategies between the 2D image data and its corresponding 3D representation and use it to perform object detection in 3D. The performance of the proposed method is evaluated on the popular KITTI 3D object detection benchmark dataset",Monocular 3D Object Detection via Ego View-to-Bird’s Eye View Translation,https://core.ac.uk/download/467103270.pdf,RIT Scholar Works,,,core
405654948,2021-04-07T00:00:00,"Speech is a behavioural biometric signal that can provide important information to understand the human intends as well as their emotional status. The paper is centered on the speech-based identification of the seniors’s emotional status during their interaction with a virtual agent playing the role of a health professional coach. Under real conditions, we can just identify a small set of task-dependent spontaneous emotions. The number of identified samples is largely different for each emotion, which results in an imbalanced dataset problem. This research proposes the dimensional model of emotions as a perceptual representation space alternative to the generally used acoustic one. The main contribution of the paper is the definition of a perceptual borderline for the oversampling of minority emotion classes in this space. This limit, based on arousal and valence criteria, leads to two methods of balancing the data: the Perceptual Borderline oversampling and the Perceptual Borderline SMOTE (Synthetic Minority Oversampling Technique). Both methods are implemented and compared to state-of-the-art approaches of Random oversampling and SMOTE. The experimental evaluation was carried out on three imbalanced datasets of spontaneous emotions acquired in human-machine scenarios in three different cultures: Spain, France and Norway. The emotion recognition results obtained by neural networks classifiers show that the proposed perceptual oversampling methods led to significant improvements when compared with the state-of-the art, for all scenarios and languages.The  research  presented  in  this  paper  is  conducted  as  partof  the  project  EMPATHIC  and  of  the  MENHIR  MSCAaction that have received funding from the European Union’s Horizon 2020 research and innovation program under grant agreements No 769872 an No 823907 respective",Perceptual borderline for balancing multi-class spontaneous emotional data,https://core.ac.uk/download/405654948.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2021.3071485,,core
491332131,2021-01-01T00:00:00,"Modeling natural hazards in 3D space constitute a significant step for managing and planning our living environment. The creation of accurate maps is needed to document the impact of natural hazards such as landslides. Loss of life, natural resources or property transform landslide phenomenon to a natural disaster. In landslide analysis different factors can be incorporated and studied such as landslides occurrence and occurrence, their distribution, mechanisms, pattern of failures. The development of reliable maps is also crucial for determining landslide susceptibility and risk. Late years, the emerging geospatial technologies are able to produce different types of 2D and 3D data. Unmanned Aerial Vehicle (UAV) or Unmanned Aerial Systems (UAS), support the acquisition of ultra-high detailed geospatial data in the 3D environment. Those systems are flexible in data acquisition, with a high temporal frequency, while it is limited for site specific mapping purposes. The exploitation of 3D point-clouds has been proven tremendously efficient for analyzing data in the field of geoscience. Point cloud advantages of documenting in 3D space, data of hazardous sites at low cost and effective performance identifies them as leading primitives for site-specific 3D landslide modelling. Given the gaps between the computer vision capabilities and their applications in landslide assessment in site-specific scale, the thesis aims at developing a general framework of predefined workflows in an object-based programming environment for detection and characterization of landslide phenomena from ultra-high-resolution UAV-derived data. The proposed framework is built up in four distinct research phases: (a) on-site data collection, (b) data preprocessing, (c) OBIA (segmentation and classification), and (d) evaluation. These phases result in various novel component-wise solutions, which particular focus on the optimization phase of OBIA for landslide assessment. Different flight acquisition configurations were tested by varying the number of images, image overlap, flight height and focal length for selecting the optimal workflow for imagery collection always considering the site specifications (topography, landslide mechanism). Each configuration was processed independently with dedicated photogrammetric software following the same template for subjective evaluation. Structure from Motion (SfM) photogrammetry has been used to provide dense 3D point clouds describing surface morphology of landslide environments. An object-based classification approach of the photogrammetric point cloud products into homogeneous and spatially connected elements has been executed. The proposed methodology has been developed based on Object-Based Image Analysis (OBIA) and fusion of multivariate data resulted from photogrammetric processing in order to take full advantage of its productivity. The focus of modeling applications was particularly on landslide with rotational and translational mechanisms of failure. A critical comparative study was conducted to analyze the influence of topographic information, scale segmentation and evaluate the object-based classification of landslide ontologies with three state-of-the-art Machine Learning classifiers, KNN, DT and RF with the inclusion of spectral, spatial, and contextual characteristics. Results highlight higher performances for landslide mapping with RF when DSM information was integrated. Thus, RF presented higher predictive performance when the model was fitted and applied to a different study area. For the ML classification of landslide zones, 60% of the reference segments have been used for training and 40% for validation of the models. The proposed thesis illustrates the effectiveness and efficiency of UAV platforms to acquire accurate photogrammetric datasets from complex surface topographies and provide an efficient and transferable object-based framework to characterize the failure site based on semantic classification of the landslide elements. The outcome can be useful for prioritizing efforts to moderate the adverse consequences of landslides and provide future mitigation strategies following landslide ontologies. UAV-based landslide modelling on the investigated sites provided an amount of undiscovered knowledge for landslide elements. Complementary to the developed workflow the accomplished real-world application, this work has shown the great potential of coupling UAV photogrammetry with object-based methods for assessing the landslide features in different hierarchical scales and provide a detailed automatic classification. In the future, the developed methodology will be further extended and tested on diverse landslide mechanisms, also including engineering geological attributes collected during the field reconnaissance.Η μοντελοποίηση των φυσικών καταστροφών σε τρισδιάστατο χώρο αποτελεί ένα σημαντικό βήμα για την διαχείριση του οικοσυστήματος στο οποίο ζούμε. Η διαχείριση φυσικών καταστροφών όπως οι κατολισθήσεις απαιτούν σχεδιασμό και προγραμματισμό για την αποφυγή τραυματισμών και ανθρώπινων απωλειών. Διαφορετικοί παράμετροι μοντελοποίησης των κατολισθήσεων έχουν υπολογιστεί όπως η κατανομή τους, οι μηχανισμοί αστοχίας κα. Για να αξιολογηθεί καλύτερα η κατολισθητική επικινδυνότητα και διακινδύνευση αυτών απαραίτητο είναι η δημιουργία αξιόπιστων χαρτών καταγραφής των φαινομένων. Με την εξέλιξη της τεχνολογίας σήμερα μπορούμε να διαφορετικούς τύπους δεδομένων σε 2D και 3D περιβάλλοντα. Μια καινοτομία στην λήψη αεροφωτογραφιών αποτελούν τα Συστήματα μη Επανδρωμένων Αεροσκαφών - ΣΜηΕΑ (UAV), τα οποία βοηθούν σημαντικά στην λήψη 3D δεδομένων, καθώς μειώνουν σημαντικά το κόστος μιας εργασίας σε σύγκριση με συμβατικές μεθόδους. Η ανάπτυξη της Τηλεπισκόπισης και των εφαρμογών της δίνουν τη δυνατότητα χρήσης δεδομένων εξαγωγής πολύ υψηλής ανάλυσης (ορθοφωτογραφία, μοντέλο εδάφους, κα). Παρατηρώντας τις δυνατότητες της “υπολογιστικής δύναμης” και των εφαρμογών σε θέματα κατολισθήσεων, η παρούσα διατριβή επιδιώκει να αναπτύξει μια μεθοδολογία βασισμένη σε αντικειμενοστραφή ανάλυση εικόνας και τη χρήση των πρωτογενών δεδομένων από πτήσεις με ΣΜηΕΑ. η οποία θα αποτελεί εργαλείο εκτίμησης των κατολισθητικών φαινομένων. Συγκεκριμένα, η παρούσα διατριβή χτίστηκε σε τέσσερις διακριτούς πυλώνες οι οποίοι είναι οι εξής: (α) επι-τόπου συλλογή δεδομένων, (β) προ-επεξεργασία δεδομένων και προετοιμασία επιπέδων ανάλυσης, (γ) αντικειμενοστραφής ανάλυση εικόνας (κατάτμηση και ταξινόμηση) και το τελικό στάδιο (δ) αξιολόγηση. Η αντικειμενοστραφής ανάλυση εικόνας (OBIA) επιτρέπει την ενσωμάτωση επιπρόσθετων μεταβλητών όπως υφή, σχήμα, περιεχόμενο καθώς άλλων γνωστικών πληροφοριών με σκοπό τη βελτίωση των ταξινομήσεων. Η εφαρμογή της αντικειμενοστραφούς ανάλυσης σε θέματα φυσικών καταστροφών προέκυψε μετά την εμφάνιση εικόνων πολύ υψηλής χωρικής ανάλυσης και από την ανάγκη για την εύρεση μιας μεθοδολογίας η οποία να μιμείται την ανθρώπινη αναλυτική σκέψη. Με τη χρήση της αντικειμενοστραφούς ανάλυσης εικόνων επιδιώκεται η δημιουργία μιας βάσης γνώσης με την οποία θα μπορεί να γίνει αυτόματη εξαγωγή της κατολισθητικής πληροφορίας. Δοκιμάστηκαν διαφορετικά σετ απόκτησης εικόνων μεταβάλλοντας τον αριθμό των εικόνων, το ποσοστό επικάλυψη, το ύψος πτήσης και την εστιακή απόσταση, πάντα λαμβάνοντας υπόψη τις εκάστοτε λεπτομέρειες της περιοχής μελέτης (τοπογραφία, μηχανισμός κατολίσθησης). Δημιουργήθηκαν αρχικά τα κατάλληλα επίπεδα κατάτμησης για κάθε σετ δεδομένων ώστε να αναπτυχθεί η βέλτιστη ταξινόμηση των διαφορετικών εικονο-αντικειμένων για κάθε περιοχή μελέτης. Στα πλαίσια της βελτιστοποίησης της κατάτμησης των δεδομένων διαφορετικά τεστ πραγματοποιήθηκαν αφού προηγήθηκε εκτενής βιβλιογραφική ανασκόπηση των υπάρχοντών εφαρμογών και αλγορίθμων. Μετά την ολοκλήρωση της διαδικασίας της κατάτμησης, την συνένωση των στοιχειών με σκοπό την δημιουργία σημασιολογικών αντικειμένων, ακολουθεί η διαδικασία της ταξινόμησης. Στο επίπεδο αυτό, διαφορετικές μέθοδοι εξετάστηκαν όπως η χρήση τριών αλγορίθμων (KNN, DT και RF) που ανήκουν στην κατηγορία της Τεχνητής Νοημοσύνης καθώς και με την χρήση συναρτήσεων συμμετοχής (membership functions) οι οποίες στηρίζουν την λειτουργία τους στην φιλοσοφία της ασαφούς λογικής. Αν και η ανάλυση μέσω Τεχνητής Νοημοσύνης και ΟΒΙΑ, απαιτεί μεγάλο πλήθος δεδομένων και ισχυρές επεξεργαστικές δυνατότητες, υπάρχουν πολλά περισσότερα πλεονεκτήματα σε σχέση με τις συμβατικές μεθόδους ανάλυσης σε επίπεδο εικονοστοιχείου. Τέλος, ακολουθήθηκαν διαδικασίες αξιολόγησης όπου εντοπίστηκαν σφάλματα τα οποία έχουν πραγματοποιηθεί κατά την ανάθεση ενός αντικειμένου σε μια κατηγορία και εξάχθηκαν τα τελικά αποτελέσματα. Η μοντελοποίηση κατολισθήσεων εξήγαγε σημαντική πληροφορία  που με τις υπάρχουσες μεθοδολογίες δεν ήταν εφικτό. Η παρούσα μέθοδος διαφοροποιείται από τις προηγούμενες μεθόδους ταξινόμησης κυρίως όσον αφορά την τελική μορφή των εξαγόμενων μοντέλων. Η παρούσα διατριβή, παρουσιάζει την αποτελεσματικότητα των ΣΜηΕΑ να συλλέξουν αξιόπιστα δεδομένα για την βέλτιστη ανάλυση των κατολισθητικών φαινομένων σε δύσκολα περιβάλλοντα μέσω της μεθόδου της αντικειμενοστραφούς ανάλυσης εικόνας",Ανίχνευση και αξιολόγηση κατολισθητικών φαινομένων με τη μέθοδο της Αντικειμενοστραφούς Ανάλυσης Εικόνας και UAV φωτογραμμετρίας,,'National Documentation Centre (EKT)',10.12681/eadd/50888,,core
357569181,2020-04-24T00:00:00,"ABSTRACT We demonstrate F, a system for building regression models over database views. At its core lies the observation that the computation and representation of materialized views, and in particular of joins, entail non-trivial redundancy that is not necessary for the efficient computation of aggregates used for building regression models. F avoids this redundancy by factorizing data and computation and can outperform the state-of-the-art systems MADlib, R, and Python StatsModels by orders of magnitude on real-world datasets. We illustrate how to incrementally build regression models over factorized views using both an in-memory implementation of F and its SQL encoding. We also showcase the effective use of F for model selection: F decouples the datadependent computation step from the data-independent convergence of model parameters and only performs once the former to explore the entire model space. WHAT IS F? F is a fast learner of regression models over training datasets defined by select-project-join-aggregate (SPJA) views. It is part of an ongoing effort to integrate databases and machine learning including MADlib [2] and Santoku  (1) The database joins are an unnecessarily expensive bottleneck for learning due to redundancy in their tabular representation. To alleviate this limitation, F learns models in one pass over factorized joins, where repeating data patterns are only computed and represented once. This has both theoretical and practical benefits. The computational complexity of F follows that of factorized materialized SPJA views  The first step computes the aggregates necessary for regression and the factorized view on the input database. The output of this step is a matrix of reals whose dimensions only depend on the arity of the view and is independent of the database size. This matrix contains the necessary information to compute the parameters of any model defined by a subset of the features in the view. This step comes in three flavors  F&apos;s factorization and task decomposition rely on a representation of data and computation as expressions in the sum-product commutative semiring, which is subject to the law of distributivity of product over sum. Results of SPJA queries are naturally represented in the semiring with Cartesian product as product and union as sum. The derivatives of the objective functions for Least-Squares, Ridge, Lasso, and Elastic-Net regression models are expressible in the sum-product semiring. Optimization methods such as gradient descent and (quasi) Newton, which rely on first and respectively second-order derivatives of such objective functions, can thus be used to train any such model using F. HOW DOES F WORK? We next explain F by means of an example for learning a least-squares regression model over a factorized join. Factorized Joins",F: Regression Models over Factorized Views,https://core.ac.uk/download/357569181.pdf,,,,core
154981603,2020-06-07T00:00:00,"Classification of multivariate time series (MTS) has been tackled with a
large variety of methodologies and applied to a wide range of scenarios.
Reservoir Computing (RC) provides efficient tools to generate a vectorial,
fixed-size representation of the MTS that can be further processed by standard
classifiers. Despite their unrivaled training speed, MTS classifiers based on a
standard RC architecture fail to achieve the same accuracy of fully trainable
neural networks. In this paper we introduce the reservoir model space, an
unsupervised approach based on RC to learn vectorial representations of MTS.
Each MTS is encoded within the parameters of a linear model trained to predict
a low-dimensional embedding of the reservoir dynamics. Compared to other RC
methods, our model space yields better representations and attains comparable
computational performance, thanks to an intermediate dimensionality reduction
procedure. As a second contribution we propose a modular RC framework for MTS
classification, with an associated open-source Python library. The framework
provides different modules to seamlessly implement advanced RC architectures.
The architectures are compared to other MTS classifiers, including deep
learning models and time series kernels. Results obtained on benchmark and
real-world MTS datasets show that RC classifiers are dramatically faster and,
when implemented using our proposed representation, also achieve superior
classification accuracy","Reservoir computing approaches for representation and classification of
  multivariate time series",http://arxiv.org/abs/1803.07870,,,,core
386380658,2020-12-01T08:00:00,"With the rise of (semi)autonomous vehicles and continuum robotics technology and applications, there has been an increasing interest in controller and haptic interface designs. The presence of nonlinearities in the vehicle dynamics is the main challenge in the selection of control algorithms for real-time regulation and tracking of (semi)autonomous vehicles. Moreover, control of continuum structures with infinite dimensions proves to be difficult due to their complex dynamics plus the soft and flexible nature of the manipulator body. The trajectory tracking and control of automobile and robotic systems requires control algorithms that can effectively deal with the nonlinearities of the system without the need for approximation, modeling uncertainties, and input disturbances. Control strategies based on a linearized model are often inadequate in meeting precise performance requirements. To cope with these challenges, one must consider nonlinear techniques. Nonlinear control systems provide tools and methodologies for enabling the design and realization of (semi)autonomous vehicle and continuum robots with extended specifications based on the operational mission profiles. This dissertation provides an insight into various nonlinear controllers developed for (semi)autonomous vehicles and continuum robots as a guideline for future applications in the automobile and soft robotics field. A comprehensive assessment of the approaches and control strategies, as well as insight into the future areas of research in this field, are presented.First, two vehicle haptic interfaces, including a robotic grip and a joystick, both of which are accompanied by nonlinear sliding mode control, have been developed and studied on a steer-by-wire platform integrated with a virtual reality driving environment. An operator-in-the-loop evaluation that included 30 human test subjects was used to investigate these haptic steering interfaces over a prescribed series of driving maneuvers through real time data logging and post-test questionnaires. A conventional steering wheel with a robust sliding mode controller was used for all the driving events for comparison. Test subjects operated these interfaces for a given track comprised of a double lane-change maneuver and a country road driving event. Subjective and objective results demonstrate that the driver’s experience can be enhanced up to 75.3% with a robotic steering input when compared to the traditional steering wheel during extreme maneuvers such as high-speed driving and sharp turn (e.g., hairpin turn) passing.  Second, a cellphone-inspired portable human-machine-interface (HMI) that incorporated the directional control of the vehicle as well as the brake and throttle functionality into a single holistic device will be presented. A nonlinear adaptive control technique and an optimal control approach based on driver intent were also proposed to accompany the mechatronic system for combined longitudinal and lateral vehicle guidance. Assisting the disabled drivers by excluding extensive arm and leg movements ergonomically, the device has been tested in a driving simulator platform. Human test subjects evaluated the mechatronic system with various control configurations through obstacle avoidance and city road driving test, and a conventional set of steering wheel and pedals were also utilized for comparison. Subjective and objective results from the tests demonstrate that the mobile driving interface with the proposed control scheme can enhance the driver’s performance by up to 55.8% when compared to the traditional driving system during aggressive maneuvers. The system’s superior performance during certain vehicle maneuvers and approval received from the participants demonstrated its potential as an alternative driving adaptation for disabled drivers. Third, a novel strategy is designed for trajectory control of a multi-section continuum robot in three-dimensional space to achieve accurate orientation, curvature, and section length tracking. The formulation connects the continuum manipulator dynamic behavior to a virtual discrete-jointed robot whose degrees of freedom are directly mapped to those of a continuum robot section under the hypothesis of constant curvature. Based on this connection, a computed torque control architecture is developed for the virtual robot, for which inverse kinematics and dynamic equations are constructed and exploited, with appropriate transformations developed for implementation on the continuum robot. The control algorithm is validated in a realistic simulation and implemented on a six degree-of-freedom two-section OctArm continuum manipulator. Both simulation and experimental results show that the proposed method could manage simultaneous extension/contraction, bending, and torsion actions on multi-section continuum robots with decent tracking performance (e.g. steady state arc length and curvature tracking error of 3.3mm and 130mm-1, respectively). Last, semi-autonomous vehicles equipped with assistive control systems may experience degraded lateral behaviors when aggressive driver steering commands compete with high levels of autonomy. This challenge can be mitigated with effective operator intent recognition, which can configure automated systems in context-specific situations where the driver intends to perform a steering maneuver. In this article, an ensemble learning-based driver intent recognition strategy has been developed. A nonlinear model predictive control algorithm has been designed and implemented to generate haptic feedback for lateral vehicle guidance, assisting the drivers in accomplishing their intended action. To validate the framework, operator-in-the-loop testing with 30 human subjects was conducted on a steer-by-wire platform with a virtual reality driving environment. The roadway scenarios included lane change, obstacle avoidance, intersection turns, and highway exit. The automated system with learning-based driver intent recognition was compared to both the automated system with a finite state machine-based driver intent estimator and the automated system without any driver intent prediction for all driving events. Test results demonstrate that semi-autonomous vehicle performance can be enhanced by up to 74.1% with a learning-based intent predictor. The proposed holistic framework that integrates human intelligence, machine learning algorithms, and vehicle control can help solve the driver-system conflict problem leading to safer vehicle operations",Nonlinear Modeling and Control of Driving Interfaces and Continuum Robots for System Performance Gains,https://core.ac.uk/download/386380658.pdf,Clemson University Libraries,,,core
483559982,2020-08-13T16:57:06,"AbstractDesign innovation projects often generate large numbers of design ideas from designers, users, and, increasingly, the crowd over the Internet. Such idea data are often used for selection and implementation but, in fact, can 1also be used as sources of inspiration for further idea generation. In particular, the elementary concepts that underlie the original ideas can be recombined to generate new ideas. But it is not a trivial task to retrieve concepts from raw lists of ideas and data sources in a manner that can stimulate or generate new ideas. A significant difficulty lies in the fact that idea data are often expressed in unstructured natural languages. This paper develops a methodology that uses natural language processing to extract key words as elementary concepts embedded in massive idea descriptions and represents the elementary concept space in a core–periphery structure to direct the recombination of elementary concepts into new ideas. We apply the methodology to mine and represent the concept space underlying massive crowdsourced ideas and use it to generate new ideas for future transportation system designs in a real public sector-sponsored project via humans and automated computer programs. Our analysis of the human and computer recombination processes and outcomes sheds light on future research directions for artificial intelligence in design ideation.</jats:p",Mining and Representing the Concept Space of Existing Ideas for Directed Ideation,,'ASME International',10.1115/1.4044399,,core
392179445,2020-06-29T00:00:00,"Classification of multivariate time series (MTS) has
been tackled with a large variety of methodologies and applied
to a wide range of scenarios. Reservoir computing (RC) provides
efficient tools to generate a vectorial, fixed-size representation of
the MTS that can be further processed by standard classifiers.
Despite their unrivaled training speed, MTS classifiers based on
a standard RC architecture fail to achieve the same accuracy of
fully trainable neural networks. In this article, we introduce the
reservoir model space, an unsupervised approach based on RC
to learn vectorial representations of MTS. Each MTS is encoded
within the parameters of a linear model trained to predict a
low-dimensional embedding of the reservoir dynamics. Compared
with other RC methods, our model space yields better representations and attains comparable computational performance due to
an intermediate dimensionality reduction procedure. As a second
contribution, we propose a modular RC framework for MTS
classification, with an associated open-source Python library. The
framework provides different modules to seamlessly implement
advanced RC architectures. The architectures are compared with
other MTS classifiers, including deep learning models and time
series kernels. Results obtained on the benchmark and real-world
MTS data sets show that RC classifiers are dramatically faster
and, when implemented using our proposed representation, also
achieve superior classification accurac",Reservoir computing approaches for representation and classification of multivariate time series,https://core.ac.uk/download/392179445.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TNNLS.2020.3001377,"[{'title': 'IEEE Transactions on Neural Networks and Learning Systems', 'identifiers': ['2162-2388', 'issn:2162-237X', 'issn:2162-2388', '2162-237x']}]",core
417850714,2020-01-01T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and analysis in order to improve the process representation. This paper presents the development and implementation of quality monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality variable, such as surface roughness. Firstly, the hybrid incremental modelling strategy is applied. Secondly, a generalized fuzzy clustering c-means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing are performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to manufacturing industry.acceptedVersionPeer reviewe",Quality monitoring of complex manufacturing systems on the basis of model driven approach,https://core.ac.uk/download/417850714.pdf,'Techno-Press',10.12989/SSS.2020.26.4.495,"[{'title': 'Smart Structures and Systems', 'identifiers': ['1738-1584', 'issn:1738-1584']}]",core
389140546,2020-02-06T00:00:00,"Os ecossistemas de software da atualidade possuem estilo arquitetural de

microsserviços e características específicas, sistemas e dados distribuídos em

diferentes fontes, o que dificulta o gerenciamento de dados. Como os modelos

conceituais do mundo real são diferentes entre os sistemas, há problemas para

integrar os dados e realizar a comunicação entre esses microsserviços, o que

implica na necessidade de matching entre os esquemas e mensagens. A

literatura evidencia problemas de matching como o tamanho do espaço de

busca, a heterogeneidade semântica dos dados, e as atualizações pelas quais

os esquemas passam constantemente, e mostra como lacunas a inadequação

das interfaces de usuários, a acomodação de alterações nas estruturas de

dados, e, ainda, a escassez de abordagens para uso prático. Assim, o presente

estudo teve por objetivo é apresentar um modelo híbrido de semantic schema

matching para microsserviços com capacidade de identificar similaridades entre

os elementos de dois esquemas em larga escala, que suporte a atualização dos

esquemas e seus dados e, considere os resultados da validação humana para

reuso. Para tanto, foi apresentada a arquitetura do modelo HSSMatch e

implementado o protótipo do HSSMatch System que permite ao usuário, por

meio de uma interface gráfica Web, gerenciar o processo de schema matching

de microsserviços. A avaliação desse protótipo, no que se refere à sua

adequação de design de interação, foi feita por meio de experimentos e

questionários aplicados a usuários que atuam na área de integração de dados e

comunicação entre sistemas de software. A avaliação também foi realizada em

experimentos com dois datasets, e mostrou aspectos que confirmam a hipótese

deste estudo, pois verificou-se melhoria na eficiência e eficácia do processo de

schema matching utilizando a abordagem híbrida que acomoda alterações nos

dados, reduz o espaço de busca e combina matchers em nível de esquema e de

instâncias. Como trabalhos futuros, podem ser explorados métodos

supervisionados de aprendizado de máquina para configuração semiautomática

de estratégias de schema matching, e ainda outras técnicas de particionamento

de esquemas, ontologias de domínio em prol da melhoria da qualidade do

resultado de schema matching para domínios específicos.Today's software ecosystems have an architectural style of microservices and

specific characteristics, systems and data distributed across different sources,

making data management difficult. Because real-world conceptual models differ

across systems, there are problems integrating data and carrying out

communication between these microservices, which implies the need for

matching between schemas and messages. The literature highlights matching

problems such as search space size, semantic heterogeneity of data, and

updates that schemas constantly go through, and shows as gaps the inadequacy

of the user interfaces, the accommodation of changes in data structures, and the

scarcity of approaches for practical use. Thus, the present study aimed to present

a hybrid model of semantic schema matching for microservices with the ability to

identify similarities between the elements of two large-scale schemes that

supports the updating of the schemes and their data and considers the results of

human validation for reuse. For this, the architecture of the HSSMatch model was

presented, and the prototype of the HSSMatch System was implemented,

allowing the user, through a web graphical interface, to manage the

microservices schema matching process. The evaluation of this prototype,

regarding its suitability of interaction design, was made through experiments and

questionnaires applied to users working in the area of data integration and

communication between software systems. The evaluation was also performed

in experiments with two datasets, and showed aspects that confirm the

hypothesis of this study, since there was an improvement in the efficiency and

effectiveness of the schema matching process using the hybrid approach that

accommodates data changes, reduces search space and combines matchers at

schema and instance levels. As future work, supervised machine learning

methods for semiautomatic configuration of schema matching strategies, as well

as other schema partitioning techniques, domain ontologies for improving the

quality of schema matching results for specific domains can be explored.Instituto Presbiteriano Mackenzi",HSSMATCH: um modelo híbrido para semantic schema matching em arquiteturas orientadas a microsserviços,https://core.ac.uk/download/389140546.pdf,Engenharia Elétrica,,,core
326245379,2020-04-28T00:00:00,"Background Social distancing measures have been put in place to reduce social interaction to slow transmission of coronavirus (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns. Objective This project aims to provide a tool for older people, their families, and peers to improve their wellbeing and health during and after regulated social distancing. Firstly, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction during social-distancing. Secondly, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymised data from the app will be aggregated with other real-world data sources to develop a machine-learning algorithm to improve COVID-19 patient identification and track for real-time use by health systems. Methods Development of this project is occurring at the time of publication, and therefore a case study design was selected to provide a systematic means of capturing software engineering in progress. To mitigate potential issues of non-adoption of the proposed intervention, the system was designed using the non-adoption, abandonment, scale-up, spread and sustainability (NASSS) framework. The application development framework utilised is based on Agile methods. The evaluation of the solution’s acceptability and usability shall be conducted as a feasibility study to analyse factors impacting the solution’s use, adoption and uptake. Results Making use of a pre-existing software framework for health behaviour change, a proof of concept was developed, and multi-stage application development and deployment for the solution created. Grant submissions to fund the project and study execution have been sought at the time of publication, and pre-discovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought. Conclusions This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness",Agile requirements engineering and software planning for a digital health platform to engage the effects of isolation caused by social distancing: A Case study and feasibility study protocol,,'JMIR Publications Inc.',10.2196/19297,"[{'title': 'JMIR Public Health and Surveillance', 'identifiers': ['2369-2960', 'issn:2369-2960']}]",core
265418942,2020-01-01T00:00:00,"Robot-assisted endobronchial intervention requires accurate localisation based on both intra- and pre-operative data. Most existing methods achieve this by registering 2D videos with 3D CT models according to a defined similarity metric with local features. Instead, we formulate the bronchoscopic localisation as a learning-based global localisation using deep neural networks. The proposed network consists of two generative architectures and one auxiliary learning component. The cycle generative architecture bridges the domain variance between the real bronchoscopic videos and virtual views derived from pre-operative CT data so that the proposed approach can be trained through a large number of generated virtual images but deployed through real images. The auxiliary learning architecture leverages complementary relative pose regression to constrain the search space, ensuring consistent global pose predictions. Most importantly, the uncertainty of each global pose is obtained through variational inference by sampling within the learned underlying probability distribution. Detailed validation results demonstrate the localisation accuracy with reasonable uncertainty achieved and its potential clinical value",Generative localisation with uncertainty estimation through video-CT data for bronchoscopic biopsy,https://core.ac.uk/download/265418942.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/LRA.2019.2955941,,core
344911416,2020-06-30T00:00:00,"Executive Summary

This white paper explores future localization and sensing opportunities for beyond fifth generation (5G) wireless communication systems by identifying key technology enablers and discussing their underlying challenges, implementation issues, and identifying potential solutions. In addition, we present exciting new opportunities for localization and sensing applications, which will disrupt traditional design principles and revolutionize the way we live, interact with our environment, and do business. In contrast to 5G and earlier generations, localization and sensing will be built-in from the outset to both cope with specific applications and use cases, and to support ﬂexible and seamless connectivity.

Following the trend initiated in the 5G new radio (NR) systems, sixth generation (6G) will continue to develop towards even higher frequency ranges, wider bandwidths, and massive antenna arrays. In turn, this will enable sensing solutions with very fine range, Doppler and angular resolutions, as well as localization to cm-level degree of accuracy. Moreover, new materials, device types, and reconfigurable surfaces will allow network operators to reshape and control the electromagnetic response of the environment. At the same time, machine learning and artificial intelligence will leverage the unprecedented availability of data and computing resources to tackle the biggest and hardest problems in wireless communication systems.

6G systems will be truly intelligent wireless systems that will not only provide ubiquitous communication but also empower high accuracy localization and high-resolution sensing services. They will become the catalyst for this revolution by bringing about a unique new set of features and service capabilities, where localization and sensing will coexist with communication, continuously sharing the available resources in time, frequency and space. Applications such as THz imaging and spectroscopy have the potential to provide continuous, real-time physiological information via dynamic, non-invasive, contactless measurements for future digital health technologies. 6G simultaneous localization and mapping (SLAM) methods will not only enable advanced cross reality (XR) applications but also enhance the navigation of autonomous objects such as vehicles and drones. In convergent 6G radar and communication systems, both passive and active radars will simultaneously use and share information, to provide a rich and accurate virtual image of the environment. In 6G, intelligent context-aware networks will be capable of exploiting localization and sensing information to optimize deployment, operation, and energy usage with no or limited human intervention.

This white paper concludes by highlighting foundational research challenges, as well as implications and opportunities related to privacy, security, and trust. Addressing these challenges will undoubtedly require an interdisciplinary and concerted effort from the research community",6G white paper on localization and sensing,,Oulun yliopisto,,,core
286816719,"January 27, 2020","The Autonomous Medical Operations (AMO) group at NASA Ames is developing a medical decision support system to enable astronauts on long-duration exploration missions to operate autonomously. The system will support clinical actions by providing medical interpretation advice and procedural recommendations during emergent care and clinical work performed by crew. The current state of development of the system, called MedICS (Medical Interpretation Classification and Segmentation) includes two separate aspects: a set of machine learning diagnostic models trained to analyze organ images and patient health records, and an interface to ultrasound diagnostic hardware and to medical repositories. Three sets of images of different organs and medical records were utilized for training machine learning models for various analyses, as follows: 1. Pneumothorax condition (collapsed lung). The trained model provides a positive or negative diagnosis of the condition. 2. Carotid artery occlusion. The trained model produces a diagnosis of 5 different occlusion levels (including normal). 3. Ocular retinal images. The model extracts optic disc pixels (image segmentation). This is a precursor step for advanced autonomous fundus clinical evaluation algorithms to be implemented in FY20. 4. Medical health records. The model produces a differential diagnosis for any particular individual, based on symptoms and other health and demographic information. A probability is calculated for each of 25 most common conditions. The same model provides the likelihood of survival. All results are provided with a confidence level. Item 1 images were provided by the US Army and were part of a data set for the clinical treatment of injured battlefield soldiers. This condition is relevant to possible space mishaps, due to pressure management issues. Item 2 images were provided by Houston Methodist Hospital, and item 3 health records were acquired from the MIT laboratory of computational physiology. The machine learning technology utilized is deep multilayer networks (Deep Learning), and new models will continue to be produced, as relevant data is made available and specific health needs of astronaut crews are identified. The interfacing aspects of the system include a GUI for running the different models, and retrieving and storing data, as well as support for integration with an augmented reality (AR) system deployed at JSC by Tietronix Software Inc. (HoloLens). The AR system provides guidance for the placement of an ultrasound transducer that captures images to be sent to the MedICS system for diagnosis. The image captured and the associated diagnosis appear in the technicians AR visual display",Medics: Medical Decision Support System for Long-Duration Space Exploration,https://core.ac.uk/download/pdf/286816719.pdf,,,,core
146477519,2020-07-16T00:00:00,"Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model, leading to the search for new simulation
strategies. Taking advantage of the sparse character of brain-like computation,
the event-driven technique could represent a way to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new avenues
for brain modelling. In this paper we introduce FNS, the first LIFL-based
spiking neural network framework, which combines spiking/synaptic neural
modelling with the event-driven approach, allowing us to define heterogeneous
neuron modules and multi-scale connectivity with delayed connections and
plastic synapses. In order to allow multi-thread implementations a novel
parallelization strategy is also introduced. This paper presents mathematical
models, software implementation and simulation routines on which FNS is based.
Finally, a brain subnetwork is modeled on the basis of real brain structural
data, and the resulting simulated activity is compared with associated brain
functional (source-space MEG) data, demonstrating a good matching between the
activity of the model and that of the experimetal data. This work aims to lay
the groundwork for future event-driven based personalised brain models.Comment: Changed title, added references, corrected typo","FNS: an event-driven spiking neural network simulator based on the LIFL
  neuron model",http://arxiv.org/abs/1801.00864,,,,core
326502054,2020-07-02T00:00:00,"Despite the remarkable performance of deep neural networks on various
computer vision tasks, they are known to be susceptible to adversarial
perturbations, which makes it challenging to deploy them in real-world
safety-critical applications. In this paper, we conjecture that the leading
cause of adversarial vulnerability is the distortion in the latent feature
space, and provide methods to suppress them effectively. Explicitly, we define
\emph{vulnerability} for each latent feature and then propose a new loss for
adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to
minimize the feature-level vulnerability during training. We further propose a
Bayesian framework to prune features with high vulnerability to reduce both
vulnerability and loss on adversarial samples. We validate our
\emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)}
method on multiple benchmark datasets, on which it not only obtains
state-of-the-art adversarial robustness but also improves the performance on
clean examples, using only a fraction of the parameters used by the full
network. Further qualitative analysis suggests that the improvements come from
the suppression of feature-level vulnerability.Comment: Accepted to ICML 2020. Code available at
  https://github.com/divyam3897/ANP_V",Adversarial Neural Pruning with Latent Vulnerability Suppression,http://arxiv.org/abs/1908.04355,,,,core
392252897,2020-01-01T00:00:00,"Orientador: Prof. Dr. Hertz Wendel de CamargoDissertação (mestrado) - Universidade Federal do Paraná, Setor de Artes, Comunicação e Design, Programa de Pós-Graduação em Comunicação. Defesa : Curitiba, 29/04/2020Inclui referências: p. 114-117Resumo: A atividade de refatoração tem como principal objetivo aplicar um conjunto de transformações em um artefato de software para melhorar sua estrutura sem alterar sua funcionalidade. Alguns estudos recentes, apresentam bons resultados ao gerarem modelos de predição de refatorações. Além disso, os estudos mostram que refatorações similares são aplicadas em diferentes contextos e podem ser aprendidas. N este sentido, a m aioria dos trabalhos existentes utiliza técnicas de aprendizado de m áquina para gerar modelos que predizem se um dado trecho de código deve ser refatorado. Entretanto, essas abordagens possuem limitações. Elas buscam por refatorações específicas e exatam ente com o aplicadas por desenvolvedores, o que lim ita que outras refatorações sejam encontradas. D ada a natureza subjetiva da atividade de refatoração de software, a exploração por refatorações com base em outros critérios tam bém é vantajosa. Existem trabalhos na área conhecida como Refatoração de Software Baseada em Busca (SBR) (do inglês, Search Based Software Refactoring), em que algoritmos de busca são utilizados para encontrar refatorações em um grande espaço de busca e visando a m elhorar diversos aspectos. Recentem ente, trabalhos em SBR com eçaram a utilizar exemplos de refatorações já aplicadas por desenvolvedores para incorporar aprendizado na busca. Entretanto, essas abordagens são limitadas em term os de generalização dos resultados, um a vez que não geram um m odelo que possa ser utilizado para diferentes program as. D esse modo, abordagens existentes de SBR devem ser configuradas e executadas a cada novo programa. N este contexto, este trabalho visa a incorporar os benefícios encontrados na área de aprendizado de m áquina e na área de SBR, apresentando um a abordagem cham ada Gorgeous (do inglês, Generation o f Refactoring Algorithms through Grammatical Evolution). Gorgeous tem como objetivo gerar algoritmos de refatoração compostos por regras, que quando executados, determ inam trechos de código que devem ser refatorados e refatorações a serem aplicadas. Os algoritmos são criados de forma que as refatorações sugeridas sejam similares a refatorações aplicadas na prática e que também melhorem a qualidade do software. Os algoritmos são criados utilizando um processo de aprendizado que primeiro extrai padrões de refatoração de programas agrupando elementos que foram refatorados de m aneira similar. A pós isso, um a evolução gram atical é executada para gerar algoritm os de refatoração com base nos padrões extraídos. Gorgeous é avaliada utilizando dados de refatoração extraídos de 40 programas Java do repositório GitHub. Como resultado, os algoritmos gerados foram capazes de obter bons resultados para diferentes programas, melhorando em média 60% a qualidade do programa e obtendo 50% de similaridade com refatorações aplicadas na prática. Palavras-chave: Refatoração, Engenharia de Software Baseada em Busca, Agrupamento, Evolução GramaticalAbstract: The refactoring activity addresses the application of a set of transformations in software artifacts to improve their structure while preserving their functionality. Recent studies present prom ising results generating prediction models for refactoring. Furtherm ore, they provide evidences that similar refactoring operations are applied in different contexts and they can be learned using M achine Learning (ML). M ost works on M L based refactoring generate models to predict if a piece of code should be refactored. Despite the capability of prediction, existing works are limited to learn specific refactoring operations as applied by developers. However, to explore refactoring operations possibilities based on other criteria is also beneficial, mainly by the subjective context of refactoring. In this context, the Search-Based Softw are Refactoring (SBR) area addresses studies using search algorithm s to find refactoring operations in a huge search space, aiming at im proving several other aspects. However, existing SBR approaches do not support generalization of results since they do not generate a model as M L studies. In this way, a SBR approach needs to be configured and executed for each program in need of refactoring. In this context, this work introduces a SBR learning approach aiming at taking most advantage of both fields. Gorgeous (Generation of Refactoring Algorithms through Grammatical Evolution) generates refactoring algorithm s com posed by several rules determ ining pieces of code that should be refactored and the refactoring types to be used. A refactoring algorithm provides as solution a set of refactoring operations to be applied in a program. In this respect, the algorithm is generated w ith the goal of increasing sim ilarity of the refactoring operations w ith the ones applied in practice, and also im proving program quality. To do this, a learning process first extracts refactoring patterns from program s by grouping their elements that were refactored in similar ways. After that, a Grammatical Evolution (GE) is executed to generate the algorithms based on the extracted patterns. Gorgeous is evaluated using refactoring data from 40 Java programs of GitHub repository. The refactoring algorithms are capable of obtaining good results to different programs, obtaining around 60% of program quality improvement and 50% of similarity with real refactoring applications. Keywords: Refactoring, Search-Based Software Engineering, Clustering, Grammatical Evolutio",Jogabilidade da Caixa Preta : rituais iconofágicos em Bloodborne,,,,,core
355146033,2020-11-29T16:17:02,"High Level Synthesis (HLS) is a process which, starting from a high-level description of an application (C/C++), generates the  corresponding RTL code describing the hardware implementation of the desired functionality. The HLS process is usually  controlled by user-given directives (e.g., directives to set whether or not to unroll a loop) which influence the resulting  implementation area and latency. By using HLS, designers are able to rapidly generate different hardware implementations of  the same application, without the burden of directly specifying the low level implementation in detail. Nonetheless, the  correlation among directives and resulting performance is often difficult to foresee and to quantify, and the high number of  available direct- ives leads to an exponential explosion in the number of possible configurations. In addition, sampling the  design space involves a time-consuming hardware synthesis, making a brute-force exploration infeasible beyond very simple  cases. However, for a given application, only few directive settings result in Pareto- optimal solutions (with respect to metrics  such as area, run-time and power), while most are dominated. The design space exploration problem aims at identifying close  to Pareto-optimal implementations while synthesising only a small portion of the possible configurations from the design space.  In this dissertation I present an overview of the HLS design flow, followed by a discussion about existing strategies in literature.  Moreover, I present new exploration methodologies able to automatically generate optimised implementations of hardware  accelerators. The proposed approaches are able to retrieve a close approximation of the real Pareto solutions while  synthesising only a small fraction of the possible design, either by smartly navigating their design space or by leveraging prior  knowledge. Herein, I also present a database of design space explorations whose goal is to help researchers to design new  strategies by offering a reliable source of knowledge for machine learning based approaches, and standardise the  methodologies evaluation. Lastly, the stepping-stones of a new approach relying on deep learning strategies with graph neural  networks is presented, and final remarks about future research directions are discussed",Design space exploration in high-level synthesis,,,,,core
346367817,2020-09-01T00:00:00,"In this paper, mm-Pose, a novel approach to detect and track human skeletons in real-time using an mmWave radar, is proposed. To the best of the authors' knowledge, this is the first method to detect >15 distinct skeletal joints using mmWave radar reflection signals. The proposed method would find several applications in traffic monitoring systems, autonomous vehicles, patient monitoring systems and defense forces to detect and track human skeleton for effective and preventive decision making in real-time. The use of radar makes the system operationally robust to scene lighting and adverse weather conditions. The reflected radar point cloud in range, azimuth and elevation are first resolved and projected in Range-Azimuth and Range-Elevation planes. A novel low-size high-resolution radar-to-image representation is also presented, that overcomes the sparsity in traditional point cloud data and offers significant reduction in the subsequent machine learning architecture. The RGB channels were assigned with the normalized values of range, elevation/azimuth and the power level of the reflection signals for each of the points. A forked CNN architecture was used to predict the real-world position of the skeletal joints in 3-D space, using the radar-to-image representation. The proposed method was tested for a single human scenario for four primary motions, (i) Walking, (ii) Swinging left arm, (iii) Swinging right arm, and (iv) Swinging both arms to validate accurate predictions for motion in range, azimuth and elevation. The detailed methodology, implementation, challenges, and validation results are presented.University of ArizonaThis item from the UA Faculty Publications collection is made available by the University of Arizona with support from the University of Arizona Libraries. If you have questions, please contact us at repository@u.library.arizona.edu",mm-Pose: Real-Time Human Skeletal Posture Estimation Using mmWave Radars and CNNs,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/jsen.2020.2991741,"[{'title': 'IEEE Sensors Journal', 'identifiers': ['issn:2379-9153', '2379-9153', '1530-437x', 'issn:1530-437X']}]",core
346449703,2020-01-01T08:00:00,"This dissertation focusses mainly on loads determination, building informatics, and geothermal energy systems. The first chapter is Low-Energy Opportunity for Multi-Family Residences: A Simulation-Based Study of a Borehole Thermal Energy Storage System. In this chapter, we propose a district borehole thermal solar energy storage (BTES) system for both retrofit and new construction for a multi-family residence in the Midwestern United States, where the climate is moderately cold with very warm summers. Actual apartment interval power and water demand data was mined and used to estimate unit level hourly space and water heating demands, which was subsequently used to design a cost-optimal BTES system. Using a dynamic simulation model to predict the system performance over a 25-year period, a parametric study was conducted that varied the sizes of the BTES system and the solar collector array. A life-cycle cost analysis concluded that is it possible for an optimally-sized system to achieve an internal rate of return (IRR) of 11%, while reducing apartment-wide energy and carbon consumption by 46% The promise for district-scale adoption of BTES in multi-family residences is established, particularly for new buildings. In the second chapter (Alternate Approach to the Calculation of Thermal Response Factors for Vertical Borehole Ground Heat Exchanger Arrays Using an Incomplete Bessel Function),  we  presents another methodology for the calculation of dimensionless thermal response factors for vertical borehole ground heat exchanger (GHX) arrays, which is a concept introduced by Eskilson (1987). The presented method is based on a well-known solution to an analogous problem in the field of well hydraulics. This solution method, known mathematically as an incomplete Bessel function, and known in the field of well hydraulics as the `leaky aquifer function\u27, describes the hydraulic head distribution in an aquifer with predominantly radial flow to a well combined with vertical `leakage\u27 from geologic layers above and below the pumped aquifer. The solution is adapted to model heat transfer from an array of arbitrarily-placed vertical boreholes of finite depth. With proper expression of parameters in the incomplete Bessel function, we show that g-functions of previous researchers can be approximated. The proposed method has been implemented into Matlab and Excel/VBA for g-function generation and monthly GHX simulation.Chapter three (Energy Data Mining to Predict Chiller Demand) discusses the existing methods for predicting the cooling load (physical based models, data based models, and hybrid models). It also, consider the concerns raises about how researchers have defined the cooling load when utilizing data-based models to predict cooling demand. In this context, the goal of chapter three is to demonstrate the value of data-based modeling to estimating savings from improved HVAC systems and controls, especially focusing on cooling load prediction. Here, it is assumed that interval building demand data is available. As well, short-term interval data is available for the chiller. Such data could be collected from a short-term data-logging effort by an energy service company.  In the end, the goal is to utilize data-based modeling to relate chiller demand to whole building demand. If a model could be developed to accurately predict chiller power from whole building demand, then chiller health could be assessed in the future simply from whole building demand. Finally, in the last chapter (Predicting Chiller Running Capacity for School Buildings Using Stacking Learning) we tried to cover in depth most of the issues that researchers face when dealing with real-world data and to provide a clear road map for utilizing machine learning in energy engineering applications with that help reduce prediction error and reduce over-fitting from data, algorithm, and process considerations. This include data pre-processing, data balance, data subset, algorithmic considerations, ensemble learning, and evaluating model performance for both classification and regression problems",Comprehensive Study Toward Energy Opportunity for Buildings Considering Potentials for Using Geothermal and Predicting Chiller Demand,,eCommons,,,core
338815138,2020-10-01T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and analysis in order to improve the process representation. This paper presents the development and implementation of quality monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality variable, such as surface roughness. Firstly, the Hybrid Incremental Modelling strategy is applied. Secondly, a Generalized Fuzzy Clustering C-Means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing is performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to manufacturing industry",Quality Monitoring of Complex Manufacturing Systems on the basis of Model Driven Approach,"http://www.techno-press.org/content/?page=article&journal=sss&volume=26&num=4&ordernum=7,",'Techno-Press',10.12989/sss.2020.26.4.495,,core
429074282,2020-11-19T00:00:00,"Millions of battery-powered sensors deployed for monitoring purposes in a multitude of scenarios, e.g., agriculture, smart cities, industry, etc., require energy-efficient solutions to prolong their lifetime. When these sensors observe a phenomenon distributed in space and evolving in time, it is expected that collected observations will be correlated in time and space. In this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling mechanism capable of taking advantage of correlated information. We design our solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The proposed mechanism is capable of determining the frequency with which sensors should transmit their updates, to ensure accurate collection of observations, while simultaneously considering the energy available. To evaluate our scheduling mechanism, we use multiple datasets containing environmental observations obtained in multiple real deployments. The real observations enable us to model the environment with which the mechanism interacts as realistically as possible. We show that our solution can significantly extend the sensors' lifetime. We compare our mechanism to an idealized, all-knowing scheduler to demonstrate that its performance is near-optimal. Additionally, we highlight the unique feature of our design, energy-awareness, by displaying the impact of sensors' energy levels on the frequency of updates",Energy Aware Deep Reinforcement Learning Scheduling for Sensors Correlated in Time and Space,,arXiv.org,,,core
334946760,2020-10-07T00:00:00,"Recent work on adversarial learning has focused mainly on neural networks and
domains where they excel, such as computer vision. The data in these domains is
homogeneous, whereas heterogeneous tabular data domains remain underexplored
despite their prevalence. Constructing an attack on models with heterogeneous
input spaces is challenging, as they are governed by complex domain-specific
validity rules and comprised of nominal, ordinal, and numerical features. We
argue that machine learning models trained on heterogeneous tabular data are as
susceptible to adversarial manipulations as those trained on continuous or
homogeneous data such as images. In this paper, we introduce an optimization
framework for identifying adversarial perturbations in heterogeneous input
spaces. We define distribution-aware constraints for preserving the consistency
of the adversarial examples and incorporate them by embedding the heterogeneous
input into a continuous latent space. Our approach focuses on an adversary who
aims to craft valid perturbations of minimal l_0-norms and apply them in real
life. We propose a neural network-based implementation of our approach and
demonstrate its effectiveness using three datasets from different content
domains. Our results suggest that despite the several constraints heterogeneity
imposes on the input space of a machine learning model, the susceptibility to
adversarial examples remains unimpaired","Not All Datasets Are Born Equal: On Heterogeneous Data and Adversarial
  Examples",http://arxiv.org/abs/2010.03180,,,,core
370420110,2020-12-18T00:00:00,"VARCLUST algorithm is proposed for clustering variables under the assumption that variables in a given cluster are linear combinations of a small number of hidden latent variables, corrupted by the random noise. The entire clustering task is viewed as the problem of selection of the statistical model, which is defined by the number of clusters, the partition of variables into these clusters and the 'cluster dimensions', i.e. the vector of dimensions of linear subspaces spanning each of the clusters. The ""optimal"" model is selected using the approximate Bayesian criterion based on the Laplace approximations and using a non-informative uniform prior on the number of clusters. To solve the problem of the search over a huge space of possible models we propose an extension of the ClustOfVar algorithm of [29, 7] which was dedicated to subspaces of dimension only 1, and which is similar in structure to the K-centroid algorithm. We provide a complete methodology with theoretical guarantees, extensive numerical experi-mentations, complete data analyses and implementation. Our algorithm assigns variables to appropriate clusterse based on the consistent Bayesian Information Criterion (BIC), and estimates the dimensionality of each cluster by the PEnalized SEmi-integrated Likelihood Criterion (PESEL) of [24], whose consistency we prove. Additionally, we prove that each iteration of our algorithm leads to an increase of the Laplace approximation to the model posterior probability and provide the criterion for the estimation of the number of clusters. Numerical comparisons with other algorithms show that VARCLUST may outperform some popular machine learning tools for sparse subspace clustering. We also report the results of real data analysis including TCGA breast cancer data and meteorological data, which show that the algorithm can lead to meaningful clustering. The proposed method is implemented in the publicly available R package varclust. Keywords variable clustering · Bayesian approach · k-means · dimensionality reduction · subspace clustering 2 P. Sobczyk, S. Wilczyński, M. Bogdan et al",VARCLUST: clustering variables using dimensionality reduction,https://core.ac.uk/download/370420110.pdf,HAL CCSD,,,core
395010466,2020-01-01T00:00:00,"Deep Neural Network (DNN) models are now commonly used to automate and optimize complicated tasks in various fields. For improved performance, models increasingly use more processing layers and are frequently over-parameterized. Together these lead to tremendous increases in their compute and memory demands. While these demands can be met in large-scale and accelerated computing environments, they are simply out of reach for the embedded devices seen at the edge of a network and near edge devices such as smart phones and etc. Yet, the demand for moving these (recognition, decision) tasks to edge devices continues to grow for increased localized processing to meet privacy, real-time data processing and decision making needs. Thus, DNNs continue to move towards the edges of the networks at `edge' or `near-edge' devices, even though a limited off-chip storage and on-chip memory and logic on the edge devices prohibit the deployment and efficient computation of large yet highly-accurate models. Existing solutions to alleviate such issues improve either the underlying algorithm of these models to reduce their size and computational complexity or the underlying computing architectures to provide efficient computing platforms for these algorithms.
While these attempts improve computational efficiency of these models,  significant reductions are only possible through optimization of both the algorithms and the hardware for DNNs.In this dissertation, we focus on improving the computation cost of DNN models by taking into account the algorithmic optimization opportunities in the models along with hardware level optimization opportunities and limitations. The techniques proposed in this dissertation lie in two categories: optimal reduction of computation precision and optimal elimination of inessential computation and memory demands. Low precision but low-cost implementation of highly frequent computation through low-cost probabilistic data structures is one of the proposed  techniques to reduce the computation cost of DNNs. To eliminate excessive computation that has no more than minimal impact on the accuracy of these models, we propose a software-hardware approach that detects and predicts the outputs of the costly layers with fewer operations. Further, through the design of a machine learning based optimization framework, it has been shown that optimal platform-aware precision reduction at both algorithmic and hardware levels minimizes the computation cost while achieving acceptable accuracy. Finally, inspired by parameter redundancy in over-parameterized models and the limitations of the hardware, reducing the number of parameters of the models through a linear approximation of the parameters from a lower dimensional space is the last approach proposed in this dissertation. We show how a collection of these measures improve deployment of sophisticated DNN models on edge devices",Algorithm-Hardware Optimization of Deep Neural Networks for Edge Applications,,"eScholarship, University of California",,,core
334595153,2020-09-23T00:00:00,"The capture and transmission of remote-sensed imagery for Earth observation is both computationally and bandwidth expensive. In the analyses of remote-sensed imagery in the visual band, atmospheric cloud cover can obstruct up to two-thirds of observations, resulting in costly imagery being discarded. Mission objectives and satellite operational details vary; however, assuming a cloud-free observation requirement, a doubling of useful data downlinked with an associated halving of delivery cost is possible through effective cloud detection. A minimal-resource, real-time inference neural network is ideally suited to perform automatic cloud detection, both for pre-processing captured images prior to transmission and preventing unnecessary images being taken by larger payload sensors. Much of the hardware complexity of modern neural network implementations resides in high-precision floating-point calculation pipelines. In recent years, research has been conducted in identifying quantized, or low-integer precision equivalents to known deep learning models, which do not require the extensive resources of their floating-point, full-precision counterparts. Our work leverages existing research on binary and quantized neural networks to develop a real-time, remote-sensed cloud detection solution using a commodity field-programmable gate array. This follows on developments of the Forwards Looking Imager for predictive cloud detection developed by Craft Prospect, a space engineering practice based in Glasgow, UK. The synthesized cloud detection accelerator achieved an inference throughput of 358.1 images per second with a maximum power consumption of 2.4 W. This throughput is an order of magnitude faster than alternate algorithmic options for the Forwards Looking Imager at around one third reduction in classification accuracy, and approximately two orders of magnitude faster than the CloudScout deep neural network, deployed with HyperScout 2 on the European Space Agency PhiSat-1 mission. Strategies for incorporating fault tolerance mechanisms are expounded",FPGA acceleration of a quantized neural network for remote-sensed cloud detection,https://core.ac.uk/download/334595153.pdf,,,,core
370614442,2020-12-18T00:00:00,"VARCLUST algorithm is proposed for clustering variables under the assumption that variables in a given cluster are linear combinations of a small number of hidden latent variables, corrupted by the random noise. The entire clustering task is viewed as the problem of selection of the statistical model, which is defined by the number of clusters, the partition of variables into these clusters and the 'cluster dimensions', i.e. the vector of dimensions of linear subspaces spanning each of the clusters. The ""optimal"" model is selected using the approximate Bayesian criterion based on the Laplace approximations and using a non-informative uniform prior on the number of clusters. To solve the problem of the search over a huge space of possible models we propose an extension of the ClustOfVar algorithm of [29, 7] which was dedicated to subspaces of dimension only 1, and which is similar in structure to the K-centroid algorithm. We provide a complete methodology with theoretical guarantees, extensive numerical experi-mentations, complete data analyses and implementation. Our algorithm assigns variables to appropriate clusterse based on the consistent Bayesian Information Criterion (BIC), and estimates the dimensionality of each cluster by the PEnalized SEmi-integrated Likelihood Criterion (PESEL) of [24], whose consistency we prove. Additionally, we prove that each iteration of our algorithm leads to an increase of the Laplace approximation to the model posterior probability and provide the criterion for the estimation of the number of clusters. Numerical comparisons with other algorithms show that VARCLUST may outperform some popular machine learning tools for sparse subspace clustering. We also report the results of real data analysis including TCGA breast cancer data and meteorological data, which show that the algorithm can lead to meaningful clustering. The proposed method is implemented in the publicly available R package varclust. Keywords variable clustering · Bayesian approach · k-means · dimensionality reduction · subspace clustering 2 P. Sobczyk, S. Wilczyński, M. Bogdan et al",VARCLUST: clustering variables using dimensionality reduction,,HAL CCSD,,,core
401852046,2020-08-23T07:00:00,"Radio frequency (RF) communications are an integral part of many situational awareness applications. Sensing data need to be processed in a timely manner, making it imperative to have a robust and reliable RF link for information dissemination. Moreover, there is an increasing need for exploiting RF communication signals directly for sensing, leading to the notion of multi-function RF.
In the first part of this dissertation, we investigate the development of a robust Multiple-Input Multiple-Output (MIMO) communication system suitable for airborne platforms.Three majors challenges in realizing MIMO capacity gain in airborne environment are addressed: 1) antenna blockage due largely to the orientation of the antenna array; 2) the presence of unknown interference inherent to the intended application; 3) the lack of channel state information (CSI) at the transmitter. Built on the Diagonal Bell-Labs Layered Space-Time (D-BLAST) MIMO architecture, the system integrates three key design approaches: spatial spreading to counter antenna blockage; temporal spreading to mitigate signal to interference and noise ratio degradation due to intended or unintended interference; and a simple low rate feedback scheme to enable real time adaptation in the absence of full transmitter CSI. Extensive experiment studies using a fully functioning $4\times 4$ MIMO system validate the developed system.
In the second part, ambient RF signals are exploited to extract situational awareness information directly. Using WiFi signals as an example, we demonstrate that the CSI obtained at the receiver contains rich information about the propagation environment. Two distinct learning systems are developed for occupancy detection using passive WiFi sensing. The first one is based on deep learning where a parallel convolutional neural network (CNN) architecture is designed to extract useful information from both magnitude and phase of the CSI. Pre-processing steps are carefully designed to preserve human motion induced channel variation while insulating against other impairments and post-processing is applied after CNN to infer presence information for instantaneous motion outputs. To alleviate the need of tedious training efforts involved in deep learning based system, a novel learning problem with contaminated sampling is formulated. This leads to a second learning system: a two-stage solution for motion detection using support vector machines (SVM). A one-class SVM model is first evaluated whose training data are from human free environment only. Decontamination of human presence data using the one-class SVM is done prior to motion detection through a two-class support vector classifier. Extensive experiments using commercial off-the-shelf WiFi devices are conducted for both systems. The results demonstrate that the learning based RF sensing provides a viable and promising alternative for occupancy detection as they are much more sensitive to human motion than passive infrared sensors which are widely deployed in commercial and residential buildings",Multi-function RF for Situational Awareness,https://core.ac.uk/download/401852046.pdf,SURFACE at Syracuse University,,,core
389147822,2020-12-01T00:00:00,"Робота публікується згідно наказу ректора від 29.12.2020 р. №580/од ""Про розміщення кваліфікаційних робіт вищої освіти в репозиторії НАУ"". Керівник проекту: к.т.н., доцентТелешко Ігор ВасильовичIn this graduation project there was covered a significant topic- Machine learning implementation in face detection and recognition. In the first part the concept of Artificial intelligence and research areas related to it. Moreover, the real-life implementation and advantages and disadvantages of using AI. In the second part it was the Machine learning and the workflow that have to followed from defining the problem to data collection, pre-processing data, and finally developing and the evaluating of the model. In addition, the type of collected data which are the labeled and unlabeled data, for that matter there are two main types of learning (supervised and unsupervised). It is also possible to find here the exact relationship and differences between AI, ML, DL. Computer vision and basic information about RGB color space by the end of this part.
In the third part it was considered the artificial neural network mechanism and the components of it, also the different operations that occur in the network such as the activation function that helps with providing nonlinearity to the perceptron, also include deep learning algorithms for instance, gradient descent 
furthermore, a deep explaining of CNN and the sequence and functions of the layers of it, although the objective of using CNN which to extract high-level features from images by using more than one ConvNets layer, typically the first ConvNets is responsible to extract the low-level features such as edges, color. With more layers the model adapts to high-level features as wellУ цьому випускному проекті було висвітлено значну тему - Впровадження машинного навчання у виявленні та розпізнаванні облич. У першій частині концепція штучного інтелекту та напрямки досліджень, пов'язані з ним. Більше того, реалізація в реальному житті та переваги та недоліки використання ШІ. У другій частині машинному навчанню та робочому циклу слід було слідувати від визначення проблеми до збору даних, попередньої обробки даних і, нарешті, розробки та оцінки моделі. Крім того, тип зібраних даних - це марковані та немарковані дані, з цього приводу є два основних типи навчання (контрольоване та неконтрольоване). Тут також можна знайти точний взаємозв'язок та відмінності між AI, ML, DL. Комп’ютерне бачення та основна інформація про кольоровий простір RGB до кінця цієї частини.
У третій частині було розглянуто механізм штучної нейронної мережі та її компоненти, а також різні операції, що відбуваються в мережі, такі як функція активації, яка допомагає забезпечити нелінійність персептрону, також включає алгоритми глибокого навчання, наприклад, градієнт спуск
крім того, глибоке пояснення CNN та послідовності та функцій його шарів, хоча мета використання CNN, яка вилучає високоякісні функції із зображень за допомогою більш ніж одного шару ConvNets, як правило, перша ConvNets відповідає за вилучення функції низького рівня, такі як краї, колір. З більшою кількістю шарів модель також адаптується до функцій високого рівн",Застосування машинного навчання для розпізнавання та ідентифікації облич,https://core.ac.uk/download/389147822.pdf,National Аviation University,,,core
305126226,2020-04-28T00:00:00,"Canonical transformation plays a fundamental role in simplifying and solving
classical Hamiltonian systems. We construct flexible and powerful canonical
transformations as generative models using symplectic neural networks. The
model transforms physical variables towards a latent representation with an
independent harmonic oscillator Hamiltonian. Correspondingly, the phase space
density of the physical system flows towards a factorized Gaussian distribution
in the latent space. Since the canonical transformation preserves the
Hamiltonian evolution, the model captures nonlinear collective modes in the
learned latent representation. We present an efficient implementation of
symplectic neural coordinate transformations and two ways to train the model.
The variational free energy calculation is based on the analytical form of
physical Hamiltonian. While the phase space density estimation only requires
samples in the coordinate space for separable Hamiltonians. We demonstrate
appealing features of neural canonical transformation using toy problems
including two-dimensional ring potential and harmonic chain. Finally, we apply
the approach to real-world problems such as identifying slow collective modes
in alanine dipeptide and conceptual compression of the MNIST dataset.Comment: Main text: 9 pages, 8 figures. Supplement: 2 page, 1 figure. GitHub
  link: https://github.com/li012589/neuralC",Neural Canonical Transformation with Symplectic Flows,http://arxiv.org/abs/1910.00024,'American Physical Society (APS)',10.1103/PhysRevX.10.021020,,core
329133379,2020-09-02T00:00:00,"Open-set recognition and adversarial defense study two key aspects of deep
learning that are vital for real-world deployment. The objective of open-set
recognition is to identify samples from open-set classes during testing, while
adversarial defense aims to defend the network against images with
imperceptible adversarial perturbations. In this paper, we show that open-set
recognition systems are vulnerable to adversarial attacks. Furthermore, we show
that adversarial defense mechanisms trained on known classes do not generalize
well to open-set samples. Motivated by this observation, we emphasize the need
of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an
Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed
network uses an encoder with feature-denoising layers coupled with a classifier
to learn a noise-free latent feature representation. Two techniques are
employed to obtain an informative latent feature space with the objective of
improving open-set performance. First, a decoder is used to ensure that clean
images can be reconstructed from the obtained latent features. Then,
self-supervision is used to ensure that the latent features are informative
enough to carry out an auxiliary task. We introduce a testing protocol to
evaluate OSAD performance and show the effectiveness of the proposed method in
multiple object classification datasets. The implementation code of the
proposed method is available at: https://github.com/rshaojimmy/ECCV2020-OSAD.Comment: Accepted by ECCV 202",Open-set Adversarial Defense,http://arxiv.org/abs/2009.00814,,,,core
395623914,2020-08-03T00:00:00,"International audienceWith the rapid advancement of power electronic technologies and the reduction of photovoltaic cell price, the share of solar energy in the total power production has been booming recently. On the one hand, the increase in the amount of power delivered by solar energy can be beneficial in many economic and environmental aspects. On the other hand, this can cause various technical challenges to network operators. One of these issues is related to classifying faults located in distribution networks with high penetration of photovoltaic systems. Although many studies have paid significant attention to developing new algorithms applicable for a more active today distribution networks, there is still space for other improvements. Hence, after reviewing stateof-the-art researches, this paper was intended to develop a fault classification that is based on artificial neural networks. In particular, a technique so-called Multiplayer Perceptron Classifier was selected for the proposed algorithm. First, the authors generated a data set for the study by modeling and simulating a real distribution network with practical parameters provided by a local utility in the environment software PowerFactory/DigSILENT. Multiple fault scenarios were simulated. Second, a part of the generated data collection was used for network learning. Finally, the performance of the proposed methodology was demonstrated via testing on the remaining number of generated data",A Fault Classification Method for Medium Voltage Networks with a high Penetration of Photovoltaic Systems using Artificial Neural Networks,,HAL CCSD,,,core
428697421,2020-08-01T00:00:00,"Modern radio telescopes combine thousands of receivers, long-distance networks, large-scale compute hardware, and intricate software. Due to this complexity, failures occur relatively frequently. In this work, we propose novel use of unsupervised deep learning to diagnose system health for modern radio telescopes. The model is a convolutional variational autoencoder (VAE) that enables the projection of the high-dimensional time–frequency data to a low-dimensional prescriptive space. Using this projection, telescope operators are able to visually inspect failures thereby maintaining system health. We have trained and evaluated the performance of the VAE quantitatively in controlled experiments on simulated data from HERA. Moreover, we present a qualitative assessment of the model trained and tested on real LOFAR data. Through the use of a naïve SVM classifier on the projected synthesized data, we show that there is a trade-off between the dimensionality of the projection and the number of compounded features in a given spectrogram. The VAE and SVM combination scores between 65 per cent and 90 per cent accuracy depending on the number of features in a given input. Finally, we show the prototype system-health-diagnostic web framework that integrates the evaluated model. The system is currently undergoing testing at the ASTRON observatory",Deep learning assisted data inspection for radio astronomy,,'Oxford University Press (OUP)',10.1093/mnras/staa1412,,core
288570495,2020-01-01T00:00:00,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust Optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-dominated sorting genetic algorithm-II, and ∼23 – 29% improvement is observed in the objective functions. Further, the spread and diversity metrics are enhanced by ∼10 – 95% as compared to those obtained using other standard uncertainty sets",Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,,'Elsevier BV',10.1016/j.ress.2020.106821,,core
363987084,2020-01-01T00:00:00,"Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted retraining can be easily implemented on top of existing methods, and is empirically shown to significantly improve their efficiency and performance on synthetic and real-world optimization problems",Sample-efficient optimization in the latent space of deep generative models via weighted retraining,,,,,core
343943374,2020-01-01T08:00:00,"With increasing energy demand and an intermittent supply of renewable energy sources, our current energy grid needs a transformation towards a more robust, reliable energy trading architecture. The smart grid promises this architecture as the future of the present energy market, where traders will use digital technologies to automate the management of power delivery. It will improve many issues of the current energy grid such as sustainable, clean, renewable, reliable and secure energy supply, customer participation in markets, distributed generation, and transparency in energy trading. Using autonomous trading agents, we can bridge several dynamic energy markets and ensure an efficient and robust trading environment for all the players in the smart grid. The Power Trading Agent Competition (Power TAC) simulation emphasizes the strategic problems that autonomous trading agents, i.e., brokers, will face in managing the economics of a smart grid.
In Power TAC, brokers make trades in multiple parallel markets such as wholesale, tariff, and balancing markets to supply energy from producers to consumers. To be successful, brokers must make reasonable predictions about future supply, demand, and prices in the wholesale and tariff markets to make trading decisions by maintaining a favorable energy imbalance in the balancing market.
Market clearing price prediction is an integral part of the broker\u27s wholesale market strategy because it helps the broker to make intelligent decisions for purchasing energy at low cost in a day-ahead wholesale market. People use machine learning methods to predict prices in the Power TAC wholesale Periodic Double Auction (PDA) market, where the brokers can take advantage of the price predictor to design bidding strategies. PDAs are commonly used in real-world energy markets to trade energy in specific time slots to balance demand on the power grid where multiple discrete trading periods are specified for a single type of good. Strategically, bidding in a PDA is complicated because the bidder must predict and plan for future auctions that may influence the bidding strategy for the current auction.
In our work, we use the RepTree model to predict prices and present a general bidding strategy for PDAs. Our wholesale market strategy uses forecasted clearing prices and Monte Carlo Tree Search (MCTS) to plan a bidding approach across multiple time-periods. Additionally, we present a fast heuristic policy that can be used either as a standalone method or as a seeding technique to initialize the search space of the MCTS bidding strategy. We evaluate our bidding strategies using a controlled PDA simulator based on the wholesale market implemented in the PowerTAC competition. We demonstrate that our strategies outperform state-of-the-art champion bidding strategies designed for that competition.
In the retail market, a broker makes sequential decisions simultaneously with other brokers to buy and sell energy through publishing tariffs where a tariff is a contract between a broker and a customer. To be as profitable as possible, a broker needs an effective energy selling retail strategy. Our work includes developing an isolated miniature retail market simulator to control the dynamic and stochastic variables of the vibrant, complex retail market so that we can understand the basic features and strategic dynamics among retail trading strategies. We apply deep reinforcement learning (DQN) to learn the best response (BR) strategy for a specific strategy played in this simulator. Using DQN as a best response learning technique, we propose ``Clustered Double Oracle Empirical Game-Theoretic Analysis (CDO-EGTA), a novel method for minimizing regret (i.e., maximizing revenues) in retail trading. CDO-EGTA method clusters the existing pool of strategies into some groups, learns a new BR strategy for each of the groups using the Double Oracle Empirical Game-Theoretic Analysis method, and outputs a class of BR strategies to play the game. Empirical results show that our method outperforms the existing methods in regret comparison",Autonomous Trading Strategies For Dynamic Energy Markets,,ScholarWorks@UTEP,,,core
323305466,2020-02-24T00:00:00,"As Deep Learning continues to yield successful applications in Computer Vision, the ability to quantify all forms of uncertainty is a paramount requirement for its safe and reliable deployment in the real-world. In this work, we leverage the formulation of variational inference in function space, where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and variational family. Since GPs are fully determined by their mean and covariance functions, we are able to obtain predictive uncertainty estimates at the cost of a single forward pass through any chosen CNN architecture and for any supervised learning task. By leveraging the structure of the induced covariance matrices, we propose numerically efficient algorithms which enable fast training in the context of high-dimensional tasks such as depth estimation and semantic segmentation. Additionally, we provide sufficient conditions for constructing regression loss functions whose probabilistic counterparts are compatible with aleatoric uncertainty quantification",Scalable uncertainty for computer vision with functional variational inference,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
323253461,2020-05-25T00:00:00,"The apparent superposition of galaxies with other astrophysical objects along
the line of sight, a problem known as blending, will be a major challenge for
upcoming, ground-based, deep, photometric galaxy surveys, such as the Vera C.
Rubin Observatory Legacy Survey of Space and Time (LSST). Blending contributes
to the systematic error budget of weak lensing studies by perturbing object
detection and affecting photometric and shape measurements. Existing deblenders
suffer from the lack of flexible yet accurate models of galaxy morphologies and
therefore rely on assumptions (analytic profiles, symmetry, sparsity in a
profile basis) to isolate galaxies within a blended scene. In this paper, we
propose instead to use generative models based on deep neural networks, namely
variational autoencoders (VAE), to learn a probabilistic model directly from
data. Specifically, we train a VAE on images of centred, isolated galaxies,
which we reuse in a second VAE-like neural network in charge of deblending
galaxies. We train our networks on simulated images created with the GalSim
software, including all six LSST bandpass filters as well as the visible and
near-infrared bands of the Euclid satellite, as our method naturally
generalises to multiple bands and data from multiple instruments. We validate
our model and quantify deblending performance by measuring reconstruction
errors in galaxy shapes and magnitudes. We obtain median errors on
ellipticities between $\pm{0.01}$ and on $r$-band magnitude between $\pm{0.05}$
in most cases and shear multiplicative biases close to $10^{-2}$ in the optimal
configuration. Finally, we discuss future challenges about training on real
data and obtain encouraging results when applying transfer learning.Comment: Abstract abridged. Our code is publicly available on GitHub here:
  https://github.com/LSSTDESC/DeblenderVA","Deblending galaxies with Variational Autoencoders: a joint multi-band,
  multi-instrument approach",http://arxiv.org/abs/2005.12039,,,,core
323254795,2020-05-27T00:00:00,"Modern radio telescopes combine thousands of receivers, long-distance
networks, large-scale compute hardware, and intricate software. Due to this
complexity, failures occur relatively frequently. In this work we propose novel
use of unsupervised deep learning to diagnose system health for modern radio
telescopes. The model is a convolutional Variational Autoencoder (VAE) that
enables the projection of the high dimensional time-frequency data to a
low-dimensional prescriptive space. Using this projection, telescope operators
are able to visually inspect failures thereby maintaining system health. We
have trained and evaluated the performance of the VAE quantitatively in
controlled experiments on simulated data from HERA. Moreover, we present a
qualitative assessment of the the model trained and tested on real LOFAR data.
Through the use of a naive SVM classifier on the projected synthesised data, we
show that there is a trade-off between the dimensionality of the projection and
the number of compounded features in a given spectrogram. The VAE and SVM
combination scores between 65% and 90% accuracy depending on the number of
features in a given input. Finally, we show the prototype
system-health-diagnostic web framework that integrates the evaluated model. The
system is currently undergoing testing at the ASTRON observatory",Deep Learning Assisted Data Inspection for Radio Astronomy,http://arxiv.org/abs/2005.13373,'Oxford University Press (OUP)',10.1093/mnras/staa1412,,core
343513008,2020-01-01T00:00:00,"The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of two 0.5 m Schmidt telescopes with cameras covering 29 square degrees at plate scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely survey the whole sky visible from Hawaii (above delta > -50 degrees) every two nights, exposing four times per night, typically reaching o < 19 magnitude per exposure when the moon is illuminated and c < 19.5 magnitude per exposure in dark skies. Construction is underway of two further units to be sited in Chile and South Africa which will result in an all-sky daily cadence from 2021. Initially designed for detecting potentially hazardous near earth objects, the ATLAS data enable a range of astrophysical time domain science. To extract transients from the data stream requires a computing system to process the data, assimilate detections in time and space and associate them with known astrophysical sources. Here we describe the hardware and software infrastructure to produce a stream of clean, real, astrophysical transients in real time. This involves machine learning and boosted decision tree algorithms to identify extragalactic and Galactic transients. Typically we detect 10-15 supernova candidates per night which we immediately announce publicly. The ATLAS discoveries not only enable rapid follow-up of interesting sources but will provide complete statistical samples within the local volume of 100 Mpc. A simple comparison of the detected supernova rate within 100 Mpc, with no corrections for completeness, is already significantly higher (factor 1.5 to 2) than the current accepted rates.National Aeronautics & Space Administration (NASA)

NN12AR55G
80NSSC18K0284
80NSSC18K1575
EU FP7/2007-2013 ERC 	
291222
STFC Grants 	
ST/P000312/1
ST/N002520/1
ST/S006109/1
QUB Kelvin HPC cluster 	 
QUB International Engagement Fund 	 
European Union (EU)

84247",Design and Operation of the ATLAS Transient Science Server,,'IOP Publishing',10.1088/1538-3873/ab936e,,core
334920641,2020-03-09T00:00:00,"Deep Neural Networks (DNNs) have emerged as a powerful mechanism and are
being increasingly deployed in real-world safety-critical domains. Despite the
widespread success, their complex architecture makes proving any formal
guarantees about them difficult. Identifying how logical notions of high-level
correctness relate to the complex low-level network architecture is a
significant challenge. In this project, we extend the ideas presented in and
introduce a way to bridge the gap between the architecture and the high-level
specifications. Our key insight is that instead of directly proving the safety
properties that are required, we first prove properties that relate closely to
the structure of the neural net and use them to reason about the safety
properties. We build theoretical foundations for our approach, and empirically
evaluate the performance through various experiments, achieving promising
results than the existing approach by identifying a larger region of input
space that guarantees a certain property on the output.Comment: 5 pag","Finding Input Characterizations for Output Properties in ReLU Neural
  Networks",http://arxiv.org/abs/2003.04273,,,,core
334944977,2020-10-03T00:00:00,"Performance is an important non-functional aspect of the software
requirement. Modern software systems are highly-configurable and
misconfigurations may easily cause performance issues. A software system that
suffers performance issues may exhibit low program throughput and long response
time. However, the sheer size of the configuration space makes it challenging
for administrators to manually select and adjust the configuration options to
achieve better performance. In this paper, we propose ConfRL, an approach to
tune software performance automatically. The key idea of ConfRL is to use
reinforcement learning to explore the configuration space by a trial-and-error
approach and to use the feedback received from the environment to tune
configuration option values to achieve better performance. To reduce the cost
of reinforcement learning, ConfRL employs sampling, clustering, and dynamic
state reduction techniques to keep states in a large configuration space
manageable. Our evaluation of four real-world highly-configurable server
programs shows that ConfRL can efficiently and effectively guide software
systems to achieve higher long-term performance.Comment: 11 page",Automated Performance Tuning for Highly-Configurable Software Systems,http://arxiv.org/abs/2010.01397,,,,core
364708500,2020-01-01T00:00:00,"Today vessel detection from remote sensing images is increasingly becoming a crucial component in maritime surveillance applications. The increasing number of very high and medium resolution (VHR and MR) optical satellites shortens the revisit time as it was never before. This makes the technology especially attractive for a variety of maritime monitoring tasks. Nevertheless, it is quite a challenge to perform object detection on enormous large satellite images that cover several hundreds of square kilometers and derive results under near real time constraints.

This thesis presents an end-to-end multiclass vessel detection method from optical satellite images. The proposed workflow covers the complete processing chain and involves rapid image enhancement techniques, the fusion with automatic identification system (AIS) data, and the detection algorithm based on convolutional neural networks (CNN). To train the CNNs, two versions of training datasets were generated. The VHR training dataset was produced from the set of WorldView-[1-3] and GeoEye-1 images and contains about 40 000 of uniquely annotated vessels divided into 14 different classes. The MR training dataset was generated from the set of Landsat-8 images and contains about 14 000 of uniquely annotated vessels of 7 different classes.

The algorithms presented are implemented in the form of independent software processors and integrated in an automated processing chain as part of the Earth Observation Maritime Surveillance System (EO-MARISS). The solution developed from the methods presented has proven its usability within different projects and is used and further developed at the ground station of the German Aerospace Center (DLR) in Neustrelitz",Deep Learning-based Vessel Detection from Very High and Medium Resolution Optical Satellite Images as Component of Maritime Surveillance Systems,https://core.ac.uk/download/364708500.pdf,,,,core
287883823,2020-01-01T00:00:00,"Biological neural computation is inherently asynchronous due to large variations in neuronal spike timing and transmission delays. So-far, most theoretical work on neural networks assumes the synchronous setting where neurons fire simultaneously in discrete rounds. In this work we aim at understanding the barriers of asynchronous neural computation from an algorithmic perspective. We consider an extension of the widely studied model of synchronized spiking neurons [Maass, Neural Networks 97] to the asynchronous setting by taking into account edge and node delays.
- Edge Delays: We define an asynchronous model for spiking neurons in which the latency values (i.e., transmission delays) of non self-loop edges vary adversarially over time. This extends the recent work of [Hitron and Parter, ESA\u2719] in which the latency values are restricted to be fixed over time. Our first contribution is an impossibility result that implies that the assumption that self-loop edges have no delays (as assumed in Hitron and Parter) is indeed necessary. Interestingly, in real biological networks self-loop edges (a.k.a. autapse) are indeed free of delays, and the latter has been noted by neuroscientists to be crucial for network synchronization. 
To capture the computational challenges in this setting, we first consider the implementation of a single NOT gate. This simple function already captures the fundamental difficulties in the asynchronous setting. Our key technical results are space and time upper and lower bounds for the NOT function, our time bounds are tight. In the spirit of the distributed synchronizers [Awerbuch and Peleg, FOCS\u2790] and following [Hitron and Parter, ESA\u2719], we then provide a general synchronizer machinery. Our construction is very modular and it is based on efficient circuit implementation of threshold gates. The complexity of our scheme is measured by the overhead in the number of neurons and the computation time, both are shown to be polynomial in the largest latency value, and the largest incoming degree ? of the original network.
- Node Delays: We introduce the study of asynchronous communication due to variations in the response rates of the neurons in the network. In real brain networks, the round duration varies between different neurons in the network. Our key result is a simulation methodology that allows one to transform the above mentioned synchronized solution under edge delays into a synchronized under node delays while incurring a small overhead w.r.t space and time",The Computational Cost of Asynchronous Neural Communication,https://core.ac.uk/download/287883823.pdf,LIPIcs - Leibniz International Proceedings in Informatics. 11th Innovations in Theoretical Computer Science Conference (ITCS 2020),10.4230/LIPIcs.ITCS.2020.48,,core
391357192,2020-01-01T00:00:00,"Canny's algorithm is a very well-known and widely implemented multistage edge detector. The extraction of coastal lines in space-borne-based synthetic aperture radar (SAR) images using this algorithm is particularly complicated because of the multiplicative speckle noise present in them and can only be used if Canny's parameters (CaPP) are chosen appropriately. This letter introduces a methodology for computing functional forms for the CaPP, using functions of the image characteristics through a system that combines artificial neural networks (ANN) with statistical regression. A set of CaPP functional forms is obtained by applying this method on synthetic SAR images. Pratt's figure of merit (PFoM) is used to measure the performance of them, obtaining more than 0.75, on average, in the 14400 synthetic SAR images analyzed. Finally, this set of formulas has been tested for extracting coastal edges from real polynyas SAR images, acquired from Sentinel-1.Fil: Nemer Pelliza, Karim Alejandra. Universidad Tecnológica Nacional. Facultad Regional Córdoba. Departamento de Electronica; ArgentinaFil: Pucheta, Martín Alejo. Universidad Tecnológica Nacional. Facultad Regional Córdoba. Departamento de Electronica; Argentina. Consejo Nacional de Investigaciones Científicas y Técnicas; ArgentinaFil: Flesia, Ana Georgina. Universidad Nacional de Córdoba. Facultad de Matemática, Astronomia y Física. Sección Matemática. Grupo de Probabilidad y Estadística; Argentina. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Córdoba. Centro de Investigación y Estudios de Matemática. Universidad Nacional de Córdoba. Centro de Investigación y Estudios de Matemática; Argentin",Optimal Canny's Parameters Regressions for Coastal Line Detection in Satellite-Based SAR Images,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/LGRS.2019.2916225,"[{'title': 'IEEE Geoscience and Remote Sensing Letters', 'identifiers': ['1545-598x', '1558-0571', 'issn:1558-0571', 'issn:1545-598X']}]",core
346366642,2020-01-01T00:00:00,"Telecommunication systems have been through continuous evolution to keep up the fast-growing network traffic demand. With developments such as HD video streaming, cloud computing, Internet of Things (IoT), hyper-scale data centers, and 5G wireless networks, more demanding network requirements raise challenges to create a more efficient optical communication system that can provide the capability to support a wide range of applications. Specifically, 5G standards require more bandwidth and ultra-low latency; metro-scale optical aggregation networks motivate more scalable and on-demand optical network capacity.
Dynamic reconfigurable optical add-drop multiplexer (ROADM) based wavelength-division multiplexing (WDM) systems in which connections are established through real-time wavelength switching operations have long been studied as a means of achieving greater scalability and increasing the network resource utilization. A new dimension referred to as disaggregated optical systems, has the potential to further drive down cost by commoditizing the hardware. While ROADMs are extensively deployed in today’s WDM systems, their interoperability and functionality remain limited. Recent advances in hardware and software such as optical physical layer software-defined networking (SDN) significantly improve the multi-layer control and management potential of ROADM systems even facilitating wavelength switching. However, ensuring stable performance and reliable quality of transmission (QoT) remain severe problems, particularly for disaggregated systems. A key challenge in enabling disaggregated optical systems is the uncertainty and optical power dynamics that arise from a variety of physical effects in the amplifiers and transmission fiber.
This thesis examines the potential for machine learning for QoT estimation in software defined networking control, and its application to disaggregated ROADM systems. Current physical layer control of flexible meshed optical networks with dynamic reconfigurability is reviewed, and future network control plane architectures based on disaggregated optical systems are discussed. To enable high capacity and low latency in inter-domain routing, a transparent software defined exchange (tSDX) is proposed and implemented. To serve a broadening range of applications and increase network efficiency, a novel transmission system based on hysteresis controlled adaptive coding is studied, which can adapt to diverse and changing transmission conditions, including optical signal-to-noise (OSNR) variations. To resolve optical channel power excursions caused by wavelength operation in optically amplified networks, the dual laser switching technique is proposed and experimentally verified, which is able to cancel out the excursion. To build an accurate numerical model for an optical amplifier, which is a critical component in the calculation of the QoT, a novel machine learning (ML) model is studied based on deep neural networks (DNN) and supervised learning. Experimental results demonstrate the superiority of ML-based modeling in prediction accuracy of the optical channel power and gain spectrum of Erbium-Doped Fiber Amplifiers (EDFA). A hybrid machine learning (HML) model, which combines a-priori knowledge (the empirical numerical model) and a-posteriori knowledge (supervised machine learning model) is proposed and realized, which is shown to reduce the training complexity, both in time and space, compared to an analytical or neural network-based model. The potential improvement to the current QoT estimation framework is proposed and analyzed, based on this enhanced EDFA model",Machine Learning Enhanced Quality of Transmission Estimation in Disaggregated Optical Systems,,The University of Arizona.,,,core
328201344,2020-08-30T00:00:00,"Hyperspectral image (HSI) classification is one of the most active research
topics and has achieved promising results boosted by the recent development of
deep learning. However, most state-of-the-art approaches tend to perform poorly
when the training and testing images are on different domains, e.g., source
domain and target domain, respectively, due to the spectral variability caused
by different acquisition conditions. Transfer learning-based methods address
this problem by pre-training in the source domain and fine-tuning on the target
domain. Nonetheless, a considerable amount of data on the target domain has to
be labeled and non-negligible computational resources are required to retrain
the whole network. In this paper, we propose a new transfer learning scheme to
bridge the gap between the source and target domains by projecting the HSI data
from the source and target domains into a shared abundance space based on their
own physical characteristics. In this way, the domain discrepancy would be
largely reduced such that the model trained on the source domain could be
applied on the target domain without extra efforts for data labeling or network
retraining. The proposed method is referred to as physically-constrained
transfer learning through shared abundance space (PCTL-SAS). Extensive
experimental results demonstrate the superiority of the proposed method as
compared to the state-of-the-art. The success of this endeavor would largely
facilitate the deployment of HSI classification for real-world sensing
scenarios","Physically-Constrained Transfer Learning through Shared Abundance Space
  for Hyperspectral Image Classification",http://arxiv.org/abs/2008.08563,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TGRS.2020.3045790,,core
395064651,2020-12-01T00:00:00,"Today vessel detection from remote sensing images is increasingly becoming a crucial component in maritime surveillance applications. The increasing number of very high and medium resolution (VHR and MR) optical satellites shortens the revisit time as it was never before. This makes the technology especially attractive for a variety of maritime monitoring tasks. Nevertheless, it is quite a challenge to perform object detection on enormous large satellite images that cover several hundreds of square kilometers and derive results under near real time constraints.

This thesis presents an end-to-end multiclass vessel detection method from optical satellite images. The proposed workflow covers the complete processing chain and involves rapid image enhancement techniques, the fusion with automatic identification system (AIS) data, and the detection algorithm based on convolutional neural networks (CNN). To train the CNNs, two versions of training datasets were generated. The VHR training dataset was produced from the set of WorldView-[1-3] and GeoEye-1 images and contains about 40 000 of uniquely annotated vessels divided into 14 different classes. The MR training dataset was generated from the set of Landsat-8 images and contains about 14 000 of uniquely annotated vessels of 7 different classes.

The algorithms presented are implemented in the form of independent software processors and integrated in an automated processing chain as part of the Earth Observation Maritime Surveillance System (EO-MARISS). The solution developed from the methods presented has proven its usability within different projects and is used and further developed at the ground station of the German Aerospace Center (DLR) in Neustrelitz",Deep Learning-based Vessel Detection from Very High and Medium Resolution Optical Satellite Images as Component of Maritime Surveillance Systems,https://core.ac.uk/download/395064651.pdf,,,,core
386386346,2020-01-01T08:00:00,"Reinforcement learning has been applied to solve several real world challenging problems, from robotics to data center cooling. Similarly, adaption of reinforcement learning for multi-agent systems facilitated applications such as optimal multi-robot control and analysis of social-dilemmas. In this dissertation, we show that multi-agent reinforcement learning algorithms suffer from several stability issues such as multi-scenario learning, unstable training in dual-reward setting, overestimation bias and value function collapse, and provide solutions to each of these problems respectively. Several contributions of this dissertation have been formalized within the framework of a defensive escort team problems, a scenario where a team of learning robots is deployed as a defensive escort team to protect a high value payload. The goal here is to automatically learn an optimal formation around the payload that will minimize potential physical threats from nearby bystanders. We first formalize the defensive escort team problem as a security game problem based on game theoretic principles. Motivated from our defensive escort team problem, we present a distributed multi-agent reinforcement learning based solution to solve large scale security games and show that an optimal formation can automatically be learnt only with self play. In addition,  The multi-scenario instability problem arises when reinforcement learning agents are deployed on environments other than they are trained on. To tackle this issue, we propose  Multi-Agent Universal Policy Gradients: A novel multi-agent reinforcement learning algorithm inspired from universal value function approximators that generalizes over set of multiple scenarios. Our results show that our proposed solution works better than scenario-dependent policies. The instability in training arises when reinforcement learning agents try to maximize entangled multi-objective reward function. This is a challenging task for current state-of-the-art multi-agent reinforcement algorithms that are designed to either maximize the global reward of the team or the individual local rewards. The problem is exacerbated when either of the rewards is sparse leading to unstable learning. To address this problem, we present Decomposed Multi-Agent Deep Deterministic Policy Gradient (DE-MADDPG): a novel cooperative multi-agent reinforcement learning framework that simultaneously learns to maximize the global and local rewards. Overestimation bias is one of the major issues in reinforcement learning that contributes towards learning sub-optimal policies. Several techniques have been proposed that utilize an ensemble of neural networks to address the overestimation bias in reinforcement learning. However, the neural networks in the ensemble collapse to the same representation space, therefore, invalidating the use of ensemble neural networks to address the overestimation bias. To mitigate this issue, we propose five regularization techniques to maximize the representation diversity in the ensemble of neural networks. Although several contributions of this dissertation have been formalized within the framework of defensive escort team problem, the techniques developed in this dissertation are directly applicable to scenarios where groups of robots and humans need to navigate in a shared space such as robotic guide dogs for visually impaired humans. The contribution in the last chapter are more generic and can be applied to any reinforcement learning algorithm that uses and ensemble of neural networks",Multi-agent Reinforcement Learning for Defensive Escort Teams,,'Information Bulletin on Variable Stars (IBVS)',,,core
334860161,2020-04-01T00:00:00,"The current practice of manually tuning quantum dots (QDs) for qubit
operation is a relatively time-consuming procedure that is inherently
impractical for scaling up and applications. In this work, we report on the
{\it in situ} implementation of a recently proposed autotuning protocol that
combines machine learning (ML) with an optimization routine to navigate the
parameter space. In particular, we show that a ML algorithm trained using
exclusively simulated data to quantitatively classify the state of a double-QD
device can be used to replace human heuristics in the tuning of gate voltages
in real devices. We demonstrate active feedback of a functional double-dot
device operated at millikelvin temperatures and discuss success rates as a
function of the initial conditions and the device performance. Modifications to
the training network, fitness function, and optimizer are discussed as a path
toward further improvement in the success rate when starting both near and far
detuned from the target double-dot range.Comment: 9 pages, 7 figure",Auto-tuning of double dot devices in situ with machine learning,http://arxiv.org/abs/1909.08030,'American Physical Society (APS)',10.1103/PhysRevApplied.13.034075,,core
334906179,2020-03-04T00:00:00,"Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.Comment: Accepted to be published in the 33rd International FLAIRS Conference,
  AI in Healthcare Informatic","CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records",http://arxiv.org/abs/2001.09346,,,,core
442490257,2020-01-01T00:00:00,"A neural network-based controller is developed to enable a chaser spacecraft to approach and capture a disabled Environmental Satellite (ENVISAT). This task is conventionally tackled by framing it as an optimal control problem. However, the optimization of such a problem is computationally expensive and not suitable for onboard implementation. In this work, a learning-based approach is used to rapidly generate the control outputs of the controller based on a series of training samples. These training samples are generated by solving multiple optimal control problems with successive iterations. Then, Radial Basis Function (RBF) neural networks are designed to mimic this optimal control strategy from the generated data. Compared with a traditional controller, the neural network controller is able to generate real-time high-quality control policies by simply passing the input through the feedforward neural network",Real-Time Optimal Approach and Capture of ENVISAT Based on Neural Networks,,'Hindawi Limited',10.1155/2020/8165147,"[{'title': 'International Journal of Aerospace Engineering', 'identifiers': ['issn:1687-5966', 'issn:1687-5974', '1687-5974', '1687-5966']}]",core
326833721,2020-06-15T07:00:00,"Advances in the area of AI systems lead to the application of complex deep neural networks (DNN) that outperform other algorithms in critical applications like predictive maintenance, healthcare or autonomous driving. Unfortunately, the properties that render them so successful also lead to vulnerabilities that can make them the subject of adversarial attacks. While these systems try to mimic human behavior when transforming large amounts of data into decision recommendations, they remain black-box models so that humans often fail to detect adversarial behavior patterns in the model training process. Therefore, we derive a taxonomy from an extensive literature review to structure the knowledge of possible attack and defense patterns to create a basis for the analysis and implementation of AI security for scientists and practitioners alike. Furthermore, we use the taxonomy to expose the most common attack pattern and, in addition, we demonstrate the application of the taxonomy by projecting two real-world cases onto the taxonomy space and discuss applicable attack and defense patterns","FOOL ME ONCE, SHAME ON YOU, FOOL ME TWICE, SHAME ON ME: A TAXONOMY OF ATTACK AND DE-FENSE PATTERNS FOR AI SECURITY",,AIS Electronic Library (AISeL),,,core
429122023,2020-11-12T00:00:00,"[EN] Integrating collaborative data in data-driven Business Intelligence (BI) system brings an opportunity to foster the decision-making process towards improving tourism competitiveness. This article presents BITOUR, a BI platform that integrates four collaborative data sources (Twitter, Openstreetmap, Tripadvisor and Airbnb). BITOUR follows a classical BI architecture and provides functionalities for data transformation, data processing, data analysis and data visualization. At the core of the data processing, BITOUR offers mechanisms to identify tourists in Twitter, assign tweets to attractions and accommodation sites from Tripadvisor and Airbnb, analyze sentiments in opinions issued by tourists, and all this using geolocation objects in Openstreetmap. With all these ingredients, BITOUR enables data analysis and visualization to answer questions like the most frequented places by tourists, the average stay length or the view of visitors of some particular destination.This work has been supported by COLCIENCIAS through a PhD scholarship. This work is supported by the Spanish MINECO project TIN2017-88476-C2-1-R.Bustamante, A.; Sebastiá Tarín, L.; Onaindia De La Rivaherrera, E. (2020). BITOUR: A Business Intelligence Platform for Tourism Analysis. ISPRS International Journal of Geo-Information. 9(11):1-23. https://doi.org/10.3390/ijgi9110671S123911Nakahira, K. T., Akahane, M., & Fukami, Y. (2012). The Difference and Limitation of Cognition for Piano Playing Skill with Difference Educational Design. Smart Innovation, Systems and Technologies, 609-617. doi:10.1007/978-3-642-29934-6_59Chua, A., Servillo, L., Marcheggiani, E., & Moere, A. V. (2016). Mapping Cilento: Using geotagged social media data to characterize tourist flows in southern Italy. Tourism Management, 57, 295-310. doi:10.1016/j.tourman.2016.06.013Karagiannakis, N., Giannopoulos, G., Skoutas, D., & Athanasiou, S. (2015). OSMRec Tool for Automatic Recommendation of Categories on Spatial Entities in OpenStreetMap. Proceedings of the 9th ACM Conference on Recommender Systems. doi:10.1145/2792838.2796555Burcher, M., & Whelan, C. (2017). Social network analysis as a tool for criminal intelligence: understanding its potential from the perspectives of intelligence analysts. Trends in Organized Crime, 21(3), 278-294. doi:10.1007/s12117-017-9313-8Alcabnani, S., Oubezza, M., & Elkafi, J. (2019). An approach for the implementation of semantic Big Data Analytics in the Social Business Intelligence process on distributed environments (Cloud computing). Proceedings of the 4th International Conference on Big Data and Internet of Things. doi:10.1145/3372938.3373003Zeng, B., & Gerritsen, R. (2014). What do we know about social media in tourism? A review. Tourism Management Perspectives, 10, 27-36. doi:10.1016/j.tmp.2014.01.001Lalicic, L. (2018). Open innovation platforms in tourism: how do stakeholders engage and reach consensus? International Journal of Contemporary Hospitality Management, 30(6), 2517-2536. doi:10.1108/ijchm-04-2016-0233Dwyer, L., & Kim, C. (2003). Destination Competitiveness: Determinants and Indicators. Current Issues in Tourism, 6(5), 369-414. doi:10.1080/13683500308667962Gomezelj, D. O., & Mihalič, T. (2008). Destination competitiveness—Applying different models, the case of Slovenia. Tourism Management, 29(2), 294-307. doi:10.1016/j.tourman.2007.03.009Zhong, L., Deng, J., & Xiang, B. (2008). Tourism development and the tourism area life-cycle model: A case study of Zhangjiajie National Forest Park, China. Tourism Management, 29(5), 841-856. doi:10.1016/j.tourman.2007.10.002Fernández, J. I. P., & Rivero, M. S. (2009). Measuring Tourism Sustainability: Proposal for a Composite Index. Tourism Economics, 15(2), 277-296. doi:10.5367/000000009788254377Cibinskiene, A., & Snieskiene, G. (2015). Evaluation of City Tourism Competitiveness. Procedia - Social and Behavioral Sciences, 213, 105-110. doi:10.1016/j.sbspro.2015.11.411Business Intelligence (BI)—Glossaryhttps://www.gartner.com/it-glossary/business-intelligence-bi/Mariani, M., Baggio, R., Fuchs, M., & Höepken, W. (2018). Business intelligence and big data in hospitality and tourism: a systematic literature review. International Journal of Contemporary Hospitality Management, 30(12), 3514-3554. doi:10.1108/ijchm-07-2017-0461Maeda, T. N., Yoshida, M., Toriumi, F., & Ohashi, H. (2016). Decision Tree Analysis of Tourists’ Preferences Regarding Tourist Attractions Using Geotag Data from Social Media. Proceedings of the Second International Conference on IoT in Urban Space. doi:10.1145/2962735.2962745Guy, I., Mejer, A., Nus, A., & Raiber, F. (2017). Extracting and Ranking Travel Tips from User-Generated Reviews. Proceedings of the 26th International Conference on World Wide Web. doi:10.1145/3038912.3052632Peng, M. Y.-P., Tuan, S.-H., & Liu, F.-C. (2017). Establishment of Business Intelligence and Big Data Analysis for Higher Education. Proceedings of the International Conference on Business and Information Management - ICBIM 2017. doi:10.1145/3134271.3134296Castellanos, M., Gupta, C., Wang, S., Dayal, U., & Durazo, M. (2012). A platform for situational awareness in operational BI. Decision Support Systems, 52(4), 869-883. doi:10.1016/j.dss.2011.11.011Cohen, L. (2017). Impacts of business intelligence on population health. Proceedings of the South African Institute of Computer Scientists and Information Technologists on - SAICSIT  ’17. doi:10.1145/3129416.3129441Love, M., Boisvert, C., Uruchurtu, E., & Ibbotson, I. (2016). Nifty with Data. Proceedings of the 2016 ACM Conference on Innovation and Technology in Computer Science Education. doi:10.1145/2899415.2899431Berndt, D. J., Hevner, A. R., & Studnicki, J. (s. f.). Hospital discharge transactions: a data warehouse component. Proceedings of the 33rd Annual Hawaii International Conference on System Sciences. doi:10.1109/hicss.2000.926791Musa, G. J., Chiang, P.-H., Sylk, T., Bavley, R., Keating, W., Lakew, B., … Hoven, C. W. (2013). Use of GIS Mapping as a Public Health Tool–-From Cholera to Cancer. Health Services Insights, 6, HSI.S10471. doi:10.4137/hsi.s10471Mooney, S. J., Westreich, D. J., & El-Sayed, A. M. (2015). Commentary. Epidemiology, 26(3), 390-394. doi:10.1097/ede.0000000000000274Wisniewski, M. F., Kieszkowski, P., Zagorski, B. M., Trick, W. E., Sommers, M., & Weinstein, R. A. (2003). Development of a Clinical Data Warehouse for Hospital Infection Control. Journal of the American Medical Informatics Association, 10(5), 454-462. doi:10.1197/jamia.m1299Miah, S. J., Vu, H. Q., Gammack, J., & McGrath, M. (2017). A Big Data Analytics Method for Tourist Behaviour Analysis. Information & Management, 54(6), 771-785. doi:10.1016/j.im.2016.11.011Li, D., Deng, L., & Cai, Z. (2019). Statistical analysis of tourist flow in tourist spots based on big data platform and DA-HKRVM algorithms. Personal and Ubiquitous Computing, 24(1), 87-101. doi:10.1007/s00779-019-01341-xKrawczyk, M., & Xiang, Z. (2015). Perceptual mapping of hotel brands using online reviews: a text analytics approach. Information Technology & Tourism, 16(1), 23-43. doi:10.1007/s40558-015-0033-0Alaei, A. R., Becken, S., & Stantic, B. (2017). Sentiment Analysis in Tourism: Capitalizing on Big Data. Journal of Travel Research, 58(2), 175-191. doi:10.1177/0047287517747753Thelwall, M. (2019). Sentiment Analysis for Tourism. Big Data and Innovation in Tourism, Travel, and Hospitality, 87-104. doi:10.1007/978-981-13-6339-9_6Höpken, W., Fuchs, M., Höll, G., Keil, D., & Lexhagen, M. (2013). Multi-Dimensional Data Modelling for a Tourism Destination Data Warehouse. Information and Communication Technologies in Tourism 2013, 157-169. doi:10.1007/978-3-642-36309-2_14Sabou, M., Onder, I., Brasoveanu, A. M. P., & Scharl, A. (2015). Towards Cross-Domain Decision Making in Tourism: A Linked Data Based Approach. SSRN Electronic Journal. doi:10.2139/ssrn.2580242Fermoso, A. M., Mateos, M., Beato, M. E., & Berjón, R. (2015). Open linked data and mobile devices as e-tourism tools. A practical approach to collaborative e-learning. Computers in Human Behavior, 51, 618-626. doi:10.1016/j.chb.2015.02.032Wöber, K. W. (2003). Information supply in tourism management by marketing decision support systems. Tourism Management, 24(3), 241-255. doi:10.1016/s0261-5177(02)00071-7Vajirakachorn, T., & Chongwatpol, J. (2017). Application of business intelligence in the tourism industry: A case study of a local food festival in Thailand. Tourism Management Perspectives, 23, 75-86. doi:10.1016/j.tmp.2017.05.003Diakopoulos, N., Naaman, M., & Kivran-Swaine, F. (2010). Diamonds in the rough: Social media visual analytics for journalistic inquiry. 2010 IEEE Symposium on Visual Analytics Science and Technology. doi:10.1109/vast.2010.5652922Bustamante, A., Sebastia, L., & Onaindia, E. (2019). Can Tourist Attractions Boost Other Activities Around? A Data Analysis through Social Networks. Sensors, 19(11), 2612. doi:10.3390/s19112612Yasseri, T., Quattrone, G., & Mashhadi, A. (2013). Temporal analysis of activity patterns of editors in collaborative mapping project of OpenStreetMap. Proceedings of the 9th International Symposium on Open Collaboration. doi:10.1145/2491055.2491068Jilani, M., Corcoran, P., & Bertolotto, M. (2013). Multi-granular Street Network Representation towards Quality Assessment of OpenStreetMap Data. Proceedings of the Sixth ACM SIGSPATIAL International Workshop on Computational Transportation Science - IWCTS  ’13. doi:10.1145/2533828.2533833Luxen, D., & Vetter, C. (2011). Real-time routing with OpenStreetMap data. Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems - GIS  ’11. doi:10.1145/2093973.2094062Baumbach, S., Rubel, C., Ahmed, S., & Dengel, A. (2019). Geospatial Customer, Competitor and Supplier Analysis for Site Selection of Supermarkets. Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis. doi:10.1145/3318236.3318264Milot, J., Munroe, P., Beaudry, E., Grondin, F., & Bourdeau, G. (2016). Lookupia. Proceedings of the 25th International Conference Companion on World Wide Web - WWW  ’16 Companion. doi:10.1145/2872518.2890485Ciepluch, B., Mooney, P., Jacob, R., & Winstanley, A. C. (2009). Using OpenStreetMap to deliver location-based environmental information in Ireland. SIGSPATIAL Special, 1(3), 17-22. doi:10.1145/1645424.1645428Del Pilar Salas-Zárate, M., López-López, E., Valencia-García, R., Aussenac-Gilles, N., Almela, Á., & Alor-Hernández, G. (2014). A study on LIWC categories for opinion mining in Spanish reviews. Journal of Information Science, 40(6), 749-760. doi:10.1177/0165551514547842Gambino, O. J., & Calvo, H. (2016). A Comparison Between Two Spanish Sentiment Lexicons in the Twitter Sentiment Analysis Task. Advances in Artificial Intelligence - IBERAMIA 2016, 127-138. doi:10.1007/978-3-319-47955-2_11Mooney, P., Corcoran, P., & Winstanley, A. C. (2010). Towards quality metrics for OpenStreetMap. Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems - GIS  ’10. doi:10.1145/1869790.1869875El-Ashmawy, K. l. A. (2016). TESTING THE POSITIONAL ACCURACY OF OPENSTREETMAP DATA FOR MAPPING APPLICATIONS. Geodesy and cartography, 42(1), 25-30. doi:10.3846/20296991.2015.116049",BITOUR: A Business Intelligence Platform for Tourism Analysis,https://riunet.upv.es/bitstream/10251/166657/1/Bustamante%3bSebast%c3%a1%3bOnanda%20-%20BITOUR%3a%20A%20Busness%20Intellgence%20Platform%20for%20Toursm%20Analyss.pdf,'MDPI AG',10.3390/ijgi9110671,,core
477990133,2020-12-01T00:00:00,"[EN] Identifying deceptive online reviews is a challenging tasks for Natural Language Processing (NLP). Collecting corpora for the task is difficult, because normally it is not possible to know whether reviews are genuine. A common workaround involves collecting (supposedly) truthful reviews online and adding them to a set of deceptive reviews obtained through crowdsourcing services. Models trained this way are generally successful at discriminating between `genuine¿ online reviews and the crowdsourced deceptive reviews. It has been argued that the deceptive reviews obtained via crowdsourcing are very different from real fake reviews, but the claim has never been properly tested. In this paper, we compare (false) crowdsourced reviews with a set of `real¿ fake reviews published on line. We evaluate their degree of similarity and their usefulness in training models for the detection of untrustworthy reviews. We find that the deceptive reviews collected via crowdsourcing are significantly different from the fake reviews published online. In the case of the artificially produced deceptive texts, it turns out that their domain similarity with the targets affects the models¿ performance, much more than their untruthfulness. This suggests that the use of crowdsourced datasets for opinion spam detection may not result in models applicable to the real task of detecting deceptive reviews. As an alternative method to create large-size datasets for the fake reviews detection task, we propose methods based on the probabilistic annotation of unlabeled texts, relying on the use of meta-information generally available on the e-commerce sites. Such methods are independent from the content of the reviews and allow to train reliable models for the detection of fake reviews.Leticia Cagnina thanks CONICET for the continued financial support. This work was funded by MINECO/FEDER (Grant No. SomEMBED TIN2015-71147-C2-1-P). The work of Paolo Rosso was partially funded by the MISMIS-FAKEnHATE Spanish MICINN research project (PGC2018-096212-B-C31). Massimo Poesio was in part supported by the UK Economic and Social Research Council (Grant Number ES/M010236/1).Fornaciari, T.; Cagnina, L.; Rosso, P.; Poesio, M. (2020). Fake Opinion Detection: How Similar are Crowdsourced Datasets to Real Data?. Language Resources and Evaluation. 54(4):1019-1058. https://doi.org/10.1007/s10579-020-09486-5S10191058544Baeza-Yates, R. (2018). Bias on the web. Communications of the ACM, 61(6), 54–61.Banerjee, S., & Chua, A. Y. (2014). Applauses in hotel reviews: Genuine or deceptive? In: Science and Information Conference (SAI), 2014 (pp. 938–942). New York: IEEE.Bhargava, R., Baoni, A., & Sharma, Y. (2018). Composite sequential modeling for identifying fake reviews. Journal of Intelligent Systems,. https://doi.org/10.1515/jisys-2017-0501.Bickel, P. J., & Doksum, K. A. (2015). Mathematical statistics: Basic ideas and selected topics (2nd ed., Vol. 1). Boca Raton: Chapman and Hall/CRC Press.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, 3(Jan), 993–1022.Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In: Proceedings of the eleventh annual conference on computational learning theory (pp. 92–100). New York: ACM.Cagnina, L. C., & Rosso, P. (2017). Detecting deceptive opinions: Intra and cross-domain classification using an efficient representation. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 25(Suppl. 2), 151–174. https://doi.org/10.1142/S0218488517400165.Cardoso, E. F., Silva, R. M., & Almeida, T. A. (2018). Towards automatic filtering of fake reviews. Neurocomputing, 309, 106–116. https://doi.org/10.1016/j.neucom.2018.04.074.Carpenter, B. (2008). Multilevel bayesian models of categorical data annotation. Retrieved from http://lingpipe.files.wordpress.com/2008/11/carp-bayesian-multilevel-annotation.pdf.Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20, 273–297.Costa, P. T., & MacCrae, R. R. (1992). Revised NEO personality inventory (NEO PI-R) and NEO five-factor inventory (NEO FFI): Professional manual. Psychological Assessment Resources.Dawid, A. P., & Skene, A. M. (1979). Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, 28(1), 20–28.Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society Series B (Methodological), 39(1), 1–38.Elkan, C., & Noto, K. (2008). Learning classifiers from only positive and unlabeled data. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 213–220). New York: ACM.Fei, G., Mukherjee, A., Liu, B., Hsu, M., Castellanos, M., & Ghosh, R. (2013). Exploiting burstiness in reviews for review spammer detection. In: Proceedings of the Seventh International AAAI Conference on Weblogs and Social Media (Vol. 13, pp. 175–184).Feng, S., Banerjee, R., & Choi, Y. (2012). Syntactic stylometry for deception detection. In: Proceedings of the 50th annual meeting of the association for computational linguistics (Vol. 2: Short Papers, pp. 171–175). Jeju Island: Association for Computational Linguistics.Forman, G. (2003). An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3, 1289–1305.Fornaciari, T., & Poesio, M. (2013). Automatic deception detection in Italian court cases. Artificial intelligence and law, 21(3), 303–340. https://doi.org/10.1007/s10506-013-9140-4.Fornaciari, T., & Poesio, M. (2014). Identifying fake amazon reviews as learning from crowds. In: Proceedings of the 14th conference of the European chapter of the Association for Computational Linguistics (pp. 279–287). Gothenburg: Association for Computational Linguistics. Retrieved from http://www.aclweb.org/anthology/E14-1030.Gelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models., Analytical methods for social research Cambridge: Cambridge University Press.Graves, A., Jaitly, N., & Mohamed, A. R. (2013). Hybrid speech recognition with deep bidirectional LSTM. In: 2013 IEEE workshop on automatic speech recognition and understanding (ASRU) (pp. 273–278). New York: IEEE.Hernández-Castañeda, Á., & Calvo, H. (2017). Deceptive text detection using continuous semantic space models. Intelligent Data Analysis, 21(3), 679–695.Hernández Fusilier, D., Guzmán, R., Móntes y Gomez, M., & Rosso, P. (2013). Using pu-learning to detect deceptive opinion spam. In: Proc. of the 4th workshop on computational approaches to subjectivity, sentiment and social media analysis (pp. 38–45).Hernández Fusilier, D., Montes-y Gómez, M., Rosso, P., & Cabrera, R. G. (2015). Detecting positive and negative deceptive opinions using pu-learning. Information Processing & Management, 51(4), 433–443.Hovy, D. (2016). The enemy in your own camp: How well can we detect statistically-generated fake reviews–an adversarial study. In: The 54th annual meeting of the association for computational linguistics (p 351).Jelinek, F., Lafferty, J. D., & Mercer, R. L. (1992). Basic methods of probabilistic context free grammars. Speech recognition and understanding (pp. 345–360). New York: Springer.Jindal, N., & Liu, B. (2008). Opinion spam and analysis. In: Proceedings of the 2008 international conference on web search and data mining (pp. 219–230). New York: ACM.Karatzoglou, A., Meyer, D., & Hornik, K. (2006). Support vector machines in R. Journal of Statistical Software, 15(9), 1–28.Kim, S., Lee, S., Park, D., & Kang, J. (2017). Constructing and evaluating a novel crowdsourcing-based paraphrased opinion spam dataset. In: Proceedings of the 26th international conference on world wide web (pp. 827–836). Geneva: International World Wide Web Conferences Steering Committee.Li, F., Huang, M., Yang, Y., & Zhu, X. (2011). Learning to identify review spam. IJCAI Proceedings-International Joint Conference on Artificial Intelligence, 22(3), 2488–2493.Li, H., Chen, Z., Liu, B., Wei, X., & Shao, J. (2014a). Spotting fake reviews via collective positive-unlabeled learning. In: 2014 IEEE international conference on data mining (ICDM) (pp. 899–904). New York: IEEE.Li, H., Fei, G., Wang, S., Liu, B., Shao, W., Mukherjee, A., & Shao, J. (2017). Bimodal distribution and co-bursting in review spam detection. In: Proceedings of the 26th international conference on world wide web (pp. 1063–1072). Geneva: International World Wide Web Conferences Steering Committee.Li, H., Liu, B., Mukherjee, A., & Shao, J. (2014b). Spotting fake reviews using positive-unlabeled learning. Computación y Sistemas, 18(3), 467–475.Li, J., Ott, M., Cardie, C., & Hovy, E. H. (2014c). Towards a general rule for identifying deceptive opinion spam. In: ACL (Vol. 1, pp. 1566–1576).Lin, C. H., Hsu, P. Y., Cheng, M. S., Lei, H. T., & Hsu, M. C. (2017). Identifying deceptive review comments with rumor and lie theories. In: International conference in swarm intelligence (pp. 412–420). New York: Springer.Liu, B., Dai, Y., Li, X., Lee, W. S., & Yu, P. S. (2003). Building text classifiers using positive and unlabeled examples. In: Third IEEE international conference on data mining (pp. 179–186). New York: IEEE.Liu, B., Lee, W. S., Yu, P. S., & Li, X. (2002). Partially supervised classification of text documents. ICML, 2, 387–394.Martens, D., & Maalej, W. (2019). Towards understanding and detecting fake reviews in app stores. Empirical Software Engineering,. https://doi.org/10.1007/s10664-019-09706-9.Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:13013781.Mukherjee, A., Kumar, A., Liu, B., Wang, J., Hsu, M., Castellanos, M., & Ghosh, R. (2013a). Spotting opinion spammers using behavioral footprints. In: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 632–640) New York: ACM.Mukherjee, A., Venkataraman, V., Liu, B., & Glance, N. S. (2013b). What yelp fake review filter might be doing? In: Proceedings of the seventh international AAAI conference on weblogs and social media.Negri, M., Bentivogli, L., Mehdad, Y., Giampiccolo, D., & Marchetti, A. (2011). Divide and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora. In: Proceedings of the conference on empirical methods in natural language processing (pp. 670–679). Stroudsburg: Association for Computational Linguistics.Ott, M., Cardie, C., & Hancock, J. T. (2013). Negative deceptive opinion spam. In: Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics: human language technologies (pp. 497–501).Ott, M., Choi, Y., Cardie, C., & Hancock, J. (2011). Finding deceptive opinion spam by any stretch of the imagination. In: Proceedings of the 49th Annual meeting of the association for computational linguistics: human language technologies (pp. 309–319). Portland, Oregon: Association for Computational Linguistics.Pennebaker, J. W., Francis, M. E., & Booth, R. J. (2001). Linguistic inquiry and word count (LIWC): LIWC2001. Mahwah: Lawrence Erlbaum Associates.Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532–1543).Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H., Florin, C., Bogoni, L., et al. (2010). Learning from crowds. Journal of Machine Learning Research, 11, 1297–1322.Ren, Y., & Ji, D. (2017). Neural networks for deceptive opinion spam detection: An empirical study. Information Sciences, 385, 213–224.Rout, J. K., Dalmia, A., Choo, K. K. R., Bakshi, S., & Jena, S. K. (2017). Revisiting semi-supervised learning for online deceptive review detection. IEEE Access, 5(1), 1319–1327.Saini, M., & Sharan, A. (2017). Ensemble learning to find deceptive reviews using personality traits and reviews specific features. Journal of Digital Information Management, 12(2), 84–94.Salloum, W., Edwards, E., Ghaffarzadegan, S., Suendermann-Oeft, D., & Miller, M. (2017). Crowdsourced continuous improvement of medical speech recognition. In: The AAAI-17 workshop on crowdsourcing, deep learning, and artificial intelligence agents.Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In: Proceedings of international conference on new methods in language processing. Retrieved from http://www.ims.uni-stuttgart.de/ftp/pub/corpora/tree-tagger1.pdf.Shehnepoor, S., Salehi, M., Farahbakhsh, R., & Crespi, N. (2017). Netspam: A network-based spam detection framework for reviews in online social media. IEEE Transactions on Information Forensics and Security, 12(7), 1585–1595.Skeppstedt, M., Peldszus, A., & Stede, M. (2018). More or less controlled elicitation of argumentative text: Enlarging a microtext corpus via crowdsourcing. In: Proceedings of the 5th workshop on argument mining (pp. 155–163).Strapparava, C., & Mihalcea, R. (2009). The lie detector: Explorations in the automatic recognition of deceptive language. In: Proceedings of the 47th annual meeting of the association for computational linguistics and the 4th international joint conference on natural language processing.Streitfeld, D. (August $$25{{\rm th}}$$, 2012). The best book reviews money can buy. The New York Times.Whitehill, J., Wu, T., Bergsma, F., Movellan, J. R., & Ruvolo, P. L. (2009). Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. Advances in neural information processing systems (pp. 2035–2043). Cambridge: MIT Press.Xie, S., Wang, G., Lin, S., & Yu, P. S. (2012). Review spam detection via temporal pattern discovery. In: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (pp 823–831). New York: ACM.Yang, Y., & Liu, X. (1999). A re-examination of text categorization methods. In: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99 (pp. 42–49). New York: ACM.Zhang, W., Bu, C., Yoshida, T., & Zhang, S. (2016). Cospa: A co-training approach for spam review identification with support vector machine. Information, 7(1), 12.Zhang, W., Du, Y., Yoshida, T., & Wang, Q. (2018). DRI-RCNN: An approach to deceptive review identification using recurrent convolutional neural network. Information Processing & Management, 54(4), 576–592.Zhou, L., Shi, Y., & Zhang, D. (2008). A Statistical Language Modeling Approach to Online Deception Detection. IEEE Transactions on Knowledge and Data Engineering, 20(8), 1077–1081",Fake Opinion Detection: How Similar are Crowdsourced Datasets to Real Data?,https://riunet.upv.es/bitstream/10251/171117/1/FornaciariCagninaRosso%20-%20Fake%20Opinion%20Detection%20How%20Similar%20are%20Crowdsourced%20Datasets%20to%20Real%20Data.pdf,'Springer Science and Business Media LLC',10.1007/s10579-020-09486-5,,core
348837257,2020-11-03T00:00:00,"The real-time implementation of wave energy control leads to non-causality as the wave load that comes in the next few seconds is used to optimize the control command. The present work tackles non-causality through online forecasting of future wave force using artificial intelligence technique. The past free surface elevation is used to forecast the incoming wave load. A feedforward artificial neural network is developed for the forecasting, which learns to establish the intrinsic link between past free surface elevation and future wave force through machine learning algorithm. With the implementation of the developed online wave force prediction algorithm, a real-time discrete control algorithm taking constraint on response amplitude into account is developed and implemented to a bi-oscillator wave energy converter in the present research. The dynamic response and the wave power extraction are simulated using a state-space hydrodynamic model. It is shown that the developed real-time control algorithm enhances the power capture substantially whereas the motion of the system is hardly increased. The prediction error effect on power extraction is investigated. The reduction of power extraction is mainly caused by phase error, whilst the amplitude error has minimal influence. A link between the power capture efficiency and the constraint on control is also identified",Development of a constraint non-causal wave energy control algorithm based on artificial intelligence,,'Elsevier BV',10.1016/j.rser.2020.110519,,core
328873796,2020-07-13T08:54:45,"Context. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory. Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention. Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters. Results. The development and implementation of this method in a five-dimensional space (l, b, ϖ, μα*, μδ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",A new method for unveiling Open Clusters in Gaia: new nearby Open Clusters confirmed by DR2,https://core.ac.uk/download/328873796.pdf,'EDP Sciences',10.1051/0004-6361/201833390,"[{'title': 'Astronomy and Astrophysics', 'identifiers': ['0004-6361', 'issn:0004-6361']}]",core
333611276,2020-01-01T00:00:00,"The article presents a new concept of “Management of knowledge about virtual promotion” on the Internet. Usually a real product or service is being divided into four components (product, price, promotion and place) in accordance with the theory of marketing. One of the components is a product promotion. But now this element is becoming a fully virtual tool. It is necessary to consider product promotion as an image or a copy of a real product in a virtual space that lives in parallel on the network. Therefore, the objective of the paper is the presentation of a new object of research based on the experience of more than thirty real projects performed in Ukraine, USA, Europe and Canada. We regard the promotion as a software product, which works according to principles of knowledge management and machine learning. It is proposed that virtual promotion is characterized by four views: customer or user, data, technology and marketing. Thus, the structure of virtual promotion business process was presented. It includes four steps: selection of hypertext sources, knowledge representation and extraction, semantic kernel building and quality criterion evaluation to stop the process. Based on the process structure the research tasks were identified. The central task is semantic kernel forming. Then the software architecture was developed. IT solution contains CRM system as accounting tool and Web site as an image of virtual promotion. CRM plays main role as a commander center. Here we form semantic kernel and then send it via marketing channels such as Web site, telegram or viber accounts. Another part of IT solution is Web service such as Bing API or Google API. They help us to build the kernel. Also the paper demonstrates the list of future tasks that should be solved and the example of real project of proposed approach.У статті представлена нова технологія «управління знаннями про віртуальне просування» в мережі Інтернет. Звичайно реальний продукт або послуга характеризуються наступними компонентами: продукт, ціна, просування і місце, згідно з теорією маркетингу. Одним з компонентів є просування товару. Однак зараз цей елемент стає повністю віртуальним інструментом. Необхідно розглядати просування продукту як відображення або копію реального продукту у віртуальному просторі. Це відображення існує паралельно у мережі та безпосередньо впливає на реальний продукт чи послугу. Тому ціллю статті є презентація нового об’єкта дослідження, поява якого основана на досвіді виконання більш ніж тридцяти реальних проектів в Україні, США, Європі та Канаді. Ми працюємо відповідно до принципів управління знаннями і машинного навчання. Передбачається, що віртуальне просування характеризується чотирма репрезентаціями: клієнт або користувач, дані, технологія та маркетинг. Далі була представлена структура бізнес–процесу віртуального просування. Він включає чотири етапи: вибір джерел гіпертексту, подання та витяг знань, побудова семантичного ядра і оцінка критерію якості для зупинки процесу. На основі структури процесу були визначені задачі дослідження. Центральна задача – формування семантичного ядра. Потім була розроблена архітектура програмного забезпечення. ІТ рішення містить CRM систему в якості інструменту обліку та Веб сайт як образ віртуального просування. CRM грає роль командного центру. Тут формується семантичне ядро і потім відправляється через маркетингові канали, такі як Веб сайт, Телеграм канали або профілі в Вайбері. Інша частина ІТ рішення – це Веб сервіс, такий як Bing API або Google API. Вони допомагають нам побудувати ядро. Також в статті наведено список майбутніх завдань, які необхідно вирішити, і приклад реальних проектів в рамках запропонованого підходу",Технологія управління знаннями про віртуальне просування,https://core.ac.uk/download/333611276.pdf,'National Technical University Kharkiv Polytechnic Institute',10.20998/2079-0023.2020.01.13,,core
343462063,2020-11-04T08:00:00,"Deep neural networks (DNNs) have achieved significant success in many applications, such as computer vision, natural language processing, robots, and self-driving cars. With the growing demand for more complex real-world applications, more complicated neural networks have been proposed. However, high capacity models result in two major problems: long training times and high inference delays, making the neural networks hard to train and infeasible to deploy for time-intensive applications or resource-limited devices. In this work, we propose multiple techniques to accelerate the training and inference speed as well as model performance
The first technique we study is model parallelization on generative adversarial networks (GANs). Multiple orthogonal generators with shared memory are employed to capture the whole data distribution space. This method can not only improve the model performance but also alleviate the mode collapse problem that is common in GANs. The second technique we investigate is the automatic network pruning. To reduce the floating-point operations (FLOPs) to a proper level without compromising accuracy, we propose a better generalized and easy-to-use pruning method, which prunes the network through optimizing a set of trainable auxiliary parameters instead of original weights. Weakly coupled gradient update rules are proposed to keep consistency with pruning tasks. The third technique is to remove the redundancy of the complicated model based on the need of applications. We treat the chemical reaction prediction as a translation problem and apply a low capacity neuron translation model to this problem. The fourth technique is to combine distillation with Differentiable Architecture Search to stabilize and improve the searching procedure. Intermediate results as well as the output logits are transferred from the teacher network to the student network. For the application of the speedup technique, we introduce neural network pruning into Materials Genomics. We propose attention based AutoPrune for the kernel pruning of a continuous filtering neural network for molecular property prediction and achieves better performance and more compact size",Speedup Techniques for Deep Neural Networks,,OpenCommons@UConn,,,core
335471008,2020-06-01T00:00:00,"Situation awareness consists of ""the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future"". Being aware of the security situation is then mandatory to launch proper security reactions in response to cybersecurity attacks. Security Incident and Event Management solutions are deployed within Security Operation Centers. Some vendors propose machine learning based approaches to detect intrusions by analysing networks behaviours. But cyberattacks like Wannacry and NotPetya, which shut down hundreds of thousands of computers, demonstrated that networks monitoring and surveillance solutions remain insufficient. Detecting these complex attacks (a.k.a. Advanced Persistent Threats) requires security administrators to retain a large number of logs just in case problems are detected and involve the investigation of past security events. This approach generates massive data that have to be analysed at the right time in order to detect any accidental or caused incident. In the same time, security administrators are not yet seasoned to such a task and lack the desired skills in data science. As a consequence, a large amount of data is available and still remains unexplored which leaves number of indicators of compromise under the radar. Building on the concept of situation awareness, we developed a situation-driven framework, called dynSMAUG, for dynamic security management. This approach simplifies the security management of dynamic systems and allows the specification of security policies at a high-level of abstraction (close to security requirements). This invited paper aims at exposing real security situations elicitation, coming from networks security experts, and showing the results of exploratory analysis techniques using complex event processing techniques to identify and extract security situations from a large volume of logs. The results contributed to the extension of the dynSMAUG solution",Dynamic security management driven by situations: An Exploratory analysis of logs for the identification of security situations,https://core.ac.uk/download/335471008.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/CSNet47905.2019.9108976,,core
324197034,2020-06-15T00:00:00,"The widespread adoption of deep learning is often attributed to its automatic
feature construction with minimal inductive bias. However, in many real-world
tasks, the learned function is intended to satisfy domain-specific constraints.
We focus on monotonicity constraints, which are common and require that the
function's output increases with increasing values of specific input features.
We develop a counterexample-guided technique to provably enforce monotonicity
constraints at prediction time. Additionally, we propose a technique to use
monotonicity as an inductive bias for deep learning. It works by iteratively
incorporating monotonicity counterexamples in the learning process. Contrary to
prior work in monotonic learning, we target general ReLU neural networks and do
not further restrict the hypothesis space. We have implemented these techniques
in a tool called COMET. Experiments on real-world datasets demonstrate that our
approach achieves state-of-the-art results compared to existing monotonic
learners, and can improve the model quality compared to those that were trained
without taking monotonicity constraints into account",Counterexample-Guided Learning of Monotonic Neural Networks,http://arxiv.org/abs/2006.08852,,,,core
421203230,2020-08-03T00:00:00,"International audienceWith the rapid advancement of power electronic technologies and the reduction of photovoltaic cell price, the share of solar energy in the total power production has been booming recently. On the one hand, the increase in the amount of power delivered by solar energy can be beneficial in many economic and environmental aspects. On the other hand, this can cause various technical challenges to network operators. One of these issues is related to classifying faults located in distribution networks with high penetration of photovoltaic systems. Although many studies have paid significant attention to developing new algorithms applicable for a more active today distribution networks, there is still space for other improvements. Hence, after reviewing stateof-the-art researches, this paper was intended to develop a fault classification that is based on artificial neural networks. In particular, a technique so-called Multiplayer Perceptron Classifier was selected for the proposed algorithm. First, the authors generated a data set for the study by modeling and simulating a real distribution network with practical parameters provided by a local utility in the environment software PowerFactory/DigSILENT. Multiple fault scenarios were simulated. Second, a part of the generated data collection was used for network learning. Finally, the performance of the proposed methodology was demonstrated via testing on the remaining number of generated data",A Fault Classification Method for Medium Voltage Networks with a high Penetration of Photovoltaic Systems using Artificial Neural Networks,,HAL CCSD,,,core
289188386,2020-03-27T00:00:00,"Οι Διατάξεις Προγραμματιζόμενης Λογικής (Field Programmable Gate Arrays -
FPGAs) έχουν αυξανόμενο αντίκτυπο σε όλο και περισσότερες εφαρμογές, από τα
νευρωνικά δίκτυα (Deep Neural Networks) έως επεκτάσεις του ρεπερτορίου εντολών
(Instruction Set Extensions) σε στενώς συνεζευγμένα συστήματα με ενσωματωμένες
FPGAs (eFPGAs). Καθώς οι εφαρμογές αποκλίνουν στην πολυπλοκότητα, τις επιδόσεις,
τις ανάγκες μνήμης και τους περιορισμούς σε έκταση, υπάρχει ανάγκη για ένα ευρύτερο
φάσμα αρχιτεκτονικών FPGA. Ωστόσο, η ανάπτυξη και υλοποίηση των αρχιτεκτονικών
αυτών παραμένει δύσκολη και απαιτεί πολύ χρόνο, λόγω των υψηλών απαιτήσεών τους
σε ειδικευμένες σχεδιάσεις (custom layout) και της ανάγκης ανάπτυξης λογισμικού 
προσαρμοσμένου για τον προγραμματισμό κάθε αρχιτεκτονικής, οδηγώντας στην
παραγωγή προϊόντων πιο γενικού σκοπού.
Πολλές ακαδημαϊκές εργασίες επικεντρώνονται στην αυτοματοποιημένη
διαδικασία παραγωγής αρχιτεκτονικών FPGA, σε μια προσπάθεια να προωθηθεί η
εξατομίκευση και να μειωθεί η χρονική περίοδος μέχρι την αγορά. Σε άλλες
προσεγγίσεις, οι ερευνητές στοχεύουν στη διαδικασία εξερεύνησης, στην οποία
αναζητούν τη βέλτιστη αρχιτεκτονική για ένα συγκεκριμένο σενάριο, χρησιμοποιώντας
μοντέλα εκτιμήσεων μεγέθους και καθυστέρησης.
Στην εργασία αυτή επιλέγουμε να συνδυάσουμε τις δύο αυτές προσεγγίσεις.
Αναπτύσσουμε μια επέκταση για το δημοφιλές εργαλείο ανοιχτού κώδικα Verilog-toRouting (VTR) προκειμένου να εξάγουμε σε Verilog την αναπαράσταση των
αρχιτεκτονικών FPGA που έχουν οριστεί από τον χρήστη, να υποστηρίξουμε ζητούμενες
μονάδες ειδικού σκοπού (hard blocks - RAMs, DSPs, FP Units) και να παράξουμε αρχεία
προγραμματισμού της FPGA (Bitstreams) για δοθέντα benchmarks. Στόχος μας είναι να
δημιουργήσουμε συνθέσιμο κώδικα RTL ανεξάρτητο από τεχνολογία, ικανό να συντεθεί
με οποιαδήποτε βιβλιοθήκη Standard Cells. Ανακαλύπτουμε τις πραγματικές
σχεδιαστικές ιδιότητες μιας αρχιτεκτονικής FPGA χρησιμοποιώντας μια προτεινόμενη
ροή σχεδιασμού υλικού (ASIC flow) και ανακτούμε πραγματικές μετρήσεις μεγέθους και
καθυστέρησης και τελικά προχωρούμε στην εξερεύνηση των βέλτιστων αρχιτεκτονικών
FPGA για συγκεκριμένα σύνολα από benchmakrs.
Χρησιμοποιώντας την επέκτασή μας, εξερευνούμε το χώρο σχεδιασμού των FPGA
για ένα σύνολο από benchmakrs υψηλών επιδόσεων που εξάγονται από την πλατφόρμα
High Level Synthesis (HLS) της Xilinx. Η εξερεύνηση μας αρχίζει με τον εντοπισμό των
βέλτιστων αρχιτεκτονικών FPGA, ξεκινώντας από το μέγεθος των προγραμματιζόμενων
πυλών (Lookup Tables - LUTs) και τον αριθμό τους ανά ομάδα (Configurable Logic Block -
CLB) και έπειτα εξετάζοντας το μέγεθος και μήκος των καναλιών διασύνδεσης τους.
Συγκρίνουμε επίσης τις βέλτιστες αρχιτεκτονικές FPGA που προκύπτουν κατά τη χρήση
των benchmakrs υψηλών επιδόσεων με τις αντίστοιχες αρχιτεκτονικές που προκύπτουν
όταν χρησιμοποιούμε τα MCNC benchmarks γενικού σκοπού.
Τέλος, δημιουργούμε σύνολα εντολών TCL για τη σύνθεση και την υλοποίηση της
τοποθέτησης και διασύνδεσης (place and route) που μπορούν να προσαρμοστούν σε
οποιοδήποτε μέγεθος και χαρακτηριστικό αρχιτεκτονικής και να αυτοματοποιήσουν τη
ροή ASIC για νέα τσιπ FPGAField Programmable Gate Arrays (FPGAs) have an ever-expanding impact to more
and more applications, ranging from Deep Neural Networks to High-Performance
Computing (HPC) and other uses such as customization of Instruction Set Extensions and
computation offloading in systems with tightly coupled embedded FPGAs (eFPGAs). As
applications diverge in complexity, performance, memory needs and area limitations,
there is a need for a wider variety of FPGA architectures. However, developing and
implementing new FPGA architectures remains challenging and requires a lot of time, due
to their high content in custom layout designs and the need for design software and flows
tailored for each specific architecture, leading to the production of more generic
products.
Many academic works are focusing on the automated FPGA design generation
process, in an attempt to promote customizability and reduce time-to-market. In other
approaches, researchers target only the exploration process, in which they seek for the
optimal architecture for a specific case scenario, using area and delay estimation models.
In this thesis we choose to combine the two approaches. We develop an extension
for the popular open-source tool Verilog-to-Routing (VTR) in order to export in Verilog
the representation of user-specified FPGA architectures, develop support for custom user 
hard-blocks (RAMs, DSPs, FP Units), and generate Bitstreams for given benchmarks. Our
objective is to create synthesizable and technology independent RTL design code, able to
be synthesized with any standard cell library. We discover the real design properties of
an FPGA architecture using our proposed ASIC flow and retrieve real area and delay
measurements and eventually proceed with the exploration of optimal FPGA
architectures for given sets of benchmarks.
Using our VTR extension, we perform FPGA design-space exploration for a set of
HPC oriented benchmarks that are derived using Xilinx's High Level Synthesis (HLS). Our
exploration starts by identifying pareto-optimal FPGA architectures starting with the size
of Lookup Tables (LUTs) and the number of LUTs per Configurable Logic Block (CLB) and
then explore the size of routing channels and wire segments' configurations. We also
compare the optimal FPGA architectures derived when using the HPC benchmarks with
the respective architectures derived when we use the generic MCNC benchmarks.
Finally, we create TCL scripts for synthesis and back-end implementation (place
and route) which can adjust to any architectural characteristic and size and automate the
ASIC flow for new FPGA chips",Εξερεύνηση αρχιτεκτονικών προγραμματιζόμενης λογικής για αποδοτική επιτάχυνση εφαρμογών υψηλών επιδόσεων,,,,,core
357234808,2020-03-05T00:00:00,"Abstract * MiTAP (MITRE Text and Audio Processing) is a prototype system available for monitoring infectious disease outbreaks and other global events. MiTAP focuses on providing timely, multi-lingual, global information access to medical experts and individuals involved in humanitarian assistance and relief work. Multiple information sources in multiple languages are automatically captured, filtered, translated, summarized, and categorized by disease, region, information source, person, and organization. Critical information is automatically extracted and tagged to facilitate browsing, searching, and sorting. The system supports shared situational awareness through collaboration, allowing users to submit other articles for processing, annotate existing documents, post directly to the system, and flag messages for others to see. MiTAP currently stores over one million articles and processes an additional 2000 to 10,000 daily, delivering up-to-date information to dozens of regular users. Global Tracking of Infectious Disease Outbreaks and Emerging Biological Threats Over the years, greatly expanded trade and travel have increased the potential economic and political impacts of major disease outbreaks, given their ability to move rapidly across national borders. These diseases can affect people (West Nile virus, HIV, Ebola, Bovine Spongiform Encephalitis), animals (foot-and-mouth disease) and plants (citrus canker in Florida). More recently, the potential of biological terrorism has become a very real threat. On September 11 th , 2001, the Center for Disease Control alerted states and local public health agencies to monitor for any unusual disease patterns, including the effects of chemical and biological agents. In addition to possible disruption and loss of life, bioterrorism could foment political instability, given the panic that fast-moving plagues have historically engendered. Appropriate response to disease outbreaks and emerging threats depends on obtaining reliable and up-to-date Approved for Public Release: Distribution Unlimited. To be published in AI Magazine, special edition highlighting best work from  information, which often means monitoring many news sources, particularly local news sources, in many languages worldwide. Analysts cannot feasibly acquire, manage, and digest the vast amount of information available 24 hours a day, seven days a week. In addition, access to foreign language documents and the local news of other countries is generally limited. Even when foreign language news is available, it is usually no longer current by the time it is translated and reaches the hands of an analyst. This is a very real problem that raises an urgent need to develop automated support for global tracking of infectious disease outbreaks and emerging biological threats. The MiTAP (MITRE Text and Audio Processing) system was created to explore the integration of synergistic TIDES language processing technologies: Translation, Information Detection, Extraction, and Summarization. TIDES aims to revolutionize the way that information is obtained from human language by enabling people to find and interpret needed information quickly and effectively, regardless of language or medium. MiTAP is designed to provide the end user with timely, accurate, novel information and present it in a way that allows the analyst to spend more time on analysis and less time on finding, translating, distilling and presenting information. On September 11 th , 2001, the research prototype system became available to real users for real problems.  Text and Audio Processing for Bio-Security MiTAP focuses on providing timely, multi-lingual, global information access to analysts, medical experts and individuals involved in humanitarian assistance and relief work. Multiple information sources (epidemiological reports, newswire feeds, email, online news) in multiple languages (English, Chinese, French, German, Italian, Portuguese, Russian, and Spanish) are automatically captured, filtered, translated, summarized, and categorized into searchable newsgroups based on disease, region, information source, person, organization, and language. Critical information is automatically extracted and tagged to facilitate browsing, searching, and sorting. The system supports shared situational awareness through collaboration, allowing users to submit other articles for processing, annotate existing documents, and post directly to the system. A web-based search engine supports sourcespecific, full-text information retrieval. Additional &quot;views&quot; into the data facilitate analysis and can serve as alerts to events, such as disease outbreaks.  Information Processing Each normalized message is passed through a zoner that uses human-generated rules to identify the source, date, and other fields such as headline or title, article body, etc. The zoned messages are preprocessed to identify paragraph, sentence, and word boundaries as well as partof-speech tags. This preprocessing is carried out by the Alembic natural language analyzer  User Interface The final phase consists of the user interface and related processing. The processed messages are converted to HTML, with color-coded named entities, and routed to newsgroups hosted by a Network News Transport Protocol (NNTP) server, InterNetNews (INN 2001). See  One major advantage to using the NNTP server is that users can access the information using a standard mail/news browser such as Netscape Messenger or Outlook Express. There is no need to install custom software, and the instant sense of familiarity with the interface is crucial in gaining user acceptance -little to no training is required. Mail readers also provide additional functionality such as alerting to new messages on specified topics, flagging messages of significance, and access to local directories that can be used as a private workspace. Other newsgroups can be created as collaborative repositories for users to share collective information.  To supplement access to the data, messages are indexed using the Lucene information retrieval system (The Jakarta Project 2001), allowing users to do full text, sourcespecific Boolean queries over the entire set of messages. As the relevance of messages tends to be time dependent, we have implemented an optimized query mechanism to do faster searches over time intervals. MiTAP Development and Deployment The initial MiTAP system was put together over a 9-month period. Our goal was to build a prototype quickly to demonstrate the results of integrating multiple natural language processing (NLP) technologies. The longer-term strategy is to upgrade the components progressively as better performing modules become available and to migrate towards our developing architecture. For the initial implementation, we chose components based on availability as well as ease of integration and modification. This meant that we used components developed at MITRE (extraction, summarization) or developed with MITRE involvement (translation support), or commercial off-theshelf (COTS) components (translation engines, information retrieval, news server, news browser interface). In cases where no component was readily available, we developed a minimal capability for MiTAP, e.g., scripts for capture of news sources, or use of named entity extraction for headline generation and binning of messages into appropriate newsgroups. Since July 2000, we have been working to incorporate modules from other groups (e.g., Columbia&apos;s Newsblaster,  As part of the long-term efforts, we have been concurrently developing a framework known as Catalyst (Mardis and Burger 2001). Catalyst provides a common data model based on standoff annotation, efficient compressed data formats, distributed processing, and annotation indexing. Standoff annotation (see, for example,  Uses of AI Technology Artificial Intelligence (AI) technology and techniques pervade MiTAP to support its multi-faceted, multi-lingual and multi-functional requirements. From automated natural language processing techniques to information retrieval, the NLP modules utilize AI extensively. The techniques utilized fall predominantly into the data-driven camp of methods. Below we describe the components, roughly in their order of processing flow. The CyberTrans machine translation server employs a combination of AI techniques to optimize the performance of COTS machine translation (MT) systems. Since system developers have only the most basic insight into the MT systems, we will not describe related AI techniques in depth here, and interested readers are referred to  COTS MT systems are designed primarily for interactive use in situations where users have control over the language, formatting and well-formedness of the input text. In adapting CyberTrans for real users and real-world data, the necessity for supporting technologies was quickly apparent. Three of these are of particular interest: automated language identification, automated code set conversion, and automated spelling correction, particularly for the incorporation of diacritics. The resulting tools can be used individually and eventually as standalone modules, but are currently integrated into the CyberTrans processing flow. The first, most essential, part of automated processing of language data is to determine both the language and code set representation of the input text. While it would seem obvious that users know at least what the language of a given document is, this has proven not to be the case, particularly in non-Romanized languages such as Arabic or Chinese. In these situations, documents appear as unintelligible byte streams. In addition, some of the data sources contain documents in a mix of languages, so knowledge of the source does not necessarily determine the language. This is a classical categorization problem with a search a space of N*M where N is the number of languages to be recognized and M the number of code set representations. The categories are determined by a combination of n-graph measurements using the Acquaintance algorithm (Huffman 1996) with simple heuristics whittling down the search space. Once the code set has been determined, it is converted into a standard representation. This process is not without information loss, so spelling corrections are applied. The most straight-forward spelling correction involves the reinsertion of diacritical markers where they are missing. This is treated as a word-sense disambiguation problem (Yarowsky 1994) and relies on both language spelling rules and trained probabilities of word occurrences. Here, the solution is a hybrid system where hand-coded rules are enforced using statistical measures of likely word occurrences.  In addition, a specialized tagging operation occurs, that of temporal resolution. While dates such as 09 September 2000 are relatively unambiguous, many time references found in natural language are not, for instance last Tuesday. To get the time sequencing of events of multiple stories correct, it is necessary to resolve the possible wide range of time references accurately. In this case, the resolution algorithm also combines basic linguistic knowledge with rules learned from corpora  Similarly, place names are often only partially specified. For example, there are a great many places in South America named La Esperanza. We are currently developing a module to apply a mix of hand-written rules and machine learning to metadata and contextual clues drawn from a large corpus to disambiguate place names. This range of tagging procedures represents a strong shift in natural language processing research over the past fifteen years towards &quot;corpus-based&quot; methods. This work begins with the manual annotation of a corpus, a set of naturally occurring linguistic artifacts, by which some level of linguistic analysis (word segmentation, part-ofspeech, semantic referent, syntactic phrase, etc.) is associated with the relevant portion of text. The resulting data provides a rich basis for empirically-driven research and development, as well as formal evaluations of systems attempting to re-create this analysis automatically. The availability of such corpora have spurred a significant interest in machine learning and statistical methods in natural language processing research, of which those mentioned above are just a few. One of the benefits of the rule-sequence model adopted in MiTAP&apos;s Alembic component is its support for easily and effectively combining automatically derived heuristics with those developed manually. This was a key element in successfully modifying the Alembic NLP system for MiTAP in the absence of any significant annotated corpus",MiTAP for Bio-Security: A Case Study Global Tracking of Infectious Disease Outbreaks and Emerging Biological Threats,,,,,core
357322010,2020-04-03T00:00:00,"ABSTRACT To speed up the convergence of reinforcement learning (RL) algorithms by more efficient use of computer simulations, three algorithmic techniques are proposed: Time Manipulation, Time Hopping, and Eligibility Propagation. They are evaluated on various robot control tasks. The proposed Time Manipulation [1] is a concept of manipulating the time inside a simulation and using it as a tool to speed up the learning process. It is applicable to a subset of RL problems whose goal is to learn a control policy to avoid failure events. Time Manipulation works by turning back the time of the simulation on failure events, thus avoiding redundant state transitions and exploring deeper the state space. This is impossible to be done in the real world, but it can easily be done in a simulation. In order to evaluate the proposed algorithm, experiments on a classical control benchmark problem are conducted: an inverted pendulum balancing robot task. The aim of the RL algorithm is to find a control policy which can prevent the pendulum from falling by moving the robot left or right, without hitting the edges of the given track. The experimental results show that Time Manipulation speeds up the learning process by 260%. It also improves the state space exploration by 12%, because it allows the RL algorithm to explore better the state space in proximity of failure states. The proposed Time Hopping [2] is a generalization of Time Manipulation, able to make arbitrary &quot;hops&quot; between states and this way traverse rapidly throughout the entire state space. Time Hopping extends the applicability of time manipulations to include not only failure-avoidance problems, but also continuous optimization problems, by creating new mechanisms to trigger the time manipulation events, to make prediction about the possible future rewards, and to select promising time hopping targets. The proposed implementation of the Time Hopping technique consists of 3 components: Hopping trigger (decides when the hopping starts), Target selection (decides where does it hop to), and Hopping (performs the actual hopping). For the implementation of the Hopping trigger component, a Gamma pruning technique is proposed, which detects and prunes unpromising exploratory paths. For the Target selection component, a Best Lasso Target Selection technique is proposed, which selects a target among the proximity of the current best policy. The evaluation of Time Hopping is performed on a biped crawling robot task. The crawling robot has 2 limbs, each with 2 segments, for a total of 4 degrees of freedom (DOF), 80 possible actions at each time step, and 13689 possible robot states. The goal of the learning process is to find a crawling motion with the maximum speed. The reward function for this task is defined as the horizontal displacement of the robot after every action. The experimental results show that Time Hopping accelerates the learning process more than 7 times. A very strong point of Time Hopping is that it is completely transparent for the RL algorithm, which offers various opportunities for combining Time Hopping with other approaches for speeding up the learning process. In addition, it can also be used as a tool for re-shaping the state probability distribution as desired  The proposed Eligibility Propagation [4] is a mechanism to further speed up Time Hopping. It provides similar abilities to what eligibility traces provide for conventional RL. Eligibility traces are one of the basic mechanisms for temporal credit assignment in reinforcement learning. An eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. Eligibility Propagation uses the transitions graph to obtain all predecessor states of an updated state. Regardless of the actual order in which Time Hopping visits the states, this oriented graph contains a record of the correct chronological sequence of state transitions. Once this oriented graph is available, it is used to propagate state value updates in the opposite direction of the state transition edges, thus making the propagation flow logically backwards in time. The evaluation of Eligibility Propagation is performed on the same biped crawling robot task as for Time Hopping. The results show that Time Hopping with Eligibility Propagation achieves 99% of the maximum possible speed almost 3 times faster than Time Hopping alone, and more than 4 times faster than conventional Q-learning. This significant speedup of the learning process is achieved despite the additional computational overhead of maintaining the transitions graph. The reason for this is the improved Gamma-pruning based on more precise future reward predictions. All experiments are conducted on a custom developed Javabased software application system, and a custom developed 2D robot physics simulation engine. The significant speed-ups achieved by the proposed algorithms make them very suitable for a wide range of robot control problems, where reducing the computational cost is importan",Time Hopping Technique for Reinforcement Learning and its Application to Robot Control,,,,,core
323983835,2020-07-04T00:00:00,"Deep Reinforcement Learning (RL) is a promising approach for adaptive robot
control, but its current application to robotics is currently hindered by high
sample requirements. To alleviate this issue, we propose to exploit the
symmetries present in robotic tasks. Intuitively, symmetries from observed
trajectories define transformations that leave the space of feasible RL
trajectories invariant and can be used to generate new feasible trajectories,
which could be used for training. Based on this data augmentation idea, we
formulate a general framework, called Invariant Transform Experience Replay
that we present with two techniques: (i) Kaleidoscope Experience Replay
exploits reflectional symmetries and (ii) Goal-augmented Experience Replay
which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI
Gym, our experimental results show significant increases in learning rates and
success rates. Particularly, we attain a 13, 3, and 5 times speedup in the
pushing, sliding, and pick-and-place tasks respectively in the multi-goal
setting. Performance gains are also observed in similar tasks with obstacles
and we successfully deployed a trained policy on a real Baxter robot. Our work
demonstrates that invariant transformations on RL trajectories are a promising
methodology to speed up learning in deep RL.Comment: 8 pages, 11 figures, additional 3 pages for appendix. IEEE Robotics
  and Automation Letters (RAL), 2020. Also in: Intelligent Robots and Systems
  (IROS","Invariant Transform Experience Replay: Data Augmentation for Deep
  Reinforcement Learning",http://arxiv.org/abs/1909.10707,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/LRA.2020.3013937,,core
346536233,2020-01-01T00:00:00,"Orientadora: Silvia Regina VergilioCoorientador: Marouane KessentiniTese (doutorado) - Universidade Federal do Paraná, Setor de Ciências Exatas, Programa de Pós-Graduação em Informática. Defesa : Curitiba, 24/04/2020Inclui referências: p. 61-66Área de concentração: Ciência da ComputaçãoResumo: A atividade de refatoração tem como principal objetivo aplicar um conjunto de transformações em um artefato de software para melhorar sua estrutura sem alterar sua funcionalidade. Alguns estudos recentes, apresentam bons resultados ao gerarem modelos de predição de refatorações. Além disso, os estudos mostram que refatorações similares são aplicadas em diferentes contextos e podem ser aprendidas. Neste sentido, a maioria dos trabalhos existentes utiliza técnicas de aprendizado de máquina para gerar modelos que predizem se um dado trecho de código deve ser refatorado. Entretanto, essas abordagens possuem limitações. Elas buscam por refatorações específicas e exatamente como aplicadas por desenvolvedores, o que limita que outras refatorações sejam encontradas. Dada a natureza subjetiva da atividade de refatoração de software, a exploração por refatorações com base em outros critérios também é vantajosa. Existem trabalhos na área conhecida como Refatoração de Software Baseada em Busca (SBR) (do inglês, Search Based Software Refactoring), em que algoritmos de busca são utilizados para encontrar refatorações em um grande espaço de busca e visando a melhorar diversos aspectos. Recentemente, trabalhos em SBR começaram a utilizar exemplos de refatorações já aplicadas por desenvolvedores para incorporar aprendizado na busca. Entretanto, essas abordagens são limitadas em termos de generalização dos resultados, uma vez que não geram um modelo que possa ser utilizado para diferentes programas. Desse modo, abordagens existentes de SBR devem ser configuradas e executadas a cada novo programa. Neste contexto, este trabalho visa a incorporar os benefícios encontrados na área de aprendizado de máquina e na área de SBR, apresentando uma abordagem chamada Gorgeous (do inglês, Generation of Refactoring Algorithms through Grammatical Evolution). Gorgeous tem como objetivo gerar algoritmos de refatoração compostos por regras, que quando executados, determinam trechos de código que devem ser refatorados e refatorações a serem aplicadas. Os algoritmos são criados de forma que as refatorações sugeridas sejam similares a refatorações aplicadas na prática e que também melhorem a qualidade do software. Os algoritmos são criados utilizando um processo de aprendizado que primeiro extrai padrões de refatoração de programas agrupando elementos que foram refatorados de maneira similar. Após isso, uma evolução gramatical é executada para gerar algoritmos de refatoração com base nos padrões extraídos. Gorgeous é avaliada utilizando dados de refatoração extraídos de 40 programas Java do repositório GitHub. Como resultado, os algoritmos gerados foram capazes de obter bons resultados para diferentes programas, melhorando em média 60% a qualidade do programa e obtendo 50% de similaridade com refatorações aplicadas na prática. Palavras-chave: Refatoração, Engenharia de Software Baseada em Busca,Agrupamento, Evolução GramaticalAbstract: The refactoring activity addresses the application of a set of transformations in software artifacts to improve their structure while preserving their functionality. Recent studies present promising results generating prediction models for refactoring. Furthermore, they provide evidences that similar refactoring operations are applied in different contexts and they can be learned using Machine Learning (ML). Most works on ML based refactoring generate models to predict if a piece of code should be refactored. Despite the capability of prediction, existing works are limited to learn specific refactoring operations as applied by developers. However, to explore refactoring operations possibilities based on other criteria is also beneficial, mainly by the subjective context of refactoring. In this context, the Search-Based Software Refactoring (SBR) area addresses studies using search algorithms to find refactoring operations in a huge search space, aiming at improving several other aspects. However, existing SBR approaches do not support generalization of results since they do not generate a model as ML studies. In this way, a SBR approach needs to be configured and executed for each program in need of refactoring. In this context, this work introduces a SBR learning approach aiming at taking most advantage of both fields. Gorgeous (Generation of Refactoring Algorithms through Grammatical Evolution) generates refactoring algorithms composed by several rules determining pieces of code that should be refactored and the refactoring types to be used. A refactoring algorithm provides as solution a set of refactoring operations to be applied in a program. In this respect, the algorithm is generated with the goal of increasing similarity of the refactoring operations with the ones applied in practice, and also improving program quality. To do this, a learning process first extracts refactoring patterns from programs by grouping their elements that were refactored in similar ways. After that, a Grammatical Evolution (GE) is executed to generate the algorithms based on the extracted patterns. Gorgeous is evaluated using refactoring data from 40 Java programs of GitHub repository. The refactoring algorithms are capable of obtaining good results to different programs, obtaining around 60% of program quality improvement and 50% of similarity with real refactoring applications. Keywords: Refactoring, Search-Based Software Engineering, Clustering, Grammatical Evolutio",Generation of refactoring algorithms through grammatical evolution,https://core.ac.uk/download/346536233.pdf,,,,core
288433288,2020-03-24T19:16:42,"Garbage collectors are nearly ubiquitous in modern programming languages, and we want to minimize the cost they impose in terms of time and space. Generally, a collector waits until its space is full and then performs a collection to reclaim needed memory. However, this is not the only option; a collection could be performed early when some free space remains. For copying collectors, which are what we consider here, the system must traverse the graph of live objects and copy them, so the cost of a collection is proportional to the volume of objects that are live. Since this value fluctuates during a program\u27s execution, a collector can minimize its cost by carefully choosing the points at which it collects.
We help to realize this goal in two ways. First, by developing an algorithm that analyzes after-the-fact traces and computes optimal collection points, we can explore the theoretical limits of garbage collectors. This gives insight into what performance gains are possible, and can guide future collector development into areas that could be most fruitful.
Second, we use techniques from machine learning to find improved garbage collection policies that could be implemented in real systems. The optimal collection points provide ground truth from which a model can learn",Optimization and Training of Generational Garbage Collectors,https://core.ac.uk/download/288433288.pdf,ScholarWorks@UMass Amherst,,,core
323104269,"March 24, 2020","The modern National Airspace System (NAS) is an extremely safe system and the aviation industry has experienced a steady decrease in fatalities over the years. This can be attributed to both improved flight critical systems with redundant hardware and software protections, as well as an increased focus on active monitoring and response to real time and historically identified vulnerabilities by implementing more resilient procedures and protocols. The main approach for identifying vulnerabilities in operations leverages domain expertise using knowledge about how the system should behave within the expected tolerances to known safety margins. This approach works well when the system has a well-defined operating condition. However, the operations in the NAS can be highly complex with various nuances that render it difficult to clearly pre-define all known safety vulnerabilities. With the advancement of data science and machine learning techniques, the potential to automatically identify emerging vulnerabilities in the observed operations has become more practical in recent years. The state-of-the-art anomaly detection approaches in aerospace data usually rely on supervised or semi-supervised learning. However, in many real-world problems such as flight safety, creating labels for the data requires huge amount of effort and is largely impractical. To address this challenge, we developed a Convolutional Variational Auto-Encoder (CVAE), which is an unsupervised learning approach for anomaly detection in high-dimensional heterogeneous time-series data. We validate performance of CVAE compared to the state-of-the-art supervised learning approach as well as unsupervised clustering-based approach using KMeans++ and kernel-based approach using One-Class Support Vector Machine (OC-SVM) on Yahoo!'s benchmark time series anomaly detection data. Finally, we showcase performance of CVAE on a case study of identifying anomalies in the first 60 seconds of commercial flights' take-offs using Flight Operational Quality Assurance (FOQA) data",Unsupervised Anomaly Detection in High-Dimensional Flight Data Using Convolutional Variational Auto-Encoder,https://core.ac.uk/download/pdf/323104269.pdf,,,,core
327064411,2020-07-20T00:00:00,"In this document, we report our proposal for modeling the risk of possible
contagiousity in a given area monitored by RGB cameras where people freely move
and interact. Our system, called Inter-Homines, evaluates in real-time the
contagion risk in a monitored area by analyzing video streams: it is able to
locate people in 3D space, calculate interpersonal distances and predict risk
levels by building dynamic maps of the monitored area. Inter-Homines works both
indoor and outdoor, in public and private crowded areas. The software is
applicable to already installed cameras or low-cost cameras on industrial PCs,
equipped with an additional embedded edge-AI system for temporary measurements.
From the AI-side, we exploit a robust pipeline for real-time people detection
and localization in the ground plane by homographic transformation based on
state-of-the-art computer vision algorithms; it is a combination of a people
detector and a pose estimator. From the risk modeling side, we propose a
parametric model for a spatio-temporal dynamic risk estimation, that, validated
by epidemiologists, could be useful for safety monitoring the acceptance of
social distancing prevention measures by predicting the risk level of the
scene",Inter-Homines: Distance-Based Risk Estimation for Human Safety,http://arxiv.org/abs/2007.10243,,,,core
328023009,2020-08-12T00:00:00,"Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space","Improving the Performance of Fine-Grain Image Classifiers via Generative
  Data Augmentation",http://arxiv.org/abs/2008.05381,,,,core
328195607,2020-08-16T00:00:00,"Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality","Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World
  Performance?",http://arxiv.org/abs/1912.06321,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/LRA.2020.3013848,,core
323323527,2020-06-02T00:00:00,"The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of
two 0.5m Schmidt telescopes with cameras covering 29 square degrees at plate
scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely
survey the whole sky visible from Hawaii (above $\delta > -50^{\circ}$) every
two nights, exposing four times per night, typically reaching $o < 19$
magnitude per exposure when the moon is illuminated and $c < 19.5$ per exposure
in dark skies. Construction is underway of two further units to be sited in
Chile and South Africa which will result in an all-sky daily cadence from 2021.
Initially designed for detecting potentially hazardous near earth objects, the
ATLAS data enable a range of astrophysical time domain science. To extract
transients from the data stream requires a computing system to process the
data, assimilate detections in time and space and associate them with known
astrophysical sources. Here we describe the hardware and software
infrastructure to produce a stream of clean, real, astrophysical transients in
real time. This involves machine learning and boosted decision tree algorithms
to identify extragalactic and Galactic transients. Typically we detect 10-15
supernova candidates per night which we immediately announce publicly. The
ATLAS discoveries not only enable rapid follow-up of interesting sources but
will provide complete statistical samples within the local volume of 100 Mpc. A
simple comparison of the detected supernova rate within 100 Mpc, with no
corrections for completeness, is already significantly higher (factor 1.5 to 2)
than the current accepted rates.Comment: 27 pages, 12 figures. Accepted for publication in PASP on 2020 May 1",Design and operation of the ATLAS Transient Science Server,http://arxiv.org/abs/2003.09052,'IOP Publishing',10.1088/1538-3873/ab936e,,core
395411988,2020-01-01T00:00:00,"Orientador: Prof. Dr. Hertz Wendel de CamargoDissertação (mestrado) - Universidade Federal do Paraná, Setor de Artes, Comunicação e Design, Programa de Pós-Graduação em Comunicação. Defesa : Curitiba, 29/04/2020Inclui referências: p. 114-117Resumo: A atividade de refatoração tem como principal objetivo aplicar um conjunto de transformações em um artefato de software para melhorar sua estrutura sem alterar sua funcionalidade. Alguns estudos recentes, apresentam bons resultados ao gerarem modelos de predição de refatorações. Além disso, os estudos mostram que refatorações similares são aplicadas em diferentes contextos e podem ser aprendidas. N este sentido, a m aioria dos trabalhos existentes utiliza técnicas de aprendizado de m áquina para gerar modelos que predizem se um dado trecho de código deve ser refatorado. Entretanto, essas abordagens possuem limitações. Elas buscam por refatorações específicas e exatam ente com o aplicadas por desenvolvedores, o que lim ita que outras refatorações sejam encontradas. D ada a natureza subjetiva da atividade de refatoração de software, a exploração por refatorações com base em outros critérios tam bém é vantajosa. Existem trabalhos na área conhecida como Refatoração de Software Baseada em Busca (SBR) (do inglês, Search Based Software Refactoring), em que algoritmos de busca são utilizados para encontrar refatorações em um grande espaço de busca e visando a m elhorar diversos aspectos. Recentem ente, trabalhos em SBR com eçaram a utilizar exemplos de refatorações já aplicadas por desenvolvedores para incorporar aprendizado na busca. Entretanto, essas abordagens são limitadas em term os de generalização dos resultados, um a vez que não geram um m odelo que possa ser utilizado para diferentes program as. D esse modo, abordagens existentes de SBR devem ser configuradas e executadas a cada novo programa. N este contexto, este trabalho visa a incorporar os benefícios encontrados na área de aprendizado de m áquina e na área de SBR, apresentando um a abordagem cham ada Gorgeous (do inglês, Generation o f Refactoring Algorithms through Grammatical Evolution). Gorgeous tem como objetivo gerar algoritmos de refatoração compostos por regras, que quando executados, determ inam trechos de código que devem ser refatorados e refatorações a serem aplicadas. Os algoritmos são criados de forma que as refatorações sugeridas sejam similares a refatorações aplicadas na prática e que também melhorem a qualidade do software. Os algoritmos são criados utilizando um processo de aprendizado que primeiro extrai padrões de refatoração de programas agrupando elementos que foram refatorados de m aneira similar. A pós isso, um a evolução gram atical é executada para gerar algoritm os de refatoração com base nos padrões extraídos. Gorgeous é avaliada utilizando dados de refatoração extraídos de 40 programas Java do repositório GitHub. Como resultado, os algoritmos gerados foram capazes de obter bons resultados para diferentes programas, melhorando em média 60% a qualidade do programa e obtendo 50% de similaridade com refatorações aplicadas na prática. Palavras-chave: Refatoração, Engenharia de Software Baseada em Busca, Agrupamento, Evolução GramaticalAbstract: The refactoring activity addresses the application of a set of transformations in software artifacts to improve their structure while preserving their functionality. Recent studies present prom ising results generating prediction models for refactoring. Furtherm ore, they provide evidences that similar refactoring operations are applied in different contexts and they can be learned using M achine Learning (ML). M ost works on M L based refactoring generate models to predict if a piece of code should be refactored. Despite the capability of prediction, existing works are limited to learn specific refactoring operations as applied by developers. However, to explore refactoring operations possibilities based on other criteria is also beneficial, mainly by the subjective context of refactoring. In this context, the Search-Based Softw are Refactoring (SBR) area addresses studies using search algorithm s to find refactoring operations in a huge search space, aiming at im proving several other aspects. However, existing SBR approaches do not support generalization of results since they do not generate a model as M L studies. In this way, a SBR approach needs to be configured and executed for each program in need of refactoring. In this context, this work introduces a SBR learning approach aiming at taking most advantage of both fields. Gorgeous (Generation of Refactoring Algorithms through Grammatical Evolution) generates refactoring algorithm s com posed by several rules determ ining pieces of code that should be refactored and the refactoring types to be used. A refactoring algorithm provides as solution a set of refactoring operations to be applied in a program. In this respect, the algorithm is generated w ith the goal of increasing sim ilarity of the refactoring operations w ith the ones applied in practice, and also im proving program quality. To do this, a learning process first extracts refactoring patterns from program s by grouping their elements that were refactored in similar ways. After that, a Grammatical Evolution (GE) is executed to generate the algorithms based on the extracted patterns. Gorgeous is evaluated using refactoring data from 40 Java programs of GitHub repository. The refactoring algorithms are capable of obtaining good results to different programs, obtaining around 60% of program quality improvement and 50% of similarity with real refactoring applications. Keywords: Refactoring, Search-Based Software Engineering, Clustering, Grammatical Evolutio",Jogabilidade da Caixa Preta : rituais iconofágicos em Bloodborne,,,,,core
237215238,2020-01-01T00:00:00,"Parking vehicle is a daunting task and a common problem in many cities around the globe. The search for parking space leads to congestion, frustration and increased air pollution. Information of a vacant parking space would facilitate to reduce congestion and subsequent air pollution. Therefore, aim of the paper is to acquire vehicle occupancy in an open parking lot using deep learning. Thermal camera was used to collect the data during varying environmental conditions such as; sunny, dusk, dawn, dark and snowy conditions. Vehicle detection with deep learning was implemented where image classification and object localization were performed for multi object detection. The dataset consists of 527 images which were manually labelled as there were no pre-labelled thermal images available. Multiple deep learning networks such as Yolo, ReNet18, ResNet50 and GoogleNet with varying layers and architectures were evaluated on vehicle detection. Yolo, GoogleNet and ResNet18 are computationally efficient detectors which took less processing time while Resnet50 produced better detection results compared to other detectors. However, ResNet18 also produced minimal miss rates and is suitable for real time vehicle detection. The detected results were compared with a template of parking spaces and IoU value is used to identify vehicle occupancy information",Deep learning-based vehicle occupancy detection in an open parking lot using thermal camera,,'Institution of Engineering and Technology (IET)',10.1049/iet-its.2019.0468,,core
337302810,2020-10-27T00:00:00,"Single-shot images are the standard readout of experiments with ultracold
atoms -- the tarnished looking glass into their many-body physics. The
efficient extraction of observables from single-shot images is thus crucial.
Here, we demonstrate how artificial neural networks can optimize this
extraction. In contrast to standard averaging approaches, machine learning
allows both one- and two-particle densities to be accurately obtained from a
drastically reduced number of single-shot images. Quantum fluctuations and
correlations are directly harnessed to obtain physical observables for bosons
in a tilted double-well potential at an unprecedented accuracy. Strikingly,
machine learning also enables a reliable extraction of momentum-space
observables from real-space single-shot images and vice versa. This obviates
the need for a reconfiguration of the experimental setup between in-situ and
time-of-flight imaging, thus potentially granting an outstanding reduction in
resources.Comment: 7+8 pages, 3+8 figures, software available at http://ultracold.or","Optimized Observable Readout from Single-shot Images of Ultracold Atoms
  via Machine Learning",http://arxiv.org/abs/2010.14510,'American Physical Society (APS)',10.1103/PhysRevA.104.L041301,,core
387846376,2020-10-25T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or
insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and
analysis in order to improve the process representation. This paper presents the development and implementation of quality
monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the
strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework
in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality
variable, such as surface roughness. Firstly, the Hybrid Incremental Modelling strategy is applied. Secondly, a Generalized Fuzzy
Clustering C-Means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality
indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The
manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing is
performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively
fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to
manufacturing industryThis work was partially supported by the project
Power2Power: Providing next-generation silicon-based
power solutions in transport and machinery for significant
decarbonisation in the next decade, funded by the Electronic
Component Systems for European Leadership (ECSEL-JU)
Joint Undertaking and the Spanish Ministry of Science,
Innovation and Universities (MICINN), under grant
agreement No 826417. In addition, this work was also funded
by the Polish National Agency for Academic Exchange
(NAWA) through the project: “Industry 4.0 in Production and
Aeronautical Engineering (IPAE)”.Peer reviewe",Quality Monitoring of Complex Manufacturing Systems on the basis of Model Driven Approach,,'Techno-Press',10.12989/sss.2020.26.4.495,"[{'title': 'Smart Structures and Systems', 'identifiers': ['1738-1584', '1738-1991', 'issn:1738-1584', 'issn:1738-1991']}]",core
344752519,2020-08-17T07:00:00,"Supervised classification methods often assume the train and test data distributions are the same and that all classes in the test set are present in the training set. However, deployed classifiers require the ability to recognize inputs from outside the training set as unknowns and update representations in near real-time to account for novel concepts unknown during offline training. This problem has been studied under multiple paradigms including out-of-distribution detection and open set recognition; however, for convolutional neural networks, there have been two major approaches: 1) inference methods to separate known inputs from unknown inputs and 2) feature space regularization strategies to improve model robustness to novel inputs. In this dissertation, we explore the relationship between the two approaches and directly compare performance on large-scale datasets that have more than a few dozen categories. Using the ImageNet large-scale classification dataset, we identify novel combinations of regularization and specialized inference methods that perform best across multiple open set classification problems of increasing difficulty level. We find that input perturbation and temperature scaling yield significantly better performance on large-scale datasets than other inference methods tested, regardless of the feature space regularization strategy. Conversely, we also find that improving performance with advanced regularization schemes during training yields better performance when baseline inference techniques are used; however, this often requires supplementing the training data with additional background samples which is difficult in large-scale problems.
To overcome this problem we further propose a simple regularization technique that can be easily applied to existing convolutional neural network architectures that improves open set robustness without the requirement for a background dataset. Our novel method achieves state-of-the-art results on open set classification baselines and easily scales to large-scale problems.
Finally, we explore the intersection of open set and continual learning to establish baselines for the first time for novelty detection while learning from online data streams. To accomplish this we establish a novel dataset created for evaluating image open set classification capabilities of streaming learning algorithms. Finally, using our new baselines we draw conclusions as to what the most computationally efficient means of detecting novelty in pre-trained models and what properties of an efficient open set learning algorithm operating in the streaming paradigm should possess",Open Set Classification for Deep Learning in Large-Scale and Continual Learning Models,https://core.ac.uk/download/344752519.pdf,RIT Scholar Works,,,core
362593791,2020-12-01T00:00:00,"Increasing deployment of terrestrial, aerial, and space-based assets designed with more demanding services and applications is dramatically escalating the need for high capacity, high data-rate, adaptive, and flexible communication networks. Cognitive, multi-user Free Space Optical Communication (FSOC) networks provide a solution to address these challenges. Such FSOC networks can potentially merge automation and intelligence, as well as offer the benefits of optical communication with enhanced bandwidth and data-rate over long communication networks. Extensive research has investigated various designs, techniques, and methods to enable desired FSOC systems. 
This dissertation reports the investigation and analysis of novel, state-of-the-art methodologies and algorithms for supporting cognitive, multi-user FSOC. This work details an investigation of the ability of diverse Optical-Multiple Access Control (O-MAC) techniques for performing multi-point communication. Independent Component Analysis (ICA) and Non-Orthogonal Multiple Access (NOMA) techniques were experimentally validated, both singularly and in a combined approach, in a high-speed FSOC link. These methods proved to successfully support multi-user FSOC when users share allocation resources (e.g., time, bandwidth, and space, among others). Additionally, transmission and channel parameters that can affect signal reconstruction performance were identified. To introduce cognition and flexibility into the network, the research reported herein details the use of several Machine Learning (ML) algorithms for estimating crucial parameters at the Physical Layer (PHY) of FSOC networks (e.g., number of transmitting users, modulation format, and quality of transmission [QoT]) for automatically supporting and decoding multiple users. In particular, a novel methodology based on a weighted clustering analysis for automatic and blind user discovery is presented in this work. Extensive experimental analysis was conducted under multiple communication scenarios to identify system performance and limitations. Experimental results demonstrated the ability of the proposed techniques to successfully estimate parameters of interest with high accuracy. Finally, this dissertation presents the design and testing of a modular, multiple node, high-speed, real-time Optical Wireless Communication (OWC) testbed, which provides a hardware and software platform for testing proposed methods and for further research development. 
This dissertation successfully proves the feasibility of cognitive, multi-user FSOC through the developed and presented 
 methodologies, as well as extensive experimental analyses. The main strength of the research outcomes of this work consists of exploiting software solutions (e.g., O-MAC, signal processing, and ML techniques) to intelligently support multiple users into a single optical channel (i.e., same allocation resources). Accordingly, Size, Weight and Power (SWaP) requirement can be reduced while achieving an increased network capacity",COGNITIVE MULTI-USER FREE SPACE OPTICAL COMMUNICATION,https://core.ac.uk/download/362593791.pdf,,,,core
387291488,2020-12-10T00:00:00,"3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.Comment: Correct author names and only include primary author emai",E3D: Event-Based 3D Shape Reconstruction,http://arxiv.org/abs/2012.05214,,,,core
389912358,2020-05-15T00:00:00,"Context. The article focuses on the question of automated decision-making analysis made by the operator in ergatic systems of critical infrastructures on the example of marine transport control in difficult navigation conditions. It is evident enough that the main criterion for an adequate perception of input information done by an operator is highly likely to predict the choice of behavioral decision-making strategies in discrete time conditions. However, the difficulty of modeling the operator’s actions is found to be lying in non-linear pattern of taking definite decisions in emergency situations and deviations from the Codes and Rules.Objective. The research purpose strategy of conducted investigation can be defined as the development of the mathematical platform for a decision support system (DSS) module with an aim to identify the class-forming set of atomic elements. In particular this issue determines the fact of distortion of the perception of information about navigation risks predicting the operator’s behavior pattern while having vessel running process. This is possible to have it depicted through formal analysis.Method. To capture the analysis of danger perception by the operator the paper introduces a mathematical model of data collection which identifies the fact of perception distortion in the form of attribute space of metadata obtained by the method of converting information from navigation devices. Besides, the factor of disorientation of the operator can be considered to be a shift on a displaced bridge which significantly affects on the analysis of information for adequate decision making. In addition, taking into account the failure of navigation equipment such as: RADAR, ARPA, AIS, ECDIS, especially while doing exit from the automatic control mode, a dangerous precedent can possibly be created for the operator not ready to perceive the complexity of the situation. To make it work a formal analysis was carried out using the extending risks possibility level tasks during the transition under these conditions. In addition to this item, a probabilistic model of perceiving the situation under the conditions of the error set is reported to have been constructed. So, as the result, the modeling process turned out to show the definite evidence of getting no way possibility to have the degree of criticality of the navigation situation determined without a clear identification of factors affecting the distortion of perception of the operator. Nevertheless, generalized statistical data are sure to be not enough and there is a special need of taking into account an individual information model of each operator for the effective work of DSS as this process faces real challenges. It must be significantly noticed that in order to analyze the perception of information by the operator a special test for defining preferences when choosing a strategy of control actions in the form of maneuvering under difficult navigation conditions purpose was created. Regarding the test results, as well as data on the passage of locations, certain attention is advised to be drawn to the classification analysis of 15 parameters using artificial neural networks having been carried out by our team and, as a consequence, the boundaries of deviations in the perception of navigational danger were found out and clarified. Additional superior item to be spoken about is certainly the introduction of rules and algorithms having been welcomed into the DSS core including the following: interaction field, RADAR and NIS synchronization tools; actual navigational hazard in a given cartographic area; ships trajectories and, as a result, simulations of probable deviations in the information perception of the operator.Results. In order to meet beneficial agreement between the effectiveness of the developed DSS with the proposed formalanalytical approaches an experiment was assumed to be appropriate to be conducted using the Navi Trainer 5000 navigation simulator (NTPRO 5000). Based on the foregoing, due to comprehensive results in experiment metadata for the 2.5 years of operation of navigation simulators and DSS software tools the identification of the deviation probabilities in the information perception of dangers was achieved and export the predicted data to new locations for the operator and cartographic areas was performed. Undoubtedly, the experimental investigation confirmed the hypothesis of the study and reflected completely the feasibility of using this DSS to make predictions of possible risks when control the vessel by analyzing the information model of the operator.Conclusions. Formal-analytical approaches presented in the study combined with the developed DSS software tools and the information itself made it possible to classify the decision-making strategies of the operator when control the vessel and to predict the probability of catastrophic consequences. The feasibility of the proposed models and methods was successfully revealed by carried out experiments. Актуальность. В статье рассматривается задача автоматизированного анализа принятия решений оператором в эргатических системах критических инфраструктур на примере управления морским транспортом в сложных навигационных условиях. Основным критерием адекватного восприятия входной информации оператором является прогнозирование поведенческих стратегий принятия решений в условиях дискретного времени. Однако, сложность моделирования действий оператора состоит в нелинейном формировании решений в условиях внештатных ситуаций и отклонений от Кодексов и Правил. Цель. Целью исследования является разработка математического обеспечения модуля системы поддержки принятия решений (СППР) для идентификации классо-образующего множества атомарных элементов, определяющих факт искажения восприятия информации о навигационных рисках путем формального анализа и прогноза моделей поведения оператора при управлении судном. Метод. С целью автоматизации анализа восприятия опасности оператором, была построена математическая модель, которая идентифицирует факт искажения восприятия в виде признакового пространства метаданных, получаемых посредством преобразования информации навигационных приборов. Фактором дезориентации оператора также может служить несение вахты на смещенном мостике, что существенно влияет на анализ информации для адекватного принятия решений. В связи с нарушением синхронизации навигационных приборов, таких как: РЛС, ARPA, АІС, ECDIS, особенно в случаях выхода из режима автоматического управления, создается опасный прецедент неготовности оператора воспринять сложность ситуации вследствие чего проведен формальный анализ на предмет повышения рисков во время перехода в этих условиях. Также построена вероятностная модель восприятия ситуации в условиях картежа погрешностей. Моделирование показало, что без четкой идентификации факторов, влияющих на искажение восприятия оператора, невозможно определить степень критичности навигационной ситуации, поэтому обобщенных статистических данных недостаточно и для результативной работы СППР необходима индивидуальная информационная модель каждого оператора. С целью анализа восприятия информации оператором был разработан тест, определяющий предпочтения при выборе стратегии управляющих воздействий в виде выполнения маневра при сложных навигационных условиях. Результаты тестирования, а также данные по прохождению локаций позволили выполнить классификационный анализ по 15 параметрам с помощью искусственных нейронных сетей и определить границы отклонений в восприятии навигационной опасности. В ядро СППР внесен ряд правил и алгоритмов, включающие: поле взаимодействия, средства синхронизации РЛС и НИС; фактическая навигационная опасность в данной картографической области; траектории движения судов и, как результат, моделирования вероятного отклонения в информационном восприятии оператора.Результаты. С целью подтверждения результативности разработанной СППР и предложенных формальноаналитических подходов был проведен эксперимент с применением навигационного тренажера Navi Trainer 5000 (NTPRO 5000). Метаданные эксперимента за 2,5 года работы навигационных тренажеров и программные средства СППР позволили идентифицировать вероятность отклонения в информационном восприятии опасностей и экспортировать прогнозированные данные в новые для оператора локации и картографические районы. Проведенный эксперимент подтвердил гипотезу исследования и показал целесообразность применения данной СППР для выполнения прогнозов возможных рисков при управлении судном путем анализа информационной модели оператора. Выводы. Представленные в исследовании информационные и формально-аналитические подходы, а также разработанные программные средства СППР позволили выполнить классификацию стратегий принятия решений оператором при управлении судном и спрогнозировать вероятности катастрофических последствий. Проведенные эксперименты подтвердили целесообразность предложенных моделей и методов. Актуальність. У статті розглядається задача автоматизованого аналізу прийняття рішень оператором в ергатичних системах критичних інфраструктур на прикладі управління морським транспортом в складних навігаційних умовах. Основним критерієм адекватного сприйняття вхідної інформації оператором є прогнозування поведінкових стратегій прийняття рішень в умовах дискретного часу. Однак, складність моделювання дій оператора полягає у нелінійному формуванні рішень в умовах позаштатних ситуацій і відхилень від Кодексів і Правил.Мета. Метою дослідження є розробка математичного забезпечення модуля системи підтримки прийняття рішень (СППР) для ідентифікації класоутворюючих множин атомарних елементів що визначають факт спотворення сприйняття інформації про навігаційні ризики шляхом формального аналізу і прогнозу моделей поведінки оператора при управлінні судном.Метод. З метою автоматизації аналізу сприйняття небезпеки оператором, була побудована математична модель, яка ідентифікує факт спотворення сприйняття у вигляді простору ознак метаданих, що одержуються за допомогою обробки інформації навігаційних приладів. Фактором дезорієнтації оператора також може служити несення вахти на зміщеному містку, що істотно впливає на аналіз інформації для адекватного прийняття рішень. У зв’язку з порушенням синхронізації навігаційних приладів, таких як: РЛС, ARPA, АІС, ECDIS, особливо у випадках виходу з режиму автоматичного управління, виникає небезпечний прецедент що полягає у неготовності оператора сприйняти складність ситуації, внаслідок чого проведено формальний аналіз на предмет підвищення ризиків під час переходу у вказаних умовах. Також побудована імовірнісна модель сприйняття ситуації в умовах картежа похибок. Моделювання показало, що без чіткої ідентифікації факторів, що впливають на спотворення сприйняття оператора, неможливо визначити ступінь критичності навігаційної ситуації, тому узагальнених статистичних даних недостатньо і для результативної роботи СППР, тобто необхідна індивідуальна інформаційна модель кожного оператора. З метою аналізу сприйняття інформації оператором був розроблений тест, що визначає переваги при виборі стратегії керуючих впливів у вигляді виконання маневру при складних навігаційних умовах. Результати тестування, а також дані по проходженню локації дозволили виконати класифікаційний аналіз по 15 параметрам за допомогою штучних нейронних мереж і визначити межі відхилень у сприйнятті навігаційної небезпеки. У ядро СППР внесений ряд правил і алгоритмів, які включають: поле взаємодії, засоби синхронізації РЛС і НІС; фактична навігаційна небезпека в даній картографічній області; траєкторії руху суден і, як результат, моделювання імовірного відхилення у сприйнятті оператора.Результати. З метою підтвердження результативності розробленої СППР і запропонованих формально-аналітичних підходів був проведений експеримент із застосуванням навігаційного тренажера Navi Trainer 5000 (NTPRO 5000). Метадані експерименту за 2,5 року роботи навігаційних тренажерів і програмних засобів СППР дозволили ідентифікувати ймовірність відхилення в інформаційному сприйнятті небезпек і експортувати прогнозовані дані в нові для оператора локації і картографічні райони. Проведений експеримент підтвердив гіпотезу дослідження і показав доцільність трансформаційних змін даної СППР для виконання прогнозів можливих ризиків при управлінні судном шляхом аналізу інформаційної моделі оператора.Висновки. Представлені в дослідженні інформаційні та формально-аналітичні підходи, а також розроблені програмні засоби СППР дозволили виконати класифікацію стратегій прийняття рішень оператором при управлінні судном і спрогнозувати ймовірність катастрофічних наслідків. Проведені експерименти підтвердили доцільність запропонованих моделей і методів.",СИСТЕМА ДІАГНОСТИКИ СПРИЙНЯТТЯ НАВІГАЦІЙНОЇ НЕБЕЗПЕКИ ПІД ЧАС ВИКОНАННЯ СКЛАДНИХ МАНЕВРІВ,,"National University ""Zaporizhzhia Polytechnic""",,,core
141530392,2020-04-17T00:00:00,"In the problem of domain generalization (DG), there are labeled training data
sets from several related prediction problems, and the goal is to make accurate
predictions on future unlabeled data sets that are not known to the learner.
This problem arises in several applications where data distributions fluctuate
because of environmental, technical, or other sources of variation. We
introduce a formal framework for DG, and argue that it can be viewed as a kind
of supervised learning problem by augmenting the original feature space with
the marginal distribution of feature vectors. While our framework has several
connections to conventional analysis of supervised learning algorithms, several
unique aspects of DG require new methods of analysis.
  This work lays the learning theoretic foundations of domain generalization,
building on our earlier conference paper where the problem of DG was introduced
Blanchard et al., 2011. We present two formal models of data generation,
corresponding notions of risk, and distribution-free generalization error
analysis. By focusing our attention on kernel methods, we also provide more
quantitative results and a universally consistent algorithm. An efficient
implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets",Domain Generalization by Marginal Transfer Learning,http://arxiv.org/abs/1711.07910,,,,core
323173707,2020-08-03T00:00:00,"In the last decade, crowd counting and localization attract much attention of
researchers due to its wide-spread applications, including crowd monitoring,
public safety, space design, etc. Many Convolutional Neural Networks (CNN) are
designed for tackling this task. However, currently released datasets are so
small-scale that they can not meet the needs of the supervised CNN-based
algorithms. To remedy this problem, we construct a large-scale congested crowd
counting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a
total of 2,133,375 annotated heads with points and boxes. Compared with other
real-world datasets, it contains various illumination scenes and has the
largest density range (0~20,033). Besides, a benchmark website is developed for
impartially evaluating the different methods, which allows researchers to
submit the results of the test set. Based on the proposed dataset, we further
describe the data characteristics, evaluate the performance of some mainstream
state-of-the-art (SOTA) methods, and analyze the new problems that arise on the
new data. What's more, the benchmark is deployed at
\url{https://www.crowdbenchmark.com/}, and the dataset/code/models/results are
available at \url{https://gjy3035.github.io/NWPU-Crowd-Sample-Code/}.Comment: Accepted by T-PAM",NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization,http://arxiv.org/abs/2001.03360,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TPAMI.2020.3013269,,core
334889215,2020-03-04T00:00:00,"Training deep neural networks at the edge on light computational devices,
embedded systems and robotic platforms is nowadays very challenging. Continual
learning techniques, where complex models are incrementally trained on small
batches of new data, can make the learning problem tractable even for CPU-only
embedded devices enabling remarkable levels of adaptiveness and autonomy.
However, a number of practical problems need to be solved: catastrophic
forgetting before anything else. In this paper we introduce an original
technique named ""Latent Replay"" where, instead of storing a portion of past
data in the input space, we store activations volumes at some intermediate
layer. This can significantly reduce the computation and storage required by
native rehearsal. To keep the representation stable and the stored activations
valid we propose to slow-down learning at all the layers below the latent
replay one, leaving the layers above free to learn at full pace. In our
experiments we show that Latent Replay, combined with existing continual
learning techniques, achieves state-of-the-art performance on complex video
benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d.
batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly
real-time continual learning on the edge through the deployment of the proposed
technique on a smartphone device.Comment: Pre-print v3: 13 pages, 9 figures, 10 tables, 1 algorith",Latent Replay for Real-Time Continual Learning,http://arxiv.org/abs/1912.01100,,,,core
346576626,2020-01-01T00:00:00,"Background: Social distancing measures have been put in place to reduce social interaction to slow transmission of coronavirus (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns.

Objective: This project aims to provide a tool for older people, their families, and peers to improve their wellbeing and health during and after regulated social distancing. Firstly, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction during social-distancing. Secondly, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymised data from the app will be aggregated with other real-world data sources to develop a machine-learning algorithm to improve COVID-19 patient identification and track for real-time use by health systems.

Methods: Development of this project is occurring at the time of publication, and therefore a case study design was selected to provide a systematic means of capturing software engineering in progress. To mitigate potential issues of non-adoption of the proposed intervention, the system was designed using the non-adoption, abandonment, scale-up, spread and sustainability (NASSS) framework. The application development framework utilised is based on Agile methods. The evaluation of the solution’s acceptability and usability shall be conducted as a feasibility study to analyse factors impacting the solution’s use, adoption and uptake.

Results: Making use of a pre-existing software framework for health behaviour change, a proof of concept was developed, and multi-stage application development and deployment for the solution created. Grant submissions to fund the project and study execution have been sought at the time of publication, and pre-discovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought.

Conclusions: This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.</p",Agile requirements engineering and software planning for a digital health platform to engage the effects of isolation caused by social distancing: a case study and feasibility study protocol,,'JMIR Publications Inc.',,,core
288813934,2020-03-27T00:00:00,"Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων
Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την
ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην
δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και
ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των
κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του
ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται
μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent
space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της 
δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ
τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο
ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή
μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και,
παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε
καθημερινά περιβάλλοντα.
Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας
κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο
πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο
και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες
μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο
με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την
αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική
εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι
σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης
(evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης
λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών.
Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση
της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών
χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός
νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning
scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών
σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για
την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη
διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού
και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό
βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,
επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα
γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού
εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi,
σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και
εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει
αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της
διαδικασίας μάθησης.
Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης
συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού
αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή
κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα
χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε
σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η
διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό
εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης.The current PhD thesis addresses the formulation and implementation of a methodological
framework for robot Learning from Demonstration (LfD). The latter refers to methodologies
that develop behavioral policies from example state-to-action mappings. To this
end, we study the reciprocal interaction of perception and action, in order to teach robots
a repertoire of novel action behaviors. Based on that, we design, develop and implement
a robust imitation framework, termed IMFO (IMitation Framework by Observation), that
facilitates imitation learning and relevant applications in human-robot interaction (HRI)
tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions,
aswell as novel ones. Mapping of human actions to the respective robotic ones is achieved
via an indeterminate depiction, termed latent space representation. The latter accomplishes
a compact, yet precise abstraction of action trajectories, effectively representing
high dimensional raw actions in a low dimensional space.
Moreover, throughout this thesis, we examine the role of time in LfD by enhancing
the aforementioned framework with the notion of learning both the spatial and temporal
characteristics of human motions. Accordingly, learned actions can be subsequently reproduced
in the context of more complex time-informed HRI scenarios. Unlike previous
LfD methods that cope only with the spatial traits of an action, the formulated scheme
effectively encompasses spatial and temporal aspects. Extensive experimentation with a
variety of real robotic platforms demonstrates the robustness and applicability of the introduced
integrated LfD scheme.
Learned actions are reproduced under the high level control of a time-informed task
planner. During the implementation of the studied scenarios, temporal and physical constraints
may impose speed adaptations in the performed actions. The employed latent
space representation readily supports such variations, giving rise to novel actions in the
temporal domain. Experimental results demonstrate the effectiveness of the proposed
enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set
of well defined evaluation metrics are introduced to assess the validity of the proposed
approach considering the temporal and spatial consistency of the reproduced behaviors.
A noteworthy extension of the above regards force-based object grasping for executing
sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised
learning scheme, termed SLF (Supervised Learning for Force-based manipulation).
SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation
to acquire sufficient training data; (b) training to facilitate grasp learning with suitable
robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following
sim-to-real transfer, operation in real environments is achieved in addition to simulated
ones, generalizing also for objects not included in the trial sessions. The proposed
learning scheme is demonstrated in object lifting tasks where the applied force varies for
different objects with similar contact friction coefficients, and likewise the grasping pose.
Experimental results on the manipulator YuMi show that the robot is able to effectively
reproduce demanding lifting and manipulation tasks after learning is accomplished.
In summary, our thesis has studied LfD and has contributed with a novel approach that
introduced latent space representations to encode the action characteristics. A framework
implementation (IMFO) of our approach allowed extensive experimentation and also conduction
of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it
to cope with complex, real-life interactions. Finally, the extension of IMFO with forcebased
grasping facilitated manipulation tasks with sensitive objects",Μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,,,,,core
323177306,2020-05-01T00:00:00,"The term “SMART Monitoring” is often used in digital projects to survey and analyze data flows in near- or realtime. The term is also adopted in the project Digital Earth (DE) which was jointly launched in 2018 by the eight Helmholtz centers of the research field Earth and Environment (E&E) within the framework of the German Ministry of Education and Research (BMBF). Within DE, the “SMART monitoring” sub-project aims at developing workflows and processes to make scientific parameters and the related datasets SMART, which means specific, measurable, accepted, relevant, and trackable (SMART).



“SMART Monitoring” in DE comprises a combination of hard- and software tools to enhance the traditional sequential monitoring approach - where data are step-by-step analyzed and processed from the sensor towards a repository - into an integrated analysis approach where information on the measured value together with the status of each sensor and possible auxiliary relevant sensor data in a sensor network are available and used in real-time to enhance the sensor output concerning data accuracy,  precision, and data availability. Thus, SMART Monitoring could be defined as a computer-enhanced monitoring network with automatic data flow control from individual sensors in a sensor network to databases enhanced by automated (machine learning) and near real-time interactive data analyses/exploration using the full potential of all available sensors within the network. Besides, “SMART monitoring” aims to help for a better adjustment of sensor settings and monitoring strategies in time and space in iterative feedback.



This poster presentation will show general concepts, workflows, and possible visualization tools based on examples that support the SMART Monitoring idea",Significance and implementation of SMART Monitoring Tools,,'Copernicus GmbH',10.5194/egusphere-egu2020-11084,,core
343943644,2020-01-01T08:00:00,"With increasing energy demand and an intermittent supply of renewable energy sources, our current energy grid needs a transformation towards a more robust, reliable energy trading architecture. The smart grid promises this architecture as the future of the present energy market, where traders will use digital technologies to automate the management of power delivery. It will improve many issues of the current energy grid such as sustainable, clean, renewable, reliable and secure energy supply, customer participation in markets, distributed generation, and transparency in energy trading. Using autonomous trading agents, we can bridge several dynamic energy markets and ensure an efficient and robust trading environment for all the players in the smart grid. The Power Trading Agent Competition (Power TAC) simulation emphasizes the strategic problems that autonomous trading agents, i.e., brokers, will face in managing the economics of a smart grid. In Power TAC, brokers make trades in multiple parallel markets such as wholesale, tariff, and balancing markets to supply energy from producers to consumers. To be successful, brokers must make reasonable predictions about future supply, demand, and prices in the wholesale and tariff markets to make trading decisions by maintaining a favorable energy imbalance in the balancing market. Market clearing price prediction is an integral part of the broker\u27s wholesale market strategy because it helps the broker to make intelligent decisions for purchasing energy at low cost in a day-ahead wholesale market. People use machine learning methods to predict prices in the Power TAC wholesale Periodic Double Auction (PDA) market, where the brokers can take advantage of the price predictor to design bidding strategies. PDAs are commonly used in real-world energy markets to trade energy in specific time slots to balance demand on the power grid where multiple discrete trading periods are specified for a single type of good. Strategically, bidding in a PDA is complicated because the bidder must predict and plan for future auctions that may influence the bidding strategy for the current auction. In our work, we use the RepTree model to predict prices and present a general bidding strategy for PDAs. Our wholesale market strategy uses forecasted clearing prices and Monte Carlo Tree Search (MCTS) to plan a bidding approach across multiple time-periods. Additionally, we present a fast heuristic policy that can be used either as a standalone method or as a seeding technique to initialize the search space of the MCTS bidding strategy. We evaluate our bidding strategies using a controlled PDA simulator based on the wholesale market implemented in the PowerTAC competition. We demonstrate that our strategies outperform state-of-the-art champion bidding strategies designed for that competition. In the retail market, a broker makes sequential decisions simultaneously with other brokers to buy and sell energy through publishing tariffs where a tariff is a contract between a broker and a customer. To be as profitable as possible, a broker needs an effective energy selling retail strategy. Our work includes developing an isolated miniature retail market simulator to control the dynamic and stochastic variables of the vibrant, complex retail market so that we can understand the basic features and strategic dynamics among retail trading strategies. We apply deep reinforcement learning (DQN) to learn the best response (BR) strategy for a specific strategy played in this simulator. Using DQN as a best response learning technique, we propose ``Clustered Double Oracle Empirical Game-Theoretic Analysis (CDO-EGTA), a novel method for minimizing regret (i.e., maximizing revenues) in retail trading. CDO-EGTA method clusters the existing pool of strategies into some groups, learns a new BR strategy for each of the groups using the Double Oracle Empirical Game-Theoretic Analysis method, and outputs a class of BR strategies to play the game. Empirical results show that our method outperforms the existing methods in regret comparison",Autonomous Trading Strategies for Dynamic Energy Markets,,ScholarWorks@UTEP,,,core
324170040,2020-01-01T00:00:00,"Ageing consists one of the biggest societal challenges worldwide. Older adults’ desire of staying healthy, staying at their own home consists an important motive for the development of assistive technologies that will realize it through remote monitoring technologies. Proliferation of low-cost, off-the-shelf IoT devices has led to the implementation of the so called smart homes projects, which are solutions that are mixing the older adults’ physical spaces with monitoring and computing capabilities, allowing the collection of high-frequency daily life data. Methods: This stydy consist of two parts. An experimental one that takes place in a lab space and a real life application of sensor monitoring technology. In particular, an ecologically valid space was created within the Thessaloniki Active & Healthy Ageing Living, where fifteen (15) older adults participated in the pilot testing of the technology. Participants visited the ecologically valid space for almost two weeks, covering eight (8) 1 hour-sessions in total. There, they followed a protocol of typical daily life activities, where several unobtrusive scenario of monitoring were interweaved. No restrictions were imposed with respect to the execution of tasks, thus making sure that the data collection follows an ecologically valid paradigm. Subsequently, senor data collected from 10 participants were statistically correlated to clinical assessment tools. Then, a series of focus groups openly discussing about technology and obtrusiveness were carried out with thirteen participants. Moving to the second part of the study, some adjustments with respect to the technology had to be considered according to the Living Lab study findings. Favorable technology was installed to 5 older womens’ homes for more than a year time resulting to hundreds of thousands data points to be collected. Different modeling techniques were employed, such as statistical modeling, and machine learning, in order to explore clinical added value of digital biomarkers. As an application scenario, the study of modelling emotional disturbances and in particular the one of geriatric depression was carried out. In addition, a longitudinal study about sensor monitoring unobtrusiveness was conducted to examine end-user acceptance. A follow-up interview was carried out with each one of the participants. Finally, we explore the possibility of mapping the results of the data-driven modelling approaches with expert-driven models and knowledge representation schemata, such as the Fuzzy Cognitive Maps (FCMs) in a way that is transparent to the clinicians. Results: With respect to the first part of the study quite a few statistical significant correlations between sensor data and clinical tools were found, e.g. mobility and affective characteristics linked to emotional health and quality of life. With respect to the results of the focus groups these were transcribed and analysed with qualitative methods in order to extract the most significant themes and to categorize them to one of the obtrusiveness framework axes. For the second part o fthe study, a generalized linear mixed prediction model of PHQ-9 was developed utilizing information about TV usage patterns. Random Forests achieved mean accuracy score >80% when given to classify between healthy and depressive cases, while another RF classifier achieved an AUC>90% when having to deal with all sorts of depressive synptoms severity (from mild to severe). The initial FCM model also achieved a high classification rate up to 96% given some synthetic cases provided by experts. Finally, older adults’ longitudinal attitudes revealed a negative stance towards the use of the mirror camera and the smart watch, as well as the Kinect device. The answers to the questionnaire of the lady that left the study at month 12, were compared against the mean value of the rest four particpants to check for any particular reasons of her decision. Significant differences from the mean value of the rest four were found for the Kinect device, the smart watch and the mirror camera. Conclusions: Clinical value of digital biomarkers has been been revealed in many publications, yet their application in longitudinal studies is absent. A great challenge nowadays remains the robust operation of such technologies under real life circumstances, without any restrictions (in the wild) as well as their acceptance from older people, and their doctors, if at any time in the future we wish to integrate such data in the clinical practice.Η γήρανση του πληθυσμού αποτελεί μία από τις μεγαλύτερες κοινωνικές προκλήσεις σε ολόκληρο τον πλανήτη. Η επιθυμία των ηλικιωμένων για καλή γήρανση, παραμένοντας στο σπίτι τους αποτελεί σημαντικό παράγοντα για την ανάπτυξη υποστηρικτικών τεχνολογιών που θα προσφέρουν αυτήν την δυνατότητα μέσω τεχνολογιών απομακρυσμένης παρακολούθησης. Η εξάπλωση φθηνών έξυπνων συσκευών άμεσα διαθέσιμων στο εμπόριο και με την δυνατότητα σύνδεσης στο λεγόμενο Διαδίκτυο των Πραγμάτων (Internet of Things - IoT) έχει στρέψει την ερευνητική κοινότητα στην ανάπτυξη των λεγόμενων έξυπνων σπιτιών. Τα έξυπνα σπίτια αποτελούν λύσεις που συνδυάζουν τεχνολογίες αισθητήρων ενσωματώνοντάς τες στον φυσικό περιβάλλοντα χώρο των σπιτιών των ηλικιωμένων, επιτρέποντας την συνεχή και λεπτομερή παρακολούθηση των καθημερινών δραστηριότητων τους. Μέθοδοι: Η μελέτη μας αποτελείται από δύο σκέλη. Ένα πειραματικό σε εργαστηριακό χώρο και μια εφαρμογή της τεχνολογίας σε πραγματικό περιβάλλον. Συγκεκριμένα, δημιουργήθηκε ένας οικολογικά έγκυρος χώρος στα πλαίσια του Ζωντανού Εργαστηρίου Ενεργού και Υγιούς Γήρανσης, όπου δέκα πέντε (15) ηλικιωμένοι συμμετέχοντες έλαβαν μέρος στην πιλοτική δοκιμή της τεχνολογίας. Οι συμμετέχοντες επισκέπτονταν τον οικολογικά έγκυρο χώρο για περίπου 2 εβδομάδες (8 συνολικά συνεδρίες) όπου ακολουθούσαν ένα πρωτόκολλο δραστηριοτήτων, που περιελάμβαναν ορισμένα διακριτικά σενάρια παρακολούθησης, χωρίς αυστηρούς περιορισμούς επιτρέποντας την συλλογή ρεαλιστικών συμπεριφορικών δεδομένων από το δίκτυο αισθητήρων. Στην συνέχεια τα δεδομένα που συλλλέχθηκαν από 10 συμμετέχοντες αναλύθηκαν στατιστικά με τα κλινικά τεστ αξιολόγησης της υγείας των ηλικιωμένων ώστε να βρεθούν τυχόν συσχετίσεις και δείκτες υγείας. Μετά το πέρας αυτών των πιλοτικών δοκιμών πραγματοποιήθηκε μια σειρά από ομάδες εστιασμένης συζήτησης όπου πραγματοποιήθηκε κουβέντα γύρω από την παρεμβατικότητα της τεχνολογίας με την υιοθέτηση ενός θεωρητικού πλαισίου ορισμού από την βιβλιογραφία. Στο δεύτερο μέρος της μελέτης, μελετήθηκαν τα αποτελέσματα τόσο από την ποιοτική ανάλυση των ομάδων εστίασης, όσο και από τα μελή της ερευνητικής ομάδας και πραγματοποιήθηκε η εγκατάσταση ενός ελαφρώς τροποποιημένου τεχνολογικού συστήματος σε 5 σπίτια ηλικιωμένων για ένα διάστημα πλέον του ενός έτους, συλλέγοντας με αυτόν τον τρόπο εκατοντάδες χιλιάδες ψηφιακά στιγμιότυπα της καθημερινότητας των ηλικιωμένων. Στην συνέχεια επιχειρήθηκε η ανάπτυξη τόσο στατιστικών μοντέλων, όσο και μοντέλων μηχανικής μάθησης για την διερεύνηση της κλινικής αξίας των ψηφιακών βιοδεικτών. Σαν μελέτη εφαρμογής αυτών των μοντέλων ήταν η πρόβλεψη συναισθηματικών διαταραχών καθώς και της καταθλιπτικής συμπτωματολογίας. Παράλληλα με αυτές τις μελέτες, διενεργήθηκε και μια μακροπρόθεσμη (Longitudinal) μελέτη της αποδοχής των αισθητήρων από τους ηλικιωμένους με έμφαση και πάλι στον βαθμό παρεμβατικότητας. Η μελέτη διενεργήθηκε με την μορφή προσωπικών συνεντεύξεων με κάθε μία από τις πέντε συμμετέχουσες. Τέλος, με την πρόταση και δημιουργία ενός εμπειρικού μοντέλου (expert model) υποστήριξης της διάγνωσης της γηριατρικής κατάθλιψης (Fuzzy Cognitive Maps), αποκρυσταλλώνοντας την γνώση των ειδικών σε ένα σχήμα αναπαράστασης γνώσης προσιτό σε αυτούς, επιχειρείται η αντιστοίχηση της νέας γνώσης που παράγεται από τους αισθητήρες με την υπάρχουσα κλινική γνώση. Αποτελέσματα: Όσον αφορά το πρώτο σκέλος της διατριβής βρέθηκαν αρκετές στατιστικά σημαντικές συσχετίσεις ανάμεσα στις επιμέρους κατηγορίες ψηφιακών δεικτών, όπως κινητικοί, συναισθηματικοί και φυσιολογικοί και των επιμέρους κατηγοριών κλινικών αξιολογήσεων της υγείας των ηλικιωμένων. Όσον αφορά τα αποτελέσματα από τις ομάδες εστιασμένης συζήτησης αυτά αναλύθηκαν με ποιοτικές μεθόδους για να εξαχθούν τα πιο βασικά θέματα που προέκυψαν από τις συζητήσεις και να κατηγοριοποιηθούν σε κάθε ένα από τους άξονες του πλαισίου παρεμβατικότητας (obtrusiveness framework). Για το δεύτερο σκέλος της διατριβής, δημιουργήθηκε ένα γραμμικό μοντέλο πρόβλεψης του PHQ-9 σκορ από τα μοτίβα λειτουργίας της τηλεόρασης. Εκεί βρέθηκαν συσχετίσεις με κάποια από τα συμπτώματα της κατάθλιψης. Το μοντέλο μηχανικής μάθησης και συγκεκριμένα, Τυχαία Δάση (Random Forests, RF) πέτυχαν με ακρίβεια άνω του 80% να διακρίνουν μεταξύ καταθλιπτικών και υγιών καθημερινών στιγμυοτύπων συμπεριφοράς, ενώ το μοντέλο ταξινόμησης καθημερινών προτύπων συμπεριφοράς σε ήπια, μέτρια και σοβαρά καταθλιπτικά περιστατικά πέτυχε εμβαδόν επιφάνειας ROC >90%. Επίσης, η αρχική αξιολόγηση του μοντέλου υποστήριξης της διάγνωσης της κατάθλιψης, με βάση το σχήμα αναπαράστασης γνώσης FCM είχε μέση ακρίβεια περίπου 96%. Τέλος, η σύγκριση των μοτίβων αντίληψης των ηλικιωμένων όσον αφορά την παρεμβατικότητα των τεχνολογιών που είχαν εγκατεστημένες στο σπίτι τους, απεκάλυψε την αρνητική τους στάση απέναντι στο έξυπνο ρολόι και τον καθρέφτη-κάμερα. Επίσης, καθώς μία από τις 5 κυρίες απεχώρησε οικιοθελώς από την μελέτη, συγκρίθηκαν οι απαντήσεις της με τον μέσο όρο των υπολοίπων ηλικιωμένων γυναικών. ώστε να αποκαλυφθούν οι αιτίες της αποχώρησης από την μελέτη. Βασικοί λόγοι αποδείχθηκαν το έξυπνο ρολόι, ο καθρέφτης με την ενσωματωμένη κάμερα αλλά και η συσκευή Kinect λόγω και της οποίας αναγκάστηκε να αλλάξει την καθημερινή ρουτίνα της. Συμπεράσματα: Η κλινική αξία των ψηφιακών βιοδεικτών έχει αναδειχθεί σε πλήθος δημοσιεύσεων, όμως η χρήση και αξιολόγησή τους σε μακροπρόθεσμες μελέτες απουσιάζει. Μεγάλη πρόκληση αποτελεί στις μέρες μας η εύρωστη λειτουργία τέτοιων τεχνολογιών υπό πραγματικές συνθήκες χωρίς πριορισμούς (in the wild) αλλά και η αποδοχή τους τόσο από τους ηλικιωμένους, όσο και από τους γιατρούς αν κάποια στιγμή στο μέλλον θελήσουμε να χρησιμοποιήσουμε τέτοιου είδους δεδομένα στην κλινική πράξη",Digital biomarkers as ecologically valid measures for the remote and longitudinal assessment of older adults health,,'National Documentation Centre (EKT)',10.12681/eadd/47534,,core
395059242,2020-10-21T00:00:00,"A neural network based z-vertex trigger is developed for the first level trigger of the upgraded flavor physics experiment Belle II at the high luminosity B factory SuperKEKB in Tsukuba, Japan. Using the hit and drift time information from the central drift chamber, a pool of expert neural networks estimates the 3D track parameters of the single tracks found by a 2D Hough finder. The neural networks are already implemented on parallel FPGA hardware for real time data processing and running pipelined in the online first level trigger of Belle II. Due to the anticipated high luminosity of up to 8 × 10³⁵ cm⁻²s⁻¹, Belle II will have to face severe levels of background tracks with vertices displaced along the beamline. The neural z-vertex algorithm presented in this thesis allows to reject displaced background tracks such that the requirements of the standard track trigger can be strongly relaxed. Especially for physics decay channels with a low track multiplicity in the final states, like τ pair production, or initial state radiation events with reduced center of mass energies, the trigger efficiencies can be significantly increased.



As an upgrade of the present 2D Hough finder in the neural network preprocessing, a model independent 3D track finder is developed that uses the additional stereo hit information of the drift chamber. Thus, the trigger efficiencies improve for tracks in the phase space of low transverse momenta and shallow polar angles. Since the cross sections of the physics signal events typically increase towards shallow polar angles, this enlarged acceptance of the track trigger provides a substantial gain in the signal efficiencies. By using an adapted pool of expert networks, the enlarged phase space provided by the 3D finder can be efficiently covered.



Studies on simulated MC background, on simulated initial state radiation events, and on recorded data from early Belle II runs demonstrate the high performance of the novel trigger algorithms. With the 3D finder an increase of the track finding rate of about 50 % is confirmed for signal tracks, while displaced background tracks are actively suppressed prior to the neural network. Based on z-vertex cuts on the tracks processed by the neural networks, a two track event efficiency of more than 99 % can be achieved with a purity of around 80 %",Efficient physics signal selectors for the first trigger level of the Belle II experiment based on machine learning,https://core.ac.uk/download/395059242.pdf,Ludwig-Maximilians-Universität München,,,core
334912046,2020-02-13T00:00:00,"Message-passing neural networks (MPNNs) have been successfully applied to
representation learning on graphs in a variety of real-world applications.
However, two fundamental weaknesses of MPNNs' aggregators limit their ability
to represent graph-structured data: losing the structural information of nodes
in neighborhoods and lacking the ability to capture long-range dependencies in
disassortative graphs. Few studies have noticed the weaknesses from different
perspectives. From the observations on classical neural network and network
geometry, we propose a novel geometric aggregation scheme for graph neural
networks to overcome the two weaknesses. The behind basic idea is the
aggregation on a graph can benefit from a continuous space underlying the
graph. The proposed aggregation scheme is permutation-invariant and consists of
three modules, node embedding, structural neighborhood, and bi-level
aggregation. We also present an implementation of the scheme in graph
convolutional networks, termed Geom-GCN (Geometric Graph Convolutional
Networks), to perform transductive learning on graphs. Experimental results
show the proposed Geom-GCN achieved state-of-the-art performance on a wide
range of open datasets of graphs. Code is available at
https://github.com/graphdml-uiuc-jlu/geom-gcn.Comment: Published as a conference paper at ICLR 202",Geom-GCN: Geometric Graph Convolutional Networks,http://arxiv.org/abs/2002.05287,,,,core
334944012,2020-10-01T00:00:00,"While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces","The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep
  Learning to Radio Frequency Applications",http://arxiv.org/abs/2010.00432,,,,core
334940848,2020-09-25T00:00:00,"Optics is a promising platform in which to help realise the next generation
of fast, parallel and energy-efficient computation. We demonstrate a
reconfigurable free-space optical multiplier that is capable of over 3000
computations in parallel, using spatial light modulators with a pixel
resolution of only 340x340. This enables vector-matrix multiplication and
parallel vector-vector multiplication with vector size of up to 56. Our design
is the first to simultaneously support optical implementation of
reconfigurable, large-size and real-valued linear algebraic operations. Such an
optical multiplier can serve as a building block of special-purpose optical
processors such as optical neural networks and optical Ising machines.Comment: 4 pages, 4 figure",Fully reconfigurable coherent optical vector-matrix multiplication,http://arxiv.org/abs/2009.12095,'The Optical Society',10.1364/OL.401675,,core
362230397,2020-10-21T00:00:00,"In the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis. This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced Blanchard et al., 2011. We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets",Domain Generalization by Marginal Transfer Learning,,HAL CCSD,,,core
322969060,2020-05-15T00:00:00,"In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.Comment: 12 pages, 14 figures, 15th International Symposium on Software
  Engineering for Adaptive and Self-Managing Systems (SEAMS2020","Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing",http://arxiv.org/abs/2005.07460,,,,core
386926580,2020-01-01T00:00:00,"The Prediction in Ungauged Basins (PUB) Initiative was created to advance scientific understanding and estimation of hydrological processes, as well as associated uncertainties, to improve prediction capabilities in basins which are poorly gauged (Sivapalan et al., 2003; Blöschl et al. 2013; Blöschl, 2016; Blöschl et al., 2019). The main objectives defined by the PUB Initiative were to (1) improve the ability of existing hydrologic models to predict with reduced uncertainty, and to (2) develop new and innovative models representing the space–time variability of hydrological processes (Hrachowitz et al., 2013). Despite the International Association of Hydrological Sciences (IAHS) dedicated an entire decade (2003-2012) to advance the problem of Prediction in Ungauged Basins (Hrachowitz et al. 2013), the central goal remains largely a challenge (Kratzert et al., 2020).This dissertation discusses the main steps and decisions required to implement, calibrate, and validate an operational (real-time) Hydrological Forecasting System (HFS), for short-range to long-range (SR2LR) daily streamflow forecasting. The HFS was implemented under an operational context, and experimentally evaluated in the “poorly-gauged” Upper Zambezi River Basin (UZRB) and its “ungauged” sub-basins. The state of art focuses on describing several hydrological modelling strategies (HMS), discussing the way hydrological ensembles have been traditionally performed, and how meteorological and hydrological uncertainty have been quantified. Additionally, a novel Variational Ensemble Forecasting (VEF) approach was applied and evaluated assuming that any combination of multiple inputs, models, and optimal parameters sets, is a practical hydrological ensemble that can be used to reproduce daily streamflow forecasts with reduced total uncertainty. 
The VEF approach implemented allowed for increasing the number of hydrological ensembles (outputs) from all possible combinations of multiple satellite products (or multiple climate models), hydrologic models, and optimal parameter sets. The performance of VEF was compared and evaluated with classical approaches used to develop hydrological ensembles (input-model-output). To complement the application of the VEF approach, three hydrologic processing hypotheses (HPH) to quantify the hydrological uncertainty propagated from the components of a modelling chain were used: (1) Hydrological Pre-Processing (HPR); (2) Hydrological Processing (HP), and (3) Hydrological Post-Processing (HPP). These HPH’s were evaluated with practical examples in the UZRB and its sub-basins, proving to be a more efficient and systematic way to quantify and reduce the uncertainty propagated from an operational VEF implementation. 
To inform the development of reliable operational hydrologic forecasting products, the HPR hypotheses are evaluated through the analysis of the spatio-temporal structure of meteorological uncertainty. This analysis allowed determining what factors dominate the propagation of meteorological uncertainty for the operational implementation of distributed hydrologic modeling strategies i.e. the spatial resolution of a meteorological input, the leading time of the forecasts associated to it, and the size of the basin under evaluation. 
The role of machine learning (ML) approaches for daily streamflow forecasting was also evaluated by coupling VEF with ML techniques (VEF-ML approach). In doing so, several hydrologic learning strategies (inference versus pattern-based) were compared and evaluated to improve the HFS performance through hydrologic post-processing hypotheses (see i.e. Nearing et al., 2020 a, b; Gauch et al., 2020). Lastly, we will discuss how the properties of hydrologic forecasts can be improved by applying Hydrologic Forecast Skill Analysis (FCA) to answer the following question: How to perform a Skill Analysis of Seasonal Hydrologic Streamflow Forecasts? The main contributions and results of this dissertation highlight several opportunities and challenges for future research aimed to advance the main goal and the corresponding objectives of the PUB initiative","Operational Short-Range to Long Range (SR2LR) Streamflow Forecasting for Poorly Gauged Basins: The Unexplored Dimension of Variational Ensemble Forecasting, the Spatio-Temporal Structure of Modeling Paradigms, and the Role of Machine Learning Strategies to Improve Hydrological Hypotheses",,The University of Arizona.,,,core
326322163,2020-06-11T00:00:00,"Cílem této práce je upravit stávající prototyp magnetického senzoru pro detekci obsazenosti parkovacího místa automobilem. Změny se týkají hardwaru, ale především detekčního algoritmu. Za tímto účelem je vyzkoušeno několik možností detekce s využitím vzájemné korelace, neuronových sítí a metody nejbližšího souseda. Vybrané metody jsou implementovány s využitím reálných dat naměřených senzorem a dále testovány na několika typech automobilů.The aim of this work is to modify the existing prototype of a magnetic sensor that detects the occupancy of a parking space by a car. The changes concern the hardware, but notably the detection algorithm. For this purpose, several detection methods are tried using cross-correlation, neural networks and the nearest neighbour method. Selected methods are implemented using real data measured by a sensor and further tested on several types of cars",Advanced methods for car detection using magnetic field measurement,,Czech Technical University in Prague. Computing and Information Centre.,,,core
346635374,2020-01-01T00:00:00,"This project was designed to test three different technologies for monitoring the fall armyworm, a highly migratory moth pest of maize in Kenya. This insect is invasive and is estimated to have caused between US$200 and US$600 million dollars’ worth of crop damage in Africa since it was first observed in western Africa in 2016. 

The three technologies we piloted were entomological radar, digital pheromone traps and an image identification app. The hypothesis we set out to test was that high-altitude moth migrations are linked to pest incidence at ground level and the later impacts of feeding damage. Our objective was to install and launch all three technologies in the space of a year, with the aim of developing an integrated dataset that would provide an overview of near-ground and upper-atmospheric movements of fall armyworm. This data would be shared with multiple different stakeholders in real-time.

Over a 12-month period between March 2019 and February 2020, we installed the entomological radar, 20 digital pheromone traps and launched the Nondo app to test this hypothesis. Although there were several challenges, the outcomes after one year include:

1.	Preliminary radar data. We were able to detect insects at heights up to 800m and the preliminary data would suggest these are targets of the appropriate mass to be classed as noctuid moths
2.	A network of digital pheromone traps. We observed fall armyworm in almost all of the locations where the traps were installed and received daily automated updates, despite the traps being hosted in very rural parts of Kenya. 
3.	An image detection algorithm for the fall armyworm and other maize pests. This provided a very high level of accuracy for species where we were able to provide more than 100 images (>90% in some cases). 

This project has profound potential for impact as it demonstrates the successful implementation of an “internet of things” approach to biological monitoring in very rural parts of Kenya. Indeed, one of the highlights of this project is the deployment of the digital trap network beyond the end of the project. This is the first time such an ambitious, multi-layered monitoring network comprising radar, pheromone traps, machine learning and decision support apps has been established in Sub-Saharan Africa. This project is a proof-of-concept that clearly demonstrates the potential of digital monitoring to deliver major impacts for farmers in Sub-Saharan Africa",Smart Armyworm Surveillance: Project Technical Report,https://core.ac.uk/download/346635374.pdf,Rothamsted Research,10.23637/rothamsted.mx3c-6k94,,core
441761640,2020-01-01T00:00:00,"Time series analysis and prediction are major scientific challenges that find their applications in fields as diverse as finance, biology, economics, meteorology, and so on. Obtaining the method with the least prediction error is one of the difficult problems of financial market and investment analysts. State space modelling is an efficient and flexible method for statistical inference of a broad class of time series and other data. The neural network is an important tool for analyzing time series especially when it is nonlinear and nonstationary. Essential tools for the study of Box-Jenkins methodology, neural networks, and extended Kalman filter were put together. We examine the use of the nonlinear autoregressive neural network method as a prediction technique for financial time series and the application of the extended Kalman filter algorithm to improve the accuracy of the model. As application on a real example, we are analyzing the time series of the daily price of steel over a 790-day period for establishing the superiority of this method over other existing methods. The simulation results using MATLAB and R software show that the model is capable of producing a reasonable accuracy",Nonlinear Autoregressive Neural Network and Extended Kalman Filters for Prediction of Financial Time Series,,'Hindawi Limited',10.1155/2020/5057801,"[{'title': 'Journal of Applied Mathematics', 'identifiers': ['issn:1110-757X', '1687-0042', 'issn:1687-0042', '1110-757x']}]",core
395058120,2020-01-01T00:00:00,"This dissertation’s innovation is concentrated on the introduction and description of a reference network framework for Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services integration and deployment on all the OSI layers in a heterogeneous wireless network. The necessity of lower layers services and applications conceptualization within the Cognitive Radio Cycle  is the main issue that this dissertation manifests whilst providing algorithms for  responding to diverse issues within the Cognitive Radio Network and Cognitive Radio Network Cloud. Radio Spectrum is a public good and the recent years due to the vast increase of mobile users and mobile applications as well as the need for Quality of Experience (QoE) of the end user, a spectrum scarcity was a main issue in wireless networks leading to the introduction of a new key technology namely the Cognitive Radio for the underutilized licensed spectrum bands broadening the radio spectrum usage and management. The current challenges in Cognitive Radio Network (CRN) are storing of large amount of data, processing them in real time and the exchanging of nodes’ current status on-the-fly. These challenges are in contrast to the limited storage and processing ability (plus battery lifetime) of Cognitive Devices thus the need for additional capabilities arise. Cognitive Radio Network Cloud (CRNC) is an infra-structure consisting of mobile nodes and the cloud whose primary goal is to keep an up-to-date status of the spectrum availability in the network. Demanding tasks, e.g. signal intelligence, could be off-loaded to powerful nodes locally allowing the local network to be self-organized. By allowing self-organizing networks to be deployed locally, huge heterogeneous wireless networks such as 5G and 6G evolve, which can mitigate their dynamic spectrum access and control to meet the end users and wireless network performance requirements.Self-organizing Cognitive Radio Networks in an immense heterogeneous wireless network along with Dynamic Spectrum Access, Management and Control Mitigation on demand or not on demand to respond to network needs in real time and on the fly can be realized with high level abstraction and conceptualization of Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services integration and deployment on all the Open Systems Interconnection (OSI) layers, cross-platform and cross-network, cross-operator. Central coordination would be applicable for triggering local nomad network to be self-organized, as well for hand-off or for meeting QoE, radio network performance, institutional metrics. Network clustering on Cognitive Medium Access Control and SDR Services/Applications and Abstract Cognitive Medium Access Control and SDR Services/Applications Level may be feasible. Artificial Intelligence and other technologies will enable efficient distributed control. Radio Environment Maps are powerful technology to this direction. Off-loading to local powerful nodes increase network performance and QoE which are essential for 5G, 6G network for urban and rural radio environments. A new mathematic method of mathematic game unfolding is introduced i.e. a new mathematic method for generating games without coordination and a corresponding mathematic game model as an application of the proposed mathematic method to for the Cognitive Radio Network and Cognitive Radio Cloud Security was introduced. Other mathematic game reaching Nash Equilibriums also were introduced. Deterministic automata and Machine Learning were also introduced as well as other mathematic formulas applicable to the corresponding network protocols, mathematic models for steady-state- Lyapunov filtering algorithm for enhanced CRN-SDR signal processing and mathematic formulas for Non-Reciprocal Channels in Massive MIMO CRN were also introduced in this dissertation. Demanding tasks, e.g. signal intelligence, could be off-loaded to powerful nodes locally allowing the local network to be self-organized. By allowing self-organizing networks to be deployed locally, huge heterogeneous wireless networks such as 5G and 6G evolve, which can mitigate their dynamic spectrum access and control to meet the end users and wireless network performance requirements. A critical issue in CR Infrastructure-less network deployment on the cloud would be the Standard Interface Operability (SIO) for CR users to connect to the cloud or local powerful nodes. SIO would allow easy access and utilization of powerful wireless and mobile local nodes and infrastructure network front-end. This dissertation and research work presented in the following chapters participated in this effort or the international research community on wireless network  by introducing Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services integration to achieve higher degree of conceptualization of Cognitive Medium Access Control and SDR Services utilized in the upper stack layers of OSI and meet a common goal such as smart radio environment utilization locally or globally, QoE, higher wireless network performance. The technologies that may be employed are current technologies such as Virtual Machines and Network Function Virtualization, Network Chaining. The concept of Cognitive Medium Access Control and SDR Services and their conceptualization to an abstract level covering the  seven layers of  the OSI stack and respond to edge requirements of the huge 5G, 6G heterogeneous networks. Cognitive Medium Access Control and SDR Services integration and deployment on all the OSI layers and 5G and 6G huge and heterogeneous network would require high level of scheduling and interaction. High level of scheduling and interaction will trigger eventually adaptation “knobs” within the local and global system would necessiate high level of interaction on all OSI stack layers of Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services in this reference network framework for cross-operator, cross-network operability context generation, QoE, radio network demands, cross-operator, cross-border. This dissertation and the subsequent research work introduced a framework of this Cognitive Medium Access Control and SDR Services and application and Abstract Cognitive Medium Access Control and SDR Services integration and deployment framework. In particular in this framework optimal algorithms have been introduced to implement protocols for solving diverse issues such as native Cognitive Medium Access Control, Software Defined Networking coordination, Sustainability, Broadcasting, Security, massive MIMO technology and imperfect CSI for reciprocal channels, Multiscale Decision Making with Machine Learning and Game Theory for Efficient Sensing Scheduling and Spectrum Sensing Control, Enhanced Signal Processing and Radio Environment Maps for  the Cognitive Radio Network and Cognitive Radio Cloud. As huge heterogeneous next generation wireless networks  is  a necessity, proposing such high level design of low layer services and applications cross-layer, cross-network, cross-operator would make feasible 5G and 6G - Space-Air-Ground-Sea integrated communication and wireless tactile network- and allow them to evolve.Η πρωτοτυπία της παρούσας διατριβής έγκειται στο ότι πρότεινε και περιέγραψε ένα πλαίσιο δικτύου αναφοράς για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Υπηρεσίες και για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Αφηρημένες Υπηρεσίες  και ολοκλήρωση τους και ανάπτυξη τους σε όλα κατά OSI επίπεδα για ετερογενή ασύρματα δίκτυα. Η αναγκαιότητα της ύπαρξης και αφαίρεσης  υπηρεσιών χαμηλού επιπέδου λειτουργιών όπως και εφαρμογών τους στα πλαίσια του Γνωστικού Κύκλου είναι κάποια από τα κύρια σημεία που η παρούσα διατριβή διακηρύσσει ενώ παράλληλα προτείνει αλγορίθμους που να ανταποκρίνονται σε ποικίλα θέματα που αφορούν τα Γνωστικά Ασύρματα Δίκτυα και Νέφος. Οι τρέχουσες προκλήσεις στα Γνωστικά Ασύρματα Δίκτυα είναι η αποθήκευση μεγάλου όγκου πληροφοριών και η επεξεργασία τους σε πραγματικό χρόνο και η ανταλλλαγή τους μεταξύ των ασύρματων κόμβων του δικτύου σε αντιδιαστολή του περιορισμένου χώρου αποθήκευσης και της υπολογιστικής ικανότητας των Γνωστικών Συσκευών που απαιτούν πρόσθετες ικανότητες. Το Νέφος Γνωστικού Ασύρματου Δικτύου αποτελεί μια υποδομή που αποτελείται από κινητούς κόμβους και το νέφος των οποίων ο πρωταρχικός στόχος είναι να διατηρούν ενημερωμένη την κατάσταση του δικτύου και της διαθεσιμότητας του φάσματος. Απαιτητικές εργασίες όπως η επεξεργασία σήματος μπορούν να ανατεθούν στο ισχυρούς κόμβους τοπικά επιτρέποντας στο δίκτυο τοπικά να αυτορυθμιστεί. Επιτρέποντας στο δίκτυο να αυτορυθμιστεί και αναπτυχθεί τοπικά, τεράστια ετερογενή ασύρματα δίκτυα όπως τα δίκτυα 5G και 6G  μπορούν να αναπτυχθούν και να μεταφέρουν την δυναμική πρόσβαση στο φάσμα και τον έλεγχο στους τελικούς χρήστες και τις απαιτήσεις της απόδοσης του ασύρματου δικτύου. Αυτορυθμιζόμενα Γνωστικά Ασύρματα Δίκτυα σε ένα τεράστιο ετερογενές ασύρματο δίκτυο μαζί με δυναμική πρόσβαση στο φάσμα και μεταβίβαση του ελέγχου κατά απαίτηση ή μη σε απάντηση των απαιτήσεων του δικτύου σε πραγματικό χρόνο μπορούν να πραγματοποιηθούν με υψηλό ποσοστό αφαίρεσης των Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR Υπηρεσίες και για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR Αφηρημένες Υπηρεσίες και ολοκλήρωση τους και ανάπτυξη τους σε όλα κατά OSI επίπεδα για ετερογενή ασύρματα δίκτυα και ανάπτυξη τους ανεξάρτητα συστήματος, δικτύου, παρόχου. Κεντρικός συντονισμός εφαρμόζεται για να ενεργοποιήσει τοπικά νομαδικά δίκτυα όπως και προκειμένου να αυτορυθμιστούν και να επιτευχθούν μετρικές ποιότητας υπηρεσίας και δικτύου όπως και του οργανισμού. Η ομαδοποίηση του δικτύου όσον αφορά την Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Υπηρεσίες και για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Αφηρημένες Υπηρεσίες προκειμένου να είναι εφικτές. Η Τεχνητή Νοημοσύνη και άλλες τεχνολογίες αποδίδουν και μπορούν να εφαρμοστούν κατανεμημένα. Οι χάρτες του φάσματος συχνοτήτων είναι μια δυναμική τεχνολογία προς αυτή την κατεύθυνση. Η μετατόπιση του φόρτου σε ισχυρούς κόμβους τοπικά αυξάνει την ικανότητα παροχής ποιότητας υπηρεσίας που είναι απαραίτητη για 5G, 6G δίκτυα για αστικά και μη αστικά περιβάλλοντα συχνοτήτων. Μια νέα μαθηματική μέθοδος για δημιουργία μαθηματικών παιγνίων προτάθηκε δηλ. Μια μέθοδος για γένεση παιγνίων χωρίς ανταλλαγή μηνυμάτων μέσω των παικτών και ένα αντίστοιχο μοντέλο παιγνίου ως εφαρμογή στην ασφάλεια των Γνωστικών Ασύρματων Δικτύων και Νέφους. Άλλα παίγνια που επιτυγχάνουν Nash Equilibrium προτάθηκαν. Ντετερμινιστικά Αυτόματα και Μηχανική Μάθηση προτάθηκαν όπως και μαθηματικοί τύποι προκειμένου να εφαρμοστούν σε δικτυακά πρωτόκολλα για μεγάλο ετερογενές δίκτυο, για πρόσβαση στο μέσο, εκπομπές, δίκτυα αισθητήρων, μαθηματικά συστημικά μοντέλα και σταθερής κατάστασης κατά Lyapunov συστήματα, αλγόριθμοι για προηγμένη επεξεργασία σήματος σε CRN-SDR και μαθηματικά μοντέλα για μη αμοιβαία κανάλια μετάδοσης για massive MIMO και αντίστοιχα LMMSE φίλτρα προτάθηκαν στην διατριβή",Αλγόριθμοι για γνωστικά ασύρματα δίκτυα και νέφος γνωστικών ασύρματων δικτύων,,'National Documentation Centre (EKT)',10.12681/eadd/47924,,core
326230945,2020-06-18T00:00:00,"Neural Networks (NNs) have increasingly apparent safety implications
commensurate with their proliferation in real-world applications: both
unanticipated as well as adversarial misclassifications can result in fatal
outcomes. As a consequence, techniques of formal verification have been
recognized as crucial to the design and deployment of safe NNs. In this paper,
we introduce a new approach to formally verify the most commonly considered
safety specification for ReLU NNs -- i.e. polytopic specifications on the input
and output of the network. Like some other approaches, ours uses a relaxed
convex program to mitigate the combinatorial complexity of the problem.
However, unique in our approach is the way we exploit the geometry of neuronal
activation regions to further prune the search space of relaxed neuron
activations. In particular, conditioning on neurons from input layer to output
layer, we can regard each relaxed neuron as having the simplest possible
geometry for its activation region: a half-space.This paradigm can be leveraged
to create a verification algorithm that is not only faster in general than
competing approaches, but is also able to verify considerably more safety
properties. For example, our approach completes the standard MNIST verification
test bench 2.7-50 times faster than competing algorithms while still proving
14-30% more properties. We also used our framework to verify the safety of a
neural network controlled autonomous robot in a structured environment, and
observed a 1900 times speed up compared to existing methods","Effective Formal Verification of Neural Networks using the Geometry of
  Linear Regions",http://arxiv.org/abs/2006.10864,,,,core
327075346,2020-07-24T00:00:00,"We consider an autonomous exploration problem in which a range-sensing mobile
robot is tasked with accurately mapping the landmarks in an a priori unknown
environment efficiently in real-time; it must choose sensing actions that both
curb localization uncertainty and achieve information gain. For this problem,
belief space planning methods that forward-simulate robot sensing and
estimation may often fail in real-time implementation, scaling poorly with
increasing size of the state, belief and action spaces. We propose a novel
approach that uses graph neural networks (GNNs) in conjunction with deep
reinforcement learning (DRL), enabling decision-making over graphs containing
exploration information to predict a robot's optimal sensing action in belief
space. The policy, which is trained in different random environments without
human intervention, offers a real-time, scalable decision-making process whose
high-performance exploratory sensing actions yield accurate maps and high rates
of information gain","Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning
  on Graphs",http://arxiv.org/abs/2007.12640,,,,core
323921387,2020-06-08T00:00:00,"The wide deployment of deep neural networks, though achieving great success
in many domains, has severe safety and reliability concerns. Existing
adversarial attack generation and automatic verification techniques cannot
formally verify whether a network is globally robust, i.e., the absence or not
of adversarial examples in the input space. To address this problem, we develop
a global robustness verification framework with three components: 1) a novel
rule-based ``back-propagation'' finding which input region is responsible for
the class assignment by logic reasoning; 2) a new network architecture Sliding
Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a
region-based global robustness verification (RGRV) approach. Moreover, we
demonstrate the effectiveness of our approach on both synthetic and real
datasets",Global Robustness Verification Networks,http://arxiv.org/abs/2006.04403,,,,core
200810619,2019-03-11T00:00:00,"Planet formation simulations are capable of directly integrating the
evolution of hundreds to thousands of planetary embryos and planetesimals, as
they accrete pairwise to become planets. In principle such investigations allow
us to better understand the final configuration and geochemistry of the
terrestrial planets, as well as to place our solar system in the context of
other exosolar systems. These simulations, however, classically prescribe
collisions to result in perfect mergers, but computational advances have begun
to allow for more complex outcomes to be implemented. Here we apply machine
learning to a large but sparse database of giant impact studies, streamlining
simulations into a classifier of collision outcomes and a regressor of
accretion efficiency. The classifier maps a 4-Dimensional parameter space
(target mass, projectile-to-target mass ratio, impact velocity, impact angle)
into the four major collision types: merger, ""graze-and-merge"", ""hit-and-run"",
and disruption. The definition of the four regimes and their boundary is fully
data-driven; the results do not suffer from any model assumption in the
fitting. The classifier maps the structure of the parameter space and provides
insights about the outcome regimes. The regressor is a neural network which is
trained to closely mimic the functional relationship between the 4-D space of
collision parameters, and a real-variable outcome, the mass of the largest
remnant. This work is a prototype of a more complete surrogate model, based on
extended sets of simulations (""big data""), that will quickly and reliably
predict specific collision outcomes for use in realistic N-body dynamical
studies of planetary formation.Comment: 18 pages, 7 figures, 3 tables. Accepted for publication on Ap",'American Astronomical Society',"Realistic On-The-Fly Outcomes of Planetary Collisions: Machine Learning
  Applied to Simulations of Giant Impacts",10.3847/1538-4357/ab0e8a,http://arxiv.org/abs/1903.04507,,core
186318291,2019-01-20T00:00:00,"We introduce a new logic programming language T-PRISM based on tensor
embeddings. Our embedding scheme is a modification of the distribution
semantics in PRISM, one of the state-of-the-art probabilistic logic programming
languages, by replacing distribution functions with multidimensional arrays,
i.e., tensors. T-PRISM consists of two parts: logic programming part and
numerical computation part. The former provides flexible and interpretable
modeling at the level of first order logic, and the latter part provides
scalable computation utilizing parallelization and hardware acceleration with
GPUs. Combing these two parts provides a remarkably wide range of high-level
declarative modeling from symbolic reasoning to deep learning. To embody this
programming language, we also introduce a new semantics, termed tensorized
semantics, which combines the traditional least model semantics in logic
programming with the embeddings of tensors. In T-PRISM, we first derive a set
of equations related to tensors from a given program using logical inference,
i.e., Prolog execution in a symbolic space and then solve the derived equations
in a continuous space by TensorFlow. Using our preliminary implementation of
T-PRISM, we have successfully dealt with a wide range of modeling. We have
succeeded in dealing with real large-scale data in the declarative modeling.
This paper presents a DistMult model for knowledge graphs using the FB15k and
WN18 datasets.Comment: In AAAI-19 Workshop on Network Interpretability for Deep Learnin",,A tensorized logic programming language for large-scale data,,http://arxiv.org/abs/1901.08548,,core
186320024,2019-01-29T00:00:00,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy",,Structural Material Property Tailoring Using Deep Neural Networks,,http://arxiv.org/abs/1901.10281,,core
286393048,"November 20, 2019","The NASA Bioculture System is an advanced cell culture closed-loop system containing highly automated flowpaths designed to conduct long term biology experiments on ISS with earth remote controllable medium flow, temperature, gas composition, medium exchange, cell sampling and fixation. This technology was already demonstrated with successful cardiomyocyte and osteocyte cultures experiments onboard the ISS and is now supporting NASA PI science. The Bioculture System, however, can only support 10 cassettes with disposable flowpaths, each containing a single hollow fiber bioreactor with a culture capacity of about 2ml. This constraint not only severely limits the number of investigators that can conduct experiments in space, but also subjects the experiments to limitations in the number of replicates and conditions that can be studied. To address these limitations, we sought a novel design solution to maximize the number of separate bioreactor cultures and volume that can be conducted simultaneously. To this end we designed, prototyped, and are now testing a six-Vitvo 3D Matrix 2ml bioreactor insert that replaces the conventional Bioculture System hollow fiber bioreactor. This design will allow the Bioculture System to support up to 60 different bioreactors and samples at once. Specifically, the novel gas-tight containment housing insert contains six COTS Rigenerand VITVO bioreactors stacked on each side of a heat sink powered by the existing heating element and pair of temperature sensors. Medium will be distributed into each bioreactor's cell-free chamber via its built-in Luer connector, then across the 3D matrix to the cell chamber, dissipating laminar flow and limiting fluid shear stresses that might mechanostimulate cell cultures. Gas (5% CO2 in air) will be supplied directly to the bioreactor gas-tight housing for exchange via the bioreactor flat-surface gas-permeable membranes, eliminating the need for the existing Bioculture System cassette oxygenator. If successfully implemented on ISS, this new multi-bioreactor insert for the Bioculture System has the potential to make real-time cell science experimentation in space more efficient and accessible to more investigators",,"Design, Prototyping, and Testing of a Novel Flowpath with an Array of Six 3D Matrix Vitvo Bioreactors for the NASA Bioculture System",,https://core.ac.uk/download/pdf/286393048.pdf,,core
334822962,2019-06-10T00:00:00,"The space of Artificial Intelligence entities is dominated by conversational
bots. Some of them fit in our pockets and we take them everywhere we go, or
allow them to be a part of human homes. Siri, Alexa, they are recognised as
present in our world. But a lot of games research is restricted to existing in
the separate realm of software. We enter different worlds when playing games,
but those worlds cease to exist once we quit. Similarly, AI game-players are
run once on a game (or maybe for longer periods of time, in the case of
learning algorithms which need some, still limited, period for training), and
they cease to exist once the game ends. But what if they didn't? What if there
existed artificial game-players that continuously played games, learned from
their experiences and kept getting better? What if they interacted with the
real world and us, humans: live-streaming games, chatting with viewers,
accepting suggestions for strategies or games to play, forming opinions on
popular game titles? In this paper, we introduce the vision behind a new
project called Thyia, which focuses around creating a present, continuous,
`always-on', interactive game-player.Comment: 8 pages, 1 figure, accepted at IEEE COG 201",,Project Thyia: A Forever Gameplayer,,http://arxiv.org/abs/1906.04023,,core
346547471,2019-01-01T00:00:00,"Conservation researchers require low-cost access to acoustic monitoring technology. However, affordable tools are often constrained to short-term studies due to high energy consumption and limited storage. To enable long-term monitoring, energy and space efficiency must be improved on such tools. This paper describes the development and deployment of three acoustic detection algorithms that reduce the power and storage requirements of acoustic monitoring on affordable, open-source hardware. The algorithms aim to detect bat echolocation, to search for evidence of an endangered cicada species, and also to collect evidence of poaching in a protected nature reserve. The algorithms are designed to run on AudioMoth: a low-cost, open-source acoustic monitoring device, developed by the authors and widely adopted by the conservation community. Each algorithm addresses a detection task of increasing complexity, implementing extra analytical steps to account for environmental conditions such as wind, analysing samples multiple times to prevent missed events, and incorporating a hidden Markov model for sample classification in both the time and frequency domain. For each algorithm, we report on real-world deployments carried out with partner organisations and also benchmark the hidden Markov model against a convolutional neural network, a deep-learning technique commonly used for acoustics. The deployments demonstrate how acoustic detection algorithms extend the use of low-cost, open-source hardware and facilitate a new avenue for conservation researchers to perform large-scale monitoring",'MDPI AG',"Deploying acoustic detection algorithms on low-cost, open-source acoustic sensors for environmental monitoring",,,,core
478143273,2019-10-03T07:51:50,"The book """"Simulation and Gaming"""" discusses the following topics and research areas: game-based methods of problem solution and data processing, analysis, and information mining; educational games and game features, including game characteristics, story, mechanics, and methodology; development of integrated games tasked with helping students in interpreting, translating, and manipulating the field of kinematics through formal presentations; possibility of research integration through real and practical examples and games as well, in the field of physics; analysis of game engines from various aspects such as modularity, performance, and usability; virtual reality (VR) and interaction mechanisms used for three-dimensional (3D) game development; analysis, development, design, implementation, and evaluation of the simulation model in the field of engineering and metallurgy, according to ADDIE model; concept of computational thinking, with an accent on its inclusion in compulsory education; overview of the current prominence of AI simulation based in the gaming leisure industry, mainly for research purposes in the context of gambling and forecasting of online casino patron's churn behavior; innovative modeling and simulation approach using newly proposed advanced game-based mathematical framework, unified game-based acquisition framework, and a set of war-gaming engines to address the challenges for acquisition of future space systems; modification of simulation of a complex system and a physics model through programming, achieved with a block-based programming language",'IntechOpen',Simulation and Gaming,10.5772/intechopen.69391,,,core
186318792,2019-02-21T00:00:00,"The way how recurrently connected networks of spiking neurons in the brain
acquire powerful information processing capabilities through learning has
remained a mystery. This lack of understanding is linked to a lack of learning
algorithms for recurrent networks of spiking neurons (RSNNs) that are both
functionally powerful and can be implemented by known biological mechanisms.
Since RSNNs are simultaneously a primary target for implementations of
brain-inspired circuits in neuromorphic hardware, this lack of algorithmic
insight also hinders technological progress in that area. The gold standard for
learning in recurrent neural networks in machine learning is back-propagation
through time (BPTT), which implements stochastic gradient descent with regard
to a given loss function. But BPTT is unrealistic from a biological
perspective, since it requires a transmission of error signals backwards in
time and in space, i.e., from post- to presynaptic neurons. We show that an
online merging of locally available information during a computation with
suitable top-down learning signals in real-time provides highly capable
approximations to BPTT. For tasks where information on errors arises only late
during a network computation, we enrich locally available information through
feedforward eligibility traces of synapses that can easily be computed in an
online manner. The resulting new generation of learning algorithms for
recurrent neural networks provides a new understanding of network learning in
the brain that can be tested experimentally. In addition, these algorithms
provide efficient methods for on-chip training of RSNNs in neuromorphic
hardware.Comment: We changed in this version 2 of the paper the name of the new
  learning algorithms to e-prop, corrected minor errors, added details --
  especially for resulting new rules for synaptic plasticity, edited the
  notation, and included new results for TIMI",,"Biologically inspired alternatives to backpropagation through time for
  learning in recurrent neural nets",,http://arxiv.org/abs/1901.09049,,core
211027196,2019-01-01T00:00:00,"This paper presents Scalpel-CD, a first-of-its-kind system that leverages both human and machine intelligence to debug noisy labels from the training data of machine learning systems. Our system identifies potentially wrong labels using a deep probabilistic model, which is able to infer the latent class of a high-dimensional data instance by exploiting data distributions in the underlying latent feature space. To minimize crowd efforts, it employs a data sampler which selects data instances that would benefit the most from being inspected by the crowd. The manually verified labels are then propagated to similar data instances in the original training data by exploiting the underlying data structure, thus scaling out the contribution from the crowd. Scalpel-CD is designed with a set of algorithmic solutions to automatically search for the optimal configurations for different types of training data, in terms of the underlying data structure, noise ratio, and noise types (random vs. structural). In a real deployment on multiple machine learning tasks, we demonstrate that Scalpel-CD is able to improve label quality by 12.9% with only 2.8% instances inspected by the crowd",'Association for Computing Machinery (ACM)',Scalpel-CD: leveraging crowdsourcing and deep probabilistic modeling for debugging noisy training data,10.1145/3308558.3313599,,,core
228089876,2019-01-01T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.QC 20190916Factories of the Future (FACT",'Institute of Electrical and Electronics Engineers (IEEE)',Vpe : Variational policy embedding for transfer reinforcement learning,10.1109/ICRA.2019.8793556,,,core
153377480,2019-01-30T00:00:00,"The rapid uptake of mobile devices and the rising popularity of mobile
applications and services pose unprecedented demands on mobile and wireless
networking infrastructure. Upcoming 5G systems are evolving to support
exploding mobile traffic volumes, agile management of network resource to
maximize user experience, and extraction of fine-grained real-time analytics.
Fulfilling these tasks is challenging, as mobile environments are increasingly
complex, heterogeneous, and evolving. One potential solution is to resort to
advanced machine learning techniques to help managing the rise in data volumes
and algorithm-driven applications. The recent success of deep learning
underpins new and powerful tools that tackle problems in this space.
  In this paper we bridge the gap between deep learning and mobile and wireless
networking research, by presenting a comprehensive survey of the crossovers
between the two areas. We first briefly introduce essential background and
state-of-the-art in deep learning techniques with potential applications to
networking. We then discuss several techniques and platforms that facilitate
the efficient deployment of deep learning onto mobile systems. Subsequently, we
provide an encyclopedic review of mobile and wireless networking research based
on deep learning, which we categorize by different domains. Drawing from our
experience, we discuss how to tailor deep learning to mobile environments. We
complete this survey by pinpointing current challenges and open future
directions for research",,Deep Learning in Mobile and Wireless Networking: A Survey,,http://arxiv.org/abs/1803.04311,,core
231783754,2019-04-01T00:00:00,"© 2004-2012 IEEE. Composition and parameterization of multicomponent predictive systems (MCPSs) consisting of chains of data transformation steps are a challenging task. Auto-WEKA is a tool to automate the combined algorithm selection and hyperparameter (CASH) optimization problem. In this paper, we extend the CASH problem and Auto-WEKA to support the MCPS, including preprocessing steps for both classification and regression tasks. We define the optimization problem in which the search space consists of suitably parameterized Petri nets forming the sought MCPS solutions. In the experimental analysis, we focus on examining the impact of considerably extending the search space (from approximately 22000 to 812 billion possible combinations of methods and categorical hyperparameters). In a range of extensive experiments, three different optimization strategies are used to automatically compose MCPSs for 21 publicly available data sets. The diversity of the composed MCPSs found is an indication that fully and automatically exploiting different combinations of data cleaning and preprocessing techniques is possible and highly beneficial for different predictive models. We also present the results on seven data sets from real chemical production processes. Our findings can have a major impact on the development of high-quality predictive models as well as their maintenance and scalability aspects needed in modern applications and deployment scenarios. Note to Practitioners - The extension of Auto-WEKA to compose and optimize multicomponent predictive systems (MCPSs) developed as part of this paper is freely available on GitHub under GPL license, and we encourage practitioners to use it on a broad variety of classification and regression problems. The software can either be used as a blackbox - where search space is made of all possible WEKA filters, predictors, and metapredictors (e.g., ensembles) - or as an optimization tool on a subset of preselected machine learning methods. The application has a graphical user interface, but it can also run from command line and can be embedded in any project as a Java library. There are three main outputs once an Auto-WEKA run has finished: 1) the trained MCPS ready to make predictions on unseen data; 2) the WEKA configuration (i.e., parameterized components); and 3) the Petri net in a Petri Net Markup Language format that can be analyzed using any tool supporting this standard language. There are, however, some practical considerations affecting the quality of the results that must be taken into consideration, such as the CPU time budget or the search starting point. These are extensively discussed in this paper",'Institute of Electrical and Electronics Engineers (IEEE)',Automatic Composition and Optimization of Multicomponent Predictive Systems with an Extended Auto-WEKA,10.1109/TASE.2018.2876430,http://hdl.handle.net/10453/131130,"[{'title': 'IEEE Transactions on Automation Science and Engineering', 'identifiers': ['1545-5955', 'issn:1545-5955']}]",core
386934077,2019-01-01T00:00:00,"The article attempts to offer answers to some provocative questions.
Is there in all cases a linearity in the deployment of certain mandatory features of the objective side of a specific corpus delicti (formal components), in particular, space as a crime scene, time, method, etc.? Could some of the mandatory attributes of the objective side of a specific criminal offense continue to be realized in time and space after the moment of its finish?
Is it possible in the future to influence the nature and degree of public danger of a crime committed in the past after a considerable period of time of its completion?
The author concludes that time in criminal law is a relational category.
This allows the dependence of the characteristics of space and time on the nature and method of interaction of objects, events, properties and relations.
Matter, space and time form a system of certain interactions. They depend on mutual movement. Time can be multidimensional in the same way as space. Thus, time can be determined using not one, but several quantities.
In certain cases, time in criminal law can be isotropic, that is, equal in all its possible directions of movement. In some cases, other mandatory attributes of the objective side of a specific criminal offense may continue to be realized in time and space even after the moment of its completion.
The rules of the current Criminal Code of Ukraine and the practice of their application indicate on possibility in the future to influence the nature and degree of public danger of a crime committed in the past, including after a significant period of time after its completion.
Thus, the author concludes that the future in criminal law can influence the past.
In the examples considered in the article with smuggling and escalation of theft into robbery, the author admits the possibility of the existence of a slight curvature of time-space, or a more or less long loop of time.
However, these cases are not time travel.
It is possible to return to the past only if history has already recorded such a return in any way. For example, an incomprehensible event occurred, which could not be properly explained, in due time. The possibility of such a return would continue to be rather inconvenient for criminal law and other branches shattering the tenet of human being’ free will, on which all theories of legal responsibility are based. Nowadays, such freedom is declared only because we cannot fully predict subsequent behavior of human being.
However, with the uprising of artificial intelligence, which is capable of processing huge contents of information regarding a specific subject, as well as due to significant achievements in the field of neurobiology, there is a real perspective not only to predict the future behavior of any person, but also to manage it (for example, when consuming goods and services, making political choices in a referendum, etc.). This creates new challenges to the legal system and human civilization as a whole.В статті здійснено спробу надати відповіді на питання, чи у всіх випадках має місце лінійність розгортання окремих обов’язкових ознак об’єктивної сторони конкретного складу злочину, зокрема, простору як місця вчинення злочину, часу, способу тощо, чи можуть продовжувати реалізуватися у часі та просторі інші, крім діяння та наслідку, обов’язкові ознаки об’єктивної сторони конкретного кримінального правопорушення вже після того, як настав момент його закінчення, чи є можливим у майбутньому вплинути на характер і ступінь суспільної небезпечності вчиненого у минулому злочину, в тому числі через значний проміжок часу після його закінчення.
Автор приходить до висновку про те, що час у кримінальному праві є реляційною категорією, і це допускає залежність характеристик простору і часу від характеру та способу взаємодії об’єктів, подій, властивостей та відносин. Матерія, простір і час утворюють систему певних відносин, вони залежать від взаємного руху, час може бути багатовимірним так само, як і простір. Таким чином, час може бути визначений за допомогою не однієї, а декількох величин. Час у кримінальному праві у певних випадках може бути ізотропним, тобто рівноправним у всіх своїх можливих напрямках руху. В окремих випадках можуть продовжувати реалізуватися у часі та просторі інші, крім діяння та наслідку, обов’язкові ознаки об’єктивної сторони конкретного кримінального правопорушення і це може мати місце вже після того, як настав момент його закінчення. Положення чинного КК України та практика їх застосування вказують на можливість у майбутньому вплинути на характер і ступінь суспільної небезпечності злочину, який був вчинений у минулому, в тому числі через значний проміжок часу після його закінчення. 
Таким чином, автор приходить до висновку про те, що майбутнє у кримінальному праві може впливати на минуле. В розглянутих у статті прикладах з контрабандою та переростанням крадіжки у розбій, автор припускає можливість існування незначного викривлення часу-простору, або більш чи менш тривалої петлі часу. При цьому згадані випадки не є подорожами у часі.
Повернутися у минуле можливо лише у тому випадку, якщо історія вже зафіксувала будь-яким чином таке повернення (наприклад, сталася незрозуміла подія, яку на той час не змогли належним чином пояснити). Можливість такого повернення продовжувала би доволі незручно для кримінального права та інших галузей розхитувати постулат про свободу волі людини, на якому базуються всі теорії юридичної відповідальності. Сьогодні про таку свободу стверджується лише тому, що ми не можемо повною мірою передбачити наступну поведінку тієї чи іншої особи. Втім, за появу штучного інтелекту, який є здатним обробляти величезні обсяги інформації відносно конкретної особи, а так само завдяки значним проривам у нейробіології та біоінженерії, виникає реальна перспектива не тільки передбачати майбутню поведінку будь-якої людини, але й керувати нею (наприклад, під час споживання товарів і послуг, здійснення політичного вибору на референдумі тощо), що утворює нові виклики правовій системі та людський цивілізації в цілому.В статье предпринята попытка предложить ответы на такие вопросы как, во всех ли случаях имеет место линейность развертывания отдельных обязательных признаков объективной стороны конкретного состава преступления, в частности, пространства как места совершения преступления, времени, способа и т.д., могут ли продолжать реализоваться во времени и пространстве иные, кроме деяния и его последствия, обязательные признаки объективной стороны конкретного уголовного правонарушения уже после того, как наступил момент его окончания, возможно ли в будущем повлиять на характер и степень общественной опасности совершенного в прошлом преступления, в том числе через значительный промежуток времени после его окончания.
Автор приходит к выводу о том, что время в уголовном праве является реляционной категорией, и это допускает зависимость характеристик пространства и времени от характера и способа взаимодействия объектов, событий, свойств и отношений. Материя, пространство и время образуют систему определенных взаимодействий, они зависят от взаимного движения, время может быть многомерным так же, как и пространство. Таким образом, время может быть определено с помощью не одной, а нескольких величин. Время в уголовном праве в определенных случаях может быть изотропным, то есть равноправным во всех своих возможных направлениях движения. В отдельных случаях могут продолжать реализоваться во времени и пространстве иные, кроме деяния и последствия, обязательные признаки объективной стороны конкретного уголовного преступления и это может иметь место уже после того, как наступил момент его окончания. Положения действующего УК Украины и практика их применения указывают на возможность в будущем повлиять на характер и степень общественной опасности преступления, совершенного в прошлом, в том числе через значительный промежуток времени после его окончания.
Таким образом, автор приходит к выводу о том, что будущее в уголовном праве может влиять на прошлое. В рассмотренных в статье примерах с контрабандой и перерастанием кражи в разбой автор допускает возможность существования незначительного искривления времени-пространства, или более или менее длительной петли времени. При этом упомянутые случаи не являются путешествиями во времени.
Вернуться в прошлое возможно лишь в том случае, если история уже зафиксировала любым способом такое возвращение (например, произошло непонятное событие, которое в свое время не смогли должным образом объяснить). Возможность такого возвращения продолжила бы довольно неудобно для уголовного права и других отраслей расшатывать постулат о свободе воли человека, на чем базируются все теории юридической ответственности. Сегодня о такой свободе утверждается только потому, что мы не можем в полной мере предсказать последующее поведение того или иного лица. Впрочем, с появлением искусственного интеллекта, способного обрабатывать огромные объемы информации в отношении конкретного субъекта, а так же, благодаря значительным достижениям в области нейробиологии и биоинженерии, возникает реальная перспектива не только предвидеть будущее поведение любого человека, но и управлять им (например, при потреблении товаров и услуг, осуществлении политического выбора на референдуме и т.д.), что создает новые вызовы правовой системе и человеческой цивилизации в целом",,The influence of the future on the past (criminal law aspect),,,,core
232648119,2019-06-05T07:00:00,"Computational imaging system design involves the joint optimization of hardware and software to deliver high fidelity images to a human user or artificial intelligence (AI) algorithm. For example, in medical tomography CAT scanners produce non-invasively cross-sectional images of the patient’s organs and then medical professionals or, increasingly, automated recognition systems perform diagnosis and decide upon a course of treatment. We refer to this operation of AI as image interpretation.
 
This talk is about a different paradigm where machine learning (ML) is used at the step of image formation itself, i.e. for image reconstruction rather than interpretation. The ML algorithm, typically implemented as a deep neural network (DNN), is programmed using physically generated or rigorously simulated examples of objects and their associated signals produced on the sensor (or camera.) The training phase consists of adjusting the connection weights of the DNN until it becomes possible, given the sensor signal from a hitherto unseen object, for the DNN to yield an accurate estimate of the object’s spatial structure.
The ML approach to solving inverse problems in such fashion has its roots in optimization methods employed long before in computational imaging, compressed sensing and dictionaries in particular. By replacing the proximal gradient step of the optimization with a DNN [K. Gregor & Y. LeCun, ICML 2010], it becomes possible to learn priors other than sparsity, and restrict the object class almost arbitrarily to facilitate the solution of “hard” inverse problems, e.g. highly ill-posed and highly noisy at the same time. Moreover, execution becomes very fast because pre-trained DNNs mostly consist of forward computations which can easily be run at real time, whereas traditional compressed sensing optimization routines are generally iterative. DNN training is time consuming too, but it is only run up front while developing the algorithm; it is not a burden during operation. Unfortunately, however, with the DNN approach some of the nice properties of compressed sensing are lost, most notably convexity.
In this talk we will review these basic developments and then discuss in detail their application to the specific problem of phase retrieval in lensless (free-space propagation) or defocused imaging systems. More specifically, we will investigate the impact of the power spectral density of the training example database on the quality of the reconstructions. We will review a sequence of papers where we first ignored this problem [A. Sinha et al, Optica 4:1117, 2017], then improved it in an ad hoc way by pre-modulation of the training examples [Li Shuai et al, Opt. Express 26:29340, 2018] and finally devised a dual-band approach where the signal is first separated into its low- and high-frequency components, their respective reconstructions are obtained by two DNNs trained separately and then re-composed by a third “synthesizer” DNN [Deng Mo et al, arXiv:1811.07945]. We will explain why each new attempt improves resolution and overall fidelity through progressively more balanced treatment of the spatial frequency spectrum.
We will also discuss implications of this method for phase retrieval under extremely low-photon (too dark) conditions [A. Goy et al, Phys. Rev. Lett. 121:243902, 2018] other related inverse problems, e.g. super resolution (too far or too small), and imaging through diffusers (too foggy.",ECI Digital Archives,"Too far, too small, too dark, too foggy: On the use of machine learning for computational imaging problems",,https://core.ac.uk/download/232648119.pdf,,core
343503308,2019-01-01T00:00:00,"© 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes,creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.In  general,  the  availability  of  enough  real  data  from  real  fog  computing  scenarios  to  produce  accurate  Machine  Learning (ML) models is rarely ensured since new equipment, techniques, etc., are continuously being deployed in  the  field.  Although  an  option  is  to  generate  data  from  simulation  and  lab  experiments,  such  data  could  not  cover  the  whole  features  space,  which  would  translate  into  ML  models  inaccuracies.  In  this  paper,  we  propose  a self-learning approach to facilitate ML deployment in real scenarios. A dataset for ML training can be initially populated  based  on  the  results  from  simulation  and  lab  experiments  and  once  ML  models  are  generated,  ML  re-training  can  be  performed  after  inaccuracies  are  detected  to  improve  their  precision.  Illustrative  numerical  results  show  the  benefits  from  the  proposed  self-learning  approach  for  two  general  use  cases  of  regression  and  classification.This work was partially supported by the EC through the METRO-HAUL project (G.A. nº 761727), from the AEI/FEDER TWINS project (TEC2017-90097-R), and from the Catalan ICREA Institution.Peer ReviewedPostprint (author's final draft",'Institute of Electrical and Electronics Engineers (IEEE)',Distributed and centralized options for self-learning,10.1109/ICTON.2019.8840514.,https://core.ac.uk/download/343503308.pdf,,core
231796070,2019-01-01T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.QC 20190916Factories of the Future (FACT",'Institute of Electrical and Electronics Engineers (IEEE)',Vpe : Variational policy embedding for transfer reinforcement learning,10.1109/ICRA.2019.8793556,,,core
265316601,2019-06-19,"The aim of my research was to develop a digital mediation system with urban data for a pedestrian immersed in the city, a link based on digital technologies to design, analyze, represent urban space and access information on this urban space. Augmented Reality is one of the tools allowing this mediation whose critical element is the location of the pedestrian and more precisely the pose calculation of the camera it carries.Thus, the main focus of my work is geolocation on site using spatial data of different dimensions. I was interested in an upstream phase that requires the implementation of data models to keep track of spatial data changes. Finally, I touched on some uses of geolocation and pose calculation. I conclude this report by presenting my research perspectives on digital mediation with urban data for pedestrians.Le but de ma recherche a été de mettre au point un système de médiation numérique avec des données urbaines pour un piéton immergé dans la ville, un lien basé sur des technologies numériques pour concevoir, analyser, représenter l’espace urbain et accéder à des informations sur cet espace urbain. La réalité augmentée est un des outils permettant cette médiation dont l’élément critique est la localisation du piéton et plus précisément le calcul de pose de la caméra qu’il transporte.Ainsi, l’axe principal de mon travail est la géolocalisation sur site à l’aide de données spatiales de différentes dimensions. Je me suis intéressée à une phase amont qui nécessite la mise en place de modèles de données pour garder trace des modifications des données spatiales. J’ai enfin abordé quelques usages de la géolocalisation et du calcul de pose. Je conclus ce mémoire en présentant mes perspectives de recherches vers une médiation numérique avec des données urbaines pour le piéton",HAL CCSD,Méthodes pour la géolocalisation du piéton sur site - vers une médiation numérique avec les données urbaines,,https://core.ac.uk/download/pdf/265316601.pdf,,core
322673529,2019,"Abnormality detection in medical images is a one-class classification problem for which typical methods use variants of kernel principal component analysis or one-class support vector machines. However, in practical deployment scenarios, many such methods are sensitive to the outliers present in the imperfectly-curated training sets. Current robust methods use heuristics for model fitting or lack formulations to leverage even a small amount of high-quality expert feedback. In contrast, we propose a novel method combining (i) robust statistical modeling, extending the multivariate generalized-Gaussian to a reproducing kernel Hilbert space, with (ii) semi-supervised learning to leverage a small expert-labeled outlier set. Results on simulated and real-world data, including endoscopy data, show that our method outperforms the state of the art in accurately detecting abnormalities. © 2019 IEEE",IEEE Computer Society,Semi-Supervised Robust One-Class Classification in RKHS for Abnormality Detection in Medical Images,10.1109/ICIP.2019.8803816,,,core
226908873,2019-07-08T00:00:00,"International audienceThe Industry 4.0 framework needs new intelligent approaches. Thus, the manufacturing industries more and more pay close attention to artificial intelligence (AI). For example, smart monitoring and diagnosis, real time evaluation and optimization of the whole production and raw materials management can be improved by using machine learning and big data tools. An accurate milling process implies a high quality of the obtained material surface (roughness, flatness). With the involvement of AI-based algorithms, milling process is expected to be more accurate during complex operations.In this work, a smart milling diagnosis has been developed for composite sandwich structures based on honey-comb core. The use of such material has grown considerably in recent years, especially in the aeronautic, aerospace, sporting and automotive industries. But the precise milling of such material presents many difficulties. The objective of this work is to develop a data-driven industrial surface quality diagnosis for the milling of honey-comb material, by using supervised machine learning methods. Therefore, cutting forces and workpiece material vibrations are online measured in order to predict the resulting surface flatness.The workpiece material studied in this investigation is Nomex® honeycomb cores with thin cell walls. The Nomex® honeycomb machining presents several defects related to its composite nature (uncut fiber, tearing of the walls), the cutting conditions and to the alveolar geometry of the structure which causes vibration on the different components of the cutting effort.Given the low level of cutting forces, the quality of the obtained machined surface allows to establish criteria for determining the machinability of the honeycomb structures. Nearly 40 features are calculated in time domain and frequency domain from the raw signal in steady state behavior (transient zones are not taken into account). The features are then normalized. The input parameters for each experiment are: the tool rotation speed, the cutting speed and the depth of cut. It is then necessary to make a dimensional reduction of that feature table in order to avoid overfitting and to reduce the computing time of the learning algorithm.In this work, several classification algorithms have been implemented such as : k-nearest neighbor (kNN), Decision trees (DT), Support Vector Machine (SVM). The different supervised learning algorithms have been implemented and compared. Each AI-based model has been applied to a set of features. From the prediction results, SVM algorithm seems to be the most efficient algorithm in this application",HAL CCSD,Milling diagnosis using machine learning approaches,,,,core
334887819,2019-11-28T00:00:00,"Our topic is the use of machine learning to improve software by making
choices which do not compromise the correctness of the output, but do affect
the time taken to produce such output. We are particularly concerned with
computer algebra systems (CASs), and in particular, our experiments are for
selecting the variable ordering to use when performing a cylindrical algebraic
decomposition of $n$-dimensional real space with respect to the signs of a set
of polynomials.
  In our prior work we explored the different ML models that could be used, and
how to identify suitable features of the input polynomials. In the present
paper we both repeat our prior experiments on problems which have more
variables (and thus exponentially more possible orderings), and examine the
metric which our ML classifiers targets. The natural metric is computational
runtime, with classifiers trained to pick the ordering which minimises this.
However, this leads to the situation were models do not distinguish between any
of the non-optimal orderings, whose runtimes may still vary dramatically. In
this paper we investigate a modification to the cross-validation algorithms of
the classifiers so that they do distinguish these cases, leading to improved
results.Comment: 16 pages. Accepted into the Proceedings of MACIS 2019. arXiv admin
  note: text overlap with arXiv:1906.0145",'Springer Science and Business Media LLC',"Improved cross-validation for classifiers that make algorithmic choices
  to minimise runtime without compromising output correctness",10.1007/978-3-030-43120-4_27,http://arxiv.org/abs/1911.12672,,core
232931862,2019-01-01T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.QC 20190916Factories of the Future (FACT",'Institute of Electrical and Electronics Engineers (IEEE)',Vpe : Variational policy embedding for transfer reinforcement learning,10.1109/ICRA.2019.8793556,,,core
199206784,2019-01-29T00:00:00,"Conservation researchers require low-cost access to acoustic monitoring technology.  However, affordable tools are often constrained to short-term studies due to high energy consumption and limited storage. To enable long-term monitoring, energy and space efficiency must be improved on such tools. This paper describes the development and deployment of three acoustic detection algorithms that reduce the power and storage requirements of acoustic monitoring on affordable, open-source hardware. The algorithms aim to detect bat echolocation, to search for evidence of a endangered cicada species, and to collect evidence of poaching in a protected nature reserve. The algorithms are designed to run on AudioMoth: a low-cost, open-source acoustic monitoring device, developed by the authors and widely adopted by the conservation community. Each algorithm addresses a detection task of increasing complexity – implementing extra analytical steps to account for environmental conditions such as wind, analysing samples multiple times to prevent missed events, and incorporating a hidden Markov model for sample classification in both the time and frequency domain. For each algorithm we report on real-world deployments carried out with partner organisations and also benchmark the hidden Markov model against a convolutional neural network, a deep-learning technique commonly used for acoustics. The deployments demonstrate how acoustic detection algorithms extend the use of low-cost, open-source hardware and facilitate a new avenue for conservation researchers to perform large-scale monitoring",'MDPI AG',"Deploying acoustic detection algorithms on low-cost, open-source acoustic sensors for environmental monitoring",10.3390/s19030553,,,core
344885719,2019-10-23T00:00:00,"International audienceSituation awareness consists of ""the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future"". Being aware of the security situation is then mandatory to launch proper security reactions in response to cybersecurity attacks. Security Incident and Event Management solutions are deployed within Security Operation Centers. Some vendors propose machine learning based approaches to detect intrusions by analysing networks behaviours. But cyberattacks like Wannacry and NotPetya, which shut down hundreds of thousands of computers, demonstrated that networks monitoring and surveillance solutions remain insufficient. Detecting these complex attacks (a.k.a. Advanced Persistent Threats) requires security administrators to retain a large number of logs just in case problems are detected and involve the investigation of past security events. This approach generates massive data that have to be analysed at the right time in order to detect any accidental or caused incident. In the same time, security administrators are not yet seasoned to such a task and lack the desired skills in data science. As a consequence, a large amount of data is available and still remains unexplored which leaves number of indicators of compromise under the radar. Building on the concept of situation awareness, we developed a situation-driven framework, called dynSMAUG, for dynamic security management. This approach simplifies the security management of dynamic systems and allows the specification of security policies at a high-level of abstraction (close to security requirements). This invited paper aims at exposing real security situations elicitation, coming from networks security experts, and showing the results of exploratory analysis techniques using complex event processing techniques to identify and extract security situations from a large volume of logs. The results contributed to the extension of the dynSMAUG solution",'Institute of Electrical and Electronics Engineers (IEEE)',Dynamic security management driven by situations: An Exploratory analysis of logs for the identification of security situations,10.1109/CSNet47905.2019.9108976,,,core
328760816,2019-03-16T00:00:00,"Background: The growth in publically available microbiome data in recent years has yielded an invaluable resource for genomic research, allowing for the design of new studies, augmentation of novel datasets and reanalysis of published works. This vast amount of microbiome data, as well as the widespread proliferation of microbiome research and the looming era of clinical metagenomics, means there is an urgent need to develop analytics that can process huge amounts of data in a short amount of time. To address this need, we propose a new method for the compact representation of microbiome sequencing data using similarity-preserving sketches of streaming k-mer spectra. These sketches allow for dissimilarity estimation, rapid microbiome catalogue searching and classification of microbiome samples in near real time. Results: We apply streaming histogram sketching to microbiome samples as a form of dimensionality reduction, creating a compressed ‘histosketch’ that can efficiently represent microbiome k-mer spectra. Using public microbiome datasets, we show that histosketches can be clustered by sample type using the pairwise Jaccard similarity estimation, consequently allowing for rapid microbiome similarity searches via a locality sensitive hashing indexing scheme. Furthermore, we use a ‘real life’ example to show that histosketches can train machine learning classifiers to accurately label microbiome samples. Specifically, using a collection of 108 novel microbiome samples from a cohort of premature neonates, we trained and tested a random forest classifier that could accurately predict whether the neonate had received antibiotic treatment (97% accuracy, 96% precision) and could subsequently be used to classify microbiome data streams in less than 3 s. Conclusions: Our method offers a new approach to rapidly process microbiome data streams, allowing samples to be rapidly clustered, indexed and classified. We also provide our implementation, Histosketching Using Little K-mers (HULK), which can histosketch a typical 2 GB microbiome in 50 s on a standard laptop using four cores, with the sketch occupying 3000 bytes of disk space",'Springer Science and Business Media LLC',Streaming histogram sketching for rapid microbiome analytics,10.1186/s40168-019-0653-2,https://core.ac.uk/download/328760816.pdf,,core
234708035,2019-01-05T00:00:00,"&nbsp;
Background: Human sperm cell counting analysis is of significant interest to biologists studying sperm function and to medical practitioners evaluating male infertility. Currently the analysis of this assessment is done manually by looking at the sperm samples through a phase-contrast microscope using expert knowledge to do a subjective judgement of the quality.
Aims: to eliminate the subjective and error prone of the manual semen analysis and to avoid inter and intra-laboratory inconsistencies in semen analysis test results
Methods: In this paper we introduce a technique for human sperm concentration. Its principle is based on the execution of three steps: The first step in unavoidable. It concerns the pretreatment of the human sperm microscopic videos which consists of a conversion of the RGB color space into the YCbCr space, the “Gaussian filtering” and the “discrete wavelet filtering”. The second step is devoted to the segmentation of the image into two classes: spermatozoas and the background. To achieve this, we used an edge detection technique “Sobel Contour detector”. The third step is to separate true sperm from false ones. It uses a machine learning technique of type decision trees that consist on two classes classification based on invariant characteristics that are the dimensions of the bounding ellipse of the spermatozoid head as well as its surface.
Results: To test the robustness of our system, we compared our results with those performed manually by andrologists. After results analysis, we can conclude that our system brings a real improvement of precision as well as treatment time which make it might be useful for groups who intend to design new CASA systems.
Conclusion: In this study, we designed and implemented a system for automatic concentration assessment based on machine learning method and image processing techniques",'Knowledge Kingdom Publishing',Automatic  Human  Sperm Concentrartion in microscopic videos,10.26415/2572-004X-vol2iss4p301-307,https://core.ac.uk/download/234708035.pdf,,core
201229644,2019-01-01T00:00:00,"Conservation researchers require low-cost access to acoustic monitoring technology. However, affordable tools are often constrained to short-term studies due to high energy consumption and limited storage. To enable long-term monitoring, energy and space efficiency must be improved on such tools. This paper describes the development and deployment of three acoustic detection algorithms that reduce the power and storage requirements of acoustic monitoring on affordable, open-source hardware. The algorithms aim to detect bat echolocation, to search for evidence of an endangered cicada species, and also to collect evidence of poaching in a protected nature reserve. The algorithms are designed to run on AudioMoth: a low-cost, open-source acoustic monitoring device, developed by the authors and widely adopted by the conservation community. Each algorithm addresses a detection task of increasing complexity, implementing extra analytical steps to account for environmental conditions such as wind, analysing samples multiple times to prevent missed events, and incorporating a hidden Markov model for sample classification in both the time and frequency domain. For each algorithm, we report on real-world deployments carried out with partner organisations and also benchmark the hidden Markov model against a convolutional neural network, a deep-learning technique commonly used for acoustics. The deployments demonstrate how acoustic detection algorithms extend the use of low-cost, open-source hardware and facilitate a new avenue for conservation researchers to perform large-scale monitoring",'MDPI AG',"Deploying Acoustic Detection Algorithms on Low-Cost, Open-Source Acoustic Sensors for Environmental Monitoring",10.3390/s19030553,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
201222235,2019-02-01T00:00:00,"Make and model recognition (MMR) of vehicles plays an important role in automatic vision-based systems. This paper proposes a novel deep learning approach for MMR using the SqueezeNet architecture. The frontal views of vehicle images are first extracted and fed into a deep network for training and testing. The SqueezeNet architecture with bypass connections between the Fire modules, a variant of the vanilla SqueezeNet, is employed for this study, which makes our MMR system more efficient. The experimental results on our collected large-scale vehicle datasets indicate that the proposed model achieves 96.3% recognition rate at the rank-1 level with an economical time slice of 108.8 ms. For inference tasks, the deployed deep model requires less than 5 MB of space and thus has a great viability in real-time applications",'MDPI AG',Real-Time Vehicle Make and Model Recognition with the Residual SqueezeNet Architecture,10.3390/s19050982,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
200797009,2019-02-06T00:00:00,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system",,Dynamic Pricing for Airline Ancillaries with Customer Context,,http://arxiv.org/abs/1902.02236,,core
200810936,2019-03-12T00:00:00,"The 5th edition of the International Conference on Cloud and Robotics (ICCR
2018 - http://cloudrobotics.info) will be held on November 12-14 2018 in Paris
and Saint-Quentin, France. The conference is a co-event with GDR ALROB and the
industry exposition Robonumerique (http://www.robonumerique.fr).
  The domain of cloud robotics aims to converge robots with computation,
storage and communication resources provided by the cloud. The cloud may
complement robotic resources in several ways, including crowd-sourcing
knowledge databases, context information, computational offloading or
data-intensive information processing for artificial intelligence. Today, the
paradigms of cloud/fog/edge computing propose software architecture solutions
for robots to share computations or offload them to ambiant and networked
resources. Yet, combining distant computations with the real time constraints
of robotics is very challenging. As the challenges in this domain are
multi-disciplinary and similar in other research areas, Cloud Robotics aims at
building bridges among experts from academia and industry working in different
fields, such as robotics, cyber-physical systems, automotive, aerospace,
machine learning, artificial intelligence, software architecture, big data
analytics, Internet-of-Things, networked control and distributed cloud systems",,"Proceedings of the Fifth International Conference on Cloud and Robotics
  (ICCR2018)",,http://arxiv.org/abs/1903.04824,,core
200826389,2019-04-16T00:00:00,"Radio fingerprinting provides a reliable and energy-efficient IoT
authentication strategy. By mapping inputs onto a very large feature space,
deep learning algorithms can be trained to fingerprint large populations of
devices operating under any wireless standard. One of the most crucial
challenges in radio fingerprinting is to counteract the action of the wireless
channel, which decreases fingerprinting accuracy significantly by disrupting
hardware impairments. On the other hand, due to their sheer size, deep learning
algorithms are hardly re-trainable in real-time. Another aspect that is yet to
be investigated is whether an adversary can successfully impersonate another
device fingerprint. To address these key issues, this paper proposes
DeepRadioID, a system to optimize the accuracy of deep-learning-based radio
fingerprinting algorithms without retraining the underlying deep learning
model. We extensively evaluate DeepRadioID on a experimental testbed of 20
nominally-identical software-defined radios, as well as on two datasets made up
by 500 ADS-B devices and by 500 WiFi devices provided by the DARPA RFMLS
program. Experimental results show that DeepRadioID (i) increases
fingerprinting accuracy by about 35%, 50% and 58% on the three scenarios
considered; (ii) decreases an adversary's accuracy by about 54% when trying to
imitate other device fingerprints by using their filters; (iii) achieves 27%
improvement over the state of the art on a 100-device dataset.Comment: To appear in ACM MobiHoc 2019, Catania, Ital",,"DeepRadioID: Real-Time Channel-Resilient Optimization of Deep
  Learning-based Radio Fingerprinting Algorithms",,http://arxiv.org/abs/1904.07623,,core
231817139,2019-01-01T00:00:00,"Computer vision-based intelligent autonomous systems engage various types of sensors to perceive the world they navigate in. Vision systems perceive their environments through inferences on entities (structures, humans) and their attributes (pose, shape, materials) that are sensed using RGB and Near-InfraRed (NIR) cameras, LAser Detection And Ranging (LADAR), radar and so on. This leads to challenging and interesting problems in efficient data-capture, feature extraction, and attribute estimation, not only for RGB but various other sensors. In some cases, we encounter very limited amounts of labeled training data. In certain other scenarios we have sufficient data, but annotations are unavailable for supervised learning. This dissertation explores two approaches to learning under conditions of minimal to no ground truth. The first approach applies projections on training data that make learning efficient by improving training dynamics. The first and second topics in this dissertation belong to this category. The second approach makes learning without ground-truth possible via knowledge transfer from a labeled source domain to an unlabeled target domain through projections to domain-invariant shared latent spaces. The third and fourth topics in this dissertation belong to this category.

 

For the first topic we study the feasibility and efficacy of identifying shapes in LADAR data in several measurement modes. We present results on efficient parameter learning with less data (for both traditional machine learning as well as deep models) on LADAR images. We use a LADAR apparatus to obtain range information from a 3-D scene by emitting laser beams and collecting the reflected rays from target objects in the region of interest. The Agile Beam LADAR concept makes the measurement and interpretation process more efficient using a software-defined architecture that leverages computational imaging principles. Using these techniques, we show that object identification and scene understanding can be accurately performed in the LADARmeasurement domain thereby rendering the efforts of pixel-based scene reconstruction superfluous.

 

Next, we explore the effectiveness of deep features extracted by Convolutional Neural Networks (CNNs) in the Discrete Cosine Transform (DCT) domain for various image classification tasks such as pedestrian and face detection, material identification and object recognition. We perform the DCT operation on the feature maps generated by convolutional layers in CNNs. We compare the performance of the same network with the same hyper-parameters with or without the DCT step. Our results indicate that a DCT operation incorporated into the network after the first convolution layer can have certain advantages such as convergence over fewer training epochs and sparser weight matrices that are more conducive to pruning and hashing techniques.

 

Next, we present an adversarial deep domain adaptation (ADA)-based approach for training deep neural networks that fit 3Dmeshes on humans in monocular RGB input images. Estimating a 3D mesh from a 2D image is helpful in harvesting complete 3Dinformation about body pose and shape. However, learning such an estimation task in a supervised way is challenging owing to the fact that ground truth 3D mesh parameters for real humans do not exist. We propose a domain adaptation based single-shot (no re-projection, no iterative refinement), end-to-end training approach with joint optimization on real and synthetic images on a shared common task. Through joint inference on real and synthetic data, the network extracts domain invariant features that are further used to estimate the 3D mesh parameters in a single shot with no supervision on real samples. While we compute regression loss on synthetic samples with ground truth mesh parameters, knowledge is transferred from synthetic to real data through ADA without direct ground truth for supervision.

 

Finally, we propose a partially supervised method for satellite image super-resolution by learning a unified representation of samples from different domains (captured by different sensors) in a shared latent space. The training samples are drawn from two datasets which we refer to as source and target domains. The source domain consists of fewer samples which are of higher resolution and contain very detailed and accurate annotations. In contrast, samples from the target domain are low-resolution and available ground truth is sparse. The pipeline consists of a feature extractor and a super-resolving module which are trained end-to-end. Using a deep feature extractor, we jointly learn (on two datasets) a common embedding space for all samples. Partial supervision is available for the samples in the source domain which have high-resolution ground truth. Adversarial supervision is used to successfully super-resolve low-resolution RGB satellite imagery from target domain without direct paired supervision from high resolution counterparts",'Wiley',DEEP INFERENCE ON MULTI-SENSOR DATA,10.13016/jcrn-mnj6,https://core.ac.uk/download/231817139.pdf,,core
478869506,2019-01-01T00:00:00,"2019 annual report for the Blue Waters ProjectNSF OCI-0725070NSF ACI-123899314.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence and Dynamos in Star-Forming Molecular Clouds and a New Paradigm for Computational Astrophysics for Spherical Systems
16.	Adam Burrows, The Computational Keys to the Supernova Puzzle:  How Multiple 3D Radiation/Hydrodynamic Models Can Unlock the Supernova Mystery
18.	Manuela Campanelli, Shedding Light on Supermassive Binary Black Hole Mergers
20.	Manuela Campanelli, Accretion Dynamics of Supermassive Black Hole Binaries
22.	Matias Carrasco Kind, Achieving Probabilistic Classification of Cosmic Web Particles Using Rapidly Generated Training Data:  A Method for Classifying Galaxies into Their Cosmic Web Structural Groups Using Supervised Machine Learning
24.	Tiziana Di Mattteo, The Epoch of the First Luminous Black Holes:  Evolving the Blue Tides Simulation into the First Billion Years of Cosmic History
26.	Jerry Draayer, Advancing First-Principle Symmetry-Guided Nuclear Modeling for Studies of Nucleosynthesis and Fundamental Symmetries in Nature
28.	Charles F. Gammie, Magnetized Models of Giant Impacts
30.	Nickolayk Gnedin, Cosmic Reionization on Computers
32.	John Hawley, Elucidating the Alignment Mechanism for Black Hole Accretion Disks Subjected to Lense-Thirring Torques
34.	Philip F. Hopkins, Understanding the Origins of the Stars and Galaxies in our Universe
36.	Eliu Huerta, Deep Learning at Scale for the Construction of Galaxy Catalogs with the Dark Energy Survey
38.	Eliu Huerta, Characterization of Numerical Relativity Waveforms of Eccentric Binary Black Hole Mergers
40.	Eiu Huerta, Fusing Numerical Relativity and Deep Learning to Detect Eccentric Binary Black Hole Mergers Using Higher-Order Waveform Multipoles
42.	Athol J. Kemball, Data-and Compute-Intensive Challenges for Observational Astronomy in the Great Survey Era
44.	Deborah A. Levin, Modal Decompositions of Shock Interactions
46.	Deborah A. Levin, Plume Plasma Spacecraft Interactions
48.	Yi-Hsin Liu, The Spreading of Three-Dimensional Magnetic Reconnection in Asymmetric Geometry
50.	Felipe Menanteau, Assembling a Map of the Universe:  Shapes and Mass Distribution for the Dark Energy Survey
52.	Philipp Mosta, Petascale Simulations of Binary Neutron Star Mergers
54.	Michael L. Norman, Development of a Scalable Gravity Solver for Enzo-E
56.	Brian O’Shea, Simulating Galaxy Formation Across Cosmic Time
58.	Donald Petravick, Processing Dark Energy Camera Data to Make the World’s Best Map of the Night Sky
60.	Nikolai Pogorelov, Coupling the Solar Wind and Local Interstellar Medium in the Era of the New Horizons, Interstellar Boundary Explorer, Parker Solar Probe, Ulysses, and Voyager Spacecraft
62.	Jane Pratt, Interior Dynamics of Young Stars Revealed by 3D Hydrodynamic Simulations
64.	Thomas Quinn, Modeling of Galaxy Populations
66.	Paul Ricker, Effects of Active Galaxy Feedback on the Intracluster Medium
68.	Stuart Shapiro, Gravitational and Electromagnetic Signatures from Binary Black Hole-Neutron Star Mergers:  A Jet Engine for Short Gamma-Ray Bursts
70.	Alexander Tchekhovskoy, Feeding Black Holes:  Tilt with a Twist
72.	Gabor Toth, Scaling the BATS-R-US MHD Model to Over 100,000 Cores with Efficient Hybrid OpenMP and MPI Parallelization
74.	Mathew Turk, Numerical Study on the Fragmentation Condition in a Primordial Accretion Disk
75.	Saul A. Teukolsky, Merging Black Holes and Neutron Stars
78.	Jennifer Corcoran, Image Processing to Build a Multitemporal Vegetation Elevation Ecosystem Model of the Great Lakes Basin
80.	Larry Di Girolamo, Petascale Processing of Satellite Earth Observations
82.	Chunyuan Diao, Large-Scale Remote Monitoring of Invasive Species Dynamics Through a Petascale High-Performance Computing System
84.	Francina Domiguez, Deforestation of the Amazon Forest:  Understanding Hydroclimate Impacts by Tracing the Water that Evaporates from the Forest
86.	Patricia M. Gregg, Forecasting Volcanic Unrest and Eruption Potential Using Statistical Data Assimilation
88.	Kaiyu Guan, Monitoring Field-Scale Crop Water Use Using a Satellite Data-Driven Mechanistic Modeling Approach
90.	Kaiyu Guan, Building an Objective Seasonal Forecasting System for U.S. Corn and Soybean Yields
92.	Sonia Lasher-Trapp, Inflow and Outflow from Thunderstorms:  Tracking Their Influence on Precipitation and Further Growth
94.	Paul Morin, Petascale Polar Topography Production
96.	Stephen W. Nesbitt, High-Resolution Numerical Simulations of Convection Initiation over the Sierras de Cordoba Mountains in Argentina
98.	Leigh G. Orf, Simulations of Violently Tornadic Supercells and Damaging Thunderstorms
100.	Nikolaos Pavlis, Prediction of Geomagnetic Secular Variation with Large-Ensemble Geomagnetic Data Assimilation
102.	Nicole Riemer, Machine Learning for Error Quantification in Simulating the Climate Impacts of Atmospheric Aerosols
104.	Clay Tabor, Simulating Hydroclimate Change in Southwest North America at 21,000 Years Ago
106.	Robert J. Trapp, Implementation and Use of a Global Nonhydrostatic Model for Extended-Range Weather Prediction during the RELAMPAGO Field Campaign
108.	John Vidale, Simulating Large California Earthquakes Before They Occur
110.	Renata Wentzcovitch Materials Simulations in Geophysics
112.	Mathew West, Simulating Aerosol Impacts on Climate, One Particle at a Time:  A Regional-Scale, Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
114.	Donald J. Wuebbles, Evolving Air Quality Under the Changing Climate
116.	Xiangdong Zhang, Sensitivity of Arctic Sea Ice Thickness Distribution to Sea Ice Internal Dynamics in a Changing Climate
120.	Narayana R. Aluru, The Mechanism of Proton Diffusion in ABO3 Perovskite Oxides
122.	Narayana R. Aluru, Identification of Amino Acids with Sensitive Nanoporous MoS2:  Toward Machine Learning-Based Prediction
124.	Narayana R Aluru, Transfer-Learning-Based Course-Graining Method for Simple Fluids:  Toward Deep Inverse Liquid-State Theory
126.	Guillermo Araya, High-End Visualization of Coherent Structures and Turbulent Events in Wall-Bounded Flows with a Passive Scalar
128.	Jerzy Bernholc, Design of Atomically Precise Nanoscale Negative Differential Resistance Devices
130.	Daniel Bodony, Using OpenMP Offloading to Run Code on Blue Waters’ GPU Nodes
132.	Christoph Brehm, Numerical Investigation of Turbulence Suppression in Rotating Flows
134.	Oliver M. F. Browne, An Efficient Method for Hypersonic Laminar-Turbulent Transition Prediction
136.	Carlo Pierleoni, Quantum Simulations:  Properties of Dense Hydrogen
138.	Huck Beng Chew, Role of Interfaces on the Shear Strength and Bending Properties of vander Waals Two-Dimensional Materials
140.	Bryan Clark, Atypically Entangled Phases and New Methods for the Quantum Many-Body Problem
142.	Lian Duan, Direct Numerical Simulation of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
144.	Aida X. El-Khadra, The Anomalous Magnetic Moment of the Muon:  An Improved Ab Initio Calculation of the Hadronic Vacuum Polarization Contribution
146.	Elif Ertekin, Accelerating Thermoelectric Materials Discovery via Dopability Predictions
148.	Jonathan Freund, Machine-Learning Turbulence Models for Simulations of Turbulent Combustion
150.	Marcelo Garcia, Turbulence-Resolving Modeling of Oscillatory Boundary Layer Flows
152.	Benjamin Hooberman, Machine Learning for Particle Physics:  Employing Deep Learning for Particle Identification and Measurement of Colliders
154.	Kathryn Huff, Molten-Salt Reactors and Their Fuel Cycles
156.	Prashant K. Jain, A Novel Crystal Structure with Spin-Protected Surface Electronic Conduction
158.	Eric Johnsen, Inertial Collapse of Individual Bubbles near Solid/Free Boundaries
160.	Harley T. Johnson, Electronic Structure of Microscale Dielectric Barrier Discharges
162.	Seid Koric, Accelerating Virtual Prototyping and Certification in the Aerospace Industry with Scalable Finite-Element Analysis
164.	Jean-Pierre Leburton, Graphene Nanopore Transistor for DNA-Nick Detection
166.	Farzad Mashayek, Compressibility Effects on Spatially Developing Plane Free Shear Layer
168.	Moshe Matalon, Outwardly Propagating Turbulent Flames
170.	Mark Neubauer, Deep Learning for Higgs Boson Identification and Searches for New Physics at the Large Hadron Collider
172.	Rajib Rahman, Designing Quantum Logic Gates on Silicon Chips with Large-Scale Multiphysics Simulations
174.	Venkat Raman, Simulation of Rotating Detonation Engines
176.	Caroline Riedl, Mapping Proton Quark Structure:  Looking Inside the Proton-How do Quarks Spin?
178.	Andre’ Schleife, Electron Dynamics of Ion-Irradiated Two-Dimensional Materials
180.	Andre’ Schleife, Discovery of New Plasmonic Materials via High-Throughput Machine Learning
182.	Brian G. Thomas, Turbulent Multiphase Thermal Flow Modeling of Defect Formation Mechanisms and Electromagnetic Force Effects in Continuous Steel Casting
184.	Rafael Tinoco Lopez, Investigation of Sediment Transport Through Aquatic Vegetation Using Large-Scale High-Fidelity Turbulence Simulations
186.	Kimani Toussaint, Machine Learning-Assisted High-Throughput Computational Design of Solvents for Liquid-Exfoliation
188.	Dallas R. Trinkle, High-Throughput Materials Modeling Optimization
190.	Lela Vukovic, Detecting Neurotransmitters with DNA-Wrapped Nanotube Sensors
192.	Lucas Wagner, Accurate Effective Interactions in Quantum Materials
194.	Zhi Jian Wang, Supersonic Jet Noise Prediction Using High-Order Large-Eddy Simulation
196.	Bin Xu, Spin Spirals in Multiferroic Bismuth Ferrite and at Metal Surface:  From Fully First Principles
198.	Zhen Xu, Numerical Simulations of a Collapsing Cavitation Bubble Near an Elastically Deformable Object
200.	Yonghua Yan, Numerical Study on Shock Wave-Boundary Layer Interaction and Its Control
202.	Jinhui Yan, Free-Surface Flow Modeling of Multiple Tidal Turbines
204.	Pui-Kuen Yeung, New Insights on Intermittency and Circulation Statistics Obtained From a Massive Turbulence Simulation Database
206.	Yang Zhang, Effects of Surface Defects on Hydrophobicity at Rare-Earth Oxide Interfaces Using Molecular Dynamics Simulations Driven by Ab Initio-Based Deep Neural Network Potentials
208.	Phiala Shanahan, Constraining the Properties and Interactions of Dark Matter
212.	Donna Cox, Cinematic Scientific Data Visualization for CADENS
214.	Iwan Duursma, The Structure and Statistics of Reed-Muller Codes
216.	William Gropp, A Parallel Framework for Scaling Phylogeny Estimation Methods to Large Genomic Data Sets
218.	William Gropp, Algorithms for Extreme-Scale Systems
220.	Ravishankar Iyer, Kaleidoscope:  Live Forensics for Large-Scale Data Center Storage Systems
222.	Shantenu Jha, Extensible and Scalable Adaptive Sampling to Fold Proteins on Supercomputers
224.	Luke Olson, Improved Scalability Through Node-Aware Communicators
226.	Luke Olson, Scalable Line and Plane Solvers
228.	Marc Snir, Detection of Silent Data Corruptions Using Machine Learning	
230.	Edgar Solomonik, Pushing the Boundaries of Large-Scale Tensor Computations
232.	Tandy Warnow, Algorithms for Large-Scale Evolutionary Tree Construction:  Improving Scalability and Accuracy through Divide-and-Conquer
234.	Justin Sirigano, HPC Development of Deep Learning Models in Scientific Computing and Finance
235.	David Taflin, Optimization of a Field Data Parallel Output Library
238.	Aleksei Aksimentiev, Resolving the Structure of Bacteriophage HK97 with Atomistic Resolution
240.	Aleksei Aksimentiev, A Nanopore System for Single-Molecule Protein Sequencing
242.	Aleksei Aksimentiev, Dynamic Interactions Between Lipid-Tethered DNA and Phospholipid Membranes
244.	Rommie Amaro, Influence Virulence and Transmissibility Through the Computational Microscope
246.	Rafael C. Bernardi, How Blue Waters Is Aiding the Fight Against Sepsis
248.	Gustavo Caetano-Annoles, A Phylogenomic History of Protein Function and Dynamics
250.	Colleen E. Clancy, Predicting Drug-Induced Cardiac Arrhythmias Using Atomistic Simulations
252.	Julie Dickerson, MRNA Isoform Prediction
254.	Ken Dill, Petascale Integrative Approaches to Protein Structure Prediction
256.	Andrew Ferguson, Discovery of Slow Kinetic Modes from Molecular Simulation Trajectories
258.	Mattia Gazzola, Harnessing Viscous Streaming in Complex Active Systems:  Minibots in Fluids
260.	Jodi A. Hadden-Perilla, Molecular Dynamics Simulations of HBV Capsid
262.	Jodi A. Hadden-Perilla, Molecular Dynamics Simulations of the HBV Capsid as a Drug Target
264.	So Hirata, Toward Predictive Computational Design of Precision Molecular Optoelectronics
266.	Mathew E. Hudson, Impact of Batch Effect and Study Design Biases on Identification of Genetic Risk Factors in Sequencing Data
268.	Tao Jiang, Microscopic Identification of PIP2 Binding Sites on a Ca2+-activated Cl- Channel
270.	Fatemah Khalili-Araghi, Paracellular Ion Transport
272.	David LeBauer, The TERRA Phenotyping Refence Platform:  Open Data and Software for Precision Field Crop Measurement and Analysis
274.	Nancy Makri, Quantum-Classical Path Integral Simulation of Proton Translocation in Biological Channels
276.	Arif Masud, A New Stabilized Fluid-Structure Interaction Method:  Coupled System of Anisotropic Viscoelastic Model for Artery and Non-Newtonian Model for Blood
278.	Jeffery S. Moore, Atomic Scale Simulation of Amyloid Beta with Dismantling Peptide-Based Inhibitors
280.	Mahmoud Moradi, Transport Mechanism of POT Transporters:  Employing Loosely Coupled Molecular Dynamics Simulations to Characterize Protein Structural Dynamics
282.	Mahmoud Moradi, Activation Mechanisms of the Mechanosensitive Channel of Large Conductance:  Employing Loosely Coupled Molecular Dynamics Simulations to Characterize Protein Structural Dynamics
284.	Juan Perilla, Molecular Mechanisms of Infection by Chlamydia
286.	Joseph R. Peterson, Calibrating the SimBioSys TumorScope for the Fight on Cancer:  A Scenario Analysis Engine for Determining Optimal Therapy Choice
288.	Kimberly Prather, Investigating the Climate-Relevant Impacts of Chemical Complexity in Marine Aerosols
290.	Benoit Roux, Molecular Dynamics Binding Free Energy Calculations Offer a Window to Understand Protein-Protein Binding Specificity
292.	Diwakar Shukla, Molecular Basis of the Nitrate Transport Mechanism in Plants
294.	Diwakar Shukla, Simulations Uncover the Mechanism of Serotonin Transport in the Brain
296.	Diwakar Shukla, Elucidating the Ligand Selectivity and Activation Mechanisms of Cannabinoid Receptors
298.	Ivan Soltesz, Full-Scale Biophysical Modeling of Hippocampal Networks During Spatial Navigation
300.	Marcos Sotomayor, Desmosomal Cadherins Beating Under Tension
302.	Ashok Srinivasan, Simulation of Viral Infection Propagation During Air Travel
304.	Brad Sutton, MRI-Based Biomarkers Through High-Performance Computing
306.	Emad Tajkhorshid, Mechanobiology:  Using Blue Waters to Decipher the Physical Principles of Protein Mechanics
308.	Emad Tajkhorshid, Atomistic Simulations of a Protocell
310.	Emad Tajkhorshid, Modeling of a Zika Virus Envelope at Atomic Resolution
312.	Gregory Voth, Multiscale Simulations of Complex Self-Assembling Biomolecules:  Targeting HIV-1
314.	Victor Anisimov, Improving the Agreement of AMBER Simulation of Crystals of Nucleic Acid Bases with Experimental Data
315.	Mohammed El-Kebir, Algorithms for Cancer Phylogenetics
316.	Taras V. Pogorelov, Amphotericin-Driven Sterol Extraction:  Probing the Mechanism
320.	Yongyang Cai, Climate Policy in a Dynamic Stochastic Economy
322.	J. Stephen Downie, Characterizing Descriptivity in Writing through Text Analysis of Books from the HathiTrust Digital Library
324.	Mao Ye, High-Frequency Trading in Nanoseconds:  Analysis, Modeling, and Policy Implications
328.	Wendy K. Tam Cho, A Massively Parallel Evolutionary Markov Chain Monte Carlo Algorithm for Sampling Spatial State Spaces
332.	Eizabeth Agee, The Contributions of Root Systems to Drought Response in the Amazon Rainforest
334.	Elaad Applebaum, Star Formation in Dwarf Galaxies:  Using Simulations to Identify Key Observables to Test Models
336.	Katelyn Barber, Improving Convectively Induced Turbulence Forecast Parameters Through Bulk Numerical Simulations for Aviation Safety
338.	Maureen T. Brooks, Modeling Nonlinear Physical-Biological Interactions:  Inertia and Sargassum in the North Atlantic
340.	Iryna Butsky, Predictions About the Invisible Gas in Galaxy Clusters
342.	Robert Cieri, Computational Fluid Dynamics Investigation into Pulmonary Airflow Patterns in Monitor  Lizards(Varanidae)
344.	Mathew Clement, The Early Instability Scenario for Planet Formation in the Solar System
346.	Salme Cook, The Distribution of Shear Stress and Nutrients in a Tidally Energetic Estuary:  The Role of Numerical Resolution and Vegetation
348.	Andrew Emerick, Exascale Astrophysics with Enzo-E:  Development of Physics Modules for Galaxy-Scale and Cosmology Simulations
350.	Forrest Glines, Magnetohydrodynamic Simulation:  Galaxies
352.	Alexander Gurvich, GPU-Accelerated Interstellar Chemistry with WIND:  A General Ordinary Differential Equation Solver
254.	Jennifer M. Hayes, Using Spectroscopic Data and Molecular Simulations to Estimate Heterogeneous Ensembles:  How to Study Complicated, Flexible Proteins When Experimental Data Are Limited
356.	Joshua Lansford, Electron Density-Based Machine Learning for Accelerating Quantum Calculations
358.	Kara Marsac, Extending the Longevity of Produced Water Disposal Wells:  Evaluation Using Reactive Transport Simulation
360.	Nicole Rosato, Improved Trumpet Initial Lapse and Shift for Binary Black Hole Simulations
361.	Shanna Chu, Understanding the Physical Processes Causing Intermediate-Depth Earthquakes
362.	Micheline Soley, Escaping From an Ultracold Inferno:  The Ultracold KRb Dimer Reaction
364.	Ronald Stenz, The Impacts of Hydrometeor Centrifuging on Tornado Dynamics:  Improving the Realism of Tornado Simulations
366.	Darius Teo, Unraveling Functional Hole Hopping Pathways in The [Fe4S4]-Containing DNA Primase
368.	Walter Torres, The Transport and Dynamics of Wave-Driven Reef Jets Under the Influence of Rotation and Bottom Friction
370.	Samuel Whitman, Simulation of Bluff Body Stabilized Flames with PeleC:  Adaptively Resolving Turbulence-Combustion Interactions in Real-World Engineering ProblemsNot Peer ReviewedOpe",,Blue Waters 2019 Annual Report,,,,core
288617485,2019-10-28T19:31:26,"Scheduling is an important problem in artificial intelligence and operations research. In production processes, it deals with the problem of allocation of resources to different tasks with the goal of optimizing one or more objectives. Job shop scheduling is a classic and very common scheduling problem. In the real world, shop environments dynamically change due to events such as the arrival of new jobs and machine breakdown. In such manufacturing environments, uncertainty in shop parameters is typical. It is of vital importance to develop methods for effective scheduling in such practical settings.

Scheduling using heuristics like dispatching rules is very popular and suitable for such environments due to their low computational cost and ease of implementation. For a dynamic manufacturing environment with varying shop scenarios, using a universal dispatching rule is not very effective. But manual development of effective dispatching rules is difficult, time consuming and requires expertise. Genetic programming is an evolutionary approach which is suitable for automatically designing effective dispatching rules. Since the genetic programming approach searches in the space of heuristics (dispatching rules) instead of building up a schedule, it is considered a hyper-heuristic approach.

Genetic programming like many other evolutionary approaches is computationally expensive. Therefore, it is of vital importance to present the genetic programming based hyper-heuristic (GPHH) system with scheduling problem instances which capture the complex shop scenarios capturing the difficulty in scheduling. Active learning is a related concept from machine learning which concerns with effective sampling of those training instances to promote the accuracy of the learned model.

The overall goal of this thesis is to develop effective and efficient genetic programming based hyper-heuristic approaches using active learning techniques for dynamic job shop scheduling problems with one or more objectives.

This thesis develops new representations for genetic programming enabling it to incorporate the uncertainty information about processing times of the jobs. Furthermore, a cooperative co-evolutionary approach is developed for GPHH which evolves a pair of dispatching rules for bottleneck and non-bottleneck machines in the dynamic environment with uncertainty in processing times arising due to varying machine characteristics. The results show that the new representations and training approaches are able to significantly improve the performance of evolved dispatching rules.

This thesis develops a new GPHH framework in order to incorporate active learning methods toward sampling DJSS instances which promote the evolution of more effective rules. Using this framework, two new active sampling methods were developed to identify those scheduling problem instances which promoted evolution of effective dispatching rules. The results show the advantages of using active learning methods for scheduling under the purview of GPHH.

This thesis investigates a coarse-grained model of parallel evolutionary approach for multi-objective dynamic job shop scheduling problems using GPHH. The outcome of the investigation was utilized to extend the coarse-grained model and incorporate an active sampling heuristic toward identifying those scheduling problem instances which capture the conflict between the objectives. The results show significant improvement in the quality of the evolved Pareto set of dispatching rules.

Through this thesis, the following contributions have been made. (1) New representations and training approaches for GPHH  to incorporate uncertainty information about processing times of jobs into dispatching rules to make them more effective in a practical shop environment. (2) A new GPHH framework which enables active sampling of scheduling problem instances toward evolving dispatching rules effective across complex shop scenarios.  (3) A new active sampling heuristic based on a coarse-grained model of parallel evolutionary approach for GPHH for multi-objective scheduling problems",'Victoria University of Wellington Library',Active Learning Methods for Dynamic Job Shop Scheduling using Genetic Programming under Uncertain Environment,,,,core
398561907,2019-01-01T00:00:00,"Adoption of digital platform innovations afford a changing nature of work, from mobile computing platforms (e.g. Apple) enabling 24/7 work connectivity, to labour marketplace platforms (e.g. Uber) enabling precarious work arrangements. Recently, organisations are adopting/investigating spatial computing platforms (e.g. Autodesk, Toyota, BNP Paribas), offering new affordances for organising (e.g. carrying out tasks, communicating and collaborating). Spatial computing concerns achieving spatial interplay between the real and digital world (Agulhon 2016), enabling perception of physically present content. An emerging paradigm of spatial computing is enabled by hardware and software innovations for; 1) digitally mapping, tracking, understanding and predicting analog audio and visual spatial fields, 2) creating digital audio and visual spatial fields, and the (3) mixing and fusing of those fields. Mixed, augmented and immersive reality is then experienced by volumetric graphic rendering onto a human's field of view (FOV) (Martín-Gutiérrez et al. 2017). Emerging marketplace examples can be seen in 'Microsoft Hololens 2' and 'Magic Leap One' platforms, both creating/enabling an ecosystem of novel applications for both industrial, educational and leisure life contexts. With further convergence of IoT, haptics, 5G, cloud and AI etc., spatial applications will range from contextually aware and interactive; digital information layering of objects, guidance and decision support systems (DSS) within business operations (such as for industrial machine manufacture, monitoring, and maintenance), digital modelling & prototyping in R&D, through to applications for communications and collaborations (such as for spatial tele co-presence of people, objects and environments). More broadly, these advances have potential to catalyse disruptions within business, through to the labour and consumer marketplace via: (1) Virtualisation of hardware resources (e.g. fully digitising workplace equipment such as displays and interfaces, raw inputs for prototyping and even digital rendering of spaces). (2) Protection and strengthening of institutional knowledge and performance via knowledge capture, guidance and decision support of labour tasks and activity (e.g. reducing labour (re)training (e.g. parts assembly), knowledge capture of practice). (3) Creation and distribution of new value propositions in goods and services (e.g. digital item ownership in a mixed-reality cloud, spatial applications for IoT enabled devices). (4) Displacement of geographic space as cost, talent, time, access and convenience constraints on business (e.g. available talent pool, partner/customer reach and relations). (5) Collaboration through new/enhanced affordances for workers (e.g. shared digitised work tools/environments). Therefore, a paradigm of spatial computing will challenge the IS community to research new ways of working, and consequences for worker experience, meaning, productivity and power. With emerging advances in AI, automation and spatial computing, one of the pertinent enquires concerns importance of workers (sense of) agency (Chandra et al. 2019). Control in the IT context has been conceptualised as control over work, control over self, and control over technology (Beaudry and Pinsonneault 2005), with prior IS work studying locus of control related to; work stress (Chandra et al. 2019), intrinsic and extrinsic motivations (Mujinga, M Eloff, MM Kroeze 2013), and performance (Vieira da Cunha et al. 2015) etc. With spatial computing platforms and their applications, what affordances of control and for whom should be developed? For example, the electronic representation of worker activity can be further enabled. Thus, tighter or looser coupling between worker activity and the reporting/outcome of work (Vieira da Cunha et al. 2015) becomes more of an organisational decision, with capability to monitor workers, and leverage AI for learning and optimisation. Furthermore, with development of spatial tele co-presence (STcP) (e.g. Mimesys), brings new affordances for communication with any worker(s), at any time, from anywhere. However, prior CMC research suggests people can choose different communication media specifically to manage social and emotional relationships (Madianou 2014) and their time (Mcloughlin et al. 2019). Hence, will such affordances serve greater identity fusion (Swann et al. 2012) and collaboration in organisations? Thus, we propose a socio-technical research agenda exploring 'control' related affordances for emerging spatial computing platforms, such as for STcP technology. In this regard, Control Theory can offer a useful starting frame, as it deals with control mechanisms governing workers organisational actions both formal (outcome and behaviour based) and informal (group and self-control), to further the interests of organisations (Kirsch 1996). We suggest, data and communication related affordances of control (e.g. privacy, exploitation, authenticity, availability and spaces) as starting points. Social Capital (Lin 2001), Social Influence (Kelman 1958), Social Identity (Ellemers and Haslam 2012), Identity Fusion (Swann et al. 2012) and Polymedia (Madianou and Miller 2012) being just some of the many relevant social theories to this endeavour",,Affordances of Control in a Paradigm of Spatial Computing Platforms,,,,core
334876410,2019-10-29T00:00:00,"New generation geostationary satellites make solar reflectance observations
available at a continental scale with unprecedented spatiotemporal resolution
and spectral range. Generating quality land monitoring products requires
correction of the effects of atmospheric scattering and absorption, which vary
in time and space according to geometry and atmospheric composition. Many
atmospheric radiative transfer models, including that of Multi-Angle
Implementation of Atmospheric Correction (MAIAC), are too computationally
complex to be run in real time, and rely on precomputed look-up tables.
Additionally, uncertainty in measurements and models for remote sensing
receives insufficient attention, in part due to the difficulty of obtaining
sufficient ground measurements. In this paper, we present an adaptation of
Bayesian Deep Learning (BDL) to emulation of the MAIAC atmospheric correction
algorithm. Emulation approaches learn a statistical model as an efficient
approximation of a physical model, while machine learning methods have
demonstrated performance in extracting spatial features and learning complex,
nonlinear mappings. We demonstrate stable surface reflectance retrieval by
emulation (R2 between MAIAC and emulator SR are 0.63, 0.75, 0.86, 0.84, 0.95,
and 0.91 for Blue, Green, Red, NIR, SWIR1, and SWIR2 bands, respectively),
accurate cloud detection (86\%), and well-calibrated, geolocated uncertainty
estimates. Our results support BDL-based emulation as an accurate and efficient
(up to 6x speedup) method for approximation atmospheric correction, where
built-in uncertainty estimates stand to open new opportunities for model
assessment and support informed use of SR-derived quantities in multiple
domains.Comment: 10 pages, 7 figures, 4 table",,"Deep Learning Emulation of Multi-Angle Implementation of Atmospheric
  Correction (MAIAC)",,http://arxiv.org/abs/1910.13408,,core
398227078,2019-01-01T00:00:00,"Adoption of digital platform innovations afford a changing nature of work, from mobile computing platforms (e.g. Apple) enabling 24/7 work connectivity, to labour marketplace platforms (e.g. Uber) enabling precarious work arrangements. Recently, organisations are adopting/investigating spatial computing platforms (e.g. Autodesk, Toyota, BNP Paribas), offering new affordances for organising (e.g. carrying out tasks, communicating and collaborating). Spatial computing concerns achieving spatial interplay between the real and digital world (Agulhon 2016), enabling perception of physically present content. An emerging paradigm of spatial computing is enabled by hardware and software innovations for; 1) digitally mapping, tracking, understanding and predicting analog audio and visual spatial fields, 2) creating digital audio and visual spatial fields, and the (3) mixing and fusing of those fields. Mixed, augmented and immersive reality is then experienced by volumetric graphic rendering onto a human's field of view (FOV) (Martín-Gutiérrez et al. 2017). Emerging marketplace examples can be seen in 'Microsoft Hololens 2' and 'Magic Leap One' platforms, both creating/enabling an ecosystem of novel applications for both industrial, educational and leisure life contexts. With further convergence of IoT, haptics, 5G, cloud and AI etc., spatial applications will range from contextually aware and interactive; digital information layering of objects, guidance and decision support systems (DSS) within business operations (such as for industrial machine manufacture, monitoring, and maintenance), digital modelling & prototyping in R&D, through to applications for communications and collaborations (such as for spatial tele co-presence of people, objects and environments). More broadly, these advances have potential to catalyse disruptions within business, through to the labour and consumer marketplace via: (1) Virtualisation of hardware resources (e.g. fully digitising workplace equipment such as displays and interfaces, raw inputs for prototyping and even digital rendering of spaces). (2) Protection and strengthening of institutional knowledge and performance via knowledge capture, guidance and decision support of labour tasks and activity (e.g. reducing labour (re)training (e.g. parts assembly), knowledge capture of practice). (3) Creation and distribution of new value propositions in goods and services (e.g. digital item ownership in a mixed-reality cloud, spatial applications for IoT enabled devices). (4) Displacement of geographic space as cost, talent, time, access and convenience constraints on business (e.g. available talent pool, partner/customer reach and relations). (5) Collaboration through new/enhanced affordances for workers (e.g. shared digitised work tools/environments). Therefore, a paradigm of spatial computing will challenge the IS community to research new ways of working, and consequences for worker experience, meaning, productivity and power. With emerging advances in AI, automation and spatial computing, one of the pertinent enquires concerns importance of workers (sense of) agency (Chandra et al. 2019). Control in the IT context has been conceptualised as control over work, control over self, and control over technology (Beaudry and Pinsonneault 2005), with prior IS work studying locus of control related to; work stress (Chandra et al. 2019), intrinsic and extrinsic motivations (Mujinga, M Eloff, MM Kroeze 2013), and performance (Vieira da Cunha et al. 2015) etc. With spatial computing platforms and their applications, what affordances of control and for whom should be developed? For example, the electronic representation of worker activity can be further enabled. Thus, tighter or looser coupling between worker activity and the reporting/outcome of work (Vieira da Cunha et al. 2015) becomes more of an organisational decision, with capability to monitor workers, and leverage AI for learning and optimisation. Furthermore, with development of spatial tele co-presence (STcP) (e.g. Mimesys), brings new affordances for communication with any worker(s), at any time, from anywhere. However, prior CMC research suggests people can choose different communication media specifically to manage social and emotional relationships (Madianou 2014) and their time (Mcloughlin et al. 2019). Hence, will such affordances serve greater identity fusion (Swann et al. 2012) and collaboration in organisations? Thus, we propose a socio-technical research agenda exploring 'control' related affordances for emerging spatial computing platforms, such as for STcP technology. In this regard, Control Theory can offer a useful starting frame, as it deals with control mechanisms governing workers organisational actions both formal (outcome and behaviour based) and informal (group and self-control), to further the interests of organisations (Kirsch 1996). We suggest, data and communication related affordances of control (e.g. privacy, exploitation, authenticity, availability and spaces) as starting points. Social Capital (Lin 2001), Social Influence (Kelman 1958), Social Identity (Ellemers and Haslam 2012), Identity Fusion (Swann et al. 2012) and Polymedia (Madianou and Miller 2012) being just some of the many relevant social theories to this endeavour",,Affordances of Control in a Paradigm of Spatial Computing Platforms,,https://core.ac.uk/download/398227078.pdf,,core
227535674,2019-09-10T00:00:00,"This work addresses three challenging issues about the overall applicability of hydrologic modelling. The first challenge is improving the collection of sub-surface data. Our approach uses a long-term deployment of wireless sensor network with environmental sensors. This approach is cost-effective when compared with the use of data-loggers and more flexible as it allows real-time monitoring of environmental variables. The plot scale environmental data is collected from our own WSN, deployed in western Pennsylvania, currently composed by 104 nodes and over 240 sensors including commercially available soil moisture, water potential and temperature sensors along with lab-made xylem sap flow sensors.

The second challenge is improving the availability and accuracy of continuous streamflow time-series estimates. The hydrometric network is modelled as a sparse Gaussian graphical model where each site represents a node in a graph. The graph model will have an edge between two sites only when their streamflow time-series are conditionally dependent given the other sites. A novel algorithm is presented, estimating a sparse graph by imposing sparsity to the precision (covariance inverse) matrix via the Graphical Lasso algorithm. The resulting graph is used for inference and a second algorithm determines which gauges can be removed with the least loss of information. The estimated streamflow time-series have better accuracy that other methods based on geographic proximity (least distance) or marginal correlation.

The third challenge is estimating the soil-water characteristics from biased and noisy observations of soil moisture. A novel method is presented for the simultaneous estimation of soil moisture and soil-related parameters. The simulation of soil moisture is performed using the Noah and the VIC models. The simulated site is a well-documented testbed in the state of Oklahoma. The calibration of the soil-related parameters uses Machine Learning techniques such as clustering, regression and classification, and soil-water correlations, providing physical and statistical constrains in the parameter space. Thus, the search is made within a reduced parameter space which makes the parameter calibration approach more effective and realistic. The performance of

v

the calibration algorithm is assessed regarding the quality of the soil moisture estimations while keeping the parameters in a feasible rang",,Understanding Hydrologic Processes and Correlations using Modeling and Machine Learning with Remote Sensing and In-Situ Wireless Sensor Network Data,,,,core
200818605,2019-05-31T00:00:00,"Due to its fast retrieval and storage efficiency capabilities, hashing has
been widely used in nearest neighbor retrieval tasks. By using deep learning
based techniques, hashing can outperform non-learning based hashing technique
in many applications. However, we argue that the current deep learning based
hashing methods ignore some critical problems (e.g., the learned hash codes are
not discriminative due to the hashing methods being unable to discover rich
semantic information and the training strategy having difficulty optimizing the
discrete binary codes). In this paper, we propose a novel image hashing method,
termed as \textbf{\underline{A}}symmetric \textbf{\underline{D}}eep
\textbf{\underline{S}}emantic \textbf{\underline{Q}}uantization
(\textbf{ADSQ}). \textbf{ADSQ} is implemented using three stream frameworks,
which consist of one \emph{LabelNet} and two \emph{ImgNets}. The
\emph{LabelNet} leverages the power of three fully-connected layers, which are
used to capture rich semantic information between image pairs. For the two
\emph{ImgNets}, they each adopt the same convolutional neural network
structure, but with different weights (i.e., asymmetric convolutional neural
networks). The two \emph{ImgNets} are used to generate discriminative compact
hash codes. Specifically, the function of the \emph{LabelNet} is to capture
rich semantic information that is used to guide the two \emph{ImgNets} in
minimizing the gap between the real-continuous features and the discrete binary
codes. Furthermore, \textbf{ADSQ} can utilize the most critical semantic
information to guide the feature learning process and consider the consistency
of the common semantic space and Hamming space. Experimental results on three
benchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the
proposed \textbf{ADSQ} can outperforms current state-of-the-art methods.Comment: Accepted to IEEE ACCESS. arXiv admin note: text overlap with
  arXiv:1812.0140",'Institute of Electrical and Electronics Engineers (IEEE)',Asymmetric Deep Semantic Quantization for Image Retrieval,10.1109/ACCESS.2019.2920712,http://arxiv.org/abs/1903.12493,,core
334837625,2019-07-18T00:00:00,"For a robot to learn a good policy, it often requires expensive equipment
(such as sophisticated sensors) and a prepared training environment conducive
to learning. However, it is seldom possible to perfectly equip robots for
economic reasons, nor to guarantee ideal learning conditions, when deployed in
real-life environments. A solution would be to prepare the robot in the lab
environment, when all necessary material is available to learn a good policy.
After training in the lab, the robot should be able to get by without the
expensive equipment that used to be available to it, and yet still be
guaranteed to perform well on the field. The transition between the lab
(source) and the real-world environment (target) is related to transfer
learning, where the state-space between the source and target tasks differ. We
tackle a simulated task with continuous states and discrete actions presenting
this challenge, using Bootstrapped Dual Policy Iteration, a model-free
actor-critic reinforcement learning algorithm, and Policy Shaping.
Specifically, we train a BDPI agent, embodied by a virtual robot performing a
task in the V-Rep simulator, sensing its environment through several proximity
sensors. The resulting policy is then used by a second agent learning the same
task in the same environment, but with camera images as input. The goal is to
obtain a policy able to perform the task relying on merely camera images",,Transfer Learning Across Simulated Robots With Different Sensors,,http://arxiv.org/abs/1907.07958,,core
334830909,2019-07-08T00:00:00,"Most deep reinforcement learning (RL) systems are not able to learn
effectively from off-policy data, especially if they cannot explore online in
the environment. These are critical shortcomings for applying RL to real-world
problems where collecting data is expensive, and models must be tested offline
before being deployed to interact with the environment -- e.g. systems that
learn from human interaction. Thus, we develop a novel class of off-policy
batch RL algorithms, which are able to effectively learn offline, without
exploring, from a fixed batch of human interaction data. We leverage models
pre-trained on data as a strong prior, and use KL-control to penalize
divergence from this prior during RL training. We also use dropout-based
uncertainty estimates to lower bound the target Q-values as a more efficient
alternative to Double Q-Learning. The algorithms are tested on the problem of
open-domain dialog generation -- a challenging reinforcement learning problem
with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we
can extract multiple different reward functions post-hoc from collected human
interaction data, and learn effectively from all of these. We test the
real-world generalization of these systems by deploying them live to converse
with humans in an open-domain setting, and demonstrate that our algorithm
achieves significant improvements over prior methods in off-policy batch RL",,"Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human
  Preferences in Dialog",,http://arxiv.org/abs/1907.00456,,core
201123275,2019-03-01T00:00:00,"Abstract Background The growth in publically available microbiome data in recent years has yielded an invaluable resource for genomic research, allowing for the design of new studies, augmentation of novel datasets and reanalysis of published works. This vast amount of microbiome data, as well as the widespread proliferation of microbiome research and the looming era of clinical metagenomics, means there is an urgent need to develop analytics that can process huge amounts of data in a short amount of time. To address this need, we propose a new method for the compact representation of microbiome sequencing data using similarity-preserving sketches of streaming k-mer spectra. These sketches allow for dissimilarity estimation, rapid microbiome catalogue searching and classification of microbiome samples in near real time. Results We apply streaming histogram sketching to microbiome samples as a form of dimensionality reduction, creating a compressed ‘histosketch’ that can efficiently represent microbiome k-mer spectra. Using public microbiome datasets, we show that histosketches can be clustered by sample type using the pairwise Jaccard similarity estimation, consequently allowing for rapid microbiome similarity searches via a locality sensitive hashing indexing scheme. Furthermore, we use a ‘real life’ example to show that histosketches can train machine learning classifiers to accurately label microbiome samples. Specifically, using a collection of 108 novel microbiome samples from a cohort of premature neonates, we trained and tested a random forest classifier that could accurately predict whether the neonate had received antibiotic treatment (97% accuracy, 96% precision) and could subsequently be used to classify microbiome data streams in less than 3 s. Conclusions Our method offers a new approach to rapidly process microbiome data streams, allowing samples to be rapidly clustered, indexed and classified. We also provide our implementation, Histosketching Using Little K-mers (HULK), which can histosketch a typical 2 GB microbiome in 50 s on a standard laptop using four cores, with the sketch occupying 3000 bytes of disk space. (https://github.com/will-rowe/hulk)",'Springer Science and Business Media LLC',Streaming histogram sketching for rapid microbiome analytics,10.1186/s40168-019-0653-2,,"[{'title': 'Microbiome', 'identifiers': ['2049-2618', 'issn:2049-2618']}]",core
196234290,2019-05-01T00:00:00,"abstract: Convolutional neural networks boast a myriad of applications in artificial intelligence, but one of the most common uses for such networks is image extraction. The ability of convolutional layers to extract and combine data features for the purpose of image analysis can be leveraged for pose estimation on an object - detecting the presence and attitude of corners and edges allows a convolutional neural network to identify how an object is positioned. This task can assist in working to grasp an object correctly in robotics applications, or to track an object more accurately in 3D space. However, the effectiveness of pose estimation may change based on properties of the object; the pose of a complex object, complexity being determined by internal occlusions, similar faces, etcetera, can be difficult to resolve.
This thesis is part of a collaboration between ASU’s Interactive Robotics Laboratory and NASA’s Jet Propulsion Laboratory. In this thesis, the training pipeline from Sharma’s paper “Pose Estimation for Non-Cooperative Spacecraft Rendezvous Using Convolutional Neural Networks” was modified to perform pose estimation on a complex object - specifically, a segment of a hollow truss. After initial attempts to replicate the architecture used in the paper and train solely on synthetic images, a combination of synthetic dataset generation and transfer learning on an ImageNet-pretrained AlexNet model was implemented to mitigate the difficulty of gathering large amounts of real-world data. Experimentation with pose estimation accuracy and hyperparameters of the model resulted in gradual test accuracy improvement, and future work is suggested to improve pose estimation for complex objects with some form of rotational symmetry",,Pose Estimation with Convolutional Neural Networks,,,,core
286729627,2019-01-01T00:00:00,"As High Performance Computing (HPC) has grown considerably and is expected to grow even more, effective resource management for distributed computing sys- tems is motivated more than ever. As the computational workloads grow in quantity, it is becoming more crucial to apply efficient resource management and workload scheduling to use resources efficiently while keeping the computational performance reasonably good. The problem of efficiently scheduling workloads on resources while meeting performance standards is hard. Additionally, non-clairvoyance of job dimen- sions makes resource management even harder in real-world scenarios. Our research methodology investigates the scheduling problem compliant for HPC and researches the challenges for deploying the scheduling in real world-scenarios using state of the art machine learning and data science techniques.To this end, this Ph.D. dissertation makes the following core contributions: a) We perform a theoretical analysis of space-sharing, non-preemptive scheduling: we studied this scheduling problem and proposed scheduling algorithms with polyno- mial computation time. We also proved constant upper-bounds for the performance of these algorithms. b) We studied the sensitivity of scheduling algorithms to the accuracy of runtime and devised a meta-learning approach to estimate prediction accuracy for newly submitted jobs to the HPC system. c) We studied the runtime prediction problem for HPC applications. For this purpose, we studied the distri- bution of available public workloads and proposed two different solutions that can predict multi-modal distributions: switching state-space models and Mixture Density Networks. d) We studied the effectiveness of recent recurrent neural network models for CPU usage trace prediction for individual VM traces as well as aggregate CPU usage traces. In this dissertation, we explore solutions to improve the performance of scheduling workloads on distributed systems.We begin by looking at the problem from the theoretical perspective. Modeling the problem mathematically, we first propose a scheduling algorithm that finds a constant approximation of the optimal solution for the problem in polynomial time. We prove that the performance of the algorithm (average completion time is the constant approximation of the performance of the optimal scheduling. We next look at the problem in real-world scenarios. Considering High-Performance Computing (HPC) workload computing environments as the most similar real-world equivalent of our mathematical model, we explore the problem of predicting application runtime. We propose an algorithm to handle the existing uncertainties in the real world and show-case our algorithm with demonstrative effectiveness in terms of response time and resource utilization. After looking at the uncertainty problem, we focus on trying to improve the accuracy of existing prediction approaches for HPC application runtime. We propose two solutions, one based on Kalman filters and one based on deep density mixture networks. We showcase the effectiveness of our prediction approaches by comparing with previous prediction approaches in terms of prediction accuracy and impact on improving scheduling performance. In the end, we focus on predicting resource usage for individual applications during their execution. We explore the application of recurrent neural networks for predicting resource usage of applications deployed on individual virtual machines. To validate our proposed models and solutions, we performed extensive trace-driven simulation and measured the effectiveness of our approaches","eScholarship, University of California","Scheduling, Characterization and Prediction of HPC Workloads for Distributed Computing Environments",,https://core.ac.uk/download/286729627.pdf,,core
216916814,2019-05-06T05:05:50,"The challenges of learning a new language can be reduced with real-time feedback on pronunciation and language usage. Today there are readily available technologies which provide such feedback on spoken languages, by translating the voice of the learner into written text. For someone seeking to learn American Sign Language (ASL), there is however no such feedback application available. A learner of American Sign Language might reference websites or books to obtain an image of a hand sign for a word. This process is like looking up a word in a dictionary, and if the person wanted to know if they were doing the sign correctly; or to know what a sign means, there is no way of checking. Because of this, the automated translation of ASL has been an active area of research since the early 2000’s. Researchers have investigated numerous ways of capturing hand signs, as well as numerous methods for recognizing or categorizing the captured data. In this work, we demonstrate an ASL translation system based on Convolutional Neural Networks (CNN), that provides a practical application for the real-time translation of 29 ASL signs, including 26 alphabet signs and three additional signs (‘space’, ‘delete’, and ‘nothing’). This application can be used to translate a hand sign for a person learning ASL as well as to facilitate communication between an ASL-signer and a non-signer. The CNN model used in this study is based on the Keras VGG161 pre-trained model and pre-processed images. It has 100% accuracy when predicting on a hold-out/cross-validation testing dataset. The keys to achieving this high precision in automated sign translation are 1) good input images, 2) starting from a pre-trained model 3) fine-tuning of the model. This paper discusses the use of contrast limiting adaptative histogram equalization (CLAHE) image pre-processing to enhance the input images, provides a high-level overview of convolution neural networks (CNN), discusses the use of the VGG161 pre- training model as a starting point for the CNN network and the fine-tuning of the resultant model, and provides an overview of the web application implemented for real-time ASL translation. The results of experiments used to assess the strength and generalization capabilities of the model are also detailed",SMU Scholar,ASL Reverse Dictionary -  ASL Translation Using Deep Learning,,https://core.ac.uk/download/216916814.pdf,,core
160782625,2019-05-04T00:00:00,"Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images
is a critical step in medical image analysis. Over the past few years, many
algorithms with impressive performances have been proposed. In this paper,
inspired by the idea of deep learning, we introduce an MRI denoising method
based on the residual encoder-decoder Wasserstein generative adversarial
network (RED-WGAN). Specifically, to explore the structure similarity between
neighboring slices, a 3D configuration is utilized as the basic processing
unit. Residual autoencoders combined with deconvolution operations are
introduced into the generator network. Furthermore, to alleviate the
oversmoothing shortcoming of the traditional mean squared error (MSE) loss
function, the perceptual similarity, which is implemented by calculating the
distances in the feature space extracted by a pretrained VGG-19 network, is
incorporated with the MSE and adversarial losses to form the new loss function.
Extensive experiments are implemented to assess the performance of the proposed
method. The experimental results show that the proposed RED-WGAN achieves
performance superior to several state-of-the-art methods in both simulated and
real clinical data. In particular, our method demonstrates powerful abilities
in both noise suppression and structure preservation.Comment: To appear on Medical Image Analysis. 29 pages, 15 figures, 7 table",,"Denoising of 3-D Magnetic Resonance Images Using a Residual
  Encoder-Decoder Wasserstein Generative Adversarial Network",,http://arxiv.org/abs/1808.03941,,core
334830531,2019-06-25T00:00:00,"Hyperparameter tuning is the main challenge of machine learning (ML)
algorithms. Grid search is a popular method in hyperparameter tuning of simple
ML algorithms; however, high computational complexity in complex ML algorithms
such as Deep Neural Networks (DNN) is the main barrier towards its practical
implementation. In this paper, two novel suboptimal grid search methods are
presented, which search the grid marginally and alternating. In order to
examine these methods, hyperparameter tuning is applied on two different DNN
based Optical Communication (OC) systems (Fiber OC, and Free Space Optical
(FSO) communication). The hyperparameter tuning of ML algorithms, despite its
importance is ignored in ML for OC investigations. In addition, this is the
first consideration of both FSO and Fiber OC systems in an ML for OC
investigation. Results indicate that despite greatly reducing computation load,
favorable performance could be achieved by the proposed methods. In addition,
it is shown that the alternating search method has better performance than
marginal grid search method. In sum, the proposed structures are
cost-effective, and appropriate for real-time applications",,"Novel Suboptimal approaches for Hyperparameter Tuning of Deep Neural
  Network [under the shelf of Optical Communication]",,http://arxiv.org/abs/1907.00036,,core
334847044,2019-08-13T00:00:00,"Automated machine learning has gained a lot of attention recently. Building
and selecting the right machine learning models is often a multi-objective
optimization problem. General purpose machine learning software that
simultaneously supports multiple objectives and constraints is scant, though
the potential benefits are great. In this work, we present a framework called
Autotune that effectively handles multiple objectives and constraints that
arise in machine learning problems. Autotune is built on a suite of
derivative-free optimization methods, and utilizes multi-level parallelism in a
distributed computing environment for automatically training, scoring, and
selecting good models. Incorporation of multiple objectives and constraints in
the model exploration and selection process provides the flexibility needed to
satisfy trade-offs necessary in practical machine learning applications.
Experimental results from standard multi-objective optimization benchmark
problems show that Autotune is very efficient in capturing Pareto fronts. These
benchmark results also show how adding constraints can guide the search to more
promising regions of the solution space, ultimately producing more desirable
Pareto fronts. Results from two real-world case studies demonstrate the
effectiveness of the constrained multi-objective optimization capability
offered by Autotune.Comment: 10 pages, 8 figures, accepted at DSAA 201",,Constrained Multi-Objective Optimization for Automated Machine Learning,,http://arxiv.org/abs/1908.04909,,core
189012049,2019-01-28T00:00:00,"International audienceDespite a large number of studies [2,3] over the years since the first discovery [7] and a couple of comprehensive reviews [8,9] the actual mechanism for PLD/CDW formation is still under debate. The most recent experimental [10-13] and theoretical [14] works focus on the large area growth of the CDW phase [13] the thickness dependence , and the possible unconventional behavior in the ultimate 2D limit of a single layer TiSe 2. [10-12,14] On the other hand, the other Ti dichalcogenides namely TiS 2 and TiTe 2 did not show any clear evidence until very recently when a CDW state was reported only for 1 monolayer (ML)-thin TiTe 2 at temperatures lower than 92 K. [15] It is surprising that the CDW in TiTe 2 was found to be totally suppressed for films thicker than 1 ML, [15] unlike the case of other TMDs where 1 ML and bulk-like films both make the transition to a CDW at nearly the same temperature. The interest about TiTe 2 is continuously increasing in view of theoretical predictions [16] and more recent experimental evidence [17] about pressure induced topological phase transitions in TiTe 2. The possibility to also manipulate superconduc-tivity by external pressure as predicted [18] and more recently evidenced [19] in bulk TiTe 2 creates the prospect to explore the emergence of topological superconductivity in this material. In the latter work [19] it has been shown that under nonhydro-static pressure, a CDW-like state with estimated transition temperature above room temperature (RT) appears in bulk TiTe 2 at around 0.5-1.8 GPa. These results call for a re-examination of the possibility to obtain a CDW in multilayer TiTe 2 and indeed at RT with good potential for real world applications utilizing the properties of the CDW state. These applications include a voltage-controlled oscillator device operating at room temperature , [20] fast electronic resistance switching for nonvolatile memories, [21,22] and field-effect transistor devices potentially suitable for implementation of non-Boolean logic. [23] In this paper it is shown that multilayer films (50 ML ≈ 32 nm), as well as single layer TiTe 2 epitaxially grown on InAs(111)/ Si(111) substrates by molecular beam epitaxy exhibit, in ambient pressure conditions, a CDW distortion at room temperature which is sustained up to higher temperatures, at least 400 °C, as evidenced by reflection high energy electron diffrac-tion (RHEED) (Figure S1, Supporting Information). The results are explained in terms of anisotropic strain imposed by the substrate. The group IVB 2D transition metal dichalcogenides are considered to be stable in the high symmetry trigonal octahedral structure due to the lack of unpaired d-electrons on the metal site. It is found that multilayer epitaxial TiTe 2 is an exception adopting a commensurate 2 × 2 × 2 charge density wave (CDW) structure at room temperature with an ABA type of stacking as evidenced by direct lattice imaging and reciprocal space mapping. The CDW is stabilized by highly anisotropic strain imposed by the substrate with an out-off-plane compression which reduces the interlayer van der Waals gap increasing the coupling between TiTe 2 layers",'Wiley',Room Temperature Commensurate Charge Density Wave in Epitaxial Strained TiTe 2 Multilayer Films,10.1002/admi.201801850,,,core
186318697,2019-01-25T00:00:00,"Despite inherent ill-definition, anomaly detection is a research endeavor of
great interest within machine learning and visual scene understanding alike.
Most commonly, anomaly detection is considered as the detection of outliers
within a given data distribution based on some measure of normality. The most
significant challenge in real-world anomaly detection problems is that
available data is highly imbalanced towards normality (i.e. non-anomalous) and
contains a most a subset of all possible anomalous samples - hence limiting the
use of well-established supervised learning methods. By contrast, we introduce
an unsupervised anomaly detection model, trained only on the normal
(non-anomalous, plentiful) samples in order to learn the normality distribution
of the domain and hence detect abnormality based on deviation from this model.
Our proposed approach employs an encoder-decoder convolutional neural network
with skip connections to thoroughly capture the multi-scale distribution of the
normal data distribution in high-dimensional image space. Furthermore,
utilizing an adversarial training scheme for this chosen architecture provides
superior reconstruction both within high-dimensional image space and a
lower-dimensional latent vector space encoding. Minimizing the reconstruction
error metric within both the image and hidden vector spaces during training
aids the model to learn the distribution of normality as required. Higher
reconstruction metrics during subsequent test and deployment are thus
indicative of a deviation from this normal distribution, hence indicative of an
anomaly. Experimentation over established anomaly detection benchmarks and
challenging real-world datasets, within the context of X-ray security
screening, shows the unique promise of such a proposed approach.Comment: Conference Submission. 8 pages, 9 figure",,"Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder
  Anomaly Detection",,http://arxiv.org/abs/1901.08954,,core
293746602,2019-01-01T00:00:00,"Fast and energy efficient processing of data has always been a key requirement in processor design. The latest developments in technology emphasize these requirements even further.
The widespread usage of mobile devices increases the demand of energy efficient solutions. Many new applications like advanced driver assistance systems focus more and more on machine learning algorithms and have to process large data sets in hard real time.
Up to the 1990s the increase in processor performance was mainly achieved by new and better manufacturing technologies for processors. That way, processors could operate at higher clock frequencies, while the processor microarchitecture was mainly the same. At the beginning of the 21st century this development stopped. New manufacturing technologies made it possible to integrate more processor cores onto one chip, but almost no improvements were achieved anymore in terms of clock frequencies. This required new approaches in both processor microarchitecture and software design. Instead of improving the performance of a single processor, the current problem has to be divided into several subtasks that can be executed in parallel on different processing elements which speeds up the application.

One common approach is to use multi-core processors or GPUs (Graphic Processing Units) in which each processing element calculates one subtask of the problem. This approach requires new programming techniques and legacy software has to be reformulated.
Another approach is the usage of hardware accelerators which are coupled to a general purpose processor. For each problem a dedicated circuit is designed which can solve the problem fast and efficiently. The actual computation is then executed on the accelerator and not on the general purpose processor. The disadvantage of this approach is that a new circuit has to be designed for each problem. This results in an increased design effort and typically the circuit can not be adapted once it is deployed.

This work covers reconfigurable hardware accelerators. They can be reconfigured during runtime so that the same hardware is used to accelerate different problems. During runtime, time consuming code fragments can be identified and the processor itself starts a process that creates a configuration for the hardware accelerator. This configuration can now be loaded and the code will then be executed on the accelerator faster and more efficient.
A coarse grained reconfigurable architecture was chosen because creating a configuration for it is much less complex than creating a configuration for a fine grained reconfigurable architecture like an FPGA (Field Programmable Gate Array). Additionally, the smaller overhead for the reconfigurability results in higher clock frequencies.
One advantage of this approach is that programmers don't need any knowledge about the underlying hardware, because the acceleration is done automatically during runtime. It is also possible to accelerate legacy code without user interaction (even when no source code is available anymore).

One challenge that is relevant for all approaches, is the efficient and fast data exchange between processing elements and main memory.
Therefore, this work concentrates on the optimization of the memory interface between the coarse grained reconfigurable hardware accelerator and the main memory. To achieve this, a simulator for a Java processor coupled with a coarse grained reconfigurable hardware accelerator was developed during this work.
Several strategies were developed to improve the performance of the memory interface. The solutions range from different hardware designs to software solutions that try to optimize the usage of the memory interface during the creation of the configuration of the accelerator.
The simulator was used to search the design space for the best implementation. With this optimization of the memory interface a performance improvement of  22.6% was achieved.

Apart from that, a first prototype of this kind of accelerator was designed and implemented on an FPGA to show the correct functionality of the whole approach and the simulator",,"Optimization of the Memory Subsystem of a Coarse
Grained Reconfigurable Hardware Accelerator",,,,core
237427583,2019-11-20T00:00:00,"The thesis studies building blocks for robot skill learning. Using these key components, learning frameworks can be constructed which provide robots with the capability to acquire a motion and manipulation skill autonomously. We study skill learning in two contexts: in-contact and free-space motions. In brief, this thesis investigates how to: (1) learn a policy for in-contact tasks; (2) generalize a free-space motion policy to new situations using a contextual skill model (CSM); and (3) transfer the CSM from simulation to real world. 

Learning an in-contact task such as wood planing from scratch can be time-consuming and dangerous. This problem can be avoided by imitating a policy from a human demonstration. However, a mere imitation may not satisfy the objective of the corresponding in-contact task. The thesis proposes a reinforcement learning (RL) framework for improving the performance of an imitated in-contact policy. The policy search for in-contact tasks has been achieved by making the motion compliant which allows for exploration in the force profile. 

Generalizing a policy to new situations is fundamental to skill learning as it alleviates the need to learn a new policy in every novel situation. Generalizing a policy refers to synthesizing a function mapping the policy to new situations. The function is referred to as a contextual policy or contextual skill model (CSM). The thesis proposes a parametric CSM. Experiments demonstrated that the parametric CSM can extract a global pattern from a database (DB) of policy parameters leading to significantly better extrapolation capability than with non-parametric CSMs. Furthermore, the underlying model of the CSM is fitted to the DB using a novel model selection approach to better represent the underlying regularities of the task. In order to speed the process of learning, the prediction uncertainty of the CSM is calculated using empirical Bayes (EB) and employed for guiding the exploration process of a model-free policy search. In addition, the most promising task is selected using a novel task manager, allowing for better future generalization performance achieved with minimum effort. In essence, the thesis presents an incremental learning framework,the main components of which are as follows: CSM, policy search, model selection, DB, EB, and a task manager implemented using active learning.

Learning a policy in a simulated environment and transferring it to the real world will alleviate the need to learn from scratch or from a demonstration. The thesis proposes to transfer a CSM instead of transferring a single control policy. We developed a simulation-to-real transfer framework which learns a source CSM in simulation incrementally and transfers it to the real world incrementally. Transference of the source CSM has been achieved using sample policies from the target environment. Experiments indicated that one sample policy is sufficient to transfer a CSM to the target environment. The target CSM improved the extrapolation capability significantly better than zero-shot transfer",'Institute of Electrical and Electronics Engineers (IEEE)',Incremental and Transfer Learning of Contextual Skill Model for Robots,10.1109/HUMANOIDS.2016.7803277,,,core
232001159,2019-05-06T05:05:50,"The challenges of learning a new language can be reduced with real-time feedback on pronunciation and language usage. Today there are readily available technologies which provide such feedback on spoken languages, by translating the voice of the learner into written text. For someone seeking to learn American Sign Language (ASL), there is however no such feedback application available. A learner of American Sign Language might reference websites or books to obtain an image of a hand sign for a word. This process is like looking up a word in a dictionary, and if the person wanted to know if they were doing the sign correctly; or to know what a sign means, there is no way of checking. Because of this, the automated translation of ASL has been an active area of research since the early 2000’s. Researchers have investigated numerous ways of capturing hand signs, as well as numerous methods for recognizing or categorizing the captured data. In this work, we demonstrate an ASL translation system based on Convolutional Neural Networks (CNN), that provides a practical application for the real-time translation of 29 ASL signs, including 26 alphabet signs and three additional signs (‘space’, ‘delete’, and ‘nothing’). This application can be used to translate a hand sign for a person learning ASL as well as to facilitate communication between an ASL-signer and a non-signer. The CNN model used in this study is based on the Keras VGG161 pre-trained model and pre-processed images. It has 100% accuracy when predicting on a hold-out/cross-validation testing dataset. The keys to achieving this high precision in automated sign translation are 1) good input images, 2) starting from a pre-trained model 3) fine-tuning of the model. This paper discusses the use of contrast limiting adaptative histogram equalization (CLAHE) image pre-processing to enhance the input images, provides a high-level overview of convolution neural networks (CNN), discusses the use of the VGG161 pre- training model as a starting point for the CNN network and the fine-tuning of the resultant model, and provides an overview of the web application implemented for real-time ASL translation. The results of experiments used to assess the strength and generalization capabilities of the model are also detailed",SMU Scholar,ASL Reverse Dictionary -  ASL Translation Using Deep Learning,,,,core
270039253,2019-10-01T00:00:00,"Intrusion Detection Systems (IDS) based on Artificial Intelligence can be deployed to protect telemetry networks against intruders. As security solutions which encrypt radio links do not accommodate the ever evolving network attacks and vulnerabilities, new defense mechanisms using machine learning and artificial intelligence can play a significant role for telemetry networks. This paper proposes a multi-layered Hidden Markov Model (HMM) IDS that addresses multi-stage attacks. This is due to the fact that intrusions are increasingly being launched through multiple phases instead of single stage intrusion. This layered model divides the problem space into smaller manageable pieces reducing the curse of dimensionality associated with HMMs. To verify the application of this model for real network, the NSL-KDD dataset is used to train and test the model.International Foundation for TelemeteringProceedings from the International Telemetering Conference are made available by the International Foundation for Telemetering and the University of Arizona Libraries. Visit http://www.telemetry.org/index.php/contact-us if you have questions about items in this collection",International Foundation for Telemetering,Multi-Stage Attack Detection Using Layered Hidden Markov Model Intrusion Detection System,,,"[{'title': None, 'identifiers': ['0884-5123', 'issn:0884-5123', 'issn:0074-9079', '0074-9079']}]",core
201721257,2019-01-01T00:00:00,"In face recognition systems, the use of convolutional neural networks (CNNs) permits to achieve good accuracy performances, which derive largely from a huge number of well-trained parameters. While using online services any mobile device can suffice for an accurate identification, in the offline scenario, implemented on a wearable mobile hardware, it is difficult to achieve both real-time responsiveness and high accuracy. In this paper we present a solution to replace a large open source face recognizer network (provided as part of the dlib libraries), distilling its learned knowledge into a less demanding CNN. The former is used as an expert oracle that provides the targets, while the latter is trained on the same input image, following a regression approach. In addition to lightness, our CNN is trained to use smaller input images, naturally allowing the recognition of identities in a wider distance range and with a reduced amount of computation. This eventually permits the porting of the network into a dedicated mobile accelerating hardware. The hypothesis we want to demonstrate is that since the feature space topology has been deeply explored during the training of the expert network, and due to the fact that no information is created during the up sampling of a tiny face to the input size of the expert oracle, the smaller network can provide the same accuracy at a reduced computational cost",MIPRO,Distillation of a CNN for a High Accuracy Mobile Face Recognition System,,,,core
231832132,2019-09-30T00:00:00,"Despite inherent ill-definition, anomaly detection is

a research endeavour of great interest within machine learning

and visual scene understanding alike. Most commonly, anomaly

detection is considered as the detection of outliers within a

given data distribution based on some measure of normality.

The most significant challenge in real-world anomaly detection

problems is that available data is highly imbalanced towards

normality (i.e. non-anomalous) and contains at most a sub-set

of all possible anomalous samples - hence limiting the use of

well-established supervised learning methods. By contrast, we

introduce an unsupervised anomaly detection model, trained only

on the normal (non-anomalous, plentiful) samples in order to

learn the normality distribution of the domain, and hence detect

abnormality based on deviation from this model. Our proposed

approach employs an encoder-decoder convolutional neural network with skip connections to thoroughly capture the multiscale distribution of the normal data distribution in image space.

Furthermore, utilizing an adversarial training scheme for this

chosen architecture provides superior reconstruction both within

image space and a lower-dimensional embedding vector space

encoding. Minimizing the reconstruction error metric within both

the image and hidden vector spaces during training aids the

model to learn the distribution of normality as required. Higher

reconstruction metrics during subsequent test and deployment

are thus indicative of a deviation from this normal distribution,

hence indicative of an anomaly. Experimentation over established anomaly detection benchmarks and challenging real-world

datasets, within the context of X-ray security screening, shows

the unique promise of such a proposed approach",'Institute of Electrical and Electronics Engineers (IEEE)',Skip-GANomaly: skip connected and adversarially trained encoder-decoder anomaly detection,10.1109/IJCNN.2019.8851808,https://core.ac.uk/download/231832132.pdf,,core
305121394,2019-12-01T00:00:00,"Big data analytics is a virtually new term in power system terminology. This concept delves into the way a massive volume of data is acquired, processed, analyzed to extract insight from available data. In particular, big data analytics alludes to applications of artificial intelligence, machine learning techniques, data mining techniques, time-series forecasting methods. Decision-makers in power systems have been long plagued by incapability and weakness of classical methods in dealing with large-scale real practical cases due to the existence of thousands or millions of variables, being time-consuming, the requirement of a high computation burden, divergence of results, unjustifiable errors, and poor accuracy of the model. Big data analytics is an ongoing topic, which pinpoints how to extract insights from these large data sets. The extant article has enumerated the applications of big data analytics in future power systems through several layers from grid-scale to local-scale. Big data analytics has many applications in the areas of smart grid implementation, electricity markets, execution of collaborative operation schemes, enhancement of microgrid operation autonomy, management of electric vehicle operations in smart grids, active distribution network control, district hub system management, multi-agent energy systems, electricity theft detection, stability and security assessment by PMUs, and better exploitation of renewable energy sources. The employment of big data analytics entails some prerequisites, such as the proliferation of IoT-enabled devices, easily-accessible cloud space, blockchain, etc. This paper has comprehensively conducted an extensive review of the applications of big data analytics along with the prevailing challenges and solutions",'Institute of Electrical and Electronics Engineers (IEEE)',Attributes of Big Data Analytics for Data-Driven Decision Making in Cyber-Physical Power Systems,10.1109/ipaps49326.2019.9069391,https://core.ac.uk/download/305121394.pdf,,core
334906654,2019-10-22T00:00:00,"Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.Comment: 23 pages, 4 figures, 10 pages of Extended Dat",'Elsevier BV',"Autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning",10.1016/j.xcrp.2020.100264,http://arxiv.org/abs/2001.09938,,core
354429012,2019-01-01T00:00:00,"2019 Fall.Includes bibliographical references.Artificial intelligence driven medical devices have created the potential for significant breakthroughs in healthcare technology. Healthcare applications using reinforcement learning are still very sparse as the medical domain is very complex and decision making requires domain expertise. High volumes of data generated from medical devices – a key input for delivering on the promise of AI, suffers from both noise and lack of ground truth. The cost of data increases as it is cleaned and annotated. Unlike other data sets, medical data annotation, which is critical for accurate ground truth, requires medical domain expertise for a high-quality patient outcome. While accurate recommendation of decisions is vital in this context, making them in near real-time on devices with computational resource constraint requires that we build efficient, compact representations of models such as deep neural networks. While deeper and wider neural networks are designed for complex healthcare applications, model compression can be an effective way to deploy networks on medical devices that often have hardware and speed constraints. Most state-of-the-art model compression techniques require a resource centric manual process that explores a large model architecture space to find a trade-off solution between model size and accuracy. Recently, reinforcement learning (RL) approaches are proposed to automate such a hand-crafted process. However, most RL model compression algorithms are model-free which require longer time with no assumptions of the model. On the contrary, model-based (MB) approaches are data driven; have faster convergence but are sensitive to the bias in the model. In this work, we report on the use of reinforcement learning to mimic the decision-making process of annotators for medical events, to automate annotation and labelling. The reinforcement agent learns to annotate alarm data based on annotations done by an expert. Our method shows promising results on medical alarm data sets. We trained deep Q-network and advantage actor-critic agents using the data from monitoring devices that are annotated by an expert. Initial results from these RL agents learning the expert-annotated behavior are encouraging and promising. The advantage actor-critic agent performs better in terms of learning the sparse events in a given state, thereby choosing more right actions compared to deep Q-network agent. To the best of our knowledge, this is the first reinforcement learning application for the automation of medical events annotation, which has far-reaching practical use. In addition, a data-driven model-based algorithm is developed, which integrates seamlessly with model-free RL approaches for automation of deep neural network model compression. We evaluate our algorithm on a variety of imaging data from dermoscopy to X-ray on different popular and public model architectures. Compared to model-free RL approaches, our approach achieves faster convergence; exhibits better generalization across different data sets; and preserves comparable model performance. The new RL methods' application to healthcare domain from this work for both false alarm detection and model compression is generic and can be applied to any domain where sequential decision making is partially random and practically controlled by the decision maker",Colorado State University. Libraries,Scalable and data efficient deep reinforcement learning methods for healthcare applications,,https://core.ac.uk/download/354429012.pdf,,core
395674862,2020-01-01T00:00:00,"Deep Neural Networks play a very significant role in computer vision applications like image classification, object recognition and detection. They have achieved great success in this field but the main obstacles for deploying a DNN model into an Autonomous Driver Assisted System (ADAS) platform are limited memory, constrained resources, and limited power. MobileNet is a very efficient and light DNN model which was developed mainly for embedded and computer vision applications, but researchers still faced many constraints and challenges to deploy the model into resource-constrained microprocessor units. Design Space Exploration of such CNN models can make them more memory efficient and less computationally intensive. We have used the Design Space Exploration technique to modify the baseline MobileNet V1 model and develop an improved version of it. This paper proposes seven modifications on the existing baseline architecture to develop a new and more efficient model. We use Separable Convolution layers, the width multiplier hyperparamater, alter the channel depth and eliminate the layers with the same output shape to reduce the size of the model. We achieve a good overall accuracy by using the Swish activation function, Random Erasing technique and a choosing good optimizer. We call the new model as Ultra-thin MobileNet which has a much smaller size, lesser number of parameters, less average computation time per epoch and negligible overfitting, with a little higher accuracy as compared to the baseline MobileNet V1. Generally, when an attempt is made to make an existing model more compact, the accuracy decreases. But here, there is no trade off between the accuracy and the model size. The proposed model is developed with the intent to make it deployable in a realtime autonomous development platform with limited memory and power and, keeping the size of the model within 5 MB. It could be successfully deployed into NXP i.MX RT1060 ADAS platform due to its small model size of 3.9 MB. It classifies images of different classes in real-time, with an accuracy of more than 90% when it is run on the above-mentioned ADAS platform. We have trained and tested the proposed architecture from scratch on the CIFAR-10 dataset",'Institute of Electrical and Electronics Engineers (IEEE)',Image Classification on NXP i.MX RT1060 using Ultra-thin MobileNet DNN,10.1109/CCWC47524.2020.9031165,https://core.ac.uk/download/395674862.pdf,,core
395004319,2020-01-01T00:00:00,"The end of Moore's law is driving the search for new techniques to improve system performance as applications continue to evolve rapidly and computing power demands continue to rise. One promising technique is to build more intelligent compilers.Compilers map high-level programs to lower-level primitives that run on hardware. During this process, compilers perform many complex optimizations to boost the performance of the generated code. These optimizations often require solving NP-Hard problems and dealing with an enormous search space. To overcome these challenges, compilers currently use hand-engineered heuristics that can achieve good but often far-from-optimal performance. Alternatively, software engineers resort to manually writing the optimizations for every section in the code, a burdensome process that requires prior experience and significantly increases the development time.In this thesis, novel approaches for automatically handling complex compiler optimization tasks are explored. End-to-end solutions using deep reinforcement learning and other machine learning algorithms are proposed. These solutions dramatically reduce the search time while capturing the code structure, different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine superior compiler optimizations. The proposed techniques can outperform existing state-of-the-art solutions while requiring shorter search time. Furthermore, unlike existing solutions, the deep reinforcement learning solutions are shown to generalize well to real benchmarks","eScholarship, University of California",Machine Learning in Compiler Optimization,,,,core
441300694,2020-01-01T00:00:00,"In recent years, visual object tracking has become a very active research field which is mainly divided into the correlation filter-based tracking and deep learning (e.g., deep convolutional neural network and Siamese neural network) based tracking. For target tracking algorithms based on deep learning, a large amount of computation is required, usually deployed on expensive graphics cards. However, for the rich monitoring devices in the Internet of Things, it is difficult to capture all the moving targets in each device in real time, so it is necessary to perform hierarchical processing and use tracking based on correlation filtering in insensitive areas to alleviate the local computing pressure. In sensitive areas, upload the video stream to a cloud computing platform with a faster computing speed to perform an algorithm based on deep features. In this paper, we mainly focus on the correlation filter-based tracking. In the correlation filter-based tracking, the discriminative scale space tracker (DSST) is one of the most popular and typical ones which is successfully applied to many application fields. However, there are still some improvements that need to be further studied for DSST. One is that the algorithms do not consider the target rotation on purpose. The other is that it is a very heavy computational load to extract the histogram of oriented gradient (HOG) features from too many patches centered at the target position in order to ensure the scale estimation accuracy. To address these two problems, we introduce the alterable patch number for target scale tracking and the space searching for target rotation tracking into the standard DSST tracking method and propose a visual object multimodality tracker based on correlation filters (MTCF) to simultaneously cope with translation, scale, and rotation in plane for the tracked target and to obtain the target information of position, scale, and attitude angle at the same time. Finally, in Visual Tracker Benchmark data set, the experiments are performed on the proposed algorithms to show their effectiveness in multimodality tracking",'Hindawi Limited',Visual Object Multimodality Tracking Based on Correlation Filters for Edge Computing,10.1155/2020/8891035,,"[{'title': 'Security and Communication Networks', 'identifiers': ['issn:1939-0122', 'issn:1939-0114', '1939-0114', '1939-0122']}]",core
299197198,2020-03-27T00:00:00,"Οι Διατάξεις Προγραμματιζόμενης Λογικής (Field Programmable Gate Arrays -
FPGAs) έχουν αυξανόμενο αντίκτυπο σε όλο και περισσότερες εφαρμογές, από τα
νευρωνικά δίκτυα (Deep Neural Networks) έως επεκτάσεις του ρεπερτορίου εντολών
(Instruction Set Extensions) σε στενώς συνεζευγμένα συστήματα με ενσωματωμένες
FPGAs (eFPGAs). Καθώς οι εφαρμογές αποκλίνουν στην πολυπλοκότητα, τις επιδόσεις,
τις ανάγκες μνήμης και τους περιορισμούς σε έκταση, υπάρχει ανάγκη για ένα ευρύτερο
φάσμα αρχιτεκτονικών FPGA. Ωστόσο, η ανάπτυξη και υλοποίηση των αρχιτεκτονικών
αυτών παραμένει δύσκολη και απαιτεί πολύ χρόνο, λόγω των υψηλών απαιτήσεών τους
σε ειδικευμένες σχεδιάσεις (custom layout) και της ανάγκης ανάπτυξης λογισμικού 
προσαρμοσμένου για τον προγραμματισμό κάθε αρχιτεκτονικής, οδηγώντας στην
παραγωγή προϊόντων πιο γενικού σκοπού.
Πολλές ακαδημαϊκές εργασίες επικεντρώνονται στην αυτοματοποιημένη
διαδικασία παραγωγής αρχιτεκτονικών FPGA, σε μια προσπάθεια να προωθηθεί η
εξατομίκευση και να μειωθεί η χρονική περίοδος μέχρι την αγορά. Σε άλλες
προσεγγίσεις, οι ερευνητές στοχεύουν στη διαδικασία εξερεύνησης, στην οποία
αναζητούν τη βέλτιστη αρχιτεκτονική για ένα συγκεκριμένο σενάριο, χρησιμοποιώντας
μοντέλα εκτιμήσεων μεγέθους και καθυστέρησης.
Στην εργασία αυτή επιλέγουμε να συνδυάσουμε τις δύο αυτές προσεγγίσεις.
Αναπτύσσουμε μια επέκταση για το δημοφιλές εργαλείο ανοιχτού κώδικα Verilog-toRouting (VTR) προκειμένου να εξάγουμε σε Verilog την αναπαράσταση των
αρχιτεκτονικών FPGA που έχουν οριστεί από τον χρήστη, να υποστηρίξουμε ζητούμενες
μονάδες ειδικού σκοπού (hard blocks - RAMs, DSPs, FP Units) και να παράξουμε αρχεία
προγραμματισμού της FPGA (Bitstreams) για δοθέντα benchmarks. Στόχος μας είναι να
δημιουργήσουμε συνθέσιμο κώδικα RTL ανεξάρτητο από τεχνολογία, ικανό να συντεθεί
με οποιαδήποτε βιβλιοθήκη Standard Cells. Ανακαλύπτουμε τις πραγματικές
σχεδιαστικές ιδιότητες μιας αρχιτεκτονικής FPGA χρησιμοποιώντας μια προτεινόμενη
ροή σχεδιασμού υλικού (ASIC flow) και ανακτούμε πραγματικές μετρήσεις μεγέθους και
καθυστέρησης και τελικά προχωρούμε στην εξερεύνηση των βέλτιστων αρχιτεκτονικών
FPGA για συγκεκριμένα σύνολα από benchmakrs.
Χρησιμοποιώντας την επέκτασή μας, εξερευνούμε το χώρο σχεδιασμού των FPGA
για ένα σύνολο από benchmakrs υψηλών επιδόσεων που εξάγονται από την πλατφόρμα
High Level Synthesis (HLS) της Xilinx. Η εξερεύνηση μας αρχίζει με τον εντοπισμό των
βέλτιστων αρχιτεκτονικών FPGA, ξεκινώντας από το μέγεθος των προγραμματιζόμενων
πυλών (Lookup Tables - LUTs) και τον αριθμό τους ανά ομάδα (Configurable Logic Block -
CLB) και έπειτα εξετάζοντας το μέγεθος και μήκος των καναλιών διασύνδεσης τους.
Συγκρίνουμε επίσης τις βέλτιστες αρχιτεκτονικές FPGA που προκύπτουν κατά τη χρήση
των benchmakrs υψηλών επιδόσεων με τις αντίστοιχες αρχιτεκτονικές που προκύπτουν
όταν χρησιμοποιούμε τα MCNC benchmarks γενικού σκοπού.
Τέλος, δημιουργούμε σύνολα εντολών TCL για τη σύνθεση και την υλοποίηση της
τοποθέτησης και διασύνδεσης (place and route) που μπορούν να προσαρμοστούν σε
οποιοδήποτε μέγεθος και χαρακτηριστικό αρχιτεκτονικής και να αυτοματοποιήσουν τη
ροή ASIC για νέα τσιπ FPGAField Programmable Gate Arrays (FPGAs) have an ever-expanding impact to more
and more applications, ranging from Deep Neural Networks to High-Performance
Computing (HPC) and other uses such as customization of Instruction Set Extensions and
computation offloading in systems with tightly coupled embedded FPGAs (eFPGAs). As
applications diverge in complexity, performance, memory needs and area limitations,
there is a need for a wider variety of FPGA architectures. However, developing and
implementing new FPGA architectures remains challenging and requires a lot of time, due
to their high content in custom layout designs and the need for design software and flows
tailored for each specific architecture, leading to the production of more generic
products.
Many academic works are focusing on the automated FPGA design generation
process, in an attempt to promote customizability and reduce time-to-market. In other
approaches, researchers target only the exploration process, in which they seek for the
optimal architecture for a specific case scenario, using area and delay estimation models.
In this thesis we choose to combine the two approaches. We develop an extension
for the popular open-source tool Verilog-to-Routing (VTR) in order to export in Verilog
the representation of user-specified FPGA architectures, develop support for custom user 
hard-blocks (RAMs, DSPs, FP Units), and generate Bitstreams for given benchmarks. Our
objective is to create synthesizable and technology independent RTL design code, able to
be synthesized with any standard cell library. We discover the real design properties of
an FPGA architecture using our proposed ASIC flow and retrieve real area and delay
measurements and eventually proceed with the exploration of optimal FPGA
architectures for given sets of benchmarks.
Using our VTR extension, we perform FPGA design-space exploration for a set of
HPC oriented benchmarks that are derived using Xilinx's High Level Synthesis (HLS). Our
exploration starts by identifying pareto-optimal FPGA architectures starting with the size
of Lookup Tables (LUTs) and the number of LUTs per Configurable Logic Block (CLB) and
then explore the size of routing channels and wire segments' configurations. We also
compare the optimal FPGA architectures derived when using the HPC benchmarks with
the respective architectures derived when we use the generic MCNC benchmarks.
Finally, we create TCL scripts for synthesis and back-end implementation (place
and route) which can adjust to any architectural characteristic and size and automate the
ASIC flow for new FPGA chips",,Εξερεύνηση αρχιτεκτονικών προγραμματιζόμενης λογικής για αποδοτική επιτάχυνση εφαρμογών υψηλών επιδόσεων,,,,core
328847576,2020-02-16T00:00:00,"La crescente attenzione riguardo alle tematiche ambientali sta portando sempre più all’attenzione i problemi legati all’inquinamento, specialmente per quanto riguarda l’emissione dei gas serra. Una delle principali cause dell’aumento di gas serra è senza dubbio l’utilizzo dei combustibili fossili, anche nel campo della mobilità. Per questo motivo, negli ultimi decenni, sono state cercate alternative più ecologiche: la tendenza attuale è senza dubbio quella di muoversi in direzione della trazione elettrica, e in special modo a guida autonoma. Il lavoro oggetto di tesi è appunto inquadrato all’interno del progetto europeo AutoDrive, il quale mira alla progettazione di componenti elettronici e architetture di tipo Fail-Operational, ovvero che continuano a eseguire un insieme definito delle loro funzioni anche in presenza di guasti, che permettano l’introduzione della guida autonoma in autoveicoli di tutte le categorie, con l’intento di contribuire a una mobilità più efficiente e sicura. Infatti, dato il crescente numero di implementazioni software e meccatroniche all’interno delle automobili, sono presenti sempre più rischi dovuti a loro possibili guasti, sia sistematici che aleatori, che devono quindi essere tenuti in considerazione al fine della sicurezza funzionale. Poiché quest’ultima venga garantita è necessario introdurre all’interno del sistema, in modo controllato, un qualche tipo di ridondanza che permetta di mascherare o individuare i guasti che si possono verificare.
La possibilità di realizzare veicoli a trazione elettrica è strettamente legata ai progressi conseguiti nel campo delle batterie, specialmente per quanto riguarda le tecnologie basate sugli ioni di Litio. Infatti, quest’ultime presentano una maggiore densità di energia e di potenza, una tensione di cella più elevata, la mancanza di effetto memoria e una minore corrente di auto scarica se confrontate con le altre chimiche esistenti. Grazie a questa serie di vantaggi, le tecnologie basate sugli ioni di Litio, rappresentano l’unico vero candidato per il futuro della mobilità elettrica. Questa tecnologia, tuttavia, presenta degli svantaggi in quanto è necessario l’inserimento di un sistema elettronico che monitori la batteria, il Battery Management System (BMS), il quale ha, tra gli altri, il compito di garantirne il corretto funzionamento in termini di range operativi di tensione, temperatura e corrente. Difatti, la fuoriuscita di queste grandezze dalla loro Safe Operating Area (SOA), oltre a portare un degradamento delle prestazioni della batteria, può provocare l’innescarsi di condizioni ben più gravi quali fughe termiche interne alle celle o persino l’esplosione delle celle stesse. Qualora il BMS identifichi una situazione critica per l’operatività della batteria, questo sistema interviene attraverso un sistema di feedback sul circuito, andando ad esempio a distaccare il carico. Il BMS misura costantemente tutte le grandezze fisiche delle celle che compongono la batteria, quali corrente, tensione e temperatura. A partire da queste, oltre a verificare se i dati sono all’interno della loro SOA, questo sistema esegue degli algoritmi di stima dello stato della batteria andando a ricavare dei parametri tipici quali lo stato di carica (SoC) e lo stato di salute (SoH) e, qualora fosse richiesto, si occupa di loggare e comunicare queste informazioni agli altri blocchi che compongono il sistema.
Nel caso in cui si voglia realizzare un intero sistema avente una batteria agli ioni litio e che presenti un comportamento di tipo Fail-Operational, come quello in esame nel progetto Autodrive, anche il BMS deve avere questa caratteristica. Per questo motivo, partendo dunque dalla struttura convenzionale del BMS, sono state aggiunte strutture ridondanti al fine di raggiungere l’obbiettivo preposto. La ridondanza può essere classificata in due grandi categorie, spaziale e temporale: la prima involve l’introduzione all’interno del sistema di componenti, o funzioni, che sarebbero inutili in ambienti privi di guasti, mentre la seconda è basata sulla ripetizione delle operazioni eseguite e il confronto con il risultato precedente. Specialmente in applicazioni safety-critical, quali quella automotive, la ridondanza di tipo spaziale è necessaria al fine di raggiungere i massimi livelli di sicurezza stabiliti dagli standard, quali ISO 26262.
Per questo motivo la struttura convenzionale del BMS è stata replicata e un’estensione del BMS stesso, che ricopre un ruolo decisionale, è stata sviluppata su una piattaforma FPGA. La scelta di utilizzare un FPGA invece di un microcontrollore, porta numerosi vantaggi, tra i quali un incremento delle capacità computazionali e una maggiore flessibilità e riconfigurabilità del sistema sviluppato. Nel dettaglio è stato scelto di modificare la struttura del BMS andando a sviluppare un sistema in triplice ridondanza nel quale, come suggerisce il nome, si ha una triplicazione parallela del sistema. I tre flussi di dati ottenuti vengono sottoposti a un sistema di voting a maggioranza, dal quale si ottiene un unico flusso di uscita, permettendo dunque di mascherare un guasto su uno dei tre ingressi. A questo scopo sono state sviluppate delle periferiche hardware su FPGA per eseguire le operazioni di voting sui dati e per stimare la regione di lavoro della batteria. È stata inoltre sviluppata una interfaccia grafica su PC, la quale permette di configurare opportunamente le periferiche implementate e mostra in tempo reale tutte le informazioni riguardo alla batteria, tra le quali i valori di tensione delle celle, le temperature e lo stato del sistema. Il sistema è stato infine testato andando a triplicare virtualmente il BMS convenzionale a nostra disposizione e, manipolando i dati su ciascuna linea in modo controllato, è stato verificato il corretto funzionamento delle periferiche sviluppate e validata l’architettura proposta.
#english version#
The growing awareness about the environmental issues is bringing to the attention the pollution topics, especially the greenhouse gas emission. One of the main causes is fossil fuel consumption for the energy production, even in the automotive systems. For this reason, during the last decades greener alternatives have been sought, and the actual trend is the one that leads to the electrical traction, especially towards the autonomous driving system. The thesis work is part of the European project AutoDrive, which aims at designing Fail-Operational electronic components and system architectures, that enables the introduction of automated driving in all car categories to make future mobility more efficient and safer. It is said that a system presents a Fail-Operational behaviour if it continues to execute a defined set of its function even in presence of faults. Given the increasing number of software and mechatronic implementations within the automotive systems, there are increasing risks from systematic failures and random hardware failures that must be considered within the scope of functional safety. Since this last one must be guaranteed, the introduction within the system of some kind of redundancy is mandatory in order to detect or mask the possible faults.
The possibility of developing electrical traction vehicles is closely related to the progress made in the battery field, especially with regards to the Lithium-ion (Li-ion) based technologies. In fact, these types of cells present a higher energy and power density, an higher cell voltage, no memory effect and a lower auto discharge current compared to the other chemistries. Thanks to these advantages, Li-ion based technologies are the only real candidate for the future electrical mobility. However, this chemistry also brings some disadvantages since a battery monitoring system, the Battery Management System (BMS), is mandatory. This system has to ensure the maintenance of the all cells composing the battery pack within their operative ranges in terms of voltages, temperatures and current. In fact, the coming out of one or more of these measures from its Safe Operating Area (SOA) brings a degradation of the cell’s performance, and could also lead to hazardous conditions, such us thermal runaway within the cell itself or even explosions. If the BMS identifies a critical condition for the battery functionality, it acts on the circuit though a feedback system, for example disconnecting the load. The BMS also uses the acquired measures in order to estimate typical battery parameters, such as State of Charge (SoC) and State of Health (SoH), and, if required, it also provides logging functionality and communicates to the other blocks composing the system.
In the case of a system containing a Li-ion battery with Fail-Operational behaviour has to be developed, such as the project AutoDrive one, even the BMS must show this characteristic. For this reason, starting from a conventional BMS, redundant structures have been added to the system in order to reach the responsible goal.
Redundancy can be classified into two main categories: space redundancy and time redundancy. The former involves the introduction within the system of components, or functions, that would be useless in a fault-free environment, while the latter is based on the repetition of the tasks and the comparison of the results to a stored copy of the previous ones. Especially in safety-critical applications, such as automotive, space redundancy is mandatory in order to ensure the safety level required by standards, such as ISO 26262. Therefore, the conventional structure of BMS has been replicated and an extension of BMS itself, which acts as decisional unit, has been developed on a FPGA platform. The FPGA approach, compared to a microcontroller-based one, brings several advantages, such as an increased computational capability and a higher flexibility and reconfigurability of the developed system. More specifically, the conventional structure of the BMS has been modified by using a Triple Modular Redundancy (TMR) approach. As the name suggests, TMR involves the triplication of the components to perform the same computation in parallel. The three obtained data flows are then subjected to a majority voting unit, which provides a single data flow as output, allowing to mask a fault in one of the inputs. For this purpose, hardware peripherals within the FPGA fabric have been developed in order to execute voting and operating area estimation algorithms. Furthermore, a PC graphical user interface has been developed and it allows to configure the hardware peripherals and to show real-time information about the battery pack, such as cell voltages, temperatures and current. The system has been finally tested by virtually triplicating the conventional BMS at our disposal and, manipulating each data flow in a controlled manner, the proper functioning of the developed peripherals has been verified and the proposed architecture has been validated",'Pisa University Press',FPGA Extension of a Battery Management System for Fail-Operational Control of Lithium-Ion Batteries in Safety-Critical Applications,,,,core
323312243,2020-05-20T00:00:00,"Well known to the machine learning community, the random feature model, originally introduced by Rahimi and Recht in 2008, is a parametric approximation to kernel interpolation or regression methods. It is typically used to approximate functions mapping a finite-dimensional input space to the real line. In this paper, we instead propose a methodology for use of the random feature model as a data-driven surrogate for operators that map an input Banach space to an output Banach space. Although the methodology is quite general, we consider operators defined by partial differential equations (PDEs); here, the inputs and outputs are themselves functions, with the input parameters being functions required to specify the problem, such as initial data or coefficients, and the outputs being solutions of the problem. Upon discretization, the model inherits several desirable attributes from this infinite-dimensional, function space viewpoint, including mesh-invariant approximation error with respect to the true PDE solution map and the capability to be trained at one mesh resolution and then deployed at different mesh resolutions. We view the random feature model as a non-intrusive data-driven emulator, provide a mathematical framework for its interpretation, and demonstrate its ability to efficiently and accurately approximate the nonlinear parameter-to-solution maps of two prototypical PDEs arising in physical science and engineering applications: viscous Burgers' equation and a variable coefficient elliptic equation",,The Random Feature Model for Input-Output Maps between Banach Spaces,,https://core.ac.uk/download/323312243.pdf,,core
322449361,2020-04-29T00:00:00,"Deep neural network (DNN) is an indispensable machine learning tool for
achieving human-level performance on many learning tasks. Yet, due to its
black-box nature, it is inherently difficult to understand which aspects of the
input data drive the decisions of the network. There are various real-world
scenarios in which humans need to make actionable decisions based on the output
DNNs. Such decision support systems can be found in critical domains, such as
legislation, law enforcement, etc. It is important that the humans making
high-level decisions can be sure that the DNN decisions are driven by
combinations of data features that are appropriate in the context of the
deployment of the decision support system and that the decisions made are
legally or ethically defensible. Due to the incredible pace at which DNN
technology is being developed, the development of new methods and studies on
explaining the decision-making process of DNNs has blossomed into an active
research field. A practitioner beginning to study explainable deep learning may
be intimidated by the plethora of orthogonal directions the field is taking.
This complexity is further exacerbated by the general confusion that exists in
defining what it means to be able to explain the actions of a deep learning
system and to evaluate a system's ""ability to explain"". To alleviate this
problem, this article offers a ""field guide"" to deep learning explainability
for those uninitiated in the field. The field guide: i) Discusses the traits of
a deep learning system that researchers enhance in explainability research, ii)
places explainability in the context of other related deep learning research
areas, and iii) introduces three simple dimensions defining the space of
foundational methods that contribute to explainable deep learning. The guide is
designed as an easy-to-digest starting point for those just embarking in the
field.Comment: Survey paper on Explainable Deep Learning, 54 pages including
  reference",,Explainable Deep Learning: A Field Guide for the Uninitiated,,http://arxiv.org/abs/2004.14545,,core
329131898,2020-08-30T00:00:00,"In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.Comment: 27 pages, 10 Figure",,"Deep Reinforcement Learning for Contact-Rich Skills Using Compliant
  Movement Primitives",,http://arxiv.org/abs/2008.13223,,core
334901983,2020-01-12T00:00:00,"With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.Comment: 7 pages, 11 figures, conferenc",,"Learning to drive via Apprenticeship Learning and Deep Reinforcement
  Learning",,http://arxiv.org/abs/2001.03864,,core
324197373,2020-06-16T00:00:00,"Many important problems in science and engineering, such as drug design,
involve optimizing an expensive black-box objective function over a complex,
high-dimensional, and structured input space. Although machine learning
techniques have shown promise in solving such problems, existing approaches
substantially lack sample efficiency. We introduce an improved method for
efficient black-box optimization, which performs the optimization in the
low-dimensional, continuous latent manifold learned by a deep generative model.
In contrast to previous approaches, we actively steer the generative model to
maintain a latent manifold that is highly useful for efficiently optimizing the
objective. We achieve this by periodically retraining the generative model on
the data points queried along the optimization trajectory, as well as weighting
those data points according to their objective function value. This weighted
retraining can be easily implemented on top of existing methods, and is
empirically shown to significantly improve their efficiency and performance on
synthetic and real-world optimization problems.Comment: 22 pages, 13 figures. Includes supplementary materia",,"Sample-Efficient Optimization in the Latent Space of Deep Generative
  Models via Weighted Retraining",,http://arxiv.org/abs/2006.09191,,core
342060224,2020-01-01T00:00:00,"Tensegrity structures are an emergent type of soft-robotics that are compliant, lightweight, and impact-resilient. In collaboration with NASA Ames Research Center, research in the Berkeley Emergent Space Tensegrities Lab at UC Berkeley has largely focused on the design and control of these novel structures as potential surface exploration robots which could act as both landers and rovers. More recently, tensegrity robots have also been proposed for applications closer to home – working as disaster response and emergency co-robots to help first responders obtain situational awareness faster and safer. Constructed using isolated rigid bodies suspended in a tension network of elastic elements, tensegrity structures exhibit unique and advantageous mechanical properties for applications in uncertain and potentially hazardous environments, albeit at the cost of increased complexity for dynamic feedback control.  In addressing these challenges, this work explores possible approaches for feedback control and state estimation for ground-based rolling locomotion with six-bar spherical tensegrities. In this dissertation, we explore problems pertaining to practical implementation – state estimation, modeling, motion planning, and optimal control of tensegrity robots under uncertainty. Leveraging the well-structured dynamics of Class-1 tensegrity robots, we implement and evaluate model-based Model Predictive Control and iterative local quadratic methods for tensegrity motion planning. Additionally, we consider alternative tensegrity topologies and actuator schema which may enable improved performance for task-specific objectives. Due to the many degrees of freedom and compliant nature of tensegrity structures, however, excessive state estimate errors may propagate catastrophically. To evaluate these effects, Bayesian state estimators are applied to tensegrity ground mobility in simulation, evaluating their performance under the additional constraints of low-cost sensors and potentially scarce and noisy sensor data. An imitation learning approach is introduced to achieve directed rolling motion using a contextual neural network policy, combining deep learning and optimal control for real-time feedback control of highly nonlinear tensegrity systems. Finally, a robust minimax control approach is proposed in order to address challenges which arise at the intersection and interaction of state estimation and trajectory optimization for flexible tensegrity robotics.  Combined, these pragmatic research developments help advance the progression of this novel technology towards becoming a viable and more widely adopted robotics paradigm","eScholarship, University of California","Design, Control, and Motion Planning of Cable-Driven Flexible Tensegrity Robots",,,,core
345057927,2020-10-01T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and analysis in order to improve the process representation. This paper presents the development and implementation of quality monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality variable, such as surface roughness. Firstly, the Hybrid Incremental Modelling strategy is applied. Secondly, a Generalized Fuzzy Clustering C-Means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing is performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to manufacturing industry",'Techno-Press',Quality Monitoring of Complex Manufacturing Systems on the basis of Model Driven Approach,10.12989/sss.2020.26.4.495,,,core
387296663,2020-12-18T00:00:00,"Green Security Games (GSGs) have been successfully used in the protection of
valuable resources such as fisheries, forests and wildlife. While real-world
deployment involves both resource allocation and subsequent coordinated
patrolling with communication and real-time, uncertain information, previous
game models do not fully address both of these stages simultaneously.
Furthermore, adopting existing solution strategies is difficult since they do
not scale well for larger, more complex variants of the game models.
  We therefore first propose a novel GSG model that combines defender
allocation, patrolling, real-time drone notification to human patrollers, and
drones sending warning signals to attackers. The model further incorporates
uncertainty for real-time decision-making within a team of drones and human
patrollers. Second, we present CombSGPO, a novel and scalable algorithm based
on reinforcement learning, to compute a defender strategy for this game model.
CombSGPO performs policy search over a multi-dimensional, discrete action space
to compute an allocation strategy that is best suited to a best-response
patrolling strategy for the defender, learnt by training a multi-agent Deep
Q-Network. We show via experiments that CombSGPO converges to better strategies
and is more scalable than comparable approaches. Third, we provide a detailed
analysis of the coordination and signaling behavior learnt by CombSGPO, showing
group formation between defender resources and patrolling formations based on
signaling and notifications between resources. Importantly, we find that
strategic signaling emerges in the final learnt strategy. Finally, we perform
experiments to evaluate these strategies under different levels of uncertainty.Comment: Accepted at AAMAS 202",'Test accounts',"Reinforcement Learning for Unified Allocation and Patrolling in
  Signaling Games with Uncertainty",10.5555/3463952,http://arxiv.org/abs/2012.10389,,core
287844227,2020-02-07T10:21:57,"Intelligent robotic systems are becoming essential for space applications, industries, nuclear plants and for harsh environments in general, such as the European Organization for Nuclear Research (CERN) particles accelerator complex and experiments. Robotics technology has huge potential benefits for people and its ultimate scope depends on the way this technology is  used. In order to increase safety and machine availability, robots can perform repetitive, unplanned and dangerous tasks, which humans either prefer to avoid or are unable to carry out due to hazards, size constraints, or the extreme environments in which they take place. Nowadays, mechatronic systems use mature technologies that allow their robust and safe use, even in collaboration with human workers. Over the past years, the progress of robots has been based on the development of smart sensors, artificial intelligence and modular mechanical systems. Due to the multiple challenges that hazardous and unstructured environments have for the application of autonomous industrial systems, there is still a high demand for intelligent and teleoperation systems that give the control of a robot (slave) to a human operator via haptic input devices (master), as well as using human-supervised telerobotic control techniques. Modern techniques like simulation and virtual reality systems can facilitate the preparation of ad-hoc mechatronic tools and robotic intervention including recovery scenarios and failure mode analysis.  The basic contribution of this thesis is the development of a novel robotic framework for autonomous inspections and supervised teleoperations in harsh environments. The proposed framework covers all aspects of a robotic intervention, from the specification and operator training, the choice of the robot and its material in accordance with possible radiological contamination risks, to the realization of the intervention, including procedures and recovery scenarios. In a second set of contributions, new methods for mutirobots maintenance operations are developed, including intervention preparation and best practices for remote handling and advanced tools. The third set of contributions is built on a novel multimodal user-friendly human-robot interface that allows  operator training using virtual reality systems and technicians not expert in robot operation to perform inspection/maintenance tasks. In this thesis, we exploit a robotic system able to navigate autonomously and to inspect unknown environments in a safe way. A new real-time control system has been implemented in order to guarantee a fast response to environmental changes and adaptation to different type of scenarios the robot may find in a semi-structured and hazardous environment. The proposed new robotic control system  has been integrated on different robots, tested and validated with several robotic interventions in the CERN hazardous particle accelerator complex",,A Novel Robotic Framework for Safe Inspection and Telemanipulation in Hazardous and Unstructured Environments,,,,core
287866167,2020-12-01T00:00:00,"A video game is an interactive software able to arouse intense emotions in players. Consequentially, different theories have been proposed to understand which game aspects are able to affect the players\u2019 emotional state. However, only few works have tried to use empirical evidence to investigate the effects of different game aspects of the players\u2019 emotions. In this paper, we present the results of a set of experiments aimed at predicting the players\u2019 emotions during video games sessions using their physiological data. We have created a physiological dataset from the data acquired by 33 participants during video game fruition using a standard monitor and a Virtual Reality headset. The dataset contains information about electrocardiogram, 5 facials electromyographies, electrodermal activity, and respiration. Furthermore, we have asked the players to self-assess their emotional state on the Arousal and Valence space. We have then analyzed the contribution of each physiological signal to the overall definition of the players\u2019 mental state. Finally, we have applied Machine Learning techniques to predict the emotional state of players during game sessions at a precision of one second. The obtained results can contribute to define game devices and engines able to detect physiological data, as well to improve the game design process",'Springer Science and Business Media LLC',An empirical study of players&#8217; emotions in VR racing games based on a dataset of physiological data,10.1007/s11042-019-08585-y,,,core
478744115,2020-01-01T00:00:00,"Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named ""Latent Replay""where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device",'Institute of Electrical and Electronics Engineers (IEEE)',Latent replay for real-time continual learning,10.1109/IROS45743.2020.9341460,,,core
395096888,2020-02-09T00:00:00,"Freight transportation industry is characterized by several decisional problems that operations managers have to cope with. Not only the routes planning must be realized before their execution, but also other types of decisions must be taken, in order to answer events that may dynamically occur during operations, as for instance road network congestion or vehicle

failures. Each decision can involve different aspects: for instance, the price negotiation of a just-in-time order should take into consideration the current routes status and planning. Off-the-shelf decision support software, although able to independently support the decision makers in each area, tend to keep tasks compartmentalized.

Trans-Cel, a small trucking company in Padova (Italy), has a Research and Development branch developing a cloud-based platform, called Chainment, able to host different decision support tools that can communicate through a data sharing system. These tools rely on an algorithmic engine that includes a routing optimization algorithm and artificial intelligence systems. In particular, the routing problem combines express couriers requirements, generally studied in urban contexts, with routes and vehicle features typical of medium- and long-haul trips, showing interesting characteristics that are worth of study in the Operation Research field.

In this thesis, we focus on the design of an optimization algorithm able to provide a solution to a Vehicle Routing Problem (VRP) inspired by the Trans-Cel scenario, that we name Express Pickup and Delivery in freight Trucking problem (EPDT).

The classical VRP definition includes a set of customers and a fleet of vehicles and aims to define a set of routes such that all customers are visited exactly once while minimizing the overall distance traveled. In the scientific literature, the basic definition of the problem has been generalized in order to consider additional attributes, often rising from real-world scenarios, as for instance capacity of vehicles, time windows and orders with both pickup and delivery operations. Often, in real-world cases, decision makers must simultaneously deal with a large number of attributes, thus defining a class of routing problems called Multi-Attribute VRP (MAVRP), which includes EPDT.

The thesis proposes a meta-heuristic algorithm for the solution of EPDT, with the aim of embedding it in the algorithmic engine of Chainment. In order to comply with the platform  requirements, the algorithm is designed so that a solution is returned within few seconds.

The solution method we propose consists of a two-level heuristic: at the first level, a Tabu Search algorithm hybridized with a Variable Neighborhood Descent explores the order-to-vehicle assignments, while, at the second level, it makes use of a Local Search to determine the sequence of customers visited and obtain an evaluation of routes.

The algorithm efficiency is enhanced by the use of a granular exploration, by procedures for fast evaluation of solutions in the neighborhoods, and parallel implementation of specific algorithmic components. These elements are adapted to the specific attributes of EPDT and represent some of the thesis contributions. The improvement in computational times have been validated by the experimental results, verifying the desired requirements for the platform integration.

The quality of the solutions obtained with the proposed meta-heuristic algorithm has been assessed both on the field, by comparison with Trans-Cel operations managers, and through bounds obtained with mathematical programming methods. To this purpose, the thesis proposes an Integer Linear Programming formulation for EPDT and a solution method for its continuous

relaxation based on Column Generation. In particular, the thesis presents new pricing procedures suitable for the specific EPDT attributes. The available bounds show optimality or near-optimality of solutions provided by the heuristic algorithm for real instances. Moreover, the algorithm has been tested on literature benchmarks related to the Pickup and Delivery

Problem with Time Windows (PDPTW), providing solutions that are competitive with the state-of-the-art.

The thesis also proposes a preliminary study of new approaches for vehicle routing problems in dynamic contexts. In particular, the thesis explores the possibility of taking advantage from the availability of historical data on orders by means of anticipatory strategies. The first strategy is based on clustering methods that are applied to the orders to define space-time

points that aggregate the information on future demand. A second strategy is based on the concept of accessibility, as defined in the discrete choice theory and urban logistic, to represent the route capability of intercepting future orders.

The heuristic algorithm proposed for EPDT has been integrated in the algorithmic engine of the Chainment platform at Trans-Cel. The thesis describes integration and the adaptation of the proposed optimization algorithms for a proper interaction with the different modules in the operational context handled by the platform, as, for instance, initial routes planning, reacting to dynamic events or order price negotiation",,Solving a Multi-Attribute Vehicle Routing Problem in the freight delivery industry,,,,core
343215263,2020-01-01T00:00:00,"Classification of multivariate time series (MTS) has
been tackled with a large variety of methodologies and applied
to a wide range of scenarios. Reservoir computing (RC) provides
efficient tools to generate a vectorial, fixed-size representation of
the MTS that can be further processed by standard classifiers.
Despite their unrivaled training speed, MTS classifiers based on
a standard RC architecture fail to achieve the same accuracy of
fully trainable neural networks. In this article, we introduce the
reservoir model space, an unsupervised approach based on RC
to learn vectorial representations of MTS. Each MTS is encoded
within the parameters of a linear model trained to predict a
low-dimensional embedding of the reservoir dynamics. Compared
with other RC methods, our model space yields better representations and attains comparable computational performance due to
an intermediate dimensionality reduction procedure. As a second
contribution, we propose a modular RC framework for MTS
classification, with an associated open-source Python library. The
framework provides different modules to seamlessly implement
advanced RC architectures. The architectures are compared with
other MTS classifiers, including deep learning models and time
series kernels. Results obtained on the benchmark and real-world
MTS data sets show that RC classifiers are dramatically faster
and, when implemented using our proposed representation, also
achieve superior classification accurac",'Institute of Electrical and Electronics Engineers (IEEE)',Reservoir computing approaches for representation and classification of multivariate time series,10.1109/TNNLS.2020.3001377,,"[{'title': 'IEEE Transactions on Neural Networks and Learning Systems', 'identifiers': ['2162-2388', 'issn:2162-237X', 'issn:2162-2388', '2162-237x']}]",core
477767287,2020-11-09T08:00:00,"Deep neural networks have become very successful at solving many complex tasks such as image classification, image segmentation, and speech recognition. These models are composed of multiple layers that have the capacity to learn increasingly higher-level features, without prior handcrafted specifications. However, the success of a deep neural network relies on finding the proper configuration for the task in hand. Given the vast number of hyperparameters and the massive search space, manually designing or fine-tuning deep learning architectures requires extensive knowledge, time, and computational resources.
There is a growing interest in developing methods that automatically design a neural network´s architecture, known as neural architecture search (NAS). NAS is usually modeled as a single-objective optimization problem where the aim is to find an architecture that maximizes the prediction´s accuracy. However, most deep learning applications require accurate as well as efficient architectures to reduce memory consumption and enable their use in computationally-limited environments. This has led to the need to model NAS as a multiple objective problem that optimizes both the predictive performance and efficiency of the network. Furthermore, most NAS framework have focused on either optimizing the micro-structure (structure of the basic cell), or macro-structure (optimal number of cells and their connection) of the architecture. Consequently, manual engineering is required to find the topology of the non-optimized structure.
Although NAS has demonstrated great potential in automatically designing an architecture, it remains a computationally expensive and time-consuming process because it requires training and evaluating many potential configurations. Recent work has focused on improving the search time of NAS algorithms, but most techniques have been developed and applied only for single-objective optimization problems. Given that optimizing multiple objectives has a higher complexity and requires more iterations to approximate the Pareto Front, it is critical to investigate algorithms that decrease the search time of multiobjective NAS.
One critical application of deep learning is medical image segmentation. Segmentation of medical images provides valuable information for various critical tasks such as analyzing anatomical structures, monitoring disease progression, and predicting patient outcomes. Nonetheless, achieving accurate segmentation is challenging due to the inherent variability in appearance, shape, and location of the region of interest (ROI) between patients and the differences in imagining equipment and acquisition protocols. Therefore, neural networks are usually tailored to a specific application, anatomical region, and image modality. Moreover, medical image data is often volumetric requiring expensive 3D operations that result in large and complex architectures. Hence, training and deploying them requires considerable storage and memory bandwidth that makes them less suitable for clinical applications.
To overcome these challenges, the main goal of this research is to automatically design accurate and efficient deep neural networks using multiobjective optimization algorithms for medical image segmentation. The proposed research consists of three major objectives: (1) to design a deep neural network that uses a multiobjective evolutionary based algorithm to automatically adapt to different medical image datasets while minimizing the model’s size; (2) to design a self-adaptive 2D-3D Fully Convolutional network (FCN) ensemble that incorporates volumetric information and optimizes both the performance and the size of the architecture; and (3) to design an efficient multiobjective neural architecture search framework that decreases the search time while simultaneously optimizing the micro- and macro-structure of the neural architecture.
For the first objective, a multiobjective adaptive convolutional neural network named AdaResU-Net is presented for 2D medical image segmentation. The proposed AdaResU-Net is comprised of a fixed architecture and a learning framework that adjusts the hyperparameters to a particular training dataset using a multiobjective evolutionary based algorithm (MEA algorithm). The MEA algorithm evolves the AdaResU-Net network to optimize both the segmentation accuracy and model size. In the second objective, a self-adaptive ensemble of 2D-3D FCN named AdaEn-Net is proposed for 3D medical image segmentation. The AdaEn-Net is comprised of a 2D FCN that extracts intra-slice and long-range 2D context, and a 3D FCN architecture that exploits inter-slice and volumetric information. The 2D and 3D FCN architectures are automatically fitted for a specific medical image segmentation task by simultaneously optimizing the expected segmentation error and size of the network using the MEA algorithm. Finally, for the third objective, an efficient multiobjective neural architecture search framework named EMONAS is presented for 3D medical image segmentation. EMONAS has two main components, a novel search space that includes the hyperparameters that define the micro- and macro-structure of the architecture, and a Surrogate-assisted multiobjective evolutionary based algorithm (SaMEA algorithm) that efficiently searches for the best hyperparameter values using a Random Forest surrogate and guiding selection probabilities.
The broader impact of the proposed research is as follows: (1) automating the design of deep neural networks’ architecture and hyperparameters to improve the performance and efficiency of the models; and (2) increase the accessibility of deep learning to a broader range of organizations and people by reducing the need of expert knowledge and GPU time when automatically designing deep neural networks. In the medical area, the proposed models aim to improve the automatic extraction of data from medical images to potentially enhance diagnosis, treatment planning and survival prediction of various diseases such as cardiac disease and prostate cancer. Although the proposed techniques are applied to medical image segmentation tasks, they can also be implemented in other applications where accurate and resource-efficient deep neural networks are needed such as autonomous navigation, augmented reality and internet-of-things",Digital Commons @ University of South Florida,Efficient Neural Architecture Search with Multiobjective Evolutionary Optimization,,,,core
346571456,2020-01-01T00:00:00,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space",'Institute of Electrical and Electronics Engineers (IEEE)',Robustness of 3D deep learning in an adversarial setting,,,,core
322980330,2020-05-18T00:00:00,"Neural networks are versatile tools for computation, having the ability to
approximate a broad range of functions. An important problem in the theory of
deep neural networks is expressivity; that is, we want to understand the
functions that are computable by a given network. We study real infinitely
differentiable (smooth) hierarchical functions implemented by feedforward
neural networks via composing simpler functions in two cases:
  1) each constituent function of the composition has fewer inputs than the
resulting function;
  2) constituent functions are in the more specific yet prevalent form of a
non-linear univariate function (e.g. tanh) applied to a linear multivariate
function.
  We establish that in each of these regimes there exist non-trivial algebraic
partial differential equations (PDEs), which are satisfied by the computed
functions. These PDEs are purely in terms of the partial derivatives and are
dependent only on the topology of the network. For compositions of polynomial
functions, the algebraic PDEs yield non-trivial equations (of degrees dependent
only on the architecture) in the ambient polynomial space that are satisfied on
the associated functional varieties. Conversely, we conjecture that such PDE
constraints, once accompanied by appropriate non-singularity conditions and
perhaps certain inequalities involving partial derivatives, guarantee that the
smooth function under consideration can be represented by the network. The
conjecture is verified in numerous examples including the case of tree
architectures which are of neuroscientific interest. Our approach is a step
toward formulating an algebraic description of functional spaces associated
with specific neural networks, and may provide new, useful tools for
constructing neural networks.Comment: 52 pages, 17 figure",,"PDE constraints on smooth hierarchical functions computed by neural
  networks",,http://arxiv.org/abs/2005.08859,,core
327989909,2020-08-10T00:00:00,"Three dimensional convolutional neural networks (3DCNNs) have been applied in
many tasks, e.g., video and 3D point cloud recognition. However, due to the
higher dimension of convolutional kernels, the space complexity of 3DCNNs is
generally larger than that of traditional two dimensional convolutional neural
networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining
environments such as embedded devices, neural network compression is a
promising approach. In this work, we adopt the tensor train (TT) decomposition,
a straightforward and simple in situ training compression method, to shrink the
3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT
format, we investigate how to select appropriate TT ranks for achieving higher
compression ratio. We have also discussed the redundancy of 3D convolutional
kernels for compression, core significance and future directions of this work,
as well as the theoretical computation complexity versus practical executing
time of convolution in TT. In the light of multiple contrast experiments based
on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT
decomposition can compress 3DCNNs by around one hundred times without
significant accuracy loss, which will enable its applications in extensive real
world scenarios.Comment: Accepted by Neural Networks. Please see the final version by the DOI
  belo",'Elsevier BV',Compressing 3DCNNs Based on Tensor Train Decomposition,10.1016/j.neunet.2020.07.028,http://arxiv.org/abs/1912.03647,,core
357279592,2020-03-06T00:00:00,"AS ROBOTS INCREASINGLY BE- come part of our everyday lives, they will serve as caretakers for the elderly and disabled, assistants in surgery and rehabilitation, and educational toys. But for this to happen, programming and control must become simpler and human-robot interaction more natural. Both challenges are particularly relevant to humanoid robots, which are highly difficult to control yet most natural for interaction with people and operation in human environments. As this article shows, we have used biologically inspired notions of behavior-based control to address these challenges at the University of Southern California&apos;s Interaction Lab, part of the USC Robotics Research Labs. By endowing robots with the ability to imitate, we can program and interact with them through human demonstration, a natural human-humanoid interface. The human ability to imitate-to observe and repeat behaviors performed by a teacher-is a poorly understood but powerful form of skill learning. Two fundamental open problems in imitation involve interpreting and understanding the observed behavior and integrating the visual perception and movement control systems to reconstruct what was observed. Our research has a similarly twofold goal: we are developing methods for segmenting and classifying visual input for recognizing human behavior as well as methods for structuring the motor control system for general movement and imitation-learning capabilities. Our approach brings these two pursuits together much as the evolutionary process brought them together in biological systems. 1,2 We structure the motor system into a collection of movement primitives, which then serve both to generate the humanoid&apos;s movement repertoire and to provide prediction and classification capabilities for visual perception and interpretation of movement. This way, what the humanoid can do helps it understand what it sees and vice versa. The more it sees, the more it learns to do, and thus the better it gets at understanding what it sees for further learning; this is the imitation process. Behavior-based robotics Our work over the last 15 years has focused on developing distributed, behavior-based methods for controlling groups of mobile robots and, most recently, humanoids. Behavior-based control involves the design of control systems consisting of a collection of behaviors.  THIS BEHAVIOR-BASED APPROACH TO STRUCTURING IEEE INTELLIGENT SYSTEMS The inspiration for behavior-based control comes from biology, where natural systems are believed to be similarly organized, from spinal reflex movements up to more complex behaviors such as flocking and foraging.  Basis behaviors and primitives. Several methods for principled behavior design and coordination are possible.  Collections of behaviors are a natural representation for controlling collections of robots. But how can we use the same idea in the humanoid control domain, where the body&apos;s individual degrees of freedom are more coupled and constrained? For this, we have combined the notion of primitives with another line of evidence from neurosciencemirror neurons-to structure humanoid motor control into a general and robust system capable of a variety of skills and learning by imitation. 6 Humanoid control and imitation. Robot control is a complex problem, involving sensory and effector limitations and various forms of uncertainty. The more complex the system to be controlled, the more we must modularize the approach to make control viable and efficient. Humanoid agents and robots are highly complex; a human arm has seven degrees of freedom (DOF), the hand has 23, and the control of an actuated human spine is beyond current consideration. Yet humans display complex dynamic behaviors in real time and learn various motor skills throughout life, often through imitation. Methods for automating robot programming are in high demand. Reinforcement learning, which lets a robot improve its behavior based on trial-and-error feedback, is very popular. However, reinforcement learning is slow, as the robot must repeatedly try various behaviors in different situations. It can also jeopardize the robot. In contrast, learning by imitation is particularly appealing because it lets the designer specify entire behaviors by demonstration, instead of using low-level programming or trial and error by the robot. In biological systems, imitation appears to be a complex learning mechanism that involves an intricate interaction between visual perception and motor control, both of which are complex in themselves. Although various animals demonstrate simple mimicry, only a very few species, including humans, chimps, and dolphins, are capable of so-called true imitation, which involves the ability to learn arbitrary new skills by observation.  Neuroscience inspiration Evidence from neuroscience studies in animals points to two neural structures we find of key relevance to imitation: spinal fields and mirror neurons. Spinal fields, found in frogs and rats so far, code for complete primitive movements (or behaviors), such as reaching and wiping.  Investigators recently found neurons with so-called mirror properties in monkeys and humans. They appear to directly connect the visual and motor control systems by mapping observed behaviors, such as reaching and grasping, to motor structures that produce them.  We combine these two lines of evidence, spinal basis fields and mirror neurons, into a more sophisticated notion of behaviors, or perceptual-motor primitives. These let a complex system, such as a humanoid, recognize, reproduce, and learn motor skills. The primitives serve as the basis set for generating movements, but also as a vocabulary for classifying observed movements into executable ones. Thus, primitives can classify, predict, and act. In our approach to imitation, the vision system continually matches any observed human movements onto its own set of motor primitives. The primitive, or combination of primitives, that best approximates the observed input also provides the best predictor of what the robot expects to observe next. This expectation facilitates visual segmentation and interpretation of the observed movement. Imitation, then, is a process of matching, classification, and prediction. Learning by imitation, in turn, creates new skills as novel sequences and superpositions of the matched and classified primitives. The hierarchical structure of our imitation approach lets the robot initially observe and imitate a skill, then perfect it through repetition, so that the skill becomes a routine and itself a primitive. The set of primitives can thus adapt over time, to allow for learning arbitrary new skills-that is, for &quot;true&quot; imitation.  Choosing the primitives Movement primitives or behaviors are the unifying mechanism between visual perception and motor control in our approach, and choosing the right ones is a research challenge, driven by several constraints. On the one hand, the motor control system imposes physical bottom-up limitations, based on its kinematic and dynamic properties. It also provides topdown constraints from the type of movements the system is expected to perform, because the primitives must be sufficient for the robot&apos;s entire movement repertoire. On the other hand, the visual system&apos;s structure and inputs influence the choice of primitives for mapping the various observed movements onto its own executable repertoire. To serve as a general and parsimonious basis set, the primitives encode groups or classes of stereotypical movements, invariant to exact position, rate of motion, size, and perspective. Thus, they represent the generic building blocks of motion that can be implemented as parametric motor controllers. Consider a primitive for reaching. Its most important parameter is the goal position of the end point-that is, the hand or held object. It might be further parametrized by a default posture for the entire arm. Such a primitive lets a robot reach toward various goals within a multitude of tasks, from grasping objects and tools, to dancing, to writing and drawing. We used just such a reaching primitive in our experiments to reconstruct the popular dance, the Macarena.  What constitutes a good set of primitives? We have experimented with two types: innate and learned. Innate primitives are userselected and preprogrammed. We have demonstrated the effectiveness of a basis set consisting of three types: • discrete straight-line movements of subsets of degrees of freedom, accounting for reaching-type motions; • continuous oscillatory movements of subsets of DOFs, accounting for repetitive motions; 9 and • postures, accounting for large subsets of the body&apos;s DOFs. 2 Our approach computes the learned primitives directly from human movement data. We gather different types of such data, using the following methods: vision-based motion tracking of the human upper body (using our tracking system), magnetic markers on the arm (using the FastTrak system), and fullbody joint angle data (using the Sarcos Sensuit). We first reduce the dimensionality of the movement data by employing principal components analysis, wavelet compression, and correlation across multiple DOF. Next, we use clustering techniques to extract patterns of similar movements in the data. These clusters or patterns form the basis for the primitives; the movements in the clusters are generalized and parameterized, to result in primitives for producing a variety of similar movements. Visual classification into primitives. Visual perception is also an important constraint on the primitives and a key component of the imitation process. Because the human (and humanoid) visual attention is resource-limited, it must select the visual features that are most relevant to the given imitation task. Determining what those features are for a given demonstration is a challenging problem. Our previous work showed that people watching videos of arm movements displayed no difference in attention whether they were just watching or intending to subsequently imitate. In both cases, they fixated at the end point-the hand or a held object.  Consequently, we can effectively bias the visual perception mechanism toward recognizing movements that it can execute, especially those movements it performs most frequently. The motor control system&apos;s structure, and its underlying set of movement primitives, provides key constraints for visual movement recognition and classification. Our primitive classifier uses the primitives&apos; descriptions to segment a given motion based on the movement data. In the experiments described below, we used end-point data for both arms as input for the vector quantization-based classifier. 11 Again, a key issue in classification is representing the primitives such that they account for significant invariances, such as position, rotation, and scaling. Our classification approach forms the original motion into a vector of relative end-point movements between successive frames, then smoothes and normalizes it. At the classification level, we ignore all other information about the movement, such as global position and arm configuration, enabling a small set of high-level primitive representations instead of a potentially prohibitively large set of detailed ones. Other information necessary for correct imitation serves for parameterizing the selected primitives at the level of movement reconstruction and execution. To simplify matching, our approach describes primitives themselves in the same normalized form. For each time step of the observed motion, we compare a fixed- IEEE INTELLIGENT SYSTEMS WE CAN EFFECTIVELY BIAS THE VISUAL PERCEPTION MECHANISM TOWARD RECOGNIZING MOVEMENTS THAT IT CAN EXECUTE, ESPECIALLY THOSE MOVEMENTS IT PERFORMS MOST FREQUENTLY. horizon window to every primitive and select the one that best matches the input. Adjacent windows with identical classifications connect to form continuous segments. For any segments that fail to match the given primitives, our approach uses the reaching primitive to move the end point frame by frame. Because the horizon window is of fixed size, the perception of a distinct match of a primitive applies only for the given timescale. We are currently working on addressing classification at multiple timescales. To validate our approach, we implemented various examples of imitation, including reaching, ball throwing, aerobics moves, and dance, all on humanoid testbeds, taking human demonstrations as input.  We also used 3D magnetic marker data from the human arm, gathered from subjects imitating videos of arm movements while wearing FastTrak markers for position recording. (These data were gathered at the National Institutes of Health Resource for the Study of Neural Models of Behavior at the University of Rochester.) We used four markers: near the shoulder, the elbow, the wrist, and the start of the middle finger. The movement data resulting from this experiment serve as input into our imitation system, as well as for automatically learning the primitives. Finally, we used full-body joint angle data gathered with the Sarcos Sensuit, a wearable exoskeleton that simultaneously records the joint positions of 35 DOF: the shoulders, elbows, wrists, hips, knees, ankles, and waist. (These data are obtained through a collaboration with the ATR Dynamic Brain Project at the Human Information Processing Labs in Kyoto, Japan.) We are currently focusing on reproducing the upper-body movements from those data on our testbeds, described next. Evaluation testbeds To properly validate our approach to humanoid motor control and imitation, we use different experimental testbeds. Most of our work so far has been done on Adonis (developed at the Georgia Institute of Technology Animation Lab), a 3D rigid-body simulation of a human with full dynamics  As the project progresses, we plan to use physical humanoid robots as the ultimate testbeds for evaluating our imitation architecture. The NASA Robonaut is one candidate, through collaboration with the Johnson Space Center. The Sarcos full-body humanoid robot is another, through collaboration with the ATR Dynamic Brain Project. Both robots are highly complex, built to approximate human body structure as faithfully as practically possible, and feature binocular cameras for embodied visual perception critical for imitation. OUR APPROACH TO HUMANOID motor control and imitation relies on the use of a set of movement primitives. We have experimented with different types of such primitives on different humanoid simulation testbeds. Specifically, we have implemented two versions of the spinal fields found in animals. One closely modeled the frog data, and used a joint-space representation-it controlled individual joints of Adonis&apos;s arms.  We tested both types of primitives on a sequential motor task, dancing the Macarena. Both proved effective, but each had limitations for particular types of movements. This led us to propose and explore a combination approach, where multiple types of primitives can be sequenced and combined. For example, we constructed a basis behavior set consisting of three types of primitives: • discrete straight-line movements using impedance control; • continuous oscillatory movements using coupled oscillators (or a collection of piece-wise linear segments using impedance control); and • postures, using PD-servos to directly control the joints. We also added a forth type of primitive, for avoidance, implemented as a repulsive vector field. The continuously active fourth primitive combined with whatever other primitive was executing to prevent any collisions 22 IEEE INTELLIGENT SYSTEMS  between body parts. In the Macarena, for example, this is necessary for arm movements around and behind the head (  Our goal is not to achieve perfect, completely precise, high-fidelity imitation. While that might be possible, through the use of exact quantitative measurements of the observed movement using signal-processing techniques, it is not what happens in imitation in nature and it is neither necessary nor helpful for our main goals: natural interaction and programming of robots. For those purposes, we aim for an approximation of the observed behavior, one that allows any necessary freedom of interpretation by the humanoid robot, but achieves the task and effectively communicates and interacts with the human. Our goal is also distinct from task-level imitation, which only achieves the demonstration&apos;s goal, but does not imitate the behaviors involved. This problem has been studied in assembly robotics, where investigators used a robotic arm to record, segment, interpret, and then repeat a series of visual images of a human performing an object-stacking task",,"Getting Humanoids to Move and Imitate THIS BEHAVIOR-BASED APPROACH TO STRUCTURING AND CONTROLLING COMPLEX ROBOTIC SYSTEMS USES IMITATION FOR INTERACTION AND LEARNING. THE USE OF BASIS BEHAVIORS, OR PRIMITIVES, LETS THESE HUMANOID ROBOTS DANCE THE MACARENA",,,,core
357564056,2020-04-24T00:00:00,"BACKGROUND: Community-acquired pneumonia (CAP) complicated by parapneumonic effusion/empyema is an infectious syndrome commonly encountered by physicians caring for children in Canada. OBJECTIVE: To investigate the incremental benefit of novel molecular testing for the microbiological diagnosis of pediatric CAP complicated by parapneumonic effusion/empyema in Canada. METHODS: A convenience sample of pleural fluid from 56 children who had been admitted to hospital in Ontario with CAP complicated by parapneumonic effusion between 2009 and 2011 was examined. Multiple uniplex real-time polymerase chain reaction (PCR) testing was performed on these pleural fluids and compared with traditional culture-based testing of blood and pleural fluid samples. RESULTS: Molecular methods detected a pathogen in 82% of cases, whereas traditional cultures of blood and pleural fluids detected a pathogen in only 25%. The majority of parapneumonic effusions were associated with pneumococcal infection; Streptococcus pneumoniae was detected in 62% of the samples using molecular methods but in only 14% of samples using culture-based methods. Streptococcus pyogenes, detected in 16% of samples using PCR, was the second most common pathogen found. No patients were found to have empyema caused by Staphylococcus aureus. DISCUSSION: The results showed that multiple uniplex real-time PCR performed substantially better than traditional culture methods for microbiological diagnosis of CAP complicated by effusion/ empyema. S pneumoniae and S pyogenes were found to be responsible for the majority of infections. The approach detected pathogens in a similar proportion of pleural fluid samples as previously reported nested PCR assays; furthermore, the real-time closed-well approach also minimized the risk of nonspecificity due to cross-contamination relative to nested PCR. CONCLUSIONS: Real-time PCR for the detection of bacterial DNA in pleural fluids has the potential to better define the microbiological cause of pediatric CAP. This approach could help clinicians provide targeted antimicrobial therapy.  Children&apos;s Hospital, 1280 Main Street West, Room 3A-30, Hamilton, Ontario L9H 4K6.  Telephone 905-521-2100 C ommunity-acquired pneumonia (CAP) commonly occurs in children. Pediatric hospitalization rates for CAP in the Western world are one to four per 1000 per year, with pneumonia accounting for up to 20% of all pediatric admissions in some settings (1). Streptococcus pneumoniae is the most common bacterial cause of pediatric CAP (2,3); the introduction of the conjugate pneumococcal vaccine in North America led to a substantial decrease in pneumonia incidence (4). Curiously, subsequent to this, rates of CAP complicated by origiNAL ArticLE This open-access article is distributed under the terms of the Creative Commons Attribution Non-Commercial License (CC BY-NC) (http:// creativecommons.org/licenses/by-nc/4.0/), which permits reuse, distribution and reproduction of the article, provided that the original work is properly cited and the reuse is restricted to noncommercial purposes. For commercial reuse, contact support@pulsus.com parapneumonic effusion were observed to increase in Canada and other countries (5-7). The optimization of the antimicrobial management of CAP with parapneumonic effusion is important because children with this type of infection are often severely ill, requiring admission to hospital, and many require radiological or operative intervention. Choosing empirical antimicrobial therapy is difficult because there are numerous pyogenic bacteria in addition to S pneumoniae that cause complicated CAP. These include Staphylococcus aureus (both methicillin sensitive and methicillin resistant), group A streptococcus (Stretococcus pyogenes), Streptococcus anginosus group organisms, Haemophilus influenzae, anaerobes and others (8-11). Specific (ie, targeted) antibiotic therapy is not possible in the majority of cases because blood and pleural fluid cultures have been shown to be positive in only 17% to 35% of children (8-13). The low sensitivity of pleural fluid cultures in particular may be due to the fact that antibiotics are often started before pleural fluid specimens are obtained. A recent prospective study that enrolled children with complicated pneumonia presenting to the largest children&apos;s hospital in Canada (11) obtained a microbiological diagnosis using culture-based techniques in only 22 of 88 participants. The vast majority of these children were treated with multiple parenteral antimicrobials and more than one-half received vancomycin, yet only a single case of methicillin-resistant S aureus empyema was documented. Molecular techniques may be uniquely well suited to the microbiological diagnosis of complicated CAP because, in contrast to traditional culture-based methods, detection is not predicated on the growth of viable bacteria. However, molecular methods have their own limitations: false-positive results may result from inadequately controlled background contamination (14) or because of very low bacterial concentrations that are not clinically significant but detectable by the assay (15), among other reasons. The purpose of the present study was to determine whether the microbiological diagnosis of CAP with parapneumonic effusion in children could be improved using nonculture-dependent molecular testing of pleural fluids for bacterial pathogens. Although similar studies have described the results of molecular testing to define bacterial etiology of pediatric parapneumonic effusion in American and European populations  METHODS Study population Children with pneumonia and parapneumonic effusion often receive thoracentesis and/or thoracostomy tube placement for both diagnostic and therapeutic purposes as part of routine care at the Children&apos;s Hospital of Eastern Ontario (CHEO, Ottawa, Ontario) and McMaster Children&apos;s Hospital (MCH, Hamilton, Ontario). A convenience sample of pleural fluids from children with a diagnosis of complicated pneumonia/ empyema/parapneumonic effusion were collected at CHEO (n=47) between January 2009 and March 2011, and at MCH (n=9) between December 2010 and March 2011. The infectious disease service at both hospitals had been involved with all study participants. Any pleural fluid from a patient without a diagnosis of complicated CAP (eg, from neonates with fetal hydrops) was not eligible for inclusion. All children with complicated CAP at both CHEO and MCH during the study period were treated empirically with broad-spectrum intravenous antimicrobials; antibacterials were continued if cultures were negative and rationalized if cultures were positive. Because the present study was retrospective in design, molecular testing results were not available to the treating clinicians. The present study was approved by the Research Ethics Boards of both CHEO and McMaster University (Hamilton, Ontario). Traditional culture-based sample processing As part of routine care, all pleural fluids were processed using standard microbiological methods. Aliquots were plated on sheep blood, chocolate and MacConkey agar under aerobic conditions at 37°C and monitored daily for five days. An aliquot was inoculated into thioglycollate broth and aliquots were plated on anaerobic agar media and kept for five days under strict anaerobic conditions; these were also checked daily after being left initially for 48 h. Bacteria were identified using standard laboratory methods. First, Gram stain, colony morphology and growth characteristics on culture media were examined. Based on these results, additional tests were performed. For example, optochin and bile solubility were performed for S pneumoniae identification; latex agglutination for group A antigen and bacitracin disk testing were performed for S pyogenes; and for streptococcal species other than S pyogenes and S pneumoniae, an API 20 Strep test (bioMérieux, USA) was performed. The majority of patients had blood drawn at the study sites and cultures processed using the BacT/Alert platform (bioMérieux, USA). Nucleic acid extraction Nucleic acids were extracted from 400 μL of pleural fluid samples. An extraction and amplification control organism that is not associated with human pulmonary infections (Bacillus atrophaeus, Steris Life Sciences, USA) was added before extraction (20). Samples underwent bead beating using 0.5 mm beads (ZR BashingBeads, Zymo Research, USA) to break down bacterial cell walls using the Disruptor Genie device (Scientific Industries Inc, USA). Samples were then extracted and purified using an automated NA extraction device (iPrep, Life Technologies, USA), with a final volume of 50 μL. Extraction time was approximately 45 min. Real-time polymerase chain reaction 5′exonuclease polymerase chain reaction (PCR) assays for the major bacterial CAP pathogens were used to test pleural fluid specimens. The PCR targets were the lytA gene for S pneumoniae (21), the hpd gene for H influenzae (21), the nuc gene for S aureus (22), the spy gene for S pyogenes (23), and the 16S ribosomal RNA gene of both Streptococcus intermedius and Streptococcus constellatus (two of the three species in the S anginosus group) (24). The limit of detection (LOD) for the assays was determined using serial dilutions of DNA extracted from reference strains of the organisms. The LOD was 100 organisms per PCR reaction for the S pyogenes, S aureus and H influenzae assays; 10 organisms per PCR reaction for the S pneumoniae assay; and one organism per PCR reaction for the S intermedius/S constellatus assay  Uniplex PCR reaction assays for each target organism were prepared in 20 μL volumes in 96-well PCR plates using an automated liquid handler (Eppendorf 5070, Eppendorf Canada, Canada). A negative control (no template) was performed with each sample. PCR plates were covered with adhesive film (MicroAmp Optical Adhesive Film, Life Technologies Inc, USA) to prevent cross-contamination. PCR was performed using a 96-well fast-cycling block on a ViiA7 thermocyler (Life Technologies Inc, USA) using 40 cycles of two-temperature thermocyling (95°C × 3 s and 60°C × 30 s), taking approximately 40 min to complete. Statistical analysis Data were analyzed using STATA version 11.0 (Stata Corp, USA); 95% CIs for proportions were calculated. Differences between PCR diagnostic results and traditional culture-based results were compared using the McNemar exact test. RESULTS A total of 56 pleural fluids were analyzed; the majority of patients had received antimicrobials before pleural fluid sampling. Molecular methods detected a pathogen in 46 of 56 samples, yielding an overall positivity rate of 82% (95% CI 70% to 91%). Pneumococcus was detected in 35 of 56 samples (62% [95% CI 49% to 75%]). The second most common causative pathogen was group A streptococcus, detected in nine of 56 samples (16% [95% CI 8% to 28%]). Two of 56 samples (3.6% [95% CI 0.4% to 12%) were positive using the S intermedius/S constellatus assay. No pleural fluids were positive for S aureus. Cultures of blood and pleural fluids detected a pathogen in 14 of 56 patients (25% [95% CI 14% to 38%]) -less than one-third the rate of detection using PCR; this difference was statistically significant (P&lt;0.0001). Cultures were positive for S pneumoniae in eight of 56 patients Every patient for whom an organism was grown in blood or pleural fluid had the same organism identified by pleural fluid PCR. Three pleural fluid samples had a positive Gram stain but were culturenegative. Two showed Gram-positive cocci in chains (one PCRpositive for group A streptococcus, one PCR-positive for S pneumoniae) and one showed Gram-negative cocci. This latter sample was PCR positive for S pneumoniae. No pleural fluids or blood cultures were positive for S aureus using culture or PCR. DISCUSSION In the present series, multiple-target uniplex real-time PCR of pleural fluids clearly outperformed traditional culture-based methods for the microbiological diagnosis of pneumonia with parapneumonic effusion. The benefit of molecular testing relative to culture was found to be greatest for the detection of S pneumoniae, the most common cause of bacterial pneumonia in children. Other investigators have also explored the use of PCR-based techniques for microbiological diagnosis of CAP complicated by parapneumonic effusion  One approach to the detection of multiple pathogens has been broad-range 16S ribsomal RNA PCR. Pathogen detection in culturenegative cases has been shown to be 55% to 65% using this technique  Microbiological diagnosis has practical value in that it can facilitate the optimization of antimicrobial therapy. The Infectious Disease Society of America recommends that &quot;empiric therapy with a 3rd-generation parenteral cephalosporin should be prescribed…for infants and children with life-threatening infection, including those with empyema&quot; (25). Our experience, and that of others  The solution may be to improve diagnostics and thereby avoid both the under-and overtreatment risks inherent in empirical antimicrobial treatment algorithms. In our series, for example, had molecular testing been available in a timely manner, many patients (ie, those with group A streptococcus isolated from pleural fluids) may have been switched immediately to penicillin G without fear of decreased efficacy. Furthermore, in our region, high-level penicillin resistance (minimum inhibitory concentration ≥8 μ/mL) in S pneumoniae is very rare, suggesting that penicillin G or ampicillin could have been used successfully in most of our patients, consistent with Infectious Diseasees Society of America recommendations (25). Molecular methods to reliably determine penicillin susceptibility of pneumococci are not yet available; therefore, future studies should be performed to verify the safety of step-down therapy with ampicillin subsequent to the identification of S pneumoniae in populations in which rates of high-level penicillin resistance are low. There were limitations to the present study. Given that our study population was a convenience sample of pleural fluids, we cannot make inferences about the relative frequency of respiratory pathogens causing pediatric complicated pneumonia in the greater population. We also did not collect data on the immunocompetence of the study participants nor their vaccination records. Regardless, it is interesting that group A streptococcus was the second-most common cause of complicated pneumonia and that we did not find S aureus in any of our patients, despite the fact that participants were recruited during the 2009 pH1N1 epidemic. It may be that our PCR assay may have had low sensitivity for S aureus; however, unlike S pneumoniae, S aureus can typically be grown in blood or pleural fluid cultures from the majority of children with empyema  There are barriers to the widespread implementation of molecular testing for the detection of bacterial pathogens. These include concerns about the costs of such testing, the requirement to train laboratory staff to perform these tests and the need for dedicated space to perform testing. However, in clinical situations in which patients have been given antibiotics before specimen collection, such as described in our study of parapneumonic effusion, molecular detection of bacteria was clearly superior to culture-based methods",,real-time polymerase chain reaction for microbiological diagnosis of parapneumonic effusions in canadian children,,,,core
477987554,2020-05-01T00:00:00,"[EN] The term ""Agri-Food 4.0"" is an analogy to the term Industry 4.0; coming from the concept ""agriculture 4.0"". Since the origins of the industrial revolution, where the steam engines started the concept of Industry 1.0 and later the use of electricity upgraded the concept to Industry 2.0, the use of technologies generated a milestone in the industry revolution by addressing the Industry 3.0 concept. Hence, Industry 4.0, it is about including and integrating the latest developments based on digital technologies as well as the interoperability process across them. This allows enterprises to transmit real-time information in terms behaviour and performance. Therefore, the challenge is to maintain these complex networked structures efficiently linked and organised within the use of such technologies, especially to identify and satisfy supply chain stakeholders dynamic requirements. In this context, the agriculture domain is not an exception although it possesses some specialities depending from the domain. In fact, all agricultural machinery incorporates electronic controls and has entered to the digital age, enhancing their current performance. In addition, electronics, using sensors and drones, support the data collection of several agriculture key aspects, such as weather, geographical spatialization, animals and crops behaviours, as well as the entire farm life cycle. However, the use of the right methods and methodologies for enhancing agriculture supply chains performance is still a challenge, thus the concept of Industry 4.0 has evolved and adapted to agriculture 4.0 in order analyse the behaviours and performance in this specific domain. Thus, the question mark on how agriculture 4.0 support a better supply chain decision-making process, or how can help to save time to farmer to make effective decision based on objective data, remains open. Therefore, in this survey, a review of more than hundred papers on new technologies and the new available supply chains methods are analysed and contrasted to understand the future paths of the Agri-Food domain.Authors of this publication acknowledge the contribution of the Project 691249, RUC-APS ""Enhancing and implementing Knowledge based ICT solutions within high Risk and Uncertain Conditions for Agriculture Production Systems"" (www.ruc-aps.eu), funded by the European Union under their funding scheme H2020-MSCARISE-2015.Lezoche, M.; Hernández, JE.; Alemany Díaz, MDM.; Panetto, H.; Kacprzyk, J. (2020). Agri-food 4.0: A survey of the supply chains and technologies for the future agriculture. Computers in Industry. 117:1-15. https://doi.org/10.1016/j.compind.2020.103187S115117Ahumada, O., & Villalobos, J. R. (2009). Application of planning models in the agri-food supply chain: A review. European Journal of Operational Research, 196(1), 1-20. doi:10.1016/j.ejor.2008.02.014Ait-Mouheb, N., Bahri, A., Thayer, B. B., Benyahia, B., Bourrié, G., Cherki, B., … Harmand, J. (2018). The reuse of reclaimed water for irrigation around the Mediterranean Rim: a step towards a more virtuous cycle? Regional Environmental Change, 18(3), 693-705. doi:10.1007/s10113-018-1292-zAli, J., & Kumar, S. (2011). Information and communication technologies (ICTs) and farmers’ decision-making across the agricultural supply chain. International Journal of Information Management, 31(2), 149-159. doi:10.1016/j.ijinfomgt.2010.07.008Ali, J., & Kumar, S. (2011). Information and communication technologies (ICTs) and farmers’ decision-making across the agricultural supply chain. International Journal of Information Management, 31(2), 149-159. doi:10.1016/j.ijinfomgt.2010.07.008Alzahrani, S. M. (2018). Development of IoT mining machine for Twitter sentiment analysis: Mining in the cloud and results on the mirror. 2018 15th Learning and Technology Conference (L&T). doi:10.1109/lt.2018.8368490Amandeep, Bhattacharjee, A., Das, P., Basu, D., Roy, S., Ghosh, S., … Rana, T. K. (2017). Smart farming using IOT. 2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON). doi:10.1109/iemcon.2017.8117219Annosi, M. C., Brunetta, F., Monti, A., & Nati, F. (2019). Is the trend your friend? An analysis of technology 4.0 investment decisions in agricultural SMEs. Computers in Industry, 109, 59-71. doi:10.1016/j.compind.2019.04.003Baio, F. H. R. (2011). Evaluation of an auto-guidance system operating on a sugar cane harvester. Precision Agriculture, 13(1), 141-147. doi:10.1007/s11119-011-9241-6Belaud, J.-P., Prioux, N., Vialle, C., & Sablayrolles, C. (2019). Big data for agri-food 4.0: Application to sustainability management for by-products supply chain. Computers in Industry, 111, 41-50. doi:10.1016/j.compind.2019.06.006Nicolaas Bezuidenhout, C., Bodhanya, S., & Brenchley, L. (2012). An analysis of collaboration in a sugarcane production and processing supply chain. British Food Journal, 114(6), 880-895. doi:10.1108/00070701211234390Bhatt, M. R., & Buch, S. (2015). Prediction of formability for sheet metal component using artificial intelligent technique. 2015 2nd International Conference on Signal Processing and Integrated Networks (SPIN). doi:10.1109/spin.2015.7095356Birkel, H. S., & Hartmann, E. (2019). Impact of IoT challenges and risks for SCM. Supply Chain Management: An International Journal, 24(1), 39-61. doi:10.1108/scm-03-2018-0142Boehlje, M. (1999). Structural Changes in the Agricultural Industries: How Do We Measure, Analyze and Understand Them? American Journal of Agricultural Economics, 81(5), 1028-1041. doi:10.2307/1244080Bonney, L., Clark, R., Collins, R., & Fearne, A. (2007). From serendipity to sustainable competitive advantage: insights from Houston’s Farm and their journey of co‐innovation. Supply Chain Management: An International Journal, 12(6), 395-399. doi:10.1108/13598540710826326Boshkoska, B. M., Liu, S., Zhao, G., Fernandez, A., Gamboa, S., del Pino, M., … Chen, H. (2019). A decision support system for evaluation of the knowledge sharing crossing boundaries in agri-food value chains. Computers in Industry, 110, 64-80. doi:10.1016/j.compind.2019.04.012Brewster, C., Roussaki, I., Kalatzis, N., Doolin, K., & Ellis, K. (2017). IoT in Agriculture: Designing a Europe-Wide Large-Scale Pilot. IEEE Communications Magazine, 55(9), 26-33. doi:10.1109/mcom.2017.1600528Bronson, K., & Knezevic, I. (2016). Big Data in food and agriculture. Big Data & Society, 3(1), 205395171664817. doi:10.1177/2053951716648174Brown, K. (2013). Global environmental change I. Progress in Human Geography, 38(1), 107-117. doi:10.1177/0309132513498837Chilcanan, D., Navas, P., & Escobar, S. M. (2017). Expert system for remote process automation in multiplatform servers, through human machine conversation. 2017 12th Iberian Conference on Information Systems and Technologies (CISTI). doi:10.23919/cisti.2017.7975913Choi, J., In, Y., Park, C., Seok, S., Seo, H., & Kim, H. (2016). Secure IoT framework and 2D architecture for End-To-End security. The Journal of Supercomputing, 74(8), 3521-3535. doi:10.1007/s11227-016-1684-0Cohen, W. M., & Levinthal, D. A. (1990). Absorptive Capacity: A New Perspective on Learning and Innovation. Administrative Science Quarterly, 35(1), 128. doi:10.2307/2393553Dabbene, F., Gay, P., & Tortia, C. (2014). Traceability issues in food supply chain management: A review. Biosystems Engineering, 120, 65-80. doi:10.1016/j.biosystemseng.2013.09.006Del Borghi, A., Gallo, M., Strazza, C., & Del Borghi, M. (2014). An evaluation of environmental sustainability in the food industry through Life Cycle Assessment: the case study of tomato products supply chain. Journal of Cleaner Production, 78, 121-130. doi:10.1016/j.jclepro.2014.04.083Devarakonda, R., Shrestha, B., Palanisamy, G., Hook, L., Killeffer, T., Krassovski, M., … Lazer, K. (2014). OME: Tool for generating and managing metadata to handle BigData. 2014 IEEE International Conference on Big Data (Big Data). doi:10.1109/bigdata.2014.7004476Nascimento, A. F. do, Mendonça, E. de S., Leite, L. F. C., Scholberg, J., & Neves, J. C. L. (2012). Calibration and validation of models for short-term decomposition and N mineralization of plant residues in the tropics. Scientia Agricola, 69(6), 393-401. doi:10.1590/s0103-90162012000600008Dolan, C., & Humphrey, J. (2000). Governance and Trade in Fresh Vegetables: The Impact of UK Supermarkets on the African Horticulture Industry. Journal of Development Studies, 37(2), 147-176. doi:10.1080/713600072Dragincic, J., Korac, N., & Blagojevic, B. (2015). Group multi-criteria decision making (GMCDM) approach for selecting the most suitable table grape variety intended for organic viticulture. Computers and Electronics in Agriculture, 111, 194-202. doi:10.1016/j.compag.2014.12.023Dworak, V., Selbeck, J., Dammer, K.-H., Hoffmann, M., Zarezadeh, A., & Bobda, C. (2013). Strategy for the Development of a Smart NDVI Camera  System for Outdoor Plant Detection and Agricultural Embedded Systems. Sensors, 13(2), 1523-1538. doi:10.3390/s130201523Eisele, M., Kiese, R., Krämer, A., & Leibundgut, C. (2001). Application of a catchment water quality model for assessment and prediction of nitrogen budgets. Physics and Chemistry of the Earth, Part B: Hydrology, Oceans and Atmosphere, 26(7-8), 547-551. doi:10.1016/s1464-1909(01)00048-xElsayed, K. M. F., Ismail, T., & S. Ouf, N. (2018). A Review on the Relevant Applications of Machine Learning in Agriculture. IJIREEICE, 6(8), 1-17. doi:10.17148/ijireeice.2018.681Esteso, A., Alemany, M. M. E., & Ortiz, A. (2017). Métodos y Modelos Deterministas e Inciertos para la Gestión de Cadenas de Suministro Agroalimentarias. Dirección y Organización, 41-46. doi:10.37610/dyo.v0i0.509Esteso, A., Alemany, M. M. E., & Ortiz, A. (2018). Conceptual framework for designing agri-food supply chains under uncertainty by mathematical programming models. International Journal of Production Research, 56(13), 4418-4446. doi:10.1080/00207543.2018.1447706GERHARDS, R., GUTJAHR, C., WEIS, M., KELLER, M., SÖKEFELD, M., MÖHRING, J., & PIEPHO, H. P. (2011). Using precision farming technology to quantify yield effects attributed to weed competition and herbicide application. Weed Research, 52(1), 6-15. doi:10.1111/j.1365-3180.2011.00893.xGovindan, K., Jafarian, A., Khodaverdi, R., & Devika, K. (2014). Two-echelon multiple-vehicle location–routing problem with time windows for optimization of sustainable supply chain network of perishable food. International Journal of Production Economics, 152, 9-28. doi:10.1016/j.ijpe.2013.12.028Gumaste, S. S., & Kadam, A. J. (2016). Future weather prediction using genetic algorithm and FFT for smart farming. 2016 International Conference on Computing Communication Control and automation (ICCUBEA). doi:10.1109/iccubea.2016.7860028Hashem, H., & Ranc, D. (2016). A review of modeling toolbox for BigData. 2016 International Conference on Military Communications and Information Systems (ICMCIS). doi:10.1109/icmcis.2016.7496565Hefnawy, A., Elhariri, T., Cherifi, C., Robert, J., Bouras, A., Kubler, S., & Framling, K. (2017). Combined use of lifecycle management and IoT in smart cities. 2017 11th International Conference on Software, Knowledge, Information Management and Applications (SKIMA). doi:10.1109/skima.2017.8294112Hosseini, S. H., Tang, C. Y., & Jiang, J. N. (2014). Calibration of a Wind Farm Wind Speed Model With Incomplete Wind Data. IEEE Transactions on Sustainable Energy, 5(1), 343-350. doi:10.1109/tste.2013.2284490Hu, Y., Zhang, L., Li, J., & Mehrotra, S. (2016). ICME 2016 Image Recognition Grand Challenge. 2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW). doi:10.1109/icmew.2016.7574663A. Irmak, J. W. Jones, W. D. Batchelor, S. Irmak, K. J. Boote, & J. O. Paz. (2006). Artificial Neural Network Model as a Data Analysis Tool in Precision Farming. Transactions of the ASABE, 49(6), 2027-2037. doi:10.13031/2013.22264Jeon, S., Kim, B., & Huh, J. (2017). Study on methods to determine rotor equivalent wind speed to increase prediction accuracy of wind turbine performance under wake condition. Energy for Sustainable Development, 40, 41-49. doi:10.1016/j.esd.2017.06.001Joly, P.-B. (2005). Resilient farming systems in a complex world — new issues for the governance of science and innovation. Australian Journal of Experimental Agriculture, 45(6), 617. doi:10.1071/ea03252Joshi, R., Banwet, D. K., & Shankar, R. (2009). Indian cold chain: modeling the inhibitors. British Food Journal, 111(11), 1260-1283. doi:10.1108/00070700911001077Kamata, T., Roshanianfard, A., & Noguchi, N. (2018). Heavy-weight Crop Harvesting Robot - Controlling Algorithm. IFAC-PapersOnLine, 51(17), 244-249. doi:10.1016/j.ifacol.2018.08.165Kamble, S. S., Gunasekaran, A., & Gawankar, S. A. (2020). Achieving sustainable performance in a data-driven agriculture supply chain: A review for research and applications. International Journal of Production Economics, 219, 179-194. doi:10.1016/j.ijpe.2019.05.022Kamilaris, A., Kartakoullis, A., & Prenafeta-Boldú, F. X. (2017). A review on the practice of big data analysis in agriculture. Computers and Electronics in Agriculture, 143, 23-37. doi:10.1016/j.compag.2017.09.037Kelepouris, T., Pramatari, K., & Doukidis, G. (2007). RFID‐enabled traceability in the food supply chain. Industrial Management & Data Systems, 107(2), 183-200. doi:10.1108/02635570710723804Khan, S. F., & Ismail, M. Y. (2018). An Investigation into the Challenges and Opportunities Associated with the Application of Internet of Things (IoT) in the Agricultural Sector-A Review. Journal of Computer Science, 14(2), 132-143. doi:10.3844/jcssp.2018.132.143Kladivko, E. J., Helmers, M. J., Abendroth, L. J., Herzmann, D., Lal, R., Castellano, M. J., … Villamil, M. B. (2014). Standardized research protocols enable transdisciplinary research of climate variation impacts in corn production systems. Journal of Soil and Water Conservation, 69(6), 532-542. doi:10.2489/jswc.69.6.532Ko, T., Lee, J., & Ryu, D. (2018). Blockchain Technology and Manufacturing Industry: Real-Time Transparency and Cost Savings. Sustainability, 10(11), 4274. doi:10.3390/su10114274KÖK, M. S. (2009). Application of Food Safety Management Systems (ISO 22000/HACCP) in the Turkish Poultry Industry: A Comparison Based on Enterprise Size. Journal of Food Protection, 72(10), 2221-2225. doi:10.4315/0362-028x-72.10.2221Kvíz, Z., Kroulik, M., & Chyba, J. (2014). Machinery guidance systems analysis concerning&nbsp;pass-to-pass accuracy as a tool for efficient plant production in fields and for soil damage reduction. Plant, Soil and Environment, 60(No. 1), 36-42. doi:10.17221/622/2012-pseLamsal, K., Jones, P. C., & Thomas, B. W. (2016). Harvest logistics in agricultural systems with multiple, independent producers and no on-farm storage. Computers & Industrial Engineering, 91, 129-138. doi:10.1016/j.cie.2015.10.018Laube, P., Duckham, M., & Palaniswami, M. (2011). Deferred decentralized movement pattern mining for geosensor networks. International Journal of Geographical Information Science, 25(2), 273-292. doi:10.1080/13658810903296630Li, F.-R., Gao, C.-Y., Zhao, H.-L., & Li, X.-Y. (2002). Soil conservation effectiveness and energy efficiency of alternative rotations and continuous wheat cropping in the Loess Plateau of northwest China. Agriculture, Ecosystems & Environment, 91(1-3), 101-111. doi:10.1016/s0167-8809(01)00265-1Liakos, K., Busato, P., Moshou, D., Pearson, S., & Bochtis, D. (2018). Machine Learning in Agriculture: A Review. Sensors, 18(8), 2674. doi:10.3390/s18082674Meichen, L., Jun, C., Xiang, Z., Lu, W., & Yongpeng, T. (2018). Dynamic obstacle detection based on multi-sensor information fusion. IFAC-PapersOnLine, 51(17), 861-865. doi:10.1016/j.ifacol.2018.08.086Louwagie, G., Northey, G., Finn, J. A., & Purvis, G. (2012). Development of indicators for assessment of the environmental impact of livestock farming in Ireland using the Agri-environmental Footprint Index. Ecological Indicators, 18, 149-162. doi:10.1016/j.ecolind.2011.11.003Luque, A., Peralta, M. E., de las Heras, A., & Córdoba, A. (2017). State of the Industry 4.0 in the Andalusian food sector. Procedia Manufacturing, 13, 1199-1205. doi:10.1016/j.promfg.2017.09.195Malhotra, S., Doja, M. ., Alam, B., & Alam, M. (2017). Bigdata analysis and comparison of bigdata analytic approches. 2017 International Conference on Computing, Communication and Automation (ICCCA). doi:10.1109/ccaa.2017.8229821Mayer, J., Gunst, L., Mäder, P., Samson, M.-F., Carcea, M., Narducci, V., … Dubois, D. (2015). «Productivity, quality and sustainability of winter wheat under long-term conventional and organic management in Switzerland». European Journal of Agronomy, 65, 27-39. doi:10.1016/j.eja.2015.01.002McGuire, S., & Sperling, L. (2013). Making seed systems more resilient to stress. Global Environmental Change, 23(3), 644-653. doi:10.1016/j.gloenvcha.2013.02.001Mekala, M. S., & Viswanathan, P. (2017). A Survey: Smart agriculture IoT with cloud computing. 2017 International conference on Microelectronic Devices, Circuits and Systems (ICMDCS). doi:10.1109/icmdcs.2017.8211551Mishra, S., Mishra, D., & Santra, G. H. (2016). Applications of Machine Learning Techniques in Agricultural Crop Production: A Review Paper. Indian Journal of Science and Technology, 9(38). doi:10.17485/ijst/2016/v9i38/95032Mocnej, J., Seah, W. K. G., Pekar, A., & Zolotova, I. (2018). Decentralised IoT Architecture for Efficient Resources Utilisation. IFAC-PapersOnLine, 51(6), 168-173. doi:10.1016/j.ifacol.2018.07.148Mohanraj, I., Gokul, V., Ezhilarasie, R., & Umamakeswari, A. (2017). Intelligent drip irrigation and fertigation using wireless sensor networks. 2017 IEEE Technological Innovations in ICT for Agriculture and Rural Development (TIAR). doi:10.1109/tiar.2017.8273682Montecinos, J., Ouhimmou, M., Chauhan, S., & Paquet, M. (2018). Forecasting multiple waste collecting sites for the agro-food industry. Journal of Cleaner Production, 187, 932-939. doi:10.1016/j.jclepro.2018.03.127Yandun Narváez, F., Gregorio, E., Escolà, A., Rosell-Polo, J. R., Torres-Torriti, M., & Auat Cheein, F. (2018). Terrain classification using ToF sensors for the enhancement of agricultural machinery traversability. Journal of Terramechanics, 76, 1-13. doi:10.1016/j.jterra.2017.10.005Nguyen, T., ZHOU, L., Spiegler, V., Ieromonachou, P., & Lin, Y. (2018). Big data analytics in supply chain management: A state-of-the-art literature review. Computers & Operations Research, 98, 254-264. doi:10.1016/j.cor.2017.07.004Nilsson, E., Hochrainer-Stigler, S., Mochizuki, J., & Uvo, C. B. (2016). Hydro-climatic variability and agricultural production on the shores of Lake Chad. Environmental Development, 20, 15-30. doi:10.1016/j.envdev.2016.09.001Nolan, P., Paley, D. A., & Kroeger, K. (2017). Multi-UAS path planning for non-uniform data collection in precision agriculture. 2017 IEEE Aerospace Conference. doi:10.1109/aero.2017.7943794Oberholster, C., Adendorff, C., & Jonker, K. (2015). Financing Agricultural Production from a Value Chain Perspective. Outlook on Agriculture, 44(1), 49-60. doi:10.5367/oa.2015.0197Opara, L. U., & Mazaud, F. (2001). Food Traceability from Field to Plate. Outlook on Agriculture, 30(4), 239-247. doi:10.5367/000000001101293724Ott, K.-H., Aranı́bar, N., Singh, B., & Stockton, G. W. (2003). Metabonomics classifies pathways affected by bioactive compounds. Artificial neural network classification of NMR spectra of plant extracts. Phytochemistry, 62(6), 971-985. doi:10.1016/s0031-9422(02)00717-3Panetto, H. (2007). Towards a classification framework for interoperability of enterprise applications. International Journal of Computer Integrated Manufacturing, 20(8), 727-740. doi:10.1080/09511920600996419Paulraj, G. J. L., Francis, S. A. J., Peter, J. D., & Jebadurai, I. J. (2018). Resource-aware virtual machine migration in IoT cloud. Future Generation Computer Systems, 85, 173-183. doi:10.1016/j.future.2018.03.024Pilli, S. K., Nallathambi, B., George, S. J., & Diwanji, V. (2015). eAGROBOT &#x2014; A robot for early crop disease detection using image processing. 2015 2nd International Conference on Electronics and Communication Systems (ICECS). doi:10.1109/ecs.2015.7124873Pinho, P., Dias, T., Cruz, C., Sim Tang, Y., Sutton, M. A., Martins-Loução, M.-A., … Branquinho, C. (2011). Using lichen functional diversity to assess the effects of atmospheric ammonia in Mediterranean woodlands. Journal of Applied Ecology, 48(5), 1107-1116. doi:10.1111/j.1365-2664.2011.02033.xPrathibha, S. R., Hongal, A., & Jyothi, M. P. (2017). IOT Based Monitoring System in Smart Agriculture. 2017 International Conference on Recent Advances in Electronics and Communication Technology (ICRAECT). doi:10.1109/icraect.2017.52Reardon, T., Echeverria, R., Berdegué, J., Minten, B., Liverpool-Tasie, S., Tschirley, D., & Zilberman, D. (2019). Rapid transformation of food systems in developing regions: Highlighting the role of agricultural research & innovations. Agricultural Systems, 172, 47-59. doi:10.1016/j.agsy.2018.01.022Ribarics, P. (2016). Big Data and its impact on agriculture. Ecocycles, 2(1), 33-34. doi:10.19040/ecocycles.v2i1.54Rosell, J. R., & Sanz, R. (2012). A review of methods and applications of the geometric characterization of tree crops in agricultural activities. Computers and Electronics in Agriculture, 81, 124-141. doi:10.1016/j.compag.2011.09.007Roshanianfard, A., Kamata, T., & Noguchi, N. (2018). Performance evaluation of harvesting robot for heavy-weight crops. IFAC-PapersOnLine, 51(17), 332-338. doi:10.1016/j.ifacol.2018.08.200Routroy, S., & Behera, A. (2017). Agriculture supply chain. Journal of Agribusiness in Developing and Emerging Economies, 7(3), 275-302. doi:10.1108/jadee-06-2016-0039Ruiz-Garcia, L., Steinberger, G., & Rothmund, M. (2010). A",'Elsevier BV',Agri-food 4.0: A survey of the supply chains and technologies for the future agriculture,10.1016/j.compind.2020.103187,http://hdl.handle.net/10251/165518,,core
429122139,2020-09-30T00:00:00,"[EN] Deep neural networks (DNNs) have emerged as a state-of-the-art tool in very different research fields due to its adaptive power to the decision space since they do not presuppose any linear relationship between data. Some of the main disadvantages of these trending models are that the choice of the network underlying architecture profoundly influences the performance of the model and that the architecture design requires prior knowledge of the field of study. The use of questionnaires is hugely extended in social/behavioral sciences. The main contribution of this work is to automate the process of a DNN architecture design by using an agglomerative hierarchical algorithm that mimics the conceptual structure of such surveys. Although the train had regression purposes, it is easily convertible to deal with classification tasks. Our proposed methodology will be tested with a database containing socio-demographic data and the responses to five psychometric Likert scales related to the prediction of happiness. These scales have been already used to design a DNN architecture based on the subdimension of the scales. We show that our new network configurations outperform the previous existing DNN architectures.The authors thank the support of the project Analysis, quality, and variability of medical data funded by Universitat Politècnica de València. JMGG and JAC acknowledge the support of the H2020 project CrowdHealth (Collective Wisdom Driving Public Health Policies - 727560) funded by the European Comission. JMGG acknowledge and to the In Advance project (Patient-Centred Pathways of Early Palliative Care, Supportive Ecosystems and Appraisal Standard - 825750) funded by the European Comission, too.Perez-Benito, FJ.; Garcia-Gomez, JM.; Navarro Pardo, E.; Conejero, JA. (2020). Community detection-based deep neural network architectures: A fully automated framework based on Likert-scale data. Mathematical Methods in the Applied Sciences. 43(14):8290-8301. https://doi.org/10.1002/mma.6567S829083014314LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. doi:10.1038/nature14539Antonov, V., Tarkhov, D., & Vasilyev, A. (2018). Unified approach to constructing the neural network models of real objects. Part 1. Mathematical Methods in the Applied Sciences, 41(18), 9244-9251. doi:10.1002/mma.5205Arifovic, J., & Gençay, R. (2001). Using genetic algorithms to select architecture of a feedforward artificial neural network. Physica A: Statistical Mechanics and its Applications, 289(3-4), 574-594. doi:10.1016/s0378-4371(00)00479-9IslamB‐U BaharudinZ RazaM‐Q NallagowndenP.Optimization of neural network architecture using genetic algorithm for load forecasting. In: 5th International Conference on Intelligent and Advanced Systems (ICIAS) 2014;2014:1‐6.KoutníkJ SchmidhuberJ GomezF.Evolving deep unsupervised convolutional networks for vision‐based reinforcement learning. In: Proceedings of the 2014 annual conference on genetic and evolutionary computation ACM;2014:541‐548.VidnerováP NerudaR.Evolving keras architectures for sensor data analysis Federated Conference on Computer Science and Information Systems (FedCSIS) 2017IEEE;2017:109‐112.Albert, R., & Barabási, A.-L. (2002). Statistical mechanics of complex networks. Reviews of Modern Physics, 74(1), 47-97. doi:10.1103/revmodphys.74.47Newman, M., Barabási, A.-L., & Watts, D. J. (2011). The Structure and Dynamics of Networks. doi:10.1515/9781400841356Albert, R., Jeong, H., & Barabási, A.-L. (1999). Diameter of the World-Wide Web. Nature, 401(6749), 130-131. doi:10.1038/43601Redner, S. (1998). How popular is your paper? An empirical study of the citation distribution. The European Physical Journal B, 4(2), 131-134. doi:10.1007/s100510050359Ito, T., Chiba, T., Ozawa, R., Yoshida, M., Hattori, M., & Sakaki, Y. (2001). A comprehensive two-hybrid analysis to explore the yeast protein interactome. Proceedings of the National Academy of Sciences, 98(8), 4569-4574. doi:10.1073/pnas.061034498BarabásiA‐L.Network medicine ‐ from obesity to the diseasom:Mass Medical Soc;2007.Jian, F., & Dandan, S. (2016). Complex Network Theory and Its Application Research on P2P Networks. Applied Mathematics and Nonlinear Sciences, 1(1), 45-52. doi:10.21042/amns.2016.1.00004FortunatoS CastellanoC.Community structure in graphs. In: Computational complexity;2012:490‐512.Kernighan, B. W., & Lin, S. (1970). An Efficient Heuristic Procedure for Partitioning Graphs. Bell System Technical Journal, 49(2), 291-307. doi:10.1002/j.1538-7305.1970.tb01770.xScott, J. (2017). Social Network Analysis. doi:10.4135/9781529716597Amaral, L. A. N., Scala, A., Barthelemy, M., & Stanley, H. E. (2000). Classes of small-world networks. Proceedings of the National Academy of Sciences, 97(21), 11149-11152. doi:10.1073/pnas.200327197Marchiori, M., & Latora, V. (2000). Harmony in the small-world. Physica A: Statistical Mechanics and its Applications, 285(3-4), 539-546. doi:10.1016/s0378-4371(00)00311-3Luo, W., Lu, N., Ni, L., Zhu, W., & Ding, W. (2020). Local community detection by the nearest nodes with greater centrality. Information Sciences, 517, 377-392. doi:10.1016/j.ins.2020.01.001YanardagP VishwanathanS‐V‐N.Deep graph kernels. In: Proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining;2015:1365‐1374.LiJ ZhangH HanZ RongY ChengH HuangJ.Adversarial attack on community detection by hiding individuals. In: Proceedings of the web conference 2020;2020:917‐927.Khodayar, M., & Wang, J. (2019). Spatio-Temporal Graph Deep Neural Network for Short-Term Wind Speed Forecasting. IEEE Transactions on Sustainable Energy, 10(2), 670-681. doi:10.1109/tste.2018.2844102Pérez-Benito, F. J., Villacampa-Fernández, P., Conejero, J. A., García-Gómez, J. M., & Navarro-Pardo, E. (2019). A happiness degree predictor using the conceptual data structure for deep learning architectures. Computer Methods and Programs in Biomedicine, 168, 59-68. doi:10.1016/j.cmpb.2017.11.004Spector, P. (1992). Summated Rating Scale Construction. doi:10.4135/9781412986038Cacioppo, J. T., & Berntson, G. G. (1994). Relationship between attitudes and evaluative space: A critical review, with emphasis on the separability of positive and negative substrates. Psychological Bulletin, 115(3), 401-423. doi:10.1037/0033-2909.115.3.401Newman, M. E. J., & Girvan, M. (2004). Finding and evaluating community structure in networks. Physical Review E, 69(2). doi:10.1103/physreve.69.026113Clauset, A., Newman, M. E. J., & Moore, C. (2004). Finding community structure in very large networks. Physical Review E, 70(6). doi:10.1103/physreve.70.066111Blondel, V. D., Guillaume, J.-L., Lambiotte, R., & Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10), P10008. doi:10.1088/1742-5468/2008/10/p10008Arenas, A., Duch, J., Fernández, A., & Gómez, S. (2007). Size reduction of complex networks preserving modularity. New Journal of Physics, 9(6), 176-176. doi:10.1088/1367-2630/9/6/176Traag, V. A. (2015). Faster unfolding of communities: Speeding up the Louvain algorithm. Physical Review E, 92(3). doi:10.1103/physreve.92.032801HagbergAric SwartPieter S ChultDaniel.Exploring network structure dynamics and function using networkx  Los Alamos National Lab.(LANL) Los Alamos NM (United States);2008.AbadiM BarhamP ChenJ et al.Tensorflow: a system for large‐scale machine learning. In: 12th USENIX symposium on operating systems design and implementation (OSDI 16);2016:265‐283.Joseph, S., Linley, P. A., Harwood, J., Lewis, C. A., & McCollam, P. (2004). Rapid assessment of well-being: The Short Depression-Happiness Scale (SDHS). Psychology and Psychotherapy: Theory, Research and Practice, 77(4), 463-478. doi:10.1348/1476083042555406Carver, C. S. (1997). You want to measure coping but your protocol’ too long: Consider the brief cope. International Journal of Behavioral Medicine, 4(1), 92-100. doi:10.1207/s15327558ijbm0401_6Francis, L. J., Brown, L. B., & Philipchalk, R. (1992). The development of an abbreviated form of the revised Eysenck personality questionnaire (EPQR-A): Its use among students in England, Canada, the U.S.A. and Australia. Personality and Individual Differences, 13(4), 443-449. doi:10.1016/0191-8869(92)90073-xSherbourne, C. D., & Stewart, A. L. (1991). The MOS social support survey. Social Science & Medicine, 32(6), 705-714. doi:10.1016/0277-9536(91)90150-bLawrenceS GilesC‐L TsoiA‐C.Lessons in neural network training: overfitting may be harder than expected. In: AAAI/IAAI;1997:540‐545",'Wiley',Community detection-based deep neural network architectures: A fully automated framework based on Likert-scale data,10.1002/mma.6567,https://riunet.upv.es/bitstream/10251/166901/7/Perez-Bento%3bGarca-Gomez%3bNAVARRO%20-%20Communty%20detecton-based%20deep%20neural%20network%20archtectures%3a%20A%20ful....pdf,,core
346440943,2020-05-18T07:00:00,"VikingBot is an automated AI that plays StarCraft by using a combination of machine learning and artificial intelligence. High level strategies are planned using the Brown-UMBC Reinforcement Learning and Planning (BURLAP), library which implements planning algorithms and provides interfaces for defining a domain and models of that domain for planning. For the planning, we used the BURLAP implementation of the sparse sampling algorithm because the time complexity is independent of the size of the state space, and we have to plan quickly in real time. SARSA reinforcement learning is used for a machine learning model that controls combat units. Various other helping functions are distributed to agent classes that aid in the AI in the different areas of the game. These agents are categorized as strategy, economy, combat, and intelligence. By using these parts in tandem VikingBot aims to use less training resources and still be able to play games at a high enough level to beat a human player",Western CEDAR,VikingBot: The StarCraft Artificial Intelligence,,https://core.ac.uk/download/346440943.pdf,,core
354424916,2020-01-01T00:00:00,"Includes bibliographical references.2020 Summer.Anomaly detection is the science of identifying one or more rare or unexplainable samples or events in a dataset or data stream. The field of anomaly detection has been extensively studied by mathematicians, statisticians, economists, engineers, and computer scientists. One open research question remains the design of distributed cloud-based architectures and algorithms that can accurately identify anomalies in previously unseen, unlabeled streaming, multivariate spatiotemporal data. With streaming data, time is of the essence, and insights are perishable. Real-world streaming spatiotemporal data originate from many sources, including mobile phones, supervisory control and data acquisition enabled (SCADA) devices, the internet-of-things (IoT), distributed sensor networks, and social media. Baseline experiments are performed on four (4) non-streaming, static anomaly detection multivariate datasets using unsupervised offline traditional machine learning (TML), and unsupervised neural network techniques. Multiple architectures, including autoencoders, generative adversarial networks, convolutional networks, and recurrent networks, are adapted for experimentation. Extensive experimentation demonstrates that neural networks produce superior detection accuracy over TML techniques. These same neural network architectures can be extended to process unlabeled spatiotemporal streaming using online learning. Space and time relationships are further exploited to provide additional insights and increased anomaly detection accuracy. A novel domain-independent architecture and set of algorithms called the Spatiotemporal Anomaly Detection Environment (STADE) is formulated. STADE is based on federated learning architecture. STADE streaming algorithms are based on a geographically unique, persistently executing neural networks using online stochastic gradient descent (SGD). STADE is designed to be pluggable, meaning that alternative algorithms may be substituted or combined to form an ensemble. STADE incorporates a Stream Anomaly Detector (SAD) and a Federated Anomaly Detector (FAD). The SAD executes at multiple locations on streaming data, while the FAD executes at a single server and identifies global patterns and relationships among the site anomalies. Each STADE site streams anomaly scores to the centralized FAD server for further spatiotemporal dependency analysis and logging. The FAD is based on recent advances in DNN-based federated learning. A STADE testbed is implemented to facilitate globally distributed experimentation using low-cost, commercial cloud infrastructure provided by Microsoft™. STADE testbed sites are situated in the cloud within each continent: Africa, Asia, Australia, Europe, North America, and South America. Communication occurs over the commercial internet. Three STADE case studies are investigated. The first case study processes commercial air traffic flows, the second case study processes global earthquake measurements, and the third case study processes social media (i.e., Twitter™) feeds. These case studies confirm that STADE is a viable architecture for the near real-time identification of anomalies in streaming data originating from (possibly) computationally disadvantaged, geographically dispersed sites. Moreover, the addition of the FAD provides enhanced anomaly detection capability. Since STADE is domain-independent, these findings can be easily extended to additional application domains and use cases",Colorado State University. Libraries,Spatiotemporal anomaly detection: streaming architecture and algorithms,,https://core.ac.uk/download/354424916.pdf,,core
391315564,2020-12-01T00:00:00,"Social practices underpin all creative processes in the performing arts.In the production of stage performance, from ideation and rehearsal to delivery, creative processes converge collaboratively, generating a shared framework for a play’s expression, atmosphere and intent. At present, many of these processes are organised through sequestered pipelines, with the creative team independently working on individual components, discussed at intervals, supported by a range of tools such as drawings, software and physical scale models. This approach constrains collective decision making as it relies on multiple communication modes for integrating and manifesting all design streams on stage. The reliance on physical co-presence and isolated desktop interaction produces barriers to unlocking creative and economic efficiencies – especially in timessuch as Covid19 where gathering in physical space is not possible.The paper thinks through ways of transforming approaches in the performance design field, focusing on how new digital technologies can enable collective work in virtual space while at the same time supporting and documenting creative processes. It presents outcomes of the iDesign ARC-Linkage Project that addresses the identified shortcomings of established pipelines by developing a networked3D cross-platform visualisation system for experimental application at NIDA and Sydney Theatre Company. This system allows comprehensive immersive set design and performance in virtual space, situating creative and technical teams in an infinitely malleable 1:1 scale 3D datascape. It enables real-time creation, development and robust testing of set designs on a digitally twinned stage, supported by an AI system. The latter acts as a virtual dramaturg – recording, monitoring and advising on design activity, based on an industry-attuned accumulative database that features customisable props, set-pieces, lighting settings and OHS protocols.iDesign hence charts avenues for reshaping the way design conversations, rehearsal and performance may be staged in the future, projecting new encounters and practices while creating a living archive of design practice – enabling users to virtually learn about the hidden and ephemeral facets of theatrical practice",,Digitally Transforming Theatrical Design Practice,,,,core
335000628,2020-01-01T00:00:00,"The aim of this doctoral thesis is the development of an original methodology for the design of compatible and performing restoration mortars for the earthquake protection of monuments and historic structures. In order to achieve this aim, the study of three important historic structures is utilized. Specifically, the Catholicon of Kaisariani Monastery, an important mid-byzantine church and the Plaka Bridge of Arachthos river, a traditional historic bridge, located in the seismic areas of Athens and Epirus respectively, are studied, as well as the Holy Aedicule of the Holy Sepulchre in Jerusalem, the most important monument of Christianity, whose structural integrity was at risk. These three monument were selected, as they present different construction materials and different structural systems, while they are also subjected to different environmental loads, thus guaranteeing the development of a universal approach which can be applied on any monument, taking its specific characteristics into account. Most importantly, these monuments were selected as study cases, on behalf of the interdisciplinarity employed during their study, which provides, within the framework of this doctoral thesis, the possibility of defining the critical thresholds of mechanical properties, which the restoration mortars must comply with in order to ensure their structural integrity and their seismic protection. The monument itself is at the core of the developed methodology. Each monument is studied in the current state and the results are taken into account in order to specify and prioritize design criteria for compatible and performing restoration mortars. Within this process, emphasis is given to the critical thresholds, which the physicochemical, microstructural and mechanical characteristics of the restoration mortar must abide to. It is obvious that the demands of the design criteria and the assignment of critical thresholds regarding the restoration mortar characteristics, create a defined space of acceptability, where the restoration mortar is both compatible and performing. Regarding the compatibility design criteria, acceptability limits set for different types of restoration mortars, related to compositional characteristics, as measured through thermogravimetric analysis and microstructural characteristics, as measured through mercury intrusion porosimetry, are utilized as critical thresholds regarding physicochemical compatibility of the restoration mortar. Characterization and decay diagnosis of the historical materials is at the center of both the design and assessment of any restoration mortar in terms of compatibility. All instrumental techniques which have already been established in the Materials Science and Engineering Laboratory, are utilized for the study of both historical materials and restoration materials, and the results allow for the definition of critical thresholds regarding additional restoration mortar characteristics, where compatibility is ensured.Regarding the performance design criteria, the mechanical properties of the restoration mortars are taken into account and assessed in relation to the critical thresholds set by the finite element model analysis (in the case of the Holy Aedicule of the Holy Sepulchre) or by the fragility analysis (in the case of the Kaisariani Monastery Catholicon) results, corresponding to the demanded value of the restoration mortars compressive strength, capable of ensuring an adequate response of the monument under static and dynamic loads. In both cases the real material data were incorporated within the computation models. Thus, structural analysis results reveal the minimum demanded compressive strength that the restoration mortar must present. In the case of the Holy Aedicule, where the proposed restoration mortar was applied during the restoration project, non-destructive techniques were utilized to assess the effectiveness of their application in terms of compatibility and performance, thus leading to a methodological approach regarding the assessment of mortar application in situ, in real scale and time. The design criteria are parameterized in order to define an area that delimits the characteristics that a restoration mortar must present in order to be compatible and performing, and which allows the selection of the most suitable raw materials from a wider range of materials. At the level of processing the big data that emerges, an original system for evaluating the restoration mortars is proposed, based on the above criteria and using importance weights, which differ for each monument, thus offering the possibility of choosing the optimal mortar for each case. The use of principal component analysis (PCA) proved particularly useful for the study of the restoration mortars examined in this thesis, as it managed to correlate their characteristics and distinguish restoration mortars according to their characteristics, thus facilitating the simultaneous investigation of both their compatibility and performance. At the same time, it is possible to use the PCA method as a tool to assess the compatibility of historical and restoration mortars in terms of composition and microstructural characteristics. In the last section of this thesis, artificial neural networks (ANNs) are developed and trained, aiming to facilitate the design of restoration mortars and creating an added, necessary innovation in the field of mortar design. It is the first time that ANNs are implemented within this framework, and the results highlight their potential as a tool in mortar design. In addition to revealing the effect of each parameter of synthesis (binder to sand ratio, water to binder ratio, maximum aggregate diameter and age of specimen) on the different mortar characteristics, ANNs can also reveal a recommended area of design, regarding mortar synthesis parameters, within which the restoration mortar is expected to develop characteristics as set by the compatibility and design criteria of each monument. Thus, while discrimination analysis within PCA facilitates the management and correlation of experimental data, the use of ANNs allows the examination of a wider area of mortar design, even of synthesis parameter combinations where no experimental data exists, allowing the study and design of the material in the multidimensional space defined by the parameters of mortar synthesis and allowing research to surpass the experimental process. The mortar design methodology developed in the current doctoral thesis, is conducted in a wide design framework, also at the level of correlation and simulation of mortar characteristics, thus configuring an integrated design framework. Mortar design is thus conducted within a continuous and integrated space of compatibility and performance, thus marking the developed methodology –parameterization of criteria, correlation of independent variables, discrimination analysis and simulation with ANNs– as an important tool for the design of compatible and performing restoration mortars, which can contribute to the earthquake protection of monuments and historic structures, as well ensure their structural integrity.Σκοπός της παρούσης διδακτορικής διατριβής είναι η ανάπτυξη μιας πρωτότυπης μεθοδολογίας σχεδιασμού συμβατών και επιτελεστικών κονιαμάτων αποκατάστασης για την αντισεισμική προστασία μνημείων και ιστορικών κατασκευών. Για τον σκοπό αυτό, αξιοποιείται η μελέτη τριών σημαντικών μνημείων και ιστορικών κατασκευών. Συγκεκριμένα, μελετώνται η Μονή Καισαριανής και το Γεφύρι της Πλάκας, μια σημαντική μεσοβυζαντινή εκκλησία και ένα ιστορικό γεφύρι που βρίσκονται στις σεισμογενείς περιοχές της Αττικής και της Ηπείρου αντίστοιχα, καθώς και το Ιερό Κουβούκλιο του Παναγίου Τάφου στην περιοχή των Ιεροσολύμων, το σημαντικότερο μνημείο της χριστιανοσύνης, του οποίου η δομική ακεραιότητα παρουσίαζε προβλήματα. Τα τρία αυτά μνημεία επιλέχθηκαν, καθώς παρουσιάζουν διαφορετικά δομικά υλικά και τρόπο δόμησης, ενώ βρίσκονται και σε διαφορετικό περιβάλλον, ως εκ τούτου διασφαλίζοντας μια καθολική προσέγγιση, που μπορεί να εφαρμοστεί σε κάθε μνημείο, λαμβάνοντας υπόψη τα ιδιαίτερα χαρακτηριστικά του. Κυρίως όμως, τα μνημεία αυτά επιλέχθηκαν γιατί η εξέτασή τους ήταν διεπιστημονική και παρέχει, στο πλαίσιο της παρούσης διδακτορικής διατριβής, τη δυνατότητα γνώσης των κρίσιμων κατωφλίων των μηχανικών απαιτήσεων των κονιαμάτων αποκατάστασης για τη διασφάλιση της δομικής τους ακεραιότητας και την αντισεισμική τους θωράκιση. Η μεθοδολογία που αναπτύσσεται έχει ως πυρήνα το μνημείο και τα αποτελέσματα της μελέτης του, ώστε να προτεραιοποιηθούν και να συγκεκριμενοποιηθούν κριτήρια σχεδιασμού συμβατών και επιτελεστικών κονιαμάτων αποκατάστασης, ενώ ιδιαίτερη έμφαση δίνεται στα κρίσιμα κατώφλια των φυσικοχημικών και μηχανικών τους χαρακτηριστικών. Είναι σαφές ότι οι απαιτήσεις και η θέσπιση κρίσιμων κατωφλίων όσον αφορά και τη συμβατότητα και την επιτελεστικότητα, δημιουργούν έναν χώρο αποδοχής, όπου το κονίαμα αποκατάστασης πληροί και τις δυο αυτές σημαντικές απαιτήσεις. Σε σχέση με τα κριτήρια συμβατότητας των κονιαμάτων αποκατάστασης, αξιοποιούνται τα όρια αποδοχής κονιαμάτων αποκατάστασης ως κρίσιμα κατώφλια των φυσικοχημικών τους χαρακτηριστικών, ενώ ο χαρακτηρισμός των ιστορικών υλικών αποτελεί πυρήνα σχεδιασμού και αποτίμησης της συμβατότητας των κονιαμάτων αποκατάστασης. Για τον χαρακτηρισμό των ιστορικών υλικών και την αποτίμηση των κονιαμάτων αποκατάστασης, χρησιμοποιούνται όλες οι ενόργανες τεχνικές που έχουν προτυποποιηθεί στο Εργαστήριο Επιστήμης και Τεχνικής των Υλικών, τα αποτελέσματα των οποίων επίσης προσφέρουν τη δυνατότητα ορισμού κρίσιμων κατωφλίων για τη διασφάλιση της συμβατότητας. Σε σχέση με τα κριτήρια επιτελεστικότητας αξιοποιούνται μετρήσεις μηχανικών αντοχών των κονιαμάτων αποκατάστασης και λαμβάνονται υπόψη τα κρίσιμα κατώφλια απαιτούμενων αντοχών, όπως αυτά προκύπτουν από την μελέτη πεπερασμένων στοιχείων (Ιερό Κουβούκλιο του Παναγίου Τάφου) και τις καμπύλες θραυστότητας (Καθολικό Μονής Καισαριανής), ενώ και στις δυο περιπτώσεις ενσωματώνονται στα υπολογιστικά μοντέλα τα πραγματικά δεδομένα που προέκυψαν από τη μελέτη των υλικών. Τα αποτελέσματα της δομικής ανάλυσης υποδεικνύουν την ελάχιστη απαιτούμενη αντοχή σε θλίψη που πρέπει να παρουσιάζει το κονίαμα αποκατάστασης, ενώ, στην περίπτωση του Ιερού Κουβουκλίου του Παναγίου Τάφου, όπου και εφαρμόστηκαν τα προτεινόμενα κονιάματα αποκατάστασης, οι μετρήσεις μη καταστρεπτικού ελέγχου επιτρέπουν την αποτίμηση της συμπεριφοράς και της εφαρμογής στους στο επίπεδο του μνημείου. Τα κριτήρια σχεδιασμού παραμετροποιούνται ώστε να διαμορφώνουν ένα πεδίο τιμών που οριοθετεί τα χαρακτηριστικά που πρέπει να εμφανίζει ένα κονίαμα αποκατάστασης, και το οποίο επιτρέπει την επιλογή των πλέον κατάλληλων πρώτων υλών από ένα ευρύτερο φάσμα υλικών. Σε επίπεδο επεξεργασίας των μεγάλων δεδομένων που προκύπτουν, προτείνεται σύστημα αξιολόγησης κονιαμάτων αποκατάστασης, βάσει των ως άνω κριτηρίων με τη χρήση βαρών σημαντικότητας στο κάθε μνημείο, το οποίο προσφέρει τη δυνατότητα επιλογής του βέλτιστου κονιάματος. Η χρήση κυρίων συνιστωσών για την μελέτη των κονιαμάτων αποκατάστασης είναι ιδιαιτέρως χρήσιμη, καθώς υποδεικνύει τις συσχετίσεις των χαρακτηριστικών τους και διακριτοποιεί τα κονιάματα αποκατάστασης αναλόγως των χαρακτηριστικών τους, διευκολύνοντας τη ταυτόχρονη διερεύνηση της συμβατότητας και της επιτελεστικότητάς τους. Παράλληλα, διαφαίνεται η δυνατότητα χρήσης της μεθόδου ως εργαλείο για την αποτίμηση της συμβατότητας ιστορικών κονιαμάτων και κονιαμάτων αποκατάστασης από άποψη σύστασης και μικροδομής. Στο τελευταίο τμήμα της διδακτορικής διατριβής, γίνεται ανάπτυξη και χρήση τεχνητών νευρωνικών δικτύων, για πρώτη φορά στον σχεδιασμό κονιαμάτων αποκατάστασης, δημιουργώντας μια επιπλέον απαραίτητη καινοτομία στο πεδίο του σχεδιασμού. Όπως αναδεικνύεται στο πλαίσιο αυτής της διδακτορικής διατριβής, η χρήση τους μπορεί να υποδείξει, βάσει των κριτηρίων σχεδιασμού για το εκάστοτε μνημείο και την οριοθέτηση των χαρακτηριστικών του κατάλληλου συμβατού και επιτελεστικού κονιάματος αποκατάστασης, μια προτεινόμενη περιοχή σχεδιασμού κονιαμάτων αποκατάστασης, όσον αφορά στις παραμέτρους της σύνθεσης (αναλογία κονίας/αδρανών, λόγος νερού κονίας). Έτσι, ενώ η ανάλυση διάκρισης επιτρέπει τη διαχείριση των πειραματικών δεδομένων, η χρήση των τεχνητών νευρωνικών δικτύων επιτρέπει και τη μελέτη του υλικού στον πολυδιάστατο χώρο που ορίζουν οι παράμετροι της σύνθεσής του, ενώ προσφέρει τη δυνατότητα διερεύνησης επιπλέον περιοχών σχεδιασμού, όπου δεν υπάρχουν διαθέσιμα πειραματικά δεδομένα, επιτρέποντας την υπέρβαση της πειραματικής διαδικασίας. Η μεθοδολογία σχεδιασμού που αναπτύσσεται στην παρούσα διδακτορική διατριβή λαμβάνει χώρα σε ένα ευρύτερο πεδίο σχεδιασμού, τόσο στο επίπεδο της συσχέτισης, όσο και στο επίπεδο της προσομοίωσης των ιδιοτήτων του κονιάματος αποκατάστασης, ώστε να διαμορφώσει ένα ολοκληρωμένο πεδίο σχεδιασμού. Ο σχεδιασμός ανάγεται ως εκ τούτου σε έναν ενιαίο και συνεχή χώρο συμβατότητας και επιτελεστικότητας, καθιστώντας την ολοκληρωμένη μεθοδολογία που αναπτύσσεται –παραμετροποίηση, συσχέτιση ανεξάρτητων μεταβλητών, ανάλυση διάκρισης και προσομοίωση με τεχνητά νευρωνικά δίκτυα– ένα σημαντικό εργαλείο σχεδιασμού συμβατών και επιτελεστικών κονιαμάτων αποκατάστασης για την αντισεισμική προστασία μνημείων και ιστορικών κατασκευών και τη διασφάλιση της δομικής τους ακεραιότητας",'National Documentation Centre (EKT)',Methodology for the design of compatible and performing restoration mortars for the earthquake protection of monuments and historical structures,10.12681/eadd/47665,,,core
328037326,2020-01-01T08:00:00,"A long-standing goal in Deep Learning (DL) research is to design efficient architectures for a given dataset that are both accurate and computationally inexpensive. At present, designing deep learning architectures for a real-world application requires both human expertise and considerable effort as they are either handcrafted by careful experimentation or modified from a handful of existing models. This method is inefficient as the process of architecture design is highly time-consuming and computationally expensive.
The research presents an approach to automate the process of deep learning architecture design through a modeling procedure. In particular, it first introduces a framework that treats the deep learning architecture design problem as a systems architecting problem. The framework provides the ability to utilize novel and intuitive search spaces to find efficient architectures using evolutionary methodologies. Secondly, it uses a parameter sharing approach to speed up the search process and explores its limitations with search space. Lastly, it introduces a multi-objective approach to facilitate architecture design based on hardware constraints that are often associated with real-world deployment.
From the modeling perspective, instead of designing and staging explicit algorithms to process images/sentences, the contribution lies in the design of hybrid architectures that use the deep learning literature developed so far. This approach enjoys the benefit of a single problem formulation to perform end-to-end training and architecture design with limited computational resources --Abstract, page iii",Scholars\u27 Mine,Computational model for neural architecture search,,https://core.ac.uk/download/328037326.pdf,,core
390139404,2020-08-03T00:00:00,"International audienceWith the rapid advancement of power electronic technologies and the reduction of photovoltaic cell price, the share of solar energy in the total power production has been booming recently. On the one hand, the increase in the amount of power delivered by solar energy can be beneficial in many economic and environmental aspects. On the other hand, this can cause various technical challenges to network operators. One of these issues is related to classifying faults located in distribution networks with high penetration of photovoltaic systems. Although many studies have paid significant attention to developing new algorithms applicable for a more active today distribution networks, there is still space for other improvements. Hence, after reviewing stateof-the-art researches, this paper was intended to develop a fault classification that is based on artificial neural networks. In particular, a technique so-called Multiplayer Perceptron Classifier was selected for the proposed algorithm. First, the authors generated a data set for the study by modeling and simulating a real distribution network with practical parameters provided by a local utility in the environment software PowerFactory/DigSILENT. Multiple fault scenarios were simulated. Second, a part of the generated data collection was used for network learning. Finally, the performance of the proposed methodology was demonstrated via testing on the remaining number of generated data",HAL CCSD,A Fault Classification Method for Medium Voltage Networks with a high Penetration of Photovoltaic Systems using Artificial Neural Networks,,,,core
200800387,2020-03-15T00:00:00,"Advances in deep neural networks (DNN) and computer vision (CV) algorithms
have made it feasible to extract meaningful insights from large-scale
deployments of urban cameras. Tracking an object of interest across the camera
network in near real-time is a canonical problem. However, current tracking
platforms have two key limitations: 1) They are monolithic, proprietary and
lack the ability to rapidly incorporate sophisticated tracking models; and 2)
They are less responsive to dynamism across wide-area computing resources that
include edge, fog and cloud abstractions. We address these gaps using Anveshak,
a runtime platform for composing and coordinating distributed tracking
applications. It provides a domain-specific dataflow programming model to
intuitively compose a tracking application, supporting contemporary CV advances
like query fusion and re-identification, and enabling dynamic scoping of the
camera network's search space to avoid wasted computation. We also offer
tunable batching and data-dropping strategies for dataflow blocks deployed on
distributed resources to respond to network and compute variability. These
balance the tracking accuracy, its real-time performance and the active
camera-set size. We illustrate the concise expressiveness of the programming
model for $4$ tracking applications. Our detailed experiments for a network of
1000 camera-feeds on modest resources exhibit the tunable scalability,
performance and quality trade-offs enabled by our dynamic tracking, batching
and dropping strategies",,"A Scalable Platform for Distributed Object Tracking across a Many-camera
  Network",,http://arxiv.org/abs/1902.05577,,core
357554258,2020-04-24T00:00:00,"ABSTRACT In robotics research, perception is one of the most challenging tasks. In contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates. To evaluate, we apply this idea to a door detection problem. A set of preliminary experiments has been completed using software agents with real vision data. Our results demonstrate that information inferred from teammate observations significantly improves the perception precision. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Intelligent agents General Terms Human Factors Keywords Robot perception, robot-human hybrid teams BACKGROUND Robot perception is generally formulated as a problem of analyzing and interpreting various sensory inputs, e.g., camera feeds. In this paper, we approach robot perception from a completely different direction. Our approach utilizes a team setting where a robot collaborates with human teammates. Motivated by the fact that humans possess superior perception skills relative to their robotic counterparts, we investigate how a robot can take advantage of its teammate&apos;s perfect vision. In general, an agent acquires new information through perception, and in turn, the agent chooses actions based on the information acquired. Let us suppose that a robot has a mental model of its human teammate such that a causal relationship is specified between information and actions. Then, by understanding the human mental model of such decision making (or planning), the robot can infer what the human teammate has seen based on the human&apos;s behavior. In other words, an observation of a human teammate can be * This work was conducted (in part) through collaborative participation in the Robotics Consortium sponsored by the U. used as evidence to infer the information perceived by the human. This, in turn, can be used to reduce uncertainty in robot perception. In this paper, we specifically focus on a motivating problem of door detection in the following scenario. Consider a team consisting of a robot and a human performing a military operation in a hostile environment. According to intelligence, armed insurgents are hiding in an urban street. The team is deployed to cover the buildings in the surrounding area, focusing on doors from which the insurgents may try to egress. This is a stealth operation. We make two specific assumptions that are reasonable in a team context. First, observing a teammate is generally more manageable than perceiving an unfamiliar environment. Second, team members share common objectives in reaching the team&apos;s goals. PERCEPTION USING VISION This section describes a purely camera-based approach. First, we find a likely semantic image segmentation using a computer vision technique called stacked hierarchical labeling  It is not constrained by shape grammars and can model a more general class of objects, but its method of constructing a hierarchical segmentation does not convey semantic meaning at a finer detail, as would be necessary to detect doors on a building. It is, however, reliable in detecting buildings as a whole, significantly reducing the search space for detecting doors in the next step. Once buildings are identified, we can apply a broad feature detector to detect likely openings on the façade of the building. As i",,Enhancing Robot Perception Using Human Teammates * (Extended Abstract),,https://core.ac.uk/download/357554258.pdf,,core
334873096,2020-02-06T00:00:00,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.deComment: Accepted at the 2020 IEEE International Conference on Robotics and
  Automation (ICRA). Video at https://www.youtube.com/watch?v=z8gG1k9kSqA
  Project page at http://robotskills.cs.uni-freiburg.d",,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,,http://arxiv.org/abs/1910.09430,,core
289188336,2020-03-27T00:00:00,"Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων
Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την
ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην
δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και
ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των
κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του
ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται
μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent
space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της 
δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ
τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο
ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή
μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και,
παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε
καθημερινά περιβάλλοντα.
Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας
κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο
πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο
και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες
μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο
με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την
αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική
εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι
σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης
(evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης
λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών.
Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση
της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών
χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός
νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning
scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών
σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για
την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη
διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού
και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό
βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,
επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα
γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού
εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi,
σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και
εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει
αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της
διαδικασίας μάθησης.
Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης
συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού
αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή
κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα
χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε
σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η
διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό
εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης.The current PhD thesis addresses the formulation and implementation of a methodological
framework for robot Learning from Demonstration (LfD). The latter refers to methodologies
that develop behavioral policies from example state-to-action mappings. To this
end, we study the reciprocal interaction of perception and action, in order to teach robots
a repertoire of novel action behaviors. Based on that, we design, develop and implement
a robust imitation framework, termed IMFO (IMitation Framework by Observation), that
facilitates imitation learning and relevant applications in human-robot interaction (HRI)
tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions,
aswell as novel ones. Mapping of human actions to the respective robotic ones is achieved
via an indeterminate depiction, termed latent space representation. The latter accomplishes
a compact, yet precise abstraction of action trajectories, effectively representing
high dimensional raw actions in a low dimensional space.
Moreover, throughout this thesis, we examine the role of time in LfD by enhancing
the aforementioned framework with the notion of learning both the spatial and temporal
characteristics of human motions. Accordingly, learned actions can be subsequently reproduced
in the context of more complex time-informed HRI scenarios. Unlike previous
LfD methods that cope only with the spatial traits of an action, the formulated scheme
effectively encompasses spatial and temporal aspects. Extensive experimentation with a
variety of real robotic platforms demonstrates the robustness and applicability of the introduced
integrated LfD scheme.
Learned actions are reproduced under the high level control of a time-informed task
planner. During the implementation of the studied scenarios, temporal and physical constraints
may impose speed adaptations in the performed actions. The employed latent
space representation readily supports such variations, giving rise to novel actions in the
temporal domain. Experimental results demonstrate the effectiveness of the proposed
enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set
of well defined evaluation metrics are introduced to assess the validity of the proposed
approach considering the temporal and spatial consistency of the reproduced behaviors.
A noteworthy extension of the above regards force-based object grasping for executing
sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised
learning scheme, termed SLF (Supervised Learning for Force-based manipulation).
SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation
to acquire sufficient training data; (b) training to facilitate grasp learning with suitable
robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following
sim-to-real transfer, operation in real environments is achieved in addition to simulated
ones, generalizing also for objects not included in the trial sessions. The proposed
learning scheme is demonstrated in object lifting tasks where the applied force varies for
different objects with similar contact friction coefficients, and likewise the grasping pose.
Experimental results on the manipulator YuMi show that the robot is able to effectively
reproduce demanding lifting and manipulation tasks after learning is accomplished.
In summary, our thesis has studied LfD and has contributed with a novel approach that
introduced latent space representations to encode the action characteristics. A framework
implementation (IMFO) of our approach allowed extensive experimentation and also conduction
of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it
to cope with complex, real-life interactions. Finally, the extension of IMFO with forcebased
grasping facilitated manipulation tasks with sensitive objects",,Μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,,,,core
289188335,2020-03-27T00:00:00,"Η εκτίμηση του Κέντρου Μάζας (CoM) διαδραματίζει κρίσιμο ρόλο στη ρομποτική βάδιση. Οι περισσότεροι σχεδιαστές κίνησης και ελεγκτές βάδισης πραγματικού χρό¬νου υποθέτουν ότι η θέση και η ταχύτητα του CoM είναι διαθέσιμες για ανατροφοδό¬τηση ανά πάσα στιγμή. Σε αυτή τη διατριβή παρουσιάζουμε έναν από τους πρώτους τρισδιάστατους εκτιμητές κατάστασης CoM για το περπάτημα των ανθρωποειδών ρομπότ. Ο προτεινόμενος εκτιμητής συνδυάζει αποτελεσματικά τις μετρήσεις από αισθητήρες πίεσης στα πόδια, κωδικοποιητές στις αρθρώσεις και αδρανειακής μο¬νάδας (IMU) στο σώμα με ένα Εκτεταμένο Φίλτρο Κάλμαν (EKF) για την ακριβή εκτίμηση τόσο της θέσης και της ταχύτητας του CoM αλλά και των εξωτερικών δυ¬νάμεων που δρουν πάνω σε αυτό. Επιπλέον, λαμβάνει υπόψιν την ανωμαλότητα του εδάφους και την στροφορμή του σώματος με αποτέλεσμα να συνδυάζει το μετωπικό με το πλευρικό επίπεδο κίνησης, χωρίς να βασίζεται σε αισθητήρες δύναμης / ροπής (F/T) στα πόδια.
Ωστόσο, είναι κοινή πρακτική να επιχειρείται η μετατροπή των μετρήσεων σε ένα αδρανειακό σύστημα αναφοράς ώστε η εκτίμηση του CoM να γίνεται σε σχέση με αυτό. Κατά συνέπεια, για την επίτευξη του παραπάνω είναι υποχρεωτικό να συνε¬κτιμηθούν η βάση και το πόδι στήριξης του ρομπότ. Για το σκοπό αυτό, επεκτείνουμε έναν καθιερωμένο στη βιβλιογραφία εκτιμητή αιωρούμενης μάζας με τη δυναμική του ποδιού στήριξης χρησιμοποιώντας μετρήσεις κινηματικής και αδρανειακής μονάδας με το Φίλτρο Κάλμαν Σφάλματος Κατάστασης (ESKF) για την κατάλληλη διαχείριση της υπερ-παραμετροποίησης των περιστροφών. Με αυτό το τρόπο,δημιουργείται ένα σύστημα σειριακής εκτίμησης κατάστασης που αποτελείται από έναν εκτιμητή βάσης και έναν εκτιμητή CoM το οποίο ονομάζουμε State Estimation RObot Walking (SEROW). Επιπλέον, για να διορθώσουμε την κινηματική απόκλιση που προκαλείται από την ολίσθηση των ποδιών κατά το περπάτημα, χρησιμοποιούμε μετρήσεις Οπτι¬κής Οδομετρίας (VO) και/ή Οδομετρίας LIDAR (LO). Δυστυχώς, τέτοιες μετρήσεις υποφέρουν από ακραίες τιμές σε ένα δυναμικό περιβάλλον, αφού κατά τον υπολογι¬σμό τους χρησιμοποιείται η υπόθεση ότι μόνο το ρομπότ βρίσκεται σε κίνηση και ο κόσμος γύρω του είναι στατικός. Για αυτό το λόγο, εισάγουμε το Σθεναρό Γκαουσια-νό Φίλτρο Κάλμαν Σφάλματος Κατάστασης (RGESKF) για την αυτόματη ανίχνευση και απόρριψη των ακραίων μετρήσεων. Το προτεινόμενο φίλτρο δεν βασίζεται σε πρότερη γνώση σχετικά με τις κατανομές των μετρήσεων και δεν χρησιμοποιεί ειδι¬κά ρυθμισμένα κατώφλια. Ως εκ τούτου,το SEROW γίνεται ένα σθεναρό σύστημα εκτίμησης κατάστασης, κατάλληλο για δυναμικά ανθρώπινα περιβάλλοντα. Προ¬κειμένου να ενισχυθούν περαιτέρω οι ερευνητικές προσπάθειες, οο SEROW δίνεται ελεύθερα στη ρομποτική κοινότητα ως ένα πακέτο ROS/C++ ανοικτού κώδικα.
Τα σύγχρονα συστήματα ελέγχου και εκτίμησης κατάστασης ανθρωποειδών ρο¬μπότ υποθέτουν ότι η κατάσταση επαφής ποδιών-εδάφους είναι γνωστή εκ των προ¬τέρων. Η ανίχνευση τέτοιων επαφών είναι ένα σημαντικό και σε μεγάλο βαθμό ανεξερεύνητο θέμα στη σύγχρονη ρομποτική έρευνα. Σε αυτή τη διατριβή, διατυ¬πώνουμε μια ευρύτερη ερώτηση: σε ποια φάση βάδισης βρίσκεται το ρομπότ; Ποςς το σκοπό αυτό, προτείνουμε ένα ολιστικό πλαίσιο βασισμένο σε μη-επιβλεπόμενη μάθηση από δεδομένα ιδιοδεκτικής αίσθησης που αντιμετωπίζει με ακρίβεια και α¬ποτελεσματικότητα αυτό το πρόβλημα. Συγκεκριμένα, ανιχνεύουμε με ακρίβεια μια από τις τρεις φάσεις βάδισης, την Αριστερή Υποστήριξη (LSS), την Διπλή Υποστήριξη (DS) και τη Δεξιά Υποστήριξη (RSS), χρησιμοποιώντας μετρήσεις από κωδικοποιητές, IMU και F/T. Αρχικά, πραγματοποιείται μείωση των διαστάσεων με Ανάλυση Κύριων Στοιχείων (PCA) ή με αυτόματους κωδικοποιητές ώστε να εξαχθούν χρήσιμα χαρα¬κτηριστικά, μια συμπαγής αναπαράσταση και να μειωθεί ο θόρυβος στα δεδομένα. Στη συνέχεια, πραγματοποιείται μια ομαδοποίηση στον χώρο χαμηλών διαστάσεων με Γκαουσιανά Μοντέλα Μίγματος (GMMs). Ως αποτέλεσμα λαμβάνονται τρία πυ¬κνά συμπλέγματα που αντιστοιχούν στις φάσεις της βάδισης. Αυτό σημαίνει ότι η δυναμική της φάσης του βαδίσματος είναι χαμηλής διάστασης το οποίο λειτουργεί ως άλλη μια ένδειξη στο ότι ολόκληρη η διαδικασία της βάδισης είναι χαμηλής διά¬στασης. Επιπλέον, δεδομένου ότι το προτεινόμενο πλαίσιο χρησιμοποιεί μετρήσεις από αισθητήρες που είναι συνήθως διαθέσιμοι στα σημερινά ανθρωποειδή ρομπότ, προσφέρουμε στη ρομποτική κοινότητα το Gait-Phase Estimation Module (GEM), μια ανοικτού κώδικα εφαρμογή σε ROS/Python.
Το SEROW και το GEM έχουν αξιολογηθεί ποσοτικά και ποιοτικά αφορικά με την ακρίβεια και την αποδοτικότητα τους τόσο σε προσομοίωση όσο και σε πραγματικές συνθήκες.Αρχικά , χρησιμοποιήθηκε ένα προσομοιωμένο ρομπότ στο MATLAB και το ανθρωποειδές ρομπότ Valkyrie της NASA στο ROS/Gazebo για να τεκμηριωθούν τα προτεινόμενα σχήματα στο βάδισμα πάνω σε ανομοιόμορφο/ανώμαλο έδαφος. τηη συνέχεια, τα προτεινόμενα σχήματα ενσωματώθηκαν στο α) μικρού μεγέθους ανθρω-ποειδές ρομπότ NAO v4.0 και β) στο πλήρους μεγέθους ανθρωποειδές WALK-MAN v2.0 για περεταίρω πειραματική επικύρωση. Με το NAO, οο SEROW εφαρμόστηκε στο ρομπότ για να παράσχει την απαραίτητη ανατροφοδότηση στον σχεδιασμό της κίνησης και τη σταθεροποίηση του βηματισμού σε πραγματικό χρόνο. Με αυτό το τρόπο επιτεύχθηκε πολυκατευθυντική βάδιση ακόμη και σε εξωτερικά/ανομοιογενή εδάφη. Επιπλέον,το SEROW χρησιμοποιήθηκε στον σχεδιασμό βημάτων για την πλοήγηση και επίσης στο Visual SLAM με το ίδιο ρομπότ. Όσον αφορά το WALK¬MAN v2.0, το SEROW εφαρμόστηκε με δεδομένα κινηματικής, αδρανειακής μονάδας και F/T για να παρέχει ανατροφοδότηση βάσης και CoM σε πραγματικό χρόνο. Στην εκτίμηση λήφθηκε υπόψη και το VO για την διόρθωση της κινηματικής απόκλισης κατά το περπάτημα. Με αυτό το τρόπο διευκολύνεται σημαντικά ο πιθανός σχεδια¬σμός βημάτων. Τλοςς, το GEM χρησιμοποιήθηκε επίσης για την εκτίμηση της φάσης της βάδισης στο δυναμικό περπάτημα του WALK-MAN.
Συνοψίζοντας, σε αυτή τη διατριβή προτείνεται ένας σθεναρός μη-γραμμικός ε¬κτιμητής κατάστασης για το βάδισμα ανθρωποειδών ρομπότ. Παρόλα αυτά, το προ¬τεινόμενο σύστημα μπορεί εύκολα να επεκταθεί και σε άλλους τύπους ρομπότ με πόδια, όπως τα τετράποδα, μιας και διαθέτουν τις ίδιες βασικές αρχές κίνησης.Center of Mass (CoM) estimation realizes a crucial role in legged locomotion. Most walking
pattern generators and real-time gait stabilizers commonly assume that the CoM position
and velocity are available for feedback. In this thesis we present one of the first
3D-CoM state estimators for humanoid robot walking. The proposed estimation scheme
fuses effectively joint encoder, inertial, and feet pressure measurements with an Extended
Kalman Filter (EKF) to accurately estimate the 3D-CoM position, velocity, and external
forces acting on the CoM. Furthermore, it directly considers the presence of uneven terrain
and the body’s angular momentum rate and thus effectively couples the frontal with
the lateral plane dynamics, without relying on feet Force/Torque (F/T) sensing.
Nevertheless, it is common practice to transform the measurements to a world frame
of reference and estimate the CoM with respect to the world frame. Consequently, the
robot’s base and support foot pose are mandatory and need to be co-estimated. To this
end, we extend a well-established in literature floating mass estimator to account for the
support foot dynamics and fuse kinematic-inertial measurements with the Error State
Kalman Filter (ESKF) to appropriately handle the overparametrization of rotations. In
such a way, a cascade state estimation scheme consisting of a base and a CoM estimator
is formed and coined State Estimation RObot Walking (SEROW). Additionally, we employ
Visual Odometry (VO) and/or LIDAR Odometry (LO) measurements to correct the kinematic
drift caused by slippage during walking. Unfortunately, such measurements suffer
from outliers in a dynamic environment, since frequently it is assumed that only the
robot is inmotion and the world around is static. Thus, we introduce the Robust Gaussian
ESKF (RGESKF) to automatically detect and reject outliers without relying on any prior
knowledge on measurement distributions or finely tuned thresholds. Therefore, SEROW
is robustified and is suitable for dynamic human environments. In order to reinforce further
research endeavors, SEROW is released to the robotic community as an open-source
ROS/C++ package.
Up to date control and state estimation schemes readily assume that feet contact status
is known a priori. Contact detection is an important and largely unexplored topic in
contemporary humanoid robotics research. In this thesis, we elaborate on a broader question:
in which gait phase is the robot currently in? To this end, we propose a holistic framework
based on unsupervised learning from proprioceptive sensing that accurately and efficiently
addresses this problem. More specifically, we robustly detect one of the three gaitphases,
namely Left Single Support (LSS), Double Support (DS), and Right Single Support (RSS) utilizing joint encoder, IMU, and F/T measurements. Initially, dimensionality reduction
with Principal Components Analysis (PCA) or autoencoders is performed to extract
useful features, obtain a compact representation, and reduce the noise. Next, clustering
is performed on the low-dimensional latent space with GaussianMixtureModels (GMMs)
and three dense clusters corresponding to the gait-phases are obtained. Interestingly, it is
demonstrated that the gait phase dynamics are low-dimensional which is another indication
pointing towards locomotion being a low dimensional skill. Accordingly, given that
the proposed framework utilizes measurements fromsensors that are commonly available
on humanoids nowadays, we offer the Gait-phase Estimation Module (GEM), an opensource
ROS/Python implementation to the robotic community.
SEROW and GEM have been quantitatively and qualitatively assessed in terms of accuracy
and efficiency both in simulation and under real-world conditions. Initially, a simulated
robot in MATLAB and NASA’s Valkyrie humanoid robot in ROS/Gazebo were employed
to establish the proposed schemes with uneven/rough terrain gaits. Subsequently,
the proposed schemes were integrated on a) the small size NAO humanoid robot v4.0 and
b) the adult size WALK-MAN v2.0 for experimental validation. With NAO, SEROW was implemented
on the robot to provide the necessary feedback for motion planning and realtime
gait stabilization to achieve omni-directional locomotion even on outdoor/uneven
terrains. Additionally, SEROW was used in footstep planning and also in Visual SLAM
with the same robot. Regarding WALK-MAN v2.0, SEROW was executed onboard with
kinematic-inertial and F/T data to provide base and CoM feedback in real-time. Furthermore,
VO has also been considered to correct the kinematic drift while walking and facilitate
possible footstep planning. GEM was also employed to estimate the gait phase in
WALK-MAN’s dynamic gaits.
Summarizing, a robust nonlinear state estimator is proposed for humanoid robot walking.
Nevertheless, this scheme can be readily extended to other type of legged robots such as quadrupeds, since they share the same fundamental principles",,Σθεναρή μη γραμμική εκτίμηση κατάστασης ανθρωποειδών ρομπότ,,,,core
141537144,2020-03-30T00:00:00,"Due to their on-body and ubiquitous nature, wearables can generate a wide
range of unique sensor data creating countless opportunities for deep learning
tasks. We propose DeepWear, a deep learning (DL) framework for wearable devices
to improve the performance and reduce the energy footprint. DeepWear
strategically offloads DL tasks from a wearable device to its paired handheld
device through local network. Compared to the remote-cloud-based offloading,
DeepWear requires no Internet connectivity, consumes less energy, and is robust
to privacy breach. DeepWear provides various novel techniques such as
context-aware offloading, strategic model partition, and pipelining support to
efficiently utilize the processing capacity from nearby paired handhelds.
Deployed as a user-space library, DeepWear offers developer-friendly APIs that
are as simple as those in traditional DL libraries such as TensorFlow. We have
implemented DeepWear on the Android OS and evaluated it on COTS smartphones and
smartwatches with real DL models. DeepWear brings up to 5.08X and 23.0X
execution speedup, as well as 53.5% and 85.5% energy saving compared to
wearable-only and handheld-only strategies, respectively",,DeepWear: Adaptive Local Offloading for On-Wearable Deep Learning,,http://arxiv.org/abs/1712.03073,,core
326510084,2020-06-01T00:00:00,"In recent years, convolutional neural networks have demonstrated promising performance in a variety of medical image segmentation tasks. However, when a trained segmentation model is deployed into the real clinical world, the model may not perform optimally. A major challenge is the potential poor-quality segmentations generated due to degraded image quality or domain shift issues. There is a timely need to develop an automated quality control method that can detect poor segmentations and feedback to clinicians. Here we propose a novel deep generative model-based framework for quality control of cardiac MRI segmentation. It first learns a manifold of good-quality image-segmentation pairs using a generative model. The quality of a given test segmentation is then assessed by evaluating the difference from its projection onto the good-quality manifold. In particular, the projection is refined through iterative search in the latent space. The proposed method achieves high prediction accuracy on two publicly available cardiac MRI datasets. Moreover, it shows better generalisation ability than traditional regression-based methods. Our approach provides a real-time and model-agnostic quality control for cardiac MRI segmentation, which has the potential to be integrated into clinical image analysis workflows",'Springer Science and Business Media LLC',Deep generative model-based quality control for cardiac MRI segmentation,10.1007/978-3-030-59719-1_9,,"[{'title': None, 'identifiers': ['0302-9743', 'issn:0302-9743']}]",core
475184996,2020-11-09T08:00:00,"Deep neural networks have become very successful at solving many complex tasks such as image classification, image segmentation, and speech recognition. These models are composed of multiple layers that have the capacity to learn increasingly higher-level features, without prior handcrafted specifications. However, the success of a deep neural network relies on finding the proper configuration for the task in hand. Given the vast number of hyperparameters and the massive search space, manually designing or fine-tuning deep learning architectures requires extensive knowledge, time, and computational resources.
There is a growing interest in developing methods that automatically design a neural network´s architecture, known as neural architecture search (NAS). NAS is usually modeled as a single-objective optimization problem where the aim is to find an architecture that maximizes the prediction´s accuracy. However, most deep learning applications require accurate as well as efficient architectures to reduce memory consumption and enable their use in computationally-limited environments. This has led to the need to model NAS as a multiple objective problem that optimizes both the predictive performance and efficiency of the network. Furthermore, most NAS framework have focused on either optimizing the micro-structure (structure of the basic cell), or macro-structure (optimal number of cells and their connection) of the architecture. Consequently, manual engineering is required to find the topology of the non-optimized structure.
Although NAS has demonstrated great potential in automatically designing an architecture, it remains a computationally expensive and time-consuming process because it requires training and evaluating many potential configurations. Recent work has focused on improving the search time of NAS algorithms, but most techniques have been developed and applied only for single-objective optimization problems. Given that optimizing multiple objectives has a higher complexity and requires more iterations to approximate the Pareto Front, it is critical to investigate algorithms that decrease the search time of multiobjective NAS.
One critical application of deep learning is medical image segmentation. Segmentation of medical images provides valuable information for various critical tasks such as analyzing anatomical structures, monitoring disease progression, and predicting patient outcomes. Nonetheless, achieving accurate segmentation is challenging due to the inherent variability in appearance, shape, and location of the region of interest (ROI) between patients and the differences in imagining equipment and acquisition protocols. Therefore, neural networks are usually tailored to a specific application, anatomical region, and image modality. Moreover, medical image data is often volumetric requiring expensive 3D operations that result in large and complex architectures. Hence, training and deploying them requires considerable storage and memory bandwidth that makes them less suitable for clinical applications.
To overcome these challenges, the main goal of this research is to automatically design accurate and efficient deep neural networks using multiobjective optimization algorithms for medical image segmentation. The proposed research consists of three major objectives: (1) to design a deep neural network that uses a multiobjective evolutionary based algorithm to automatically adapt to different medical image datasets while minimizing the model’s size; (2) to design a self-adaptive 2D-3D Fully Convolutional network (FCN) ensemble that incorporates volumetric information and optimizes both the performance and the size of the architecture; and (3) to design an efficient multiobjective neural architecture search framework that decreases the search time while simultaneously optimizing the micro- and macro-structure of the neural architecture.
For the first objective, a multiobjective adaptive convolutional neural network named AdaResU-Net is presented for 2D medical image segmentation. The proposed AdaResU-Net is comprised of a fixed architecture and a learning framework that adjusts the hyperparameters to a particular training dataset using a multiobjective evolutionary based algorithm (MEA algorithm). The MEA algorithm evolves the AdaResU-Net network to optimize both the segmentation accuracy and model size. In the second objective, a self-adaptive ensemble of 2D-3D FCN named AdaEn-Net is proposed for 3D medical image segmentation. The AdaEn-Net is comprised of a 2D FCN that extracts intra-slice and long-range 2D context, and a 3D FCN architecture that exploits inter-slice and volumetric information. The 2D and 3D FCN architectures are automatically fitted for a specific medical image segmentation task by simultaneously optimizing the expected segmentation error and size of the network using the MEA algorithm. Finally, for the third objective, an efficient multiobjective neural architecture search framework named EMONAS is presented for 3D medical image segmentation. EMONAS has two main components, a novel search space that includes the hyperparameters that define the micro- and macro-structure of the architecture, and a Surrogate-assisted multiobjective evolutionary based algorithm (SaMEA algorithm) that efficiently searches for the best hyperparameter values using a Random Forest surrogate and guiding selection probabilities.
The broader impact of the proposed research is as follows: (1) automating the design of deep neural networks’ architecture and hyperparameters to improve the performance and efficiency of the models; and (2) increase the accessibility of deep learning to a broader range of organizations and people by reducing the need of expert knowledge and GPU time when automatically designing deep neural networks. In the medical area, the proposed models aim to improve the automatic extraction of data from medical images to potentially enhance diagnosis, treatment planning and survival prediction of various diseases such as cardiac disease and prostate cancer. Although the proposed techniques are applied to medical image segmentation tasks, they can also be implemented in other applications where accurate and resource-efficient deep neural networks are needed such as autonomous navigation, augmented reality and internet-of-things",Digital Commons @ University of South Florida,Efficient Neural Architecture Search with Multiobjective Evolutionary Optimization,,,,core
343463592,2020-01-01T00:00:00,"Orientadora: Silvia Regina VergilioCoorientador: Marouane KessentiniTese (doutorado) - Universidade Federal do Paraná, Setor de Ciências Exatas, Programa de Pós-Graduação em Informática. Defesa : Curitiba, 24/04/2020Inclui referências: p. 61-66Área de concentração: Ciência da ComputaçãoResumo: A atividade de refatoração tem como principal objetivo aplicar um conjunto de transformações em um artefato de software para melhorar sua estrutura sem alterar sua funcionalidade. Alguns estudos recentes, apresentam bons resultados ao gerarem modelos de predição de refatorações. Além disso, os estudos mostram que refatorações similares são aplicadas em diferentes contextos e podem ser aprendidas. Neste sentido, a maioria dos trabalhos existentes utiliza técnicas de aprendizado de máquina para gerar modelos que predizem se um dado trecho de código deve ser refatorado. Entretanto, essas abordagens possuem limitações. Elas buscam por refatorações específicas e exatamente como aplicadas por desenvolvedores, o que limita que outras refatorações sejam encontradas. Dada a natureza subjetiva da atividade de refatoração de software, a exploração por refatorações com base em outros critérios também é vantajosa. Existem trabalhos na área conhecida como Refatoração de Software Baseada em Busca (SBR) (do inglês, Search Based Software Refactoring), em que algoritmos de busca são utilizados para encontrar refatorações em um grande espaço de busca e visando a melhorar diversos aspectos. Recentemente, trabalhos em SBR começaram a utilizar exemplos de refatorações já aplicadas por desenvolvedores para incorporar aprendizado na busca. Entretanto, essas abordagens são limitadas em termos de generalização dos resultados, uma vez que não geram um modelo que possa ser utilizado para diferentes programas. Desse modo, abordagens existentes de SBR devem ser configuradas e executadas a cada novo programa. Neste contexto, este trabalho visa a incorporar os benefícios encontrados na área de aprendizado de máquina e na área de SBR, apresentando uma abordagem chamada Gorgeous (do inglês, Generation of Refactoring Algorithms through Grammatical Evolution). Gorgeous tem como objetivo gerar algoritmos de refatoração compostos por regras, que quando executados, determinam trechos de código que devem ser refatorados e refatorações a serem aplicadas. Os algoritmos são criados de forma que as refatorações sugeridas sejam similares a refatorações aplicadas na prática e que também melhorem a qualidade do software. Os algoritmos são criados utilizando um processo de aprendizado que primeiro extrai padrões de refatoração de programas agrupando elementos que foram refatorados de maneira similar. Após isso, uma evolução gramatical é executada para gerar algoritmos de refatoração com base nos padrões extraídos. Gorgeous é avaliada utilizando dados de refatoração extraídos de 40 programas Java do repositório GitHub. Como resultado, os algoritmos gerados foram capazes de obter bons resultados para diferentes programas, melhorando em média 60% a qualidade do programa e obtendo 50% de similaridade com refatorações aplicadas na prática. Palavras-chave: Refatoração, Engenharia de Software Baseada em Busca,Agrupamento, Evolução GramaticalAbstract: The refactoring activity addresses the application of a set of transformations in software artifacts to improve their structure while preserving their functionality. Recent studies present promising results generating prediction models for refactoring. Furthermore, they provide evidences that similar refactoring operations are applied in different contexts and they can be learned using Machine Learning (ML). Most works on ML based refactoring generate models to predict if a piece of code should be refactored. Despite the capability of prediction, existing works are limited to learn specific refactoring operations as applied by developers. However, to explore refactoring operations possibilities based on other criteria is also beneficial, mainly by the subjective context of refactoring. In this context, the Search-Based Software Refactoring (SBR) area addresses studies using search algorithms to find refactoring operations in a huge search space, aiming at improving several other aspects. However, existing SBR approaches do not support generalization of results since they do not generate a model as ML studies. In this way, a SBR approach needs to be configured and executed for each program in need of refactoring. In this context, this work introduces a SBR learning approach aiming at taking most advantage of both fields. Gorgeous (Generation of Refactoring Algorithms through Grammatical Evolution) generates refactoring algorithms composed by several rules determining pieces of code that should be refactored and the refactoring types to be used. A refactoring algorithm provides as solution a set of refactoring operations to be applied in a program. In this respect, the algorithm is generated with the goal of increasing similarity of the refactoring operations with the ones applied in practice, and also improving program quality. To do this, a learning process first extracts refactoring patterns from programs by grouping their elements that were refactored in similar ways. After that, a Grammatical Evolution (GE) is executed to generate the algorithms based on the extracted patterns. Gorgeous is evaluated using refactoring data from 40 Java programs of GitHub repository. The refactoring algorithms are capable of obtaining good results to different programs, obtaining around 60% of program quality improvement and 50% of similarity with real refactoring applications. Keywords: Refactoring, Search-Based Software Engineering, Clustering, Grammatical Evolutio",,Generation of refactoring algorithms through grammatical evolution,,,,core
429122025,2020-03-10T00:00:00,"[EN] This article shows a novel geo-visualization method of dynamic spatiotemporal data that allows mobility and concentration of criminal activity to be study. The method was developed using, only and significantly, real data of Santiago de Cali (Colombia), collected by the Colombian National Police (PONAL). This method constitutes a tool that allows criminal influx to be analyzed by concentration, zone, time slot and date. In addition to the field experience of police commanders, it allows patterns of criminal activity to be detected, thereby enabling a better distribution and management of police resources allocated to crime deterrence, prevention and control. Additionally, it may be applied to the concepts of safe city and smart city of the PONAL within the architecture of Command and Control System (C2S) of Command and Control Centers for Public Safety. Furthermore, it contributes to a better situational awareness and improves the future projection, agility, efficiency and decision-making processes of police officers, which are all essential for fulfillment of police missions against crime. Finally, this was developed using an open source software, it can be adapted to any other city, be used with real-time data and be implemented, if necessary, with the geographic software of any other C2S.This work was co-funded by the European Commission as part of H2020 call SEC-12-FCT-2016-thrtopic3 under the project VICTORIA (No. 740754). This publication reflects the views only of the authors, and the Commission cannot be held responsible for any use which may be made of the information contained therein. The authors would like to thank Colombian National Police and its Office of Telematics for their support on development of this project.Salcedo-González, ML.; Suarez-Paez, JE.; Esteve Domingo, M.; Gomez, J.; Palau Salvador, CE. (2020). A Novel Method of Spatiotemporal Dynamic Geo-Visualization of Criminal Data, Applied to Command and Control Centers for Public Safety. ISPRS International Journal of Geo-Information. 9(3):1-17. https://doi.org/10.3390/ijgi9030160S11793Lacinák, M., & Ristvej, J. (2017). Smart City, Safety and Security. Procedia Engineering, 192, 522-527. doi:10.1016/j.proeng.2017.06.090Neumann, M., & Elsenbroich, C. (2016). Introduction: the societal dimensions of organized crime. Trends in Organized Crime, 20(1-2), 1-15. doi:10.1007/s12117-016-9294-zPhillips, P., & Lee, I. (2012). Mining co-distribution patterns for large crime datasets. Expert Systems with Applications, 39(14), 11556-11563. doi:10.1016/j.eswa.2012.03.071Linning, S. J. (2015). Crime seasonality and the micro-spatial patterns of property crime in Vancouver, BC and Ottawa, ON. Journal of Criminal Justice, 43(6), 544-555. doi:10.1016/j.jcrimjus.2015.05.007Spicer, V., & Song, J. (2017). The impact of transit growth on the perception of crime. Journal of Environmental Psychology, 54, 151-159. doi:10.1016/j.jenvp.2017.09.002Beland, L.-P., & Brent, D. A. (2018). Traffic and crime. Journal of Public Economics, 160, 96-116. doi:10.1016/j.jpubeco.2018.03.002Newspaper of National Circulation in Colombia, E.T. Robos en Trancones en El Tintal—Bogotá—.ELTIEMPO.COM https://www.eltiempo.com/bogota/robos-en-trancones-en-el-tintal-168226Nueva Modalidad de Atraco a Conductores en Los Trancones de Bogotá|ELESPECTADOR.COM http://www.elespectador.com/noticias/bogota/nueva-modalidad-de-atraco-conductores-en-los-trancones-de-bogota-articulo-697716Carrillo, P. E., Lopez-Luzuriaga, A., & Malik, A. S. (2018). Pollution or crime: The effect of driving restrictions on criminal activity. Journal of Public Economics, 164, 50-69. doi:10.1016/j.jpubeco.2018.05.007Twinam, T. (2017). Danger zone: Land use and the geography of neighborhood crime. Journal of Urban Economics, 100, 104-119. doi:10.1016/j.jue.2017.05.006Sadler, R. C., Pizarro, J., Turchan, B., Gasteyer, S. P., & McGarrell, E. F. (2017). Exploring the spatial-temporal relationships between a community greening program and neighborhood rates of crime. Applied Geography, 83, 13-26. doi:10.1016/j.apgeog.2017.03.017Roth, R. E., Ross, K. S., Finch, B. G., Luo, W., & MacEachren, A. M. (2013). Spatiotemporal crime analysis in U.S. law enforcement agencies: Current practices and unmet needs. Government Information Quarterly, 30(3), 226-240. doi:10.1016/j.giq.2013.02.001Sustainable Development Goals|UNDP https://www.undp.org/content/undp/en/home/sustainable-development-goals.htmlGiménez-Santana, A., Caplan, J. M., & Drawve, G. (2018). Risk Terrain Modeling and Socio-Economic Stratification: Identifying Risky Places for Violent Crime Victimization in Bogotá, Colombia. European Journal on Criminal Policy and Research, 24(4), 417-431. doi:10.1007/s10610-018-9374-5Kim, S., Jeong, S., Woo, I., Jang, Y., Maciejewski, R., & Ebert, D. S. (2018). Data Flow Analysis and Visualization for Spatiotemporal Statistical Data without Trajectory Information. IEEE Transactions on Visualization and Computer Graphics, 24(3), 1287-1300. doi:10.1109/tvcg.2017.2666146Kounadi, O., & Leitner, M. (2014). Spatial Information Divergence: Using Global and Local Indices to Compare Geographical Masks Applied to Crime Data. Transactions in GIS, 19(5), 737-757. doi:10.1111/tgis.12125Khalid, S., Shoaib, F., Qian, T., Rui, Y., Bari, A. I., Sajjad, M., … Wang, J. (2017). Network Constrained Spatio-Temporal Hotspot Mapping of Crimes in Faisalabad. Applied Spatial Analysis and Policy, 11(3), 599-622. doi:10.1007/s12061-017-9230-xLopez-Cuevas, A., Medina-Perez, M. A., Monroy, R., Ramirez-Marquez, J. E., & Trejo, L. A. (2018). FiToViz: A Visualisation Approach for Real-Time Risk Situation Awareness. IEEE Transactions on Affective Computing, 9(3), 372-382. doi:10.1109/taffc.2017.2741478Xue, Y., & Brown, D. E. (2006). Spatial analysis with preference specification of latent decision makers for criminal event prediction. Decision Support Systems, 41(3), 560-573. doi:10.1016/j.dss.2004.06.007Nakaya, T., & Yano, K. (2010). Visualising Crime Clusters in a Space-time Cube: An Exploratory Data-analysis Approach Using Space-time Kernel Density Estimation and Scan Statistics. Transactions in GIS, 14(3), 223-239. doi:10.1111/j.1467-9671.2010.01194.xAnuar, N. B., & Yap, B. W. (2018). Data Visualization of Violent Crime Hotspots in Malaysia. Soft Computing in Data Science, 350-363. doi:10.1007/978-981-13-3441-2_27Malik, A., Maciejewski, R., Towers, S., McCullough, S., & Ebert, D. S. (2014). Proactive Spatiotemporal Resource Allocation and Predictive Visual Analytics for Community Policing and Law Enforcement. IEEE Transactions on Visualization and Computer Graphics, 20(12), 1863-1872. doi:10.1109/tvcg.2014.2346926Arietta, S. M., Efros, A. A., Ramamoorthi, R., & Agrawala, M. (2014). City Forensics: Using Visual Elements to Predict Non-Visual City Attributes. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2624-2633. doi:10.1109/tvcg.2014.2346446Hu, Y., Wang, F., Guin, C., & Zhu, H. (2018). A spatio-temporal kernel density estimation framework for predictive crime hotspot mapping and evaluation. Applied Geography, 99, 89-97. doi:10.1016/j.apgeog.2018.08.001Yang, D., Heaney, T., Tonon, A., Wang, L., & Cudré-Mauroux, P. (2017). CrimeTelescope: crime hotspot prediction based on urban and social media data fusion. World Wide Web, 21(5), 1323-1347. doi:10.1007/s11280-017-0515-4ToppiReddy, H. K. R., Saini, B., & Mahajan, G. (2018). Crime Prediction & Monitoring Framework Based on Spatial Analysis. Procedia Computer Science, 132, 696-705. doi:10.1016/j.procs.2018.05.075Devia, N., & Weber, R. (2013). Generating crime data using agent-based simulation. Computers, Environment and Urban Systems, 42, 26-41. doi:10.1016/j.compenvurbsys.2013.09.001Kuo, P.-F., Lord, D., & Walden, T. D. (2013). Using geographical information systems to organize police patrol routes effectively by grouping hotspots of crash and crime data. Journal of Transport Geography, 30, 138-148. doi:10.1016/j.jtrangeo.2013.04.006Camacho-Collados, M., & Liberatore, F. (2015). A Decision Support System for predictive police patrolling. Decision Support Systems, 75, 25-37. doi:10.1016/j.dss.2015.04.012Kagawa, T., Saiki, S., & Nakamura, M. (2019). Analyzing street crimes in Kobe city using PRISM. International Journal of Web Information Systems, 15(2), 183-200. doi:10.1108/ijwis-04-2018-0032Jentner, W., Sacha, D., Stoffel, F., Ellis, G., Zhang, L., & Keim, D. A. (2018). Making machine intelligence less scary for criminal analysts: reflections on designing a visual comparative case analysis tool. The Visual Computer, 34(9), 1225-1241. doi:10.1007/s00371-018-1483-0Suarez-Paez, J., Salcedo-Gonzalez, M., Esteve, M., Gómez, J. A., Palau, C., & Pérez-Llopis, I. (2018). Reduced computational cost prototype for street theft detection based on depth decrement in Convolutional Neural Network. Application to Command and Control Information Systems (C2IS) in the National Police of Colombia. International Journal of Computational Intelligence Systems, 12(1), 123. doi:10.2991/ijcis.2018.25905186Suarez-Paez, J., Salcedo-Gonzalez, M., Climente, A., Esteve, M., Gómez, J. A., Palau, C. E., & Pérez-Llopis, I. (2019). A Novel Low Processing Time System for Criminal Activities Detection Applied to Command and Control Citizen Security Centers. Information, 10(12), 365. doi:10.3390/info10120365Esteve, M., Perez-Llopis, I., & Palau, C. E. (2013). Friendly Force Tracking COTS solution. IEEE Aerospace and Electronic Systems Magazine, 28(1), 14-21. doi:10.1109/maes.2013.6470440Esteve, M., Perez-Llopis, I., Hernandez-Blanco, L. E., Palau, C. E., & Carvajal, F. (2007). SIMACOP: Small Units Management C4ISR System. Multimedia and Expo, 2007 IEEE International Conference on. doi:10.1109/icme.2007.4284862OpenStreetMap http://www.openstreetmap.or",'MDPI AG',"A Novel Method of Spatiotemporal Dynamic Geo-Visualization of Criminal Data, Applied to Command and Control Centers for Public Safety",10.3390/ijgi9030160,https://riunet.upv.es/bitstream/handle/10251/166659/Salcedo-Gonz%c3%a1lez%3bSuarez-Paez%3bEsteve%20-%20A%20Novel%20Method%20of%20Spatotemporal%20Dynamc%20Geo-Vsualzaton%20of%20Cr....pdf?sequence=1&isAllowed=y,,core
157604624,2018-04-04T00:00:00,"RealityTech is supported by Region Aquitaine, Pole Aquinetic and Unitec. The Interactive Map Application for Visually Impaired People is conducted with the support of the Erasmus+ Program of the European Union Pr. no 2016-1-EL01-KA201-023731.International audienceIn this demonstration we showcase a new spatial augmented reality device (interactive projection) with three applications: education and experimentation of color models, map exploration for visually impaired people and scientific vulgarization of machine learning. The first exhibition is an interactive exploration about the nature of light. Visitors can experiment with additive subtractive color models. We engage them with questions, and they have to reply using cards to find out answers. This exhibit is suitable for children. The second exhibition is about map exploration and creation for Visually Impaired Persons (VIP). VIP generally use tactile maps with braille to learn about an unknown environment. However, these maps are not accessible to the 80% of VIP who don't read braille. Our prototype augments raised-line maps with audio output. The third exhibition is destined to be used for scientific outreach. It enables the creation of artificial neural networks (ANN) using tangible interfaces. Neurons are represented by laser-cut diamond shaped tokens, and the data to learn is printed on cards. The ANN learns to differentiate shapes, and the whole learning process is made visible and interactive. These three applications demonstrate the capabilities of our hardware and software development kit in different scenarios. At ReVo, each demonstration will have its own setup and interactive space",'Association for Computing Machinery (ACM)',Nectar: Multi-user Spatial Augmented Reality for everyone: Three live demonstrations of educative applications,10.1145/3234253.3234317,,,core
304935586,2018-06-30T22:22:06,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",,Software and System Health Management for Autonomous Robotics Missions,10.1184/r1/6710654.v1,,,core
322366596,2018-12-31T00:00:00,"Deterministic and, in a certain sense, ""linear"" interpretation of the world often leads to the recognition of the fact that the more accurate model we need, the more complex it must be (as in case of a formalized reproduction of the real system, or the implementation of the desired system properties in the process of formal synthesis of something new). Instead, following the principle of synergy leads to the conviction that there is always a certain model of optimal complexity e.g. in the synthesis of the new system, and in the analysis of real system peculiarities. However, the model of reality could be a part of this reality that is included to the carefully structured formal description.  Since we cannot penetrate into the working space of the serial engine while testing, we should use a test engine of a special construction when the working space corresponds to the laws of similarity and this engine will serve as a model of the working space of the serial engine.
&nbsp;
&nbsp;
The study illustrates the effectiveness of hard-soft technology while investigating the peculiarities of heat generation and heat consumption in the internal combustion engine, which will combine mathematic and algorithmic means of modelling as well as the means of real simulation. The necessity of hard-soft technology introduction arises from the excessive complexity of thermal phenomena occurring in the internal combustion engine (ICE), and the inability to fully subordinate these phenomena to existing analytical models.
The combination of original and analytical properties, reality and virtual reality while modelling the processes in internal combustion engines allows us to substantially improve the quality of information in the process of design and engine construction. Taking this into consideration, there are some natural grounds to apply principles of heuristic self-organization, self-learning, means of the neural networks, etc. in the design implementation.
The study demonstrates the example of modelling the real working space of ICE with the forced start that serves as a supplement to the mathematical algorithmic two-zone model of heat generation / heat consumption / heat extraction.
The basic information that can be obtained by means of hard-soft technology in the framework of, for example, the two-zone model of the work process in the gasoline engine, is the variability with the change in the angle of rotation of the crankshaft of the engine: absolute pressure (indicative diagram); absolute temperature; heat transmitted inside the cylinder between zones; coefficient of excess air; coefficient of heat transfer; intensity of heat extraction in the process of combustion of fuel; intensity of heat transfer through the walls of the cylindeДетерміністичне і в певному сенсі «лінійне» трактування світу часто веде до визнання того, що чим точнішою потрібна його модель, тим складнішою вона має бути (як у разі формалізованого відтворення реальної системи, так і у разі втілення бажаних системних властивостей у процесі формалізованого синтезу чогось нового). Натомість дотримання принципу синергетичності веде до переконання, що завжди існує якась модель оптимальної складності — і тоді, коли йдеться про синтез нової системи, і тоді, коли провадиться аналіз властивостей реальної системи. Але ж моделлю реальності може слугувати також і якась частина цієї реальності, долучена до ретельно структурованого формального опису. Оскільки дослідними засобами проникнути в робочий простір серійного двигуна нема змоги, то доводиться використовувати дослідний двигун особливої конструкції, робочий простір якого відповідає законам подібності і слугуватиме моделлю-аналогом робочого простору серійного двигуна.
Мета роботи — обґрунтувати ефективність hard-soft-технології дослідження особливостей теплотворення і теплоспоживання в двигуні внутрішнього згоряння, яка б системно поєднувала в собі засоби математичного й алгоритмічного моделювання та засоби натурного симулювання. Необхідність впровадження hard-soft-технології випливає з надмірної складності теплових явищ, що перебігають у двигуні внутрішнього згоряння, та неможливості уповні підпорядкувати ці явища існуючим аналітичним модельним уявленням.
Поєднання натурності та аналітичності, реальності та віртуальності в моделюванні процесів у двигунах внутрішнього згоряння дозволяє принципово підвищити якість інформаційного забезпечення процесу проектування й конструювання двигунів. При цьому виникають природні підстави для втілення у моделювання принципів евристичної самоорганізації, самонавчання, засобів штибу нейронних мереж тощо.
Наводиться приклад формування реального робочого простору двигуна внутрішнього згоряння з примусовим запаленням, покликаного доповнити математично-алгоритмічну двозонну модель теплотворення/теплоспоживання/тепловідведення.
Основною інформацією, яку можна добувати засобами hard-soft-технології в рамках, приміром, двозонної моделі робочого процесу в бензиновому двигуні, є змінюваність зі зміною кута повороту колінчастого вала двигуна: абсолютного тиску (індикаторна діаграма); абсолютної температури; теплоти, що пересилається всередині циліндра між зонами; коефіцієнта надлишку повітря; коефіцієнта тепловіддачі; інтенсивності тепловиділення у процесі згоряння палива; інтенсивність тепловідведення через стінки циліндра",'Lviv State University of Life Safety',HARD-SOFT-ТЕХНОЛОГІЯ ІНФОРМАЦІЙНОГО СУПРОВОДУ  ПРОЦЕСУ МОДЕЛЮВАННЯ ТЕПЛОТВОРЕННЯ/ТЕПЛОСПОЖИВАННЯ  В ДВИГУНІ ВНУТРІШНЬОГО ЗГОРЯННЯ,10.32447/20784643.18.2018.01,https://core.ac.uk/download/322366596.pdf,,core
186263009,2018-09-15T00:00:00,"Network embedding maps a network into a low-dimensional Euclidean space, and
thus facilitate many network analysis tasks, such as node classification, link
prediction and community detection etc, by utilizing machine learning methods.
In social networks, we may pay special attention to user privacy, and would
like to prevent some target nodes from being identified by such network
analysis methods in certain cases. Inspired by successful adversarial attack on
deep learning models, we propose a framework to generate adversarial networks
based on the gradient information in Graph Convolutional Network (GCN). In
particular, we extract the gradient of pairwise nodes based on the adversarial
network, and select the pair of nodes with maximum absolute gradient to realize
the Fast Gradient Attack (FGA) and update the adversarial network. This process
is implemented iteratively and terminated until certain condition is satisfied,
i.e., the number of modified links reaches certain predefined value.
Comprehensive attacks, including unlimited attack, direct attack and indirect
attack, are performed on six well-known network embedding methods. The
experiments on real-world networks suggest that our proposed FGA behaves better
than some baseline methods, i.e., the network embedding can be easily disturbed
using FGA by only rewiring few links, achieving state-of-the-art attack
performance",,Fast Gradient Attack on Network Embedding,,http://arxiv.org/abs/1809.02797,,core
186276176,2018-10-25T00:00:00,"The wide implementation of electronic health record (EHR) systems facilitates
the collection of large-scale health data from real clinical settings. Despite
the significant increase in adoption of EHR systems, this data remains largely
unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations
and predicting health outcomes. However, the heterogeneity, sparsity, noise,
and bias in this data present many complex challenges. This complexity makes it
difficult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec,
to learn an interpretable deep representation of longitudinal EHR data which is
personalized for each patient. To evaluate this approach, we apply it to the
prediction of future hospitalizations using real EHR data and compare its
predictive performance with baseline methods. Patient2Vec produces a vector
space with meaningful structure and it achieves an AUC around 0.799
outperforming baseline methods. In the end, the learned feature importance can
be visualized and interpreted at both the individual and population levels to
bring clinical insights.Comment: Accepted by IEEE Acces",'Institute of Electrical and Electronics Engineers (IEEE)',"Patient2Vec: A Personalized Interpretable Deep Representation of the
  Longitudinal Electronic Health Record",10.1109/ACCESS.2018.2875677,http://arxiv.org/abs/1810.04793,,core
189835451,2018-06-29T00:00:00,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context",'Institute of Electrical and Electronics Engineers (IEEE)',"Navigating the landscape for real-time localisation and mapping for robotics, virtual and augmented reality",10.1109/JPROC.2018.2856739,,"[{'title': 'Proceedings of the IEEE', 'identifiers': ['0018-9219', 'issn:0018-9219']}]",core
212853933,2018-01-01T00:00:00,"A fundamental problem in perception-based systems is to define

and learn representations of the scene that are more robust and adaptive to several nuisance

factors. Over the recent past, for a variety of tasks involving images, learned representations  have been empirically shown to outperform handcrafted ones.

However, their inability to generalize across varying data distributions poses the following

question: Do representations learned using deep networks just fit a given data distribution or do

they sufficiently model the underlying structure of the problem ? This question could be

understood using a simple example: If a learning algorithm is shown a number of images of a simple handwritten digit, then the representation learned should be generic enough to identify the same digit in

a different form. With regards to deep networks, although the learned representation has been shown

to be robust to various forms of synthetic distortions such as random noise, they fail in the presence of

more implicit forms of naturally occurring distortions. In this dissertation, we

propose approaches to mitigate the effect of such distortions and in the process, study some

vulnerabilities of deep networks to small imperceptible changes that occur in the given input.

The research problems that comprise this dissertation lie in the cross section of two open topics: (1)

Studying and developing methods that enable neural networks learn robust representations (2) Improving

generalization of neural nets across domains.  The first part of the dissertation  approaches the problem of robustness from two broad viewpoints:  Robustness to external nuisance factors that occur in the data and robustness

(or a lack thereof) to perturbations of the learned feature space.  In the second part, we focus on learning representations that are invariant to external covariate

shift, which is more commonly termed as domain shift. 

Towards learning representations robust

to external nuisance factors, we propose an approach that couples a deep convolutional neural

network with a low-dimensional discriminative embedding learned using triplet probability

constraints to solve the unconstrained face analysis problem. While previous approaches in this area have proposed scalable yet ad-hoc solutions to this problem, we propose a principled and parameter free formulation which is based on maximum likelihood estimation. In addition, we employ the principle of transfer learning to realize a deep network architecture that can train faster and on lesser data yet significantly outperforms existing approaches on the unconstrained face verification task. We demonstrate the robustness

of the approach to challenges including age, pose, blur and clutter by performing clustering

experiments on challenging benchmarks.

Recent seminal works have shown that deep neural networks are susceptible to visually imperceptible perturbations of the input. In this dissertation, we build on their ideas in two unique ways: (a) We show that neural networks that perform pixel-wise semantic segmentation tasks also suffer from this vulnerability, despite being trained with more extra information compares to simple classification tasks.  In addition, we present a novel self correcting mechanism in segmentation networks and provide an efficient way to generate such perturbations (b) We present a novel approach to regularize deep neural networks by perturbing intermediate layer activations in an efficient manner, thereby exploring the trade-off between conventional regularization and adversarial robustness within the context of very deep networks. Both of these works provide interesting directions towards understanding the secure nature of deep learning algorithms. 

While humans find it extremely simple to generalize their knowledge across domains, machine learning algorithms including deep neural networks suffer from the problem of domain shift across what are commonly termed as 'source' (S) 

and 'target' (T) distributions. Let the data that a learning algorithm

is trained on be sampled from  S. If the real data used to evaluate the model is

then sampled from T, then the learnt model under-performs on the target data. This inability to generalize is characterized as domain shift. 

Our attempt to address this problem involves learning a common

feature subspace, where distance between source and target distributions are minimized. Estimating the distance between different domains is highly non-trivial and is an open research

problem in itself. In our approach we parameterize the distance measure by using a Generative

Adversarial Network (GAN). A GAN involves a two player game between two mappings com-

monly termed as generator and discriminator. These mappings are learned simultaneously by

employing an adversarial game, i.e. by letting the generator fool the discriminator and enabling

the discriminator to outperform the generator. This adversarial game can be formulated as a

minimax problem. In our approach, we learn three mappings simultaneously: the generator,

discriminator and a feature mapping that contains information about both the content and the

domain of the input. We deploy a two-level minimax game, where the first level is a competition

between the generator and a discriminator similar to a GAN; the second level game is where the

feature mapping attempts to fool the discriminator thereby introducing domain invariance in

the learned feature representation. We have extensively evaluated this approach for different

tasks such as object classification and semantic segmentation, where we achieve state of the

art results across several real datasets. In addition to the conceptual novelty, our approach

presents a more efficient and scalable solution compared to other approaches that attempt to

solve the same problem.

In the final part of this dissertation, we describe some ongoing efforts and future directions of research. Inspired from the study of perturbations described above, we propose a novel metric on how to effectively choose pixels to label given an image, for a pixel-wise segmentation task. This has the potential to significantly reduce the labeling effort and our preliminary results for the task of semantic segmentation are encouraging.  While the domain adaptation approach proposed above considered static images, we propose an extension to video data aided by the use of recurrent neural networks.  Use of full temporal information, when available, provides the perceptual system additional context to disambiguate among smaller object classes  that commonly occur in real scenes",'Wiley',Towards robust and domain invariant feature representations in Deep Learning,10.13016/M2MK65C49,https://core.ac.uk/download/212853933.pdf,,core
154985357,2018-03-30T00:00:00,"Computer vision has advanced significantly that many discriminative
approaches such as object recognition are now widely used in real applications.
We present another exciting development that utilizes generative models for the
mass customization of medical products such as dental crowns. In the dental
industry, it takes a technician years of training to design synthetic crowns
that restore the function and integrity of missing teeth. Each crown must be
customized to individual patients, and it requires human expertise in a
time-consuming and labor-intensive process, even with computer-assisted design
software. We develop a fully automatic approach that learns not only from human
designs of dental crowns, but also from natural spatial profiles between
opposing teeth. The latter is hard to account for by technicians but important
for proper biting and chewing functions. Built upon a Generative Adversar-ial
Network architecture (GAN), our deep learning model predicts the customized
crown-filled depth scan from the crown-missing depth scan and opposing depth
scan. We propose to incorporate additional space constraints and statistical
compatibility into learning. Our automatic designs exceed human technicians'
standards for good morphology and functionality, and our algorithm is being
tested for production use",,"Learning Beyond Human Expertise with Generative Models for Dental
  Restorations",,http://arxiv.org/abs/1804.00064,,core
203960489,2018-10-15,"2018-10-16Cognitive architectures model fixed structures underlying intelligence and seek to heed the original goal of AI—a working implementation of a full cognitive system in aid of creating synthetic agents with human capabilities. Sigma is a cognitive architecture, developed with the immediate aim of supporting real time needs of intelligent agents, robots and virtual humans. In Sigma, this requirement manifests as a system whose development heuristically is guided by knowledge about human cognition with the ultimate desire to explain human intelligence at an appropriate level of abstraction. ❧ Spoken language processing is an important cognitive capability and yet not addressed by existing cognitive architectures. This is indicative of the mixed—symbolic and probabilistic—nature of the speech problem. Sigma, guided in its development by a core set of desiderata that are an evolution of the desiderata implicit in Newell’s Unified Theories of Cognition, presents a unique opportunity to attempt the integration of spoken language understanding in a cognitive architecture. Such attempt is an exercise to push cognitive architectures beyond what they are capable of, taking a first step towards enabling an architecturally based theory of spoken language understanding—deconstructed in terms of the interplay between various cognitive and sub-cognitive capabilities that play an important role in the comprehension process. ❧ This dissertation investigates the issues involved in integration of incremental speech and language processing, with cognition, in aid of spoken language understanding, guided by the desiderata driving Sigma’s development. The space of possibilities this integration enables is explored and a suitable spoken language understanding task is chosen to evaluate the key properties of the theory of spoken language understanding developed in Sigma. Speech signal obtained from an external speech front end is combined with linguistic knowledge in the form of phonetic, lexical and semantic knowledge sources. The linguistic input is converted into meaning using a Natural Language Understanding (NLU) scheme implemented on top of the architecture. ❧ In addition to phonetic, lexical and semantic processing, language processing involves a syntactic component. Probabilistic context free grammar parsing is an important form of grammar processing that has not been possible to realize in cognitive architectures. Probabilistic context free grammar parsing poses a challenge to Sigma’s grounding in graphical models. Sigma is shown to be able to perform syntactic processing via Sum Product Networks (SPNs), a new kind of deep architecture that allows efficient, tractable and exact inference in a wide class of problems, including grammar parsing. It is shown that Sigma’s cognitive language is sufficient to specify any arbitrary valid SPN, with the tractability and exactness expected of them. This shows Sigma’s ability to efficiently specify a wide range of problems. The implications of this are discussed, along with Sigma mechanisms that allow for specifying SPNs. This leads to a novel relationship between neural networks and SPNs in the context of Sigma",University of Southern California. Libraries,Speech and language understanding in the Sigma cognitive architecture,,,,core
186292144,2018-11-18T00:00:00,"Deep Learning is increasingly being adopted by industry for computer vision
applications running on embedded devices. While Convolutional Neural Networks'
accuracy has achieved a mature and remarkable state, inference latency and
throughput are a major concern especially when targeting low-cost and low-power
embedded platforms. CNNs' inference latency may become a bottleneck for Deep
Learning adoption by industry, as it is a crucial specification for many
real-time processes. Furthermore, deployment of CNNs across heterogeneous
platforms presents major compatibility issues due to vendor-specific technology
and acceleration libraries. In this work, we present QS-DNN, a fully automatic
search based on Reinforcement Learning which, combined with an inference engine
optimizer, efficiently explores through the design space and empirically finds
the optimal combinations of libraries and primitives to speed up the inference
of CNNs on heterogeneous embedded devices. We show that, an optimized
combination can achieve 45x speedup in inference latency on CPU compared to a
dependency-free baseline and 2x on average on GPGPU compared to the best vendor
library. Further, we demonstrate that, the quality of results and time
""to-solution"" is much better than with Random Search and achieves up to 15x
better results for a short-time search",'Institute of Electrical and Electronics Engineers (IEEE)',"Learning to infer: RL-based search for DNN primitive selection on
  Heterogeneous Embedded Systems",10.23919/DATE.2019.8714959,http://arxiv.org/abs/1811.07315,,core
186296062,2018-11-27T00:00:00,"This work examines the implications of uncoupled intersections with local
real-world topology and sensor setup on traffic light control approaches.
Control approaches are evaluated with respect to: Traffic flow, fuel
consumption and noise emission at intersections.
  The real-world road network of Friedrichshafen is depicted, preprocessed and
the present traffic light controlled intersections are modeled with respect to
state space and action space.
  Different strategies, containing fixed-time, gap-based and time-based control
approaches as well as our deep reinforcement learning based control approach,
are implemented and assessed. Our novel DRL approach allows for modeling the
TLC action space, with respect to phase selection as well as selection of
transition timings. It was found that real-world topologies, and thus
irregularly arranged intersections have an influence on the performance of
traffic light control approaches. This is even to be observed within the same
intersection types (n-arm, m-phases). Moreover we could show, that these
influences can be efficiently dealt with by our deep reinforcement learning
based control approach.Comment: 32nd Conference on Neural Information Processing Systems, within
  Workshop on Machine Learning for Intelligent Transportation System",,"Distributed traffic light control at uncoupled intersections with
  real-world topology by deep reinforcement learning",,http://arxiv.org/abs/1811.11233,,core
215385330,2018-07-10T16:30:00,"A smart environment is a physical space that is seamlessly embedded with sensors, actuators, displays, and computing devices, connected through communication networks for data collection, to enable various pervasive applications. Radio frequency identification (RFID) and Wireless Sensor Networks (WSNs) can be used to create such smart environments, performing sensing, data acquisition, and communication functions, and thus connecting physical devices together to form a smart environment.
This thesis first examines the features and requirements a smart industrial environment. It then focuses on the realization of such an environment by integrating RFID and industrial WSNs. ISA100.11a protocol is considered in particular for WSNs, while High Frequency RFID is considered for this thesis. This thesis describes designs and implementation of the hardware and software architecture necessary for proper integration of RFID and WSN systems. The hardware architecture focuses on communication interface and AI/AO interface circuit design; while the driver of the interface is implemented through embedded software. Through Web-based Human Machine Interface (HMI), the industrial users can monitor the process parameters, as well as send any necessary alarm information. In addition, a standard Mongo database is designed, allowing access to historical and current data to gain a more in-depth understanding of the environment being created. The information can therefore be uploaded to an IoT Cloud platform for easy access and storage.
Four scenarios for smart industrial environments are mimicked and tested in a laboratory to demonstrate the proposed integrated system. The experimental results have showed that the communication from RFID reader to WSN node and the real-time wireless transmission of the integrated system meet design requirements. In addition, compared to a traditional wired PLC system where measurement error of the integrated system is less than 1%. The experimental results are thus satisfactory, and the design specifications have been achieved",Scholarship@Western,Integration of RFID and Industrial WSNs to Create A Smart Industrial Environment,,https://core.ac.uk/download/215385330.pdf,,core
304992940,2018-01-22T21:00:55,"The development of appropriate flight tests has proven to be a critical element in the development process of many revolutionary next-generation aerospace vehicles. For example, in the case of hypersonic vehicles with air-breathing SCRAMjet engines, sophisticated computational analyses have been developed which require extensive validation and calibration with physical test data. The current state of hypersonic ground testing facilities has not yet been able to accommodate these demands due to the inability to replicate hypersonic flow conditions with sufficient accuracy. These deficiencies have put increased demand and pressure on hypersonic flight testing experiments which have historically proven to produce the highest quality results but at the potential price of extreme complexity and expense. In the case of hypersonic flight testing for SCRAMjet vehicles, the combination of high expense, high complexity, and high modeling uncertainties has led to conservative, risk-averse experiments. These efforts have historically yielded little gain in knowledge, observing only marginal improvements to prediction confidence in the computational models. There is an entire discipline devoted to the process of design and information extraction from aerospace-type experiments known as aircraft system identification (SysID) which combines three interdependent topics: (i) computational modeling and simulation, (ii) experimental design methods, and (iii) statistical estimation techniques. Essentially, SysID attempts to develop time-dynamic experiments so that statistical estimation techniques can most effectively be used to identify high-confidence physics-based models. An implicit limitation to this process lies within the topic of dynamic experiment design, often posed as a mixed parameter optimization/optimal control problem for the concurrent design of aircraft maneuver inputs, instrumentation system parameters, flight conditions, test duration, etc. Here, Fisher information-based optimality criterion are sought to be used for the quantification of information quality; however, these metrics can only be accurately computed if the true values of the unknown model parameters (e.g. SCRAMjet aero-propulsive-elastic stability and control coefficients, vehicle mass/inertia parameters, etc.) are known prior to conducting an actual experiment, which is often not the case. This is commonly referred to as the circulatory problem in statistics literature, suggesting that dynamic optimal experiment design (DOED) requires an augmented robust-optimization approach (DROED) to account for modeling uncertainties. This research focuses on the design of flight-dynamic experiments from the perspective of an integrated system for the concurrent design of information-dense flight experiments which are robust with respect to model parameter uncertainties. The proposed methodology is called TEMPUS, which stands for Time-dynamic Experiment design using a Model-based approach to Propagate Uncertainty for System identification. By using the top-down design decision support process within the Georgia Tech Integrated Product/Process Development methodology (GT-IPPD), TEMPUS fuses elements from two existing experiment design methodologies to enable a systems engineering approach to the design of large-scale robust-optimal dynamic system identification experiments (such as the design of SCRAMjet-powered flight tests). Within this method the generation of feasible design alternatives is achieved via a sizing and synthesis method, providing for the concurrent design of measurement system parameters, control system architecture and parameters, probabilistic uncertainty models, aero-thermal-fluids models, design constraints, and even vehicle geometry and mission-level parameters. To assess the performance of a given experiment design, a variety of different information quality metrics are able to be calculated from a dynamic high-order sensitivity analysis, providing for an a priori estimate of expected goodness-of-fit quality in the a posteriori parameter estimators. To evaluate feasible alternatives, a virtual experimentation strategy is utilized to assess information performance metrics of a given alternative via nondeterministic techniques (e.g. Monte Carlo methods). Implementation of TEMPUS depends on the capability to perform a high-order dynamic sensitivity analysis on nonlinear industrial-sized aerospace flight-dynamic models (including guidance, navigation, and control logic) in a fashion that is both automatable and easily implementable by flight test designers and control systems engineers, all the while without introducing computational uncertainties. To address this challenge, an automatic differentiation tool specialized for use in dynamic experiment design was developed, providing for the ability to automatically compute robust-optimal Fisher information performance metrics by constructing variational asymptotic expansions (i.e. time-dynamic arrays of multivariate Taylor series expansions, parameterized by design and uncertainty perturbation variables). In general, these variational asymptotic expansions (VAEs) allow for a number of desirable capabilities for SysID applications, because they can essentially be considered as asymptotically accurate surrogate models to solutions of dynamic systems, including: (i) the construction of nominal Fisher information metrics (requiring at least 1st-order output-to-parameter sensitivity trajectories to be computed); (ii) the construction of arbitrarily high-order robust-optimal Fisher information metrics using both (deterministic and nondeterministic approaches to calculate robustness); (iii) rapid exploration of neighboring solutions to optimal control problems; and (iv) the implementation of arbitrarily high-order optimization algorithms (e.g. high-accuracy nonlinear parameter estimators in SysID) (not considered in this work). High-order VAEs can suffer from many of the same complications that often hinder high-order multivariate response surface equations (RSEs), such as: (i) poor numerical conditioning, (ii) diminishing returns on accuracy (e.g. slow rates of convergence, finite radii of convergence, etc.), and (iii) the curse of dimensionality (e.g. large computational times and memory requirements). Therefore prior to using VAEs for dynamic experiment design problems within TEMPUS, four developmental experiments were designed to study the adverse effects of diminishing returns on accuracy, the curse of dimensionality, and application of VAEs to create surrogates of optimal control problems on simple dynamic systems. These include: (i) investigating the potential improvements of using alternative sets of basis functions on problems where diminishing returns on accuracy are observed for the standard Taylor (monomial) basis; (ii) investigating the effects of diminishing returns on accuracy in dynamic uncertainty propagation using high-order VAEs and various probabilistic uncertainty models; (iii) investigating the computational time and memory complexities of high-order, high-dimensional VAEs for use in dynamic experiment design; and (iv) investigating how automatic differentiation can be used to generate high-order VAEs to solutions of optimal control problems. The objective of the fourth experiment is to overcome the limitations that many indirect numerical optimization methods possess, namely, being cumbersome, nonautomatable analyses which hinder the ability to perform design space exploration and uncertainty propagation analyses due to a human-in-the-loop dependency. The results of the first experiment suggest that the use of Chebyshev basis functions can alleviate problems where the diminishing returns on accuracy are observed when Taylor basis functions are used. In the second experiment, it was observed that even for uncertainty propagation with high-order VAEs that slow/poor convergence characteristics can result in adverse effects, such as artificial multi-modality in propagated uncertainty distributions. The results of the third experiment suggest that high-dimensional problems (such as experiment design problems) scale exponentially with increasing order, and therefore high-performance computing capabilities will be necessary to practically obtain robust-optimal dynamic experiment designs for large industrial-sized aerospace problems. In the final experiment, two high-order optimal control formulations were developed for computing VAE surrogates. Promising results were observed for a simple optimal control problem where VAE surrogates were successfully computed; however, more effort is needed before these formulations can be applied to larger dynamic experiment design problems. In light of the results of the aforementioned experiments, the TEMPUS methodology was applied to two design problems: (i) a simple mass-spring-damper problem under sinusoidal forcing, and (ii) the Generic Hypersonic Vehicle (GHV) model to design information-dense SCRAMjet-powered flight tests at steady-level flight under multi-sine forcing. In the first study, the small problem size allowed for investigation of high-order VAEs without experiencing the adverse effects due to the curse of dimensionality. Here, it was observed that robust-optimal experiment designs did produce probabilistic information metric distributions with better robustness with respect to parameter uncertainties than designs using the traditional nominal information metrics, and all experiment designs were found to produce intuitive results, serving as a form of validation (e.g. the sinusoidal forcing frequency was designed to excite the system near the expected natural frequency to maximize output-to-parameter sensitivities). For the flight test design problems, a nonlinear robust-adaptive flight controller is required to ensure safe operation throughout flight, because the GHV open-loop dynamics possess unstable, non-minimum phase behavior in the aero-propulsive-elastic modes in addition to the parametric uncertainties within the aero-propulsive-elastic stability and control coefficients. As a result, the complexity of the overall closed-loop model is greatly increased; however, computation of high-order VAEs for this system does not require any special attention in regards to practical implementation, but a substantial increase in computation time and memory was observed. The objective of experiment designs for the SCRAMjet-powered flight tests was to generate data for the system identification of eight thrust force stability and control coefficients: CTPA3, CTPA2, CTPA, CTP, CTA3, CTA2, CTA, CT0. For the combination of the adaptive control architecture and multi-sine excitation maneuvers implemented here, this experimental objective proved difficult to obtain where adaptation is known to have a canceling effect on the open-loop dynamics, therefore, making it difficult to excite the system enough to generate sufficient amounts of the high angle of attack data for improving the information content of the high-order coefficients CTPA3 and CTA3. It is hypothesized that alternative control strategies, employing machine learning for real-time estimation of open-loop natural frequencies, may improve the information quality, but implementation of this is beyond the scope of this work. Nevertheless, TEMPUS does allow for the robust-optimal assessment of information quality for alternative flight test designs (by using the computation of variational asymptotic expansions to overcome the deficiencies of the circulatory problem), implying that trade-offs between alternative controls architectures, measurement systems, etc. is now an available capability to the flight test designer and controls system engineer.Ph.D",Georgia Institute of Technology,TEMPUS: A methodology for model-based robust-optimal design of time-dynamic system identification experiments using variational asymptotic expansions,,https://core.ac.uk/download/304992940.pdf,,core
211826997,2018-01-01T00:00:00,"Sprowitz AT, Tuleu A, Ajallooeian M, et al. Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs. FRONTIERS IN ROBOTICS AND AI. 2018;5: 18.We present Oncilla robot, a novel mobile, quadruped legged locomotion machine. This large-cat sized, 5.1 kg robot is one of a kind of a recent, bioinspired legged robot class designed with the capability of model-free locomotion control. Animal legged locomotion in rough terrain is clearly shaped by sensor feedback systems. Results with Oncilla robot show that agile and versatile locomotion is possible without sensory signals to some extend, and tracking becomes robust when feedback control is added (Ajallooeian, 2015). By incorporating mechanical and control blueprints inspired from animals, and by observing the resulting robot locomotion characteristics, we aim to understand the contribution of individual components. Legged robots have a wide mechanical and control design parameter space, and a unique potential as research tools to investigate principles of biomechanics and legged locomotion control. But the hardware and controller design can be a steep initial hurdle for academic research. To facilitate the easy start and development of legged robots, Oncilla-robot's blueprints are available through open-source. The robot's locomotion capabilities are shown in several scenarios. Specifically, its spring-loaded pantographic leg design compensates for overdetermined body and leg postures, i.e., during turning maneuvers, locomotion outdoors, or while going up and down slopes. The robot's active degree of freedom allow tight and swift direction changes, and turns on the spot. Presented hardware experiments are conducted in an open-loop manner, with little control and computational effort. For more versatile locomotion control, Oncilla-robot can sense leg joint rotations, and leg-trunk forces. Additional sensors can be included for feedback control with an open communication protocol interface. The robot's customized actuators are designed for robust actuation, and efficient locomotion. It trots with a cost of transport of 3.2 J/(Nm),at a speed of 0.63 m s(-1) (Froude number 0.25). The robot trots inclined slopes up to 10 degrees, at 0.25 m s(-1). The multi-body Webots model of Oncilla robot, and Oncilla robot's extensive software architecture enables users to design and test scenarios in simulation. Controllers can directly be transferred to the real robot. Oncilla robot's blueprints are open-source published (hardware GLP v3, software LGPL v3)",'Frontiers Media SA',Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs,10.3389/frobt.2018.00067,,,core
286413436,2018-01-01T00:00:00,"Model-free reinforcement learning has recently been shown to be effective at learning navigation policies from complex image input. However, these algorithms tend to require large amounts of interaction with the environment, which can be prohibitively costly to obtain on robots in the real world. We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multi- ple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time. The dataset and code required to reproduce these results and apply the technique to other datasets and robots is made publicly available at rl-navigation.github.io/deployable ",Proceedings of Machine Learning Research,Learning deployable navigation policies at kilometer scale from a single traversal,,,,core
157738801,2018-06-12T00:00:00,"The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.Comment: 18 pages, accepted by Astronomy and Astrophysics (A&A) the 11th June,
  201",'EDP Sciences',"A new method for unveiling Open Clusters in Gaia: new nearby Open
  Clusters confirmed by DR2",10.1051/0004-6361/201833390,http://arxiv.org/abs/1805.03045,,core
160785022,2018-08-20T00:00:00,"Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.Comment: Proceedings of the IEEE 201",'Institute of Electrical and Electronics Engineers (IEEE)',"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality",10.1109/JPROC.2018.2856739,http://arxiv.org/abs/1808.06352,,core
203998804,2018-11-19,"2018-11-20As robots increasingly perform tasks in a diverse set of real-world environments, they are expected to not only operate in close proximity to humans but interact with them as well. This has led to great interest in the communication challenges associated with the varying degrees of coordination and collaboration required between humans and robots for these tasks. Non-humanoid robots can benefit from the use of nonverbal signals as they often lack the communication modalities that humans intrinsically rely on to obtain important state information. ❧ The goal of this thesis is to enable non-humanoid robots to intelligently utilize nonverbal signals to communicate information about their internal state. As interaction is a complex process, we propose a computational framework that formalizes the robot’s communication behavior as a decision making problem under uncertainty. Building on prior work in notification systems, this framework takes into account information about the human and robot and attempts to balance their individual objectives to create more acceptable robot behavior. ❧ To inform the framework’s Markov Decision Process model, we explored the design space of light, sound, and motion for nonverbal signaling during a human-robot collaboration task. We present three user studies that identify underlying signal design principles based on human perceptions. We applied the findings of these studies to interaction scenarios in three different experiments. To increase the generalizability of this research, we employed several types of non-humanoid robot platform that vary in appearance and capabilities. ❧ Finally, we applied the communication framework to a simulated human-robot collaboration task. A policy for the robot’s nonverbal signaling behavior was generated using model-free reinforcement learning. This experiment evaluated the impact of the robot’s actions on participants’ perceptions of the robot as a teammate. Results showed that the use of this framework enables the robot to not only improve its own task-oriented outcomes but to act as a more thoughtful and considerate agent during interaction with humans. ❧ This research contributes to both the design and planning of nonverbal communication for non-humanoid robot platforms with both theoretically and empirically driven methodologies. Although the number of non-humanoid robots deployed in the world is growing, this field of research is still maturing. This work provides a foundation for future human-robot interaction research in these areas while promoting generalizability and standardization of robot behaviors across the diverse set of existing non-humanoid robots",University of Southern California. Libraries,Nonverbal communication for non-humanoid robots,,,,core
162433564,2018-01-01T00:00:00,"L’impegno e la precisione che Eduardo Souto de Moura dedica a ogni occasione di dialogo con i luoghi del suo operare, conferma la sua adesione alla concretezza e la sua aspirazione alla verità. L’architettura priva di quelle fondamenta che la ancorano alla realtà e alla specificità dei luoghi, solo pensata e discussa in termini di potenzialità e astrazione, cieca e sorda alle suggestioni dell’ambiente, non esiste. Non è Architettura. Non è, almeno, l’Architettura di Souto de Moura, che è capace, invece, di trasformare le occasioni di dialogo con il contesto in accese discussioni, tra pensiero e realtà, tra desiderio e opportunità, tra segno progettuale e atto costruttivo; dispute, tra l’immagine primitiva che si forma nella mente – un dettaglio, un materiale, uno spazio – e la sua effettiva realizzabilità; confronti, che si annullano con la presa di coscienza delle immense opportunità offerte al progetto dal contatto con il reale; e il progetto, come in un film, esegue un montaggio e una verifica delle immagini, suggestioni e sequenze che l’occhio percepisce e che la grande poesia di Herberto Helder sottende ed esalta nel suo poema “Memoria Montagem”, uno dei grandi riferimenti del maestro portoghese.

Questo metodo, che mette al vaglio le ipotesi e le cala nei luoghi, è applicato indistintamente alle varie scale e ai diversi ambiti di lavoro, alle questioni di composizione volumetrica, al sistema distributivo, a quello tecnologico e impiantistico, alle scelte figurative e materiche. Esso si risolve in una conversazione continua, eterna, un dialogo antico, tra l’architetto – ogni architetto, di ogni tempo e di ogni luogo – e gli spazi in cui egli opera, un dialogo che non ha principio né fine, che si ripete sempre uguale da millenni, che pone e ripone le stesse domande e, alla fine, riceve le stesse risposte.The commitment and precision Eduardo Souto de Moura delivers in every dialogue opportunity with the places of his work confirms his bonds to concreteness and his ambition to truth. Architecture not founded on bases linked to reality and to place specificity; architecture as a mere result of debate and thinking on abstraction, does not exist. It is no Architecture. It isn’t Souto de Moura’s Architecture, which instead is able to transform the opportunities of establishing a dialogue with the context into fervid discussions between thinking and reality, between desire and possibility, between the marks of design and the act of bulding; into discussions between the primitive image forming in the architect’s min – a detais, a material, a space – and its actual feasibility; discussions that are meant to resolve themselves into the awareness of the limitless possibilities given to the project bt the contact with reality. The project, as in a movie, performs then an editing and a test of the many images, suggestions and sequences the eye senses; the same as in Herberto Helder’s poem “Memoria Montagem”, one of the most relevant references of the portuguese master architect.

This method of evaluating the hypothesis – impressions, opinions or even a sort of architectural schemes in progress – and lowering them into the sites, is implemented indifferently at several scales and several programs; at the issues of volumetric composition, at the level of both technology and plants; at the figurative and material options. It is resumed in a continuous and eternal conversation, in an ancient dialogue between the architect – every architect, of any time and place – and the spaces the architect works into; a dialogue having no beginning nor ending, that keeps happening in the centuries always in the same way, asking again and again the same questions, getting, at the end, always the same anwers",,"EDUARDO SOUTO DE MOURA. UN ""DIALOGO ANTICO"" TRA MATERIA, TECNICA E PROGETTO",10.13128/Techne-23985,https://core.ac.uk/download/162433564.pdf,,core
195379882,2018-01-01T00:00:00,"abstract: The rapid improvement in computation capability has made deep convolutional neural networks (CNNs) a great success in recent years on many computer vision tasks with significantly improved accuracy. During the inference phase, many applications demand low latency processing of one image with strict power consumption requirement, which reduces the efficiency of GPU and other general-purpose platform, bringing opportunities for specific acceleration hardware, e.g. FPGA, by customizing the digital circuit specific for the deep learning algorithm inference. However, deploying CNNs on portable and embedded systems is still challenging due to large data volume, intensive computation, varying algorithm structures, and frequent memory accesses. This dissertation proposes a complete design methodology and framework to accelerate the inference process of various CNN algorithms on FPGA hardware with high performance, efficiency and flexibility. 

As convolution contributes most operations in CNNs, the convolution acceleration scheme significantly affects the efficiency and performance of a hardware CNN accelerator. Convolution involves multiply and accumulate (MAC) operations with four levels of loops. Without fully studying the convolution loop optimization before the hardware design phase, the resulting accelerator can hardly exploit the data reuse and manage data movement efficiently. This work overcomes these barriers by quantitatively analyzing and optimizing the design objectives (e.g. memory access) of the CNN accelerator based on multiple design variables. An efficient dataflow and hardware architecture of CNN acceleration are proposed to minimize the data communication while maximizing the resource utilization to achieve high performance.

Although great performance and efficiency can be achieved by customizing the FPGA hardware for each CNN model, significant efforts and expertise are required leading to long development time, which makes it difficult to catch up with the rapid development of CNN algorithms. In this work, we present an RTL-level CNN compiler that automatically generates customized FPGA hardware for the inference tasks of various CNNs, in order to enable high-level fast prototyping of CNNs from software to FPGA and still keep the benefits of low-level hardware optimization. First, a general-purpose library of RTL modules is developed to model different operations at each layer. The integration and dataflow of physical modules are predefined in the top-level system template and reconfigured during compilation for a given CNN algorithm. The runtime control of layer-by-layer sequential computation is managed by the proposed execution schedule so that even highly irregular and complex network topology, e.g. GoogLeNet and ResNet, can be compiled. The proposed methodology is demonstrated with various CNN algorithms, e.g. NiN, VGG, GoogLeNet and ResNet, on two different standalone FPGAs achieving state-of-the art performance.

Based on the optimized acceleration strategy, there are still a lot of design options, e.g. the degree and dimension of computation parallelism, the size of on-chip buffers, and the external memory bandwidth, which impact the utilization of computation resources and data communication efficiency, and finally affect the performance and energy consumption of the accelerator. The large design space of the accelerator makes it impractical to explore the optimal design choice during the real implementation phase. Therefore, a performance model is proposed in this work to quantitatively estimate the accelerator performance and resource utilization. By this means, the performance bottleneck and design bound can be identified and the optimal design option can be explored early in the design phase.Dissertation/ThesisDoctoral Dissertation Electrical Engineering 201",,Hardware Acceleration of Deep Convolutional Neural Networks on FPGA,,https://core.ac.uk/download/195379882.pdf,,core
186564469,2018-01-01T00:00:00,"Розглядається метод інформаційно-екстремального машинного навчання системи функціонального діагностування технічного стану складної машини з оптимізацією ієрархічної структури вхідних даних. Показано, що на функціональну ефективність машинного навчання системи функціонального діагностування суттєво впливає розміщення в ієрархічній структурі класів розпізнавання, які характеризують
технічний стан машини та її вузлів. При цьому для кожної страти ієрархічної структури накладаються обмеження на кількість класів розпізнавання, що дозволяє зменшити ступінь їх перетину в просторі діагностичних ознак. Оптимізація ієрархічної структури здійснюється в процесі інформаційно-екстремального машинного навчання системи функціонального діагностування, що дозволяє максимізувати
інформаційну спроможність системи. Як критерій оптимізації параметрів машинного навчання розглядається модифікована інформаційна міра Кульбака, яка є функціоналом точнісних характеристик діагностичних рішень. При цьому алгоритм машинного навчання представляв собою багатоциклічну ітераційну процедуру пошуку максимального глобального значення інформаційного критерію оптимізації параметрів машинного навчання в робочій (допустимій) області визначення його функції. В результаті для страт всіх ярусів ієрархічної структури сформовано алфавіти класів розпізнавання, які забезпечили максимальну функціональну ефективність машинного навчання. За отриманими в процесі машинного навчання оптимальними геометричними параметрами контейнерів класів розпізнавання побудовано вирішальні правила, які дозволяють приймати діагностичні рішення в реальному темпі часу. Крім того, вирішальні правила, побудовані в рамках геометричного підходу, є практично інваріантними до багатовимірності вхідних даних, що є їх суттєвою перевагою перед штучними нейронними мережами. Як приклад реалізації запропонованого методу розглядалося машинне навчання системи функціонального діагностування шахтної підйомної машини з оптимізацією структури вхідних даних.The conclusions about the strata of society, various parties are supported by, have been made. The method of information-extreme machine learning of the system of functional diagnosis of the technical state of a complex machine with the optimization of the hierarchical data structure is considered. It is shown that the functional efficiency of machine learning of the system of functional diagnosis is significantly influenced by the location in the hierarchical structure of the recognition classes characterizing the technical state of the machine and its nodes. At the same time, for each level of the hierarchical structure under consideration, a restriction on the number of recognition classes is imposed, which makes it possible to reduce the degree of their intersection in the space of diagnostic features. Optimization of the hierarchical structure was carried out in the process of information-extreme machine learning of the system of functional diagnosis, which allows to maximize the information capacity of the system. As a criterion for optimizing the parameters of machine learning, we considered a modi fied information measure of Kulbak, which is a functional of the accurate characteristics of diagnostic solutions. In this case, the algorithm of machine learning represented a multi-cycle iterative procedure of finding the maximum global value of the information criterion for optimizing learning parameters in the working (permissible) domain of determining its function. Based on the optimal geometric parameters of recognition class containers obtained in the course of machine learning, decision rules have been constructed that allow making diagnostic decisions in a real time. As an example of the implementation of the method of optimization the structure of input data, the machine learning of the system for the functional diagnosis of a mine hoist was considered. As a result, alphabets of recognition classes have been created for strata of all tiers of the hierarchical structure, providing the maximum functional efficiency of machine learnin","НТУ ""ХПІ""",Optimization of hierarchical data structure of intelligent system of functional diagnosis of technical condition of complex machines,,https://core.ac.uk/download/186564469.pdf,,core
199655452,2018-05-01T00:00:00,"The unprecedented volume and rate of transient events that will be discovered by the Large Synoptic Survey Telescope (LSST) demand that the astronomical community update its follow-up paradigm. Alert-brokers-automated software system to sift through, characterize, annotate, and prioritize events for follow-up-will be critical tools for managing alert streams in the LSST era. The Arizona-NOAO Temporal Analysis and Response to Events System (ANTARES) is one such broker. In this work, we develop a machine learning pipeline to characterize and classify variable and transient sources only using the available multiband optical photometry. We describe three illustrative stages of the pipeline, serving the three goals of early, intermediate, and retrospective classification of alerts. The first takes the form of variable versus transient categorization, the second a multiclass typing of the combined variable and transient data set, and the third a purity-driven subtyping of a transient class. Although several similar algorithms have proven themselves in simulations, we validate their performance on real observations for the first time. We quantitatively evaluate our pipeline on sparse, unevenly sampled, heteroskedastic data from various existing observational campaigns, and demonstrate very competitive classification performance. We describe our progress toward adapting the pipeline developed in this work into a real-time broker working on live alert streams from time-domain surveys.Lasker Fellowship at the Space Telescope Science Institute; SKA SA; NRF; AIMS; NSF INSPIRE grant [CISE AST-1344204]; NOAO Community Science Data Center (CSDC)This item from the UA Faculty Publications collection is made available by the University of Arizona with support from the University of Arizona Libraries. If you have questions, please contact us at repository@u.library.arizona.edu",'American Astronomical Society',Machine-learning-based Brokers for Real-time Classification of the LSST Alert Stream,10.3847/1538-4365/aab781,,"[{'title': 'The Astrophysical Journal Supplement Series', 'identifiers': ['1538-4365', 'issn:1538-4365']}]",core
201544188,2018-12-01T00:00:00,"Immersive techniques such as augmented reality through devices such as the AR-Sandbox and deep learning through convolutional neural networks (CNN) provide an environment that is potentially applicable for motor rehabilitation and early education. However, given the orientation towards the creation of topographic models and the form of representation of the AR-Sandbox, the classification of images is complicated by the amount of noise that is generated in each capture. For this reason, this research has the purpose of establishing a model of a CNN for the classification of geometric figures by optimizing hyperparameters using Random Search, evaluating the impact of the implementation of a previous phase of color&#8315;space segmentation to a set of tests captured from the AR-Sandbox, and evaluating this type of segmentation using similarity indexes such as Jaccard and S&#248;rensen&#8315;Dice. The aim of the proposed scheme is to improve the identification and extraction of characteristics of the geometric figures. Using the proposed method, an average decrease of 39.45% to a function of loss and an increase of 14.83% on average in the percentage of correct answers is presented, concluding that the selected CNN model increased its performance by applying color&#8315;space segmentation in a phase that was prior to the prediction, given the nature of multiple pigmentation of the AR-Sandbox",'MDPI AG',Hyperparameter Optimization for Image Recognition over an AR-Sandbox Based on Convolutional Neural Networks Applying a Previous Phase of Segmentation by Color–Space,10.3390/sym10120743,,"[{'title': 'Symmetry', 'identifiers': ['2073-8994', 'issn:2073-8994']}]",core
186281718,2018-10-24T00:00:00,"We demonstrate, that artificial neural networks (ANN) can be trained to
emulate single or multiple basic quantum operations. In order to realize a
quantum state, we implement a novel ""quantumness gate"" that maps an arbitrary
matrix to the real representation of a positive hermitean normalized density
matrix. We train the CNOT gate, the Hadamard gate and a rotation in Hilbert
space as basic building blocks for processing the quantum density matrices of
two entangled qubits. During the training process the neural networks learn to
represent the complex structure, the hermiticity, the normalization and the
positivity of the output matrix. The requirement of successful training allows
us to find a critical bottleneck dimension which reflects the relevant quantum
information. Chains of individually trained neural quantum gates can be
constructed to realize any unitary transformation. For scaling to larger
quantum systems, we propose to use correlations of stochastic macroscopic
two-level observables or classical bits. This novel concept provides a path for
a classical implementation of computationally relevant quantum information
processing on classical neural networks, in particular on neuromorphic
computing machines featuring stochastic operations.Comment: 6 pages , 3 figure",,Emulating quantum computation with artificial neural networks,,http://arxiv.org/abs/1810.10335,,core
189597487,"August 6, 2018","This paper discusses a machine learning-based approach to routing for delay tolerant networks (DTNs) [1]. DTNs are networks which experience frequent disconnections between nodes, uncertainty of an end-to-end path, long one-way trip times, and may have high error rates and asymmetric links. Such networks exist in deep space satellite networks, very rural environments, disaster areas and underwater environments. In this work, we use machine learning classifiers to predict a set of neighboring nodes which are the most likely to deliver a message to a desired location based on message history delivery information.We use the Common Open Research Emulator (CORE) [2] to emulate the DTN environment based on real-world location traces and collect network traffic statistics from the Bundle Protocol implementation IBR-DTN [3]. The software architecture for classification-based routing, analysis and preparation of the network history data and prediction results are discussed",,Delay Tolerant Network Routing as a Machine Learning Classification Problem,,https://core.ac.uk/download/pdf/189597487.pdf,,core
295381316,2018-01-31,"No mundo atual, não consegue-se pensar em jogos que não tenham um método de Inteligência Artificial implementado, mesmo que sejam soluções simples.  Entretanto, boa parte dos métodos implementados em sistemas comerciais ainda não utilizam técnicas de Aprendizado de Máquina, devido ao custo de treinar e obter resultados em tempo real.Nesse trabalho, foi implementado um algoritmo de Aprendizado de Máquina para dotar o agente principal do jogo Bomberman de uma técnica de Aprendizado de Máquina, que pudesse ser treinado offline e usar o modelo aprendido durante o jogo online.  Um algoritmo de Aprendizado por Reforço foi selecionado como método a ser desenvolvido, especificamente o algoritmo Q-learning clássico.  Esse algoritmo se vale de conceitos como estado,ações, objetivos e recompensa para determinar uma solução para o problema apresentado.Os experimentos em uma arena padrão do Bomberman, demonstraram que o agente não converge para uma solução ótima, devido ao tamanho do espaço de busca explorado e ao ambiente não-determinístico, gerado pelo oponente.  Quando uma arena simplificada comum oponente imóvel foi utilizada nos experimentos, o Q-learning convergiu para uma solução ótima com facilidade.  Os resultados do agente com Aprendizado por Reforço foram comparados aos resultados obtidos por um agente que utiliza a árvore de busca de Monte Carlo, uma técnica que tem se mostrado uma substituta efetiva ao clássico Minimax, e que não utiliza Aprendizado de Máquina. Os resultados exibidos apontam a superioridade do Agente com Aprendizado por Reforço quando comparado com o Agente que utiliza o Monte Carlo.Nowadays, there are few exceptions to video games that do not employ an Artificial
Intelligence method, even if such a method is quite simple. However, a good chunk of
commercials systems still do not use Machine Learning because of the necessary cost to
train a model in real time. In this work, an agent-oriented Machine Learning algorithm
was developed for the game Bomberman so that such an agent could be trained offline
and use the learned model during the game. The classic Q-learning, a Reinforcement
Learning method, was chosen to be implemented. This algorithm relies on concepts such
as states, actions, objectives and rewards to solve a problem. The experiments ran in the
Bomberman default arena demonstraded that the agent was not capable of converging to
an optimal solution due to the size of the sample space of possible states and due to the
non-deterministic enviroment generated by the opponent. When a simplified Bomberman
arena was used for the experiments, the Q-learning agent converged rather easily to an
optimal solution. The Q-learning agent results were compared to the results of a Monte
Carlo tree search agent, a method that does not use Machine Learning and has been
proved to be as an effective replacement for the traditional Minimax algorithm. The
obtained results indicated that the Reinforcement Learning agent is more effective for the
Bomberman when compared with the Monte Carlo agent",Niterói,Uma implementação de um agente autônomo para o jogo Bomberman com aprendizado por reforço,,,,core
297050421,2018-10-16T00:00:00,"Context. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.
				Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.
				Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.
				Results. The development and implementation of this method in a five-dimensional space (l, b, ϖ, μα*, μδ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",'EDP Sciences',A new method for unveiling open clusters in,10.1051/0004-6361/201833390,,,core
346551229,2018-01-01T00:00:00,"Three-dimensional (3D) fetal neurosonography is used clinically to detect cerebral abnormalities and to assess growth in the developing brain. However, manual identification of key brain structures in 3D ul- trasound images requires expertise to perform and even then is tedious. Inspired by how sonographers view and interact with volumes during real-time clinical scanning, we propose an efficient automatic method to simultaneously localize multiple brain structures in 3D fetal neurosonography. The proposed View-based Projection Networks (VP-Nets) , uses three view-based Convolutional Neural Networks (CNNs), to simplify 3D localizations by directly predicting 2D projections of the key structures onto three anatom- ical views.  While designed for efficient use of data and GPU memory, the proposed VP-Nets allows for full- resolution 3D prediction. We investigated parameters that influence the performance of VP-Nets, e.g. depth and number of feature channels. Moreover, we demonstrate that the model can pinpoint the struc- ture in 3D space by visualizing the trained VP-Nets, despite only 2D supervision being provided for a sin- gle stream during training. For comparison, we implemented two other baseline solutions based on Ran- dom Forest and 3D U-Nets. In the reported experiments, VP-Nets consistently outperformed other meth- ods on localization. To test the importance of loss function, two identical models are trained with binary corss-entropy and dice coefficient loss respectively. Our best VP-Net model achieved prediction center deviation: 1.8 ±1.4 mm, size difference: 1.9 ±1.5 mm, and 3D Intersection Over Union (IOU): 63.2 ±14.7% when compared to the ground truth. To make the whole pipeline intervention free, we also implement a skull-stripping tool using 3D CNN, which achieves high segmentation accuracy. As a result, the proposed processing pipeline takes a raw ultrasound brain image as input, and output a skull-stripped image with five detected key brain structures.</p",'Elsevier BV',VP-Nets : Efficient automatic localization of key brain structures in 3D fetal neurosonography,,,,core
323533353,2018-01-01T00:00:00,"У статті досліджено особливості розвитку цифрового маркетингу в сучасних умовах. Розглянуто специфіку запровадження інноваційних цифрових технологій у маркетингові діяльності. Визначено специфіку розвитку віртуальної реальності та особливості її використання у системі цифрового маркетингу. У науковому дослідженні представлено приклади передового використання технологій віртуальної реальності компаніями світу при реалізації їх маркетингових стратегій. Висвітлено специфіку застосування технології віртуальної реальності за умови витрат значних фінансових ресурсів. Також наведено приклад використання дешевих аналогів цифрових технологій за умови використання смартфонів. Висвітлено специфіку залучення клієнтів до дешевих цифрової технологій та особливості збільшення інтересу цільової аудиторії. Отримані результати дають можливість визначити основні тенденції розвитку цифрового маркетингу з застосуванням технологій віртуальної реальності. У дослідженні значну увагу приділено питанням реалізації технологій використанню LED панелей. Обґрунтовано доцільність використання зазначеної технології для indoor маркетингу – виду діяльності всередині приміщень (торговельних закладів, кафе, офісів компаній тощо) та outdoor маркетингу – виду діяльності у зовнішньому просторі (на улицях, парках та ін.). Наведено приклади застосування LED панелей передовими компаніями світу. Висвітлено специфіку застосування даної технології у сфері харчування. Встановлено, що LED панелі відіграють важливу роль у цифровій маркетинговій стратегії кафе та ресторанів. Визначено, що цифрові панелі сприяють зростанню кількості клієнтів. Доведено, що запровадження систем штучного інтелекту у подальшому сприятиме збільшенню інтерактивності та персоніфікації контенту у відповідності з умовами, які будуть проявлятись у певний момент часу в конкретному місці. Встановлено, що зазначений підхід дозволить національним компаніями підвищити рівень їх конкурентоспроможності на національному та міжнародному ринках. Доведено необхідність запровадження державної програми сприяння розвитку цифрового маркетингу в Україні.In the article the features of development of digital marketing in modern conditions are investigated. The specifics of introduction of innovative technologies in marketing strategies of companies are considered. The specificity of the development of virtual reality and the peculiarities of its using in the digital marketing system are determined. In the scientific research, examples of the advanced use of technologies of virtual reality by companies of the world in the implementation of their marketing strategies are presented. The specificity of the application of the virtual reality technology is highlighted with the cost of significant financial resources. The example is the using of cheap analogues of digital technology such as using smartphones. The specifics of attraction the clients to cheap digital technologies and interest increase features of the target audience are highlighted. The obtained results give an opportunity to define the basic tendencies of digital marketing development with application of virtual reality technologies. The study focuses on the implementation of technology for the use of LED panels. The expediency of using this technology for indoor marketing is grounded - the type of activity in the middle of the premises (shopping centers, cafes, offices of companies, etc.) and outdoor marketing - the type of activity in the outer space (in streets, parks, etc.). One example of the using of LED panels is presented by leading companies of the world. The application specifics of this technology in the field of food are highlighted. It has been established that LED panels play the important role in the digital marketing strategy of cafes and restaurants. It is determined that digital panels help to increase the number of clients. It is proved that the introduction of artificial intelligence systems will further enhance the interactivity and personalization of content in accordance with conditions that will manifest at a certain point in time in a special place. It has been established that this approach will allow national companies to increase their competitiveness in the national and international markets. The necessity of implementation of the state program of digital marketing promotion in Ukraine has been proved",'Kyiv Politechnic Institute',Practical Aspects of Innovative Digital Tecnologies Application in Marketing,10.20535/2307-5651.15.2018.139575,https://core.ac.uk/download/323533353.pdf,,core
195380051,2018-01-01T00:00:00,"abstract: Deep learning (DL) has proved itself be one of the most important developements till date with far reaching impacts in numerous fields like robotics, computer vision, surveillance, speech processing, machine translation, finance, etc. They are now widely used for countless applications because of their ability to generalize real world data, robustness to noise in previously unseen data and high inference accuracy. With the ability to learn useful features from raw sensor data, deep learning algorithms have out-performed tradinal AI algorithms and pushed the boundaries of what can be achieved with AI. In this work, we demonstrate the power of deep learning by developing a neural network to automatically detect cough instances from audio recorded in un-constrained environments. For this, 24 hours long recordings from 9 dierent patients is collected and carefully labeled by medical personel. A pre-processing algorithm is proposed to convert event based cough dataset to a more informative dataset with start and end of coughs and also introduce data augmentation for regularizing the training procedure. The proposed neural network achieves 92.3% leave-one-out accuracy on data captured in real world. 

Deep neural networks are composed of multiple layers that are compute/memory intensive. This makes it difficult to execute these algorithms real-time with low power consumption using existing general purpose computers. In this work, we propose hardware accelerators for a traditional AI algorithm based on random forest trees and two representative deep convolutional neural networks (AlexNet and VGG). With the proposed acceleration techniques, ~ 30x performance improvement was achieved compared to CPU for random forest trees. For deep CNNS, we demonstrate that much higher performance can be achieved with architecture space exploration using any optimization algorithms with system level performance and area models for hardware primitives as inputs and goal of minimizing latency with given resource constraints. With this method, ~30GOPs performance was achieved for Stratix V FPGA boards. 

Hardware acceleration of DL algorithms alone is not always the most ecient way and sucient to achieve desired performance. There is a huge headroom available for performance improvement provided the algorithms are designed keeping in mind the hardware limitations and bottlenecks. This work achieves hardware-software co-optimization for Non-Maximal Suppression (NMS) algorithm. Using the proposed algorithmic changes and hardware architecture 

With CMOS scaling coming to an end and increasing memory bandwidth bottlenecks, CMOS based system might not scale enough to accommodate requirements of more complicated and deeper neural networks in future. In this work, we explore RRAM crossbars and arrays as compact, high performing and energy efficient alternative to CMOS accelerators for deep learning training and inference. We propose and implement RRAM periphery read and write circuits and achieved ~3000x performance improvement in online dictionary learning compared to CPU. 

This work also examines the realistic RRAM devices and their non-idealities. We do an in-depth study of the effects of RRAM non-idealities on inference accuracy when a pretrained model is mapped to RRAM based accelerators. To mitigate this issue, we propose Random Sparse Adaptation (RSA), a novel scheme aimed at tuning the model to take care of the faults of the RRAM array on which it is mapped. Our proposed method can achieve inference accuracy much higher than what traditional Read-Verify-Write (R-V-W) method could achieve. RSA can also recover lost inference accuracy 100x ~ 1000x faster compared to R-V-W. Using 32-bit high precision RSA cells, we achieved  ~10% higher accuracy using fautly RRAM arrays compared to what can be achieved by mapping a deep network to an 32 level RRAM array with no variations.Dissertation/ThesisDoctoral Dissertation Electrical Engineering 201",,Algorithm and Hardware Design for Efficient Deep Learning Inference,,https://core.ac.uk/download/195380051.pdf,,core
287819264,2019-02-22T08:00:00,"The fully connected world is quickly becoming a reality. Architects and developers of this new world must understand both the hardware and software basics of IoT and IIoT systems as well as the proven way to deal with the complexities of the integration of sensors, processors, wireless connectivity, edge to cloud networks, data partitioning and processing, AI, machine language, digital threads and twins, and much more. Such complexity can only be handled with a systems-of-systems (SoS) engineering approach.
But while systems engineering may hold many of the solutions to IoT challenges, systems engineering must evolve from its traditional role. Some have even suggested that the data requirements and digitization of the IoT and corresponding digital threads are putting the engineering back into systems engineering via model-based designs. This will also help reconnect system engineering to system science.
This presentation will show how the IoT hardware and software technologies are changing the traditional systems engineering approach. Further, professionals that are so prepared with both the basics of IoT and systems engineering will stand a better chance of competing in the IoT space.https://pdxscholar.library.pdx.edu/systems_science_seminar_series/1078/thumbnail.jp",PDXScholar,IoT and Digitization Will Reconnect System Engineering and Science,,https://core.ac.uk/download/287819264.pdf,,core
334847458,2019-08-14T00:00:00,"In machine learning applications for online product offerings and marketing
strategies, there are often hundreds or thousands of features available to
build such models. Feature selection is one essential method in such
applications for multiple objectives: improving the prediction accuracy by
eliminating irrelevant features, accelerating the model training and prediction
speed, reducing the monitoring and maintenance workload for feature data
pipeline, and providing better model interpretation and diagnosis capability.
However, selecting an optimal feature subset from a large feature space is
considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum
Relevance) feature selection framework solves this problem by selecting the
relevant features while controlling for the redundancy within the selected
features. This paper describes the approach to extend, evaluate, and implement
the mRMR feature selection methods for classification problem in a marketing
machine learning platform at Uber that automates creation and deployment of
targeting and personalization models at scale. This study first extends the
existing mRMR methods by introducing a non-linear feature redundancy measure
and a model-based feature relevance measure. Then an extensive empirical
evaluation is performed for eight different feature selection methods, using
one synthetic dataset and three real-world marketing datasets at Uber to cover
different use cases. Based on the empirical results, the selected mRMR method
is implemented in production for the marketing machine learning platform. A
description of the production implementation is provided and an online
experiment deployed through the platform is discussed",,"Maximum Relevance and Minimum Redundancy Feature Selection Methods for a
  Marketing Machine Learning Platform",,http://arxiv.org/abs/1908.05376,,core
334886259,2019-11-25T00:00:00,"Existing deep learning methods for action recognition in videos require a
large number of labeled videos for training, which is labor-intensive and
time-consuming. For the same action, the knowledge learned from different media
types, e.g., videos and images, may be related and complementary. However, due
to the domain shifts and heterogeneous feature representations between videos
and images, the performance of classifiers trained on images may be
dramatically degraded when directly deployed to videos. In this paper, we
propose a novel method, named Deep Image-to-Video Adaptation and Fusion
Networks (DIVAFN), to enhance action recognition in videos by transferring
knowledge from images using video keyframes as a bridge. The DIVAFN is a
unified deep learning model, which integrates domain-invariant representations
learning and cross-modal feature fusion into a unified optimization framework.
Specifically, we design an efficient cross-modal similarities metric to reduce
the modality shift among images, keyframes and videos. Then, we adopt an
autoencoder architecture, whose hidden layer is constrained to be the semantic
representations of the action class names. In this way, when the autoencoder is
adopted to project the learned features from different domains to the same
space, more compact, informative and discriminative representations can be
obtained. Finally, the concatenation of the learned semantic feature
representations from these three autoencoders are used to train the classifier
for action recognition in videos. Comprehensive experiments on four real-world
datasets show that our method outperforms some state-of-the-art domain
adaptation and action recognition methods.Comment: Accepted by IEEE Transactions on Image Processing, codes can be found
  at https://yangliu9208.github.io/DIVAFN",'Institute of Electrical and Electronics Engineers (IEEE)',"Deep Image-to-Video Adaptation and Fusion Networks for Action
  Recognition",10.1109/TIP.2019.2957930,http://arxiv.org/abs/1911.10751,,core
200406146,2019-05-15T00:25:37Z,"Pawsey Supercomputing Centre is one of two national high performance computing centres in Australia
providing ‘tier 1’ HPC services to researchers. Pawsey Supercomputing Centre is operated by a joint venture
made of up of partner universities and CSIRO with operational and capital investment from the WA State
government and the Australian Federal Government. Pawsey is currently in the early stages of the design
and procurement of a new generation of computational infrastructure with a new $70m investment, made
available in mid 2018. Rapid and emerging change in the demands made on the infrastructure by an
increasingly diverse research community requires specific technical solutions to be developed during this
process. We will discuss this rapid change, where Pawsey is now and how we are addressing evolving
requirements with our current infrastructure. We will also discuss where HPC is challenged in the coming
period and how we are expecting to meet these challenges. <div><br></div><div><b><u>ABOUT THE AUTHOR(S) </u></b></div><div>Ugo Varetto, Chief Technical Officer, is an accomplished manager, bringing international experience, including computational
science, research, supporting complex, distributed software solutions, software engineering and
development, architecture and testing to Pawsey. </div><div><br></div><div>His focus is particularly on data-intensive high-performance computing, interactive-distributed
visualisation solutions, with ‘real world’ experience of leading distributed teams across several
continents. He has held key roles in significant projects, such as the EU funded ‘Human Brain
Project’ and also contributed to numerous research projects with the European supercomputing
infrastructure. 
His current work includes developing scalable artificial intelligence (AI) and machine learning
solutions on HPC systems and integrating interactive, in situ visualisation with data mining.</div><div><br></div><div>Mark Gray is the Cloud Lead at Pawsey Supercomputing Centre where he manages Nimbus, a
national cloud service for researchers in Australia. Mark has experience in DevOps roles at
Pawsey, IMOS, AusCover and NASA Goddard Space Flight Center. </div><div><br></div><div>Mark represents Pawsey Supercomputing Centre both nationally and internationally, with respect to
the use of Nimbus. He also provides project management skills for projects of key strategic
importance to the Centre. </div><div><br></div><div>Mr Gray brings a strong research and data management background to his role of managing all
aspects of this service. This includes procurement, deployment, training, user expectation
management – and leadership of the team who administer and operate the service.</div",,Current and future directions for HPC at Pawsey Supercomputing Centre,10.6084/m9.figshare.8066837.v1,,,core
305125096,2019-01-01T00:00:00,"In recent years, machine learning (ML) and, more noticeably, deep learning (DL), have be- come increasingly ubiquitous. Applications of these technologies are being seen in many fields, including health care, manufacturing, and end-consumer services. In terms of deployment, deep neural networks (DNNs) are found in consumer devices, small internet-of-things devices, embedded in vehicles, and on a large scale in data centers and servers. The trend indicates that the use of DL in smart applications will continue to increase in the coming years.As the name suggests, learning is an integral part of the functionality of DNNs, whether this learning takes place off-line before deployment, or happens in real time while the DNN is carrying out its assigned task. As part of the learning process, training is required to set the parameters, also known as weights, of the DNN in order to achieve high accuracy in the assigned task. Without training, the DNN is rendered useless, given that the parameters are not set correctly. It has been shown that this training process requires large amounts of data and a high number of training iterations for the DNN model to be effective. The weights are updated in each iteration based on the subset of the training data provided. The training process has proven to be a challenge given the long timescale involved. The amount of training data, the number of weights, and the computational complexity of updating those weights are all factors that contribute to this challenge. One way to reduce training time is to allocate processes to a multitude of processors, thus achieving some sort of sub-optimal parallelism. One approach is to have this decision be carried out by ML or DL experts. The problem with this is the absence of concrete information to ensure the best decision is taken: the time it takes for a particular process to run on a particular processor, and the costs of inter-communication between processors, are in fact unknown. Even with the intuition of an expert in this domain, a sub-optimal solution that outperforms a single-processor use case is not achieved.In this dissertation, a hybrid-based multi-step optimization framework is presented. The framework explores the vast design space of mapping processes to processors. The search and evaluation are conducted in real time while training the DNN. In the first stage of the framework we compare the algorithmic intuitive approach with the Bayesian optimization (BO) approach. In the second stage of the framework, we create a predictive function for the performance of a single iteration of training, comparing the accuracy of different predictive functions created by different ML algorithms. The developed predictive model is then used as a surrogate function when identifying the best mapping. This stage in the search applies genetic algorithms (GA). An adaptive feature is also presented and tested for responsiveness to any changes that affect the performance of the training in the system.We also present heterogeneous earliest finish time (HEFT): a deterministic approach to map- ping. In addition, we present the concept of node splitting, which refers to the computational graph of the DNN being split in order to accommodate a higher level of model parallelism. It is noted that this would also affect the accuracy of the DNN, since the hyperparameters are affected.The framework and methodologies were evaluated in real, non-simulated systems using wall- clock time. The DNNs were built using Google’s ML/DL library, TensorFlow (TF)","eScholarship, University of California",Deep Learning Performance Optimization via Model Parallelization,,,,core
211248000,2019-06-19T00:00:00,"FINDING effective clustering methods for a high dimensional dataset is challenging due to the curse of dimensionality. These challenges can usually make the most of basic common algorithms fail in highdimensional spaces from tackling problems such as large number of groups, and overlapping. Most domains uses some parameters to describe the appearance, geometry and dynamics of a scene. This has motivated the implementation of several techniques of a high-dimensional data for finding a low-dimensional space. Many proposed methods fail to overcome the challenges, especially when the data input is high-dimensional, and the clusters have a complex.



REGULARLY in high dimensional data, lots of the data dimensions are not related and might hide the existing clusters in noisy data. High-dimensional data often reside on some low dimensional subspaces. The problem of subspace clustering algorithms is to uncover the type of relationship of an objects from one dimension that are related in different subsets of another dimensions. The state-of-the-art methods for subspace segmentation which included the Low Rank Representation (LRR) and Sparse Representation (SR). The former

seeks the global lowest-rank representation but restrictively assumes the independence among subspaces, whereas the latter seeks the clustering of disjoint or overlapped subspaces through locality measure, which, however, causes failure in the case of large noise.



THIS thesis aims are to identify the key problems and obstacles that have challenged the researchers in recent years in clustering high dimensional data, then to implement an effective subspace clustering methods for solving high dimensional crimes domains for both real events and synthetic data which has complex data structure with 168 different offence crimes. As well as to overcome the disadvantages of existed subspace algorithms techniques. To this end, a Low-Rank Sparse Representation (LRSR) theory, the future will refer to as Criminal Data Analysis Based on LRSR will be examined, then to be used to recover and segment embedding subspaces. The results of these methods will be discussed and compared with what already have been examined on previous approaches such as K-mean and PCA segmented based on K-means. The previous approaches have helped us to chose the right subspace clustering methods. The Proposed method based on subspace segmentation method named Low Rank subspace Sparse Representation (LRSR) which not only recovers the low-rank subspaces but also gets a relatively sparse segmentation with respect to disjoint subspaces or even overlapping subspaces.



BOTH UCI Machine Learning Repository, and crime database are the best to find and compare the best subspace clustering algorithm that fit for high dimensional space data. We used many Open-Source Machine Learning Frameworks and Tools for both employ our machine learning tasks and methods including preparing, transforming, clustering and visualizing the high-dimensional crime dataset, we precisely have used the most modern and powerful Machine Learning Frameworks data science that known as SciKit-Learn for library for the Python programming language, as well as we have used R, and Matlab in previous experiment",,Criminal data analysis based on low rank sparse representation,,https://core.ac.uk/download/211248000.pdf,,core
185560963,2019-01-01T00:00:00,"Extreme classification is a rapidly growing research area within machine learning focusing on multi-class and multi-label problems involving an extremely large number of labels (even more than a million). Many applications of extreme classification have been found in diverse areas ranging from language modeling to document tagging in NLP, face recognition to learning universal feature representations in computer vision, gene function prediction in bioinformatics, etc. Extreme classification has also opened up a new paradigm for key industrial applications such as ranking and recommendation by reformulating them as multi-label learning tasks where each item to be ranked or recommended is treated as a separate label. Such reformulations have led to significant gains over traditional collaborative filtering and content-based recommendation techniques. Consequently, extreme classifiers have been deployed in many real-world applications in industry.

Extreme classification has raised many new research challenges beyond the pale of traditional machine learning including developing log-time and log-space algorithms, deriving theoretical bounds that scale logarithmically with the number of labels, learning from biased training data, developing performance metrics, etc. The seminar aimed at bringing together experts in machine learning, NLP, computer vision, web search and recommendation from academia and industry to make progress on these problems. We believe that this seminar has encouraged the inter-disciplinary collaborations in the area of extreme classification, started discussion on identification of thrust areas and important research problems, motivated to improve the algorithms upon the state-of-the-art, as well to work on the theoretical foundations of extreme classification","Dagstuhl Reports. Dagstuhl Reports, Volume 8, Issue 7",Extreme Classification (Dagstuhl Seminar 18291),10.4230/DagRep.8.7.62,,,core
482723679,2019-08-26T00:00:00,"The building sector globally accounts for more than one third of the nal energy consumption. In industrialized countries about 50% of the consumed energy in buildings is used for heating, ventilation, and air conditioning. Consequently, increasing the energy e ciency of buildings has a relevant impact on climate change mitigation. Most residential space heating and cooling systems are currently controlled by a simple rule-based controller, also known as on-o or hysteresis controller. Despite its simplicity rule-based controllers are rather energy ine cient. Model predictive control has demonstrated to be an energy e cient approach for building climate control. However, the modelling process for rst-principle based models is assumed to be too expensive in terms of time, money, and expert knowledge for widespread application of model predictive control in residential buildings. As the digital revolution is also taking place in the building domain the number of installed sensors, smart meters, and monitoring systems increases steadily. Thus, the availability of historical data from buildings enables the usage of machine learning algorithms for black-box modelling. Control-oriented machine learning models could potentially allow to reduce the modelling e ort signi cantly. Data-driven models based on random forests combined with linear models have been shown to be a promising approach in simulation studies. However, there is no demonstration of the applicability of data predictive control based on random forests combined with linear models in real-life so far. During this master thesis a data predictive control algorithm based on random forests combined with linear models was implemented and demonstrated for energy optimization and climate control in a residential apartment of the NEST demonstrator building at Empa D ubendorf. The data predictive controller reduced the energy consumption in various cooling experiment by about 30% compared to a hysteresis controller. Moreover, the data predictive controller provides higher thermal comfort by having up to 80% less cumulative constraint violations. In an experimental sensitivity analysis, the data predictive control algorithm showed relative robust performance for di erent control parameters. Furthermore, the performed cost-bene t analysis revealed that data predictive control might have the potential to become a commercial alternative to standard hysteresis controller in single family houses and apartments",ETH Zurich,Energy Optimization and Climate Control of a NEST Unit at Empa Dübendorf: A Data Predictive Control Approach,10.3929/ethz-b-000487694,,,core
477984512,2019-01-01T00:00:00,"Розвиток інформаційних технологій змінює всі усталені процеси функціонування як економіки в цілому, так і фінансового сектору зокрема. У той же час, цифровізація фінансового сектори створила сприятливі умови для розвитку кіберзлочинності, так здійснення фінансових злочинів перейшло з реального у віртуальний простір та зосередилось на викраданні інформації про банківські рахунки й банківські карти, зломи паролів, шахрайство з банкоматами та інше. У статті проведено бібліометричний аналіз наукових публікацій, присвячених проблемі кіберзлочинності в умовах цифровізації фінансового сектору економіки України. В результаті побудови графіків динаміки публікацій бази даних Scopus встановлено зростаючи тенденції публікативної активності науковців в сфері кіберзлочинності. Найвищу активність у вирішенні проблем пов’язаних з кіберзлочинами проявляють науковці США та Великобританії, як передових країн в сфері електронної комерції.Развитие информационных технологий меняет все устоявшиеся процессы функционирования как экономики в целом, так и финансового сектора в частности. В то же время, цифровизация финансового сектора создала благоприятные условия для развития киберпреступности, так осуществление финансовых преступлений перешло из реальной в виртуальное пространство и сосредоточилось на похищении информации о банковских счетах и ​​банковские карты, взломы паролей, мошенничество с банкоматами и прочее. В статье проведен библиометрические анализ научных публикаций, посвященных проблеме киберпреступности в условиях цифровизации финансового сектора экономики Украины. В результате построения графиков динамики публикаций базы данных Scopus установлено вырастая тенденции публикативнои активности ученых в сфере киберпреступности. Наивысшую активность в решении проблем связанных с киберпреступностью проявляют ученые США и Великобритании, как передовых стран в сфере электронной коммерции.The development of information technology changes the long-standing processes of functioning of both the economy in general and the financial sector in particular. Automation of business processes of financial institutions and digitalization of products of financial intermediaries create new conditions for the implementation of economic activities of all economic agents. Meanwhile, the digitalization of the financial sector has created favorable conditions for the development of cybercrime. Thus, financial crimes moved from the real to the virtual space and focused on the theft of information about bank accounts and bank cards, password cracking, ATM fraud, etc. The purpose of this article is to conduct a bibliometric analysis of scientific publications that deal with the issue of cybercrime in the context of digitalization of the financial sector of the Ukrainian economy. The research was carried out based on publications indexed in the Scopus database during the last 7 years. The plotting of the dynamics of publications in the Scopus database showed an increase in the general trend of publication activity in the field of cybercrime and the field of its interaction with the financial sector. The histogram of the geography of studies dealing with the issue of cybercrime in the context of digitalization of the financial sector of the economy showed the highest activity in solving the problem by scientists from the USA and Great Britain, which are leaders in the field of e-commerce. The bibliometric map of keywords of publications, built using the VOSviewer software, allowed identifying 7 clusters representing the areas of research: computer security, analysis of large databases, digital data storage, artificial intelligence, e-commerce, digital and mobile forensics. The results obtained can be used to identify the most potential areas for the spread of cybercrimes in the digital financial space, as well as tools to counter these types of crimes in Ukraine",'Khmelnytskyi National University',Бібліометричний аналіз досліджень кіберзлочинності в умовах цифровізації фінансового сектору економіки держави,10.31891/2307-5740-2019-276-6(2)-253-259.,,,core
162588046,2019,"Background: Three-dimensional camera systems that integrate depth assessment with traditional two-dimensional images, such as the Microsoft Kinect, Intel Realsense, StereoLabs Zed and Orbecc, hold great promise as physical function assessment tools. When combined with point cloud and skeleton pose tracking software they can be used to assess many different aspects of physical function and anatomy. These assessments have received great interest over the past decade, and will likely receive further study as the integration of depth sensing and augmented reality smartphone cameras occurs more in everyday life. Research Question: The aim of this review is to discuss how these devices work, what options are available, the best methods for performing assessments and how they can be used in the future. Methods: Firstly, a review of the Microsoft Kinect devices and associated artificial intelligence, automated skeleton tracking algorithms is provided. This includes a narrative critique of the validity and clinical utility of these devices for assessing different aspects of physical function including spatiotemporal, kinematic and inverse dynamics data derived from gait and balance trials, and anatomical assessments performed using the depth sensor information. Methods for improving the accuracy of data are examined, including multiple-camera systems and sensor fusion with inertial monitoring units, model fitting, and marker tracking. Secondly, alternative hardware, including other structured light and time of flight methods, stereoscopic cameras and augmented reality leveraging smartphone and tablet cameras to perform measurements in three-dimensional space are summarised. Software options related to depth sensing cameras are then discussed, focussing on recent advances such as OpenPose and web-based methods such as PoseNet. Results and Significance: The clinical and non-laboratory utility of these devices holds great promise for physical function assessment, and recent developments could strengthen their ability to provide important and impactful health-related data",Elsevier BV,"Three-dimensional cameras and skeleton pose tracking for physical function assessment: A review of uses, validity, current developments and Kinect alternatives",10.1016/j.gaitpost.2018.11.029,,,core
227726119,"May 21, 2019","With the advancement of space hardware technologies such as smaller spacecraft, component and instrument miniaturization and high performance space processors, and with the advancement of software technologies in artificial intelligence, big data analysis and autonomous decision making, Earth Science is looking at novel ways to observe phenomena that previously could not have been studied or would have been too expensive to study with traditional missions. In particular, the New Observing Strategies (NOS) component of the NASA Earth Science Technology Office (ESTO) Advanced Information Systems Technology (AIST) Program aims at leveraging these novel technologies as well as low cost and easy access to space to acquire multi-temporal or simultaneous multi-angular, multi-locations, multi-resolution and multi-spectral observations that will provide better multi-source measurements and will build a more dynamic and comprehensive picture of Earth Science phenomena that need to be studied and analyzed. For applications such as water resources management, air quality monitoring, biodiversity studies or disaster management, NOS will integrate the use of small instruments, small spacecraft, constellations of spacecraft and networks of sensors to design new missions that will provide the necessary measurements to improve future forecast and science modeling systems.Measurement acquisition will therefore be approached as a system of systems rather than on a mission basis, and a system of this complexity should not be expected to work without full integration and experimental characterization. Although most of the individual technologies enabling to link and coordinate multi-source observations are more or less mature, a few technologies need to be developed and all of them need to be integrated and tested as a system. In order for this validation to occur, the AIST Program is developing the NOS Testbed that includes 3 main goals:1.Validate novel NOS technologies, independently and as a system2.Demonstrate novel distributed operations concepts3.Socialize new Distributed Spacecraft Mission (DSM) and SensorWeb (SW) technologies and concepts to the science community by significantly retiring the risk of integrating these new technologies.The NOS Testbed will consist of multiple sensing nodes, simulated or actual, representing space, air and/or ground measurements, that are interconnected by a communications fabric (infrastructure that permits nodes to transmit and receive data between one another and interact with each other). Each node will be supported by hardware capabilities required to perform nodes monitoring and command & control, as well as intelligent ""onboard"" computing. The nodes will work together in a collaborative manner to demonstrate optimal science capabilities. The testbed will enable to validate technologies such as inter-node communication models, techniques and protocols; inter-node coordination; real-time data fusion and understanding; planning; sensor re-targeting; etc. Additionally, the testbed will have the capability to interact with various mission design tools, OSSEs and one or several forecast models. More details about the NOS Testbed will be presented at the confererence",,Earth Science Technology Office (ESTO) New Observing Strategies (NOS) and NOS-Testbed (NOS-T),,https://core.ac.uk/download/pdf/227726119.pdf,,core
231905919,2019-01-01T00:00:00,"The scope of this PhD is to propose, develop and assess several upgrades to existing shape optimization methods based on Evolutionary Algorithms (EAs). The efficiency of the proposed improvements is demonstrated in a number of real-world applications in the field of fluid mechanics (aerodynamic, hydrodynamics and turbomachinery) which are associated with computationally expensive evaluation software. They noticeably reduce the computational cost of optimization compared to the available (background) methods, which are still based on EAs enhanced by metamodels (Metamodel-Assisted EAs or MAEAs) and distributed search. Metamodels, mainly Radial Basis Function networks, are online trained personalized surrogate evaluation models, meaning that a local metamodel is trained for the pre-evaluationof each new individual generated during the evolution. This is in contrast to the common use of offline trained metamodels widely used by other relevant methods. Parallelization, in the form of concurrent evaluations of the candidate solutions on the multiprocessor platform of the Parallel CFD & Optimization Unit (PCOpt) of the Lab of Thermal Turbomachines of the NTUA is an indispensable feature of the proposed method variants. All developments have been made in the generic optimization platform EASY (Evolutionary Algorithm SYstem) developed by the PCOpt/NTUA. In all but one optimization problems, the problem-specificmodel to evaluate the candidate solutions is the GPU-enabled CFD solver PUMA developed by the same group. Only in the case of the optimization of the valveless diaphragm micropump, a different in-house CFD tool based on the cut-cell method is used instead. The most important contributions of this thesis are listed below:a) The use of Principal Component Analysis (PCA) to assist the EAs during the evolution. In this thesis, the Kernel PCA is used and is shown to provide better results compared to the Linear PCA used so far. In each generation of the EA, the PCA performs an eigendecomposion of the offspring population. The resulting eigenvectors define a new feature space, which the population members are transformed into; the evolution operators are applied in the feature space in which they perform optimally. Moreover, the PCA assists the MAEAs. Metamodels are only trained on the most important variables (directions in the feature space) indicated by the PCA, while the rest are safely truncated, as these generate noise at the predictions. The metamodels are trained with transformed by the PCApatterns, with truncated design variables, leading to reduced training cost and more dependable predictions. The two-fold usage of PCA drives the EA-based search in a much better way.b) A PCAbased Hybrid Algorithm aiming at maximum efficiency in Multi-Objective Optimization (MOO). This hybrid method combines the advantages of EA and Gradient-Based(GB) optimization. The EA explores the design space while the GB method regularly upgrades the most promising solutions. The required gradients of the objective functions with respect to the design variables are efficiently computed with the continuous adjoint method developed and programmed in the PCOpt/NTUA, at a cost which is independent of the number of design variables. In MOO, the direction along which the GB method updates the selected individuals is of outmost importance. Herein, the Linear PCA computes the principalcomponents of the objective space by processing the objective function values of individuals forming the current front of non-dominated solutions. The principal component (direction) corresponding to the minimum variance is perpendicular to the current front and points towards the direction of the simultaneous improvement of all objective functions, so this is used for the GB refinement. The proposed hybrid method performs better than the non-hybridized EA-based search.c) Multi-Criteria Decision Making (MCDM) within EAs to account for the Decision Maker’s (DM) preferences during the evolution. In contrast to standard multi-objective EAs which may insufficiently populate the preferred area(s) of the objective space, more non-dominatedsolutions are now driven towards them. This is achieved by using the MCDM Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), which affects the parent selection and the non-dominated front trimming operators.d) Flow prediction with Deep Neural Networks (DNNs) to assist the design/optimization of aerodynamic shapes. Trained on databases of CFD simulations, DNNs learn to predict the flow field around/inside these bodies, such as airfoils, wings and turbomachinery cascades. In this thesis, inputs and outputs are processed as images, in 2D cases, or raw data, in 3D cases. The DNNs are validated on new shapes and their ability to replicate the CFD results with high precision and low computational cost is demonstrated. The DNNs are employed as offline trained metamodels during the EA-based search, in contrast to the online trained metamodels used in the aforementioned MAEAs. The background and the aforementioned methods can work synergistically or separately to improve the performance of EA-basedoptimization methods as it is demonstrated in two groups of CFD applications. The first group consists of some ""standard"" CFD-based optimization problems, the so-called benchmarkcases. Each time a new variant is presented, these are revisited. By doing so, the reader should clearly assess the improvement offered by the proposed method. In a separate chapter of this thesis, a number of industrial cases are presented and optimized with the most efficient methods presented. These include the shape optimization of : (a) an Aircraft Wing-Body Configuration, (b) the DrivAer Car, (c) an Ultralight Aircraft, (d) a Francis Runner and (e) a Valveless Diaphragm Micropump.Σκοπός της Διδακτορικής Διατριβής είναι να προτείνει, προγραμματίσσει και αξιολογήσειαναβαθμίσεις σε υπάρχουσες μεθόδους βελτιστοποίησης αεροδυναμικών/υδροδυναμικών μορφών με βάση τους εξελικτικούς αλγορίθμους (EA). Η αποδοτικότητα των προτεινόμενων βελτιώσεων επιδεικνύεται σε πραγματικές εφαρμογές απο τον τομέα της υππολογιστικής ρευστοδυναμικής (ΥΡΔ) που σχετίζονται με υπολογιστικά ακριβό λογισμικό αξιολόγησης. Οι προτεινόμενες μέθοδοι μειώνουν αισθητά το υπολογιστικό κόστος βελτιστοποίησης σε σύγκριση με  προϋπάρχουσες μεθόδους, οι οποίες βασίζονται σε ΕΑ ενισχυμένους με χρήση μεταπροτύπων  (Metamodel-Assisted ΕΑ, ΜΑΕΑ) και κατανεμημένης αναζήτησης. Όλα τα μεταπρότυπα, κυρίως δίκτυα ακτινικής βάσης, είναι εξατομικευμένα και συνδεδεμένα με την εξέλιξη, που σημαίνει ότι, κατά τη διάρκεια της εξέλιξης, ένα μεταπρότυπο τοπικής εμβέλειας εκπαιδεύεται για κάθε νέο άτομο προς αξιολόγηση. Η χρήση αυτή των μεταπροτύπων έρχεται σε αντίθεση με τη χρήση μεταπροτύπων αποσυνδεδεμένων από την εξέλιξη, η οποία συνηθίζεται στην πλειονότητα των συναφών μεθόδων της βιβλιογραδίας. Δεδομένου του υψηλού υπολογιστικού κόστους ανά αξιολόγηση, η παραλληλοποίηση, με τη έννοια της ταυτόχρονης αξιολόγησης των υποψήφιων λύσεων στην πολυεπεξεργαστική συστοιχία υπολογιστών της  Μονάδας Παράλληλης Υπολογιστικής Ρευστοδυναμικής & Βελτιστοποίησης (ΜΠΥΡΒ) του Εργαστηρίου Θερμικών Στροβιλομηχανών του ΕΜΠ, είναι απολύτως απαραίτητη. Όλες οι προτεινόμενες μέθοδοι έχουν αναπτυχθεί στη γενικής φύσης πλατφόρμα βελτιστοποίησης EASY (Evolutionary Algorithm SYstem) που αναπτύχθηκε από την ΜΠΥΡΒ. Σε όλα τα προβλήματα βελτιστοποίησης, το λογισμικό αξιολόγησης για κάθε υποψήφια λύση είναι  ο βασιζόμενος σε κάρτες γραφικών επιλύτης ροών με τεχνικές ΥΡΔ PUMA, που έχει αναπτυχθεί από την ίδια ομάδα. Μόνο στην περίπτωση της βελτιστοποίησης διαφραγματικής μικρο-αντλίας χωρίς βαλβίδες, χρησιμοποιείται ένας διαφορετικός  επιλύτης ΥΡΔ που βασίζεται στη μέθοδο των τεμνόμενων κυψελών. Οι πιο σημαντικές συνεισφορές αυτής της εργασίας είναι οι ακόλουθες:α) Η χρήση της Ανάλυσης Κύριων Συνιστωσών (ΑΚΣ) για την υποβοήθηση των EA. Σε αυτήν την εργασία, χρησιμοποιείται η ΑΚΣ με συναρτήσεις πυρήνα και  δείχνεται ότι παρέχει καλύτερα αποτελέσματα σε σύγκριση με τη γραμμική ΑΚΣ, που χρησιμοποιείτο προηγουμένως. Στην αρχή κάθε γενιάς, η ΑΚΣ εκτελεί μια ιδιοανάλυση του πληθυσμού των απογόνων. Τα προκύπτοντα ιδιοδιανύσματα ορίζουν ένα νέο χώρο των χαρακτηριστικών, στον οποίο μετασχηματίζονται τα μέλη του πληθυσμού. Οι τελεστές εξέλιξης εφαρμόζονται στο χώρο των χαρακτηριστικών όπου εκεί λειτουργούν βέλτιστα. Τα μεταπρότυπα εκπαιδεύονται με τις πιο σημαντικές μεταβλητές ενώ οι υπόλοιπες αποκόπτονται με ασφάλεια, καθώς εισάγουν θόρυβο στις προβλέψεις. Η ΑΚΣ προσδιορίζει τις πιο σημαντικές κατευθύνσεις (μεταβλητές) στο χώρο των χαρακτηριστικών. Τα μεταπρότυπα εκπαιδεύονται με δεδομένα τα οποία μετασχηματίζονται στο χώρο των χαρακτηριστικών όπου αποκόπτονται οι λιγότερο σημαντικές μεταβλητές. Κατά συνέπεια, μειώνεται το κόστος εκπαίδευσης των μεταπροτύπων και οι προβλέψεις τους γίνονται πιο αξιόπιστες. Η διπλή χρήση της ΑΚΣ οδηγεί σε καλύτερη απόδοση του EA.β) Ένας Υβριδικός Αλγόριθμος βασισμένος στην ΑΚΣ με στόχο τη μέγιστη απόδοση σε προβλήματα πολυκριτηριακής βελτιστοποίησης. Αυτή η υβριδική μέθοδος συνδυάζει τα πλεονεκτήματα του EA και εκείνα της μεθόδου βελτιστοποίησης με παραγώγους (ΒμΠ).Ο ΕΑ διερευνά το χώρο σχεδιασμού, ενώ η μέθοδος ΒμΠ ανανεώνει τακτικά τις πιο υποσχόμενες λύσεις. Οι απαιτούμενες κλίσεις των αντικειμενικών συναρτήσεων ως προς τις μεταβλητές σχεδιασμού υπολογίζονται αποτελεσματικά με τη συνεχή συζυγή μέθοδο που αναπτύχθηκε και προγραμματίστηκε στη ΜΠΥΡΒ, το κόστος της οποίας είναι ανεξάρτητο από τον αριθμό των μεταβλητών σχεδιασμού. Στη πολυκριτηριακή βελτιστοποίηση, η κατεύθυνση κατά την οποία η μέθοδος ΒμΠ ανανεώνει τα επιλεγμένα άτομα είναι εξαιρετικής σημασίας. Η γραμμική ΑΚΣ υπολογίζει τις κύριες συνιστώσες του χώρου των αντικειμενικών συναρτήσεων χρησιμοποιώντας τις τιμές των αντικειμενικών συναρτήσεων των ατόμων του τρέχοντος μέτωπου των μη-κυριαρχούμενων λύσεων. Η κύρια συνιστώσα (κατεύθυνση) που αντιστοιχεί στην ελάχιστη διακύμανση είναι κάθετη στο τρέχον μέτωπο και δείχνει προς την κατεύθυνση η οποία ταυτόχρονα βελτιώνει όλες τις αντικειμενικές συναρτήσεις, για αυτό και χρησιμοποιείται για την ανανέωση με ΒμΠ. Η προτεινόμενη υβριδική μέθοδος αποδίδει καλύτερα από τους μη-υβριδικούς ΕΑ.γ) Πολυκριτηριακή Λήψη Αποφάσεων (ΠΛΑ) εντός των EA για να ληφθούν υπόψην οι προτιμήσεις του Λήπτη Αποφάσεων κατά την εξέλιξη. Σε αντίθεση με τους EA πολλών στόχων που ενδέχεται να μην καλύπτουν επαρκώς τις προτιμώμενες περιοχες του χώρου των αντικειμενικών, η χρήση τεχνικών ΠΛΑ οδηγεί περισσότερες μη-κυριαρχούμενες λύσεις προς τις περιοχές αυτές. Αυτό, στη Διδακροτική αυτή Διατριβή, επιτυγχάνεται με τη χρήση της μεθόδου ΠΛΑ Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), η οποία επηρεάζει την επιλογή γονέων και την αποκοπή των μη-κυριαρχούμενων λύσεων από το τρέχον μέτωπο.δ) Προβλέψη ροής με Βαθιά Νευρωνικά Δίκτυα (ΒΝΔ) για την υποβοήθηση στο σχεδιασμό/βελτιστοποίηση αεροδυναμικών σχημάτων. Αφού έχουν εκπαιδευτεί  σε βάσεις δεδομένων προσομοιώσεων οι οποίες πραγματοποιήθηκαν με την χρήση λογισμικού ΥΡΔ, τα ΒΝΔ μαθαίνουν να προβλέπουν το πεδίο ροής γύρω/μέσα στα αεροδυναμικά σώματα, όπως αεροσκάφη, πτερύγια και θερμικές στροβιλομηχανές. Στην Διδακτορική αυτή  Διατριβή, η επεξεργασία των δεδομένων εισόδου και εξόδου του ΒΝΔ γίνεται σε μορφή εικόνων, σε 2D περιπτώσεις, και σε μορφή ανεπεξέργαστων δεδομένων, σε 3D περιπτώσεις. Τα ΒΝΔ δοκιμάζονται σε νέα αεροδυναμικά σχήματα και αποδεικνύεται η ικανότητά τους να αναπαράγουν τα αποτελέσματα  κωδίκσν ΥΡΔ με υψηλή ακρίβεια και χαμηλό υπολογιστικό κόστος (μη συμπεριλαμβανομένου του κόστους εκπαίδευσης). Τα ΒΝΔ χρησιμοποιούνται ως αποσυνδεδεμένα από την εξέλιξη μεταπρότυπα, σε αντίθεση με  τα συνδεδεμένα με την εξέλιξη μεαταπρότυπα που παρουσιάστηκαν προηγουμένως.Οι προαναφερθείσες μέθοδοι μπορούν να λειτουργούν συνεργατικά ή ξεχωριστά για τη βελτίωση της απόδοσης των μεθόδων βελτιστοποίησης που βασίζονται σε EA, όπως αποδεικνύεται σε δύο ομάδες εφαρμογών. Η πρώτη ομάδα αποτελείται από κάποια ""τυπικά"" προβλήματα αεροδυναμικής βελτιστοποίησης, που ονομάζονται προβλήματα αναφοράς. Κάθε φορά που παρουσιάζεται μια νέα προτεινόμενη μέθοδος, τα προβλήματα αυτά επανεξετάζονται. Με αυτόν τον τρόπο, ο αναγνώστης θα μπορεί να αξιολογεί με σαφήνεια και συγκριτικά τα πλεονεκτήματα κάθε προτεινόμενης παραλλαγής. Σε ξεχωριστό κεφάλαιο της Διδακτορικής Διατριβής, παρουσιάζονται και βελτιστοποιούνται ορισμένα βιομηχανικά προβλήματα  με τις πιο αποτελεσματικές από τις προτεινόμενες μεθόδους. Στα προβλήματα αυτά περιλαμβάνουν τη βελτιστοποίηση του σχήματος: (α) μιας διαμόρφωσης πτέρυγας-ατράκτου αεροσκάφους, (β) του αυτοκινήτου DrivAer, (γ) ενός ελαφρού ανεμόπτερου, (δ) ενός δρομέα υδροστροβίλου Francis και (ε) μιας διαφραγματικής μικρο-αντλίας χωρίς βαλβίδες",'National Documentation Centre (EKT)',Εξελικτικοί αλγόριθμοι χαμηλού κόστους υποβοηθούμενοι από μεταπρότυπα και εφαρμογές τους στη βελτιστοποίηση  μορφής στη ρευστοδυναμική,10.12681/eadd/46420,,,core
275572318,2019-01-01T00:00:00,"The rapid digitization of healthcare has led to a proliferation of clinical data, manifesting through electronic health records, biorepositories, and disease registries. This dissertation addresses the question of how machine learning (ML) techniques can capitalize on these data resources to assist clinicians in predicting, preventing and treating illness. To this end, we develop a set of MLbased, data-driven models of patient outcomes that we envision to be embedded within systems of decision support deployed at different stages of patient care.We focus on two broad setups for analyzing clinical data: (1) the cross-sectional setup wherein data is collected by observing many patients at a particular point of time, and (2) the longitudinal setup in which repeated observations of the same patient are collected over time. In both setups, we develop models that are: (a) capable of answering counter-factual questions, i.e., can predict outcomes under alternative treatment scenarios, (b) interpretable in the sense that clinicians can understand how the model predictions for individual patients are issued, and (c) automated in the sense that they adaptively tune their modeling choices for the dataset at hand, with little or no need for expert intervention. Models satisfying these three requirements would enable the realization of actionable, transparent and automated decision support systems that operate symbiotically within existing clinical workflows.Our technical contributions are multi-faceted. In the cross-sectional data setup, we develop ML models that fulfill the aforementioned requirements (a)-(c) as follows. We start by developing a comprehensive theoretical framework for causal inference, whereby we quantify the limits to how well ML models can recover the causal effects of counter-factual treatment decisions on individual patients using observational (retrospective) data, and we build ML models — based on Gaussian processes — that achieve these limits. Next, we develop a novel symbolic meta-modeling approach for interpreting the predictions of any ML-based prognostic model by converting the “black-box” model into an understandable symbolic equation that relates patients’ features to their predicted outcomes. Finally, we develop a model selection approach based on Bayesian optimization that enables the automation of predictive and causal modeling. In the longitudinal data setup, we develop a novel deep probabilistic model for sequential clinical data that satisfies requirements (a)- (c) by capitalizing on the strengths of both state-space models and deep recurrent neural networks.To demonstrate the utility of our models, we evaluate their performance on various real-world datasets for cohorts of breast cancer, cardiovascular disease and cystic fibrosis patients. We show that, compared to existing clinical scorers, our ML-based models can improve the accuracy of predicting individual-level prognoses, guide treatment decisions for individual patients, and provide insights into underlying disease mechanisms","eScholarship, University of California",Discovering Data-Driven Actionable Intelligence for Clinical Decision Support,,,,core
211033610,2019-01-01T00:00:00,"Multi-criteria decision making under uncertainty is a common practice followed in industries and academia. Among several types of uncertainty handling techniques, Chance Constrained Programming (CCP) is considered as an efficient and tractable approach provided one has accessibility to distribution of the data for uncertain parameters. However, the assumption that the uncertain parameters must follow some well-behaved probability distribution is a myth for most of the practical applications. This paper proposes a methodology to amalgamate machine learning algorithms with CCP and thereby make it data-driven. A novel fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enable intelligent Sobol based sampling in these regions for use in CCP. The Fuzzy clustering mechanism used in the proposed method transforms the existing fuzzy C-means technique such that the decision variables are significantly reduced. This enables evolutionary optimizers to obtain better approximations of the uncertain space by identifying the true clusters. A highly nonlinear real life model for continuous casting from steelmaking industries is considered as a case study for testing the efficiency of data based CCP along with a comprehensive comparison between conventional CCP approach using box uncertainty set and proposed methodology. As the resulting CCP problem is multi-objective in nature, the Pareto solutions are obtained by NSGA II",'Institute of Electrical and Electronics Engineers (IEEE)',A Chance Constrained Programming Based Multi-Criteria Decision Making under Uncertainty,10.1109/INDIANCC.2019.8715586,,,core
199175443,2019-04-05T00:00:00,"Positron emission tomography (PET) provides functional images useful to track metabolic processes in the body and enables the diagnosis of several diseases. The technique is based on the use of radiotracers that emit positrons whose annihilation with electrons in the human body produces photons which travel away in almost anti-parallel directions. A ring of detectors is used to detect them and an event is counted when two detectors are activated within a time window (coincidence window). Each couple of detectors defines a line of response (LOR) to which events are associated. After the scan, a reconstruction algorithm transforms the acquired data into a map of activity in the patient’s body. Photons do not travel in vacuum but in human body, thus a correction for their attenuation is required.
PET images are characterized by limited spatial resolution. In order to get morphological details to combine to functional ones, PET-CT (PET and computed tomography) and PET-MR (PET and magnetic resonance) systems have been developed. Linear attenuation coefficient maps are obtainable directly from the CT scan in the case of PET-CT by means of an accurate energy rescaling to 511 keV.
Unfortunately, there is no straightforward technique to be used in PET-MR to derive the attenuation properties of tissues from MR signals. Plenty of techniques have been developed to address such kind of problem and in this work we explore an original approach based on deep neural networks. These could provide a boost in the direction of a data-driven algorithm for attenuation correction by using structural, T 1 weighted, MR images transformed into pseudo-CTs, i.e. images whose intensity values are similar to the ones expected in a CT image.
Already implemented deep learning techniques to this purpose require paired data.
Unfortunately, it is quite hard to obtain a big dataset of paired medical images, i.e. MR and CT images belonging to the same patient. To overcome this limitation, we chose to develop an approach based on a Generative Adversarial Network (GAN) trained on unpaired data.
A GAN is a deep learning architecture composed by two neural networks, a generator and a discriminator, fighting against each other: the generator tries to map the input to the desired output and the discriminator tells if the generated output is good or not. In the training phase, the generator has to maximize the similarity to the desired output and the score provided by discriminator; the discriminator instead has to distinguish between the fakes, produced by the generator, and the original data. After the training phase, the generator is capable of mapping any point in the input space (MR images) to a point in the output space (pseudo-CT images). The generation of pseudo-CTs from MRs with an unpaired training set in the case here proposed has been approached by using a CycleGAN (with some ad-hoc developed modifications), characterized by the presence of four networks: two generators, the transformations from MR to CT domain (MR2CT) and vice versa (CT2MR) and two discriminators (fake CT vs. real CT, fake MR vs. real MR). A cyclic consistency constraint imposes that the whole cycle is the identity operator: MR ≈ MR2CT(CT2MR(MR)). This requirement, introduced in the loss function, guides the network training to generate not just an image but an image of the specific input patient.
We collected a dataset of structural MR brain images coming from the Autism Brain Imaging Data Exchange (ABIDE: http://fcon_1000.projects.nitrc.org/indi/abide/) project and CT scans provided by the NeuroAnatomy and image Processing LABoratory (NAPLAB) of the IRCCS SDN (Naples, IT). We used these unpaired examples to train a CycleGAN-like network.
Prior implementations of deep learning models for the generation of medical images require working on single slices of the acquired images, due to the availability of algorithms developed for 2D natural images and limitations in computing power. The proposed approach has been developed to work directly on 3D data. A registration step that aligns all images to approximately the same orientation has proved to be necessary due to the low number of training examples. In fact, no paired data is required, but in any case retrieving a brain CT from a MR is a major issue which needs a simplification at first stage.
Structural similarity index computed between the generated output and the expected one shows satisfactory results. Despite a validation on a more populated dataset is needed to release the current requirements on the initial image alignment, the proposed approach opens to the perspective of using data driven methods to several processing pipelines on medical images, including data augmentation, segmentation and classification. Further investigation on the behaviour of the network in case of abnormalities in the images is required.
An advantage of this technique with respect to other currently available procedures for attenuation correction in PET-MR is that it does not need any extra MR acquisition: only the standard diagnostic T 1 -weighted image is used and, due to the low computational cost, images are translated from the MR to the CT domain in a couple of seconds. Building a large collection of publicly available images could undoubtedly lead to avoiding some preprocessing steps and to achieve better overall results",'Pisa University Press',A generative adversarial network approach for the attenuation correction in PET-MR hybrid imaging,,,,core
232211197,2019-02-01T00:00:00,"Genetic algorithms (GAs) are widely used in machine learning and optimization. This paper proposes a time-dependent genetic algorithm (TDGA) based on real-coded genetic algorithm (RCGA) to improve the convergence performance of functions over time such as a foot trajectory. TDGA has several distinguishing features when compared with traditional RCGA. First, individuals are arranged over time, and then the individuals are optimized in sequence. Second, search spaces of design variables are newly comprised of processes of reductions for search spaces. Third, the search space for crossover operations is expanded to avoid local minima traps that can occur in new search spaces up to the previous search space before performing any reduction of search space, and boundary mutation operation is performed to the new search spaces. Computer simulations are implemented to verify the convergence performance of the robot locomotion optimized by TDGA. Then, TDGA optimizes the desired feet trajectories of quadruped robots that climb up a slope and the impedance parameters of admittance control so that quadruped robots can trot stably over irregular terrains. Simulation results clearly represent that the convergence performance is improved by TDGA, which also shows that TDGA could be broadly used in robot locomotion research. (C) 2018 Elsevier B.V. All rights reserved",'Elsevier BV',Time-dependent genetic algorithm and its application to quadruped&rsquo;s locomotion,10.1016/j.robot.2018.10.015,,"[{'title': 'Robotics and Autonomous Systems', 'identifiers': ['1872-793x', 'issn:1872-793X', 'issn:0921-8890', '0921-8890']}]",core
275657672,2019,"Current demand for sustainment of critical aerospace assets requires management of complex systems and a product life cycle solution. This is a major concern both in civil and defence sectors in Australia. The emergence of new technologies including additive manufacturing (AM), virtual prototyping, simulated/augmented reality (AR) and artificial intelligence (AI) provide a unique opportunity for Innovative Sustainment. This paper discusses the importance of a network/system approach to sustainment and the role that emerging technologies including AM and AI can play in overcoming current challenges. In addition, the paper discusses the strategic research opportunities that can be harnessed by all stakeholders using a collaborative framework. The proposed framework can result in reduced cost of ownership, reduced logistics footprint, and enhanced resilience and flexibility by in-depth analyses of the challenges ahead. Aerospace composite material systems are used to exemplify the implementation pathway of this framework","Engineers Australia, Royal Aeronautical Society (Melbourne, Australia)","Virtual Design, Optimisation and Testing (VDOT) framework for innovative sustainment",,,,core
200801634,2019-06-13T00:00:00,"Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue",,"Autonomous Airline Revenue Management: A Deep Reinforcement Learning
  Approach to Seat Inventory Control and Overbooking",,http://arxiv.org/abs/1902.06824,,core
228151143,2019-01-01T00:00:00,"Visual enhancement is concerned with problems to improve the visual quality and viewing experience for images and videos. Researchers have been actively working on this area due to its theoretical and practical interest. However, obtaining high visual quality often comes with a cost of computational efficiency. With the growth of mobile applications and cloud services, it is crucial to develop effective and efficient algorithms for generating visually attractive images and videos. In this thesis, we address the visual enhancement problems in three aspects, including the spatial, temporal, and the joint spatial-temporal domains. We propose efficient algorithms based on deep convolutional neural networks for solving various visual enhancement problems.First, we address the problem of spatial enhancement for single-image super-resolution. We propose a deep Laplacian Pyramid Network to reconstruct a high-resolution image from an input low-resolution input in a coarse-to-fine manner. Our model directly extracts features from input LR images and progressively reconstructs the sub-band residuals. We train the proposed model with a multi-scale training, deep supervision, and robust loss functions to achieve state-of-the-art performance. Furthermore, we exploit the recursive learning technique to share parameters across and within pyramid levels to significantly reduce the model parameters. As most of the operations are performed on a low-resolution space, our model requires less memory and runs faster than state-of-the-art methods.Second, we address the temporal enhancement problem by learning the temporal consistency in videos. Given an input video and a per-frame processed video (processed by an existing image-based algorithm), we learn a recurrent network to reduce the temporal flickering and generate a temporally consistent video. We train the proposed network by minimizing both short-term and long-term temporal losses as well as a perceptual loss to strike a balance between temporal coherence and perceptual similarity with the processed frames. At test time, our model does not require computing optical flow and thus runs at 400+ FPS on GPU for high-resolution videos. Our model is task independent, where a single model can handle multiple and unseen tasks, including but not limited to artistic style transfer, enhancement, colorization, image-to-image translation and intrinsic image decomposition.Third, we address the spatial-temporal enhancement problem for video stitching. Inspired by the pushbroom cameras, we cast the stitching as a spatial interpolation problem. We propose a pushbroom stitching network to learn dense flow fields to smoothly align the input videos. The stitched videos can be generated from an efficient pushbroom interpolation layer. Our approach generates more temporally stable and visually pleasing results than existing video stitching approaches and commercial software. Furthermore, our algorithm has immediate applications in many areas such as virtual reality, immersive telepresence, autonomous driving, and video surveillance","eScholarship, University of California",Learning Spatial and Temporal Visual Enhancement,,https://core.ac.uk/download/228151143.pdf,,core
211242274,2019-05-01T00:00:00,"Over the last decade, many interesting route planning problems can be solved by finding the shortest path in a weighted graph that represents a transportation network. Such networks are private transport networks or timetabled public transportation networks. In the shortest path problem, every network type requires different algorithms to compute one or more than one shortest path. However, routing in a public transportation network is completely different and is much more complex than routing in a private transport network, and therefore different algorithms are required.

 

For large networks, the standard shortest path algorithms - Dijkstra's algorithm (1959) and Bellman's algorithm (1958)- are too slow. Consequently, faster algorithms have been designed to speed up the search. However, these algorithms often consider only the simplest scenario of finding an optimal route on a graph with static real edge costs. But real map routing problems are often not that simple – it is often necessary to consider time-dependent edge costs. For example, in public transportation routing, consideration of the time-dependent model of these networks is mandatory.

 

However, there are a number of transportation applications that use informed search algorithms (where the algorithm uses heuristics that guide the search toward the destination), rather than one of the standard static shortest path algorithms. This is primarily due to shortest paths needing to be rapidly identified either because an immediate response is required. For example, the A* algorithm (Nilsson, 1971) is widely used in artificial intelligence. Heuristic information (in the form of estimated distance to the destination) is used to focus the search towards the destination node. This results in finding the shortest path faster than the standard static search algorithms.

 

Road traffic congestion has become an increasingly significant problem in a modern society. In a dynamic traffic environment, traffic conditions are time-dependent. For instance, when travelling from home to the work, although an optimal route can be planned prior to departure based on the traffic conditions at that time, it may be necessary to adjust the route while en route because traffic conditions change all the time. In some cases, it is necessary to modify the travelling route from time to time and re-plan a new route from the current location to the destination, based on the real-time traffic information. The challenge lies in the fact that any modification to the optimal route to adapt to the dynamic environment necessitates speeding up of the search efforts. Among the algorithms suggested for the dynamic shortest path problem is the algorithm of Lifelong Planning A* algorithm (LPA*) (Koenig, Likhachev and Furcy, 2004). This algorithm has been given this name because of its ability to reuse information from previous searches. It is used to adjust a shortest path to adapt to the dynamic transportation network.

 

Search space and fast shortest path queries can be used for finding fastest updated route on road and bus networks. Consequently, the efficient processing of both types of queries is of first-rate significance. However, most search methods focus only on one type of query and do not efficiently support the other. To address this challenge, this research presents the first novel approach; an Optimised Lifelong Planning A* (OLPA*) algorithm. The OLPA* used an appropriate data structure to improve the efficiency of the dynamic algorithms implementation making it capable of improving the search performance of the algorithm to solve the dynamic shortest path problem, which is where the traveller may have to re-compute the shortest path while travelling in a dynamic transportation environment.

 

This research has also proposed bi-directional LPA* (BLPA*) algorithm. The proposed algorithm BLPA* used bi-directional search strategy and the main idea in this strategy is to divide the search problem into two separate problems. One search proceeds forwards from the start node, while the other search proceeds backwards from the end node. The solution requires the two search problems to meet at one middle node. The BLPA* algorithm has the same overall structure as the LPA* algorithm search, with some differences that the BLPA* contains a priority queue for each direction.

 

This research presented another algorithm that designed to adaptively derive the shortest path to the desired destination by making use of previous search results and reducing the total execution time by using the benefits of a bi-directional search strategy . This novel algorithm has been called the bi-directional optimised Lifelong A* algorithm (BiOLPA*). It was originally proposed for road transport networks and later also applied to public transportation networks. For the road transport network, the experimental results demonstrate that the proposed incremental search approach considerably outperforms the original approach method, which recomputed the shortest path from scratch each time without utilization of the previous search results. However, for public transportation, the significant problem is that it is not possible to apply a bi-directional search backwards using estimated arrival time. This has been further investigated and a better understanding of why this technique fails has been documented. While the OLPA* algorithms give an impressive result when applied on bus network compared with original A* algorithms, and our experimental results demonstrate that the BiOLPA* algorithm on road network is significantly faster than the LPA*, OLPA* and the A* algorithms, not only in terms of number of expansion nodes but also in terms of computation time",,Shortest path algorithms for dynamic transportation networks,,https://core.ac.uk/download/211242274.pdf,,core
224733267,2019-02-26T10:50:08,"Immersive techniques such as augmented reality through devices such as the AR-Sandbox and deep learning through convolutional neural networks (CNN) provide an environment that is potentially applicable for motor rehabilitation and early education. However, given the orientation towards the creation of topographic models and the form of representation of the AR-Sandbox, the classification of images is complicated by the amount of noise that is generated in each capture. For this reason, this research has the purpose of establishing a model of a CNN for the classification of geometric figures by optimizing hyperparameters using Random Search, evaluating the impact of the implementation of a previous phase of color-space segmentation to a set of tests captured from the AR-Sandbox, and evaluating this type of segmentation using similarity indexes such as Jaccard and Sorensen-Dice. The aim of the proposed scheme is to improve the identification and extraction of characteristics of the geometric figures. Using the proposed method, an average decrease of 39.45% to a function of loss and an increase of 14.83% on average in the percentage of correct answers is presented, concluding that the selected CNN model increased its performance by applying color-space segmentation in a phase that was prior to the prediction, given the nature of multiple pigmentation of the AR-Sandbox",'MDPI AG',Hyperparameter Optimization for Image Recognition over an AR-Sandbox Based on Convolutional Neural Networks Applying a Previous Phase of Segmentation by Color-Space,10.3390/sym10120743,,"[{'title': 'Symmetry', 'identifiers': ['2073-8994', 'issn:2073-8994']}]",core
275765257,2019-01-01T00:00:00,"In this paper, we present a deep neural network based real-time integrated framework to detect objects, lane markings, and drivable space using a monocular camera for advanced driver assistance systems. The object detection framework detects and tracks objects on the road such as cars, trucks, pedestrians, bicycles, motorcycles, and traffic signs. The lane detection framework identifies the different lane markings on the road and also distinguishes between the ego lane and adjacent lane boundaries. The free space detection framework estimates the drivable space in front of the vehicle. In our integrated framework, we propose a pipeline combining the three deep neural networks into a single framework, for object detection, lane detection, and free space detection simultaneously. The integrated framework is implemented in C++ and runs real-time on the Nvidia's Drive PX 2 platform",'Institute of Electrical and Electronics Engineers (IEEE)',"An integrated framework for autonomous driving:object detection, lane detection, and free space detection",,,,core
286618536,2019-01-01T00:00:00,"1.	Abu-Taieh C., Evon J.: Technology Engineering and Management in Aviation: Advancements and Discoveries. Information Science Reference, 2011.
2.	Ajam M, Woolard C, Wiljoen CL. Biomass pyrolysis oil as a renewable feedstock for bio-jet fuel. In: Proceedings of the 13th international conference on stability, handling and use of liquid fuels (IASH2013), Rhodes, Greece; October 2013. p. 6–10.
3.	Аnnual report to Parliament on the renewable transport fuel obligation. Renewable Fuels Agency. The Stationery Office, 2011.
4.	Agarwal S., Chhibber V. K., Bhatnagar A. K.:Tribological behavior of diesel fuels and the effect of anti-wear additives. Fuel. Vol. 106, 2013, p. 21–29, 
5.	Alves S. M., Barros B.S., Trajano M.F.: Tribological behavior of vegetable oil-based lubricants with nanoparticles of oxides in boundary lubrication conditions. Tribology International. Vol. 65, 2013, p. 28–36.
6.	Asgari H., Chen X., Sainudiin R.: Modelling and simulation of gas turbines. International Journalof Modelling, Identification and Control, Vol.25, No.3, 2013, p. 1–15.
7.	Bartis James T. LaTourrette T., Dixon L.: Oil Shale Development in the United States: Prospects and Policy Issues. Santa Monica, Calif.: RAND Corporation, MG-414-NETL, 2005.
8.	Bassam N. El.: Handbook of Bioenergy Crops: A Complete Reference to Species. Development and Applications Earthscan, 2010.
9.	Bazazzadeh M., Badihi H., Shahriari A.: Gas Turbine Engine Control Design Using Fuzzy Logic and Neural Networks. International Journal of Aerospace Engineering. Vol. 1, 2011, p. 1–13. 
10.	Blakey S, Rye L, Wilson C.W.: Aviation gas turbine alternative fuels: A review.  P Combust Inst, No. 33, 2011, p. 2863–2885.
11.	Boichenko S., Iakovlieva A., Vovk O.: Traditional and alternative jet fuels: problems of quality standardization. Journal of Petroleum & Environmental Biotechnology. Vol. 4. Iss. 3, 2013.
12.	Boichenko S., Shkilniuk I., Turchak V.. The problems of biopollution with jet fuels and the way of achieving solution. Transport. 23, 2008; p. 253–257.
13.	Boichenko S., Yakovleva A. Prospects of biofuels introduction into aviation. Transport engineering and management: Proceedings of the 15-th conference for Lithuania Junior researchers. Science – future of Lithuania, 4 May 2012. Vilnius: Technika. p. 90–94.
14.	Boichenko S., Yakovlieva A., Gryshchenko O., Zinchuk A. Prospects of using different generations biofuels for minimizing impact of modern aviation on environment, Энерготехнологии и ресурсосбережение, № 1, 2018, p. 10–20.
15.	Boichenko S., Lejda K., Yakovlieva A., Vovk O. Comparative characteristics of low-temperature properties of jet fuels modified with bio-additives, International Automotive Conference (KONMOT2018). IOP Conf. Series: Materials Science and Engineering 421, 2018.
16.	Breil C., Meullemiestre A., Vian M., Chemat F.: Bio-Based Solvents for Green Extraction of Lipids from Oleaginous Yeast Biomass for Sustainable Aviation Biofuel. Molecules. Iss. 21(196), 2016, p. 1–14.
17.	Carels N., Sujatha M., Bahadur B.: Jatropha, Challenges for a New Energy Crop. Vol. 1: Farming, Economics and Biofuel. Springer Science & Business Media, 2012.
18.	Cavani F., Albonetti S., Basile F., Gandini A.: Chemicals and Fuels from Bio-Based Building Blocks. John Wiley & Sons, 2015.
19.	Cermak S. C., Evangelista R. L., Kenar J. A.: Distillation of Natural Fatty Acids and Their Chemical Derivatives, Distillation - Advances from Modeling to Applications, Dr. Sina Zereshki (Ed.), InTech, 2012. – р. 5. – 140. 
20.	Chai M. Thermal Decomposition of Methyl Esters in Biodiesel Fuel: Kinetics, Mechanisms and Products, Ph.D. Thesis, University оf Cincinnati, 2012.
21.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 1: emulsion production. Biomass Bioenergy, No. 25, 2003, p. 85–99.
22.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 2: tests in diesel engines. Biomass Bioenergy, No. 25, 2003, p. 101–11.
23.	Chuck C.J., Donnelly J.: The compatibility of potential bioderived fuels with Jet A-1 aviation kerosene. Applied Energy. Vol. 118, 2014, p. 83–91.
24.	Cleveland C.J., Morris C. G.: Handbook of energy. Volume II: Cronologies, top ten lists, and words clouds. Elsvier Inc., 2014.
25.	Cushion E., Whiteman A., Dieterle G.: Bioenergy Development: Issues and Impacts for Poverty and Natural Resource Management. World Bank Publications, 2010.
26.	Daggett D. L., Hendricks R.C., Walther R., Corporan E.: Alternative fuels for use in commercial aircrafts. The Boeing Company, 2007.
27.	Dahlquist E.: Biomass as Energy Source. Resources, Systems and Applications. CRC Press, 2013.
28.	Delmon B., Grange P., Froment G.F.: Hydrotreatment and Hydrocracking of Oil Fractions. Elsevier, 1999.
29.	Doc 9889 Airport Air Quality Manual. International Civil Aviation Organization, 2011. 
30.	Doc 9977. Manual on Civil Aviation Jet Fuel Supply, 2012.
31.	Edwards T.: Advancements in Gas Turbine Fuels from 1943 to 2005. J Eng Gas Power, No. 129, 2007, p. 13–20.
32.	Firrisa M. T., Van Duren I., Voinov A.: Energy efficiency for rapeseed biodiesel production in different farming systems. Energy Efficiency, 2013.
33.	Garcia-Anton J., Monzo J., Guninon J.L.: Study of corrosion on copper strips by petroleum naphtha in the ASTM D-130 test by means of electronic microscopy (SEM) and energy dispersive X-ray (EDX). Fresenius Journal of Analytical Chemistry. Iss. 337, 1990, p. 382–388.
34.	Garcia Santander C.M., Gymez Rueda S.M., de Lima da Silva N.: Measurements of normal boiling points of fatty acid esters and triacylglycerols by thermogravimetric analysis, Fuel, Iss. 92, 2012, p. 158–161.
35.	Geller D. P., Goodrum J.: W. Effects of speciﬁc fatty acid methyl esters on diesel fuel lubricity, Fuel, Vol. 83, 2004, p. 2351–2356.
36.	Gupta, K. K, Rehman A, Sarviya R. M.: Bio-fuels for the gas turbine: A review. Renew. Sust. Energ. Rev. No. 14, 2010, p. 2946–2955.
37.	Harvey B. G, Merriman W.W., Koontz T.A.: High-Density Renewable Diesel and Jet Fuels Prepared from Multicyclic Sesquiterpanes and a 1‑Hexene-Derived Synthetic Paraffinic Kerosene, Energy Fuels, 2013.
38.	Hemighaus G., Boval T., Bosley C.: Alternative Jet Fuels. Addendum 1 to Aviation Fuels Technical Review (FTR-3/A1). Chevron Corporation, 2006.
39.	Hileman J.I., Stratton R.W.: Alternative jet fuel feasibility. Transport Policy. Vol. 34, 2014, p. 52–62.
40.	Hileman J.I., Wong H.M., Waitz I.: Near-Term Feasibility of Alternative Jet Fuels. Santa Monica, California: RAND Corporation, 2009.
41.	Hileman, J. Ortiz D., Bartis J.: Near-Term Feasibility of Alternative Jet Fuels. Jointly published by the RAND Corporation (Report No. TR-554-FAA) and the Partnership for Air Transportation Noise and Emissions Reduction, 2009.
42.	Honga T.D., Soerawidjajab T.H., Reksowardojoa I.K.: A study on developing aviation biofuel for the Tropics: Production process – Experimental and theoretical evaluation of their blends with fossil kerosene, Chemical Engineering and Processing: Process Intensification, Vol. 74, 2013, p. 124–130.
43.	Hristova M., Tchaoushev S.: Сalculation of flash points and flammability limits of substances and mixtures. Journal of the University of Chemical Technology and Metallurgy, Iss. 41(3), p. 291–296, 2006.
44.	Hu J., Du Z., Li C., Min E.: Study on the lubrication properties of biodiesel as fuel lubricity enhancers, Fuel. Vol. 84, 2005. p. 1601–1606. 
45.	Iakovlieva A., Boichenko S., Vovk O.: Investigation of the fractional composition of rape oil-derived aviation biofuels. Aviation in the XXI-st century. Safety in aviation and space technologies: the fifth world congress, 25–27 September 2012: abstracts. Kyiv, Vol. 3, 2012, p. 5.41–5.43.
46.	Iakovlieva A.V. Boichenko S.V., Vovk O.O.: Overview of innovative technologies for aviation fuels production. Journal of Chemistry and chemical technology, Vol. 7. Iss. 3, 2013, p. 305–312.
47.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Peculiarities of the development and implementation of aviation biofuels in Ukraine. World Congress on Petrochemistry and Chemical Engineering. Journal of Petroleum & Environmental Biotechnology. November 2013, San Antonio. Vol.4. Iss. 6, 2013, p. 47.
48.	Iakovlieva A., Boichenko S., Gay A.: Cause-Effect Analysis of the Modern State in Production of Jet Fuels. Journal of Сhemistry & Chemical Technology. Vol. 8. No 1, 2014, p. 107–116.
49.	Iakovlieva A., Boichenko S., Vovk O., Lejda K.: Potential of jet biofuels production and application in Ukraine and Poland. International Journal of Sustainable Aviation. Vol. 1. No.4, 2015, p. 314–323.
50.	Iakovlieva A., Boichenko S., Lejda K.: Impact of rape oil ethyl esters additives on some characteristics of jet fuel. Проблеми хіммотології. Теорія та практика раціонального використання традиційних і альтернативних паливно -мастильних матеріалів: V міжнар. наук.-техн. конф., 6–10 жовт. 2014. Київ, c. 286 – 289. 
51.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Skilniuk I.: Vacuum Distillation of Rapeseed Oil Esters for Production of Jet Fuel Bio-Additives, Procedia Engineering, Vol. 187, 2017, p. 363 – 370. 
52.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Рotential of jet biofuels production and application in Ukraine and Poland. Proceedings of the 1st International Simposium on Sustainable Aviation.–31 May–03 June 2015, Isntanbul, p. 137.
53.	Iakovlieva A., Boichenko S., Lejda K.: Experimental study on antiwear properties for blends of jet fuel with biocomponents derived from rapeseed oil. Eastern-European journal of enterprise technologies. No. 5/8(77), 2015, p. 20–28.
54.	Iakovlieva A., Vovk O., Boichenko S.: Еxperimental study of rape oil esters influence on physical-chemical properties of jet fuels. Proceedings of the 19th Conference for Junior Researchers ‘Science – Future of Lithuania’ Тransport engineering and management, 6 May 2016, Vilnius. p. 85–89.
55.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Kuszewski H. Improvement of technological scheme of fatty acids ethyl esters production for use as jet fuels biocomponents. International Journal of Theoretical and Applied Science. Iss. 11(19), 2014, p. 44–55.
56.	International Air Transport organization. Vision 2050. Report. Montreal. Geneva, 2011.
57.	Jansen R. A.: Second Generation Biofuels and Biomass: Essential Guide for Investors, Scientists and Decision Makers. Wiley. 2012.
58.	Jenkins R.W., Munro M., Christopher S.N., Chuck C.: Potential renewable oxygenated biofuels for the aviation and road transport sectors. Fuel, Vol. 103, 2013, p. 593–599.
59.	Jacyna M., Żak J., Jacyna-Gołda I., Merkisz J., Merkisz-Guranowska A., Pielecha J.: Selected aspects of the model of proecological transport system. Journal of KONES Powertrain and Transport, Vol. 20, No. 3, 2013, p. 193 – 202.
60.	Kallio P., Pasztor A., Akhtar M.K., Jones P.R.: Renewable jet fuel. Current Opinion in Biotechnology. Vol. 26, 2014, p. 50–55.
61.	Kandaramath Hari T., Yaakob Z., Binitha N.N.: Aviation biofuel from renewable resources: Routes, opportunities and challenges. Renewable and Sustainable Energy Reviews. Vol. 42, 2015, p. 1234–1244. 
62.	Kinder J. D., Rahmes T.: Evaluation of Bio-Derived Synthetic Paraffinic Kerosene (Bio-SPK). The Boeing Company Sustainable Biofuels Research&Technology Program, 2009.
63.	Kirklin P.W., David. P.: Aviation Fuel: Thermal Stability. ASTM International, 1992.
64.	Lapuerta M., Rodriguez-Fernandeza J., Estevez C., Bayarri N.: Properties of fatty acid glycerol formal ester (FAGE) for use as a component in blends for diesel engines. Biomass and bioenergy. Vol. 76, 2015, p. 130–140. 
65.	Lebedevas S., Vaicekauskas A.: Research into the application of biodiesel in the transport sector of Lithuania. Transport. Vol. 21, Iss. 2, 2006, p. 80–87.
66.	Liu G., Yan B., Chen G.: Technical review on jet fuel production. Renewable and Sustainable Energy Reviews. Vol. 25, 2013, p. 59–70. 
67.	Lu M., Chai M.: Experimental Investigation of the Oxidation of Methyl Oleate: One of the Major Biodiesel Fuel Components Synthetic Liquids Production and Refining. Chapter 13, P. 289–312. American Chemical Society. 2011
68.	Merkisz J., Merkisz-Guranowska, A., Pielecha J., Nowak M., Jacyna M., Lewczuk K., Żak J.: Exhaust emission measurements in the development of sustainable road transport. Journal of KONES Powertrain and Transport, Vol. 20, No. 4 2013, p. 277 – 284.
69.	Maksimuk Yu., Antonova Z., Fes’ko V., Kursevich V.: Diesel biofuel viscosity and heat of combustion. Chemistry and technology of fuels and oils. Iss. 45, 2009, p. 343–346.
70.	Maru M. M., Trommer R.M., Cavalcanti K.F.: The Stribeck curve as a suitable characterization method of the lubricity of biodiesel and diesel blends. Energy. Vol. 69, 2014, p. 673–681.
71.	Maurice L.Q., Lander H., Edwards T., Harrison W.E.: Advanced aviation fuels: a look ahead via a historical perspective. Fuel. Vol. 80, Iss. 5, 2001, p. 747–756.
72.	Merkisz J., Markowski J., Pielecha J. Emission tests of the AI-14RA aircraft engine under real operating conditions of PZL-104"" Wilga"" plane. Silniki Spalinowe. No. 3, 2009, p. 64–70.
73.	Merkisz J., Galant M., Karpiński D., Kubiak, K. Evaluation of possibility to use the LTO cycle for emission test on example of the model turbine engine GTM-120 Journal of Mechanical and Transport Engineering. Vol. 66, No. 2, 2014, p. 25—33.
74.	Murphy D.J., Hall C.A.S.: Year in review—EROI or energy return on (energy) invested. Annals of the New York academy of sciences. Issue: Ecological Economics Reviews. Iss. 1185, 2010, p. 102–118.
75.	Murphy D.J., Hall C.A.S., Powers B.:New perspectives on the energy return on (energy) investment (EROI) of corn ethanol. Environment, Development and Sustainability. Vol. 13, Iss. 1, 2011, p. 179–202.
76.	Naik S.N., Goud V.V., Rout P.K., Dalai A.K.: Production of first and second generation biofuels: A comprehensive review. Renew. Sust. Energ. Rev., No. 14, 2010, p. 578–597.
77.	Nollet Leo M. L.: Handbook of Food Analysis: Physical characterization and nutrient analysis. CRC Press, 2004.
78.	Orszulik S.: Environmental Technology in the Oil Industry. Springer Science & Business Media, 2013.
79.	Pandey A.: Biofuels: Alternative Feedstocks and Conversion Processes. Academic Press, 2011.
80.	Pearlson M.N.: A techno-economic and environmental assessment of hydroprocessed renewable distillate fuels. Master of Science in Technology and Policy. Massachiussets Institute of Technology. June 2011.
81.	Prag P.: Renewable Energy in the Countryside. Taylor & Francis, 2014.
82.	Prussi M, Chiaramonti D, Recchia L, Martelli F, Guidotti F, Pari L.: Alternative feedstock for the biodiesel and energy production: the OVEST project. Energy Journal, No. 58, 2013, p. 2–8.
83.	Rahmes T.F., Kinder J.D., Henry T.M., etc.: Sustainable Bio-Derived Synthetic Paraffinic Kerosene (BioSPK) Jet Fuel Flights and Engine Tests Program Results. American Institute of Aeronautics and Astronautics, 2009.
84.	Rajagopal D., Zilberman D.: Environmental, Economic and Policy Aspects of Biofuels. Nеw Publishers Inc., 2008.
85.	Report on alternative fuels. International Air Transport Association IATA. http://www.iata.org/publications/Documents/2012-report-alternativefuels. pdf; 2012
86.	Rosillo Calle F, Trhan D, Seiffert M, Teeluckingh S. The potential and role of biofuels in commercial air transport – biojetfuels. Task 40 sustainable international bioenergy trade. IEA Bioenergy
87.	Sarin R., Kumar R., Srivastav B., etc.: Biodiesel surrogates: Achieving performance demands. Bioresource Technology. Vol. 100, Iss. 12, 2009, p. 3022–3028. 
88.	Shen Y.. Аn experimental study on thermal stability of FAEE biodiesel fuel with ethanol. Master Thesis, 2015.
89.	Shepherd J.E., Nuyt C.D., Lee J.J.: Flash Point and Chemical Composition of Aviation Kerosene (Jet A). National Transportation Safety Board, 2000.
90.	Singh B.: Biofuel Crops: Production, Physiology and Genetics. CABI, 2013.
91.	Singh B.: Biofuel Crop Sustainability. John Wiley & Sons, 2013.
92.	Sperling D., Cannon J.S.: Reducing Climate Impacts in the Transportation Sector. Springer Science & Business Media, 2011.
93.	Szczerek M., Tuszyсski W. Tribological researches – scuffing. Radom: Institute for Sustainable Technologies – National Research Institute, 2000.
94.	The jet engine. Rolls-Royce plc. Renault Printing Co Ltd., 1996.
95.	T-02U. Aparat czterokulowy – instrukcja obsługi. Radom: Wydawnictwo Instytutu Technologii Eksploatacji, 2011.
96.	Wcisło G.: Determination of the impact of FAME biocomponent on the fractional composition of diesel engine fuels. Combustion Engines. Iss. 154(3), 2013, p. 1098–1103.
97.	Xu Y., Wang Q., Hu X.: Characterization of the lubricity of bio-oil/diesel fuel blends by high frequency reciprocating test rig. Energy. Vol. 35, Iss. 1, 2010, p. 283–287. 
98.	Yakovleva A.V., Boichenko S.V., Lejda K, Vovk O.O., Kuszewski H.: Antiwear Properties of Plant—Mineral-Based Fuels for Airbreathing Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 1, 2017, p. 1–9. 
99.	Yakovlieva A.V., Boichenko S.V., Leida K., Vovk O.A., Kuzhevskii Kh.. Influence of Rapeseed Oil Ester Additives on Fuel Quality Index for Air Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 3, 2017. p. 308–317.
100.	Yakovlieva A., Boichenko S., Vovk O., Lejda K., Gryshchenko O.. Case Study of Alternative Jet Fuel Production with Bio-additives from Plant Oils in Ukraine and Poland. Advances in Sustainable Aviation. Springer International Publishing, 2018. Chapter 4.
101.	Yakovlieva A., Boshkov V. Experimental study of low-temperature properties of alternative aviation fuels, Proceedings of the 21th Conference for Junior Researchers ‘Science – Future of Lithuania’ Transport Engineering and Management, 4-5 May 2018, Vilnius, Lithuania. 2018. p. 130 – 134.
102.	Yildirim U, Abanteriba S.: Manufacture, qualification and approval of new aviation turbine fuels and additives, proceedia Engineering, No. 49, 2012, p. 310 – 315.
103.	Yutko B. and Hansman J., Approaches to Representing Aircraft Fuel Efficiency Performance for the Purpose of a Commercial Aircraft Certification Standard, MITInternational Center for Air Transportation, Cambridge, Mass, 2011.
104.	Zhu Y.: An Experimental Study on Thermal Stability of Biodiesel Fuel. Master Thesis. – 2012. – 160 p.
105.	Авиационный турбореактивный двигатель РУ 19A-300, руководство по эксплуатации и техническому обслуживанию, ЗАО «АНТЦ Технолог», 2001.
106.	Азев В.С., Середа А.В.: Влияние соединений серы на противоизносные свойства дизельных топлив, Химия и технология топлив и масел. № 3, 2009, c. 23–27.
107.	Андіїшин М.П., Марчук Я.С., Бойченко С.В., Рябоконь Л.А.: Газ природний, палива та оливи. Одеса: Астропринт, 2010.
108.	Бойченко С.В., Спіркін В.Г. Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт, Ч.1., 2009.
109.	Бойченко С.В., Любінін Й.А., Спіркін В.Г.: Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт. Ч.2., 2010.
110.	Бойченко С.В., Черняк Л.М., Яковлєва А.В.: Традиційні технології виробництва палив для повітряно-реактивних двигунів. Вісник Національного авіаційного університету. № 2 (55), 2013, с. 195–209.
111.	Бойченко С. В., Яковлева А. В., Волошинец В. А., Лейда К. Модифицирование эфиров рапсового масла вакуумным фракционированием, Технологии нефти и газа, №5, 2018, c. 15–20
112.	Братичак М.М.: Основи промислової нафтохімії, Львів: Вид-во НУ «Львівська політехніка», 2008.
113.	Васильев И.П.: Влияние топлив растительного происхождения на экологические и экономические показатели дизеля, Луганск: Изд-во ВНУ им. В. Даля, 2009.
114.	Волошинець В.А. Фізична та колоїдна хімія: Фізико-хімія дисперсних систем та полімерів: навч.посіб. Львів : Вид-во Львів. політехніки, 2013. – 200 с.
115.	Голоскоков А.Н. Критерии сравнения эффективности традиционных и альтернативных энергоресурсов. Нефтегазовое дело. № 1, 2011, c. 285–301.
116.	Голоскоков А.Н. Пик добычи нефти и начало мирового энергетического кризиса. Нефтегазовое дело. 2010, c. 1–13.
117.	Данилов А.М., Каминский Э.Ф., Хавкин В.А.: Альтернативные топлива: достоинства и недостатки. Проблемы применения. Российский химический журнал (Журнал Российского химического общества им. Д.И. Менделеева). Т. XLVII. № 6, 2003, c. 4–11.
118.	Дворецкий С.И., Нагорнов С.А., Романцова С.В. и др.: Производство биодизельного топлива из органического сырья. Вопросы современной науки и практики. № 39, 2012, c. 126– 35.
119.	Девянин С.Н., Марков В.А., Семенов В.Г.: Растительные масла и топлива на их основе для дизельных двигателей. Харьков: Новое слово. 2007.
120.	Ергин Д.: Добыча: Всемирная история борьбы за нефть, деньги и власть. Москва,: Альпина Паблишер, 2011.
121.	Запорожець А.О.: Дослідження стехіометричної суміші «повітря ‒ паливо» органічних сполук. Частина 1. Алкани. Наукоємні технології. № 2(22), 2014, c. 163–167.
122.	Кириченко В., Бойченко С., Кириченко В., Нездоровин В.: Комплексная переработка технических растительных масел: концепция, методы и технологи. «Systems and means of motor transport» Seria: Transport. Monografia. № 4, 2013, p. 357–370.
123.	Колодницька Р.В., Семенов В.Г.: Моделювання низькотемпературних властивостей біодизельних палив. Вісник СевНТУ. Серія: Машиноприладобудування та транспорт. № 134, 2012, c. 135–138.
124.	Коллоидная химия нефти и нефтепродуктов: Сборник материалов, посвященных научной деятельности проф. Г.И. Фукса. Москва: Изд-во «Техника». ООО «Тума Групп», 2001.
125.	Крылов И.Ф., Емельянов В.Е.: Альтернативные моторные топлива. Производство, применение",'National Aviation University',Modification of jet fuels composition with renewable bio-additives,10.18372/37895,https://core.ac.uk/download/286618536.pdf,,core
369839311,2019-06-19T00:00:00,"The aim of my research was to develop a digital mediation system with urban data for a pedestrian immersed in the city, a link based on digital technologies to design, analyze, represent urban space and access information on this urban space. Augmented Reality is one of the tools allowing this mediation whose critical element is the location of the pedestrian and more precisely the pose calculation of the camera it carries.Thus, the main focus of my work is geolocation on site using spatial data of different dimensions. I was interested in an upstream phase that requires the implementation of data models to keep track of spatial data changes. Finally, I touched on some uses of geolocation and pose calculation. I conclude this report by presenting my research perspectives on digital mediation with urban data for pedestrians.Le but de ma recherche a été de mettre au point un système de médiation numérique avec des données urbaines pour un piéton immergé dans la ville, un lien basé sur des technologies numériques pour concevoir, analyser, représenter l’espace urbain et accéder à des informations sur cet espace urbain. La réalité augmentée est un des outils permettant cette médiation dont l’élément critique est la localisation du piéton et plus précisément le calcul de pose de la caméra qu’il transporte.Ainsi, l’axe principal de mon travail est la géolocalisation sur site à l’aide de données spatiales de différentes dimensions. Je me suis intéressée à une phase amont qui nécessite la mise en place de modèles de données pour garder trace des modifications des données spatiales. J’ai enfin abordé quelques usages de la géolocalisation et du calcul de pose. Je conclus ce mémoire en présentant mes perspectives de recherches vers une médiation numérique avec des données urbaines pour le piéton",HAL CCSD,Méthodes pour la géolocalisation du piéton sur site - vers une médiation numérique avec les données urbaines,,,,core
275581201,2019-09-03T00:00:00,"This thesis investigates a combination of visual 3D object trackers and 2D object detectors for mutual improvement in accuracy and runtime. In the light of an application in the field of optical navigation, requirements such as high reliability, real-time capability and general applicability are pursued for the implementation.

The combined application builds on the framework of the Integrated Positioning System - a multi-sensor system primarily used for self-localization and environment reconstruction. The developed object tracker approach works on point clouds, represents objects through Intrinsic Shape Signatures and Color Signature of Histograms of Orientations and locates objects using an advanced particle filter. As object detector, the Deep Learning based method YOLOv3 is employed. The semantics and the localization of the object detections are used to restrict the search space during object tracking. In return, the object tracking result is projected back onto the image, to additionally supplement object detections and enhance their precision.

The combined method is compared to each separate method using established metrics. The experiments are based on complementary datasets from the real world and 3D simulations. Within both datasets, parameter setups, object classes and environmental scenarios are varied according to different attributes.

The results show a faster and more accurate object tracking through the object detections. However, due to lower reliability, the proposed object tracker is not suitable for improving YOLOv3 detections",,Mutual Improvement of 2D Object Detection and 3D Tracking in Optical Navigation,,https://core.ac.uk/download/275581201.pdf,,core
395072474,2019-01-01T00:00:00,"Adoption of digital platform innovations afford a changing nature of work, from mobile computing platforms (e.g. Apple) enabling 24/7 work connectivity, to labour marketplace platforms (e.g. Uber) enabling precarious work arrangements. Recently, organisations are adopting/investigating spatial computing platforms (e.g. Autodesk, Toyota, BNP Paribas), offering new affordances for organising (e.g. carrying out tasks, communicating and collaborating). Spatial computing concerns achieving spatial interplay between the real and digital world (Agulhon 2016), enabling perception of physically present content. An emerging paradigm of spatial computing is enabled by hardware and software innovations for; 1) digitally mapping, tracking, understanding and predicting analog audio and visual spatial fields, 2) creating digital audio and visual spatial fields, and the (3) mixing and fusing of those fields. Mixed, augmented and immersive reality is then experienced by volumetric graphic rendering onto a human's field of view (FOV) (Martín-Gutiérrez et al. 2017). Emerging marketplace examples can be seen in 'Microsoft Hololens 2' and 'Magic Leap One' platforms, both creating/enabling an ecosystem of novel applications for both industrial, educational and leisure life contexts. With further convergence of IoT, haptics, 5G, cloud and AI etc., spatial applications will range from contextually aware and interactive; digital information layering of objects, guidance and decision support systems (DSS) within business operations (such as for industrial machine manufacture, monitoring, and maintenance), digital modelling & prototyping in R&D, through to applications for communications and collaborations (such as for spatial tele co-presence of people, objects and environments). More broadly, these advances have potential to catalyse disruptions within business, through to the labour and consumer marketplace via: (1) Virtualisation of hardware resources (e.g. fully digitising workplace equipment such as displays and interfaces, raw inputs for prototyping and even digital rendering of spaces). (2) Protection and strengthening of institutional knowledge and performance via knowledge capture, guidance and decision support of labour tasks and activity (e.g. reducing labour (re)training (e.g. parts assembly), knowledge capture of practice). (3) Creation and distribution of new value propositions in goods and services (e.g. digital item ownership in a mixed-reality cloud, spatial applications for IoT enabled devices). (4) Displacement of geographic space as cost, talent, time, access and convenience constraints on business (e.g. available talent pool, partner/customer reach and relations). (5) Collaboration through new/enhanced affordances for workers (e.g. shared digitised work tools/environments). Therefore, a paradigm of spatial computing will challenge the IS community to research new ways of working, and consequences for worker experience, meaning, productivity and power. With emerging advances in AI, automation and spatial computing, one of the pertinent enquires concerns importance of workers (sense of) agency (Chandra et al. 2019). Control in the IT context has been conceptualised as control over work, control over self, and control over technology (Beaudry and Pinsonneault 2005), with prior IS work studying locus of control related to; work stress (Chandra et al. 2019), intrinsic and extrinsic motivations (Mujinga, M Eloff, MM Kroeze 2013), and performance (Vieira da Cunha et al. 2015) etc. With spatial computing platforms and their applications, what affordances of control and for whom should be developed? For example, the electronic representation of worker activity can be further enabled. Thus, tighter or looser coupling between worker activity and the reporting/outcome of work (Vieira da Cunha et al. 2015) becomes more of an organisational decision, with capability to monitor workers, and leverage AI for learning and optimisation. Furthermore, with development of spatial tele co-presence (STcP) (e.g. Mimesys), brings new affordances for communication with any worker(s), at any time, from anywhere. However, prior CMC research suggests people can choose different communication media specifically to manage social and emotional relationships (Madianou 2014) and their time (Mcloughlin et al. 2019). Hence, will such affordances serve greater identity fusion (Swann et al. 2012) and collaboration in organisations? Thus, we propose a socio-technical research agenda exploring 'control' related affordances for emerging spatial computing platforms, such as for STcP technology. In this regard, Control Theory can offer a useful starting frame, as it deals with control mechanisms governing workers organisational actions both formal (outcome and behaviour based) and informal (group and self-control), to further the interests of organisations (Kirsch 1996). We suggest, data and communication related affordances of control (e.g. privacy, exploitation, authenticity, availability and spaces) as starting points. Social Capital (Lin 2001), Social Influence (Kelman 1958), Social Identity (Ellemers and Haslam 2012), Identity Fusion (Swann et al. 2012) and Polymedia (Madianou and Miller 2012) being just some of the many relevant social theories to this endeavour",,Affordances of Control in a Paradigm of Spatial Computing Platforms,,,,core
395096827,2019-12-02T00:00:00,"In this thesis, we first present a unified look to several well known 3D feature representations, ranging from hand-crafted design to learning based ones. Then, we propose three kinds of feature representations from both RGB-D data and point cloud, addressing different problems and aiming for different functionality.



With RGB-D data, we address the existing problems of 2D feature representation in visual perception by integrating with the 3D information. We propose an RGB-D data based feature representation which fuses object's statistical color model and depth information in a probabilistic manner. The depth information is able to not only enhance the discriminative power of the model toward clutters with a different range but also can be used as a constraint to properly update the model and reduce model drifting. The proposed representation is then evaluated in our proposed object tracking algorithm (named MS3D) on a public RGB-D object tracking dataset. It runs in real-time and produces the best results compared against the other state-of-the-art RGB-D trackers. Furthermore, we integrate MS3D tracker in an RGB-D camera network in order to handle long-term and full occlusion. The accuracy and robustness of our algorithm are evaluated in our presented dataset and the results suggest our algorithm is able to track multiple objects accurately and continuously in the long term.



For 3D point cloud, the current deep learning based feature representations often discard spatial arrangements in data, hence falling short of respecting the parts-to-whole relationship, which is critical to explain and describe 3D shapes. Addressing this problem, we propose 3D point-capsule networks, an autoencoder designed for unsupervised learning of feature representations from sparse 3D point clouds while preserving spatial arrangements of the input data into different feature attentions. 3D capsule networks arise as a direct consequence of our unified formulation of the common 3D autoencoders. The dynamic routing scheme and the peculiar 2D latent feature representation deployed by our capsule networks bring in improvements for several common point cloud-related tasks, such as object classification, object reconstruction and part segmentation as substantiated by our extensive evaluations. Moreover, it enables new applications such as part interpolation and replacement.



Finally, towards rotation equivariance of the 3D feature representation, we present a 3D capsule architecture for processing of point clouds that is equivariant with respect to the SO(3) rotation group, translation, and permutation of the unordered input sets. The network operates on a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end equivariance through a novel 3D quaternion group capsule layer, including an equivariant dynamic routing procedure. The capsule layer enables us to disentangle geometry from the pose, paving the way for more informative descriptions and structured latent space. In the process, we theoretically connect the process of dynamic routing between capsules to the well-known Weiszfeld algorithm, a scheme for solving iterative re-weighted least squares (IRLS) problems with provable convergence properties, enabling robust pose estimation between capsule layers. Due to the sparse equivariant quaternion capsules, our architecture allows joint object classification and orientation estimation, which we validate empirically on common benchmark datasets",,3D feature representations for visual perception and geometric shape understanding,,,,core
344203551,2019-06-19T00:00:00,"The aim of my research was to develop a digital mediation system with urban data for a pedestrian immersed in the city, a link based on digital technologies to design, analyze, represent urban space and access information on this urban space. Augmented Reality is one of the tools allowing this mediation whose critical element is the location of the pedestrian and more precisely the pose calculation of the camera it carries.Thus, the main focus of my work is geolocation on site using spatial data of different dimensions. I was interested in an upstream phase that requires the implementation of data models to keep track of spatial data changes. Finally, I touched on some uses of geolocation and pose calculation. I conclude this report by presenting my research perspectives on digital mediation with urban data for pedestrians.Le but de ma recherche a été de mettre au point un système de médiation numérique avec des données urbaines pour un piéton immergé dans la ville, un lien basé sur des technologies numériques pour concevoir, analyser, représenter l’espace urbain et accéder à des informations sur cet espace urbain. La réalité augmentée est un des outils permettant cette médiation dont l’élément critique est la localisation du piéton et plus précisément le calcul de pose de la caméra qu’il transporte.Ainsi, l’axe principal de mon travail est la géolocalisation sur site à l’aide de données spatiales de différentes dimensions. Je me suis intéressée à une phase amont qui nécessite la mise en place de modèles de données pour garder trace des modifications des données spatiales. J’ai enfin abordé quelques usages de la géolocalisation et du calcul de pose. Je conclus ce mémoire en présentant mes perspectives de recherches vers une médiation numérique avec des données urbaines pour le piéton",HAL CCSD,Méthodes pour la géolocalisation du piéton sur site - vers une médiation numérique avec les données urbaines,,,,core
211049053,2019-04-05T00:00:00,"Autonomic optical transmission and networking requires machine learning (ML) models to be trained with large datasets. However, the availability of enough real data to produce accurate ML models is rarely ensured since new optical equipment and techniques are continuously being deployed in the network. One option is to generate data from simulations and lab experiments, but such data could not cover the whole features space and would translate into inaccuracies in the ML models. In this paper, we propose an ML-based algorithm life cycle to facilitate ML deployment in real operator networks. The dataset for ML training can be initially populated based on the results from simulations and lab experiments. Once ML models are generated, ML retraining can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits of the proposed learning cycle for general use cases. In addition, two specific use cases are proposed and demonstrated that implement different learning strategies: (i) a two-phase strategy performing out-of-field training using data from simulations and lab experiments with generic equipment, followed by an in-field adaptation to support heterogeneous equipment (the accuracy of this strategy is shown for a use case of failure detection and identification), and (ii) in-field retraining, where ML models are retrained after detecting model inaccuracies. Different approaches are analyzed and evaluated for a use case of autonomic transmission, where results show the significant benefits of collective learning.Peer ReviewedPostprint (published version",'The Optical Society',Learning life cycle to speed up autonomic optical transmission and networking adoption,10.1364/JOCN.11.000226,https://core.ac.uk/download/211049053.pdf,"[{'title': 'Journal of Optical Communications and Networking', 'identifiers': ['1943-0620', 'issn:1943-0620']}]",core
322991098,2019-01-01T00:00:00,"1.	Abu-Taieh C., Evon J.: Technology Engineering and Management in Aviation: Advancements and Discoveries. Information Science Reference, 2011.
2.	Ajam M, Woolard C, Wiljoen CL. Biomass pyrolysis oil as a renewable feedstock for bio-jet fuel. In: Proceedings of the 13th international conference on stability, handling and use of liquid fuels (IASH2013), Rhodes, Greece; October 2013. p. 6–10.
3.	Аnnual report to Parliament on the renewable transport fuel obligation. Renewable Fuels Agency. The Stationery Office, 2011.
4.	Agarwal S., Chhibber V. K., Bhatnagar A. K.:Tribological behavior of diesel fuels and the effect of anti-wear additives. Fuel. Vol. 106, 2013, p. 21–29, 
5.	Alves S. M., Barros B.S., Trajano M.F.: Tribological behavior of vegetable oil-based lubricants with nanoparticles of oxides in boundary lubrication conditions. Tribology International. Vol. 65, 2013, p. 28–36.
6.	Asgari H., Chen X., Sainudiin R.: Modelling and simulation of gas turbines. International Journalof Modelling, Identification and Control, Vol.25, No.3, 2013, p. 1–15.
7.	Bartis James T. LaTourrette T., Dixon L.: Oil Shale Development in the United States: Prospects and Policy Issues. Santa Monica, Calif.: RAND Corporation, MG-414-NETL, 2005.
8.	Bassam N. El.: Handbook of Bioenergy Crops: A Complete Reference to Species. Development and Applications Earthscan, 2010.
9.	Bazazzadeh M., Badihi H., Shahriari A.: Gas Turbine Engine Control Design Using Fuzzy Logic and Neural Networks. International Journal of Aerospace Engineering. Vol. 1, 2011, p. 1–13. 
10.	Blakey S, Rye L, Wilson C.W.: Aviation gas turbine alternative fuels: A review.  P Combust Inst, No. 33, 2011, p. 2863–2885.
11.	Boichenko S., Iakovlieva A., Vovk O.: Traditional and alternative jet fuels: problems of quality standardization. Journal of Petroleum & Environmental Biotechnology. Vol. 4. Iss. 3, 2013.
12.	Boichenko S., Shkilniuk I., Turchak V.. The problems of biopollution with jet fuels and the way of achieving solution. Transport. 23, 2008; p. 253–257.
13.	Boichenko S., Yakovleva A. Prospects of biofuels introduction into aviation. Transport engineering and management: Proceedings of the 15-th conference for Lithuania Junior researchers. Science – future of Lithuania, 4 May 2012. Vilnius: Technika. p. 90–94.
14.	Boichenko S., Yakovlieva A., Gryshchenko O., Zinchuk A. Prospects of using different generations biofuels for minimizing impact of modern aviation on environment, Энерготехнологии и ресурсосбережение, № 1, 2018, p. 10–20.
15.	Boichenko S., Lejda K., Yakovlieva A., Vovk O. Comparative characteristics of low-temperature properties of jet fuels modified with bio-additives, International Automotive Conference (KONMOT2018). IOP Conf. Series: Materials Science and Engineering 421, 2018.
16.	Breil C., Meullemiestre A., Vian M., Chemat F.: Bio-Based Solvents for Green Extraction of Lipids from Oleaginous Yeast Biomass for Sustainable Aviation Biofuel. Molecules. Iss. 21(196), 2016, p. 1–14.
17.	Carels N., Sujatha M., Bahadur B.: Jatropha, Challenges for a New Energy Crop. Vol. 1: Farming, Economics and Biofuel. Springer Science & Business Media, 2012.
18.	Cavani F., Albonetti S., Basile F., Gandini A.: Chemicals and Fuels from Bio-Based Building Blocks. John Wiley & Sons, 2015.
19.	Cermak S. C., Evangelista R. L., Kenar J. A.: Distillation of Natural Fatty Acids and Their Chemical Derivatives, Distillation - Advances from Modeling to Applications, Dr. Sina Zereshki (Ed.), InTech, 2012. – р. 5. – 140. 
20.	Chai M. Thermal Decomposition of Methyl Esters in Biodiesel Fuel: Kinetics, Mechanisms and Products, Ph.D. Thesis, University оf Cincinnati, 2012.
21.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 1: emulsion production. Biomass Bioenergy, No. 25, 2003, p. 85–99.
22.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 2: tests in diesel engines. Biomass Bioenergy, No. 25, 2003, p. 101–11.
23.	Chuck C.J., Donnelly J.: The compatibility of potential bioderived fuels with Jet A-1 aviation kerosene. Applied Energy. Vol. 118, 2014, p. 83–91.
24.	Cleveland C.J., Morris C. G.: Handbook of energy. Volume II: Cronologies, top ten lists, and words clouds. Elsvier Inc., 2014.
25.	Cushion E., Whiteman A., Dieterle G.: Bioenergy Development: Issues and Impacts for Poverty and Natural Resource Management. World Bank Publications, 2010.
26.	Daggett D. L., Hendricks R.C., Walther R., Corporan E.: Alternative fuels for use in commercial aircrafts. The Boeing Company, 2007.
27.	Dahlquist E.: Biomass as Energy Source. Resources, Systems and Applications. CRC Press, 2013.
28.	Delmon B., Grange P., Froment G.F.: Hydrotreatment and Hydrocracking of Oil Fractions. Elsevier, 1999.
29.	Doc 9889 Airport Air Quality Manual. International Civil Aviation Organization, 2011. 
30.	Doc 9977. Manual on Civil Aviation Jet Fuel Supply, 2012.
31.	Edwards T.: Advancements in Gas Turbine Fuels from 1943 to 2005. J Eng Gas Power, No. 129, 2007, p. 13–20.
32.	Firrisa M. T., Van Duren I., Voinov A.: Energy efficiency for rapeseed biodiesel production in different farming systems. Energy Efficiency, 2013.
33.	Garcia-Anton J., Monzo J., Guninon J.L.: Study of corrosion on copper strips by petroleum naphtha in the ASTM D-130 test by means of electronic microscopy (SEM) and energy dispersive X-ray (EDX). Fresenius Journal of Analytical Chemistry. Iss. 337, 1990, p. 382–388.
34.	Garcia Santander C.M., Gymez Rueda S.M., de Lima da Silva N.: Measurements of normal boiling points of fatty acid esters and triacylglycerols by thermogravimetric analysis, Fuel, Iss. 92, 2012, p. 158–161.
35.	Geller D. P., Goodrum J.: W. Effects of speciﬁc fatty acid methyl esters on diesel fuel lubricity, Fuel, Vol. 83, 2004, p. 2351–2356.
36.	Gupta, K. K, Rehman A, Sarviya R. M.: Bio-fuels for the gas turbine: A review. Renew. Sust. Energ. Rev. No. 14, 2010, p. 2946–2955.
37.	Harvey B. G, Merriman W.W., Koontz T.A.: High-Density Renewable Diesel and Jet Fuels Prepared from Multicyclic Sesquiterpanes and a 1‑Hexene-Derived Synthetic Paraffinic Kerosene, Energy Fuels, 2013.
38.	Hemighaus G., Boval T., Bosley C.: Alternative Jet Fuels. Addendum 1 to Aviation Fuels Technical Review (FTR-3/A1). Chevron Corporation, 2006.
39.	Hileman J.I., Stratton R.W.: Alternative jet fuel feasibility. Transport Policy. Vol. 34, 2014, p. 52–62.
40.	Hileman J.I., Wong H.M., Waitz I.: Near-Term Feasibility of Alternative Jet Fuels. Santa Monica, California: RAND Corporation, 2009.
41.	Hileman, J. Ortiz D., Bartis J.: Near-Term Feasibility of Alternative Jet Fuels. Jointly published by the RAND Corporation (Report No. TR-554-FAA) and the Partnership for Air Transportation Noise and Emissions Reduction, 2009.
42.	Honga T.D., Soerawidjajab T.H., Reksowardojoa I.K.: A study on developing aviation biofuel for the Tropics: Production process – Experimental and theoretical evaluation of their blends with fossil kerosene, Chemical Engineering and Processing: Process Intensification, Vol. 74, 2013, p. 124–130.
43.	Hristova M., Tchaoushev S.: Сalculation of flash points and flammability limits of substances and mixtures. Journal of the University of Chemical Technology and Metallurgy, Iss. 41(3), p. 291–296, 2006.
44.	Hu J., Du Z., Li C., Min E.: Study on the lubrication properties of biodiesel as fuel lubricity enhancers, Fuel. Vol. 84, 2005. p. 1601–1606. 
45.	Iakovlieva A., Boichenko S., Vovk O.: Investigation of the fractional composition of rape oil-derived aviation biofuels. Aviation in the XXI-st century. Safety in aviation and space technologies: the fifth world congress, 25–27 September 2012: abstracts. Kyiv, Vol. 3, 2012, p. 5.41–5.43.
46.	Iakovlieva A.V. Boichenko S.V., Vovk O.O.: Overview of innovative technologies for aviation fuels production. Journal of Chemistry and chemical technology, Vol. 7. Iss. 3, 2013, p. 305–312.
47.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Peculiarities of the development and implementation of aviation biofuels in Ukraine. World Congress on Petrochemistry and Chemical Engineering. Journal of Petroleum & Environmental Biotechnology. November 2013, San Antonio. Vol.4. Iss. 6, 2013, p. 47.
48.	Iakovlieva A., Boichenko S., Gay A.: Cause-Effect Analysis of the Modern State in Production of Jet Fuels. Journal of Сhemistry & Chemical Technology. Vol. 8. No 1, 2014, p. 107–116.
49.	Iakovlieva A., Boichenko S., Vovk O., Lejda K.: Potential of jet biofuels production and application in Ukraine and Poland. International Journal of Sustainable Aviation. Vol. 1. No.4, 2015, p. 314–323.
50.	Iakovlieva A., Boichenko S., Lejda K.: Impact of rape oil ethyl esters additives on some characteristics of jet fuel. Проблеми хіммотології. Теорія та практика раціонального використання традиційних і альтернативних паливно -мастильних матеріалів: V міжнар. наук.-техн. конф., 6–10 жовт. 2014. Київ, c. 286 – 289. 
51.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Skilniuk I.: Vacuum Distillation of Rapeseed Oil Esters for Production of Jet Fuel Bio-Additives, Procedia Engineering, Vol. 187, 2017, p. 363 – 370. 
52.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Рotential of jet biofuels production and application in Ukraine and Poland. Proceedings of the 1st International Simposium on Sustainable Aviation.–31 May–03 June 2015, Isntanbul, p. 137.
53.	Iakovlieva A., Boichenko S., Lejda K.: Experimental study on antiwear properties for blends of jet fuel with biocomponents derived from rapeseed oil. Eastern-European journal of enterprise technologies. No. 5/8(77), 2015, p. 20–28.
54.	Iakovlieva A., Vovk O., Boichenko S.: Еxperimental study of rape oil esters influence on physical-chemical properties of jet fuels. Proceedings of the 19th Conference for Junior Researchers ‘Science – Future of Lithuania’ Тransport engineering and management, 6 May 2016, Vilnius. p. 85–89.
55.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Kuszewski H. Improvement of technological scheme of fatty acids ethyl esters production for use as jet fuels biocomponents. International Journal of Theoretical and Applied Science. Iss. 11(19), 2014, p. 44–55.
56.	International Air Transport organization. Vision 2050. Report. Montreal. Geneva, 2011.
57.	Jansen R. A.: Second Generation Biofuels and Biomass: Essential Guide for Investors, Scientists and Decision Makers. Wiley. 2012.
58.	Jenkins R.W., Munro M., Christopher S.N., Chuck C.: Potential renewable oxygenated biofuels for the aviation and road transport sectors. Fuel, Vol. 103, 2013, p. 593–599.
59.	Jacyna M., Żak J., Jacyna-Gołda I., Merkisz J., Merkisz-Guranowska A., Pielecha J.: Selected aspects of the model of proecological transport system. Journal of KONES Powertrain and Transport, Vol. 20, No. 3, 2013, p. 193 – 202.
60.	Kallio P., Pasztor A., Akhtar M.K., Jones P.R.: Renewable jet fuel. Current Opinion in Biotechnology. Vol. 26, 2014, p. 50–55.
61.	Kandaramath Hari T., Yaakob Z., Binitha N.N.: Aviation biofuel from renewable resources: Routes, opportunities and challenges. Renewable and Sustainable Energy Reviews. Vol. 42, 2015, p. 1234–1244. 
62.	Kinder J. D., Rahmes T.: Evaluation of Bio-Derived Synthetic Paraffinic Kerosene (Bio-SPK). The Boeing Company Sustainable Biofuels Research&Technology Program, 2009.
63.	Kirklin P.W., David. P.: Aviation Fuel: Thermal Stability. ASTM International, 1992.
64.	Lapuerta M., Rodriguez-Fernandeza J., Estevez C., Bayarri N.: Properties of fatty acid glycerol formal ester (FAGE) for use as a component in blends for diesel engines. Biomass and bioenergy. Vol. 76, 2015, p. 130–140. 
65.	Lebedevas S., Vaicekauskas A.: Research into the application of biodiesel in the transport sector of Lithuania. Transport. Vol. 21, Iss. 2, 2006, p. 80–87.
66.	Liu G., Yan B., Chen G.: Technical review on jet fuel production. Renewable and Sustainable Energy Reviews. Vol. 25, 2013, p. 59–70. 
67.	Lu M., Chai M.: Experimental Investigation of the Oxidation of Methyl Oleate: One of the Major Biodiesel Fuel Components Synthetic Liquids Production and Refining. Chapter 13, P. 289–312. American Chemical Society. 2011
68.	Merkisz J., Merkisz-Guranowska, A., Pielecha J., Nowak M., Jacyna M., Lewczuk K., Żak J.: Exhaust emission measurements in the development of sustainable road transport. Journal of KONES Powertrain and Transport, Vol. 20, No. 4 2013, p. 277 – 284.
69.	Maksimuk Yu., Antonova Z., Fes’ko V., Kursevich V.: Diesel biofuel viscosity and heat of combustion. Chemistry and technology of fuels and oils. Iss. 45, 2009, p. 343–346.
70.	Maru M. M., Trommer R.M., Cavalcanti K.F.: The Stribeck curve as a suitable characterization method of the lubricity of biodiesel and diesel blends. Energy. Vol. 69, 2014, p. 673–681.
71.	Maurice L.Q., Lander H., Edwards T., Harrison W.E.: Advanced aviation fuels: a look ahead via a historical perspective. Fuel. Vol. 80, Iss. 5, 2001, p. 747–756.
72.	Merkisz J., Markowski J., Pielecha J. Emission tests of the AI-14RA aircraft engine under real operating conditions of PZL-104"" Wilga"" plane. Silniki Spalinowe. No. 3, 2009, p. 64–70.
73.	Merkisz J., Galant M., Karpiński D., Kubiak, K. Evaluation of possibility to use the LTO cycle for emission test on example of the model turbine engine GTM-120 Journal of Mechanical and Transport Engineering. Vol. 66, No. 2, 2014, p. 25—33.
74.	Murphy D.J., Hall C.A.S.: Year in review—EROI or energy return on (energy) invested. Annals of the New York academy of sciences. Issue: Ecological Economics Reviews. Iss. 1185, 2010, p. 102–118.
75.	Murphy D.J., Hall C.A.S., Powers B.:New perspectives on the energy return on (energy) investment (EROI) of corn ethanol. Environment, Development and Sustainability. Vol. 13, Iss. 1, 2011, p. 179–202.
76.	Naik S.N., Goud V.V., Rout P.K., Dalai A.K.: Production of first and second generation biofuels: A comprehensive review. Renew. Sust. Energ. Rev., No. 14, 2010, p. 578–597.
77.	Nollet Leo M. L.: Handbook of Food Analysis: Physical characterization and nutrient analysis. CRC Press, 2004.
78.	Orszulik S.: Environmental Technology in the Oil Industry. Springer Science & Business Media, 2013.
79.	Pandey A.: Biofuels: Alternative Feedstocks and Conversion Processes. Academic Press, 2011.
80.	Pearlson M.N.: A techno-economic and environmental assessment of hydroprocessed renewable distillate fuels. Master of Science in Technology and Policy. Massachiussets Institute of Technology. June 2011.
81.	Prag P.: Renewable Energy in the Countryside. Taylor & Francis, 2014.
82.	Prussi M, Chiaramonti D, Recchia L, Martelli F, Guidotti F, Pari L.: Alternative feedstock for the biodiesel and energy production: the OVEST project. Energy Journal, No. 58, 2013, p. 2–8.
83.	Rahmes T.F., Kinder J.D., Henry T.M., etc.: Sustainable Bio-Derived Synthetic Paraffinic Kerosene (BioSPK) Jet Fuel Flights and Engine Tests Program Results. American Institute of Aeronautics and Astronautics, 2009.
84.	Rajagopal D., Zilberman D.: Environmental, Economic and Policy Aspects of Biofuels. Nеw Publishers Inc., 2008.
85.	Report on alternative fuels. International Air Transport Association IATA. http://www.iata.org/publications/Documents/2012-report-alternativefuels. pdf; 2012
86.	Rosillo Calle F, Trhan D, Seiffert M, Teeluckingh S. The potential and role of biofuels in commercial air transport – biojetfuels. Task 40 sustainable international bioenergy trade. IEA Bioenergy
87.	Sarin R., Kumar R., Srivastav B., etc.: Biodiesel surrogates: Achieving performance demands. Bioresource Technology. Vol. 100, Iss. 12, 2009, p. 3022–3028. 
88.	Shen Y.. Аn experimental study on thermal stability of FAEE biodiesel fuel with ethanol. Master Thesis, 2015.
89.	Shepherd J.E., Nuyt C.D., Lee J.J.: Flash Point and Chemical Composition of Aviation Kerosene (Jet A). National Transportation Safety Board, 2000.
90.	Singh B.: Biofuel Crops: Production, Physiology and Genetics. CABI, 2013.
91.	Singh B.: Biofuel Crop Sustainability. John Wiley & Sons, 2013.
92.	Sperling D., Cannon J.S.: Reducing Climate Impacts in the Transportation Sector. Springer Science & Business Media, 2011.
93.	Szczerek M., Tuszyсski W. Tribological researches – scuffing. Radom: Institute for Sustainable Technologies – National Research Institute, 2000.
94.	The jet engine. Rolls-Royce plc. Renault Printing Co Ltd., 1996.
95.	T-02U. Aparat czterokulowy – instrukcja obsługi. Radom: Wydawnictwo Instytutu Technologii Eksploatacji, 2011.
96.	Wcisło G.: Determination of the impact of FAME biocomponent on the fractional composition of diesel engine fuels. Combustion Engines. Iss. 154(3), 2013, p. 1098–1103.
97.	Xu Y., Wang Q., Hu X.: Characterization of the lubricity of bio-oil/diesel fuel blends by high frequency reciprocating test rig. Energy. Vol. 35, Iss. 1, 2010, p. 283–287. 
98.	Yakovleva A.V., Boichenko S.V., Lejda K, Vovk O.O., Kuszewski H.: Antiwear Properties of Plant—Mineral-Based Fuels for Airbreathing Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 1, 2017, p. 1–9. 
99.	Yakovlieva A.V., Boichenko S.V., Leida K., Vovk O.A., Kuzhevskii Kh.. Influence of Rapeseed Oil Ester Additives on Fuel Quality Index for Air Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 3, 2017. p. 308–317.
100.	Yakovlieva A., Boichenko S., Vovk O., Lejda K., Gryshchenko O.. Case Study of Alternative Jet Fuel Production with Bio-additives from Plant Oils in Ukraine and Poland. Advances in Sustainable Aviation. Springer International Publishing, 2018. Chapter 4.
101.	Yakovlieva A., Boshkov V. Experimental study of low-temperature properties of alternative aviation fuels, Proceedings of the 21th Conference for Junior Researchers ‘Science – Future of Lithuania’ Transport Engineering and Management, 4-5 May 2018, Vilnius, Lithuania. 2018. p. 130 – 134.
102.	Yildirim U, Abanteriba S.: Manufacture, qualification and approval of new aviation turbine fuels and additives, proceedia Engineering, No. 49, 2012, p. 310 – 315.
103.	Yutko B. and Hansman J., Approaches to Representing Aircraft Fuel Efficiency Performance for the Purpose of a Commercial Aircraft Certification Standard, MITInternational Center for Air Transportation, Cambridge, Mass, 2011.
104.	Zhu Y.: An Experimental Study on Thermal Stability of Biodiesel Fuel. Master Thesis. – 2012. – 160 p.
105.	Авиационный турбореактивный двигатель РУ 19A-300, руководство по эксплуатации и техническому обслуживанию, ЗАО «АНТЦ Технолог», 2001.
106.	Азев В.С., Середа А.В.: Влияние соединений серы на противоизносные свойства дизельных топлив, Химия и технология топлив и масел. № 3, 2009, c. 23–27.
107.	Андіїшин М.П., Марчук Я.С., Бойченко С.В., Рябоконь Л.А.: Газ природний, палива та оливи. Одеса: Астропринт, 2010.
108.	Бойченко С.В., Спіркін В.Г. Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт, Ч.1., 2009.
109.	Бойченко С.В., Любінін Й.А., Спіркін В.Г.: Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт. Ч.2., 2010.
110.	Бойченко С.В., Черняк Л.М., Яковлєва А.В.: Традиційні технології виробництва палив для повітряно-реактивних двигунів. Вісник Національного авіаційного університету. № 2 (55), 2013, с. 195–209.
111.	Бойченко С. В., Яковлева А. В., Волошинец В. А., Лейда К. Модифицирование эфиров рапсового масла вакуумным фракционированием, Технологии нефти и газа, №5, 2018, c. 15–20
112.	Братичак М.М.: Основи промислової нафтохімії, Львів: Вид-во НУ «Львівська політехніка», 2008.
113.	Васильев И.П.: Влияние топлив растительного происхождения на экологические и экономические показатели дизеля, Луганск: Изд-во ВНУ им. В. Даля, 2009.
114.	Волошинець В.А. Фізична та колоїдна хімія: Фізико-хімія дисперсних систем та полімерів: навч.посіб. Львів : Вид-во Львів. політехніки, 2013. – 200 с.
115.	Голоскоков А.Н. Критерии сравнения эффективности традиционных и альтернативных энергоресурсов. Нефтегазовое дело. № 1, 2011, c. 285–301.
116.	Голоскоков А.Н. Пик добычи нефти и начало мирового энергетического кризиса. Нефтегазовое дело. 2010, c. 1–13.
117.	Данилов А.М., Каминский Э.Ф., Хавкин В.А.: Альтернативные топлива: достоинства и недостатки. Проблемы применения. Российский химический журнал (Журнал Российского химического общества им. Д.И. Менделеева). Т. XLVII. № 6, 2003, c. 4–11.
118.	Дворецкий С.И., Нагорнов С.А., Романцова С.В. и др.: Производство биодизельного топлива из органического сырья. Вопросы современной науки и практики. № 39, 2012, c. 126– 35.
119.	Девянин С.Н., Марков В.А., Семенов В.Г.: Растительные масла и топлива на их основе для дизельных двигателей. Харьков: Новое слово. 2007.
120.	Ергин Д.: Добыча: Всемирная история борьбы за нефть, деньги и власть. Москва,: Альпина Паблишер, 2011.
121.	Запорожець А.О.: Дослідження стехіометричної суміші «повітря ‒ паливо» органічних сполук. Частина 1. Алкани. Наукоємні технології. № 2(22), 2014, c. 163–167.
122.	Кириченко В., Бойченко С., Кириченко В., Нездоровин В.: Комплексная переработка технических растительных масел: концепция, методы и технологи. «Systems and means of motor transport» Seria: Transport. Monografia. № 4, 2013, p. 357–370.
123.	Колодницька Р.В., Семенов В.Г.: Моделювання низькотемпературних властивостей біодизельних палив. Вісник СевНТУ. Серія: Машиноприладобудування та транспорт. № 134, 2012, c. 135–138.
124.	Коллоидная химия нефти и нефтепродуктов: Сборник материалов, посвященных научной деятельности проф. Г.И. Фукса. Москва: Изд-во «Техника». ООО «Тума Групп», 2001.
125.	Крылов И.Ф., Емельянов В.Е.: Альтернативные моторные топлива. Производство, применение",'National Aviation University',Modification of jet fuels composition with renewable bio-additives,10.18372/37895,https://core.ac.uk/download/322991098.pdf,,core
303023755,2019-01-29T00:00:00,"Conservation researchers require low-cost access to acoustic monitoring technology.
However, affordable tools are often constrained to short-term studies due to high energy consumption
and limited storage. To enable long-term monitoring, energy and space efficiency must be improved
on such tools. This paper describes the development and deployment of three acoustic detection
algorithms that reduce the power and storage requirements of acoustic monitoring on affordable,
open-source hardware. The algorithms aim to detect bat echolocation, to search for evidence of an
endangered cicada species, and also to collect evidence of poaching in a protected nature reserve.
The algorithms are designed to run on AudioMoth: a low-cost, open-source acoustic monitoring
device, developed by the authors and widely adopted by the conservation community. Each algorithm
addresses a detection task of increasing complexity, implementing extra analytical steps to account
for environmental conditions such as wind, analysing samples multiple times to prevent missed
events, and incorporating a hidden Markov model for sample classification in both the time and
frequency domain. For each algorithm, we report on real-world deployments carried out with partner
organisations and also benchmark the hidden Markov model against a convolutional neural network,
a deep-learning technique commonly used for acoustics. The deployments demonstrate how acoustic
detection algorithms extend the use of low-cost, open-source hardware and facilitate a new avenue
for conservation researchers to perform large-scale monitoring",'MDPI AG',"Deploying acoustic detection algorithms on low-cost, open-source acoustic sensors for environmental monitoring",10.3390/s19030553,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
334855451,2019-11-18T00:00:00,"Robustness of Deep Reinforcement Learning (DRL) algorithms towards
adversarial attacks in real world applications such as those deployed in
cyber-physical systems (CPS) are of increasing concern. Numerous studies have
investigated the mechanisms of attacks on the RL agent's state space.
Nonetheless, attacks on the RL agent's action space (AS) (corresponding to
actuators in engineering systems) are equally perverse; such attacks are
relatively less studied in the ML literature. In this work, we first frame the
problem as an optimization problem of minimizing the cumulative reward of an RL
agent with decoupled constraints as the budget of attack. We propose a
white-box Myopic Action Space (MAS) attack algorithm that distributes the
attacks across the action space dimensions. Next, we reformulate the
optimization problem above with the same objective function, but with a
temporally coupled constraint on the attack budget to take into account the
approximated dynamics of the agent. This leads to the white-box Look-ahead
Action Space (LAS) attack algorithm that distributes the attacks across the
action and temporal dimensions. Our results shows that using the same amount of
resources, the LAS attack deteriorates the agent's performance significantly
more than the MAS attack. This reveals the possibility that with limited
resource, an adversary can utilize the agent's dynamics to malevolently craft
attacks that causes the agent to fail. Additionally, we leverage these attack
strategies as a possible tool to gain insights on the potential vulnerabilities
of DRL agents.Comment: Version 2 with supplementary material",,"Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement
  Learning Agents",,http://arxiv.org/abs/1909.02583,,core
186276619,2019-07-24T00:00:00,"Multi-objective optimization is a crucial matter in computer systems design
space exploration because real-world applications often rely on a trade-off
between several objectives. Derivatives are usually not available or
impractical to compute and the feasibility of an experiment can not always be
determined in advance. These problems are particularly difficult when the
feasible region is relatively small, and it may be prohibitive to even find a
feasible experiment, let alone an optimal one.
  We introduce a new methodology and corresponding software framework,
HyperMapper 2.0, which handles multi-objective optimization, unknown
feasibility constraints, and categorical/ordinal variables. This new
methodology also supports injection of the user prior knowledge in the search
when available. All of these features are common requirements in computer
systems but rarely exposed in existing design space exploration systems. The
proposed methodology follows a white-box model which is simple to understand
and interpret (unlike, for example, neural networks) and can be used by the
user to better understand the results of the automatic search.
  We apply and evaluate the new methodology to the automatic static tuning of
hardware accelerators within the recently introduced Spatial programming
language, with minimization of design run-time and compute logic under the
constraint of the design fitting in a target field-programmable gate array
chip. Our results show that HyperMapper 2.0 provides better Pareto fronts
compared to state-of-the-art baselines, with better or competitive hypervolume
indicator and with 8x improvement in sampling budget for most of the benchmarks
explored.Comment: 12 pages, MASCOTS 2019 conference
  (https://sites.google.com/view/mascots-2019",,Practical Design Space Exploration,,http://arxiv.org/abs/1810.05236,,core
477767348,2019-11-08T08:00:00,"This thesis presents a total of 3 groups of contributions related to multi-objective optimization.  The first group includes the development of a new algorithm and an open-source user-friendly package for optimization over the efficient set for bi-objective mixed integer linear programs.  The second group includes an application of a special case of optimization over the efficient on conservation planning problems modeled with modern portfolio theory. Finally, the third group presents a machine learning framework to enhance criterion space search algorithms for multi-objective binary linear programming.
In the first group of contributions, this thesis presents the first (criterion space search) algorithm for optimizing a linear function over the set of efficient solutions of bi-objective mixed integer linear programs. The proposed algorithm is developed based on the triangle splitting method (Boland et al.), which can find a full representation of the nondominated frontier of any bi-objective mixed integer linear program. The proposed algorithm is easy to understand and implement, and converges quickly to an optimal solution. An extensive computational study shows the efficacy of the algorithm. Is numerically shown in this thesis that the proposed algorithm can be used to quickly generate a provably high-quality approximate solution because it maintains a lower and an upper bound on the optimal value of the linear function at any point in time. Additionally, this thesis presents OOESAlgorithm.jl, a comprehensive julia package based on the proposed algorithm. The proposed package ex- tends the first implementation of the algorithm by adding two main features: (a) in addition to CPLEX, the package allows employing any single-objective solver supported by Math- ProgBase.jl, for example, GLPK, CPLEX, and SCIP; (b) the package supports execution
on multiple processors and is compatible with the JuMP modeling language. An extensive computational study shows the efficacy of the package and its features.
In the  second group of contributions, this thesis presents a Nash bargaining solu- tion approach for spatial conservation planning problems modeled with modern portfolio theory.  The proposed modern portfolio optimization formulation corresponds to a spatial conservation planning problem involving two conflicting objectives: maximizing return and minimizing risk. A Nash bargaining solution approach is presented in this thesis to directly compute a desirable Pareto-optimal (nondominated) solution for the proposed bi-objective optimization formulation in natural resource management problems. Numerical examples in this thesis show that to directly compute a Nash bargaining solution, a Binary Quadratically Constrained Quadratic Program (BQCQP) can be solved. This thesis also shows that the proposed approach (implementable with commercial  solvers such as CPLEX) can effectively solve the proposed BQCQP for much larger problems than previous approaches published in the ecological literature.  The new approach expands considerably the applicability of such optimization methods to address real spatial conservation planning problems.
In the third group of contributions, this thesis investigates the possibility of improving the performance of multi-objective optimization solution approaches using machine learning techniques. Specifically, this thesis focus on multi-objective binary linear programs and employs one of the most effective and recently developed criterion space search algorithms, the so-called KSA, during our study.  This algorithm computes all nondominated points of a problem with p objectives by searching on a projected criterion space, i.e., a (p − 1)- dimensional criterion space.  This thesis presents an effective and fast learning approach to identify on which projected space the KSA should work, and also presents several generic features that can be used in machine learning techniques for identifying the best-projected space. Finally, a bi-objective optimization-based heuristic for selecting the best subset of the features to overcome the issue of overfitting in learning is presented. Through an extensive computational study, the performance of the proposed learning approach is tested",Digital Commons @ University of South Florida,Algorithms for Multi-Objective Mixed Integer Programming Problems,,,,core
323458821,2019-10-01T00:00:00,"La stimulation cérébrale profonde (SCP) constitue un traitement chirurgical validé pour certaines formes de maladie de Parkinson, de tremblement essentiel ou de dystonies.La principale étape de cette procédure est le ciblage de la structure cérébrale dans laquelle sera délivré le courant par les électrodes implantées (cible). Les cibles de la SCP sont de l’ordre du millimètre, correspondant à des sous-parties de noyaux gris centraux (noyaux sous-thalamique – NST, globus pallidus interne – GPi, noyau ventral-intermédiaire du thalamus – VIM) ou à des régions autour de ces noyaux dans lesquelles transitent les faisceaux de fibres blanches à destination de ceux-ci. L’imagerie par résonnance magnétique (IRM) permet de visualiser certains de ces noyaux, mais avec une résolution insuffisante pour guider avec précision l’implantation des électrodes pour ce qui est du STN et du VIM, rendant pour certains auteurs l’électrophysiologie peropératoire indispensable. D’autre part, la définition anatomique des cibles est sujette à controverses et la nature même de la structure visée varie entre les différents centres. Ces éléments constituent des sources d’erreur dans le ciblage et peuvent rendre compte de l’absence d’efficacité de la procédure, ou de son efficacité partielle, chez certains patients. L’objectif de ce travail était d’optimiser le ciblage en SCP en définissant une cible non pas anatomique mais fonctionnelle : pour un patient donné, trouver la position d’une cible dont la stimulation aboutira à un excellent résultat clinique.Pour cela, nous avons résolu un problème inverse, grâce à des méthodes d’apprentissage statistique. La base d’entrainement était constituée par la position des électrodes implantées chez des patients ayant un excellent résultat clinique post-opératoire d’une part, et la position de structures anatomiques avoisinantes visibles sur une IRM à 1,5Tesla chez ces mêmes patients, d’autre part. Trois approches d’apprentissage ont été utilisées : la régression de type RKHS, puis les SVR (support vector régression) et les réseaux de neurones (apprentissage profond). 15 patients atteints d’un tremblement essentiel (29 électrodes) opérés avec un excellent résultat ont été inclus pour la définition d’une cible « VIM ». 18 points de repères par hémisphère ont été définis dans la région des noyaux gris centraux.Les modèles de prédiction ont été validés en calculant la distance euclidienne entre la cible prédite et la cible « réelle », à savoir le centre du contact actif de l’électrode implantée. Ensembles d’apprentissage et de validation étaient partitionnés de manière itérative selon la méthode de validation croisée type leave-one-out. Nous avons également normalisé la position des contacts actifs et des cibles prédites sur un cerveau moyen (MNI template) et avons calculé la distance minimale entre la cible prédite et le VIM donné par un atlas (Ewert) normalisé sur ce template, d’une part, et entre le contact actif et le VIM de cet atlas d’autre part. Nous avons ainsi pu comparer les distance cibles prédites – VIM et contact actif – VIM.En parallèle, nous avons développé un logiciel (Optim DBS), permettant de visualiser directement la cible prédite à partir des points de repères sur l’IRM de n’importe quel patient devant être opéré.Enfin, nous avons mis en place et démarré une étude prospective multicentrique permettant de valider la cible « VIM » sur le tremblement essentiel. Il est prévu d’inclure22 patients en 2 ans et de les opérer sous anesthésie générale sans électrophysiologie peropératoire en utilisant la cible développée dans ce travail pour implanter l’électrode.Deep brain stimulation (DBS) is a surgical treatment for some forms of Parkinson's disease, essential tremor and dystonia. The main step in this procedure is the targeting of the brain structure in which the current will be delivered by the implanted electrodes (target). Targets of the SCP are of the order of a millimeter, corresponding to sub-parts of basal ganglia (subthalamic nucleus - STN, globus pallidus internal - GPi, ventral intermediate nucleus of the thalamus - VIM) or regions around these nuclei in which pass the white fibers destined for these nuclei. Magnetic resonance imaging (MRI) allows viewing some of these nuclei, but with insufficient resolution to guide accurate implantation of electrodes to the STN and the VIM, making for some authors essential intraoperative electrophysiology. On the other hand, the anatomic target definition is controversial and the nature of the target structure varies between different centers. These elements are sources of error in targeting and can account for the lack of efficiency of the surgery, or its partial effectiveness in some patients. The objective of this work was to optimize targeting in DBS by setting a functional target and non-anatomically: for a given patient, to find the position of a target whose stimulation will lead to an excellent clinical outcome. For this, we resolved a reverse problem through statistical learning methods. The training base was formed by the position of the electrodes implanted in patients with an excellent postoperative clinical result on the one hand, and the position of anatomical structures nearby visible on an MRI at 1.5 Tesla in these same patients, on the other hand. We used three machine-learning approches: RKHS (Reproducing Kernel Hilbert Space), SVR (support vector regression) as well as deep neural networks. 15 patients with an essential tremor (29 electrodes) operated with an excellent result have been included to the definition of a 'VIM' target. 18 points of reference by hemisphere have been defined in the region of the basal ganglia. The prediction model has been validated by calculating the Euclidean distance between the predicted target and the 'real' target distance, which is the center of the active contact of the implanted electrode. The validation was done according to leave-one-out cross-validation approach. We also normalized the position of active contacts and targets predicted on an average brain (MNI template) and have calculated the minimum distance between the predicted target and the VIM given by an atlas (Ewert) normalized on this template, on the one hand, and between the active contact and the VIM of this atlas on the other hand. We were able to compare the distances predicted targets - VIM and active contact - VIM. In parallel, we developed a software (OptimDBS), to visualize directly the target predicted from landmarks on the MRI of any patient to be operated on. Finally, we set up and started a multi-center prospective study to validate the ""VIM"" target on essential tremor. It is planned to include 22 patients in 2 years who will be operated under general anesthesia without intraoperative electrophysiology using the target developed in this work to implant the electrode",,Optimization of the targeting of basal ganglia in stereotactic neurosurgery,,,,core
335618006,2019-08-01T00:00:00,"[EN] In this paper, we detail why the stack smashing protector (SSP), one of the most effective techniques to mitigate stack bufferoverflow attacks, fails to protect the Android operating system and thus causes a false sense of security that affects all Androiddevices. We detail weaknesses of existing SSP implementations, revealing that current SSP is not secure. We propose SSPFA,the first effective and practical SSP for Android devices. SSPFA provides security against stack buffer overflows withoutchanging the underlying architecture. SSPFA has been implemented and tested on several real devices showing that it is notintrusive, and it is binary-compatible with Android applications. Extensive empirical validation has been carried out over theproposed solution.This work was partially funded by Universitat Politecnica de Valencia (Grant No. 20160251-ASLR-NG).Marco Gisbert, H.; Ripoll Ripoll, JI. (2019). SSPFA: Effective Stack Smashing Protection for Android OS. International Journal of Information Security. 18(4):519-532. https://doi.org/10.1007/s10207-018-00425-8S519532184Buchanan, W.J., Chiale, S., Macfarlane, R.: A methodology for the security evaluation within third-party android marketplaces. Digit. Investig. 23(Supplement C), 88–98 (2017). https://doi.org/10.1016/j.diin.2017.10.002Tian, D., Jia, X., Chen, J., Hu, C., Xue, J.: A practical online approach to protecting kernel heap buffers in kernel modules. China Commun. 1, 143–152 (2016)One, A.: Smashing the stack for fun and profit. Phrack, 7(49) (1996)Younan, Y., Pozza, D., Piessens, F., Joosen, W.: Extended protection against stack smashing attacks without performance loss. In: In Proceedings of ACSAC (2006)Abadi, M., Budiu, M., Erlingsson, U., Ligatti, J.: Control-flow Integrity. In: Proceedings of the 12th ACM Conference on Computer and Communications Security, Series CCS ’05, pp. 340–353. ACM, New York (2005). https://doi.org/10.1145/1102120.1102165Wartell, R., Mohan, V., Hamlen, K.W., Lin, Z.: Binary stirring: self-randomizing instruction addresses of legacy x86 binary code. In: Proceedings of the 2012 ACM Conference on Computer and Communications Security, Series CCS ’12, pp. 157–168. ACM, New York (2012). https://doi.org/10.1145/2382196.2382216Roglia, G.F., Martignoni, L., Paleari, R., Bruschi, D.: Surgically returning to randomized lib(c). In: Proceedings of the 2009 Annual Computer Security Applications Conference, Series ACSAC ’09, pp. 60–69. IEEE Computer Society, Washington (2009). https://doi.org/10.1109/ACSAC.2009.16Roemer, R., Buchanan, E., Shacham, H., Savage, S.: Return-oriented programming: systems, languages, and applications. ACM Trans. Inf. Syst. Secur. 15(1), 2:1–2:34 (2012). https://doi.org/10.1145/2133375.2133377Pappas, V., Polychronakis, M., Keromytis, A.: Smashing the gadgets: hindering return-oriented programming using in-place code randomization. In: 2012 IEEE Symposium on Security and Privacy (SP), pp. 601–615 (2012)S. R. to Thwart Return Oriented Programming in Embedded Systems, Stack Redundancy to Thwart Return Oriented Programming in Embedded Systems, IEEE Embedded Systems Letters, vol. (first on-line), pp. 1–1 (2018)Moula, V., Niksefat, S.: ROPK++: an enhanced ROP attack detection framework for Linux operating system. In: International Conference on Cyber Security And Protection Of Digital Services (Cyber Security). IEEE (2017)Das, S., Zhang, W., Liu, Y.: A fine-grained control flow integrity approach against runtime memory attacks for embedded systems. IEEE Trans. Very Large Scale Integr. VLSI Syst. 25, 3193–3207 (2016)Alam, M., Roy, D.B., Bhattacharya, S., Govindan, V., Chakraborty, R.S., Mukhopadhyay, D.: SmashClean: a hardware level mitigation to stack smashing attacks in OpenRISC. In: ACM/IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE), pp. 1–4. IEEE (2016)Kananizadeh, S., Kononenko, K.: Development of dynamic protection against timing channels. Int. J. Inf. Secur. 16, 641–651 (2017)Bhatkar, S., DuVarney, D.C., Sekar, R.: Address obfuscation: an efficient approach to combat a board range of memory error exploits. In: Proceedings of the 12th Conference on USENIX Security Symposium—volume 12, Series SSYM’03, p. 8. USENIX Association, Berkeley (2003). http://dl.acm.org/citation.cfm?id=1251353.1251361 . Accessed 18 Jan 2019Snow, K.Z., Monrose, F., Davi, L., Dmitrienko, A., Liebchen, C., Sadeghi, A.-R.: Just-in-time code reuse: on the effectiveness of fine-grained address space layout randomization. In: 2013 IEEE Symposium on Security and Privacy (SP), pp. 574–588. IEEE (2013)Kumar, K.S., Kisore, N.R.: Protection against buffer overflow attacks through runtime memory layout randomization. In: International Conference on Information Technology (ICIT). IEEE (2014)Oberheide, J.: A look at ASLR in Android ice cream sandwich 4.0 (2012). https://www.duosecurity.com/blog/a-look-at-aslr-in-android-ice-cream-sandwich-4-0 . Accessed 18 Jan 2019Zabrocki, A.P.: Scraps of notes on remote stack overflow exploitation (2010). http://www.phrack.org/issues.html?issue=67&id=13#article . Accessed 18 Jan 2019Saito, T., Watanabe, R., Kondo, S., Sugawara, S., Yokoyama, M.: A survey of prevention/mitigation against memory corruption attacks. In: 19th International Conference on Network-Based Information Systems (NBiS). IEEE (2016)Meike, G.B.: Inside the Android OS: Building, Customizing, Managing and Operating Android System Services, illustrated ed., P. Education, Ed. Pearson Education, vol. 1 (2018). https://www.amazon.com/Inside-Android-OS-Customizing-Operating/dp/0134096347?SubscriptionId=0JYN1NVW651KCA56C102&tag=techkie-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=0134096347 . Accessed 18 Jan 2019Cowan, C., Pu, C., Maier, D., Hintongif, H., Walpole, J., Bakke, P., Beattie, S., Grier, A., Wagle, P., Zhang, Q.: StackGuard: automatic adaptive detection and prevention of buffer-overflow attacks. In: Proceedings of the 7th USENIX Security Symposium, pp. 63–78 (1998)’xorl’: Linux GLibC stack canary values (2010). http://xorl.wordpress.com/2010/10/14/linux-glibc-stack-canary-values/ . Accessed 18 Jan 2019Lee, B., Lu, L., Wang, T., Kim, T., Lee, W.: From zygote to morula: fortifying weakened ASLR on Android. In: Proceedings of the 2014 IEEE Symposium on Security and Privacy, Series SP ’14, pp. 424–439. IEEE Computer Society, Washington (2014). https://doi.org/10.1109/SP.2014.34Miller, D.: Security measures in OpenSSH (2007). http://www.openbsd.org/papers/openssh-measures-asiabsdcon2007-slides.pdf . Accessed 18 Jan 2019Molnar, I.: Exec shield, new Linux security feature (2003). https://lwn.net/Articles/31032/ . Accessed 18 Jan 2019Wagle, P., Cowan, C.: StackGuard: simple stack smash protection for GCC. In: Proceedings of the GCC Developers Summit, pp. 243–256 (2003)Etoh, H.: GCC extension for protecting applications from stack-smashing attacks (ProPolice) (2003). http://www.trl.ibm.com/projects/security/ssp/ . Accessed 18 Jan 2019Erb, C., Collins, M., Greathouse, J. L.: Dynamic buffer overflow detection for GPGPUs. In: IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp. 61–73 IEEE (2017)Molnar, I.: Stackprotector updates for v3.14 (2014). https://lwn.net/Articles/584278/Shen, H.: Add a new option “-fstack-protector-strong” (2012). http://gcc.gnu.org/ml/gcc-patches/2012-06/msg00974.html . Accessed 18 Jan 2019Guan, X., Ji, J., Jiang, J., Zhang, S.: Stack overflow protection device, method, and related compiler and computing device, August 22 2013, uS Patent App. 13/772,858. https://www.google.com/patents/US20130219373 . Accessed 18 Jan 2019Backes, M., Bugiel, S., Derr, E.: Reliable third-party library detection in Android and its security applications. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Series. CCS ’16, pp. 356–367. ACM, New York (2016)Greenberg, A.: SC magazine: trojanized Android apps steal authentication tokens, put accounts at risk (2014). www.scmagazine.com/trojanized-android-apps-steal-authentication-tokens-put-accounts-at-risk/article/342208/Enck, W., Octeau, D., McDaniel, P., Chaudhuri, S.: A study of android application security. In: Proceedings of the 20th USENIX Conference on Security, Series SEC’11, pp. 21–21. USENIX Association, Berkeley (2011) http://dl.acm.org/citation.cfm?id=2028067.2028088 . Accessed 18 Jan 2019Poll: How often do you reboot? (2014). http://www.androidcentral.com/poll-how-often-do-you-reboot . Accessed 18 Jan 2019Wang, H., Li, H., Li, L., Guo, Y., Xu, G.: Why are android apps removed from Google play? A large-scale empirical study. In Proceedings of the 15th International Conference on Mining Software Repositories, Series MSR ’18, pp. 231–242. ACM, New York (2018). http://doi.acm.org/10.1145/3196398.3196412Marco-Gisbert, H., Ripoll, I.: Preventing brute force attacks against stack canary protection on networking servers. In: 12th International Symposium on Network Computing and Applications, pp. 243–250 (2013)Petsios, T., Kemerlis, V.P., Polychronakis, M., Keromytis, A.D.: DynaGuard: armoring canary-based protections against brute-force attacks. In: Proceedings of the 31st Annual Computer Security Applications Conference, Series ACSAC 2015, pp. 351–360. ACM, New York (2015). http://doi.acm.org/10.1145/2818000.281803",'Springer Science and Business Media LLC',SSPFA: Effective Stack Smashing Protection for Android OS,10.1007/s10207-018-00425-8,https://riunet.upv.es/bitstream/10251/150623/1/Marco%3bRipoll%20-%20SSPFA%3a%20Effective%20Stack%20Smashing%20Protection%20for%20Android%20OS.pdf,,core
200819689,2019-04-01T00:00:00,"Understanding the spatial arrangement and nature of real-world objects is of
paramount importance to many complex engineering tasks, including autonomous
navigation. Deep learning has revolutionized state-of-the-art performance for
tasks in 3D environments; however, relatively little is known about the
robustness of these approaches in an adversarial setting. The lack of
comprehensive analysis makes it difficult to justify deployment of 3D deep
learning models in real-world, safety-critical applications. In this work, we
develop an algorithm for analysis of pointwise robustness of neural networks
that operate on 3D data. We show that current approaches presented for
understanding the resilience of state-of-the-art models vastly overestimate
their robustness. We then use our algorithm to evaluate an array of
state-of-the-art models in order to demonstrate their vulnerability to
occlusion attacks. We show that, in the worst case, these networks can be
reduced to 0% classification accuracy after the occlusion of at most 6.5% of
the occupied input space.Comment: 10 pages, 8 figures, 1 tabl",,Robustness of 3D Deep Learning in an Adversarial Setting,,http://arxiv.org/abs/1904.00923,,core
334876638,2019-10-30T00:00:00,"Semantic understanding of scenes in three-dimensional space (3D) is a
quintessential part of robotics oriented applications such as autonomous
driving as it provides geometric cues such as size, orientation and true
distance of separation to objects which are crucial for taking mission critical
decisions. As a first step, in this work we investigate the possibility of
semantically classifying different parts of a given scene in 3D by learning the
underlying geometric context in addition to the texture cues BUT in the absence
of labelled real-world datasets. To this end we generate a large number of
synthetic scenes, their pixel-wise labels and corresponding 3D representations
using CARLA software framework. We then build a deep neural network that learns
underlying category specific 3D representation and texture cues from color
information of the rendered synthetic scenes. Further on we apply the learned
model on different real world datasets to evaluate its performance. Our
preliminary investigation of results show that the neural network is able to
learn the geometric context from synthetic scenes and effectively apply this
knowledge to classify each point of a 3D representation of a scene in
real-world.Comment: Accepted in 3rd Edition of Deep Learning for Automated Driving (DLAD)
  workshop, IEEE International Conference on Intelligent Transportation Systems
  (ITSC'19) [see
  https://sites.google.com/view/dlad-bp-itsc2019/schedule?authuser=0#h.p_gI84BCoB0_bJ",,Multi Modal Semantic Segmentation using Synthetic Data,,http://arxiv.org/abs/1910.13676,,core
334869946,2019-11-06T00:00:00,"While a wide range of interpretable generative procedures for graphs exist,
matching observed graph topologies with such procedures and choices for its
parameters remains an open problem. Devising generative models that closely
reproduce real-world graphs requires domain knowledge and time-consuming
simulation. While existing deep learning approaches rely on less manual
modelling, they offer little interpretability. This work approaches graph
generation (decoding) as the inverse of graph compression (encoding). We show
that in a disentanglement-focused deep autoencoding framework, specifically
Beta-Variational Autoencoders (Beta-VAE), choices of generative procedures and
their parameters arise naturally in the latent space. Our model is capable of
learning disentangled, interpretable latent variables that represent the
generative parameters of procedurally generated random graphs and real-world
graphs. The degree of disentanglement is quantitatively measured using the
Mutual Information Gap (MIG). When training our Beta-VAE model on ER random
graphs, its latent variables have a near one-to-one mapping to the ER random
graph parameters n and p. We deploy the model to analyse the correlation
between graph topology and node attributes measuring their mutual dependence
without handpicking topological properties.Comment: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Workshop on Graph Representation Learnin",,"Disentangling Interpretable Generative Parameters of Random and
  Real-World Graphs",,http://arxiv.org/abs/1910.05639,,core
186311375,2019-01-06T00:00:00,"Many datacenter applications such as machine learning and streaming systems
do not need the complete set of data to perform their computation. Current
approximate applications in datacenters run on a reliable network layer like
TCP. To improve performance, they either let sender select a subset of data and
transmit them to the receiver or transmit all the data and let receiver drop
some of them. These approaches are network oblivious and unnecessarily transmit
more data, affecting both application runtime and network bandwidth usage. On
the other hand, running approximate application on a lossy network with UDP
cannot guarantee the accuracy of application computation. We propose to run
approximate applications on a lossy network and to allow packet loss in a
controlled manner. Specifically, we designed a new network protocol called
Approximate Transmission Protocol, or ATP, for datacenter approximate
applications. ATP opportunistically exploits available network bandwidth as
much as possible, while performing a loss-based rate control algorithm to avoid
bandwidth waste and re-transmission. It also ensures bandwidth fair sharing
across flows and improves accurate applications' performance by leaving more
switch buffer space to accurate flows. We evaluated ATP with both simulation
and real implementation using two macro-benchmarks and two real applications,
Apache Kafka and Flink. Our evaluation results show that ATP reduces
application runtime by 13.9% to 74.6% compared to a TCP-based solution that
drops packets at sender, and it improves accuracy by up to 94.0% compared to
UDP",,ATP: a Datacenter Approximate Transmission Protocol,,http://arxiv.org/abs/1901.01632,,core
83852823,2019-02-20T00:00:00,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.Comment: ICLR 2019, further information and videos at
  https://ge.in.tum.de/publications/2017-prantl-defonn",,Generating Liquid Simulations with Deformation-aware Neural Networks,,http://arxiv.org/abs/1704.07854,,core
200412086,2019-05-10T05:09:29Z,"<p>The control of soft continuum robots is challenging owing to their mechanical elasticity and complex dynamics. An additional challenge emerges when we want to apply Learning from Demonstration (LfD) and need to collect necessary demonstrations due to the inherent control difficulty. In this paper, we provide a multi-level architecture from low-level control to high-level motion planning for the Bionic Handling Assistant (BHA) robot. We deploy learning across all levels to enable the application of LfD for a real-world manipulation task. To record the demonstrations, an actively compliant controller is used. A variant of dynamical systems' application that are able to encode both position and orientation then maps the recorded 6D end-effector pose data into a virtual attractor space. A recent LfD method encodes the pose attractors within the same model for point-to-point motion planning. In the proposed architecture, hybrid models that combine an analytical approach and machine learning techniques are used to overcome the inherent slow dynamics and model imprecision of the BHA. The performance and generalization capability of the proposed multi-level approach are evaluated in simulation and with the real BHA robot in an apple-picking scenario which requires high accuracy to control the pose of the robot's end-effector.</p",,Multi-level control architecture for Bionic Handling Assistant robot augmented by learning from demonstration for apple-picking,10.6084/m9.figshare.8107610.v1,,,core
286035275,2019-01-01T00:00:00,"To retrieve aerosol properties from satellite measurements of the oxygen A-band in the near-infrared, a line-by-line radiative transfer model implementation requires a large number of calculations. These calculations severely restrict a retrieval algorithm's operational capability as it can take several minutes to retrieve the aerosol layer height for a single ground pixel. This paper proposes a forward modelling approach using artificial neural networks to speed up the retrieval algorithm. The forward model outputs are trained into a set of neural network models to completely replace line-by-line calculations in the operational processor. Results comparing the forward model to the neural network alternative show an encouraging outcome with good agreement between the two when they are applied to retrieval scenarios using both synthetic and real measured spectra from TROPOMI (TROPOspheric Monitoring Instrument) on board the European Space Agency (ESA) Sentinel-5 Precursor mission. With an enhancement of the computational speed by 3 orders of magnitude, TROPOMI's operational aerosol layer height processor is now able to retrieve aerosol layer heights well within operational capacity.Atmospheric Remote Sensin",'Copernicus GmbH',A neural network radiative transfer model approach applied to the Tropospheric Monitoring Instrument aerosol height algorithm,10.5194/amt-12-6619-2019,,,core
395096808,2019-12-30T00:00:00,"Robotics systems are now increasingly widespread in our day-life. For instance, robots have been successfully used in several fields, like, agriculture, construction, defense, aerospace, and hospitality. However, there are still several issues to be addressed for allowing the large scale deployment of robots. Issues related to security, and manufacturing and operating costs are particularly relevant. Indeed, differently from industrial applications, service robots should be cheap and capable of operating in unknown, or partially-unknown environments, possibly with minimal human intervention. To deal with these challenges, in the last years the research community focused on deriving learning algorithms capable of providing flexibility and adaptability to the robots. In this context, the application of Machine Learning and Reinforcement Learning techniques turns out to be especially useful. In this manuscript, we propose different learning algorithms for robotics systems. In Chapter 2, we propose a solution for learning the geometrical model of a robot directly from data, combining proprioceptive measures with data collected with a 2D camera. Besides testing the accuracy of the kinematic models derived with real experiments, we validate the possibility of deriving a kinematic controller based on the model identified. Instead, in Chapter 3, we address the robot inverse dynamics problem. Our strategy relies on the fact that the robot inverse dynamics is a polynomial function in a particular input space. Besides characterizing the input space, we propose a data-driven solution based on Gaussian Process Regression (GPR). Given the type of each joint, we define a kernel named Geometrically Inspired Polynomial (GIP) kernel, which is given by the product of several polynomial kernels. To cope with the dimensionality of the resulting polynomial, we use a variation of the standard polynomial kernel, named Multiplicative Polynomial kernel, further discussed in Chapter 6. Tests performed on simulated and real environments show that, compared to other data-driven solutions, the GIP kernel-based estimator is more accurate and data-efficient.

In Chapter 4, we propose a proprioceptive collision detection algorithm based on GPR. Compared to other proprioceptive approaches, we closely inspect the robot behaviors in quasi-static configurations, namely, configurations in which joint velocities are null or close to zero. Such configurations are particularly relevant in the Collaborative Robotics context, where humans and robots work side-by-side sharing the same environment. Experimental results performed with a UR10 robot confirm the relevance of the problem and the effectiveness of the proposed solution.

Finally, in Chapter 5, we present MC-PILCO, a model-based policy search algorithm inspired by the PILCO algorithm. As the original PILCO algorithm, MC-PILCO models the system evolution relying on GPR, and improves the control policy minimizing the expected value of a cost function. However, instead of approximating the expected cost by moment matching, MC-PILCO approximates the expected cost with a Monte Carlo particle-based approach; no assumption about the type of GPR model is necessary. Thus, MC-PILCO allows more freedom in designing the GPR models, possibly leading to better models of the system dynamics. Results obtained in a simulated environment show consistent improvements with respect to the original algorithm, both in terms of speed and success rate",,Learning algorithms for robotics systems,,,,core
237080308,2019-01-01T00:00:00,"Intelligent robotic coworkers are considered a valuable addition in many application areas. This applies not only to terrestrial domains, but also to the exploration of our solar system. As humankind moves toward an ever increasing presence in space, infrastructure has to be constructed and maintained on distant planets such as Mars. AI-enabled robots will play a major role in this scenario. The space agencies envisage robotic co-workers to be deployed to set-up habitats, energy, and return vessels for future human scientists. By leveraging AI planning methods, this vision has already become one step closer to reality. In the METERON SUPVIS Justin experiment, the intelligent robotic coworker Rollin’ Justin was controlled from Astronauts aboard the International Space Station (ISS) in order to maintain a Martian mock-up solar panel farm located on Earth to demonstrate the technology readiness of the developed methods. For this work, the system is demonstrated at AAAI 2019, controlling Rollin’ Justin located in Munich, Germany from Honolulu, Hawaii",,Global Remote Operation of Intelligent Space Robot Assistants,,https://core.ac.uk/download/237080308.pdf,,core
304117829,2019-01-01T00:00:00,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search",'Institute of Electrical and Electronics Engineers (IEEE)',Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,10.23919/DATE.2019.8714959,https://core.ac.uk/download/304117829.pdf,,core
375437035,2019-12-01T00:00:00,"Big data analytics is a virtually new term in power system terminology. This concept delves into the way a massive volume of data is acquired, processed, analyzed to extract insight from available data. In particular, big data analytics alludes to applications of artificial intelligence, machine learning techniques, data mining techniques, time-series forecasting methods. Decision-makers in power systems have been long plagued by incapability and weakness of classical methods in dealing with large-scale real practical cases due to the existence of thousands or millions of variables, being time-consuming, the requirement of a high computation burden, divergence of results, unjustifiable errors, and poor accuracy of the model. Big data analytics is an ongoing topic, which pinpoints how to extract insights from these large data sets. The extant article has enumerated the applications of big data analytics in future power systems through several layers from grid-scale to local-scale. Big data analytics has many applications in the areas of smart grid implementation, electricity markets, execution of collaborative operation schemes, enhancement of microgrid operation autonomy, management of electric vehicle operations in smart grids, active distribution network control, district hub system management, multi-agent energy systems, electricity theft detection, stability and security assessment by PMUs, and better exploitation of renewable energy sources. The employment of big data analytics entails some prerequisites, such as the proliferation of IoT-enabled devices, easily-accessible cloud space, blockchain, etc. This paper has comprehensively conducted an extensive review of the applications of big data analytics along with the prevailing challenges and solutions",'Institute of Electrical and Electronics Engineers (IEEE)',Attributes of Big Data Analytics for Data-Driven Decision Making in Cyber-Physical Power Systems,10.1109/ipaps49326.2019.9069391,https://core.ac.uk/download/375437035.pdf,,core
200862582,2019-04-01T00:00:00,"As the Internet-of-Things (IoT) and edge computing have been major paradigms for distributed data collection, communication, and processing, smart city applications in the real world tend to adopt IoT and edge computing broadly. Today, more and more machine learning algorithms would be deployed into front-end sensors, devices, and edge data centres rather than centralised cloud data centres. However, front-end sensors and devices are usually not so capable as those computing units in huge data centres, and for this sake, in practice, engineers choose to compromise for limited capacity of embedded computing and limited memory, e.g., neural network models being pruned to fit embedded devices. Visual object tracking is one of many important elements of a smart city, and in the IoT and edge computing context, high requirements to computing power and memory space severely prevent massive and accurate tracking. In this paper, we report on our contribution to object tracking on lightweight computing including (1) using limited computing capacity and memory space to realise tracking; (2) proposing a new algorithm region proposal correlation filter fitting for most edge devices. Systematic evaluations show that (1) our techniques can fit most IoT devices; (2) our techniques can keep relatively high accuracy; and (3) the generated model size is much less than others",'MDPI AG',Object Tracking for a Smart City Using IoT and Edge Computing,10.3390/s19091987,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
226964524,2019,"Adaptive Human-Machine Interfaces and Interactions (HMI2) are closed-loop cyber-physical systems comprising a network of sensors measuring human, environmental and mission parameters, in conjunction with suitable software for adapting the HMI2 (command, control and display functions) in response to these real-time measurements. Cognitive HMI2 are a particular subclass of these systems, which support dynamic HMI2 adaptations based on the user&#039;s cognitive state. These states are estimated in real-time using various neuro-physiological parameters from gaze, cardiorespiratory and brain signals, which are processed by an Adaptive Neuro-Fuzzy Inference System (ANFIS). However, the accuracy and precision of neuro-physiological measurements are affected by a variety of environmental factors and therefore need to be accurately characterised prior to operational use. This paper describes the characterisation activities performed on two types of eye tracking devices used in the Aerospace Intelligent and Autonomous Systems (AIAS) laboratory of RMIT University to support the development of cognitive human-machine systems. The uncertainty associated with the ANFIS outputs is quantified by propagating the uncertainties in the input data (determined experimentally) through the inference engine. This process is of growing relevance because similar machine learning techniques are now being developed for an increasing number of applications including aerospace, transport, biomedical and defence cyber-physical systems",Elsevier (Netherlands),Experimental Characterisation of Eye-Tracking Sensors for Adaptive Human-Machine Systems,,,,core
294759144,2019-01-01T00:00:00,"Center of Mass (CoM) estimation realizes a crucial role in legged locomotion. Most walking pattern generators and real-time gait stabilizers commonly assume that the CoM position and velocity are available for feedback. In this thesis we present one of the first 3D-CoM state estimators for humanoid robot walking. The proposed estimation scheme fuses effectively joint encoder, inertial, and feet pressure measurements with an Extended Kalman Filter (EKF) to accurately estimate the 3D-CoM position, velocity, and external forces acting on the CoM. Furthermore, it directly considers the presence of uneven terrain and the body’s angular momentum rate and thus effectively couples the frontal with the lateral plane dynamics, without relying on feet Force/Torque (F/T) sensing.         Nevertheless, it is common practice to transform the measurements to a world frame of reference and estimate the CoM with respect to the world frame. Consequently, the robot’s base and support foot pose are mandatory and need to be co-estimated. To this end, we extend a well-established in literature floating mass estimator to account for the support foot dynamics and fuse kinematic-inertial measurements with the Error State Kalman Filter (ESKF) to appropriately handle the overparametrization of rotations. In such a way, a cascade state estimation scheme consisting of a base and a CoM estimator is formed and coined State Estimation RObot Walking (SEROW). Additionally, we employ Visual Odometry (VO) and/or LIDAR Odometry (LO) measurements to correct the kinematic drift caused by slippage during walking. Unfortunately, such measurements suffer from outliers in a dynamic environment, since frequently it is assumed that only the robot is in motion and the world around is static. Thus, we introduce the Robust Gaussian ESKF (RGESKF) to automatically detect and reject outliers without relying on any prior knowledge on measurement distributions or finely tuned thresholds. Therefore, SEROW is robustified and is suitable for dynamic human environments. In order to reinforce further research endeavors, SEROW is released to the robotic community as an open-source ROS/C++ package.Up to date control and state estimation schemes readily assume that feet contact status is known a priori. Contact detection is an important and largely unexplored topic in contemporary humanoid robotics research. In this thesis, we elaborate on a broader question: in which gait phase is the robot currently in? To this end, we propose a holistic framework based on unsupervised learning from proprioceptive sensing that accurately and efficiently addresses this problem. More specifically, we robustly detect one of the three gait- phases, namely Left Single Support (LSS), Double Support (DS), and Right Single Support(RSS) utilizing joint encoder, IMU, and F/T measurements. Initially, dimensionality reduction with Principal Components Analysis (PCA) or autoencoders is performed to extract useful features, obtain a compact representation, and reduce the noise. Next, clustering is performed on the low-dimensional latent space with Gaussian Mixture Models (GMMs) and three dense clusters corresponding to the gait-phases are obtained. Interestingly, it is demonstrated that the gait phase dynamics are low-dimensional which is another indication pointing towards locomotion being a low dimensional skill. Accordingly, given that the proposed framework utilizes measurements from sensors that are commonly available on humanoids nowadays, we offer the Gait-phase Estimation Module (GEM), an open-source ROS/Python implementation to the robotic community.         SEROW and GEM have been quantitatively and qualitatively assessed in terms of accuracy and efficiency both in simulation and under real-world conditions. Initially, a simulated robot in MATLAB and NASA’s Valkyrie humanoid robot in ROS/Gazebo were employed to establish the proposed schemes with uneven/rough terrain gaits. Subsequently, the proposed schemes were integrated on a) the small size NAO humanoid robot v4.0 and b) the adult size WALK-MAN v2.0 for experimental validation. With NAO, SEROW was implemented on the robot to provide the necessary feedback for motion planning and real-time gait stabilization to achieve omni-directional locomotion even on outdoor/uneven terrains. Additionally, SEROW was used in footstep planning and also in Visual SLAM with the same robot. Regarding WALK-MAN v2.0, SEROW was executed onboard with kinematic-inertial and F/T data to provide base and CoM feedback in real-time. Furthermore, VO has also been considered to correct the kinematic drift while walking and facilitate possible footstep planning. GEM was also employed to estimate the gait phase in WALK-MAN’s dynamic gaits.Summarizing, a robust nonlinear state estimator is proposed for humanoid robot walking. Nevertheless, this scheme can be readily extended to other type of legged robots such as quadrupeds, since they share the same fundamental principles.Η εκτίμηση του Κέντρου Μάζας (CoM) διαδραματίζει κρίσιμο ρόλο στη ρομποτική βάδιση. Οι περισσότεροι σχεδιαστές κίνησης και ελεγκτές βάδισης πραγματικού χρόνου υποθέτουν ότι η θέση και η ταχύτητα του CoM είναι διαθέσιμες για ανατροφοδότηση ανά πάσα στιγμή. Σε αυτή τη διατριβή παρουσιάζουμε έναν από τους πρώτους τρισδιάστατους εκτιμητές κατάστασης CoM για το περπάτημα των ανθρωποειδών ρομπότ. Ο προτεινόμενος εκτιμητής συνδυάζει αποτελεσματικά τις μετρήσεις από αισθητήρες πίεσης στα πόδια, κωδικοποιητές στις αρθρώσεις και αδρανειακής μονάδας (IMU) στο σώμα με ένα Εκτεταμένο Φίλτρο Κάλμαν (EKF) για την ακριβή εκτίμηση τόσο της θέσης και της ταχύτητας του CoM αλλά και των εξωτερικών δυνάμεων που δρουν πάνω σε αυτό. Επιπλέον, λαμβάνει υπόψιν την ανωμαλότητα του εδάφους και την στροφορμή του σώματος με αποτέλεσμα να συνδυάζει το μετωπικό με το πλευρικό επίπεδο κίνησης, χωρίς να βασίζεται σε αισθητήρες δύναμης / ροπής (F/T) στα πόδια. Ωστόσο, είναι κοινή πρακτική να επιχειρείται η μετατροπή των μετρήσεων σε ένα αδρανειακό σύστημα αναφοράς ώστε η εκτίμηση του CoM να γίνεται σε σχέση με αυτό. Κατά συνέπεια, για την επίτευξη του παραπάνω είναι υποχρεωτικό να συνεκτιμηθούν η βάση και το πόδι στήριξης του ρομπότ. Για το σκοπό αυτό, επεκτείνουμε έναν καθιερωμένο στη βιβλιογραφία εκτιμητή αιωρούμενης μάζας με τη δυναμική του ποδιού στήριξης χρησιμοποιώντας μετρήσεις κινηματικής και αδρανειακής μονάδας με το Φίλτρο Κάλμαν Σφάλματος Κατάστασης (ESKF) για την κατάλληλη διαχείριση της υπερ-παραμετροποίησης των περιστροφών. Με αυτό το τρόπο, δημιουργείται ένα σύστημα σειριακής εκτίμησης κατάστασης που αποτελείται από έναν εκτιμητή βάσης και έναν εκτιμητή CoM το οποίο ονομάζουμε State Estimation RObot Walking (SEROW). Επιπλέον, για να διορθώσουμε την κινηματική απόκλιση που προκαλείται από την ολίσθηση των ποδιών κατά το περπάτημα, χρησιμοποιούμε μετρήσεις Οπτικής Οδομετρίας (VO) και/ή Οδομετρίας LIDAR (LO). ∆υστυχώς, τέτοιες μετρήσεις υποφέρουν από ακραίες τιμές σε ένα δυναμικό περιβάλλον, αφού κατά τον υπολογισμό τους χρησιμοποιείται η υπόθεση ότι μόνο το ρομπότ βρίσκεται σε κίνηση και ο κόσμος γύρω του είναι στατικός. Για αυτό το λόγο, εισάγουμε το Σθεναρό Γκαουσιανό Φίλτρο Κάλμαν Σφάλματος Κατάστασης (RGESKF) για την αυτόματη ανίχνευση και απόρριψη των ακραίων μετρήσεων. Το προτεινόμενο φίλτρο δεν βασίζεται σε πρότερη γνώση σχετικά με τις κατανομές των μετρήσεων και δεν χρησιμοποιεί ειδικά ρυθμισμένα κατώφλια. Ως εκ τούτου, το SEROW γίνεται ένα σθεναρό σύστημα εκτίμησης κατάστασης, κατάλληλο για δυναμικά ανθρώπινα περιβάλλοντα. Προκειμένου να ενισχυθούν περαιτέρω οι ερευνητικές προσπάθειες, το SEROW δίνεται ελεύθερα στη ρομποτική κοινότητα ως ένα πακέτο ROS/C++ ανοικτού κώδικα. Τα σύγχρονα συστήματα ελέγχου και εκτίμησης κατάστασης ανθρωποειδών ρομπότ υποθέτουν ότι η κατάσταση επαφής ποδιών-εδάφους είναι γνωστή εκ των προτέρων. Η ανίχνευση τέτοιων επαφών είναι ένα σημαντικό και σε μεγάλο βαθμό ανεξερεύνητο θέμα στη σύγχρονη ρομποτική έρευνα. Σε αυτή τη διατριβή,  διατυπώνουμε μια ευρύτερη ερώτηση: σε ποια φάση βάδισης βρίσκεται το ρομπότ; Προς το σκοπό αυτό, προτείνουμε ένα ολιστικό πλαίσιο βασισμένο σε μη-επιβλεπόμενη μάθηση από δεδομένα ιδιοδεκτικής αίσθησης που αντιμετωπίζει με ακρίβεια και αποτελεσματικότητα αυτό το πρόβλημα. Συγκεκριμένα, ανιχνεύουμε με ακρίβεια μια από τις τρεις φάσεις βάδισης, την Αριστερή Υποστήριξη (LSS), την ∆ιπλή Υποστήριξη (DS) και τη ∆εξιά Υποστήριξη (RSS), χρησιμοποιώντας μετρήσεις από κωδικοποιητές, IMU και F/T. Αρχικά, πραγματοποιείται μείωση των διαστάσεων με Ανάλυση Κύριων Στοιχείων (PCA) ή με αυτόματους κωδικοποιητές ώστε να εξαχθούν χρήσιμα χαρακτηριστικά, μια συμπαγής αναπαράσταση και να μειωθεί ο θόρυβος στα δεδομένα. Στη συνέχεια, πραγματοποιείται μια ομαδοποίηση στον χώρο χαμηλών διαστάσεων με Γκαουσιανά Μοντέλα Μίγματος (GMMs). Ως αποτέλεσμα λαμβάνονται τρία πυκνά συμπλέγματα που αντιστοιχούν στις φάσεις της βάδισης. Αυτό σημαίνει ότι η δυναμική της φάσης του βαδίσματος είναι χαμηλής διάστασης το οποίο λειτουργεί ως άλλη μια ένδειξη στο ότι ολόκληρη η διαδικασία της βάδισης είναι χαμηλής διάστασης. Επιπλέον, δεδομένου ότι το προτεινόμενο πλαίσιο χρησιμοποιεί μετρήσεις από αισθητήρες που είναι συνήθως διαθέσιμοι στα σημερινά ανθρωποειδή ρομπότ, προσφέρουμε στη ρομποτική κοινότητα το Gait-Phase Estimation Module (GEM), μια ανοικτού κώδικα εφαρμογή σε ROS/Python. Το SEROW και το GEM έχουν αξιολογηθεί ποσοτικά και ποιοτικά αναφορικά με την ακρίβεια και την αποδοτικότητα τους τόσο σε προσομοίωση όσο και σε πραγματικές συνθήκες. Αρχικά, χρησιμοποιήθηκε ένα προσομοιωμένο ρομπότ στο MATLAB και το ανθρωποειδές ρομπότ Valkyrie της NASA στο ROS/Gazebo για να τεκμηριωθούν τα προτεινόμενα σχήματα στο βάδισμα πάνω σε ανομοιόμορφο/ανώμαλο έδαφος. Στη συνέχεια, τα προτεινόμενα σχήματα ενσωματώθηκαν στο α) μικρού μεγέθους ανθρωποειδές ρομπότ NAO v4.0 και β) στο πλήρους μεγέθους ανθρωποειδές WALK-MAN v2.0 για περεταίρω πειραματική επικύρωση. Με το NAO, το SEROW εφαρμόστηκε στο ρομπότ για να παράσχει την απαραίτητη ανατροφοδότηση στον σχεδιασμό της κίνησης και τη σταθεροποίηση του βηματισμού σε πραγματικό χρόνο. Με αυτό το τρόπο επιτεύχθηκε πολυκατευθυντική βάδιση ακόμη και σε εξωτερικά/ανομοιογενή εδάφη. Επιπλέον, το SEROW χρησιμοποιήθηκε στον σχεδιασμό βημάτων για την πλοήγηση και επίσης στο Visual SLAM με το ίδιο ρομπότ. Όσον αφορά το WALK-MAN v2.0, το SEROW εφαρμόστηκε με δεδομένα κινηματικής, αδρανειακής μονάδας και F/T για να παρέχει ανατροφοδότηση βάσης και CoM σε πραγματικό χρόνο. Στην εκτίμηση λήφθηκε υπόψη και το VO για την διόρθωση της κινηματικής απόκλισης κατά το περπάτημα. Με αυτό το τρόπο διευκολύνεται σημαντικά ο πιθανός σχεδιασμός βημάτων. Τέλος, το GEM χρησιμοποιήθηκε επίσης για την εκτίμηση της φάσης της βάδισης στο δυναμικό περπάτημα του WALK-MAN. Συνοψίζοντας, σε αυτή τη διατριβή προτείνεται ένας σθεναρός μη-γραμμικός εκτιμητής κατάστασης για το βάδισμα ανθρωποειδών ρομπότ. Παρόλα αυτά, το προτεινόμενο σύστημα μπορεί εύκολα να επεκταθεί και σε άλλους τύπους ρομπότ με πόδια, όπως τα τετράποδα, μιας και διαθέτουν τις ίδιες βασικές αρχές κίνησης",'National Documentation Centre (EKT)',Σθεναρή μη γραμμική εκτίμηση κατάστασης ανθρωποειδών ρομπότ,10.12681/eadd/47240,,,core
294759151,2019-01-01T00:00:00,"The current PhD thesis addresses the formulation and implementation of a methodological framework for robot Learning from Demonstration (LfD). The latter refers to methodologies that develop behavioral policies from example state-to-action mappings. To this end, we study the reciprocal interaction of perception and action, in order to teach robots a repertoire of novel action behaviors. Based on that, we design, develop and implement a robust imitation framework, termed IMFO (IMitation Framework by Observation), that facilitates imitation learning and relevant applications in human-robot interaction (HRI) tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions, as well as novel ones. Mapping of human actions to the respective robotic ones is achieved via an indeterminate depiction, termed latent space representation. The latter accomplishes a compact, yet precise abstraction of action trajectories, effectively representing high dimensional raw actions in a low dimensional space.Moreover, throughout this thesis, we examine the role of time in LfD by enhancing the aforementioned framework with the notion of learning both the spatial and temporal characteristics of human motions. Accordingly, learned actions can be subsequently reproduced in the context of more complex time-informed HRI scenarios. Unlike previous LfD methods that cope only with the spatial traits of an action, the formulated scheme effectively encompasses spatial and temporal aspects. Extensive experimentation with a variety of real robotic platforms demonstrates the robustness and applicability of the introduced integrated LfD scheme. Learned actions are reproduced under the high level control of a time-informed task planner. During the implementation of the studied scenarios, temporal and physical constraints may impose speed adaptations in the performed actions. The employed latent space representation readily supports such variations, giving rise to novel actions in the temporal domain. Experimental results demonstrate the effectiveness of the proposed enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set of well-defined evaluation metrics are introduced to assess the validity of the proposed approach considering the temporal and spatial consistency of the reproduced behaviors. A noteworthy extension of the above regards force-based object grasping for executing sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised learning scheme, termed SLF (Supervised Learning for Force-based manipulation). SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation to acquire sufficient training data; (b) training to facilitate grasp learning with suitable robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following sim-to-real transfer, operation in real environments is achieved in addition to simulated ones, generalizing also for objects not included in the trial sessions. The proposed learning scheme is demonstrated in object lifting tasks where the applied force varies for different objects with similar contact friction coefficients, and likewise the grasping pose. Experimental results on the manipulator YuMi show that the robot is able to effectively reproduce demanding lifting and manipulation tasks after learning is accomplished. In summary, our thesis has studied LfD and has contributed with a novel approach that introduced latent space representations to encode the action characteristics. A framework implementation (IMFO) of our approach allowed extensive experimentation and also conduction of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it to cope with complex, real-life interactions. Finally, the extension of IMFO with force-based grasping facilitated manipulation tasks with sensitive objects.Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και, παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε καθημερινά περιβάλλοντα.Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης (evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών. Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,  επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi, σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της διαδικασίας μάθησης. Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή  κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης",'National Documentation Centre (EKT)',Μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,10.12681/eadd/47241,,,core
227010770,2019-01-01T00:00:00,"In the recent era, multi-criteria decision making under uncertainty is gaining importance due to its wide range of applicability. Among several types of uncertainty handling techniques, Robust Optimization (RO) is considered as an efficient and tractable approach provided one has accessibility to data in uncertain regions. However, solutions of RO may actually deviate from actual results in real scenarios, due to conservative sampling. This paper proposes a methodology to amalgamate unsupervised machine learning algorithms with RO which thereby makes it data-driven. A novel evolutionary fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enables intelligent Sobol based sampling in these regions for use in RO. Results of two test cases with varying dimensions are presented along with a comprehensive comparison between conventional RO approach using box uncertainty set and proposed methodology. Considered case studies include highly nonlinear real life model for continuous casting from steelmaking industries, where a time expensive multi-objective optimization problem under uncertainty is formulated to resolve the conflict in productivity and energy consumption. Optimal Artificial Neural Network (ANN) surrogate assisted optimization under uncertainty for casting model is performed to obtain solutions in realistic time. The resulting RO problem being multi-objective in nature, the Pareto solutions are obtained by NSGA II",'Institute of Electrical and Electronics Engineers (IEEE)',An Evolutionary Machine Learning Approach Towards Less Conservative Robust Optimization,10.1109/CEC.2019.8790094,,,core
304995515,2019-08-21T13:54:59,"Complex systems, such as healthcare systems, cities, and information networks, often produce a large volume of time series data, along with ordered event data, which are discrete in time and space, and rich in other features (e.g., markers or texts). We model the asynchronous event data as point processes. It is essential to understand and model the complex dynamics of these time series and event data so that accurate prediction, reliable detection, or smart intervention can be carried out for social goods. Specifically, my thesis focuses on the following aspects: (1) new statistical models and effective learning algorithms for complex dynamics exhibited in event data; (2) new inference algorithms for change-point detection, and temporal logic reasoning involving time series and event data. In Chapter 1, we propose a kernel-based nonparametric change-point detection method for high-dimensional streaming data. Change-point detection is an essential topic in modern complex systems. For example, wearable sensors are nowadays common in healthcare systems, which make it possible to monitor patients' health status in real time. Early event detection of deterioration is helpful and can even save patients' lives. However, it is challenging to aggregate measurements from different sensors to form one indicator, and it is not clear how to define pre- and post- change-point distributions. To tackle this problem, in Chapter 1, we propose a distribution-free and computationally efficient kernel-based nonparametric change-point detection method, which enjoys fewer assumptions on the distributions and can handle high-dimensional streaming data. Theoretical tail probability approximation of the nonparametric statistic is also proposed, which provides a statistically principled way to determine the detection thresholds. The proposed nonparametric method shows excellent performance on real human-activity detection dataset and speech dataset. In Chapter 2, we model networked asynchronous event data as point processes and propose a continuous-time change-point detection framework to detect dynamic changes in networks. We cast the problem into a sequential hypothesis test, and derive the generalized likelihood-ratio (GLR) statistic for networked point processes by considering the network topology. The constructed statistic can achieve weak signal detection by aggregating local statistics over time and networks. We further propose to evaluate the proposed GLR statistic via an efficient EM-like algorithm which can be implemented in a distributed fashion across dimensions. Similarly, we obtain a highly accurate theoretical threshold characterization for the proposed GLR statistic and demonstrate the excellent performance of our method on real social media datasets, such as Twitter and Memetracker. In Chapter 3, we propose an expressive model for the event data and further propose an adversarial learning framework to uncover the temporal dynamics. When modeling event data as point processes, instead of hand-crafting the occurrence intensity function by a parametric form, we leverage recent advances in deep learning and parameterize the intensity function as a recurrent neural network (RNN). RNN is a composition of a series of highly flexible nonlinear functions, which allows the model to capture complex dynamics in event data and make the generative process mimic the real data much better than the prior art. Fitting neural network models for even data is challenging. We develop a novel adversarial learning framework to address this challenge and further avoid model-misspecification. Our method provides a novel connection of such event data fitting method to inverse reinforcement learning, where a stochastic policy and the associated reward function are learned simultaneously. The proposed framework has been evaluated on real crime, social network, and healthcare datasets, and outperforms the state-of-the-art methods in data description. In Chapter 4, we propose a unified framework to integrate first-order temporal logic rules into point process models for event data. The proposed modeling framework excels in small data regime and has the ability to incorporate domain knowledge. The proposed temporal logic point processes model the intensity function of the event starts and ends via a set of first-order temporal logic rules. Using softened representation of temporal relations, and a weighted combination of logic rules, our framework can also deal with uncertainty in event data. Furthermore, many existing point process models can be interpreted as special cases of our framework given simple temporal logic rules. We derive a maximum likelihood estimation procedure for the proposed temporal logic point processes, and show that it can lead to accurate predictions when data are sparse and domain knowledge is critical. The proposed framework has been evaluated on real healthcare datasets, and outperforms the neural network models in event predication on small data and is easy to interpret.Ph.D",Georgia Institute of Technology,"Statistical inference, modeling, and learning of point processes",,https://core.ac.uk/download/304995515.pdf,,core
200925205,2019-03-01T00:00:00,"The overcrowding of the wireless space has triggered a strict competition for scare network resources. Therefore, there is a need for a dynamic spectrum access (DSA) technique that will ensure fair allocation of the available network resources for diverse network elements competing for the network resources. Spectrum handoff (SH) is a DSA technique through which cognitive radio (CR) promises to provide effective channel utilization, fair resource allocation, as well as reliable and uninterrupted real-time connection. However, SH may consume extra network resources, increase latency, and degrade network performance if the spectrum sensing technique used is ineffective and the channel selection strategy (CSS) is poorly implemented. Therefore, it is necessary to develop an SH policy that holistically considers the implementation of effective CSS, and spectrum sensing technique, as well as minimizes communication delays. In this work, two reinforcement learning (RL) algorithms are integrated into the CSS to perform channel selection. The first algorithm is used to evaluate the channel future occupancy, whereas the second algorithm is used to determine the channel quality in order to sort and rank the channels in candidate channel list (CCL). A method of masking linearly dependent and useless state elements is implemented to improve the convergence of the learning. Our approach showed a significant reduction in terms of latency and a remarkable improvement in throughput performance in comparison to conventional approaches",'MDPI AG',An Effective Spectrum Handoff Based on Reinforcement Learning for Target Channel Selection in the Industrial Internet of Things,10.3390/s19061395,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
217997791,2019-01-28T00:00:00,"International audienceDespite a large number of studies [2,3] over the years since the first discovery [7] and a couple of comprehensive reviews [8,9] the actual mechanism for PLD/CDW formation is still under debate. The most recent experimental [10-13] and theoretical [14] works focus on the large area growth of the CDW phase [13] the thickness dependence , and the possible unconventional behavior in the ultimate 2D limit of a single layer TiSe 2. [10-12,14] On the other hand, the other Ti dichalcogenides namely TiS 2 and TiTe 2 did not show any clear evidence until very recently when a CDW state was reported only for 1 monolayer (ML)-thin TiTe 2 at temperatures lower than 92 K. [15] It is surprising that the CDW in TiTe 2 was found to be totally suppressed for films thicker than 1 ML, [15] unlike the case of other TMDs where 1 ML and bulk-like films both make the transition to a CDW at nearly the same temperature. The interest about TiTe 2 is continuously increasing in view of theoretical predictions [16] and more recent experimental evidence [17] about pressure induced topological phase transitions in TiTe 2. The possibility to also manipulate superconduc-tivity by external pressure as predicted [18] and more recently evidenced [19] in bulk TiTe 2 creates the prospect to explore the emergence of topological superconductivity in this material. In the latter work [19] it has been shown that under nonhydro-static pressure, a CDW-like state with estimated transition temperature above room temperature (RT) appears in bulk TiTe 2 at around 0.5-1.8 GPa. These results call for a re-examination of the possibility to obtain a CDW in multilayer TiTe 2 and indeed at RT with good potential for real world applications utilizing the properties of the CDW state. These applications include a voltage-controlled oscillator device operating at room temperature , [20] fast electronic resistance switching for nonvolatile memories, [21,22] and field-effect transistor devices potentially suitable for implementation of non-Boolean logic. [23] In this paper it is shown that multilayer films (50 ML ≈ 32 nm), as well as single layer TiTe 2 epitaxially grown on InAs(111)/ Si(111) substrates by molecular beam epitaxy exhibit, in ambient pressure conditions, a CDW distortion at room temperature which is sustained up to higher temperatures, at least 400 °C, as evidenced by reflection high energy electron diffrac-tion (RHEED) (Figure S1, Supporting Information). The results are explained in terms of anisotropic strain imposed by the substrate. The group IVB 2D transition metal dichalcogenides are considered to be stable in the high symmetry trigonal octahedral structure due to the lack of unpaired d-electrons on the metal site. It is found that multilayer epitaxial TiTe 2 is an exception adopting a commensurate 2 × 2 × 2 charge density wave (CDW) structure at room temperature with an ABA type of stacking as evidenced by direct lattice imaging and reciprocal space mapping. The CDW is stabilized by highly anisotropic strain imposed by the substrate with an out-off-plane compression which reduces the interlayer van der Waals gap increasing the coupling between TiTe 2 layers",'Wiley',Room Temperature Commensurate Charge Density Wave in Epitaxial Strained TiTe 2 Multilayer Films,10.1002/admi.201801850,,,core
334864337,2019-09-16T00:00:00,"Modern reinforcement learning methods suffer from low sample efficiency and
unsafe exploration, making it infeasible to train robotic policies entirely on
real hardware. In this work, we propose to address the problem of sim-to-real
domain transfer by using meta learning to train a policy that can adapt to a
variety of dynamic conditions, and using a task-specific trajectory generation
model to provide an action space that facilitates quick exploration. We
evaluate the method by performing domain adaptation in simulation and analyzing
the structure of the latent space during adaptation. We then deploy this policy
on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a
hockey puck to a target. Our method shows more consistent and stable domain
adaptation than the baseline, resulting in better overall performance.Comment: Submitted to ICRA 202",,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,,http://arxiv.org/abs/1909.12906,,core
201224818,2019-02-01T00:00:00,"Convergence of Machine Learning, Internet of Things, and computationally powerful single-board computers has boosted research and implementation of smart spaces. Smart spaces make predictions based on historical data to enhance user experience. In this paper, we present a low-cost, low-energy smart space implementation to detect static and dynamic human activities that require simple motions. We use low-resolution (4 &#215; 16) and non-intrusive thermal sensors to collect data. We train six machine learning algorithms, namely logistic regression, naive Bayes, support vector machine, decision tree, random forest and artificial neural network (vanilla feed-forward) on the dataset collected in our lab. Our experiments reveal a very high static activity detection rate with all algorithms, where the feed-forward neural network method gives the best accuracy of 99.96%. We also show how data collection methods and sensor placement plays an important role in the resulting accuracy of different machine learning algorithms. To detect dynamic activities in real time, we use cross-correlation and connected components of thermal images. Our smart space implementation, with its real-time properties, can be used in various domains and applications, such as conference room automation, elderly health-care, etc",'MDPI AG',Static and Dynamic Activity Detection with Ambient Sensors in Smart Spaces,10.3390/s19040804,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
475185446,2019-11-08T08:00:00,"This thesis presents a total of 3 groups of contributions related to multi-objective optimization.  The first group includes the development of a new algorithm and an open-source user-friendly package for optimization over the efficient set for bi-objective mixed integer linear programs.  The second group includes an application of a special case of optimization over the efficient on conservation planning problems modeled with modern portfolio theory. Finally, the third group presents a machine learning framework to enhance criterion space search algorithms for multi-objective binary linear programming.
In the first group of contributions, this thesis presents the first (criterion space search) algorithm for optimizing a linear function over the set of efficient solutions of bi-objective mixed integer linear programs. The proposed algorithm is developed based on the triangle splitting method (Boland et al.), which can find a full representation of the nondominated frontier of any bi-objective mixed integer linear program. The proposed algorithm is easy to understand and implement, and converges quickly to an optimal solution. An extensive computational study shows the efficacy of the algorithm. Is numerically shown in this thesis that the proposed algorithm can be used to quickly generate a provably high-quality approximate solution because it maintains a lower and an upper bound on the optimal value of the linear function at any point in time. Additionally, this thesis presents OOESAlgorithm.jl, a comprehensive julia package based on the proposed algorithm. The proposed package ex- tends the first implementation of the algorithm by adding two main features: (a) in addition to CPLEX, the package allows employing any single-objective solver supported by Math- ProgBase.jl, for example, GLPK, CPLEX, and SCIP; (b) the package supports execution
on multiple processors and is compatible with the JuMP modeling language. An extensive computational study shows the efficacy of the package and its features.
In the  second group of contributions, this thesis presents a Nash bargaining solu- tion approach for spatial conservation planning problems modeled with modern portfolio theory.  The proposed modern portfolio optimization formulation corresponds to a spatial conservation planning problem involving two conflicting objectives: maximizing return and minimizing risk. A Nash bargaining solution approach is presented in this thesis to directly compute a desirable Pareto-optimal (nondominated) solution for the proposed bi-objective optimization formulation in natural resource management problems. Numerical examples in this thesis show that to directly compute a Nash bargaining solution, a Binary Quadratically Constrained Quadratic Program (BQCQP) can be solved. This thesis also shows that the proposed approach (implementable with commercial  solvers such as CPLEX) can effectively solve the proposed BQCQP for much larger problems than previous approaches published in the ecological literature.  The new approach expands considerably the applicability of such optimization methods to address real spatial conservation planning problems.
In the third group of contributions, this thesis investigates the possibility of improving the performance of multi-objective optimization solution approaches using machine learning techniques. Specifically, this thesis focus on multi-objective binary linear programs and employs one of the most effective and recently developed criterion space search algorithms, the so-called KSA, during our study.  This algorithm computes all nondominated points of a problem with p objectives by searching on a projected criterion space, i.e., a (p − 1)- dimensional criterion space.  This thesis presents an effective and fast learning approach to identify on which projected space the KSA should work, and also presents several generic features that can be used in machine learning techniques for identifying the best-projected space. Finally, a bi-objective optimization-based heuristic for selecting the best subset of the features to overcome the issue of overfitting in learning is presented. Through an extensive computational study, the performance of the proposed learning approach is tested",Digital Commons @ University of South Florida,Algorithms for Multi-Objective Mixed Integer Programming Problems,,,,core
200756595,2019-03-01T00:00:00,"Background The growth in publically available microbiome data in recent years has yielded an invaluable resource for genomic research, allowing for the design of new studies, augmentation of novel datasets and reanalysis of published works. This vast amount of microbiome data, as well as the widespread proliferation of microbiome research and the looming era of clinical metagenomics, means there is an urgent need to develop analytics that can process huge amounts of data in a short amount of time. To address this need, we propose a new method for the compact representation of microbiome sequencing data using similarity-preserving sketches of streaming k-mer spectra. These sketches allow for dissimilarity estimation, rapid microbiome catalogue searching and classification of microbiome samples in near real time. Results We apply streaming histogram sketching to microbiome samples as a form of dimensionality reduction, creating a compressed ‘histosketch’ that can efficiently represent microbiome k-mer spectra. Using public microbiome datasets, we show that histosketches can be clustered by sample type using the pairwise Jaccard similarity estimation, consequently allowing for rapid microbiome similarity searches via a locality sensitive hashing indexing scheme. Furthermore, we use a ‘real life’ example to show that histosketches can train machine learning classifiers to accurately label microbiome samples. Specifically, using a collection of 108 novel microbiome samples from a cohort of premature neonates, we trained and tested a random forest classifier that could accurately predict whether the neonate had received antibiotic treatment (97% accuracy, 96% precision) and could subsequently be used to classify microbiome data streams in less than 3 s. Conclusions Our method offers a new approach to rapidly process microbiome data streams, allowing samples to be rapidly clustered, indexed and classified. We also provide our implementation, Histosketching Using Little K-mers (HULK), which can histosketch a typical 2 GB microbiome in 50 s on a standard laptop using four cores, with the sketch occupying 3000 bytes of disk space. (https://github.com/will-rowe/hulk)",'Springer Science and Business Media LLC',Streaming histogram sketching for rapid microbiome analytics,10.1186/s40168-019-0653-2,,"[{'title': 'Microbiome', 'identifiers': ['2049-2618', 'issn:2049-2618']}]",core
301275242,2019-04-05T00:00:00,"Autonomic optical transmission and networking requires machine learning (ML) models to be trained with large datasets. However, the availability of enough real data to produce accurate ML models is rarely ensured since new optical equipment and techniques are continuously being deployed in the network. One option is to generate data from simulations and lab experiments, but such data could not cover the whole features space and would translate into inaccuracies in the ML models. In this paper, we propose an ML-based algorithm life cycle to facilitate ML deployment in real operator networks. The dataset for ML training can be initially populated based on the results from simulations and lab experiments. Once ML models are generated, ML retraining can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits of the proposed learning cycle for general use cases. In addition, two specific use cases are proposed and demonstrated that implement different learning strategies: (i) a two-phase strategy performing out-of-field training using data from simulations and lab experiments with generic equipment, followed by an in-field adaptation to support heterogeneous equipment (the accuracy of this strategy is shown for a use case of failure detection and identification), and (ii) in-field retraining, where ML models are retrained after detecting model inaccuracies. Different approaches are analyzed and evaluated for a use case of autonomic transmission, where results show the significant benefits of collective learning.Peer Reviewe",'The Optical Society',Learning life cycle to speed up autonomic optical transmission and networking adoption,10.1364/JOCN.11.000226,,"[{'title': 'Journal of Optical Communications and Networking', 'identifiers': ['1943-0620', 'issn:1943-0620']}]",core
237402232,"November 7, 2019","Artificial Intelligence (AI) is a growing field of computa- tional science techniques designed to mimic functions per- formed by people. Advancements in autonomy will depend on a portfolio of AI technologies. Automated planning and scheduling is a venerable field of study in AI, and is needed for a variety of mission planning functions. Plan execution technology is less well studied, but important for auton- omy and robotics. Specialized forms of automated reason- ing and machine learning are key technologies to enable fault management. Over the past decade, the NASA Au- tonomous Systems and Operations (ASO) project has devel- oped and demonstrated numerous autonomy enabling tech- nologies employing AI techniques. Our work has employed AI in three distinct ways to enable autonomous mission op- erations capabilities. Crew Autonomy gives astronauts tools to assist in the performance of each of these mission oper-ations functions. Vehicle System Management uses AI tech- niques to turn the astronaut's spacecraft into a robot, allow- ing it to operate when astronauts are not present, or to reduce astronaut workload. AI technology also enables Autonomous Robots as crew assistants or proxies when the crew are not present. When these capabilities are used to enable astro- nauts to operate autonomously, they must be integrated with user interfaces, introducing numerous human factors con- siderations; when these capabilities are used to enable vehi- cle system management, they must be integrated with flight software, and run on embedded processors under the control of real-time operating systems.We first describe human spaceflight mission operations capabilities. The remainder of the paper will describe the ASO project, and the development and demonstration per- formed by ASO since 2011. We will describe the AI tech- niques behind each of these demonstrations, which include a variety of symbolic automated reasoning and machine learn- ing based approaches. Finally, we conclude with an assess- ment of future development needs for AI to enable NASA's future Exploration missions",,Artificial Intelligence: Powering Human Exploration of the Moon and Mars,,,,core
250308286,2019-01-01T00:00:00,"The advances in Internet of Things lead to an increased number of devices generating and streaming data. These devices can be useful data sources for Activity Recognition by using Machine Learning. However, as the set of available sensors may vary over time, e.g. due to mobility of the sensors and technical failures, the feature space might also change over time. Moreover, the labelled data necessary for the training is often costly to acquire. Active Learning is a type of Interactive Machine Learning where the model is given a budget for requesting labels from an oracle, and aims to maximize accuracy by careful selection of what data points to label. It is generally assumed that a query always gets a correct response, but in many real-world scenarios this is not a realistic assumption. In this work we investigate different Proactive Learning strategies, which explore the human factors of the oracle and aspects that might influence a user to provide or withhold labels. We implemented four proactive strategies and hybrid versions of them. They were evaluated on two datasets to examine how a more proactive, or reluctant, user affects performance. The results show that a more proactive user can improve the performance, especially when the user is influenced by the accuracy of earlier predictions. The experiments also highlight challenges related to evaluating performance when the set of classes is changing over time",'American College of Medical Physics (ACMP)',Interactive Machine Learning for the Internet of Things : A Case Study on Activity Detection,,,,core
305124974,2019-01-01T00:00:00,"General N-body problems are a set of problems in which an update to a single element in the system depends on every other element. N-body problems are ubiquitous, with applications in various domains ranging from scientific computing simulations in molecular dynamics, astrophysics, acoustics, and fluid dynamics all the way to computer vision, data mining and machine learning problems. Different N-body algorithms have been designed and implemented in these various fields. However, there is a big gap between the algorithm one designs on paper and the code that runs efficiently on a parallel system. It is time-consuming to write fast, parallel, and scalable code for these problems. On the other hand, the sheer scale and growth of modern scientific datasets necessitate exploiting the power of both parallel and approximation algorithms where there is a potential to trade-off accuracy for performance. The main problem that we are tackling in this thesis is how to automatically generate asymptotically optimal N-body algorithms from the high-level specification of the problem. We combine the body of work in performance optimizations, compilers and the domain of N-body problems to build a unified system where domain scientists can write programs at the high level while attaining performance of code written by an expert at the low level.In order to generate a high-performance, scalable code for this group of problems, we take the following steps in this thesis; first, we propose a unified algorithmic framework named PASCAL in order to address the challenge of designing a general algorithmic template to represent the class of N-body problems. PASCAL utilizes space-partitioning trees and user-controlled pruning/approximations to reduce the asymptotic runtime complexity from linear to logarithmic in the number of data points. In PASCAL, we design an algorithm that automatically generates conditions for pruning or approximation of an N-body problem considering the problem's definition. In order to evaluate PASCAL, we developed tree-based algorithms for six well-known problems: k-nearest neighbors, range search, minimum spanning tree, kernel density estimation, expectation maximization, and Hausdorff distance. We show that applying domain-specific optimizations and parallelization to the algorithms written in PASCAL achieves 10x to 230x speedup compared to state-of-the-art libraries on a dual-socket Intel Xeon processor with 16 cores on real-world datasets. Second, we extend the PASCAL framework to build PASCAL-X that adds support for NUMA-aware parallelization. PASCAL-X  also presents insights on the influence of tuning parameters. Tuning parameters such as leaf size (influences the shape of the tree) and cut-off level (controls the granularity of tasks) of the space-partitioning trees result in performance improvement of up to 4.6x. A key goal is to generate scalable and high-performance code automatically without sacrificing productivity. That implies minimizing the effort the users have to put in to generate the desired high-performance code. Another critical factor is the adaptivity,  which indicates the amount of effort that is required to extend the high-performance code generation to new N-body problems. Finally, we consider these factors and develop a domain-specific language and code generator named Portal, which is built on top of PASCAL-X. Portal's language design is inspired by the mathematical representation of N-body problems, resulting in an intuitive language for rapid implementation of a variety of problems. Portal's back-end is designed and implemented to generate optimized, parallel, and scalable implementations for multi-core systems. We demonstrate that the performance achieved by using Portal is comparable to that of expert hand-optimized code while providing productivity for domain scientists. For instance, using Portal for the k-nearest neighbors problem gains performance that is similar to the hand-optimized code, while reducing the lines of code by 68x.  To the best of our knowledge, there are no known libraries or frameworks that implement parallel asymptotically optimal algorithms for the class of general N-body problems and this thesis primarily aims to fill this gap. Finally, we present a case study of Portal for the real-world problem of face clustering. In this case study, we show that Portal not only provides a fast solution for the face clustering problem with similar accuracy as the state-of-the-art algorithm, but also it provides productivity by implementing the face clustering algorithm in only 14 lines of Portal code","eScholarship, University of California",A High-Performance Domain-Specific Language and Code Generator for General N-body Problems,,https://core.ac.uk/download/305124974.pdf,,core
225561379,2019-04-10T00:00:00,"Planet formation simulations are capable of directly integrating the evolution of hundreds to thousands of planetary embryos and planetesimals as they accrete pairwise to become planets. In principle, these investigations allow us to better understand the final configuration and geochemistry of the terrestrial planets, and also to place our solar system in the context of other exosolar systems. While these simulations classically prescribe collisions to result in perfect mergers, recent computational advances have begun to allow for more complex outcomes to be implemented. Here we apply machine learning to a large but sparse database of giant impact studies, which allows us to streamline the simulations into a classifier of collision outcomes and a regressor of accretion efficiency. The classifier maps a four-dimensional (4D) parameter space (target mass, projectile-to-target mass ratio, impact velocity, impact angle) into the four major collision types: merger, graze-and-merge, hit-and-run, and disruption. The definition of the four regimes and their boundary is fully data-driven. The results do not suffer from any model assumption in the fitting. The classifier maps the structure of the parameter space and it provides insights into the outcome regimes. The regressor is a neural network that is trained to closely mimic the functional relationship between the 4D space of collision parameters, and a real-variable outcome, the mass of the largest remnant. This work is a prototype of a more complete surrogate model, that will be based on extended sets of simulations (big data), that will quickly and reliably predict specific collision outcomes for use in realistic N-body dynamical studies of planetary formation.NASA Planetary Science Division; University of ArizonaThis item from the UA Faculty Publications collection is made available by the University of Arizona with support from the University of Arizona Libraries. If you have questions, please contact us at repository@u.library.arizona.edu",'American Astronomical Society',Realistic On-the-fly Outcomes of Planetary Collisions: Machine Learning Applied to Simulations of Giant Impacts,10.3847/1538-4357/ab0e8a,https://core.ac.uk/download/225561379.pdf,"[{'title': 'The Astrophysical Journal', 'identifiers': ['issn:1538-4357', '1538-4357']}]",core
374478332,2019-12-01T00:00:00,"Big data analytics is a virtually new term in power system terminology. This concept delves into the way a massive volume of data is acquired, processed, analyzed to extract insight from available data. In particular, big data analytics alludes to applications of artificial intelligence, machine learning techniques, data mining techniques, time-series forecasting methods. Decision-makers in power systems have been long plagued by incapability and weakness of classical methods in dealing with large-scale real practical cases due to the existence of thousands or millions of variables, being time-consuming, the requirement of a high computation burden, divergence of results, unjustifiable errors, and poor accuracy of the model. Big data analytics is an ongoing topic, which pinpoints how to extract insights from these large data sets. The extant article has enumerated the applications of big data analytics in future power systems through several layers from grid-scale to local-scale. Big data analytics has many applications in the areas of smart grid implementation, electricity markets, execution of collaborative operation schemes, enhancement of microgrid operation autonomy, management of electric vehicle operations in smart grids, active distribution network control, district hub system management, multi-agent energy systems, electricity theft detection, stability and security assessment by PMUs, and better exploitation of renewable energy sources. The employment of big data analytics entails some prerequisites, such as the proliferation of IoT-enabled devices, easily-accessible cloud space, blockchain, etc. This paper has comprehensively conducted an extensive review of the applications of big data analytics along with the prevailing challenges and solutions",'Institute of Electrical and Electronics Engineers (IEEE)',Attributes of Big Data Analytics for Data-Driven Decision Making in Cyber-Physical Power Systems,10.1109/ipaps49326.2019.9069391,https://core.ac.uk/download/374478332.pdf,,core
289201335,2019-10-28T19:31:26Z,"Scheduling is an important problem in artificial intelligence and operations research. In production processes, it deals with the problem of allocation of resources to different tasks with the goal of optimizing one or more objectives. Job shop scheduling is a classic and very common scheduling problem. In the real world, shop environments dynamically change due to events such as the arrival of new jobs and machine breakdown. In such manufacturing environments, uncertainty in shop parameters is typical. It is of vital importance to develop methods for effective scheduling in such practical settings.

Scheduling using heuristics like dispatching rules is very popular and suitable for such environments due to their low computational cost and ease of implementation. For a dynamic manufacturing environment with varying shop scenarios, using a universal dispatching rule is not very effective. But manual development of effective dispatching rules is difficult, time consuming and requires expertise. Genetic programming is an evolutionary approach which is suitable for automatically designing effective dispatching rules. Since the genetic programming approach searches in the space of heuristics (dispatching rules) instead of building up a schedule, it is considered a hyper-heuristic approach.

Genetic programming like many other evolutionary approaches is computationally expensive. Therefore, it is of vital importance to present the genetic programming based hyper-heuristic (GPHH) system with scheduling problem instances which capture the complex shop scenarios capturing the difficulty in scheduling. Active learning is a related concept from machine learning which concerns with effective sampling of those training instances to promote the accuracy of the learned model.

The overall goal of this thesis is to develop effective and efficient genetic programming based hyper-heuristic approaches using active learning techniques for dynamic job shop scheduling problems with one or more objectives.

This thesis develops new representations for genetic programming enabling it to incorporate the uncertainty information about processing times of the jobs. Furthermore, a cooperative co-evolutionary approach is developed for GPHH which evolves a pair of dispatching rules for bottleneck and non-bottleneck machines in the dynamic environment with uncertainty in processing times arising due to varying machine characteristics. The results show that the new representations and training approaches are able to significantly improve the performance of evolved dispatching rules.

This thesis develops a new GPHH framework in order to incorporate active learning methods toward sampling DJSS instances which promote the evolution of more effective rules. Using this framework, two new active sampling methods were developed to identify those scheduling problem instances which promoted evolution of effective dispatching rules. The results show the advantages of using active learning methods for scheduling under the purview of GPHH.

This thesis investigates a coarse-grained model of parallel evolutionary approach for multi-objective dynamic job shop scheduling problems using GPHH. The outcome of the investigation was utilized to extend the coarse-grained model and incorporate an active sampling heuristic toward identifying those scheduling problem instances which capture the conflict between the objectives. The results show significant improvement in the quality of the evolved Pareto set of dispatching rules.

Through this thesis, the following contributions have been made. (1) New representations and training approaches for GPHH  to incorporate uncertainty information about processing times of jobs into dispatching rules to make them more effective in a practical shop environment. (2) A new GPHH framework which enables active sampling of scheduling problem instances toward evolving dispatching rules effective across complex shop scenarios.  (3) A new active sampling heuristic based on a coarse-grained model of parallel evolutionary approach for GPHH for multi-objective scheduling problems",Victoria University of Wellington,Active Learning Methods for Dynamic Job Shop Scheduling using Genetic Programming under Uncertain Environment,,,,core
322676703,2019,"Intelligent automation and trusted autonomy are being introduced in aerospace cyber-physical systems to support diverse tasks including data processing, decision-making, information sharing and mission execution. Due to the increasing level of integration/collaboration between humans and automation in these tasks, the operational performance of closed-loop human-machine systems can be enhanced when the machine monitors the operator&#039;s cognitive states and adapts to them in order to maximise the effectiveness of the Human-Machine Interfaces and Interactions (HMI2). Technological developments have led to neurophysiological observations becoming a reliable methodology to evaluate the human operator&#039;s states using a variety of wearable and remote sensors. The adoption of sensor networks can be seen as an evolution of this approach, as there are notable advantages if these sensors collect and exchange data in real-time, while their operation is controlled remotely and synchronised. This paper discusses recent advances in sensor networks for aerospace cyber-physical systems, focusing on Cognitive HMI2 (CHMI2) implementations. The key neurophysiological measurements used in this context and their relationship with the operator&#039;s cognitive states are discussed. Suitable data analysis techniques based on machine learning and statistical inference are also presented, as these techniques allow processing both neurophysiological and operational data to obtain accurate cognitive state estimations. Lastly, to support the development of sensor networks for CHMI2 applications, the paper addresses the performance characterisation of various state-of-the-art sensors and the propagation of measurement uncertainties through a machine learning-based inference engine. Results show that a proper sensor selection and integration can support the implementation of effective human-machine systems for various challenging aerospace applications, including Air Traffic Management (ATM), commercia",M D P I AG (Switzerland),Sensor Networks for Aerospace Human-Machine Systems,,,,core
275648601,2019-07-25T00:00:00,"[EN] This contribution shows the comparison, investigation, and implementation of different access strategies on multimodal data. The first part of the research is structured as a theoretical part opposing and explaining the terms of conventional access, virtual archival access, and virtual museums while additionally referencing related work. Especially, issues that still persist in repositories like the ambiguity or missing of metadata is pointed out. The second part explains the practical implementation of a workflow from a large image repository to various four-dimensional applications. Mainly, the filtering of images and in the following, the orientation of images is explained. Selection of the relevant images is partly done manually but also with the use of deep convolutional neural networks for image classification. In the following, photogrammetric methods are used for finding the relative orientation between image pairs in a projective frame. For this purpose, an adapted Structure from Motion (SfM) workflow is presented, in which the step of feature detection and matching is replaced by the Radiant-Invariant Feature Transform (RIFT) and Matching On Demand with View Synthesis (MODS). Both methods have been evaluated on a benchmark dataset and performed superior than other approaches. Subsequently, the oriented images are placed interactively and in the future automatically in a 4D browser application showing images, maps, and building models Further usage scenarios are presented in several Virtual Reality (VR) and Augmented Reality (AR) applications. The new representation of the archival data enables spatial and temporal browsing of repositories allowing the research of innovative perspectives and the uncovering of historical details.Highlights:Strategies for a completely automated workflow from image repositories to four-dimensional (4D) access approaches.The orientation of historical images using adapted and evaluated feature matching methods.4D access methods for historical images and 3D models using web technologies and Virtual Reality (VR)/Augmented Reality (AR).[ES] Esta contribución muestra la comparación, investigación e implementación de diferentes estrategias de acceso a datos multimodales. La primera parte de la investigación se estructura en una parte teórica en la que se oponen y explican los términos de acceso convencional, acceso a los archivos virtuales, y museos virtuales, a la vez que se hace referencia a trabajos  relacionados.  En  especial,  se  señalan  los  problemas  que  aún  persisten  en  los  repositorios,  como  la ambigüedad o la falta de metadatos. La segunda parte explica la implementación práctica de un flujo de trabajo desde un gran repositorio de imágenes a varias aplicaciones en cuatro dimensiones (4D). Principalmente, se explica el filtrado de imágenes y, a continuación, la orientación de las mismas. La selección de las imágenes relevantes se hace en parte manualmente,  pero  también  con  el  uso  de  redes  neuronales  convolucionales  profundas  para  la  clasificación  de  las imágenes.  A  continuación,  se  utilizan  métodos  fotogramétricos  para  encontrar  la  orientación  relativa  entre  pares  de imágenes en un marco proyectivo. Para ello, se presenta un flujo de trabajo adaptado a partir de Structure from Motion, (SfM),  en  el  que  el  paso  de la detección  y la correspondencia  de entidades es  sustituido  por  la Transformación  de entidades  invariante  a  la  radiancia  (Radiant-Invariant  Feature  Transform, RIFT)  y  la Correspondencia  a  demanda con vistas sintéticas (Matching on Demand with View Synthesis, MODS). Ambos métodos han sido evaluados sobre la base de  un  conjunto  de  datos  de  referencia  y funcionaron  mejor  que otros procedimientos.  Posteriormente,  las  imágenes orientadas se colocan interactivamente y en el futuro automáticamente en una aplicación de navegador 4D que muestra imágenes,  mapas  y  modelos  de edificios.  Otros  escenarios  de  uso  se  presentan  en  varias  aplicación es  de  Realidad Virtual  (RV)  y  Realidad  Aumentada  (RA).  La  nueva  representación  de  los  datos  archivados permite  la  navegación espacial y temporal de los repositorios, lo que permite la investigación en perspectivas innovadoras y el descubrimiento de detalles históricos.The  research  upon  which  this  paper  is  based  is  part  of the  junior research  group  UrbanHistory4D’s  activities which  has  received funding  from  the  German  Federal Ministry   of   Education   and Research   under   grant agreement No 01UG1630. This work   was   supported   by   the   German Federal Ministry     of     Education     and     Research     (BMBF, 01IS18026BA-F)  by funding  the  competence  center  for Big Data “ScaDS Dresden/Leipzig”.Maiwald, F.; Bruschke, J.; Lehmann, C.; Niebling, F. (2019). Un sistema de información 4D para la exploración de imágenes y mapas multitemporales utilizando fotogrametría, tecnologías web y VR/AR. Virtual Archaeology Review. 10(21):1-13. https://doi.org/10.4995/var.2019.11867SWORD1131021Ackerman, A., & Glekas, E. (2017). Digital Capture and Fabrication Tools for Interpretation of Historic Sites. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, IV-2/W2, 107-114. doi:10.5194/isprs-annals-IV-2-W2-107-2017Armingeon, M., Komani, P., Zanwar, T., Korkut, S., & Dornberger, R. (2019). A Case Study: Assessing Effectiveness of the Augmented Reality Application in Augusta Raurica Augmented Reality and Virtual Reality (pp. 99-111): Springer.Artstor. (2019). Artstor Digital Library. Retrieved April 30, 2019, from https://library.artstor.orgBay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF: Speeded Up Robust Features. Paper presented at the European Conference on Computer Vision, Berlin, Heidelberg.Beaudoin, J. E., & Brady, J. E. (2011). Finding visual information: a study of image resources used by archaeologists, architects, art historians, and artists. Art Documentation: Journal of the Art Libraries Society of North America, 30(2), 24-36.Beltrami, C., Cavezzali, D., Chiabrando, F., Iaccarino Idelson, A., Patrucco, G., & Rinaudo, F. (2019). 3D Digital and Physical Reconstruction of a Collapsed Dome using SFM Techniques from Historical Images. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W11, 217-224. doi:10.5194/isprs-archives-XLII-2-W11-217-2019Bevilacqua, M. G., Caroti, G., Piemonte, A., & Ulivieri, D. (2019). Reconstruction of lost Architectural Volumes by Integration of Photogrammetry from Archive Imagery with 3-D Models of the Status Quo. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W9, 119-125. doi:10.5194/isprs-archives-XLII-2-W9-119-2019Bitelli, G., Dellapasqua, M., Girelli, V. A., Sbaraglia, S., & Tinia, M. A. (2017). Historical Photogrammetry and Terrestrial Laser Scanning for the 3d Virtual Reconstruction of Destroyed Structures: A Case Study in Italy. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-5/W1, 113-119. doi:10.5194/isprs-archives-XLII-5-W1-113-2017Bruschke, J., Niebling, F., Maiwald, F., Friedrichs, K., Wacker, M., & Latoschik, M. E. (2017). Towards browsing repositories of spatially oriented historic photographic images in 3D web environments. Paper presented at the Proceedings of the 22nd International Conference on 3D Web Technology.Bruschke, J., Niebling, F., & Wacker, M. (2018). Visualization of Orientations of Spatial Historical Photographs. Paper presented at the Eurographics Workshop on Graphics and Cultural Heritage.Bruschke, J., & Wacker, M. (2014). Application of a Graph Database and Graphical User Interface for the CIDOC CRM. Paper presented at the Access and Understanding-Networking in the Digital Era. Session J1. The 2014 annual conference of CIDOC, the International Committee for Documentation of ICOM.Burdea, G. C., & Coiffet, P. (2003). Virtual reality technology: John Wiley & Sons.Callieri, M., Cignoni, P., Corsini, M., & Scopigno, R. (2008). Masked photo blending: Mapping dense photographic data set on high-resolution sampled 3D models. Computers & Graphics, 32(4), 464-473.Chum, O., & Matas, J. (2005). Matching with PROSAC-progressive sample consensus. Paper presented at the Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on.Coordination and Support Action Virtual Multimodal Museum (ViMM). (2018). ViMM. Retrieved April 30, 2019, from https://www.vi-mm.eu/CultLab3D. (2019). CultLab3D. Retrieved April 30, 2019, from https://www.cultlab3d.deDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. Paper presented at the 2009 IEEE conference on computer vision and pattern recognition.Deutsches Archäologisches Institut (DAI). (2019). iDAI.objects arachne (Arachne). Retrieved April 30, 2019, from https://arachne.dainst.org/Efron, B., & Tibshirani, R. J. (1994). An introduction to the bootstrap: CRC press.Europeana. (2019). Europeana Collections. Retrieved 30.04.2019, from https://www.europeana.euEvens, T., & Hauttekeete, L. (2011). Challenges of digital preservation for cultural heritage institutions. Journal of Librarianship and Information Science, 43(3), 157-165.Fischler, M. A., & Bolles, R. C. (1981). Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6), 381-395.Fleming‐May, R. A., & Green, H. (2016). Digital innovations in poetry: Practices of creative writing faculty in online literary publishing. Journal of the Association for Information Science and Technology, 67(4), 859-873.Franken, T., Dellepiane, M., Ganovelli, F., Cignoni, P., Montani, C., & Scopigno, R. (2005). Minimizing user intervention in registering 2D images to 3D models. The visual computer, 21(8-10), 619-628.Girardi, G., von Schwerin, J., Richards-Rissetto, H., Remondino, F., & Agugiaro, G. (2013). The MayaArch3D project: A 3D WebGIS for analyzing ancient architecture and landscapes. Literary and Linguistic Computing, 28(4), 736-753. doi:10.1093/llc/fqt059Grussenmeyer, P., & Al Khalil, O. (2017). From Metric Image Archives to Point Cloud Reconstruction: Case Study of the Great Mosque of Aleppo in Syria. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W5, 295-301. doi:10.5194/isprs-archives-XLII-2-W5-295-2017Gutierrez, M., Vexo, F., & Thalmann, D. (2008). Stepping into virtual reality: Springer Science & Business Media.Guttentag, D. A. (2010). Virtual reality: Applications and implications for tourism. Tourism Management, 31(5), 637-651.Hartley, R., & Zisserman, A. (2003). Multiple view geometry in computer vision: Cambridge university press.Koutsoudis, A., Arnaoutoglou, F., Tsaouselis, A., Ioannakis, G., & Chamzas, C. (2015). Creating 3D Replicas of Medium-to Large-Scale Monuments for Web-Based Dissemination Within the Framework of the 3D-Icons Project. CAA2015, 971.Li, J., Hu, Q., & Ai, M. (2018). RIFT: Multi-modal Image Matching Based on Radiation-invariant Feature Transform. arXiv preprint arXiv:1804.09493.Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91-110.Maietti, F., Di Giulio, R., Piaia, E., Medici, M., & Ferrari, F. (2018). Enhancing Heritage fruition through 3D semantic modelling and digital tools: the INCEPTION project. Paper presented at the IOP Conference Series: Materials Science and Engineering.Maiwald, F., Schneider, D., Henze, F., Münster, S., & Niebling, F. (2018). Feature Matching of Historical Images Based on Geometry of Quadrilaterals. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2, 643-650. doi:10.5194/isprs-archives-XLII-2-643-2018Maiwald, F., Vietze, T., Schneider, D., Henze, F., Münster, S., & Niebling, F. (2017). Photogrammetric analysis of historical image repositories for virtual reconstruction in the field of digital humanities. The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 42, 447.Matas, J., Chum, O., Urban, M., & Pajdla, T. (2004). Robust wide-baseline stereo from maximally stable extremal regions. Image and Vision Computing, 22(10), 761-767.Melero, F. J., Revelles, J., & Bellido, M. L. (2018). Atalaya3D: making universities' cultural heritage accessible through 3D technologies.Milgram, P., Takemura, H., Utsumi, A., & Kishino, F. (1995). Augmented reality: A class of displays on the reality-virtuality continuum. Paper presented at the Telemanipulator and telepresence technologies.Mishkin, D., Matas, J., & Perdoch, M. (2015). MODS: Fast and robust method for two-view matching. Computer Vision and Image Understanding, 141, 81-93.Moulon, P., Monasse, P., & Marlet, R. (2012). Adaptive structure from motion with a contrario model estimation. Paper presented at the Asian Conference on Computer Vision.Münster, S., Kamposiori, C., Friedrichs, K., & Kröber, C. (2018). Image libraries and their scholarly use in the field of art and architectural history. International journal on digital libraries, 19(4), 367-383.Niebling, F., Bruschke, J., & Latoschik, M. E. (2018). Browsing Spatial Photography for Dissemination of Cultural Heritage Research Results using Augmented Models.Niebling, F., Maiwald, F., Barthel, K., & Latoschik, M. E. (2017). 4D Augmented City Models, Photogrammetric Creation and Dissemination Digital Research and Education in Architectural Heritage (pp. 196-212). Cham: Springer International Publishing.Oliva, L. S., Mura, A., Betella, A., Pacheco, D., Martinez, E., & Verschure, P. (2015). Recovering the history of Bergen Belsen using an interactive 3D reconstruction in a mixed reality space the role of pre-knowledge on memory recollection. Paper presented at the 2015 Digital Heritage.Pani Paudel, D., Habed, A., Demonceaux, C., & Vasseur, P. (2015). Robust and optimal sum-of-squares-based point-to-plane registration of image sets and structured scenes. Paper presented at the Proceedings of the IEEE International Conference on Computer Vision.Ross, S., & Hedstrom, M. (2005). Preservation research and sustainable digital libraries. International journal on digital libraries, 5(4), 317-324.Schindler, G., & Dellaert, F. (2012). 4D Cities: Analyzing, Visualizing, and Interacting with Historical Urban Photo Collections. Journal of Multimedia, 7(2), 124-131.Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. Paper presented at the Proceedings of the IEEE International Conference on Computer Vision.Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.Slater, M., & Sanchez-Vives, M. V. (2016). Enhancing our lives with immersive virtual reality. Frontiers in Robotics and AI, 3, 74.Styliani, S., Fotis, L., Kostas, K., & Petros, P. (2009). Virtual museums, a survey and some issues for consideration. Journal of cultural Heritage, 10(4), 520-528.Tschirschwitz, F., Büyüksalih, G., Kersten, T., Kan, T., Enc, G., & Baskaraca, P. (2019). Virtualising an Ottoman Fortress - Laser Scanning and 3D Modelling for the Development of an Interactive, Immersive Virtual Reality Application. International archives of the photogrammetry, remote sensing and spatial information sciences, 42(2/W9).Web3D Consortium. (2019). Open Standards for Real-Time 3D Communication. Retrieved April 30, 2019, from http://www.web3d.org/Wu, C. (2013). Towards linear-time incremental structure from motion. Paper presented at the 3D Vision-3DV 2013, 2013 International conference on.Wu, Y., Ma, W., Gong, M., Su, L., & Jiao, L. (2015). A Novel Point-Matching Algorithm Based on Fast Sample Consensus for Image Registration. IEEE Geosci. Remote Sensing Lett., 12(1), 43-47.Yoon, J., & Chung, E. (2011). Understanding image needs in daily life by analyzing questions in a social Q&A site. Journal of the American Society for Information Science and Technology, 62(11), 2201-2213",'Universitat Politecnica de Valencia',"A 4D information system for the exploration of multitemporal images and maps using photogrammetry, web technologies and VR/AR",10.4995/var.2019.11867,https://riunet.upv.es/bitstream/10251/124242/4/11867-48104-5-PB.pdf,,core
189048857,2019-01-28T00:00:00,"International audienceDespite a large number of studies [2,3] over the years since the first discovery [7] and a couple of comprehensive reviews [8,9] the actual mechanism for PLD/CDW formation is still under debate. The most recent experimental [10-13] and theoretical [14] works focus on the large area growth of the CDW phase [13] the thickness dependence , and the possible unconventional behavior in the ultimate 2D limit of a single layer TiSe 2. [10-12,14] On the other hand, the other Ti dichalcogenides namely TiS 2 and TiTe 2 did not show any clear evidence until very recently when a CDW state was reported only for 1 monolayer (ML)-thin TiTe 2 at temperatures lower than 92 K. [15] It is surprising that the CDW in TiTe 2 was found to be totally suppressed for films thicker than 1 ML, [15] unlike the case of other TMDs where 1 ML and bulk-like films both make the transition to a CDW at nearly the same temperature. The interest about TiTe 2 is continuously increasing in view of theoretical predictions [16] and more recent experimental evidence [17] about pressure induced topological phase transitions in TiTe 2. The possibility to also manipulate superconduc-tivity by external pressure as predicted [18] and more recently evidenced [19] in bulk TiTe 2 creates the prospect to explore the emergence of topological superconductivity in this material. In the latter work [19] it has been shown that under nonhydro-static pressure, a CDW-like state with estimated transition temperature above room temperature (RT) appears in bulk TiTe 2 at around 0.5-1.8 GPa. These results call for a re-examination of the possibility to obtain a CDW in multilayer TiTe 2 and indeed at RT with good potential for real world applications utilizing the properties of the CDW state. These applications include a voltage-controlled oscillator device operating at room temperature , [20] fast electronic resistance switching for nonvolatile memories, [21,22] and field-effect transistor devices potentially suitable for implementation of non-Boolean logic. [23] In this paper it is shown that multilayer films (50 ML ≈ 32 nm), as well as single layer TiTe 2 epitaxially grown on InAs(111)/ Si(111) substrates by molecular beam epitaxy exhibit, in ambient pressure conditions, a CDW distortion at room temperature which is sustained up to higher temperatures, at least 400 °C, as evidenced by reflection high energy electron diffrac-tion (RHEED) (Figure S1, Supporting Information). The results are explained in terms of anisotropic strain imposed by the substrate. The group IVB 2D transition metal dichalcogenides are considered to be stable in the high symmetry trigonal octahedral structure due to the lack of unpaired d-electrons on the metal site. It is found that multilayer epitaxial TiTe 2 is an exception adopting a commensurate 2 × 2 × 2 charge density wave (CDW) structure at room temperature with an ABA type of stacking as evidenced by direct lattice imaging and reciprocal space mapping. The CDW is stabilized by highly anisotropic strain imposed by the substrate with an out-off-plane compression which reduces the interlayer van der Waals gap increasing the coupling between TiTe 2 layers",'Wiley',Room Temperature Commensurate Charge Density Wave in Epitaxial Strained TiTe 2 Multilayer Films,10.1002/admi.201801850,,,core
363914753,2019-12-15T00:00:00,"Many expert systems have been developed for self-adaptive PID controllers of mobile robots. However, the high computational requirements of the expert systems layers, developed for the tuning of the PID controllers, still require previous expert knowledge and high efficiency in algorithmic and software execution for real-time applications. To address these problems, in this paper we propose an expert agent-based system, based on a reinforcement learning agent, for self-adapting multiple low-level PID controllers in mobile robots. For the formulation of the artificial expert agent, we develop an incremental model-free algorithm version of the double Q-Learning algorithm for fast on-line adaptation of multiple low-level PID controllers. Fast learning and high on-line adaptability of the artificial expert agent is achieved by means of a proposed incremental active-learning exploration-exploitation procedure, for a non-uniform state space exploration, along with an experience replay mechanism for multiple value functions updates in the double Q-learning algorithm. A comprehensive comparative simulation study and experiments in a real mobile robot demonstrate the high performance of the proposed algorithm for a real-time simultaneous tuning of multiple adaptive low-level PID controllers of mobile robots in real world conditions.Fil: Carlucho, Ignacio. Universidad Nacional del Centro de la Provincia de Buenos Aires. Facultad de Ingeniería Olavarría. Departamento de Electromecánica. Grupo INTELYMEC; Argentina. Universidad Nacional del Centro de la Provincia de Buenos Aires. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Provincia de Buenos Aires. Gobernación. Comisión de Investigaciones Científicas. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires; ArgentinaFil: de Paula, Mariano. Universidad Nacional del Centro de la Provincia de Buenos Aires. Facultad de Ingeniería Olavarría. Departamento de Electromecánica. Grupo INTELYMEC; Argentina. Universidad Nacional del Centro de la Provincia de Buenos Aires. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Provincia de Buenos Aires. Gobernación. Comisión de Investigaciones Científicas. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires; ArgentinaFil: Acosta, Gerardo Gabriel. Universidad Nacional del Centro de la Provincia de Buenos Aires. Facultad de Ingeniería Olavarría. Departamento de Electromecánica. Grupo INTELYMEC; Argentina. Universidad Nacional del Centro de la Provincia de Buenos Aires. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Provincia de Buenos Aires. Gobernación. Comisión de Investigaciones Científicas. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires; Argentin",'Elsevier BV',Double Q-PID algorithm for mobile robot control,10.1016/j.eswa.2019.06.066,,"[{'title': 'Expert Systems with Applications', 'identifiers': ['0957-4174', 'issn:1873-6793', '1873-6793', 'issn:0957-4174']}]",core
226070622,2017,"This paper presents a revision of Real Logic and its implementation with Logic Tensor Networks to address the problem of Semantic Image Interpretation. Real Logic is a framework where learning from numerical data and logical reasoning are integrated using first order logic syntax. The symbols of the signature of Real Logic are interpreted in the data-space, i.e, on the domain of real numbers. The integration of learning and reasoning obtained in Real Logic allows us to formalize learning as approximate satisfiability in the presence of logical constraints, and to perform inference on symbolic and numerical data. After introducing a refined version of the formalism, we describe its implementation into Logic Tensor Networks which uses deep learning within Google's Tensorflow. We evaluate LTN on the task of classifying objects and their parts in images, where we combine state-of-the-art-object detectors with a part-of ontology. LTN outperforms the state-of-the-art on object classification, and improves the performances on part-of relation detection with respect to a rule-based baseline",Learning and Reasoning in Logic Tensor Networks: Theory and Application to Semantic Image Interpretation,10.1145/3019612.3019642,,,,core
343432594,2017-03-01T00:00:00,"Frank Kendall, then Under Secretary of Defense for Acquisition, Technology and Logistics, released the first defense acquisition system performance report in June 2013. This report focused primarily on performance related to the collective outcomes of Major Defense Acquisition Programs (MDAPs), but additionally explored various descriptive dimensions and acquisition approaches of the same (Kendall, 2013). Each annual report builds on the work previously conducted, and focuses on data-driven analysis relying on statistical techniques to identify trends that improve the defense acquisition communityﾒs insights into how contract incentives are motivating better contractor/vendor performance (Kendal, 2016). Nevertheless, large amounts of data (in modern jargon, ﾓBig Dataﾔ) are now available for research in the area of defense acquisition. Over the past several years, changes in electronic commerce have increased the amounts of both structured and unstructured data availableﾗboth in runtime and archived environments. This electronic data, from a variety of different acquisition agencies, can be obtained by a variety of means and used for a multitude of purposes (Snider et al., 2014). Traditional statistical and trend analysis methods thus far have been primarily relied upon to explore trends and test metrics in the sets of acquisition data at hand. Sometimes, spreadsheets of linear regression correlation are employed, or, in some more modern applications, multivariate structural equation models via scientific applications such as SPSS and AMOS are leveraged for their ability to evaluate complex variable relationships, such as nested or recursive if-then patterns (Byrne, 2016). However, not only are todayﾒs modern datasets large in magnitude, they are also large in variety and complexity (Gartner, 2013). Furthermore, to address this state of data, new statistical modeling techniques, more powerful than before, have had to be created. This is due to the older methods finding difficulty with some of the size problems Big Data represents, such as privacy and security concerns (Parms, 2017). Thankfully, computer power necessary to employ the modern techniques is less expensive today, the software near free, and the storage capacities available now yield bewildering capacities at a fingertip, and with amazingly fast access speed. In fact, these performance parameters appear to continue along a Mooreﾒs trend line against critical opposition (Magee, Basnet, Funk, & Benson, 2015). Presently, one of the more interesting of the new statistical modeling techniques is neural network algorithm machine learning. Neural network modeling involves utilizing a ﾓpowerful computational data model that is able to capture and represent input/output relationships.ﾔ This model was developed out of the desire to create artificial intelligence systems capable of completing functions that were previously executed solely by the human brain. One benefit of using neural network modeling lies with its capacity to display and comprehend both linear and non-linear relationships from the data to which it is supplied (NeuroSolutions, 2015). Research Question Because ﾓBig Dataﾔ is present in the Defense Acquisition Business space, and, because the demand to critically understand real cause-and-effect relationships between variables within that data is persistent from the Acquisition community, this paperﾒs research question is, Can a neural network modeling technique be confidently relied upon to meaningfully explore variable relationships within acquisition business datasets? Because, if it is, then any question may be reasonably asked by anyone of such a dataset; and, via the neural network-enabled tool, the answers they receive will come with scientific statistical confidence as to whether they can be trusted as interesting or useful answers.1 In order to explore this research question, the study opted to use business data on contractor performance and attempted to isolate predictive variables from past performance information predictive of good performance.Naval Postgraduate School Acquisition Research Progra",Data Consolidation of Disparate Procurement Data Sources for Correlated Performance-Based Acquisition Decision Support,,https://core.ac.uk/download/343432594.pdf,"Monterey, California.  Naval Postgraduate School",,core
417852328,2017-06-01T00:00:00,"A valid model of the air surveillance system performance is highly valued when making decisions related to the optimal control of the system. We formulate a model for a multi-radar tracker system by combining a radar performance model with a tracker performance model. A tracker as a complex software system is hard to model mathematically and physically. Our novel approach is to utilize machine learning to create a tracker model based on measurement data from which the input and target output for the model are calculated. The measured data comprises the time series of 3D coordinates of cooperative aircraft flights, the corresponding target detection recordings from multiple radars, and the related multi-radar track recordings. The collected data is used to calculate performance measures for the radars and the tracker at specific locations in the air space. We apply genetic programming to learning such rules from radar performance measures that explain tracker performance. The easily interpretable rules are intended to reveal the real behavior of the system providing comprehension for its control and further development. The learned rules allow predicting tracker performance level for the system control in all radar geometries, modes, and conditions at any location. In the experiments, we show the feasibility of our approach to learning a tracker model and compare our rule learner with two tree classifiers, another rule learner, a neural network, and an instance-based classifier using the real air surveillance data. The tracker model created by our rule learner outperforms the models by the other methods except for the neural network whose prediction performance is equal.acceptedVersionPeer reviewe",Learning of a tracker model from multi-radar data for performance prediction of air surveillance system,10.1109/CEC.2017.7969562,https://core.ac.uk/download/417852328.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,core
375521228,2017-09-01T00:00:00,"Bulk body motion may randomly occur during PET acquisitions introducing blurring, attenuation-emission mismatches and, in dynamic PET, discontinuities in the measured time activity curves between consecutive frames. Meanwhile, dynamic PET scans are longer, thus increasing the probability of bulk motion. In this study, we propose a streamlined 3D PET motion-compensated image reconstruction (3D-MCIR) framework, capable of robustly deconvolving intra-frame motion from a static or dynamic 3D sinogram. The presented 3D-MCIR methods need not partition the data into multiple gates, such as 4D MCIR algorithms, or access list-mode (LM) data, such as LM MCIR methods, both associated with increased computation or memory resources. The proposed algorithms can support compensation for any periodic and non-periodic motion, such as cardio-respiratory or bulk motion, the latter including rolling, twisting or drifting. Inspired from the widely adopted point-spread function (PSF) deconvolution 3D PET reconstruction techniques, here we introduce an image-based 3D generalized motion deconvolution method within the standard 3D maximum-likelihood expectation-maximization (ML-EM) reconstruction framework. In particular, we initially integrate a motion blurring kernel, accounting for every tracked motion within a frame, as an additional MLEM modeling component in the image space (integrated 3D-MCIR). Subsequently, we replaced the integrated model component with a nested iterative Richardson-Lucy (RL) image-based deconvolution method to accelerate the MLEM algorithm convergence rate (RL-3D-MCIR). The final method was evaluated with realistic simulations of whole-body dynamic PET data employing the XCAT phantom and real human bulk motion profiles, the latter estimated from volunteer dynamic MRI scans. In addition, metabolic uptake rate Ki parametric images were generated with the standard Patlak method. Our results demonstrate significant improvement in contrast-to-noise ratio (CNR) and noise-bias performance in both dynamic and parametric images. The proposed nested RL-3D-MCIR method is implemented on the Software for Tomographic Image Reconstruction (STIR) open-source platform and is scheduled for public release",Quantitative PET image reconstruction employing nested expectation-maximization deconvolution for motion compensation,10.1016/j.compmedimag.2016.11.006,,'Elsevier BV',,core
201590612,2017-12-01T00:00:00,"Abstract Background In metagenomics, the separation of nucleotide sequences belonging to an individual or closely matched populations is termed binning. Binning helps the evaluation of underlying microbial population structure as well as the recovery of individual genomes from a sample of uncultivable microbial organisms. Both supervised and unsupervised learning methods have been employed in binning; however, characterizing a metagenomic sample containing multiple strains remains a significant challenge. In this study, we designed and implemented a new workflow, Coverage and composition based binning of Metagenomes (CoMet), for binning contigs in a single metagenomic sample. CoMet utilizes coverage values and the compositional features of metagenomic contigs. The binning strategy in CoMet includes the initial grouping of contigs in guanine-cytosine (GC) content-coverage space and refinement of bins in tetranucleotide frequencies space in a purely unsupervised manner. With CoMet, the clustering algorithm DBSCAN is employed for binning contigs. The performances of CoMet were compared against four existing approaches for binning a single metagenomic sample, including MaxBin, Metawatt, MyCC (default) and MyCC (coverage) using multiple datasets including a sample comprised of multiple strains. Results Binning methods based on both compositional features and coverages of contigs had higher performances than the method which is based only on compositional features of contigs. CoMet yielded higher or comparable precision in comparison to the existing binning methods on benchmark datasets of varying complexities. MyCC (coverage) had the highest ranking score in F1-score. However, the performances of CoMet were higher than MyCC (coverage) on the dataset containing multiple strains. Furthermore, CoMet recovered contigs of more species and was 18 - 39% higher in precision than the compared existing methods in discriminating species from the sample of multiple strains. CoMet resulted in higher precision than MyCC (default) and MyCC (coverage) on a real metagenome. Conclusions The approach proposed with CoMet for binning contigs, improves the precision of binning while characterizing more species in a single metagenomic sample and in a sample containing multiple strains. The F1-scores obtained from different binning strategies vary with different datasets; however, CoMet yields the highest F1-score with a sample comprised of multiple strains",CoMet: a workflow using contig coverage and composition for binning a metagenomic sample with high precision,10.1186/s12859-017-1967-3,,'Springer Science and Business Media LLC',"[{'title': 'BMC Bioinformatics', 'identifiers': ['1471-2105', 'issn:1471-2105']}]",core
201560971,2017-04-01T00:00:00,"For centuries, humans’ capacity to capture and depict physical space has played a central role in industrial and societal development. However, the digital revolution and the emergence of networked devices and services accelerate geospatial capture, coordination, and intelligence in unprecedented ways. Underlying the digital transformation of industry and society is the fusion of the physical and digital worlds – ‘perceptality’ – where geospatial perception and reality merge. This paper analyzes the myriad forces that are driving perceptality and the future of geospatial intelligence and presents real-world implications and examples of its industrial application. Applications of sensors, robotics, cameras, machine learning, encryption, cloud computing and other software, and hardware intelligence are converging, enabling new ways for organizations and their equipment to perceive and capture reality. Meanwhile, demands for performance, reliability, and security are pushing compute ‘to the edge’ where real-time processing and coordination are vital. Big data place new restraints on economics, as pressures abound to actually use these data, both in real-time and for longer term strategic analysis and decision-making. These challenges require orchestration between information technology (IT) and operational technology (OT) and synchronization of diverse systems, data-sets, devices, environments, workflows, and people",The future of geospatial intelligence,10.1080/10095020.2017.1337318,,'Informa UK Limited',"[{'title': 'Geo-spatial Information Science', 'identifiers': ['issn:1993-5153', 'issn:1009-5020', '1993-5153', '1009-5020']}]",core
84832400,2017-06-15T00:00:00,"Moderní Satelity produkují velké množství telemetrických dat (> 10.000 parametrů), z nichž ty nejdůležitější jsou monitorovány provozovatelé během průchodů. Zpracování tohoto množství dat v reálném čase přesáhne schopnost analýzy založené na lidských. To vedlo k nárůstu takzvaných velkých dat a strojového učení systémů, které se učí od tohoto data. V posledních letech se řešení byla vyvinuta pro detekci dosud neznámých situací v těchto parametrů a ve stavu systému obecně. S touhou pro analýzu dat v reálném čase přístup, který využívá open source technologií a distribuované výpočetní je žádoucí. Předchozí implementace algoritmu zjišťování nadměrných odchylek, je k dispozici, jakož i reálných dat ESA telemetrie. Tato práce řeší problém živé detekce odlehlých v prostředí vesmírných misích, zejména zpracování v časovém omezení jediné nadjezdu.Modern satellites produce a large amount of telemetry data (> 10.000 parameters) among which the most important ones are monitored by operators during passes. Processing this amount of data in real time exceeds the capability of human-based analyses. This has led to a rise in the so-called big data and machine learning systems that learn from this data. In the last years, solutions have been developed for detecting previously unknown situations in these parameters and in the system state in general. With the desire to analyse data in real time an approach that leverages open source technologies and distributed computing is desired. A previous implementation of an outlier detection algorithm is provided, as well as real world ESA telemetry data. This thesis solves the problem of live outlier detection in the environment of Space missions, especially the processing in the time-constraint of a single overpass",Novelty Detection in Spacecraft Telemetry Data,,,Czech Technical University in Prague. Computing and Information Centre.,,core
237120514,2017-08-20T00:00:00,"Understanding large multidimensional datasets is one of the most challenging problems in visual data exploration. One key challenge that increases the size of the exploration space is the number of views that one can generate from a single dataset, based on the use of multiple parameter values and exploration paths. Often, no such single view contains all needed insights. The question thus arises of how we can efficiently combine insights from multiple views of a dataset. We propose a set of techniques that considerably reduce the exploration effort for such situations, based on the explicit depiction of the view space, using a small multiple metaphor. We leverage this view space by offering interactive techniques that enable users to explicitly create, visualize, and follow their exploration path. This way, partial insights obtained from each view can be efficiently and effectively combined. We demonstrate our approach by applications using real-world datasets from air traffic control, software maintenance, and machine learning",Multidimensional Data Exploration by Explicitly Controlled Animation,10.3390/informatics4030026,,'MDPI AG',,core
83834662,2017-12-12T00:00:00,"We study two procedures (reverse-mode and forward-mode) for computing the
gradient of the validation error with respect to the hyperparameters of any
iterative learning algorithm such as stochastic gradient descent. These
procedures mirror two methods of computing gradients for recurrent neural
networks and have different trade-offs in terms of running time and space
requirements. Our formulation of the reverse-mode procedure is linked to
previous work by Maclaurin et al. [2015] but does not require reversible
dynamics. The forward-mode procedure is suitable for real-time hyperparameter
updates, which may significantly speed up hyperparameter optimization on large
datasets. We present experiments on data cleaning and on learning task
interactions. We also present one large-scale experiment where the use of
previous gradient-based methods would be prohibitive.Comment: - Posted the ICML Camera Ready version. - Added a link to a newer
  package implementation of the algorithm",Forward and Reverse Gradient-Based Hyperparameter Optimization,,http://arxiv.org/abs/1703.01785,,,core
141530749,2017-11-22T00:00:00,"The goal of graph representation learning is to embed each vertex in a graph
into a low-dimensional vector space. Existing graph representation learning
methods can be classified into two categories: generative models that learn the
underlying connectivity distribution in the graph, and discriminative models
that predict the probability of edge existence between a pair of vertices. In
this paper, we propose GraphGAN, an innovative graph representation learning
framework unifying above two classes of methods, in which the generative model
and discriminative model play a game-theoretical minimax game. Specifically,
for a given vertex, the generative model tries to fit its underlying true
connectivity distribution over all other vertices and produces ""fake"" samples
to fool the discriminative model, while the discriminative model tries to
detect whether the sampled vertex is from ground truth or generated by the
generative model. With the competition between these two models, both of them
can alternately and iteratively boost their performance. Moreover, when
considering the implementation of generative model, we propose a novel graph
softmax to overcome the limitations of traditional softmax function, which can
be proven satisfying desirable properties of normalization, graph structure
awareness, and computational efficiency. Through extensive experiments on
real-world datasets, we demonstrate that GraphGAN achieves substantial gains in
a variety of applications, including link prediction, node classification, and
recommendation, over state-of-the-art baselines.Comment: The 32nd AAAI Conference on Artificial Intelligence (AAAI 2018), 8
  page",GraphGAN: Graph Representation Learning with Generative Adversarial Nets,,http://arxiv.org/abs/1711.08267,,,core
201658400,2017-08-01T00:00:00,"Understanding large multidimensional datasets is one of the most challenging problems in visual data exploration. One key challenge that increases the size of the exploration space is the number of views that one can generate from a single dataset, based on the use of multiple parameter values and exploration paths. Often, no such single view contains all needed insights. The question thus arises of how we can efficiently combine insights from multiple views of a dataset. We propose a set of techniques that considerably reduce the exploration effort for such situations, based on the explicit depiction of the view space, using a small multiple metaphor. We leverage this view space by offering interactive techniques that enable users to explicitly create, visualize, and follow their exploration path. This way, partial insights obtained from each view can be efficiently and effectively combined. We demonstrate our approach by applications using real-world datasets from air traffic control, software maintenance, and machine learning",Multidimensional Data Exploration by Explicitly Controlled Animation,10.3390/informatics4030026,,'MDPI AG',"[{'title': 'Informatics', 'identifiers': ['2227-9709', 'issn:2227-9709']}]",core
78511725,2017-02-21T00:00:00,"Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.Comment: 15 pages, 10 figures, 7 tables. To be published in a scientific
  journa","Towards a Common Implementation of Reinforcement Learning for Multiple
  Robotic Tasks",10.1016/j.eswa.2017.11.011,http://arxiv.org/abs/1702.06329,'Elsevier BV',,core
78508072,2017-02-08T00:00:00,"In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the ""product"" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This ""product"" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet).Comment: 8 pages (double column), 2 figures, 1 tabl",Energy Saving Additive Neural Network,,http://arxiv.org/abs/1702.02676,,,core
73423146,2017-03-27T00:00:00,"Despite the overwhelming success of deep learning in various speech
processing tasks, the problem of separating simultaneous speakers in a mixture
remains challenging. Two major difficulties in such systems are the arbitrary
source permutation and unknown number of sources in the mixture. We propose a
novel deep learning framework for single channel speech separation by creating
attractor points in high dimensional embedding space of the acoustic signals
which pull together the time-frequency bins corresponding to each source.
Attractor points in this study are created by finding the centroids of the
sources in the embedding space, which are subsequently used to determine the
similarity of each bin in the mixture to each source. The network is then
trained to minimize the reconstruction error of each source by optimizing the
embeddings. The proposed model is different from prior works in that it
implements an end-to-end training, and it does not depend on the number of
sources in the mixture. Two strategies are explored in the test time, K-means
and fixed attractor points, where the latter requires no post-processing and
can be implemented in real-time. We evaluated our system on Wall Street Journal
dataset and show 5.49\% improvement over the previous state-of-the-art methods.Comment: 2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP",Deep attractor network for single-microphone speaker separation,10.1109/ICASSP.2017.7952155,http://arxiv.org/abs/1611.08930,'Institute of Electrical and Electronics Engineers (IEEE)',,core
199457819,2017-01-01T00:00:00,"В даній роботі розглядається один із етапів розпізнавання зображення, кодування та аналіз інформації, що успішно використовується в методах штучного інтелекту. Запропонований символьний опис зображення «центр-образ» представляється в ущільненому вигляді та передбачає більш прості перетворення для виділення метричних ознак. Створені еталони, що враховують властивості математичної моделі, розширюють інформаційний простір ознак, який прийнятний для аналізу плоских та просторових образів в технічних засобах. Для апаратурної реалізації в кожному конкретному випадку слід вибирати компромісні варіанти та забезпечувати обробку зображення в системах реального часу. Реалізація блока обробки в таких системах розпізнавання з використанням нанотехнологій дозволяє досягати високої продуктивності, забезпечувати високу швидкість, інформаційну щільність, широку полосу частот пропускання та малі витрати на передачу.В данной работе рассматривается один из этапов распознавания изображений, кодирование и анализ информации, который успешно используется в методах искусственного интеллекта. Предложенное символьное описание изображения «центр-образ» представляется в сжатом виде и предусматривает более простые преобразования для выделения метрических признаков. Созданные эталоны, учитывающие свойства математической модели, расширяют информационное пространство признаков, которые приемлемы для анализа плоских и пространственных образов в технических средствах. Для аппаратурной реализации в каждом конкретном случае следует выбирать компромиссные варианты и обеспечивать обработку изображения в системах реального времени. Реализация блока обработки в таких системах распознавания с использованием нанотехнологий позволяет достигать высокой производительности, обеспечивать высокую скорость, информационную плотность, широкую полосу частот пропускания и малые затраты на передачу.In this paper, one of the stages of image recognition, coding and analysis of information that is successfully used in artificial intelligence methods is considered. The proposed symbolic description of the image the ""center-pattern"" is presented in a compressed form and provides simpler transformations for the allocation of metric features. The created standards, that consider the properties of the mathematical model, expand the information space of features that are acceptable for analyzing flat and spatial images in technical devices. For hardware implementation in each specific case choose compromise options and provide image processing in real-time systems. The implementation of the processing unit in such recognition systems using nanotechnology allows achieving high performance, providing high speed, information density, wide bandwidth and low transmission costs",Метричні ознаки в двовимірному та тривимірному просторі,,https://core.ac.uk/download/199457819.pdf,ВНТУ,"[{'title': 'Optoelectronic Information-Power Technologies', 'identifiers': ['issn:2311-2662', 'issn:1681-7893', '2311-2662', '1681-7893']}]",core
343439754,2017-03-01T00:00:00,"Frank Kendall, then Under Secretary of Defense for Acquisition, Technology and Logistics, released the first defense acquisition system performance report in June 2013. This report focused primarily on performance related to the collective outcomes of Major Defense Acquisition Programs (MDAPs), but additionally explored various descriptive dimensions and acquisition approaches of the same (Kendall, 2013). Each annual report builds on the work previously conducted, and focuses on data-driven analysis relying on statistical techniques to identify trends that improve the defense acquisition communityﾒs insights into how contract incentives are motivating better contractor/vendor performance (Kendal, 2016). Nevertheless, large amounts of data (in modern jargon, ﾓBig Dataﾔ) are now available for research in the area of defense acquisition. Over the past several years, changes in electronic commerce have increased the amounts of both structured and unstructured data availableﾗboth in runtime and archived environments. This electronic data, from a variety of different acquisition agencies, can be obtained by a variety of means and used for a multitude of purposes (Snider et al., 2014). Traditional statistical and trend analysis methods thus far have been primarily relied upon to explore trends and test metrics in the sets of acquisition data at hand. Sometimes, spreadsheets of linear regression correlation are employed, or, in some more modern applications, multivariate structural equation models via scientific applications such as SPSS and AMOS are leveraged for their ability to evaluate complex variable relationships, such as nested or recursive if-then patterns (Byrne, 2016). However, not only are todayﾒs modern datasets large in magnitude, they are also large in variety and complexity (Gartner, 2013). Furthermore, to address this state of data, new statistical modeling techniques, more powerful than before, have had to be created. This is due to the older methods finding difficulty with some of the size problems Big Data represents, such as privacy and security concerns (Parms, 2017). Thankfully, computer power necessary to employ the modern techniques is less expensive today, the software near free, and the storage capacities available now yield bewildering capacities at a fingertip, and with amazingly fast access speed. In fact, these performance parameters appear to continue along a Mooreﾒs trend line against critical opposition (Magee, Basnet, Funk, & Benson, 2015). Presently, one of the more interesting of the new statistical modeling techniques is neural network algorithm machine learning. Neural network modeling involves utilizing a ﾓpowerful computational data model that is able to capture and represent input/output relationships.ﾔ This model was developed out of the desire to create artificial intelligence systems capable of completing functions that were previously executed solely by the human brain. One benefit of using neural network modeling lies with its capacity to display and comprehend both linear and non-linear relationships from the data to which it is supplied (NeuroSolutions, 2015). Research Question Because ﾓBig Dataﾔ is present in the Defense Acquisition Business space, and, because the demand to critically understand real cause-and-effect relationships between variables within that data is persistent from the Acquisition community, this paperﾒs research question is, Can a neural network modeling technique be confidently relied upon to meaningfully explore variable relationships within acquisition business datasets? Because, if it is, then any question may be reasonably asked by anyone of such a dataset; and, via the neural network-enabled tool, the answers they receive will come with scientific statistical confidence as to whether they can be trusted as interesting or useful answers.1 In order to explore this research question, the study opted to use business data on contractor performance and attempted to isolate predictive variables from past performance information predictive of good performance.Naval Postgraduate School Acquisition Research Progra",Data Consolidation of Disparate Procurement Data Sources for Correlated Performance-Based Acquisition,,https://core.ac.uk/download/343439754.pdf,"Monterey, California.  Naval Postgraduate School",,core
132714466,2017-9,"Bulk body motion may randomly occur during PET acquisitions introducing blurring, attenuation emission mismatches and, in dynamic PET, discontinuities in the measured time activity curves between consecutive frames. Meanwhile, dynamic PET scans are longer, thus increasing the probability of bulk motion. In this study, we propose a streamlined 3D PET motion-compensated image reconstruction (3D-MCIR) framework, capable of robustly deconvolving intra-frame motion from a static or dynamic 3D sinogram. The presented 3D-MCIR methods need not partition the data into multiple gates, such as 4D MCIR algorithms, or access list-mode (LM) data, such as LM MCIR methods, both associated with increased computation or memory resources. The proposed algorithms can support compensation for any periodic and non-periodic motion, such as cardio-respiratory or bulk motion, the latter including rolling, twisting or drifting. Inspired from the widely adopted point-spread function (PSF) deconvolution 3D PET reconstruction techniques, here we introduce an image-based 3D generalized motion deconvolution method within the standard 3D maximum-likelihood expectation-maximization (ML-EM) reconstruction framework. In particular, we initially integrate a motion blurring kernel, accounting for every tracked motion within a frame, as an additional MLEM modeling component in the image space (integrated 3D-MCIR). Subsequently, we replaced the integrated model component with a nested iterative Richardson-Lucy (RL) image-based deconvolution method to accelerate the MLEM algorithm convergence rate (RL-3D-MCIR). The final method was evaluated with realistic simulations of whole-body dynamic PET data employing the XCAT phantom and real human bulk motion profiles, the latter estimated from volunteer dynamic MRI scans. In addition, metabolic uptake rate K-i parametric images were generated with the standard Patlak method. Our results demonstrate significant improvement in contrast-to-noise ratio (CNR) and noise bias performance in both dynamic and parametric images. The proposed nested RL-3D-MCIR method is implemented on the Software for Tomographic Image Reconstruction (STIR) open-source platform and is scheduled for public release. (C) 2016 Elsevier Ltd. All rights reserved",Quantitative PET image reconstruction employing nested expectation-maximization deconvolution for motion compensation,10.1016/j.compmedimag.2016.11.006,,,,core
201032537,2017-01-01T00:00:00,"The relevance in development of intelligent systems for classification of complex structured images occurs during processing of images taken from cameras of UAV used for navigational purposes where there is no communication with artificial Earth satellites, or in the analysis of images in real time by the operator. The developed method provides high requirements to quality of classification of objects on images, as well as to fast selection and classification of investigated segments of images. For classification of such images the appropriate computer technologies based on the boosting methodology are offered. The space of Informative features is formed by spectral windows obtained by scanning of the original image. Spectral windows belonging to different classes, are arranged in the form of clusters on Kohonen plane. To form a cluster, the rules of correction of vectors of weights are used, and such rules make it possible to reduce the values of insignificant components of the vectors and the coordinates of the clusters centers are identified. Strong classifiers are built on the basis of the cluster structure of Kohonen plane. There has been designed and demonstrated the structure of the strong classifier on neural networks of direct distribution referred to block type, which was implemented for classification of chest X-ray images",METHOD OF CLASSIFICATION  OF COMPLEX STRUCTURED IMAGES  ON THE BASIS OF SELF-ORGANIZED  NEURAL NETWORK STRUCTURES,10.21778/2413-9599-2016-4-57-65,https://core.ac.uk/download/201032537.pdf,'CRI Electronics',"[{'title': 'Radio Industry (Russia)', 'identifiers': ['issn:2413-9599', 'issn:2541-870X', '2541-870x', '2413-9599']}]",core
159235367,2018-01-01T08:00:00,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS",Data-driven model-free slip control of anti-lock braking systems using reinforcement Q-learning,10.1016/j.neucom.2017.08.036,,'Elsevier BV',,core
130246457,2017-10-27T00:00:00,"Activity Recognition (AR) is nowadays a fervent research area which gives many new challenges to deal with. In particular, we focus our attention on the development and analysis of classifiers based on Machine Learning (ML) approaches in the areas of Human Activity Recognition (HAR) and biologging. The literature already presents many different approaches for ML classifiers for HAR problems, especially in monitoring of humans in domestic environments. Less common is the application of ML classifiers in biologging, which is still commonly studied through traditional methods. In our research we applied classifiers implemented by ML approaches from the classes of Neural Network (NN) and of Support Vector Machine (SVM). The classification of the activities was performed over time−series collected by sensory devices. The devices were worn by each subject of the case studies considered. The classifiers were configured and tuned specifically for each case study at hand. In this thesis we dealt with four cases of study: with humans to identify daily activities, with tortoises to identify the digging activity, and with penguins and seals to identify the prey handling activity. These case studies covered an heterogeneous set of both HAR and biologging problems. The classifier applied via shift-window over the time and specifically tuned by input sequences (windows shifted over the input time−series) is implemented by Input Delay
Neural Network (IDNN), Convolutional Neural Network (CNN), and SVM which naturally deal with these input. For the same case studies, we implemented the classifier by models from the Recurrent Neural Networks (RNN) class, which naturally apply over streams by taking advantage from their internal memory. We evaluated each implementation of the classifier by means of its accuracy and F1 score reached in classification, and by assessing its feasibility for its use into embedded devices in terms of memory space. We demonstrated that with sequences as input, the IDNN model provides a good trade off between performance (accuracy of the model) and feasibility (memory footprint of the
model). Instead with streams we observed that the Echo State Network (ESN) reaches a good performance and it is feasible as well because the reservoir can be kept small without a significant penalty in terms of performance of classification.
The results of this analysis would contribute to improve future activity recognition methods. We showed that it was possible to implement efficient classifiers in selected real−world case studies. In particular, such efficiency of the classifiers allows to meet the performance requirements of real applications enabling the embedding of the classifiers into low−power devices. In perspective, we believe that this research will support future research directions with the focus on
stimulating research in the directions of animals’ monitoring/protection",Analysis of vertebrates’ activity by machine learning,,,'Pisa University Press',,core
145162947,2017-06-16T00:00:00,"Part 1: AlgorithmsInternational audienceThe size, complexity and dimensionality of data collections are ever increasing from the beginning of the computer era. Clustering is used to reveal structures and to reduce large amounts of raw data. There are two main issues when clustering based on unsupervised learning, such as Growing Neural Gas (GNG) [9], is performed on vast high dimensional data collection – the fast growth of computational complexity with respect to growing data dimensionality, and the specific similarity measurement in a high-dimensional space. These two factors reduce the effectiveness of clustering algorithms in many real applications. The growth of computational complexity can be partially solved using the parallel computation facilities, such as High Performance Computing (HPC) cluster with MPI. An effective parallel implementation of GNG is discussed in this paper, while the main focus is on minimizing of interprocess communication. The achieved speed-up was better than previous approach and the results from the standard and parallel version of GNG are same",Optimalization of Parallel GNG by Neurons Assigned to Processes,10.1007/978-3-319-59105-6_6,,'Springer Science and Business Media LLC',,core
322435195,2018-08-01T00:00:00,"Standard security systems are widely implemented in the industry. These systems consume considerable computational resources. Devices in the Internet of Things [IoT] are very limited with processing capacity, memory and storage. Therefore, existing security systems are not applicable for IoT. To cope with it, we propose downsizing of existing security processes. In this chapter, we describe three areas, where we reduce the required storage space and processing power. The first is the classification process required for ongoing anomaly detection, whereby values accepted or generated by a sensor are classified as valid or abnormal. We collect historic data and analyze it using machine learning techniques to draw a contour, where all streaming values are expected to fall within the contour space. Hence, the detailed collected data from the sensors are no longer required for real-time anomaly detection. The second area involves the implementation of the Random Forest algorithm to apply distributed and parallel processing for anomaly discovery. The third area is downsizing cryptography calculations, to fit IoT limitations without compromising security. For each area, we present experimental results supporting our approach and implementation",An Adaptive Lightweight Security Framework Suited for IoT,10.5772/intechopen.73712,https://core.ac.uk/download/322435195.pdf,'IntechOpen',,core
211495095,2018-01-01T00:00:00,"The Big Data revolution has created new requirements for the design of applications and operators that are able to handle the volume of the data sources. The adoption of distributed architectures and the increasing popularity of the Cloud paradigm has complexed their structure, making the problem of modeling their behavior increasingly difficulty. Moreover, the wide variety of the existing datasets have complicated the problem of selecting the appropriate inputs for a given operator, since the examination of the data utility for a given workflow is a largely manual process that requires exhaustive execution for the entirety of the available datasets. This thesis attempts to model the behavior of an arbitrary Big Data operator from two different viewpoints.First, we wish to model the operator’s performance when deployed under different resource configurations. To this end, we present an adaptive performance modeling methodology that relies on recursively partitioning the configuration space in disjoint regions, distributing a predefined number of samples to each region based on different region characteristics (i.e., size, modeling error) and deploying the given operator for the selected samples. The performance is, then, approximated for the entire space using a combination of linear models for each subregion. Intuitively, this approach attempts to compromise the contradicting aspects of exploring the configuration space and exploiting the obtained knowledge through focusing on areas with higher approximation error.Second and in order to accelerate data analysis, we wish to model the operator’s output when deployed over different datasets. Based on the observation that similar datasets tend to affect the operators that are applied to them similarly, we propose a content-based methodology that models the output of a provided operator for all datasets. Our approach measures the similarity between the different datasets in the light of some fundamental properties commonly used in data analysis tasks, i.e., the statistical distribution, the dataset size and the tuple ordering. These similarities are, next, projected to a low dimensional metric space that is utilized as an input domain by Neural Networks in order to approximate the operator’s output for all datasets, given the actual operator output for a mere subset of them. Our evaluation, conducted using several real-world operators applied for real and synthetic datasets, indicated that the introduced methodologies manage to accurately model the operator’s behavior from both angles. The adoption of a divide-and-conquer approach that equally respects space exploration and knowledge exploitation for the performance modeling part, proved to be the main reason that our scheme outperforms other state-of-the-art methodologies. On the same time, the construction of a low dimensional dataset metric space for the second part, proved to be particularly informative in order to allow Machine Learning models to approximate operator output for a wide variety of operators with diverse characteristics.Η επανάσταση των Μεγάλων Δεδομένων έχει δημιουργήσει νέες απαιτήσεις για το σχεδιασμό εφαρμογών και τελεστών έτσι ώστε να μπορούν να διαχειρίζονται τον τεράστιο όγκο δεδομένων. Η υιοθέτηση κατανεμημένων τεχνικών και η ολοένα αυξανόμενη δημοτικότητα των Υπολογιστικών Νεφών έχουν συντελέσει στην αύξηση της πολυπλοκότητας της αρχιτεκτονικής των τελεστών Μεγάλων Δεδομένων, κάνοντας το πρόβλημα της μοντελοποίησης της συμπεριφοράς τους ολοένα και πιο δύσκολο. Παράλληλα, η μεγάλη ποικιλομορφία των διαφορετικών πηγών δεδομένων έχει περιπλέξει το πρόβλημα της επιλογής των κατάλληλων εισόδων για έναν τελεστή, καθώς η εξέταση της χρησιμότητας των δεδομένων εισόδου για αυτόν είναι μια μη αυτοματοποιημένη διαδικασία που στηρίζεται στην εξαντλητική εκτέλεση του τελεστή για το σύνολο των διαθέσιμων δεδομένων. Η διατριβή αυτή προσπαθεί να μοντελοποιήσει τη συμπεριφορά ενός δοθέντος τελεστή Μεγάλων Δεδομένων, υπό το πρίσμα δύο διαφορετικών κατευθύνσεων.Πρώτον, η διατριβή αυτή ασχολείται με το πρόβλημα της μοντελοποίησης της απόδοσης ενός τελεστή όταν αυτός εγκαθίσταται με διαφορετικές παραμέτρους. Για το σκοπό αυτό, παρουσιάζεται μια προσαρμοστική μεθοδολογία μοντελοποίησης της απόδοσης μιας εφαρμογής, που στηρίζεται: (α) στην αναδρομική διαμέριση του χώρου παραμέτρων, (β) στην κατανομή ενός προαποφασισμένου αριθμού δειγμάτων σε κάθε υποπεριοχή σύμφωνα με διαφορετικά χαρακτηριστικά της (π.χ., το μέγεθός της, το σφάλμα μοντελοποίησης, κλπ) και (γ) στην φυσική εγκατάσταση της εφαρμογής για τα επιλεχθέντα σύνολα παραμέτρων. Η απόδοση προσεγγίζεται για ολόκληρο το χώρο χρησιμοποιώντας ένα συνδυασμό γραμμικών μοντέλων που εφαρμόζεται σε κάθε υποπεριοχή. Διαισθητικά, η προσέγγιση αυτή προσπαθεί να συμβιβάσει τις αντίρροπες κατευθύνσεις της εξερεύνησης του χώρου παραμέτρων και της εκμετάλλευσης της αποκτηθείσας γνώσης (μέσω των δειγμάτων που έχουν επιλεγεί προηγούμενα) διαμέσου της συγκέντρωσης σε περιοχές με υψηλό σφάλμα προσέγγισης.Δεύτερον και με σκοπό την επιτάχυνση της ανάλυσης των δεδομένων, η διατριβή αυτή προτείνει μια μεθοδολογία για τη μοντελοποίηση της εξόδου ενός τελεστή όταν αυτός εφαρμόζεται σε διαφορετικά σύνολα δεδομένων εισόδου. Με βάση την παρατήρηση ότι όμοια σύνολα δεδομένων τείνουν να επηρεάζουν έναν τελεστή με παρόμοιο τρόπο, προτείνεται μια βασισμένη στο περιεχόμενο μεθοδολογία που μοντελοποιεί την έξοδο ενός τελεστή για όλα τα δεδομένα εισόδου. Η προσέγγιση αυτή ποσοτικοποιεί την ομοιότητα μεταξύ των διαφορετικών συνόλων δεδομένων υπό το πρίσμα τριών θεμελιωδών ιδιοτήτων: (α) τη στατιστική κατανομή τους, (β) το μέγεθος τους και (γ) τη σειρά εμφάνισης των πλειάδων τους. Η ομοιότητα μεταξύ των διαφορετικών συνόλων προβάλλεται, εν συνεχεία, σε ένα μετρικό χώρο χαμηλής διάστασης και χρησιμοποιείται σαν σύνολο ορισμού από ένα Νευρωνικό Δίκτυο που έχει ως σκοπό την προσέγγιση της εξόδου του τελεστή για όλα τα σύνολα, δοθέντων την πραγματικών τιμών εξόδου για ένα μικρό υποσύνολο τους.Η πειραματική αξιολόγηση, που πραγματοποιήθηκε χρησιμοποιώντας μεγάλη πληθώρα πραγματικών τελεστών που εκτελούνται τόσο για πραγματικά όσο και συνθετικά δεδομένα εισόδου, έδειξε ότι οι προτεινόμενες μεθοδολογίες μπορούν μοντελοποιήσουν με υψηλή ακρίβεια τη συμπεριφορά ενός τελεστή Μεγάλων Δεδομένων και από τις δυο εξεταζόμενες σκοπιές. Η υιοθέτηση της τεχνικής “διαίρει και βασίλευε” που σέβεται εξίσου την εξερεύνηση του χώρου παραμέτρων και την εκμετάλλευση της παραγόμενης γνώσης σχετικά με τη μοντελοποίησης της απόδοσης, είναι ο κύριος λόγος που εξηγεί την υψηλότερη ακρίβεια που πετυχαίνει η προταθείσα μεθοδολογία, εν συγκρίσει με άλλες, παρεμφερείς μεθοδολογίες μοντελοποίησης απόδοσης. Παράλληλα, ο μετρικός χώρος χαμηλής διάστασης, που κατασκευάζεται σχετικά με το δεύτερο κομμάτι της διατριβής, περιέχει αρκετή πληροφορία για να επιτρέψει μοντέλα Μηχανικής Μάθησης να προσεγγίσουν την έξοδο ενός μεγάλου αριθμού τελεστών με διαφορετικά χαρακτηριστικά",Modeling big data applications and operators in cloud environments,10.12681/eadd/45004,,'National Documentation Centre (EKT)',,core
428380032,2017-05-01T07:00:00,"Report from a meeting held on the topic of disinformation, the Internet, and public diplomacy held at the Hoover Institution, Stanford University, in 2017.
Executive Summary
Scientific progress continues to accelerate, and while we’ve witnessed a revolution in communication technologies in the past ten years, what proceeds in the next ten years may be far more transformative. It may also be extremely disruptive, challenging long held conventions behind public diplomacy (PD) programs and strategies. In order to think carefully about PD in this ever and rapidly changing communications space, the Advisory Commission on Public Diplomacy (ACPD) convened a group of private sector, government, and academic experts at Stanford University’s Hoover Institution to discuss the latest trends in research on strategic communication in digital spaces. The results of that workshop, refined by a number of follow-on interviews and discussions, are included in this report. I encourage you to read each of the fourteen essays that follow, which are divided into three thematic sections: Digital’s Dark Side, Disinformation, and Narratives.
Digital’s Dark Side focuses on the emergence of social bots, artificial intelligence, and computational propaganda. Essays in this section aim to raise awareness regarding how technology is transforming the nature of digital communication, offer ideas for competing in this space, and raise a number of important policy and research questions needing immediate attention. The Disinformation section confronts Oxford English Dictionary’s 2016 word of the year – “post-truth” – with a series of compelling essays from practitioners, a social scientist, and philosopher on the essential roles that truth and facts play in a democratic society. Here, theory, research, and practice neatly align, suggesting it is both crucial and effective to double-down on fact-checking and evidence-based news and information programming in order to combat disinformation campaigns from our adversaries. The Narrative section concludes the report by focusing on how technology and facts are ultimately part of, and dependent on, strategic narratives. Better understanding how these narratives form, and what predicts their likely success, is necessary to think through precisely how PD can, indeed, survive the Internet. Below are some key takeaways from the report.
In Defense of Truth
• We are not living in a “post-truth” society. Every generation tends to think that the current generation is less honest than the previous generation. This is an old human concern, and should be seen today as a strategic narrative (see Hancock, p. 49; Roselle, p. 77). Defending the value and search for truth is crucial. As Jason Stanley notes (p. 71), “without truth, there is just power.”
• Humans are remarkably bad at detecting deception. Studies show that people tend to trust what others say, an effect called the truth bias. This bias is actually quite rational—most of the messages that a person encounters in a day are honest, so being biased toward the truth is almost always the correct response (see Hancock, p.49).
• At the same time people are also continuously evaluating the validity of their understanding of the world. This process is called “epistemic vigilance,” a continuous process checking that the information that a person believes they know about the world is accurate. While we have a difficult time detecting deception from interpersonal cues, people can detect lies when they have the time, resources, and motivation. Lies are often discovered through contradicting information from a third source, or evidence that challenges a deceptive account (see Hancock, p. 49).
• Fact checking can be effective, even in hyper-partisan settings (see Porter, p. 55), and is crucial for sustained democratic dialogue (Bennett, p. 61; Stanley, p. 71). Moreover, it is possible, using digital tools, to detect and effectively combat disinformation campaigns in real time (Henick and Walsh, p. 65).
Computational Propaganda
• Computational propaganda refers to the coordinated use of social media platforms, autonomous agents and big data directed towards the manipulation of public opinion.
• Social media bots (or “web robots”) are the primary tools used in the dissemination of computational propaganda. In their most basic form, bots provide basic answers to simple questions, publish content on a schedule or disseminate stories in response to triggers (e.g. breaking news). Bots can have a disproportionate impact because it is easy to create a lot of them and they can post a high-volume content at a high frequency (see Woolley, p.13).
• Political bots aim to automate political engagement in an attempt to manipulate public opinions. They allow for massive amplification of political views and can empower a small group of people to set conversation agenda’s online. Political bots are used over social media to manufacture trends, game hashtags, megaphone particular content, spam opposition and attack journalists. The noise, spam and manipulation inherent in many bot deployment techniques threaten to disrupt civic conversations and organization worldwide (see Chessen, p.19).
• Advances in artificial intelligence (AI) – an evolving constellation of technologies enabling computers to simulate cognitive processes – will soon enable highly persuasive machine-generated communications. Imagine an automated system that uses the mass of online data to infer your personality, political preferences, religious affiliation, demographic data and interests. It knows which news websites and social media platforms you frequent and it controls multiple user accounts on those platforms. The system dynamically creates content specifically designed to plug into your particular psychological frame and achieve a particular outcome (see Chessen, p. 39).
• Digital tools have tremendous advantages over humans. Once an organization creates and configures a sophisticated AI bot, the marginal cost of running it on thousands or millions of user accounts is relatively low. They can operate 24/7/365 and respond to events almost immediately. AI bots can be programmed to react to certain events and create content at machine speed, shaping the narrative almost immediately. This is critical in an information environment where the first story to circulate may be the only one that people recall, even if it is untrue (see Chessen, p. 39).
• PD practitioners need to consider the question of how they can create and sustain meaningful conversations and engagements with audiences if the mediums typically relied upon are becoming less trusted, compromised and dominated by intelligent machines.
• Challenging computational propaganda should include efforts to ensure the robustness and integrity of the marketplace of information online. Defensively, this strategy would focus on producing patterns of information exchange among groups that would make them difficult to sway using techniques of computational propaganda. Offensively, the strategy would seek to distribute the costs of counter-messaging broadly, shaping the social ecosystem to enable alternative voices to effectively challenge campaigns of misinformation (see Hwang, p. 27). In the persuasive landscape formed by social media and computational propaganda, it may be at times more effective to build tools, rather than construct a specific message.
• Practitioners are not alone in their concern about the escalating use of social bots by adversarial state actors. The private sector is, too. Social media platforms see this trend as a potentially existential threat to their business models, especially if the rise of bots and computational propaganda weakens users’ trust in the integrity of the platforms themselves. Coordination with private sector is key, as their policies governing autonomous bots will adapt and, thus, shape what is and isn’t feasible online.
Moving Past Folk Theories
• Folk theories, or how people think a particular process works, are driving far too many digital strategies. One example of a folk theory is in the prevalence of echo chambers online, or the idea that people are increasingly digitally walled off from one another, engaging only with content that fits cognitive predispositions and preferences.
• Research suggests that the more users rely on digital platforms (e.g. Twitter and Facebook) for their news and information, the more exposure they have to a multitude of sources and stories. This remains true even among partisans (though to a lesser extent than non-partisans). It turns out we haven’t digitally walled ourselves off after all (see Henick and Walsh, p. 65).
• Despite increased exposure to a pluralistic media ecosystem, we are becoming more and more ideological and partisan, and becoming more walled off at the interpersonal and physical layers. For example, marriages today are twice as likely to be between two people with similar political views than they were in 1960.
• Understanding this gap between a robustly diverse news environment and an increasingly “siloed” physical environment is crucial to more effectively engaging with target audiences around the world. Interpersonal and in-person engagement, including exchange programs, remain crucial for effective PD moving forward (see Wharton, p. 7).
• Despite this growing ideological divide, people are increasingly willing to trust one another, even complete strangers, when their goals are aligned (see the sharing economy, for example). This creates interesting opportunities for PD practitioners. Targeting strategies based on political attitudes or profiles may overshadow the possibility of aligned goals on important policy and social issues (see Hancock, p. 49).
Rethinking Our Digital Platforms and Metrics
Virality – the crown jewel in the social media realm – is overemphasized often at the expense of more important metrics like context and longevity. Many of the metrics used to measure the effectiveness of social media campaigns are vulnerable to manipulation, and more importantly, don’t measure engagement in any meaningful way. These metrics were built for an industry reliant on advertising for revenue generation, and as a result, may not be well-suited when applied to the context of PD (see Ford, p. 33; Woolley, p. 13).
• Overemphasizing certain metrics, such as reach or impressions, fails to account for the risks created by relaying on the same portals as other, less truthful and more nefarious actors. We need to be cautious and aware of the various ways in which the digital media business industries are shaping PD content, be aware of the risks, and think carefully about safeguarding the credibility U.S. Department of State PD programs operating in this space (see Wharton, p. 7; Ford, p. 33).
Strategic Narratives
• Strategic narratives—a means for political actors to construct a shared meaning of the past, present and future of politics in order to shape the behavior of other actors.” They provide the ideological backdrop for how audiences assess the meaning and significance of current events and breaking news. Put another way, they help people make sense of what would otherwise be a dizzying onslaught of news they are exposed to on a daily basis (see Roselle, p. 77; Kounalakis, p. 91).
• Crafting effective narratives require a genuine consensus--even if limited or temporary--on our policy priorities and their underlying values, as well as a detailed understanding and appreciation of local grievances and concerns about the related policy issue (see Wharton, p. 7; Roselle. P. 77). As such, effective strategic narratives must be mutually constructed.
• Rather than focusing on trending news topics and stories alone, we need to develop greater capacity to understand competing public narratives in foreign contexts and track how they adapt over time. Understanding distinctions between system (or governance), value, and identity narratives would allow PD practitioners to construct policy narratives that speak to, or at least acknowledge, the underlying pillars of belief in a given community (see Walker, p. 83; Roselle, p. 77).
• Every new administration creates new opportunities for foreign engagement. A shift towards a more transactional approach to PD, focused less on values but more on shared policy priorities, could allow for improved relations and cooperation with a number of countries previously hostile to American PD efforts and programs (see Kounalakis, p. 91)","Can Public Diplomacy Survive the Internet? Bots, Echo Chambers, and Disinformation",,https://core.ac.uk/download/428380032.pdf,DigitalCommons@University of Nebraska - Lincoln,,core
154804451,2017-01-01T00:00:00,"In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable functions. This network, called additive neural network, is very suitable for mobile computing. The neural structure is based on a novel vector product definition, called ef-operator, that permits a multiplier-free implementation. In ef-operation, the 'product' of two real numbers is defined as the sum of their absolute values, with the sign determined by the sign of the product of the numbers. This 'product' is used to construct a vector product in n-dimensional Euclidean space. The vector product induces the lasso norm. The proposed additive neural network successfully solves the XOR problem. The experiments on MNIST dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron. © 2017 IEEE",An energy efficient additive neural network,10.1109/SIU.2017.7960263,https://core.ac.uk/download/154804451.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,core
158423500,2018-06-01T00:00:00Z,"Obtaining good quality image features is of remarkable importance for most computer vision tasks. It has been demonstrated that the first layers of the human visual cortex are devoted to feature detection. The need for these features has made line, segment, and corner detection one of the most studied topics in computer vision. HT3D is a recent variant of the Hough transform for the combined detection of corners and line segments in images. It uses a 3D parameter space that enables the detection of segments instead of whole lines. This space also encloses canonical configurations of image corners, transforming corner detection into a pattern search problem. Spiking neural networks (SNN) have previously been proposed for multiple image processing tasks, including corner and line detection using the Hough transform. Following these ideas, this paper presents and describes in detail a model to implement HT3D as a Spiking Neural Network for corner detection. The results obtained from a thorough testing of its implementation using real images evince the correctness of the Spiking Neural Network HT3D implementation. Such results are comparable to those obtained with the regular HT3D implementation, which are in turn superior to other corner detection algorithms",A Spiking Neural Model of HT3D for Corner Detection,10.3389/fncom.2018.00037/full,,Frontiers Media S.A.,"[{'title': None, 'identifiers': ['issn:1662-5188', '1662-5188']}]",core
189940868,2018-01-01T00:00:00,"In order to find ways to address problems of motivation and engagement in civil engineering students, and provide students with a space to develop sense of belonging and engage with their peers through a co-curricular experience, the School of Civil Engineering at [BLINDED FOR REVIEW] in 2015 developed the Icarus program. The purpose of this exploratory study is to present preliminary information about the implementation of Icarus, as an engineering education experiment. The program's goal was to provide students with a different space to develop the competencies and skills desired while simultaneously they form their identity as engineers. The sample was 116 civil engineering students, 49 of them enrolled in the Icarus program in its first semester. Results showed that the main motivation to join the Icarus program was to apply theory from class into engineering real world issues, and to work and engage with peers. In addition, Icarus students have higher levels of aspirations on how well they will do in their engineering courses, and higher levels of deep learning when compared to other non-Icarus engineering students in the same year. Further Implications are provided",Icarus: the development of a voluntary research program to increase engineering students' engagement,10.18260/1-2--29637,,'American Society for Engineering Education',,core
93943607,2018-08-20T00:00:00,"Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.Comment: Adding hardware acceleration for real-time execution of defender
  module",DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,,http://arxiv.org/abs/1709.02538,,,core
201673930,2018-12-01T00:00:00,"While neuromorphic systems may be the ultimate platform for deploying spiking neural networks (SNNs), their distributed nature and optimization for specific types of models makes them unwieldy tools for developing them. Instead, SNN models tend to be developed and simulated on computers or clusters of computers with standard von Neumann CPU architectures. Over the last decade, as well as becoming a common fixture in many workstations, NVIDIA GPU accelerators have entered the High Performance Computing field and are now used in 50 % of the Top 10 super computing sites worldwide. In this paper we use our GeNN code generator to re-implement two neo-cortex-inspired, circuit-scale, point neuron network models on GPU hardware. We verify the correctness of our GPU simulations against prior results obtained with NEST running on traditional HPC hardware and compare the performance with respect to speed and energy consumption against published data from CPU-based HPC and neuromorphic hardware. A full-scale model of a cortical column can be simulated at speeds approaching 0.5× real-time using a single NVIDIA Tesla V100 accelerator—faster than is currently possible using a CPU based cluster or the SpiNNaker neuromorphic system. In addition, we find that, across a range of GPU systems, the energy to solution as well as the energy per synaptic event of the microcircuit simulation is as much as 14× lower than either on SpiNNaker or in CPU-based simulations. Besides performance in terms of speed and energy consumption of the simulation, efficient initialization of models is also a crucial concern, particularly in a research context where repeated runs and parameter-space exploration are required. Therefore, we also introduce in this paper some of the novel parallel initialization methods implemented in the latest version of GeNN and demonstrate how they can enable further speed and energy advantages",GPUs Outperform Current HPC and Neuromorphic Solutions in Terms of Speed and Energy When Simulating a Highly-Connected Cortical Model,10.3389/fnins.2018.00941,https://core.ac.uk/download/201673930.pdf,'Frontiers Media SA',"[{'title': 'Frontiers in Neuroscience', 'identifiers': ['issn:1662-453X', '1662-453x']}]",core
228554112,2017-07-07T00:00:00,"This paper describes and discusses a research work on ""DeliBOT – A Mobile Robot with Implementation of SLAM utilizing Computer Vision/Machine Learning Techniques"". The principle objective is to study about the utilization of Kinect in mobile robotics and use it to assemble an integrated system framework equipped for building a map of environment, and localizing mobile robot with respect to the map using visual cues. There were four principle work stages. The initial step was studying and testing solutions for mapping and navigation with a RGB-D sensor, the Kinect. The accompanying stage was implementing a system framework equipped for identifying and localizing objects from the point cloud given by the Kinect, permitting the execution of further errands on the system framework, i.e. considering the computational load. The third step was identifying the landmarks and the improvement they can present in the framework. At last, the joining of the previous modules was led and experimental evaluation and validation of the integrated system. The demand of substitution of human by a robot is winding up noticeably more probable eager these days because of the likelihood of less mistakes that the robot apparently makes. Amid the previous couple of years, the technology turn out to be more accurate and legitimate outcomes with less errors, and researches started to consolidate more sensors. By utilizing accessible sensors, robot will perceive and identify environment it is in and makes map. Additionally, robot will have element of itself locating inside environment. Robot fundamental operations are identification of objects and localization for conduction of the services. Robot conduct appropriate path planning and avoidance of object by setting a target or determining goal [1]. Because of the outstanding research and robotics applications in almost every segments of life of human's, from space surveillance to health-care, solution is created for autonomous mobile robots direct tasks excluding human intervention in indoor environment [2], a few applications like cleaning facilities and transportation fields. Robot navigation in environment that is safe that performs profoundly, require environment map. Since in the greater part of applications in real-life map is not given, exploration algorithm is used",DELIBOT WITH SLAM IMPLEMENTATION,,https://core.ac.uk/download/228554112.pdf,International Journal of Innovative Technology and Research,,core
217173097,2018-03-12T07:00:00,"This paper presents the design, development, and implementation of a bioinspired fault diagnosis scheme applied to a cold gas–based spacecraft prototype. The proposed framework relies on the artificial immune system metaphor with the goal of monitoring the health of the spacecraft by detecting subsystem upset conditions such as actuator malfunctions. A nonlinear dynamic inversion baseline controller with adaptation capabilities has been developed and implemented for attitude control of the spacecraft. The performance of the proposed health-monitoring scheme is determined by analyzing the system response under different actuator failures while the spacecraft is operated with and without adaptive augmentation. In both cases, the results show acceptable performance in terms of high detection activity and low false alarms with real-time capabilities. The application of this artificial intelligence–based scheme to aerospace systems will provide a high impact on space exploration by increasing mission protection and performance",Spacecraft Heath Monitoring Using a Biomimetic Fault Diagnosis Scheme,,,SelectedWorks,,core
158354020,2018-05-23T00:00:00,"Physical law based models (also known as white box models) are widely applied in the aerospace industry, providing models for dynamic systems such as helicopter flight simulators. To meet the criteria of real-time simulation, simplifications to the underlying physics sometimes have to be applied, leading to errors in the model’s predictions. Grey-box models use both physics-based and data-based models. They have potential to reduce the difference between a simulator’s and real rotorcraft’s response. In the current work, a preliminary step to the grey-box approach, a machine learnt data-based, i.e ‘black box’ model is applied to the dynamic response of a helicopter. The machine learning methods used are probabilistic and can capture uncertainties associated with the model’s prediction. In the current paper, machine learning is used to create a Gaussian Process (GP) non-linear autoregressive (NARX) model that predicts pitch, roll and yaw rate. The predictions are compared to a physical law based model created using FLIGHTLAB software. The GP outperforms the FLIGHTLAB model in terms of root mean squared error, when predicting the pitch, roll and yaw rate of a Bo105 helicopter",Towards Gaussian Process Models of Complex Rotorcraft Dynamics,,https://core.ac.uk/download/158354020.pdf,,,core
304993626,2018-05-31T18:12:19,"The objective of the proposed research is to develop methodologies, support algorithms and software-hardware infrastructure for detection and diagnosis of parametric failures, transient soft errors and security attacks in linear and nonlinear circuits and systems for sensing and control. This research is motivated by the proliferation of autonomous sense-and-control real-time systems, such as intelligent robots and self-driven cars, that must maintain a minimum level of performance in the presence of unavoidable electro-mechanical degradation of system-level components in the field as well as external security attacks. A key focus is on rapid recovery from the effects of such anomalies and impairments with minimal impact on system performance while maintaining low implementation overhead as opposed to traditional schemes for recovery that rely on duplication or triplication. Real-time detection and diagnosis techniques are investigated and rely on analysis of state-space encoding based check signatures. For on-line error detection and diagnosis in control systems, linear and nonlinear state space encodings of the system behavior are analyzed in real-time. Recovery is initiated using guided reinforcement learning algorithms that determine how best the system should be controlled in the presence of the diagnosed performance impairments. For cyber-physical systems, these state-space encodings are used to detect malicious security attacks and to diagnose the affected components swiftly. These checks are utilized for fast recovery from such attacks while avoiding catastrophic system failure. Further research in this area will pave the way for successful deployment of self-healing autonomous systems and resilient cyber-physical systems.Ph.D",State-space encoding driven error resilience in control systems and circuits,,https://core.ac.uk/download/304993626.pdf,Georgia Institute of Technology,,core
159314970,2018-06-07T00:00:00,"While the idea of a city built for people is gaining more and more acceptance today, many of our urban environments remain focused and built around the car. Currently, day-to-day life in the city means the frequent interaction between pedestrians and drivers, a situation which can be dangerous, or, at worst, deadly.


This project, a collaboration between the Complex Systems group at IN3 (CoSIN3) of the Universitat Oberta de Catalunya (UOC), the Dirección General de Tráfico (DGT) and the Guàrdia Urbana de Barcelona, aims to quantify the issue of car-pedestrian collisions by characterising specific street areas with an indicator of pedestrian safety based on the structural properties of the street. Concretely, the project will generate this safety index for Spain’s two largest cities, Madrid and Barcelona, but the methodology and pipeline are applicable theoretically to any urban setting.


As a base unit for measuring pedestrian safety over space, the total pedestrian area of the city (sidewalks and crossings) will be tessellated into small, regular segments. Each of these segments will be assigned various indicator values, from simple geometric properties (distance to the closest pedestrian crossing; width of sidewalk) to more complex measures such as driver visibility.


Geometric operations to arrive at these values are performed on a PostGIS-build geo-database, over a variety of data, including street, sidewalk and block geometries, from diverse sources of open GIS data (Instituto Geográfico Nacional, Institut Cartogràfic i Geològic de Catalunya, OpenStreetMap, municipal data sources).Visibility values will be derived from a combination of deep learning technologies with GIS. A deep learning architecture will deliver computer-segmented street-scene images from Google Streetview. Each labeled image will be paired with an image from a simplified 3-dimensional model of the city, replicating its point of view (rendered with open-source 3D mapping software). The model will be clean of all street features (parked cars, trees, etc.) besides sidewalks and buildings. Comparison between the real and simplified images will thus permit the identification of sidewalk areas invisible to drivers due to visual obstructions.


The results of the project will be presented as online “heatmap” visualisations of safety indexes for the focus cities, open to the public for browsing and research. Additionally, a purpose-built API (Application Programming Interface) will provide public and private organisations working in the area of traffic safety access to the results for integration in their own internal or public application",Espacio persona: Big data to make urban streets safer,,,Universitat de Girona. Servei de Sistemes d'Informació Geogràfica i Teledetecció,,core
215189519,2017-12-01T00:00:00,"The objectives of this dissertation work are developing an enhanced intelligent radar signal and data processing framework for aviation hazard detection, classification and monitoring, and real-time implementation on massive parallel platforms. Variety of radar sensor platforms are used to prove the concept including airborne precipitation radar and different ground weather radars.

As a focused example of the proposed approach, this research applies evolutionary machine learning technology to turbulence level classification for civil aviation. An artificial neural network (ANN) machine learning approach based on radar observation is developed for classifying the cubed root of the Eddy Dissipation Rate (EDR), a widely-accepted measure of turbulence intensity. The approach is validated using typhoon weather data collected by Hong Kong Observatory’s (HKO) Terminal Doppler Weather Radar (TDWR) located near Hong Kong International Airport (HKIA) and comparing HKO-TDWR EDR$^{1/3}$ detections and predictions with in situ EDR$^{1/3}$ measured by commercial aircrafts. The testing results verified that machine learning approach performs reasonably well for both detecting and predicting tasks.

As the preliminary step to explore the possibility of acceleration by integrating General Purpose Graphic Processing Unit (GPGPU), this research introduces a practical approach to implement real-time processing algorithms for general surveillance radar based on NVIDIA graphical processing units (GPUs). The pulse compression algorithms are implemented using compute unified device architecture (CUDA) libraries such as CUDA basic linear algebra subroutines and CUDA fast Fourier transform library, which are adopted from open source libraries and optimized for the NVIDIA GPUs. For more advanced, adaptive processing algorithms such as adaptive pulse compression, customized kernel optimization is investigated. A statistical optimization approach is developed for this purpose without needing much knowledge of the physical configurations of the kernels. It was found that the kernel optimization approach can significantly improve the performance. Benchmark performance is compared with the CPU performance in terms of processing accelerations. The proposed implementation framework can be used in various radar systems including ground-based phased array radar, airborne sense and avoid radar, and aerospace surveillance radar. After the investigation of the GPGPU on radar signal processing chain, the benchmark of applying machine learning approach on embedded GPU platform was performed. According to the performance, real-time requirement of the machine learning method of turbulence detection developed in this research could be met as well as Size, Weight and Power (SWaP) restrictions on embedded GPGPU platforms",Advanced Aviation Weather Radar Data Processing and Real-Time Implementations,,https://core.ac.uk/download/215189519.pdf,,,core
211494876,2018-01-01T00:00:00,"The objective of this research is to develop more accurate, robust and reliable microscopic models. Anintegrated methodological framework based on non–parametric approaches is proposed for estimationof data–driven microscopic traffic simulation models. The methodology is implemented using differentma-chine learning techniques such as clustering, classification, locally weighted regression, splinefitting, Gaussian processes, Kernel support vector machines and neural networks. The methodology isdemonstrated using real trajectory data from three different sources and specifically an experimentfrom Naples, NGSIM data and non–lane disciplinary trajectory data from India. The focus is given oncar–following models and Gipps’ model, one of the most extensively used car–following models, iscalibrated against the same data in order to be used as a reference benchmark. Many parametersaffect driving behavior and it is explored how the performance of the models is improved by includingmore explanatory variables. Then, a practical and simple approach is developed and motivated for theonline calibration of microscopic traffic simulation models, which considers dynamic parameters forindividual drivers, in time and space. The model adapts to driving behavior in a rolling horizon andleads to less than 10% error in speed prediction even for ten steps into the future. This research alsoexamines the feasibility and the benefits of using data–driven models on mixed traffic trajectory data,including non–lane discipline and heterogeneity in vehicle types, common characteristics in cities indeveloping countries. Although typical car–following models are theoretically justified, data–drivenapproaches are more flexible and allow the easy incorporation of additional information to the processof speed estimation. The results indicate that data–driven models could ensure reliability andimprovement in estimation of microscopic models.Στόχος της έρευνας είναι η ανάπτυξη πιο αξιόπιστων μικροσκοπικών κυκλοφοριακών προτύπων. Αναπτύσσεται μια ολοκληρωμένη μεθοδολογία για την εκτίμηση προτύπων κυκλοφοριακής προσομοίωσης με τη χρήση καινοτόμων και ευέλικτων μεθόδων μηχανικής μάθησης, όπως η ταξινόμηση, η ομαδοποίηση, η τοπικά σταθμισμένη παλινδρόμηση (loess), οι καμπύλες splines, οι Gaussian διαδικασίες, οι διανυσματικές μηχανές υποστήριξης και τα νευρωνικά δίκτυα. Τα δεδομένα που χρησιμοποιήθηκαν στην έρευνα αυτή περιλαμβάνουν δεδομένα από τρεις διαφορετικές πηγές, δεδομένα από τη Νάπολη, τα NGSIM δεδομένα και δεδομένα από την Ινδία. Δίνεται έμφαση στα πρότυπα ακολουθίας οχημάτων και για τα ίδια δεδομένα εφαρμόζεται το μοντέλο του Gipps, ένα γνωστό μοντέλο ακολουθίας οχημάτων που χρησιμοποιείται ως μοντέλο αναφοράς στην παρούσα έρευνα. Επειδή πολλοί παράγοντες επηρεάζουν τη συμπεριφορά του οδηγού, εξετάζεται κατά πόσο βελτιώνεται το μοντέλο ενσωματώνοντας περισσότερες μεταβλητές. Επιπλέον, εξετάζεται η δυναμική βαθμονόμηση κυκλοφοριακών προτύπων λαμβάνοντας υπόψη τη δυναμική μεταβολή των παραμέτρων για κάθε οδηγό, στον χρόνο και το χώρο. Οι παράμετροι μεταβάλλονται σε έναν κυλιόμενο χρονικό ορίζοντα και επιτυγχάνεται πρόβλεψη της ταχύτητας έως 10% για δέκα βήματα στο μέλλον. Διερευνάται η χρήση μοντέλων καθοδηγούμενων από τα δεδομένα σε συνθήκες μεικτής κυκλοφορίας χωρίς τήρηση των λωρίδων κυκλοφορίας και με μεγάλη ποικιλία ως προς τον τύπο των οχημάτων, κοινά χαρακτηριστικά των αναπτυσσόμενων χωρών. Αν και τα κλασσικά πρότυπα ακολουθίας οχημάτων είναι θεωρητικά τεκμηριωμένα, τα πρότυπα βασισμένα σε δεδομένα προσφέρουν μεγαλύτερη ευελιξία και επιτρέπουν την εύκολη ενσωμάτωση νέων μεταβλητών. Τα αποτελέσματα υποδεικνύουν ότι τα πρότυπα που βασίζονται σε δεδομένα μπορούν να συμβάλλουν στην εκτίμηση πιο αξιόπιστων μικροσκοπικών προτύπων",Προς την ανάπτυξη ευέλικτων μικροσκοπικών κυκλοφοριακών προτύπων βασισμένων σε δεδομένα,10.12681/eadd/44979,,'National Documentation Centre (EKT)',,core
159636017,2018-01-01T00:00:00,"Le frane per crollo da ammassi rocciosi fratturati sono tra i processi di instabilità gravitativa che più frequentemente interessano opere antropiche quali tagli su versanti naturali o artificiali, pareti di cava, trincee stradali, autostradali o ferroviarie, sia per ciò che attiene le aree di distacco che per quelle di accumulo. Nell’ambito dell’applicazione di sistemi di early warning per la gestione del rischio geologico legato a queste tipologie di frana, una sperimentazione della tecnica del monitoraggio nanosismometrico è stata effettuata presso due siti estrattivi non più in attività: le “Pirrere” della Baia di Cala Rossa sull’isola di Favignana (Trapani), in Sicilia, e la cava dismessa di Acuto (Frosinone), in Italia Centrale. Il monitoraggio nanosismometrico è una tecnica di indagine che consente di individuare e localizzare deboli eventi sismici, fino a magnitudo locale (ML) nell’ordine di -3, attraverso l’impiego di quattro sensori sismometrici disposti secondo una specifica geometria di array detta SNS (Seismic Navigation System).

Nel presente lavoro, mediante il software NanoseismicSuite sono stati analizzati 73 eventi di crollo indotti artificialmente attraverso la caduta controllata di blocchi di roccia nei due siti estrattivi abbandonati; sono stati lanciati, simulando fenomeni di rockfalls, rispettivamente 47 blocchi di roccia nella cava di Acuto e 26 eventi in quattro diverse cave a cielo aperto presenti nel settore occidentale di Cala Rossa. Tali eventi, avendo punto epicentrale noto, hanno permesso di determinare il miglior modello di sottosuolo in termini di valori di velocità delle onde P ed S attraverso un’operazione di back analysis. L’analisi è stata, infatti, effettuata variando i valori di velocità e scegliendo quelli relativi all’epicentro teorico ottenuto dall’analisi dell’evento che fosse il più vicino possibile al punto reale di impatto del blocco di roccia. Al fine di valutare la sensibilità della geometria dell’array SNS e l’influenza del sito di installazione sulla capacità di individuare e localizzare gli eventi, le sperimentazioni sono state condotte sia variando il raggio di apertura che la zona di installazione degli array: presso Acuto le acquisizioni di segnale sono state condotte prima con un array SNS con apertura di 20 m e successivamente con un array di apertura 10 m, mentre presso Cala Rossa l’array è stato installato alternativamente all’aperto in un’area di plateau roccioso ed in una galleria facente parte dell’area di cava abbandonata.

Analizzando i dati si è ottenuta una precisione dell’ubicazione epicentrale compresa tra il 10 ed il 22% della distanza che intercorre tra la sorgente e l’array nanosismometrico. Il miglior modello di sottosuolo ottenuto per entrambi i casi di studio è risultato avere una velocità delle onde P pari a 900 m/s ed un rapporto VP/VS pari a 1.73, valori in accordo con le condizioni di intenso stato di fratturazione delle rocce carbonatiche affioranti nelle due zone di cava. Per gli eventi di crollo indotti la magnitudo ML è risultata essere compresa tra -2.8 e -1.3; considerando l’energia sviluppata dall’impatto, legata alla massa del blocco ed all’altezza e alla velocità di caduta, non è stato possibile definire una relazione tra magnitudo ed energia, probabilmente a causa delle differenti caratteristiche del punto di impatto dei diversi blocchi. In generale, si è osservato che la precisione di ubicazione degli eventi, in termini di azimuth e distanza dal reale epicentro, è risultata paragonabile sia variando l’apertura dell’array che variando il sito di installazione. Per il sito sperimentale di Acuto, il processo di picking manuale del tempo di primo arrivo delle onde P è risultato essere più affidabile nel caso di array con apertura pari a 10 m. La sperimentazione effettuata a Cala Rossa ha permesso, invece, di osservare una migliore capacità di individuazione degli eventi nelle tracce relative all’array posizionato in galleria a causa della minore rumorosità di base del sito di installazione.

Tra le registrazioni sismometriche sono state identificate varie tipologie di segnali, oltre a quelli generati dal lancio dei blocchi, alcune riconducibili ad eventi naturali di crollo altre a deboli terremoti. L’analisi dei segnali riferibili alla prima tipologia di eventi naturali, effettuata tenendo in considerazione i modelli di sottosuolo precedentemente calibrati, ha portato all’identificazione in ambedue i siti di aree aventi maggiore suscettibilità a frane per crollo. In definitiva, si può ritenere che i risultati ottenuti in questo studio siano incoraggianti rispetto all’efficacia della tecnica di monitoraggio nanosismometrico nell’individuazione e nell’ubicazione di fenomeni di crollo in roccia e portano a ritenere questa tecnica potenzialmente applicabile in aree in cui tali eventi possono interferire con infrastrutture antropiche.In the frame of early warning and risk mitigation studies for landslide processes involving rock masses, two quarry areas (Cala Rossa Bay in Sicily and Acuto in Central Italy) were monitored with SNS (Seismic Navigation System) arrays. In this study, 73 rockfalls were simulated by launches of rock blocks. This allowed to perform a back analysis for defining the best seismic velocity model of the subsoil half-space; the records related to each impact caused by the rock block launch were managed by the nanoseismic monitoring approach, varying the velocity model to obtain a theoretical epicentre as close as possible to the actual location of the impact point. In order to evaluate the sensibility of the SNS array, the results obtained by different array apertures and positions were compared in terms of azimuth and distance error with respect to the real epicentres. On the other hand, several natural rockfalls were detected; their analysis allowed to identify areas having higher susceptibility to rockfalls by using the previously calibrated subsoil half-space model. Further studies are required to better define the areas prone to rockfall generation in the considered test sites; nevertheless, the here obtained results show an encouraging perspective about the application of the nanoseismic monitoring with respect to vulnerable infrastructures in rockfall prone areas",Nanoseismic monitoring for detection of rockfalls. Experiments in quarry areas,10.4408/IJEGE.2018-01.O-03,https://core.ac.uk/download/159636017.pdf,Sapienza Università Editrice,,core
201481984,2018-10-01T00:00:00,"Deep convolutional neural networks (CNN) have been recently applied to synthetic aperture radar (SAR) for automatic target recognition (ATR) and have achieved state-of-the-art results with significantly improved recognition performance. However, the training period of deep CNN is long, and the size of the network is huge, sometimes reaching hundreds of megabytes. These two factors of deep CNN hinders its practical implementation and deployment in real-time SAR platforms that are typically resource-constrained. To address this challenge, this paper presents three strategies of network compression and acceleration to decrease computing and memory resource dependencies while maintaining a competitive accuracy. First, we introduce a new weight-based network pruning and adaptive architecture squeezing method to reduce the network storage and the time of inference and training process, meanwhile maintain a balance between compression ratio and classification accuracy. Then we employ weight quantization and coding to compress the network storage space. Due to the fact that the amount of calculation is mainly reflected in the convolution layer, a fast approach for pruned convolutional layers is proposed to reduce the number of multiplication by exploiting the sparsity in the activation inputs and weights. Experimental results show that the convolutional neural networks for SAR-ATR can be compressed by     40 &times;     without loss of accuracy, and the number of multiplication can be reduced by     15 &times;    . Combining these strategies, we can easily load the network in resource-constrained platforms, speed up the inference process to get the results in real-time or even retrain a more suitable network with new image data in a specific situation",Slim and Efficient Neural Network Design for Resource-Constrained SAR Target Recognition,10.3390/rs10101618,,'MDPI AG',"[{'title': 'Remote Sensing', 'identifiers': ['2072-4292', 'issn:2072-4292']}]",core
231778730,2018-09-03T00:00:00,"© 2018 Association for Computing Machinery. Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods",Improving automatic source code summarization via deep reinforcement learning,10.1145/3238147.3238206,https://opus.lib.uts.edu.au/bitstream/10453/126042/4/OCC-121665_AM.pdf,'Association for Computing Machinery (ACM)',,core
154376382,2017-06-01T07:00:00,"Spacecraft health monitoring is essential to ensure that a spacecraft is operating properly and has no anomalies that could jeopardize its mission. Many of the current methods of monitoring system health are difficult to use as the complexity of spacecraft increase, and are in many cases impractical on CubeSat satellites which have strict size and resource limitations. To overcome these problems, new data-driven techniques such as Inductive Monitoring System (IMS), use data mining and machine learning on archived system telemetry to create models that characterize nominal system behavior. The models that IMS creates are in the form of clusters that capture the relationship between a set of sensors in time series data. Each of these clusters define a nominal operating state of the satellite and the range of sensor values that represent it. These characterizations can then be autonomously compared against real-time telemetry on-board the spacecraft to determine if the spacecraft is operating nominally.
This thesis presents an adaption of IMS to create a spacecraft health monitoring system for CubeSat missions developed by the PolySat lab. This system is integrated into PolySat\u27s flight software and provides real time health monitoring of the spacecraft during its mission. Any anomalies detected are reported and further analysis can be done to determine the cause. The system can also be used for the analysis of archived events. The IMS algorithms used by the system were validated, and ground testing was done to determine the performance, reliability, and accuracy of the system. The system was successful in the detection and identification of known anomalies in archived flight telemetry from the IPEX mission. In addition, real-time monitoring performed on the satellite yielded great results that give us confidence in the use of this system in all future missions",A Data-Driven Approach to Cubesat Health Monitoring,,https://core.ac.uk/download/154376382.pdf,DigitalCommons@CalPoly,,core
154990049,2018-04-12T00:00:00,"A fundamental pursuit of microwave metrology is the determination of the
characteristic impedance profile of microwave systems. Among other methods,
this can be practically achieved by means of time-domain reflectometry (TDR)
that measures the reflections from a device due to an applied stimulus.
Conventional TDR allows for the measurement of systems comprising a single
impedance. However, real systems typically feature impedance variations that
obscure the determination of all impedances subsequent to the first one. This
problem has been studied previously and is generally known as scattering
inversion or, in the context of microwave metrology, time-domain ""peeling"". In
this article, we demonstrate the implementation of a space-time efficient
peeling algorithm that corrects for the effect of prior impedance mismatch in a
nonuniform lossless transmission line, regardless of the nature of the
stimulus. We generalize TDR measurement analysis by introducing two tools: A
stochastic machine learning clustering tool and an arbitrary lossy transmission
line modeling tool. The former mitigates many of the imperfections typically
plaguing TDR measurements (except for dispersion) and allows for an efficient
processing of large datasets; the latter allows for a complete transmission
line characterization including both conductor and dielectric loss.Comment: 10 two-column pages with references; authors' biographies with pics;
  8 figures; 1 appendi",Machine Learning Peeling and Loss Modelling of Time-Domain Reflectometry,,http://arxiv.org/abs/1804.04756,,,core
154991500,2018-04-17T00:00:00,"Ensembles are popular methods for solving practical supervised learning
problems. They reduce the risk of having underperforming models in
production-grade software. Although critical, methods for learning
heterogeneous regression ensembles have not been proposed at large scale,
whereas in classical ML literature, stacking, cascading and voting are mostly
restricted to classification problems. Regression poses distinct learning
challenges that may result in poor performance, even when using well
established homogeneous ensemble schemas such as bagging or boosting.
  In this paper, we introduce MetaBags, a novel, practically useful stacking
framework for regression. MetaBags is a meta-learning algorithm that learns a
set of meta-decision trees designed to select one base model (i.e. expert) for
each query, and focuses on inductive bias reduction. A set of meta-decision
trees are learned using different types of meta-features, specially created for
this purpose - to then be bagged at meta-level. This procedure is designed to
learn a model with a fair bias-variance trade-off, and its improvement over
base model performance is correlated with the prediction diversity of different
experts on specific input space subregions. The proposed method and
meta-features are designed in such a way that they enable good predictive
performance even in subregions of space which are not adequately represented in
the available training data.
  An exhaustive empirical testing of the method was performed, evaluating both
generalization error and scalability of the approach on synthetic, open and
real-world application datasets. The obtained results show that our method
significantly outperforms existing state-of-the-art approaches",MetaBags: Bagged Meta-Decision Trees for Regression,,http://arxiv.org/abs/1804.06207,,,core
237331362,2018-10-08T00:00:00,"International audienceThis work deals with the optimization of Deep Convolutional Neural Networks (ConvNets). It elaborates on the concept of Adaptive Energy-Accuracy Scaling through multi-precision arithmetic, a solution that allows ConvNets to be adapted at run-time and meet different energy budgets and accuracy constraints. The strategy is particularly suited for embedded applications made run at the “edge” on resource-constrained platforms. After the very basics that distinguish the proposed adaptive strategy, the paper recalls the software-to-hardware vertical implementation of precision scalable arithmetic for ConvNets, then it focuses on the energy-driven per-layer precision assignment problem describing a meta-heuristic that searches for the most suited representation of both weights and activations of the neural network. The same heuristic is then used to explore the optimal trade-off providing the Pareto points in the energy-accuracy space. Experiments conducted on three different ConvNets deployed in real-life applications, i.e. Image Classification, Keyword Spotting, and Facial Expression Recognition, show adaptive ConvNets reach better energy-accuracy trade-off w.r.t. conventional static fixed-point quantization methods",Energy-Accuracy Scalable Deep Convolutional Neural Networks: A Pareto Analysis,10.1007/978-3-030-23425-6_6,,'Springer Science and Business Media LLC',,core
304993087,2018-01-22T21:12:32,"If radar measurements are performed in the polar coordinates of range and angle and the error orthogonal to the range dimension is much greater than the error in range, the true error region in Cartesian space is no longer well approximated by a Gaussian distribution. This effect is known as the ""contact-lens"" effect due to the shape of the error distribution in Cartesian space. In this dissertation, a method is presented for modeling Cartesian converted measurement distributions which suffer from the contact-lens effect using Maximum Likelihood (ML) Gaussian mixture (GM) parameters. In order to allow an efficient implementation of this process in a GM Kalman filter, a novel normalization of the ML parameters is introduced so that parameters can be efficiently stored in a lookup table for real-time use. Additionally, the measurement update process in the resulting GM filter is modified using a preconditioning process so that the GM measurement PDF is located in close proximity to the support of the state estimate PDF. This preconditioning allows fewer GM components to be used in the model, which significantly reduces the computational cost of the tracking. These techniques are then combined into the Measurement-Adaptive Gaussian Mixture Filter (MAGMF), and this filter is applied to tracking with measurements from a 2D monostatic radar, 2D bistatic radar, and 3D monostatic radar. For all three of these cases, the MAGMF is shown to have track accuracy and covariance consistency performance comparable to solutions that use a particle filter that requires significantly more computations.Ph.D",Use of Gaussian mixture distribution models to address non-Gaussian errors in radar target tracking,,https://core.ac.uk/download/304993087.pdf,Georgia Institute of Technology,,core
345087530,2018-01-01T00:00:00,"Big data analytics provides an interdisciplinary framework that is essential to support the current trend for solving real-world problems collaboratively. The progression of big data analytics framework must be clearly understood so that novel approaches can be developed to advance this state-of-the-art discipline. An ignorance of observing the progression of this fast-growing discipline may lead to duplications in research and waste of efforts. Its main companion field, machine learning, helps solve many big data analytics problems; therefore, it is also important to understand the progression of machine learning in the big data analytics framework. One of the current research efforts in big data analytics is the integration of deep learning and Bayesian optimization, which can help the automatic initialization and optimization of hyperparameters of deep learning and enhance the implementation of iterative algorithms in software. The hyperparameters include the weights used in deep learning, and the number of clusters in Bayesian mixture models that characterize data heterogeneity. The big data analytics research also requires computer systems and software that are capable of storing, retrieving, processing, and analyzing big data that are generally large, complex, heterogeneous, unstructured, unpredictable, and exposed to scalability problems. Therefore, it is appropriate to introduce a new research topic—transformative knowledge discovery—that provides a research ground to study and develop smart machine learning models and algorithms that are automatic, adaptive, and cognitive to address big data analytics problems and challenges. The new research domain will also create research opportunities to work on this interdisciplinary research space and develop solutions to support research in other disciplines that may not have expertise in the research area of big data analytics. For example, the research, such as detection and characterization of retinal diseases in medical sciences and the classification of highly interacting species in environmental sciences can benefit from the knowledge and expertise in big data analytics",Big data analytics: Machine learning and Bayesian learning perspectives—What is done? What is not?,,https://core.ac.uk/download/345087530.pdf,,,core
189834501,2018-05-31T00:00:00,"Since neural networks renaissance, convolutional neural networks (ConvNets) have demonstrated a state-of-the-art performance in several emerging artificial intelligence tasks. The deployment of ConvNets in real-life applications requires power-efficient designs that meet the application-level performance needs. In this context, field-programmable gate arrays (FPGAs) can provide a potential platform that can be tailored to application-specific requirements. However, with the complexity of ConvNet models increasing rapidly, the ConvNet-to-FPGA design space becomes prohibitively large. This paper presents fpgaConvNet, an end-to-end framework for the optimized mapping of ConvNets on FPGAs. The proposed framework comprises an automated design methodology based on the synchronous dataflow (SDF) paradigm and defines a set of SDF transformations in order to efficiently navigate the architectural design space. By proposing a systematic multiobjective optimization formulation, the presented framework is able to generate hardware designs that are cooptimized for the ConvNet workload, the target device, and the application's performance metric of interest. Quantitative evaluation shows that the proposed methodology yields hardware designs that improve the performance by up to 6.65x over highly optimized graphics processing unit designs for the same power constraints and achieve up to 2.94x higher performance density compared with the state-of-the-art FPGA-based ConvNet architectures",fpgaConvNet: mapping regular and irregular convolutional neural networks on FPGAs,10.1109/TNNLS.2018.2844093,,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': 'IEEE Transactions on Neural Networks and Learning Systems', 'identifiers': ['2162-2388', 'issn:2162-2388']}]",core
231879718,2018-01-01T00:00:00,"Proactive auto-scaling methods dynamically manage the resources for an application according to the current and future load predictions to preserve the desired performance at a reduced cost. However, auto-scaling web applications remain challenging mainly due to dynamic workload intensity and characteristics which are difficult to predict. Most existing methods mainly predict the request arrival rate which only partially captures the workload characteristics and the changing system dynamics that influence the resource needs. This may lead to inappropriate resource provisioning decisions. In this paper, we address these challenges by proposing a framework for prediction of dynamic workload patterns as follows. First, we use an unsupervised learning method to analyze the web application access logs to discover URI (Uniform Resource Identifier) space partitions based on the response time and the document size features. Then for each application URI, we compute its distribution across these partitions based on historical access logs to accurately capture the workload characteristics compared to just representing the workload using the request arrival rate. These URI distributions are then used to compute the Probabilistic Workload Pattern (PWP), which is a probability vector describing the overall distribution of incoming requests across URI partitions. Finally, the identified workload patterns for a specific number of last time intervals are used to predict the workload pattern of the next interval. The latter is used for future resource demand prediction and proactive auto-scaling to dynamically control the provisioning of resources. The framework is implemented and experimentally evaluated using historical access logs of three real web applications, each with increasing, decreasing, periodic, and randomly varying arrival rate behaviors. Results show that the proposed solution yields significantly more accurate predictions of workload patterns and resource demands of web applications compared to existing approaches. ? 2018 Elsevier LtdThis work was made possible by NPRP grant # 7-481-1-088 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors. Waheed Iqbal is a Postdoc researcher with the Department of Computer Science and Engineering, Qatar University. He also holds a position of Assistant Professor at Punjab University College of Information Technology, University of the Punjab, Lahore, Pakistan. His research interests lie in cloud computing, distribute systems, machine learning, and large scale system performance evaluation. Waheed received his Ph.D. degree from the Asian Institute of Technology, Thailand. He received dual Masters degrees in Computer Science and Information Technology from the Asian Institute of Technology and the Technical University of Catalonia (UPC), Barcelona, Spain, respectively. Abdelkarim Erradi is an Assistant Professor in the Computer Science and Engineering Department at Qatar University. His research and development activities and interests focus on autonomic computing, self-managing systems and cybersecurity. He leads several funded research projects in these areas. He has authored several scientific papers in international conferences and journals. He received his Ph.D. in computer science from the University of New South Wales, Sydney, Australia. Besides his academic experience, he possesses 12 years professional experience as a Designer and a Developer of large scale enterprise applications. Arif Mahmood is an Associate Professor in the Department of Computer Science, Information Technology University (ITU). He received his Masters and the PhD degrees in Computer Science from the Lahore University of Management Sciences in 2003 and 2011 respectively with Gold Medal and academic distinction. He also worked as Postdoc researcher with Qatar University and as Research Assistant Professor with the School of Mathematics and Statistics, and with the College of Computer Science and Software Engineering, the University of the Western Australia (UWA). His major research interests are in Computer Vision and Pattern Recognition. More specifically he has performed research in data clustering, classification, action and object recognition using image sets, scene background modeling, and person segmentation and action recognition in crowds.Scopu",Dynamic workload patterns prediction for proactive auto-scaling of web applications,10.1016/j.jnca.2018.09.023,,'Elsevier BV',,core
96423157,2017-01-13T00:00:00,"The Fast TracKer project and in particular the Associative Memory system aims at setting new standards for speed of computation for pattern recognition, enabling technological advancements useful to research and society. This technology is based on a Processing Unit made of the combination of FPGAs and a full custom associative memory AM-chip. In the Associative Memory system, pattern matching is executed with the maximum parallelism, and the results are then refined using FPGAs.
The Processing Unit has been developed for high energy physics, and its purpose is the real time track reconstruction at hadron collider experiments which is a crucial task for the success of such experiments. There, the most interesting processes are very rare and hidden in an extremely large level of background information. Selecting interesting events from the background in real time is therefore essential to fully exploit the physics potential of experiments where only a very limited fraction of the produced data can be recorded. Only 1 over 107 produced data sets, called ""events"", can be written to disk to perform physics analysis. Therefore, the selection system must be extremely accurate and fast in order to store and post-process potentially interesting events. Tracking devices, and in particular silicon detectors that are becoming the predominant tracking technology, play an essential role in the identification of interesting
events. In fact, they provide very detailed information for charged particles and they can separate most of the different particle trajectories in the overlapping collisions recorded in the same event. However, these detectors contain hundreds of millions of channels, so they make the problem of complete tracking a formidable challenge even for large computing farms. The events contain many soft, not interesting collisions superimposed to the interesting one, the hard scattering. This level of confusion is due to the
collider extremely high collision rate necessary to produce rare particles, as the Higgs boson recently discovered at CERN, at an appreciable rate. These conditions are going to worsen in the future experiments. The Large Hadron Collider (LHC) at CERN will produce 80 overlapped events before 2020 and this number will grow up to hundreds of collisions for the following machine upgrade.
On the other hand, the state-of-the-art electronics are advanced enough to overcome the problem. We provided real-time tracking using a massively-parallel high-performance system. Our solution provides the required performance for a relatively low cost, lower energy consumption, and saving space (by using a more compact system). We implemented an innovative strategy, based on the optimal mapping of a complex algorithm on different technologies. Our target is to get the optimal results by combining the high performance of rigid dedicated hardware with the distinctive flexibility of the FPGA and of general-purpose, but lower-performance, CPUs. The architecture’s key role is played by FPGAs, while the majority of computing power is provided by cooperating full-custom ASICs named Associative Memory. The AM-chip is suitable for massive
parallelism in data correlation searches and it has a key role in the system. One Processing Unit hosts 64 AM-chips, and it is able to perform bitwise comparisons at 120 Pbit/s. The memory access bandwidth and number of comparisons per second has, to the best of our knowledge, no equal in commercial resources. It takes full advantage of the intrinsic parallel nature of the combinatorial problem by comparing at once the
data under analysis to a set of pre-calculated ""expectations"", or patterns. This approach reduces to linear the exponential complexity of CPU-based algorithms and the problem is solved by the time data are loaded into the system.
Data processing speed is achieved with pipelining, and parallel processing. Track reconstruction is executed with a two steps pipeline architecture. The AM system implements the first stage by recognizing track candidates at low resolution. The second stage, the Track Fitter, is implemented using FPGAs. The Track Fitter receives track candidates and high resolution hits to refine pattern recognition at the associative memory output rate. ""Hit"" refers to the centroid of the charge left by the ionization process due to the crossing particle. Track fitting is done rapidly by replacing a helical fit with a simplified calculation that is linear in the local hit position in each silicon layer. The calculation is a set of scalar products of the hit coordinates and pre-calculated constants that take into account the detector geometry and alignment. While FTK is under construction at ATLAS experiment, the CMS experiment is developing its R&D for online tracking. The CMS R&D exploits a similar approach for real time track reconstruction at much higher rates in the CMS upgraded experiment that should take data after 2020.
In this thesis is briefly described the Large Hadron Collider and the ATLAS experiment in Chapter 1.
Chapter 2 shows the Fast TracKer project, and a description of the main parts of the system.
Chapter 3 describes the Associative Memory system, and the detailed description of the main elements of the system. The main activities and results of my PhD studies are described in this chapter. I designed the motherboard (AMB) and the daughter-board (LAMB). I performed an interesting study concerning signal integrity in an high serial links density PCB. I presented these results in San Diego 2015 IEEE Nuclear Science Symposium and Medical Imaging Conference, and the results have been published on a journal article on IEEE Transactions on Nuclear Science. I designed the programmable logic and wrote the VHDL code for the FPGAs on those boards. I gave my contribution and advise to design the AM-chip package, and the new generation of AM-chip. At the end of the Chapter 3 there are also the tests, results and validation procedure that I performed before the production and installation phase. The final results and performances
are described in Section 3.6.3.
In Chapter 4 is described the Associative Memory system infrastructure. Since the AM system is a custom processor with high density of power consumption, we designed a dedicated rack layout and designed a custom fan unit in order to maintain the temperatures low. Concerning these issues I designed the PCB with particular care to the power dissipation and the low air flow resistance. I gave also my contribution to the
temperature simulations that we performed on the chips, boards and on the crate. These results have been presented in the 20th IEEE Real Time Conference 2016 and the relative IEEE TNS paper will be published soon.
The AM system has been developed for high energy physics, but it is a flexible and powerful embedded system for potential application in a wide range of fields. These future possible evolutions are described in the Chapter 5. I gave my contributions and feedbacks concerning these future applications which are under developing. The very first results have been published and presented in the 14th Vienna Conference on Instrumentation, and an IEEE TNS paper (to be published) related to the presentation at the 20th IEEE Real Time Conference 2016.
Con il progetto Fast TracKer ed in particolare con il sistema Associative Memory System si vogliono raggiungere prestazioni per l’esecuzione di algoritmi di pattern recognition mai raggiunte prima, rendendo disponibili le sue potenzialità tecnologiche all’ambito della ricerca e della società. Questa tecnologia è costituita da un’unità di processamento, chiamata Processing Unit. La PU è a sua volta composta da numerosi FPGA e da decine di chip full custom di memoria associativa, chiamati AM chip. Nel sistema di Memoria Associativa, l’esecuzione dell’algoritmo di pattern matching è estremante parallelizzato, infatti è eseguito da decine di AM chip che lavorano in parallelo. Il risultato è successivamente elaborato da decine di FPGA. La PU è stata sviluppata per elaborare dati prodotti in esperimenti nel campo della fisica delle alte energie, e il suo scopo è quello di ricostruire in tempo reale tutte le tracce prodotte in esperimenti ai collisori adronici. Questo è un compito cruciale per la riuscita di tali esperimenti. In questo ambito i processi di fisica più interessanti sono molto rari e sono nascosti da un rumore di fondo ordini di grandezza più grande. Selezionare i set di dati (chiamati eventi) interessanti in tempo reale è essenziale, poichè è impossibile salvare tutti i dati prodotti per un’analisi successiva. Solo 1 evento su 107 può essere salvato su disco per analisi successive. Quindi la selezione degli eventi deve essere estremamente accurata e veloce. I sistemi di tracciatura e in particolare quelli in silicio, che stanno diventando sempre più predominanti, giocano un ruolo fondamentale nell’identificazione di eventi interessanti. Infatti forniscono informazioni molto dettagliate e possono discriminare le differenti traiettorie di particelle prodotte in collisioni simultanee all’interno dello stesso evento. D’altro canto, questi sistemi di tracciatura in silicio hanno centinaia di milioni di canali di lettura in uscita, che rendono il problema
della completa tracciatura di un evento una sfida impossibile anche per grandi computer farm. Gli eventi contengono decine di collisioni non interessanti sovrapposte a quella interessante e questo livello di confusione è dovuto all’alta frequenza di collisioni necessaria a produrre particelle rare, come il Bosone di Higgs scoperto al CERN. Queste condizioni saranno sempre più estreme in esterimenti futuri. Il Large Hadron Collider al CERN produrrà 80 collisioni sovrapposte per ogni singolo evento entro il 2020 e
questo numero supererà il centinaio negli anni successivi. D’altro canto anche lo stato dell’arte dell’elettronica è migliorato in termini di prestazioni ed è quindi possibile superare questi problemi. Il nostro progetto è costituito da un sistema di processamento estremamente parallelizzato che esegue ricostruzione di tracce in tempo reale ad elevate prestazioni. La nostra soluzione fornisce le performance
richieste ad un costo più basso, bassa potenza ed elevata compattezza se paragonato ad un possibile equivalente sistema realizzato con CPU. Abbiamo implementato una strategia innovativa basata sulla mappatura ottima di un algoritmo su differenti tecnologie.
L’obiettivo è quello di ottimizzare il sistema, combinando le elevate prestazioni di un ASIC con la flessibilità degli FPGA e delle CPU. Grazie alla sua flessibilità un rolo chiave nell’architettura proposta è giocato dal FPGA, mentre la maggior parte della potenza di calcolo è fornita dal ASIC AM chip. Il chip di Memoria Associativa è adatto a eseguire ricerche di correlazioni tra dati in un’architettura fortemente parallelizzata. La PU ospita 64 AM chip, e 4 FPGA, ed è in grado di confrontare 120 Pbit/s. Considerando
la quantità di confronti al secondo e la banda di input/output del chip, non esiste, al meglio delle nostre conoscenze, un dispositivo in commercio paragonabile al AM chip.
Anche l’architettura del chip è fortemente parallela, atta a confrontare simultaneamente il dato in ingresso con tutta la memoria. Quest’ ultima è costituita da set di dati attesi che sono calcolati e salvati nel chip nella fase di configurazione. Questo dispositivo permette di ridurre la complessità dell’algoritmo da esponenziale (se eseguito da CPU) a lineare.
La ricostruzione delle tracce è eseguita in due stadi successivi, grazie ad un’architettura a pipeline. Il sistema di Memoria Associativa implementa il primo stadio della pipeline identificando le tracce a una risoluzione ridotta. Il secondo stadio è chiamato Track Fitter, ed è implementato completamente con FPGA. Il Track Fitter riceve le tracce a bassa risoluzione trovate dal sistema di Memoria Associativa e combinandole
con gli HIT ad alta risoluzione è in grado di ricostruire i parametri delle tracce a piena risoluzione. HIT si riferisce al baricentro della carica depositata dai processi di ionizzazione dovuti alle particelle che attraversano il rivelatore. La ricostruzione della traccia è eseguita rapidamente utilizzando un’approssimazione lineare nell’intorno del HIT, invece che utilizzare una ricostruzione non lineare che sarebbe più precisa ma più lenta da eseguire. Infatti il calcolo consiste solamente nell’esecuzione di prodotti scalari tra i vettori di coordinate degli HIT e delle matrici di costanti precalcolate che tengono
conto della geometria del rivelatore.
Mentre FTK è in fase di costruzione e installazione presso l’esperimento ATLAS, anche l’esperimento CMS ha approvato un progetto di ricerca e sviluppo per ricostruzione di tracce in tempo reale. Tale progetto sfrutta un approccio simile a FTK per la ricostruzione di tracce a velocità più alte e potrebbe essere installato già dopo il 2020.
In questo elaborato, nel Capitolo 1 è descritto brevemente il Large Hadron Collider e l’esperimento ATLAS.
Nel Capitolo 2 è descritto il progetto Fast TracKer e sono descritte tutte le parti che lo compongono.
Nel Capitolo 3 è esposto il sistema di Memoria Associativa, oltre che una descrizione dettagliata dei principali elementi del sistema. Le principali attività e risultati del mio percorso di dottorato sono riportate in questo capitolo. In particolare, ho progettato la scheda madre chiamata AMB e le mezzanine chiamate LAMB. In questo contesto ho studiato l’interessante problema di signal integrity nel caso di alta densità di linee differenziali ad alta frequenza. Ho presentato questi risultati durante la conferenza ""IEEE
Nuclear Science Symposium and Medical Imaging Conference 2015"" svoltasi a San Diego, e sono stati pubblicati su un articolo sulla rivista ""IEEE Transaction on Nuclear Science"". Ho inoltre progettato la logica digitale, configurando gli FPGA con codice VHDL. Ho dato il mio contributo e nella progettazione del package del AM chip, e per il progetto della nuova versione del chip di memoria associativa. Alla fine del Capitolo
3 sono riportati i test, le procedure di validazione, e i risultati delle misure che ho fatto per validare il sistema prima della produzione e installazione. I risultati finali sono riportati nella sezione 3.6.3.
Nel Capitolo 4 è descritta l’infrastruttura costruita per ospitare tutto il sistema di Memoria Associativa. Dal momento che tale sistema è un processore custom con un’alta densità superficiale di potenza dissipata, abbiamo progettato dei rack dedicati e progettato delle unità di raffreddamento in modo da contenere le temperature del sistema. A tal proposito ho progettato i PCB ponendo particolare attenzione alla massimizzazione della dissipazione termica e alla limitazione della resistenza meccanica al flusso d’aria
delle ventole. Ho inoltre contribuito alle simulazioni termiche fatte per lo studio delle caratteristiche del chip, delle schede e dell’intero rack. Questi risultati sono stati presentati alla conferenza ""20th IEEE Real Time Conference 2016"" e il relativo articolo sarà pubblicato sulla rivista ""IEEE Transaction on Nuclear Science"".
Il sistema di Memoria Associativa è stato progettato e sviluppato per analizzare i dati prodotti in esperimenti di fisica delle alte energie, ma rimane un sistema con elevate potenzialità anche per applicazioni al di fuori di questo campo. Queste possibili evoluzioni e altre applicazioni sono descritte nel Capitolo 5. Ho dato il mio contributo a sviluppare tali idee e alcune applicazioni sono in fase di studio e sviluppo. I primi risultati di questi studi sono stati presentati alla conferenza ""14th Vienna Conference on Instrumentation"" e alla conferenza "" 20th IEEE Real Time Conference 2016"" e il relativo articolo verrá pubblicato sulla rivista ""IEEE Transaction on Nuclear Science""",REAL-TIME PATTERN MATCHING SYSTEM FOR THE FTK PROCESSOR AT ATLAS EXPERIMENT,,,'Pisa University Press',,core
80311420,2017-01-01T00:00:00,"The use of identical robots in the RoboCup Standard Platform League (SPL) made software development the key aspect to achieve good results in competitions. In particular, the visual detection process is crucial for extracting information about the environment. In this paper, we present a novel approach for object detection and classification based on Convolutional Neural Networks (CNN). The approach is designed to be used by NAO robots and is made of two stages: image region segmentation, for reducing the search space, and Deep Learning, for validation. The proposed method can be easily extended to deal with different objects and adapted to be used in other RoboCup leagues. Quantitative experiments have been conducted on a data set of annotated images captured in real conditions from NAO robots in action. The used data set is made available for the community. © 2017, Springer International Publishing AG",A Deep Learning Approach for Object Recognition with NAO Soccer Robots,10.1007/978-3-319-68792-6_33,https://core.ac.uk/download/80311420.pdf,'Springer Science and Business Media LLC',,core
186292063,2018-11-17T00:00:00,"Code summarization provides a high level natural language description of the
function performed by code, as it can benefit the software maintenance, code
categorization and retrieval. To the best of our knowledge, most
state-of-the-art approaches follow an encoder-decoder framework which encodes
the code into a hidden space and then decode it into natural language space,
suffering from two major drawbacks: a) Their encoders only consider the
sequential content of code, ignoring the tree structure which is also critical
for the task of code summarization, b) Their decoders are typically trained to
predict the next word by maximizing the likelihood of next ground-truth word
with previous ground-truth word given. However, it is expected to generate the
entire sequence from scratch at test time. This discrepancy can cause an
\textit{exposure bias} issue, making the learnt decoder suboptimal. In this
paper, we incorporate an abstract syntax tree structure as well as sequential
content of code snippets into a deep reinforcement learning framework (i.e.,
actor-critic network). The actor network provides the confidence of predicting
the next word according to current state. On the other hand, the critic network
evaluates the reward value of all possible extensions of the current state and
can provide global guidance for explorations. We employ an advantage reward
composed of BLEU metric to train both networks. Comprehensive experiments on a
real-world dataset show the effectiveness of our proposed model when compared
with some state-of-the-art methods","Improving Automatic Source Code Summarization via Deep Reinforcement
  Learning",,http://arxiv.org/abs/1811.07234,,,core
186263760,2018-09-14T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but
resulting policies might perform poorly in environments that are even slightly
different. In robotics especially, training and deployment conditions often
vary and data collection is expensive, making retraining undesirable.
Simulation training allows for feasible training times, but on the other hand
suffers from a reality-gap when applied in real-world settings. This raises the
need of efficient adaptation of policies acting in new environments. We
consider this as a problem of transferring knowledge within a family of similar
Markov decision processes.
  For this purpose we assume that Q-functions are generated by some
low-dimensional latent variable. Given such a Q-function, we can find a master
policy that can adapt given different values of this latent variable. Our
method learns both the generative mapping and an approximate posterior of the
latent variables, enabling identification of policies for new tasks by
searching only in the latent space, rather than the space of all policies. The
low-dimensional space, and master policy found by our method enables policies
to quickly adapt to new environments. We demonstrate the method on both a
pendulum swing-up task in simulation, and for simulation-to-real transfer on a
pushing task",VPE: Variational Policy Embedding for Transfer Reinforcement Learning,,http://arxiv.org/abs/1809.03548,,,core
149314152,2017-12-29T18:12:23Z,"<div><br></div><div>This dataset contains the source code for the Survival Factorization Framework published as:</div><div><br></div><div>Nicola Barbieri, Giuseppe Manco, Ettore Ritacco: <b>Survival Factorization on Diffusion Networks</b>. THE EUROPEAN CONFERENCE ON MACHINE LEARNING & PRINCIPLES AND PRACTICE OF KNOWLEDGE DISCOVERY IN DATABASES, 2017. </div><div><br></div><div>The dataset is in .zip format that can be uncompressed by standard and openly accessible file zip utilities. Code is stored in .java and .jar files that can be accessed and edited by standard and openly accessible text edit software. Testing and training datasets containing Users and Cascade Timestamps are available in various text file formats. Figures and tables from the related publication are included in .pdf format. </div><div><br></div><div>See the description below for more detail on file formats and instructions on building the model and Network Reconstruction.</div><div><br></div><div>In the related paper we propose a survival factorization framework that models information cascades by tying together social influence patterns, topical structure and temporal dynamics. This is achieved through the introduction of a latent space which encodes: (a) the relevance of a information cascade on a topic; (b) the topical authoritativeness and the susceptibility of each individual involved in the information cascade, and (c) temporal topical patterns. By exploiting the cumulative properties of the survival function and of the likelihood of the model on a given adoption log, which records the observed activation times of users and side-information for each cascade, we show that the inference phase is linear in the number of users and in the number of adoptions. The evaluation on both synthetic and real-world data shows the effectiveness of the model in detecting the interplay between topics and social influence patterns, which ultimately provides high accuracy in predicting users activation times.</div><div><br></div><div><br></div><div>###############################</div><div><br></div><div>How to Build the Model:</div><div><br></div><div>Run <b>survivalFactorizationEM.SurvivalFactorizationEM_Runner</b> providing a path of a configuration file. Given a dataset, this script generates several instances of class <b>survivalFactorizationEM.SurvivalFactorizationEM_Model</b>.</div><div>The configuration file must to be written according to the .”properties” syntax. The fields are:</div><div><br></div><div>n_factors = </div><div>output = </div><div>max_iterations = </div><div>assignment_file = </div><div>event_file = </div><div>[ content_file =  ]</div><div><br></div><div>where</div><div><br></div><div>: an integer list separated by “;”</div><div>e.g. n_factors = 2;4;8;16;32;64;128</div><div>This list sets the number of models to build, one for each number of factors.</div><div><br></div><div>: a String</div><div>e.g. output = resources/datasets/synth/models/Synth</div><div>This string contains two elements:</div><div>- the path of folder where the built models (in the example “resources/datasets/synth/models”) will be stored</div><div>- the prefix of the name of the file which will contain the model (in the example “Synth”).</div><div>For each topic number in , a model, with the corresponding number of factors, will be created and will be stored in the folder; the name of the model file is a concatenation of the prefix + “_” +  + “.model”</div><div><br></div><div>: an integer</div><div>e.g. max_iterations = 1000</div><div>The fix point iterations will continue until convergence or when this number (burn-in phase included) is reached</div><div><br></div><div>: a String</div><div>e.g. assignment_file = resources/datasets/synth/models/Synth</div><div>This field is similar to . Each assignment file will contain the association cascade - topic for each cascade</div><div><br></div><div>: a String</div><div>e.g. event_file = resources/datasets/Synth/cascades_training.txt</div><div>The name of a file containing the cascades of events (e.g. tweets) exploited to build the model</div><div><br></div><div>: a String</div><div>e.g. event_file = resources/datasets/Synth/text_training.txt</div><div>The name of a file containing the text information for each cascade. Note: this field is optional</div><div><br></div><div><br></div><div>###############################</div><div><br></div><div>Cascade file format. A text document containing this information:</div><div><br></div><div>NodeIdCascadeIdTimeStamp</div><div>144911222254982000</div><div>693011222277866000</div><div>246621222281238000</div><div>…</div><div><br></div><div>Each row is an activation, the separator is the tab character “\t”</div><div><br></div><div><br></div><div>###############################</div><div><br></div><div>Text file format. A text document containing this information:</div><div><br></div><div>WordIdCascadeIdFrequency</div><div>124811</div><div>80415</div><div>678823</div><div>813421</div><div>…</div><div><br></div><div>Each row is an word in an event, the separator is the tab character “\t”</div><div><br></div><div><br></div><div>###############################</div><div><br></div><div>How to perform the Network Reconstruction:</div><div><br></div><div>Run <b>survivalFactorizationEM.FullTestNetworkReconstruction</b> providing a path of a configuration file. Given a folder containing built models and a test network, this script will generate the network reconstruction for each model. The configuration file must to be written according to the .”properties” syntax. The fields are:</div><div><br></div><div>model_folder = </div><div>model_files = </div><div>test_file = </div><div>output_folder = </div><div>output_files = </div><div><br></div><div>where:</div><div><br></div><div>: a String</div><div>e.g. model_folder = resources/datasets/synth/models</div><div>The folder containing the built models</div><div><br></div><div>: a String list whose elements are separated by “;”</div><div>e.g. model_files = Synth_2f.model;Synth_4f.model;Synth_8f.model;Synth_16f.model;Synth_32f.model;Synth_64f.model;Synth_128f.model</div><div>This list contains the file names where the models are stored</div><div><br></div><div>: a String</div><div>e.g. test_file = resources/datasets/Synth/s2/links_reduced-FF1400.remapped_two_hops</div><div>The file containing the test network to reconstruct. Note: the syntax of the test file is equal to the Cascade file format.</div><div><br></div><div>: a String</div><div>e.g. output_folder = resources/datasets/Synth/preds</div><div>The folder where to put the reconstructed network files (one for each model)</div><div><br></div><div>: a String list whose elements are separated by “;”</div><div>e.g. output_files = Synth_2f.pred;Synth_4f.pred;Synth_8f.pred;Synth_16f.pred;Synth_32f.pred;Synth_64f.pred;Synth_128f.pred</div><div>The file names of the reconstructed networks</div><div><br></div><div><br></div><div>###############################</div><div><br></div><div>Network reconstruction output prediction file format.</div><div><br></div><div>PredictionActualClass</div><div>1.15421803E-61</div><div>2.27729428E-61</div><div>1.16779013E-72</div><div>…</div><div><br></div><div>Each row is a prediction, the separator is the tab character “\t”</div",Survival Factorization Framework source code and data,10.6084/m9.figshare.5411341.v1,,,,core
161103586,2018-01-01T00:00:00,"Nearly three decades back nonlinear system identification consisted of several ad-hoc approaches, which were restricted to a very limited class of systems. However, with the advent of the various soft computing methodologies like neural networks and the fuzzy logic combined with optimization techniques, a wider class of systems can be handled at present. Complex systems may be of diverse characteristics and nature. These systems may be linear or nonlinear, continuous or discrete, time varying or time invariant, static or dynamic, short term or long term, central or distributed, predictable or unpredictable, ill or well defined. Neurofuzzy hybrid modelling approaches have been developed as an ideal technique for utilising linguistic values and numerical data. This Thesis is focused on the development of advanced neurofuzzy modelling architectures and their application to real case studies. Three potential requirements have been identified as desirable characteristics for such design: A model needs to have minimum number of rules; a model needs to be generic acting either as Multi-Input-Single-Output (MISO) or Multi-Input-Multi-Output (MIMO) identification model; a model needs to have a versatile nonlinear membership function.

Initially, a MIMO Adaptive Fuzzy Logic System (AFLS) model which incorporates a prototype defuzzification scheme, while utilising an efficient, compared to the Takagi–Sugeno–Kang (TSK) based systems, fuzzification layer has been developed for the detection of meat spoilage using Fourier transform infrared (FTIR) spectroscopy. The identification strategy involved not only the classification of beef fillet samples in their respective quality class (i.e. fresh, semi-fresh and spoiled), but also the simultaneous prediction of their associated microbiological population directly from FTIR spectra. In the case of AFLS, the number of memberships for each input variable was directly associated to the number of rules, hence, the “curse of dimensionality” problem was significantly reduced. Results confirmed the advantage of the proposed scheme against Adaptive Neurofuzzy Inference System (ANFIS), Multilayer Perceptron (MLP) and Partial Least Squares (PLS) techniques used in the same case study.

In the case of MISO systems, the TSK based structure, has been utilized in many neurofuzzy systems, like ANFIS. At the next stage of research, an Adaptive Fuzzy Inference Neural
Network (AFINN) has been developed for the monitoring the spoilage of minced beef utilising multispectral imaging information. This model, which follows the TSK structure,
incorporates a clustering pre-processing stage for the definition of fuzzy rules, while its final fuzzy rule base is determined by competitive learning. In this specific case study, AFINN model was also able to predict for the first time in the literature, the beef’s temperature directly from imaging information. Results again proved the superiority of the adopted model. By extending the line of research and adopting specific design concepts from the previous case studies, the Asymmetric Gaussian Fuzzy Inference Neural Network (AGFINN) architecture has been developed. This architecture has been designed based on the above design principles. A clustering preprocessing scheme has been applied to minimise the number of fuzzy rules. AGFINN incorporates features from the AFLS concept, by having the
same number of rules as well as fuzzy memberships. In spite of the extensive use of the standard symmetric Gaussian membership functions, AGFINN utilizes an asymmetric
function acting as input linguistic node. Since the asymmetric Gaussian membership function’s variability and flexibility are higher than the traditional one, it can partition the input space more effectively. AGFINN can be built either as an MISO or as an MIMO system. In the MISO case, a TSK defuzzification scheme has been implemented, while two different learning algorithms have been implemented. AGFINN has been tested on real datasets related to electricity price forecasting for the ISO New England Power Distribution System. Its performance was compared against a number of alternative models, including ANFIS, AFLS, MLP and Wavelet Neural Network (WNN), and proved to be superior. The concept of asymmetric functions proved to be a valid hypothesis and certainly it can find application to other architectures, such as in Fuzzy Wavelet Neural Network models, by designing a suitable flexible wavelet membership function. AGFINN’s MIMO characteristics also make the proposed architecture suitable for a larger range of applications/problems",Neuro-Fuzzy Based Intelligent Approaches to Nonlinear System Identification and Forecasting,,https://core.ac.uk/download/161103586.pdf,,,core
337600826,2018-05-01T07:00:00,"Audio is represented in two mathematically equivalent ways: the real-valued time domain (i.e., waveform) and the complex-valued frequency domain (i.e., spectrum). There are advantages to the frequency-domain representation, e.g., the human auditory system is known to process sound in the frequency-domain. Furthermore, linear time-invariant systems are convolved with sources in the time-domain, whereas they may be factorized in the frequency-domain. Neural networks have become rather useful when applied to audio tasks such as machine listening and audio synthesis, which are related by their dependencies on high quality acoustic models. They ideally encapsulate fine-scale temporal structure, such as that encoded in the phase of frequency-domain audio, yet there are no authoritative deep learning methods for complex audio. This manuscript is dedicated to addressing the shortcoming. Chapter 2 motivates complex networks by their affinity with complex-domain audio, while Chapter 3 contributes methods for building and optimizing complex networks. We show that the naive implementation of Adam optimization is incorrect for complex random variables and show that selection of input and output representation has a significant impact on the performance of a complex network. Experimental results with novel complex neural architectures are provided in the second half of this manuscript. Chapter 4 introduces a complex model for binaural audio source localization. We show that, like humans, the complex model can generalize to different anatomical filters, which is important in the context of machine listening. The complex model\u27s performance is better than that of the real-valued models, as well as real- and complex-valued baselines. Chapter 5 proposes a two-stage method for speech enhancement. In the first stage, a complex-valued stochastic autoencoder projects complex vectors to a discrete space. In the second stage, long-term temporal dependencies are modeled in the discrete space. The autoencoder raises the performance ceiling for state of the art speech enhancement, but the dynamic enhancement model does not outperform other baselines. We discuss areas for improvement and note that the complex Adam optimizer improves training convergence over the naive implementation",Complex Neural Networks for Audio,,https://core.ac.uk/download/337600826.pdf,Dartmouth Digital Commons,,core
354416839,2017-01-01T00:00:00,"2017 Fall.Includes bibliographical references.Rainfall estimation based on satellite measurements has proven to be very useful for various applications. A number of precipitation products at multiple time and space scales have been developed based on satellite observations. For example, the National Oceanic and Atmospheric Administration (NOAA) Climate Prediction Center has developed a morphing technique (i.e., CMORPH) to produce global precipitation products by combining existing space-based observations and retrievals. The CMORPH products are derived using infrared (IR) brightness temperature information observed by geostationary satellites and passive microwave-(PMW) based precipitation retrievals from low earth orbit satellites. Although space-based precipitation products provide an excellent tool for regional, local, and global hydrologic and climate studies as well as improved situational awareness for operational forecasts, their accuracy is limited due to restrictions of spatial and temporal sampling and the applied parametric retrieval algorithms, particularly for light precipitation or extreme events such as heavy rain. In contrast, ground-based radar is an excellent tool for quantitative precipitation estimation (QPE) at finer space-time scales compared to satellites. This is especially true after the implementation of dual-polarization upgrades and further enhancement by urban scale X-band radar networks. As a result, ground radars are often critical for local scale rainfall estimation and for enabling forecasters to issue severe weather watches and warnings. Ground-based radars are also used for validation of various space measurements and products. In this study, a new S-band dual-polarization radar rainfall algorithm (DROPS2.0) is developed that can be applied to the National Weather Service (NWS) operational Weather Surveillance Radar-1988 Doppler (WSR-88DP) network. In addition, a real-time high-resolution QPE system is developed for the Engineering Research Center for Collaborative Adaptive Sensing of the Atmosphere (CASA) Dallas-Fort Worth (DFW) dense radar network, which is deployed for urban hydrometeorological applications via high-resolution observations of the lower atmosphere. The CASA/DFW QPE system is based on the combination of a standard WSR-88DP (i.e., KFWS radar) and a high-resolution dual-polarization X-band radar network. The specific radar rainfall methodologies at Sand X-band frequencies, as well as the fusion methodology merging radar observations at different temporal resolutions are investigated. Comparisons between rainfall products from the DFW radar network and rainfall measurements from rain gauges are conducted for a large number of precipitation events over several years of operation, demonstrating the excellent performance of this urban QPE system. The real-time DFW QPE products are extensively used for flood warning operations and hydrological modelling. The high-resolution DFW QPE products also serve as a reliable dataset for validation of Global Precipitation Measurement (GPM) satellite precipitation products. This study also introduces a machine learning-based data fusion system termed deep multi-layer perceptron (DMLP) to improve satellite-based precipitation estimation through incorporating ground radar-derived rainfall products. In particular, the CMORPH technique is applied first to derive combined PMW-based rainfall retrievals and IR data from multiple satellites. The combined PMW and IR data then serve as input to the proposed DMLP model. The high-quality rainfall products from ground radars are used as targets to train the DMLP model. In this dissertation, the prototype architecture of the DMLP model is detailed. The urban scale application over the DFW metroplex is presented. The DMLP-based rainfall products are evaluated using currently operational CMORPH products and surface rainfall measurements from gauge networks","Radar and satellite observations of precipitation: space time variability, cross-validation, and fusion",,https://core.ac.uk/download/354416839.pdf,Colorado State University. Libraries,,core
213622948,2018-12-01T00:00:00,"Avec l’intérêt que la technologie d’aujourd’hui a sur les données, il est facile de supposer que l’information est au bout des doigts, prêt à être exploité. Les méthodologies et outils de recherche sont souvent construits sur cette hypothèse. Cependant, cette illusion d’abondance

se brise souvent lorsqu’on tente de transférer des techniques existantes à des applications industrielles.

Par exemple, la recherche a produit divers méthodologies permettant d’optimiser l’utilisation des ressources de grands systèmes complexes, tels que les avioniques de l’Airbus A380. Ces approches nécessitent la connaissance de certaines mesures telles que les temps d’exécution, la consommation de mémoire, critères de communication, etc. La conception de ces systèmes complexes a toutefois employé une combinaison de compétences de différents domaines (probablement avec des connaissances en génie logiciel) qui font que les données caractéristiques au système sont incomplètes ou manquantes. De plus, l’absence d’informations

pertinentes rend difficile de décrire correctement le système, de prédire son comportement, et améliorer ses performances. Nous faisons recours au modèles probabilistes et des techniques d’apprentissage automatique pour remédier à ce manque d’informations pertinentes. La théorie des probabilités, en particulier, a un grand potentiel pour décrire les systèmes partiellement observables. Notre objectif est de fournir des approches et des solutions pour produire des informations pertinentes. Cela permet une description appropriée des systèmes complexes pour faciliter l’intégration, et permet l’utilisation des techniques d’optimisation existantes. Notre première étape consiste à résoudre l’une des difficultés rencontrées lors de l’intégration de système : assurer le bon comportement temporelle des composants critiques des systèmes. En raison de la mise à l’échelle de la technologie et de la dépendance croissante à l’égard des architectures à multi-coeurs, la surcharge de logiciels fonctionnant sur différents coeurs et le partage d’espace mémoire n’est plus négligeable. Pour tel, nous étendons la boîte à outils des système temps réel avec une analyse temporelle probabiliste statique qui estime avec précision l’exécution d’un logiciel avec des considerations pour les conflits de mémoire partagée. Le

modèle est ensuite intégré dans un simulateur pour l’ordonnancement de systèmes temps réel multiprocesseurs. ----------ABSTRACT: In today’s data-driven technology, it is easy to assume that information is at the tip of our fingers, ready to be exploited. Research methodologies and tools are often built on top of this assumption. However, this illusion of abundance often breaks when attempting

to transfer existing techniques to industrial applications. For instance, research produced various methodologies to optimize the resource usage of large complex systems, such as the avionics of the Airbus A380. These approaches require the knowledge of certain metrics such as the execution time, memory consumption, communication delays, etc. The design of these complex systems, however, employs a mix of expertise from different fields (likely with limited knowledge in software engineering) which might lead to incomplete or missing specifications. Moreover, the unavailability of relevant information makes it difficult to properly describe

the system, predict its behavior, and improve its performance. We fall back on probabilistic models and machine learning techniques to address this lack of

relevant information. Probability theory, especially, has great potential to describe partiallyobservable systems. Our objective is to provide approaches and solutions to produce relevant information. This enables a proper description of complex systems to ease integration, and allows the use of existing optimization techniques. Our first step is to tackle one of the difficulties encountered during system integration: ensuring the proper timing behavior of critical systems. Due to technology scaling, and with the growing reliance on multi-core architectures, the overhead of software running on different cores and sharing memory space is no longer negligible. For such, we extend the real-time

system tool-kit with a static probabilistic timing analysis technique that accurately estimates the execution of software with an awareness of shared memory contention. The model is then incorporated into a simulator for scheduling multi-processor real-time systems",Handling Information and its Propagation to Engineer Complex Embedded Systems,,https://core.ac.uk/download/213622948.pdf,,,core
84292749,2017-08-05T17:00:00,"Spacecraft health monitoring is essential to ensure that a spacecraft is operating properly and has no anomalies that could jeopardize its mission. Many current methods of monitoring system health are difficult to use as the complexity of spacecraft increase, and are in many cases impractical on CubeSat’s which have strict size and resource limitations. To overcome these problems, new data-driven techniques such as Inductive Monitoring System (IMS), use data mining and machine learning on archived system telemetry to create models that characterize nominal system behavior. These characterizations can then be autonomously compared against real-time telemetry on-board the spacecraft to determine if the spacecraft is operating nominally.
This paper presents an adaption of IMS to create a spacecraft health monitoring system for CubeSat missions developed by the PolySat lab. This system is integrated into PolySat\u27s flight software and provides real time health monitoring of the spacecraft during its mission. Any anomalies detected are reported and further analysis can be done to determine the cause. The system was successful in the detection and identification of known anomalies in archived flight telemetry from the IPEX mission. In addition, real-time monitoring performed on the satellite yielded great results that give us confidence in the use of this system in all future missions",A Data-Driven Approach to CubeSat Health Monitoring,,https://core.ac.uk/download/84292749.pdf,DigitalCommons@USU,,core
227004401,2018-10-31T00:00:00,"Humans perform remarkably well in many cognitive tasks including pattern recognition. However, the neuronal mechanisms underlying this process are not well understood. Nevertheless, artificial neural networks, inspired in brain circuits, have been designed and used to tackle spatio-temporal pattern recognition tasks. In this paper we present a multi-neuronal spike pattern detection structure able to autonomously implement online learning and recognition of parallel spike sequences (i.e., sequences of pulses belonging to different neurons/neural ensembles). The operating principle of this structure is based on two spiking/synaptic neurocomputational characteristics: spike latency, which enables neurons to fire spikes with a certain delay and heterosynaptic plasticity, which allows the own regulation of synaptic weights. From the perspective of the information representation, the structure allows mapping a spatio-temporal stimulus into a multi-dimensional, temporal, feature space. In this space, the parameter coordinate and the time at which a neuron fires represent one specific feature. In this sense, each feature can be considered to span a single temporal axis. We applied our proposed scheme to experimental data obtained from a motor-inhibitory cognitive task. The results show that out method exhibits similar performance compared with other classification methods, indicating the effectiveness of our approach. In addition, its simplicity and low computational cost suggest a large scale implementation for real time recognition applications in several areas, such as brain computer interface, personal biometrics authentication, or early detection of diseases.GS acknowledges financial support by the Spanish Ministry of Economy and Competitiveness (PTA-2015-10395-I).

Research by author LC is supported by Viera y Clavijo fellowship from Tenerife, Spain.

ML is supported by a postdoctoral fellowship from the Spanish Ministry of Economy and Competitiveness (IJCI-2016-30662).CM and EP acknowledge support from the Spanish Ministry of Economy and Competitiveness and Fondo Europeo de Desarrollo Regional (FEDER) through projects TEC2016-80063-C3-3-R (AEI/FEDER, UE).

CM acknowledges the Spanish State Research Agency, through the María de Maeztu Program for Units of Excellence in R&D (MDM-2018-2022).Peer reviewe","A Neuro-Inspired System for Online Learning and Recognition of Parallel Spike Trains, Based on Spike Latency, and Heterosynaptic STDP",10.3389/fnins.2018.00780,,'Frontiers Media SA',"[{'title': 'Frontiers in Neuroscience', 'identifiers': ['issn:1662-453X', 'issn:1662-4548', '1662-453x', '1662-4548']}]",core
323893305,2017-11-01T00:00:00,"© 2017, The Author(s). A robot agent designed to engage in real-world human–robot joint action must be able to understand the social states of the human users it interacts with in order to behave appropriately. In particular, in a dynamic public space, a crucial task for the robot is to determine the needs and intentions of all of the people in the scene, so that it only interacts with people who intend to interact with it. We address the task of estimating the engagement state of customers for a robot bartender based on the data from audiovisual sensors. We begin with an offline experiment using hidden Markov models, confirming that the sensor data contains the information necessary to estimate user state. We then present two strategies for online state estimation: a rule-based classifier based on observed human behaviour in real bars, and a set of supervised classifiers trained on a labelled corpus. These strategies are compared in offline cross-validation, in an online user study, and through validation against a separate test corpus. These studies show that while the trained classifiers are best in a cross-validation setting, the rule-based classifier performs best with novel data; however, all classifiers also change their estimate too frequently for practical use. To address this issue, we present a final classifier based on Conditional Random Fields: this model has comparable performance on the test data, with increased stability. In summary, though, the rule-based classifier shows competitive performance with the trained classifiers, suggesting that for this task, such a simple model could actually be a preferred option, providing useful online performance while avoiding the implementation and data-scarcity issues involved in using machine learning for this task",Automatically Classifying User Engagement for Dynamic Multi-party Human–Robot Interaction,10.1007/s12369-017-0414-y,https://core.ac.uk/download/323893305.pdf,'Springer Science and Business Media LLC',,core
226714732,2018,"In Smart Environments, people are immersed in time and space in an augmented reality: computing systems are ubiquitous, intelligence pervades the whole environment and exploits space and time awareness to provide a globally contextualised, adaptive user experience. Because of their socio-technical nature, the development of Smart Environments calls for skills, concepts, methodologies, technologies from the most diverse fields – AI, coordination, distributed systems, organisational sciences, etc. –, thus inherently calling for a multi-paradigm perspective. This paper explores the Smart Environment context moving from two basic bricks: Butlers for Smart Spaces, a framework for the design of smart services for users immersed and interacting with their surrounding environment; and Home Manager, a multi-paradigm, agent-based platform for the implementation of Smart Living contexts—particularly focused on the reasoning aspects, so as to anticipate the users’ needs. For concreteness, we take a Smart Kitchen as our running example: we first discuss how it can be devised using Butlers for Smart Spaces, and then deployed on the Home Manager platform—highlighting in particular the context reasoning, prediction and adaptation aspects",Context Reasoning and Prediction in Smart Environments: the Home Manager case,10.1007/978-3-319-59480-4_45,,Springer International Publishing,,core
74204352,2017-03-20T00:00:00,"Two of the main problems encountered in the development and accurate
validation of photometric redshift (photo-z) techniques are the lack of
spectroscopic coverage in feature space (e.g. colours and magnitudes) and the
mismatch between photometric error distributions associated with the
spectroscopic and photometric samples. Although these issues are well known,
there is currently no standard benchmark allowing a quantitative analysis of
their impact on the final photo-z estimation. In this work, we present two
galaxy catalogues, Teddy and Happy, built to enable a more demanding and
realistic test of photo-z methods. Using photometry from the Sloan Digital Sky
Survey and spectroscopy from a collection of sources, we constructed datasets
which mimic the biases between the underlying probability distribution of the
real spectroscopic and photometric sample. We demonstrate the potential of
these catalogues by submitting them to the scrutiny of different photo-z
methods, including machine learning (ML) and template fitting approaches.
Beyond the expected bad results from most ML algorithms for cases with missing
coverage in feature space, we were able to recognize the superiority of global
models in the same situation and the general failure across all types of
methods when incomplete coverage is convoluted with the presence of photometric
errors - a data situation which photo-z methods were not trained to deal with
up to now and which must be addressed by future large scale surveys. Our
catalogues represent the first controlled environment allowing a
straightforward implementation of such tests. The data are publicly available
within the COINtoolbox (https://github.com/COINtoolbox/photoz_catalogues).Comment: 19 pages, 10 figures. Minor revision accepted by MNRAS on 2017 March
  1","On the realistic validation of photometric redshifts, or why Teddy will
  never be Happy",10.1093/mnras/stx687,http://arxiv.org/abs/1701.08748,'Oxford University Press (OUP)',,core
83860597,2017-05-16T00:00:00,"Subspace data representation has recently become a common practice in many
computer vision tasks. It demands generalizing classical machine learning
algorithms for subspace data. Low-Rank Representation (LRR) is one of the most
successful models for clustering vectorial data according to their subspace
structures. This paper explores the possibility of extending LRR for subspace
data on Grassmann manifolds. Rather than directly embedding the Grassmann
manifolds into the symmetric matrix space, an extrinsic view is taken to build
the LRR self-representation in the local area of the tangent space at each
Grassmannian point, resulting in a localized LRR method on Grassmann manifolds.
A novel algorithm for solving the proposed model is investigated and
implemented. The performance of the new clustering algorithm is assessed
through experiments on several real-world datasets including MNIST handwritten
digits, ballet video clips, SKIG action clips, DynTex++ dataset and highway
traffic video clips. The experimental results show the new method outperforms a
number of state-of-the-art clustering methodsComment: IEEE Transactions on Circuits and Systems for Video Technology with
  Minor Revisions. arXiv admin note: text overlap with arXiv:1504.0180",Localized LRR on Grassmann Manifolds: An Extrinsic View,,http://arxiv.org/abs/1705.06599,,,core
157651217,2018-05-01T00:00:00Z,"Many researchers have explored the relationship between recurrent neural networks and finite state machines. Finite state machines constitute the best-characterized computational model, whereas artificial neural networks have become a very successful tool for modeling and problem solving. The neurally-inspired lateral inhibition method, and its application to motion detection tasks, have been successfully implemented in recent years. In this paper, control knowledge of the algorithmic lateral inhibition (ALI) method is described and applied by means of finite state machines, in which the state space is constituted from the set of distinguishable cases of accumulated charge in a local memory. The article describes an ALI implementation for a motion detection task. For the implementation, we have chosen to use one of the members of the 16-nm Kintex UltraScale+ family of Xilinx FPGAs. FPGAs provide the necessary accuracy, resolution, and precision to run neural algorithms alongside current sensor technologies. The results offered in this paper demonstrate that this implementation provides accurate object tracking performance on several datasets, obtaining a high F-score value (0.86) for the most complex sequence used. Moreover, it outperforms implementations of a complete ALI algorithm and a simplified version of the ALI algorithm&mdash;named &ldquo;accumulative computation&rdquo;&mdash;which was run about ten years ago, now reaching real-time processing times that were simply not achievable at that time for ALI",A Finite State Machine Approach to Algorithmic Lateral Inhibition for Real-Time Motion Detection †,10.3390/s18051420,,MDPI AG,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
268659813,2017-08-01T07:00:00,"Deep Convolutional Neural Networks (CNN) have emerged as the dominant approach for solving many problems in computer vision and perception. Most state-of-the-art approaches to these problems are designed to operate using RGB color images as input. However, there are several settings that CNNs have been deployed in where more information about the state of the environment is available. Systems that require real-time perception such as self-driving cars or drones are typically equipped with sensors that can oﬀer several complementary representations of the environment. CNNs designed to take advantage of features extracted from multiple sensor modalities have the potential to increase the perception capability of autonomous systems. The work in this thesis extends the real-time CNN segmentation model ENet [39] to learn using a multimodal representation of the environment. Namely we investigate learning from disparity images generated by SGM [20] in conjunction with RGB color images from the Cityscapes dataset [10]. To do this we create a network architecture called MM-ENet composed of two symmetric feature extraction branches followed by a modality fusion network. We avoid using depth encoding strategies such as HHA [15] due to their computational cost and instead operate on raw disparity images. We also constrain the resolution of training and testing images to be relatively small, downsampled by a factor of four with respect to the original Cityscapes resolution. This is because a deployed version of this system must also run SGM in real-time, which could become a computational bottleneck if using higher resolution images. To design the best model for this task, we train several architectures that diﬀer with respect to the operation used to combine features: channel-wise concatenation, element-wise multiplication, and element-wise addition. We evaluate all models using Intersection-over-Union (IoU) as a primary performance metric and the instance-level IoU (iIoU) as a secondary metric. Compared to the baseline ENet model, we achieve comparable segmentation performance and are also able to take advantage of features
that cannot be extracted from RGB images alone. The results show that at this particular fusion location, elementwise multiplication is the best overall modality combination method. Through observing feature activations at diﬀerent points in the network we show that depth information helps the network reason about object edges and boundaries that are not as salient in color space, particularly with respect to spatially small object classes such as persons. We also present results that suggest that even though each branch learned to extract unique features, these features can have complementary properties. By extending the ENet model to learn from multimodal data we provide it with a richer representation of the environment. Because this extension simply duplicates layers in the encoder to create two symmetric feature extraction branches, the network also maintains real-time inference performance. Due to the network being trained on smaller resolution images to remain within the constraints of an embedded system, the overall performance is competitive but below state-of-the-art models reported on the Cityscapes leaderboard. When deploying this model in a high-performance system such as an autonomous vehicle that has the ability to generate disparity maps in real-time at a high resolution, MM-ENet can take advantage of unused data modalities to improve overall performance on semantic segmentation",Deep Multimodal Fusion Networks for Semantic Segmentation,,https://core.ac.uk/download/268659813.pdf,Clemson University Libraries,,core
373108474,2018-01-01T00:00:00,"У статті досліджено особливості розвитку цифрового маркетингу в сучасних умовах. Розглянуто специфіку запровадження інноваційних цифрових технологій у маркетингові діяльності. Визначено специфіку розвитку віртуальної реальності та особливості її використання у системі цифрового маркетингу. У науковому дослідженні представлено приклади передового використання технологій віртуальної реальності компаніями світу при реалізації їх маркетингових стратегій. Висвітлено специфіку застосування технології віртуальної реальності за умови витрат значних фінансових ресурсів. Також наведено приклад використання дешевих аналогів цифрових технологій за умови використання смартфонів. Висвітлено специфіку залучення клієнтів до дешевих цифрової технологій та особливості збільшення інтересу цільової аудиторії. Отримані результати дають можливість визначити основні тенденції розвитку цифрового маркетингу з застосуванням технологій віртуальної реальності. У дослідженні значну увагу приділено питанням реалізації технологій використання LED панелей. Обґрунтовано доцільність використання зазначеної технології для indoor маркетингу – виду діяльності всередині приміщень (торговельних закладів, кафе, офісів компаній тощо) та outdoor маркетингу – виду діяльності у зовнішньому просторі (на улицях, парках та ін.). Наведено приклади застосування LED панелей передовими компаніями світу. Висвітлено специфіку застосування даної технології у сфері харчування. Встановлено, що LED панелі відіграють важливу роль у цифровій маркетинговій стратегії кафе та ресторанів. Визначено, що цифрові панелі сприяють зростанню кількості клієнтів. Доведено, що запровадження систем штучного інтелекту у подальшому сприятиме збільшенню інтерактивності та персоніфікації контенту у відповідності з умовами, які будуть проявлятись у певний момент часу в конкретному місці. Встановлено, що зазначений підхід дозволить національним компаніям підвищити рівень їх конкурентоспроможності на національному та міжнародному ринках. Доведено необхідність запровадження державної програми сприяння розвитку цифрового маркетингу в Україні.In the article the features of development of digital marketing in modern conditions are investigated. The specifics of introduction of innovative technologies in marketing strategies of companies are considered. The specificity of the development of virtual reality and the peculiarities of its using in the digital marketing system are determined. In the scientific research, examples of the advanced use of technologies of virtual reality by companies of the world in the implementation of their marketing strategies are presented. The specificity of the application of the virtual reality technology is highlighted with the cost of significant financial resources. The example is the using of cheap analogues of digital technology such as using smartphones. The specifics of attraction the clients to cheap digital technologies and interest increase features of the target audience are highlighted. The obtained results give an opportunity to define the basic tendencies of digital marketing development with application of virtual reality technologies. The study focuses on the implementation of technology for the use of LED panels. The expediency of using this technology for indoor marketing is grounded - the type of activity in the middle of the premises (shopping centers, cafes, offices of companies, etc.) and outdoor marketing - the type of activity in the outer space (in streets, parks, etc.). One example of the using of LED panels is presented by leading companies of the world. The application specifics of this technology in the field of food are highlighted. It has been established that LED panels play the important role in the digital marketing strategy of cafes and restaurants. It is determined that digital panels help to increase the number of clients. It is proved that the introduction of artificial intelligence systems will further enhance the interactivity and personalization of content in accordance with conditions that will manifest at a certain point in time in a special place. It has been established that this approach will allow national companies to increase their competitiveness in the national and international markets. The necessity of implementation of the state program of digital marketing promotion in Ukraine has been proved",Practical aspects of innovative digital technologies application in marketing,,https://core.ac.uk/download/373108474.pdf,,"[{'title': 'Економічний вісник Національного технічного університету України «Київський політехнічний інститут»', 'identifiers': ['2307-5651', 'issn:2412-5296', '2412-5296', 'issn:2307-5651']}]",core
303038963,2018-04-23T00:00:00,"Three-dimensional (3D) fetal neurosonography is used clinically to detect cerebral abnormalities and to assess growth in the developing brain. However, manual identification of key brain structures in 3D ul- trasound images requires expertise to perform and even then is tedious. Inspired by how sonographers view and interact with volumes during real-time clinical scanning, we propose an efficient automatic method to simultaneously localize multiple brain structures in 3D fetal neurosonography. The proposed View-based Projection Networks (VP-Nets) , uses three view-based Convolutional Neural Networks (CNNs), to simplify 3D localizations by directly predicting 2D projections of the key structures onto three anatom- ical views.

While designed for efficient use of data and GPU memory, the proposed VP-Nets allows for full- resolution 3D prediction. We investigated parameters that influence the performance of VP-Nets, e.g. depth and number of feature channels. Moreover, we demonstrate that the model can pinpoint the struc- ture in 3D space by visualizing the trained VP-Nets, despite only 2D supervision being provided for a sin- gle stream during training. For comparison, we implemented two other baseline solutions based on Ran- dom Forest and 3D U-Nets. In the reported experiments, VP-Nets consistently outperformed other meth- ods on localization. To test the importance of loss function, two identical models are trained with binary corss-entropy and dice coefficient loss respectively. Our best VP-Net model achieved prediction center deviation: 1.8 ±1.4 mm, size difference: 1.9 ±1.5 mm, and 3D Intersection Over Union (IOU): 63.2 ±14.7% when compared to the ground truth. To make the whole pipeline intervention free, we also implement a skull-stripping tool using 3D CNN, which achieves high segmentation accuracy. As a result, the proposed processing pipeline takes a raw ultrasound brain image as input, and output a skull-stripped image with five detected key brain structures.</p",VP-Nets : Efficient automatic localization of key brain structures in 3D fetal neurosonography,10.1016/j.media.2018.04.004,,'Elsevier BV',"[{'title': 'Medical Image Analysis', 'identifiers': ['1361-8415', 'issn:1361-8415']}]",core
200641548,2018-10-31T04:19:04Z,"<p>Humans perform remarkably well in many cognitive tasks including pattern recognition. However, the neuronal mechanisms underlying this process are not well understood. Nevertheless, artificial neural networks, inspired in brain circuits, have been designed and used to tackle spatio-temporal pattern recognition tasks. In this paper we present a multi-neuronal spike pattern detection structure able to autonomously implement online learning and recognition of parallel spike sequences (i.e., sequences of pulses belonging to different neurons/neural ensembles). The operating principle of this structure is based on two spiking/synaptic neurocomputational characteristics: spike latency, which enables neurons to fire spikes with a certain delay and heterosynaptic plasticity, which allows the own regulation of synaptic weights. From the perspective of the information representation, the structure allows mapping a spatio-temporal stimulus into a multi-dimensional, temporal, feature space. In this space, the parameter coordinate and the time at which a neuron fires represent one specific feature. In this sense, each feature can be considered to span a single temporal axis. We applied our proposed scheme to experimental data obtained from a motor-inhibitory cognitive task. The results show that out method exhibits similar performance compared with other classification methods, indicating the effectiveness of our approach. In addition, its simplicity and low computational cost suggest a large scale implementation for real time recognition applications in several areas, such as brain computer interface, personal biometrics authentication, or early detection of diseases.</p","Data_Sheet_1_A Neuro-Inspired System for Online Learning and Recognition of Parallel Spike Trains, Based on Spike Latency, and Heterosynaptic STDP.PDF",10.3389/fnins.2018.00780.s001,,,,core
430162782,2018-06-01T00:00:00,"Obtaining good quality image features is of remarkable importance for most computer vision tasks. It has been demonstrated that the first layers of the human visual cortex are devoted to feature detection. The need for these features has made line, segment, and corner detection one of the most studied topics in computer vision. HT3D is a recent variant of the Hough transform for the combined detection of corners and line segments in images. It uses a 3D parameter space that enables the detection of segments instead of whole lines. This space also encloses canonical configurations of image corners, transforming corner detection into a pattern search problem. Spiking neural networks have previously been proposed for multiple image processing tasks, including corner and line detection using the Hough transform. Following these ideas, this paper presents and describes in detail a model to implement HT3D as a Spiking Neural Network for corner detection. The results obtained from a thorough testing of its implementation using real images evince the correctness of the Spiking Neural Network HT3D implementation. Such results are comparable to those obtained with the regular HT3D implementation, which are turn superior to other corner detection algorithms",A Spiking Neural Model of HT3D for Corner Detection,10.3389/fncom.2018.00037/full,https://core.ac.uk/download/430162782.pdf,'Frontiers Media SA',,core
147103749,2018-01-19T11:24:26,"Massive Open Online Courses (MOOCs) have existed as a disruptive educational phenomenon  for nine years. Grounded in the roots of distance education, open education, Open Educational  Resources, and OpenCourseWare, MOOCs have now survived various critics and have  continued growing globally. Reports about MOOCs in both the press and scholarly publications  began to grow significantly in 2013 (Sánchez-Vera, Leon Urrutia, & Davis, 2015; Zancanaro &  Domingues, 2017) and, since then, more and more researchers have joined the discussions,  developing them to explore various new topics. To contribute to the literature of MOOC studies,  this doctoral thesis begins with an in-depth analysis of the background, history, growth, and  vision, and proposes a tentative definition of MOOCs. Meanwhile, by conducting bibliometric  research to review MOOC studies conducted between 2015 and 2017, this thesis fills in the gap  that has existed due to a lack of systematic reviews of MOOC literature since 2015. The results  of the bibliometric research summarised the relevant MOOC research into nine categories,  including learner focused, commentary and concepts, case reports or evaluations, pedagogy,  curriculum and design, course object focused, provider focused, technology, systematic review  of literature, and learning analytics and big data. They also suggested a limited amount of  provider focused research, which became the research interest and focus of this thesis. In the  centre of the Europe, Swiss universities have marched forward in the MOOC movement,  together with other over 550 universities (Shah, 2016) around the world. Università della  Svizzera italiana (USI; Lugano, Switzerland), a Swiss public university, became a MOOC  provider in 2015 and offered the first MOOC in the topic of eTourism: eTourism: Communication  Perspectives. This doctoral thesis is closely related to this university-level initiative, which was  dedicated to producing the first pilot MOOC at USI. Therefore, the cases chosen by this thesis  are positioned in the discipline of tourism and hospitality. The first MOOC with a large audience  taught artificial intelligence in 2011 (Zancanaro & Domingues, 2017). Nowadays, MOOCs have  broken the barrier of space and time to educate the masses in a wide range of subjects.  However, the provision of MOOCs in the subject of tourism and hospitality did not appear until  2013, when two MOOCs from two American universities became available. In the past four  years since these MOOCs were launched, the number of tourism and hospitality MOOCs  available in the market has remained limited (Tracey, Murphy, & Horton-Tognazzini, 2016). This  scarcity contradicts the fact that tourism and hospitality is the field that contributes the most to  the employment of the global workforce. Pressing problems, such as high turnover, seasonality,  and new global challenges have urged for solutions to quickly training people working in this  area to become available (Cantoni, Kalbaska, & Inversini, 2009). A call for more studies about  tourism and hospitality MOOCs has emerged. The combined reality of the lack of studies  regarding MOOC providers, opportunities for first-hand experience of producing a tourism  MOOC in a university, and the deficiency in both the research and practises of tourism and  hospitality MOOCs has inspired the direction of this thesis in regard to exploring MOOC  instructors’ experiences, using cases in the field of tourism and hospitality. It cumulates six  studies, using a mixed methods approach, to tackle the two main research objectives: to  investigate at large the tourism and hospitality MOOC provisions between 2008 and 2015 and to  report the experiences of Università della Svizzera italiana (USI) when producing the eTourism  MOOC. In order, the first two studies in Chapter 3 of this thesis focus on tourism and hospitality  MOOCs in general and produce a big picture context for the other four studies in Chapter 4. The  first study proposes a conceptual framework through which to describe and analyse the course  design of a MOOC and applies it to 18 tourism and hospitality MOOCs produced between 2008  and 2015. The second study then continues to interview six tourism and hospitality MOOC  instructors, to describe their experiences and perspectives of teaching MOOCs. After exploring  a holistic view of the overall development of MOOCs in tourism and hospitality and gaining a  deep understanding of the instructors behind these offerings, this thesis introduces the  experiences of one single MOOC provider: Università della Svizzera italiana (USI) in Chapter 4.  It first introduces its overall implementation process (Study 3), and further elaborates three  phases of this process: how it selected a suitable MOOC platform at the beginning (Study 4);  how it assessed learner engagement in the MOOC (Study 5); and, eventually, how it evaluated  the performance of the MOOC (Study 6). This thesis was written mainly from the perspective of  eLearning, with the intention of benefiting its community of scholars and practitioners. It has  contributed to the literature by developing a framework with which to review MOOCs (in Study  1), the implementation process of producing MOOCs (in Study 2), practical review schema of  MOOC platforms (in Study 4), the MOOC Learner Engagement Online Survey (in Study 5), and  how to use the Kirkpatrick model to evaluate MOOCs (in Study 6). These conceptual  frameworks and experiential tools can benefit future researchers and practitioners. Meanwhile,  due to its intimate connection with the field of tourism and hospitality, by directly using its cases,  the research outputs of the six studies can also benefit the tourism and hospitality education and  training sector as a reference for further action",Exploring the experiences of instructors teaching massive open online courses in tourism and hospitality: a mixed methods approach,,https://core.ac.uk/download/147103749.pdf,,,core
224433751,2018-01-01T00:00:00,"With the support of big-data and big-compute, deep learning has reshaped the landscape of research and applications in artificial intelligence. Whilst traditional hand-guided feature engineering in many cases is simplified, the deep network architectures become increasingly more complex. A central question is whether we can distill the minimal set of structural priors that can provide us the maximal flexibility, and lead us to richer sets of structural primitives. Those structural priors will make the learning process more effective, and potentially lay the foundations towards the ultimate goal of building general intelligent systems. This dissertation focuses on how we can tackle different real world problems in computer vision and machine learning with carefully designed neural network architectures, guided by simple yet effective structural priors. In particular, this thesis focuses on two structural priors that have proven to be useful and generalizable in many different scenarios: the multi-scale prior, with an application in edge detection, and the sparse-connectivity prior implemented for generic visual recognition. Examples will be presented in the last part, on how to learn meaningful structures directly from data, rather than hard-wiring them by, for example, learning a convolutional pseudo-prior in the label space, or adopting a dynamic self-attention mechanism",Deep Representation Learning with Induced Structural Priors,,,"eScholarship, University of California",,core
160771987,2018-07-11T00:00:00,"Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable","Learning Deployable Navigation Policies at Kilometer Scale from a Single
  Traversal",,http://arxiv.org/abs/1807.05211,,,core
186292536,2018-11-15T00:00:00,"We apply numerical methods in combination with finite-difference-time-domain
(FDTD) simulations to optimize transmission properties of plasmonic mirror
color filters using a multi-objective figure of merit over a five-dimensional
parameter space by utilizing novel multi-fidelity Gaussian processes approach.
We compare these results with conventional derivative-free global search
algorithms, such as (single-fidelity) Gaussian Processes optimization scheme,
and Particle Swarm Optimization---a commonly used method in nanophotonics
community, which is implemented in Lumerical commercial photonics software. We
demonstrate the performance of various numerical optimization approaches on
several pre-collected real-world datasets and show that by properly trading off
expensive information sources with cheap simulations, one can more effectively
optimize the transmission properties with a fixed budget.Comment: NIPS 2018 Workshop on Machine Learning for Molecules and Materials.
  arXiv admin note: substantial text overlap with arXiv:1811.0075",Optimizing Photonic Nanostructures via Multi-fidelity Gaussian Processes,,http://arxiv.org/abs/1811.07707,,,core
295400735,2018-09-01T00:00:00,"The present work proposes a methodology that covers the whole process of classifying hydrate formation-related faults on production lines of an offshore oil platform. Three datasets are analyzed in this work, where each one of them is composed of a variety of sensor measurements related to the wells of a different offshore oil platform. Our methodology goes through each step of dataset cleaning, which includes: identification of numerical and categorical tags, removal of spurious values and outliers, treatment of missing data by interpolation and the identification of relevant faults and tags on the platform. The present work designs a framework that puts together many Machine Learning classic techniques to perform the failure identification. The system is composed of three major blocks: the first block performs feature extraction: as the input data is a set of time-series signals we represent each signal using its statistical metrics computed over a sliding window; the second block maps the previous block output to a more suitable space, this transformation uses the z-score normalization and the Principal Components Analysis (PCA); the last block is the classifier, the one we adopted was the Random Forest classifier due to its simple tuning and excellent performance. We also propose a technique to increase the reliability of the normal operation data. When handling a database composed by real data, it is usual to face a lot of mislabeled data, which can significantly jeopardize the model performance. Therefore, we deploy a technique to reduce the mislabeled samples, which presented an improvement of 7.93%, on average, reaching over 80% of accuracy in all single-class scenarios.Este trabalho apresenta uma metodologia que cobre todo o desenvolvimento de um sistema de classificação de falhas relacionadas à formação de hidrato em linhas de produção de plataformas de petróleo. Serão utilizadas três bases de dados no desenvolvimento desse trabalho, onde cada uma delas é composta por uma variedade de medidas provenientes de sensores relacionados a poços. Nossa metodologia cobre todas as etapas de limpeza dessas bases: identificação de tags numéricas e categóricas; remoção de valores espúrios e de outliers; tratamento de dados faltantes através de interpolação; e a identificação de falhas e tags relevantes na plataforma. Desenvolvemos um framework formado por diversas técnicas clássicas da área de Aprendizado de Máquina. O sistema proposto é composto por três grandes blocos: o primeiro irá extrair as características estatísticas de cada sinal de entrada através de uma janela deslizante; o segundo bloco irá mapear a saída do bloco anterior em um espaço mais apropriado através de duas transformações: z-score e Principal Components Analysis (PCA); o último bloco é o classificador, que no caso optamos por ser o classificador Random Forest. Também propomos uma técnica para aumentar a confiabilidade das amostras referentes ao estado de operação normal da plataforma. Quando lidamos com dados reais, é muito comum que muitas amostras estejam marcadas erradas, ou seja, os seus rótulos não refletem o estado real de operação da plataforma. Para suavizar esse efeito indesejado, desenvolvemos um método para remover amostras com marcações erradas, com o qual melhoramos a performance do modelo em 7,93%, na média, alcançando mais de 80% de acurácia em todos os cenários de classificação de uma única classe",Machine learning tecniques applied to hydrate failure detection on production lines,,https://core.ac.uk/download/295400735.pdf,'Programa de Pos-graduacao em Ciencias Contabeis da UFRJ',,core
189837365,2018-07-10T00:00:00,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment based on how good the decisions are and tries to find an optimal decision-making policy that maximises its longterm cumulative reward. This paper presents a novel approach which has showon promise in applying accelerated simulation of RL policy training to automating the control of a real robot arm for specific applications. The approach has two steps. First, design space exploration techniques are developed to enhance performance of an FPGA accelerator for RL policy training based on Trust Region Policy Optimisation (TRPO), which results in a 43% speed improvement over a previous FPGA implementation, while achieving 4.65 times speed up against deep learning libraries running on GPU and 19.29 times speed up against CPU. Second, the trained RL policy is transferred to a real robot arm. Our experiments show that the trained arm can successfully reach to and pick up predefined objects, demonstrating the feasibility of our approach",Towards Hardware Accelerated Reinforcement Learning for Application-Specific Robotic Control,10.1109/ASAP.2018.8445099,,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': None, 'identifiers': ['1063-6862', 'issn:1063-6862']}]",core
322490268,2018-09-17T00:00:00,"© 2018 The Author(s).This paper presents a methodology to monitor the fatigue life of aerospace structures and hence the remaining allowable fatigue life. In fatigue clearance, conservative load assumptions are made. However, in reality, a structure may see much lower loads and so would be usable for much longer. An example ofthis is air carried guided missiles. In the UK, missiles must be decommissioned after a period of carriage. The implementation of a system that can monitor the usage of a missile during its time in service is advantageous to the military customer and provides a competitive advantage for the missile manufacture inexport markets where reduced through-life costs, longer in-service lives and increased safety are desired. The proposed methodology provides a means to monitor the service life of a missile. This paper describes how machine learning algorithms can be used with accelerometers to determine loads on a missile structure which would then be used to predict how long the missile has left in service",A methodology using health and usage monitoring system data for payload life prediction,,,"KU Leuven, Faculty of Arts",,core
161937450,2018-12-12T00:00:00,"While neuromorphic systems may be the ultimate platform for deploying spiking neural networks (SNNs), their distributed nature and optimisation for specific types of models makes them unwieldy tools for developing them. Instead, SNN models tend to be developed and simulated on computers or clusters of computers with standard von Neumann CPU architectures. Over the last decade, as well as becoming a common fixture in many workstations, NVIDIA GPU accelerators have entered the High Performance Computing field and are now used in 50% of the Top 10 super computing sites worldwide. In this paper we use our GeNN code generator to re-implement two neo-cortex-inspired, circuit-scale, point neuron network models on GPU hardware. We verify the correctness of our GPU simulations against prior results obtained with NEST running on traditional HPC hardware and compare the performance with respect to speed and energy consumption against published data from CPU-based HPC and neuromorphic hardware. A full-scale model of a cortical column can be simulated at speeds approaching 0.5× real-time using a single NVIDIA Tesla V100 accelerator – faster than is currently possible using a CPU based cluster or the SpiNNaker neuromorphic system. In addition, we find that, across a range of GPU systems, the energy to solution as well as the energy per synaptic event of the microcircuit simulation is as much as 14× lower than either on SpiNNaker or in CPU-based simulations. Besides performance in terms of speed and energy consumption of the simulation, efficient initialisation of models is also a crucial concern, particularly in a research context where repeated runs and parameter-space exploration are required. Therefore, we also introduce in this paper some of the novel parallel initialisation methods implemented in the latest version of GeNN and demonstrate how they can enable further speed and energy advantages",GPUs outperform current HPC and neuromorphic solutions in terms of speed and energy when simulating a highly-connected cortical model,10.3389/fnins.2018.00941,https://core.ac.uk/download/161937450.pdf,'Frontiers Media SA',,core
304993677,2018-05-31T18:15:03,"The exploration of oil and gas is a vital part of today's increasing power demands to meet the energy we need to power our homes, businesses, and transportation. Oil and gas explorers use seismic surveys, both onshore and offshore, to produce detailed images of the various rock types, layers, and their locations beneath the Earth's subsurface. The acquired data undergo a series of processing steps, which require powerful computing hardware, sophisticated software, and specialized manpower. To extract useful information from seismic data, interpreters manually delineate important geological structures, which contain hints about petroleum and gas reservoirs such as salt domes, faults, channels, fractures, and horizons. These structures typically span over several square kilometers and are delineated based on correlation, changes in illumination, intensity, contrast, and texture of seismic data. There are limited tools available for automatic detection and manual interpretation is becoming extremely time consuming and labor intensive. In this dissertation, we propose novel seismic attributes based on texture dissimilarity, visual-attention theory, the modeling of human visual system, and machine learning to quantify changes and highlight geological features in a three-dimensional space. To automate the process of seismic interpretation, we develop interpreter-assisted, fully-, and semi-automated workflows that are interactive and easy-to-use for the delineation of important geological structures within seismic volumes. Experimental results on real and synthetic datasets show that our proposed algorithms outperform the state-of-the-art methods for seismic interpretation. In a nutshell, this dissertation introduces novel seismic attributes and automated, interactive, and interpreter-assisted workflows, which have a very promising future in effective seismic interpretation. The proposed research is computationally inexpensive and is expected to not only reduce the time for seismic interpretation but also become a handy tool in the interpreter's toolbox for detecting and delineating important geological structures.Ph.D","Computational seismic interpretation using attention models, texture dissimilarity, and learning",,https://core.ac.uk/download/304993677.pdf,Georgia Institute of Technology,,core
151245668,2018-02-01T00:00:00,"Surveillance is essential for the safety of power substation. The detection
of whether wearing safety helmets or not for perambulatory workers is the key
component of overall intelligent surveillance system in power substation. In
this paper, a novel and practical safety helmet detection framework based on
computer vision, machine learning and image processing is proposed. In order to
ascertain motion objects in power substation, the ViBe background modelling
algorithm is employed. Moreover, based on the result of motion objects
segmentation, real-time human classification framework C4 is applied to locate
pedestrian in power substation accurately and quickly. Finally, according to
the result of pedestrian detection, the safety helmet wearing detection is
implemented using the head location, the color space transformation and the
color feature discrimination. Extensive compelling experimental results in
power substation illustrate the efficiency and effectiveness of the proposed
framework",Automatic Safety Helmet Wearing Detection,,http://arxiv.org/abs/1802.00264,,,core
201275310,2018-12-01T00:00:00Z,"The conclusions about the strata of society, various parties are supported by, have been made. The method of information-extreme machine learning of the system of functional diagnosis of the technical state of a complex machine with the optimization of the hierarchical data structure is considered. It is shown that the functional efficiency of machine learning of the system of functional diagnosis is significantly influenced by the location in the hierarchical structure of the recognition classes characterizing the technical state of the machine and its nodes. At the same time, for each level of the hierarchical structure under consideration, a restriction on the number of recognition classes is imposed, which makes it possible to reduce the degree of their intersection in the space of diagnostic features. Optimization of the hierarchical structure was carried out in the process of information-extreme machine learning of the system of functional diagnosis, which allows to maximize the information capacity of the system. As a criterion for optimizing the parameters of machine learning, we considered a modi fied information measure of Kulbak, which is a functional of the accurate characteristics of diagnostic solutions. In this case, the algorithm of machine learning represented a multi-cycle iterative procedure of finding the maximum global value of the information criterion for optimizing learning parameters in the working (permissible) domain of determining its function. Based on the optimal geometric parameters of recognition class containers obtained in the course of machine learning, decision rules have been constructed that allow making diagnostic decisions in a real time. As an example of the implementation of the method of optimization the structure of input data, the machine learning of the system for the functional diagnosis of a mine hoist was considered. As a result, alphabets of recognition classes have been created for strata of all tiers of the hierarchical structure, providing the maximum functional efficiency of machine learning",OPTIMIZATION OF HIERARCHICAL DATA STRUCTURE OF INTELLIGENT SYSTEM OF FUNCTIONAL DIAGNOSIS OF TECHNICAL CONDITION OF COMPLEX MACHINES,,,"National Technical University ""Kharkiv Polytechnic Institute""","[{'title': None, 'identifiers': ['issn:2410-2857', 'issn:2079-0023', '2410-2857', '2079-0023']}]",core
293738204,2018-01-01T00:00:00,"Control of nonlinear systems on continuous domains is a challenging task for various reasons.
For robust and accurate control of complex systems a precise model of the system dynamics is
essential. Building such highly precise dynamics models from physical knowledge often requires
substantial manual effort and poses a great challenge in industrial applications. Acquiring a model
automatically from system measurements employing regression techniques allows to decrease
manual effort and, thus, poses an interesting alternative to knowledge-based modeling. Based on
such a learned dynamics model, an approximately optimal controller can be inferred automatically.
Such approaches are the subject of model-based reinforcement learning (RL) and learn optimal
control from interactions with the system. Especially when probabilistic dynamics models such
as Gaussian processes are employed, model-based RL has been tremendously successful and has
attracted much attention from both the control and machine learning communities. However,
several problems need to be solved to facilitate widespread deployment of model-based RL for
learning control in real world scenarios. In this thesis, we address two current limitations of
model-based RL that are indispensable prerequisites for widespread deployment of model-based
RL in real world tasks.
In many real world applications a poor controller can cause severe damage to the system or
even put the safety of humans at risk. Thus, it is essential to ensure that the controlled system
behaves as desired. While this question has been studied extensively in classical control, stability
of closed-loop control systems with dynamics given as a Gaussian process has not been considered
yet. We propose an automatic tool to compute regions of the state space where the desired behavior
of the system can be guaranteed. We consider dynamics given as the mean of a GP as well as
the full GP posterior distribution. In the first case, the proposed tool constructs regions of the
state space, such that the trajectories starting in this region converge to the target state. From this
asymptotic result, we follow statements for finite time horizons and stability under the presence
of disturbances. In the second case the system dynamics is given as a GP posterior distribution.
Thus, computation of multi-step-ahead predictions requires averaging over all plausible dynamics
models given the observations. A a consequence, multi-step-ahead predictions become analytically
intractable. We propose an approximation based on numerical quadrature that can handle complex
state distributions, e.g., with multiple modes and provides upper bounds for the approximation
error. Exploiting these error bounds, we present an automatic tool to compute stability regions. In
these regions of the state space, our tool guarantees that for a finite time horizon the system behaves
as desired with a given probability. Furthermore, we analyze asymptotic behavior of closed-loop
control systems with dynamics given as a GP posterior distribution. In this case we show that for
some common choices of the prior, the system has a unique stationary distribution to which the
system state converges irrespective of the starting state.
Another major challenge of RL for real world control applications is to minimize interactions
with the system required for learning. While RL approaches based on GP dynamics models
have demonstrated great data efficiency, the average amount of required system interactions can further be reduced. To achieve this goal, we propose to employ the numerical quadrature based
approximation to propagate the value of a state. To show how this approximation can further
increase data efficiency, we employ it in the two main classes of model-based RL: policy search
and value iteration. In policy search, the state distribution must be computed to evaluate the
expected long-term reward for a policy. The proposed numerical quadrature based approximation
substantially improves estimates of the expected long-term reward and its gradients. As a result,
data efficiency is significantly increased.
For the value function based approaches for policy learning, the value propagation step is
completely characterized by the Bellman equation. However, this equation is intractable for
nonlinear dynamics. In this case, we propose a projection-based value iteration approach. We
employ numerical quadrature to facilitate projection of the value function onto a linear feature
space. Suitable features for value function representation are learned online without manual effort.
This feature learning is constructed such that upper bounds for the projection error can be obtained.
The proposed value iteration approach learns globally optimal policies and significantly benefits
from the introduced highly accurate approximations",Gaussian Processes in Reinforcement Learning: Stability Analysis and Efficient Value Propagation,,,,,core
42661039,2018-04-25T00:00:00,"Recent works on zero-shot learning make use of side information such as
visual attributes or natural language semantics to define the relations between
output visual classes and then use these relationships to draw inference on new
unseen classes at test time. In a novel extension to this idea, we propose the
use of visual prototypical concepts as side information. For most real-world
visual object categories, it may be difficult to establish a unique prototype.
However, in cases such as traffic signs, brand logos, flags, and even natural
language characters, these prototypical templates are available and can be
leveraged for an improved recognition performance. The present work proposes a
way to incorporate this prototypical information in a deep learning framework.
Using prototypes as prior information, the deepnet pipeline learns the input
image projections into the prototypical embedding space subject to minimization
of the final classification loss. Based on our experiments with two different
datasets of traffic signs and brand logos, prototypical embeddings incorporated
in a conventional convolutional neural network improve the recognition
performance. Recognition accuracy on the Belga logo dataset is especially
noteworthy and establishes a new state-of-the-art. In zero-shot learning
scenarios, the same system can be directly deployed to draw inference on unseen
classes by simply adding the prototypical information for these new classes at
test time. Thus, unlike earlier approaches, testing on seen and unseen classes
is handled using the same pipeline, and the system can be tuned for a trade-off
of seen and unseen class performance as per task requirement. Comparison with
one of the latest works in the zero-shot learning domain yields top results on
the two datasets mentioned above.Comment: 12 Pages, 6 Figures, 2 Tables, in British Machine Vision Conference
  (BMVC), 201",Prototypical Priors: From Improving Classification to Zero-Shot Learning,,http://arxiv.org/abs/1512.01192,,,core
186288738,2018-12-15T00:00:00,"In this paper we introduce evidence transfer for clustering, a deep learning
method that can incrementally manipulate the latent representations of an
autoencoder, according to external categorical evidence, in order to improve a
clustering outcome. By evidence transfer we define the process by which the
categorical outcome of an external, auxiliary task is exploited to improve a
primary task, in this case representation learning for clustering. Our proposed
method makes no assumptions regarding the categorical evidence presented, nor
the structure of the latent space. We compare our method, against the baseline
solution by performing k-means clustering before and after its deployment.
Experiments with three different kinds of evidence show that our method
effectively manipulates the latent representations when introduced with real
corresponding evidence, while remaining robust when presented with low quality
evidence","Evidence Transfer for Improving Clustering Tasks Using External
  Categorical Evidence",10.1109/IJCNN.2019.8852384,http://arxiv.org/abs/1811.03909,'Institute of Electrical and Electronics Engineers (IEEE)',,core
159637925,2018-06-07,"While the idea of a city built for people is gaining more and more acceptance today, many of our urban environments remain focused and built around the car. Currently, day-to-day life in the city means the frequent interaction between pedestrians and drivers, a situation which can be dangerous, or, at worst, deadly.
 
 
 This project, a collaboration between the Complex Systems group at IN3 (CoSIN3) of the Universitat Oberta de Catalunya (UOC), the Dirección General de Tráfico (DGT) and the Guàrdia Urbana de Barcelona, aims to quantify the issue of car-pedestrian collisions by characterising specific street areas with an indicator of pedestrian safety based on the structural properties of the street. Concretely, the project will generate this safety index for Spain’s two largest cities, Madrid and Barcelona, but the methodology and pipeline are applicable theoretically to any urban setting.
 
 
 As a base unit for measuring pedestrian safety over space, the total pedestrian area of the city (sidewalks and crossings) will be tessellated into small, regular segments. Each of these segments will be assigned various indicator values, from simple geometric properties (distance to the closest pedestrian crossing; width of sidewalk) to more complex measures such as driver visibility.
 
 
 Geometric operations to arrive at these values are performed on a PostGIS-build geo-database, over a variety of data, including street, sidewalk and block geometries, from diverse sources of open GIS data (Instituto Geográfico Nacional, Institut Cartogràfic i Geològic de Catalunya, OpenStreetMap, municipal data sources).Visibility values will be derived from a combination of deep learning technologies with GIS. A deep learning architecture will deliver computer-segmented street-scene images from Google Streetview. Each labeled image will be paired with an image from a simplified 3-dimensional model of the city, replicating its point of view (rendered with open-source 3D mapping software). The model will be clean of all street features (parked cars, trees, etc.) besides sidewalks and buildings. Comparison between the real and simplified images will thus permit the identification of sidewalk areas invisible to drivers due to visual obstructions.
 
 
 The results of the project will be presented as online “heatmap” visualisations of safety indexes for the focus cities, open to the public for browsing and research. Additionally, a purpose-built API (Application Programming Interface) will provide public and private organisations working in the area of traffic safety access to the results for integration in their own internal or public application",Espacio persona: Big data to make urban streets safer,,,Universitat de Girona. Servei de Sistemes d'Informació Geogràfica i Teledetecció,,core
212848228,2018-01-01T08:00:00,"Cyber-Physical Systems (CPSs) are combinations of physical processes and network computation. Modern CPSs such as smart buildings, power plants, transportation networks, and power-grids have shown tremendous potential for increased efficiency, robustness, and resilience. However, such modern CPSs encounter a large variety of physical faults and cyber anomalies, and in many cases are vulnerable to catastrophic fault propagation scenarios due to strong connectivity among their sub-systems. To address these issues, this study proposes a graphical modeling framework to monitor and predict the performance of CPSs in a scalable and robust way.
This thesis investigates on two critical CPS applications to evaluate the effectiveness of this proposed framework, namely (i) health monitoring of highway traffic sensors and (ii) building energy consumption prediction. In highway traffic sensor networks, accurate traffic sensor data is essential for traffic operation management systems and acquisition of real-time traffic surveillance data depends heavily on the reliability of the physical systems. Therefore, detecting the health status of the sensors in a traffic sensor network is critical for the departments of transportation as well as other public and private entities, especially in the circumstances where real-time decision making is required. With the purpose of efficiently determining the traffic network status and identifying failed sensor(s), this study proposes a cost-effective spatiotemporal graphical modeling approach called spatiotemporal pattern network (STPN). Traffic speed and volume measurement sensors are used in this work to formulate and analyze the proposed sensor health monitoring system. The historical time-series data from the networked traffic sensors on the Interstate 35 (I-35) within the state of Iowa is used for validation. Based on the validation results, this study demonstrates that the proposed graphical modeling approach can: (i) extract spatiotemporal dependencies among the different sensors which lead to an efficient graphical representation of the sensor network in the information space, and (ii) distinguish and quantify a sensor issue by leveraging the extracted spatiotemporal relationship of the candidate sensor(s) to the other sensors in the network.
In the building energy consumption prediction case, we consider the fact that energy performance of buildings is primarily affected by the heat exchange with the building outer skin and the surrounding environment. In addition, it is a common practice in building energy simulation (BES) to predict energy usage with a variable degree of accuracy. Therefore, to account for accurate building energy consumption, especially in urban environments with a lot of anthropogenic heat sources, it is necessary to consider the microclimate conditions around the building. These conditions are influenced by the immediate environment, such as surrounding buildings, hard surfaces, and trees. Moreover, deployment of sensors to monitor the microclimate information of a building can be quite challenging and therefore, not scalable. Instead of applying local weather data directly on building energy simulation (BES) tools, this work proposes a spatiotemporal pattern network (STPN) based machine learning framework to predict the microclimate information based on the local weather station, which leads to better energy consumption prediction in buildings",Spatiotemporal graphical modeling for cyber-physical systems,,https://core.ac.uk/download/212848228.pdf,Iowa State University Digital Repository,,core
163029625,2018-01-01T00:00:00,"The papers in this special section focus on the use of artificial intelligence (AI) for long term autonomy. Autonomous systems have a long history in the fields of AI and robotics. However, only through recent advances in technology has it been possible to create autonomous systems capable of operating in long-term, real-world scenarios. Examples include autonomous robots that operate outdoors on land, in air, water, and space; and indoors in offices, care homes, and factories. Designing, developing, and maintaining intelligent autonomous systems that operate in real-world environments over long periods of time, i.e. weeks, months, or years, poses many challenges. This special issue focuses on such challenges and on ways to overcome them using methods from AI. Long-term autonomy can be viewed as both a challenge and an opportunity. The challenge of long-term autonomy requires system designers to ensure that an autonomous system can continue operating successfully according to its real-world application demands in unstructured and semi-structured environments. This means addressing issues related to hardware and software robustness (e.g., gluing in screws and profiling for memory leaks), as well as ensuring that all modules and functions of the system can deal with the variation in the environment and tasks that is expected to occur over its operating time",Introduction to the Special Issue on AI for Long-Term Autonomy,10.1109/LRA.2018.2870466,https://core.ac.uk/download/163029625.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,core
217037123,2018-01-01T08:00:00,"Density estimation has wide applications in machine learning and data analysis techniques including clustering, classification, multimodality analysis, bump hunting and anomaly detection. In high-dimensional space, sparsity of data in local neighborhood makes many of parametric and nonparametric density estimation methods mostly inefficient.
This work presents development of computationally efficient algorithms for high-dimensional density estimation, based on Bayesian sequential partitioning (BSP). Copula transform is used to separate the estimation of marginal and joint densities, with the purpose of reducing the computational complexity and estimation error. Using this separation, a parallel implementation of the density estimation algorithm on a 4-core CPU is presented. Also, some example applications of the high-dimensional density estimation in density-based classification and clustering are presented.
Another challenge in the area of density estimation rises in dealing with online sources of data, where data is arriving over an open-ended and non-stationary stream. This calls for efficient algorithms for online density estimation. An online density estimator needs to be capable of providing up-to-date estimates of the density, bound to the available computing resources and requirements of the application. In response to this, BBSP method for online density estimation is introduced. It works based on collecting and processing the data in blocks of fixed size, followed by a weighted averaging over block-wise estimates of the density. Proper choice of block size is discussed via simulations for streams of synthetic and real datasets.
Further, with the purpose of efficiency improvement in offline and online density estimation, progressive update of the binary partitions in BBSP is proposed, which as simulation results show, leads into improved accuracy as well as speed-up, for various block sizes",Offline and Online Density Estimation for Large High-Dimensional Data,,https://core.ac.uk/download/217037123.pdf,Digital Commons @ Michigan Tech,,core
186676922,2018,"Розглядається метод інформаційно-екстремального машинного навчання системи функціонального діагностування технічного стану складної машини з оптимізацією ієрархічної структури вхідних даних. Показано, що на функціональну ефективність машинного навчання системи функціонального діагностування суттєво впливає розміщення в ієрархічній структурі класів розпізнавання, які характеризують
технічний стан машини та її вузлів. При цьому для кожної страти ієрархічної структури накладаються обмеження на кількість класів розпізнавання, що дозволяє зменшити ступінь їх перетину в просторі діагностичних ознак. Оптимізація ієрархічної структури здійснюється в процесі інформаційно-екстремального машинного навчання системи функціонального діагностування, що дозволяє максимізувати
інформаційну спроможність системи. Як критерій оптимізації параметрів машинного навчання розглядається модифікована інформаційна міра Кульбака, яка є функціоналом точнісних характеристик діагностичних рішень. При цьому алгоритм машинного навчання представляв собою багатоциклічну ітераційну процедуру пошуку максимального глобального значення інформаційного критерію оптимізації параметрів машинного навчання в робочій (допустимій) області визначення його функції. В результаті для страт всіх ярусів ієрархічної структури сформовано алфавіти класів розпізнавання, які забезпечили максимальну функціональну ефективність машинного навчання. За отриманими в процесі машинного навчання оптимальними геометричними параметрами контейнерів класів розпізнавання побудовано вирішальні правила, які дозволяють приймати діагностичні рішення в реальному темпі часу. Крім того, вирішальні правила, побудовані в рамках геометричного підходу, є практично інваріантними до багатовимірності вхідних даних, що є їх суттєвою перевагою перед штучними нейронними мережами. Як приклад реалізації запропонованого методу розглядалося машинне навчання системи функціонального діагностування шахтної підйомної машини з оптимізацією структури вхідних даних.The conclusions about the strata of society, various parties are supported by, have been made. The method of information-extreme machine learning of the system of functional diagnosis of the technical state of a complex machine with the optimization of the hierarchical data structure is considered. It is shown that the functional efficiency of machine learning of the system of functional diagnosis is significantly influenced by the location in the hierarchical structure of the recognition classes characterizing the technical state of the machine and its nodes. At the same time, for each level of the hierarchical structure under consideration, a restriction on the number of recognition classes is imposed, which makes it possible to reduce the degree of their intersection in the space of diagnostic features. Optimization of the hierarchical structure was carried out in the process of information-extreme machine learning of the system of functional diagnosis, which allows to maximize the information capacity of the system. As a criterion for optimizing the parameters of machine learning, we considered a modi fied information measure of Kulbak, which is a functional of the accurate characteristics of diagnostic solutions. In this case, the algorithm of machine learning represented a multi-cycle iterative procedure of finding the maximum global value of the information criterion for optimizing learning parameters in the working (permissible) domain of determining its function. Based on the optimal geometric parameters of recognition class containers obtained in the course of machine learning, decision rules have been constructed that allow making diagnostic decisions in a real time. As an example of the implementation of the method of optimization the structure of input data, the machine learning of the system for the functional diagnosis of a mine hoist was considered. As a result, alphabets of recognition classes have been created for strata of all tiers of the hierarchical structure, providing the maximum functional efficiency of machine learnin",Optimization of hierarchical data structure of intelligent system of functional diagnosis of technical condition of complex machines,,https://core.ac.uk/download/pdf/186676922.pdf,"НТУ ""ХПІ""",,core
211981078,2018-02-19T12:47:46,"Aside from intentional interference, multipath is the most significant error source for Global Navigation Satellite Systems (GNSS) receivers in many operational scenarios. In this thesis, we study the multipath estimation from two different perspectives: to retrieve useful information from it using GNSS-Reflectometry (GNSS-R) techniques; and to mitigate its effects or to estimate its direction-of-arrival (DOA) as well as the line-of-sight (LOS) signal¿s using synthetic aperture (SA) processing.

The first part of the thesis focuses on precision bounds for GNSS-R techniques for ground-based receivers, in scenarios where a single antenna simultaneously receives the LOS signal and a specular reflection. First, we derive the Cramér-Rao bound (CRB) of the receiver¿s height and the reflection coefficient, with the latter depending on the surface¿s electrical properties. More specifically, we propose a CRB derivation applicable to GNSS-R techniques that make use of the phase information and long observation times, such as the interference pattern technique (IPT). The derivation is based on the parameter transformation of the Fisher information matrix. We study the dependence of the computed CRB on the scenario and the receiver bandwidth. The CRB results for the simulated scenarios are consistent with the precision reported for many GNSS-R techniques used in these scenarios. The proposed CRB is meant to benchmark and compare new and existing techniques.

Besides the derived CRB, we propose an algorithm to obtain the maximum-likelihood (ML) estimator of the parameters of interest with the IPT: the segmented ML estimator (SML). The SML transforms a complex multivariate optimization problem into multiple simpler ones by dividing the parameter search space taking advantage of the cost function¿s particular structure. The SML is validated with simulated signal and asymptotically cross-validates the CRB results.

The second part of the thesis is devoted to the study of the SA processing of GNSS signals. The goal is to estimate the DOA of the signals received, and mitigate errors in the navigation solution caused by interfering signals, such as multipath. We start by deriving the CRB for the SA context, as a function of the antenna trajectory. This CRB considers the effect of the antenna complex gain, and we show in simulations that it is possible to achieve meaningful DOA estimation only by changing the antenna¿s orientation. We continue by proposing a development framework built upon a signal tracking architecture integrating SA processing. Before any SA processing, it is necessary to estimate and compensate any carrier phase contribution not related to the antenna motion. To do so, we propose two new sequential techniques based on the extended Kalman filter (EKF): EKF1 and EKF2. Also, we develop an open-loop version of the proposed SA tracking architecture, more robust than its closed-loop counterpart. Finally, we validate the proposed architecture and SA-based techniques with synthetic GPS signals at first, and then with real signals, recorded using an antenna mounted on a mechanical rotating arm. The obtained results validate the implemented techniques and show how the proposed SA architecture can ultimately mitigate the position bias error observed in environments with severe multipath interference",Parameter Estimation with GNSS-Reflectometry and GNSS Synthetic Aperture Techniques,10.5075/epfl-thesis-8350,,"Lausanne, EPFL",,core
185535082,2018-01-01T00:00:00,"Graphs are a natural choice to encode data in many real–world applications. In fact, a graph can describe a given pattern as a complex structure made up of parts (the nodes) and relationships between them (the edges). Despite their rich representational power, most of machine learning approaches cannot deal directly with inputs encoded by graphs. Indeed, Graph Neural Networks (GNNs) have been devised as an extension of recursive models, able to process general graphs, possibly undirected and cyclic. In particular, GNNs can be trained to approximate all the “practically useful” functions on the graph space, based on the classical inductive learning approach, realized within the supervised framework. However, the information encoded in the edges can actually be used in a more refined way, to switch from inductive to transductive learning. In this paper, we present an inductive–transductive learning scheme based on GNNs. The proposed approach is evaluated both on artificial and real–world datasets showing promising results. The recently released GNN software, based on the Tensorflow library, is made available for interested users",Inductive–Transductive Learning with Graph Neural Networks,10.1007/978-3-319-99978-4_16,,'Springer Science and Business Media LLC',,core
153723823,2018-03-29T02:35:15Z,"How can we use machine learning to provoke new conversations about gender? <br>This talk will discuss a project that questioned the role of technology and design within complex social issues like gender inequality. Drawing from feminist theory and linguistics algorithms, this project developed a research device that monitored spoken ‘gendered language’ within a space and revealed these patterns back to users in real time. This device was implemented in a variety of settings and assessed through the interviews and documentary film. In addition to serving as a heuristic learning tool to generate complex discussions about gender identity, this device explored what additional research value can be gained when turning quantitative data into a qualitative experience.<br",GenderTron: exploring implicit gender bias,10.4225/03/5abc50e48db39,,,,core
234970564,2018-04-11T00:00:00,"The Reporter is a publication produced by Western Carolina University featuring news, events, and campus community updates for faculty and staff. The publication began in August of 1970 and continues digitally today. Click on the link in the “Related Materials” field to access recent issues.The Reporter
News for the Faculty and Staff of Western Carolina University
June 24, 1998
Food Services to
Switch July 1
See you at Starbucks!
Cullowhee, North Carolina
Western's Learning Communities
Helping Freshmen Find a Niche
Jr Dining in takes on
a new dimension
July 1 at Western
with a changeover4
in food services. ARAMARK
Corp. has been selected to operate
the university's campus dining
services, including those at
Dodson and Brown cafeterias, the
food court areas at Hinds
University Center and Dodson,
concessions at the Ramsey
Regional Activity Center, and
catering services throughout
the campus.
Among the changes expected
under ARAMARKs management
are new menus and formats
providing a wide variety of student
dining options. These include addi­tional
nationally known franchises,
such as Starbucks Coffee and Little
Ceasars Pizza, as well as demon­stration
cooking areas where
customers can watch as their food
is prepared, and expansion of full
menu cafeteria-style service and
""grab-and-go"" items.
Clete Myers, operations
manager of food services at
Clemson University, will become
general manager of campus dining
services at Western. ARAMARK
hopes to retain personnel currently
employed in WClFs food service
when the management change
occurs.
Retention Services Director Susan Clarke Smith (far right) and Peer Mentor
Bryan Dodge (center), meet entering LC students.
""An institution's capacity to retain students is directly related to its ability to reach out and make
contact with students and integrate them into the social and intellectual fabric of institutional life.
It hinges on the establishment of a healthy, caring environment which enables individuals to find a
niche in the social and intellectual communities of the institution."" —VINCENT TINTO IN LEAVING COLLEGE
a ±JL good many of us came of age
when linking the notion of caring
with any institution would have been
greeted with skepticism—if not out­right
scorn. But Western, changing
with, and in many instances ahead of,
the times, will set the standard for
high tech and high touch this fall.
By embracing the idea of learning
communities as part of its freshman
year experience program, Western
makes use of a proven method for
meeting both the individual's need
to belong and the institution's need
to retain.
Randomly selected as participants
in the Learning Community (or LC
Pilot) project beginning this fall, some
170 of Western's estimated 1,200
first-year students will live together
and learn together in classes linked to
their common interests and conduct­ed
by a core group of instructors—all
in an effort to address the problems
presented by the transition from high
school to college. Meanwhile, the
institution will look closely to see if
the initiative addresses its own prob­lems
associated with an unusually
high rate of student attrition,
particularly after the first year.
Frank Prochaska, associate vice
chancellor for academic affairs and
chief architect of the LC Pilot project,
believes that establishing learning
communities at Western, even on a
limited basis initially, will deliver on
both counts. The numbers would
appear to bear him out, with
universities where LCs have become
a standard for first-year programs
reporting significantly higher GPAs
and retention rates among students
who participate.
Elizabeth Shelly coordinates the
Freshman Year Experience Program
through Student Affairs and is herself
a product of an undergraduate LC.
Shelly describes it as setting out in
college life with an ""instant group of
friends"" to go to at any time for
support. She explains that Western's
approach to the idea conforms to a
pretty standard model but incorporates
some new features made possible by
our unique environment.
The pilot group is divided into eight
communities of about twenty students
each, with several of the communities
grouped according to a declared major
or according to undergraduate college.
Three LCs are made up of freshmen
undecided upon majors. Each LC will
be housed in a suite-style layout in
Walker Hall with common areas set
aside for studying and group
activities. In addition to a resident
assistant on the floor, each commu­nity
will be assigned an upperclass
peer mentor who will live with the
group and work with the USI130
class designed for it.
A revamped USI course is key to
the LC Pilot, according to Prochaska.
The USI 130 course for the individual
communities will be co-taught by a
faculty member and a student affairs
professional, both trained in freshman
issues. Themes introduced for
discussion in this course will carry
over to the other General Education
courses linked for the purposes of each
community and taught by specially
selected instructors. For example,
Instructor Nory Prochaska's USI 130
section, and its linked English,
continued on page 2
Learning, continued
computer science, and math courses,
will examine the advantages and
disadvantages of the ""virtual
university"" idea, from the perspective
of LC students who have a particular
interest in technology and a close
comfort with using electronic media.
Susan Clarke Smith, director of
Retention Services, sees the linked
courses as an effective means of
addressing the ""intimidation factor,""
identified by most retention experts
such as Vincent Tinto as one of the
main reasons students leave college.
Smith asserts that by establishing
opportunities for interaction in and
out of the classroom, students and
faculty can begin to ""break down
barriers."" Smith says, ""Students can
see that their professors are human,
and faculty can feel less hesitation, as
part of a wider network of support, to
communicate concern on a more
personal level.""
To some extent, we need look no
farther than our own campus for an
example of a learning community
beginning to deliver on its promise.
Now entering its second year, The
Honors College combines an academic
emphasis with a residential and social
component. Brian Railsback, acting
dean of the college, calculates a 92
percent fall-spring retention rate for
honors freshmen for 1997-98. (The
university's rate for freshmen was
83 percent.) ""That figure,"" Railsback
says, ""obviously points to a good bond
and a real desire for these students to
stay together.""
Perhaps the most ambitious and
the most advantageous aspects of
initiating a learning community
program now come to a confluence
with the computer implementation
project, also spearheaded by
Prochaska. Student Affairs Vice
Chancellor Robert Caruso sees the
efforts as quite extraordinary. ""The
added component of new technologies
in the classroom and the residence
halls will revolutionize our whole
notion of community,"" Caruso says.
""With something on the order of only
forty higher education institutions
out of about 2,800 nationwide that
have a computer requirement, I think
everyone is going to be looking to
Western as a model for creating the
learning community of the twenty-first
century.""
Michael Dougherty
Named Dean of
Education, Allied
Professions
Carolina
Board of
Governors.
Associate
dean of the
College of
Education
and Allied
Professions
since 1996, Dougherty is a professor
in the Department of Human
Services and is a former head of the
department. He earned his bachelor's
degree from the University of Notre
Dame in 1968, master's degrees from
Oakland University in 1970 and
Notre Dame in 1971, and doctoral
degree from Indiana State University
in 1974. He has been a member of the
human services faculty since 1976.
The 1988 recipient of Western's
Paul A. Reid Distinguished Service
Award for Faculty, he also has been
nominated for the Chancellor's
Distinguished Teaching Award and
the Taft Botner Award for Superior
Teaching. Prior to coming to Western,
he was a teacher and counselor in
public schools in Detroit; Mattoon, 111.;
and Taylor County, Fla.
Dougherty is a member of several
professional organizations, including
the American Counseling Association
and the Association for Educational
and Psychological Consultation. His
research activities have focused on
study skills and locus of control, the
effects of counseling techniques on
incarcerates, and consultation styles.
Dougherty's appointment will be
effective July 1, upon the retirement
of Chambers, who served seventeen
years as the college's dean.
Highlighting the summer's cultural
events on campus are this weekend's
concerts by the Atlanta Ballet and the
Cassatt String Quartet.
Hailed as one of the nation's
outstanding young ensembles, the
Cassatt String Quartet will perform in
recital at 8 p.m. this Friday and will
also perform as part of the Atlanta
Ballet programs at 2 p.m. and 8 p.m.
on Saturday. All performances are in
Hoey Auditorium.
The oldest continually operating
ballet company in the United States,
the acclaimed Atlanta Ballet travels to
the mountains each year for a summer
residency. The two performances on
Saturday come in conjunction with
Western's hosting the second annual
Atlanta Ballet Centre for Dance Educa­tion
summer dance camp. Selected
participants in the June 14-July 4
camp, which attracts more than fifty of
the top ballet students in the Southeast,
will share the stage with the
professional dancers for one piece.
The ballet programs on Saturday
include ""Intermezzo,"" featuring three
couples in an intricate series of
dances set to music by Johannes
Brahms; ""Prisma,"" featuring music
by Charles Ives and Atlanta Ballet
executive/music director Robert
Chumbley performed by the Cassatt
String Quartet; and ""II Distrato,"" an
abstract ballet in five movements
demonstrating how the different
parts of the dancer's body work
separately and as a unit.
The Cassatt String Quartet's
Friday program features music for
strings by Beethoven and Ravel.
Admission for the quartet's recital
is $10 for adults, $5 for WCU
students and children. Admission for
the ballet's performances is $20 for
adults and $5 for WCU students and
children. For tickets, call 227-7397.
Architects Tapped for Construction Projects
A pair of major construction projects planned for Western moved closer to reality
recently with the board of trustees' selection of two Charlotte architecture firms
to design an expansion of the university center and a federally funded workforce
development facility.
The trustees named Lee Nichols Hepler Architecture of Charlotte to design
the expansion of the Hinds University Center and Jenkins-Peer Architects of
Charlotte to design the proposed new Western North Carolina laborforce high-technology
education and training center.
The Hinds University Center project is the second in a three-phased
expansion effort designed to enhance the quality of student life at WCU. Plans
call for the construction of approximately 31,000 additional square feet to add a
retail shopping area, a movie theater, increased meeting and office space for
student organizations, and a multicultural center. Preliminary estimates for the
expansion set the cost at about $4.5 million.
The regional high-tech workforce training center, announced last fall by U.S.
Rep. Charles Taylor, is designed to help raise the economic potential of the
region by improving the availability of high-technology education to the
mountains' workforce. The center could include such high-tech training tools as
an industrial laser lab, artificial intelligence lab, geographic information lab,
robotics training, and sound and video production facilities complete with digital
editing capabilities. The facility, pending funding from Congress and the federal
Economic Development Administration, is expected to be built adjacent to the
Belk Building.
Michael Dougherty, associate dean of
the College of Education and Allied
Professions, has been named by
WClFs board of trustees to succeed
Gurney E. Chambers as dean.
Appointment of Dougherty to the
dean's post was approved by the
board Wednesday, June 10, at its
quarterly meeting. The appointment
is subject to the approval of the
University
I of North
WCU Campus
Plays Host to
Weekend of CI
Music and
June 24,1998 • T he Reporter • p age 2
University Awards Top Honors for Teaching, Research, and Service
Western's top faculty and staff
awards for teaching, research, and
service for the 1997-98 academic
year were presented at the annual
spring General Faculty and Awards
Convocation in May.
Mary C. ""Katie"" Ray, assistant
professor of elementary and middle
grades education, won the
Chancellor's Distinguished Teaching
Award. The Paul A. Reid Distin­guished
Service Award for faculty
went to Gordon Mercer, professor
of political science and public
affairs, and the Paul A. Reid
Distinguished Service Award for
administrative staff went to
Stephen White, sports information
director. David J. Butcher,
associate professor of chemistry,
received the University Scholar
Award for distinguished scholarly
achievement.
The honors, presented by
Chancellor John Bardo, carry $1,000
cash awards and engraved plaques
for each recipient. Bardo also
presented the Academic Award of
Excellence to the Department of
English. The award provides
$10,000 for program and staff
development.
Chancellor's Distinguished
Teaching Award
Katie Ray joined WCU's faculty
in 1994 after seven years of teaching
in elementary and middle schools in
New York City. In presenting the
award, Bardo quoted from Ray's
comments to the awards selection
committee.
""For students to be great
teachers, they must be passionate
about living and learning,"" Ray said.
""Consequently, I must show
students by my own passion. The
challenge I face every day is not to
know about good teaching, but to
demonstrate it in every interaction I
have with my students.""
Paul A. Reid
Distinguished Service
Award—Faculty
Gordon Mercer has been
a member of the WCU
faculty since 1980. Among
his accomplishments are the
creation of the annual
Undergraduate Research
| Conference and the
organization of faculty
forum assemblies to foster
communication about
athletic director. He received twenty-six
publication awards from the
College Sports Information Directors of
America, and eight Football Writers
Association of America awards for
""Outstanding Press Box Service.""
Following his retirement on June 30,
integrate computers and new
technology with writing and research
in a way that reflects ""real-world""
practices. The department has become
a primary user of the electronic
classrooms, and during the 1997-98
academic year, every freshman
years of continuous service leave
from usual work commitments to
pursue concentrated scholarly work.
Recipients are chosen on a competi­tive
basis by a faculty committee.
important campus issues.
He has held leadership
positions in WCU's Faculty
Senate and the University of
North Carolina's Faculty Assembly.
Mercer also has been described as ""a
superior teacher"" by his students and
has been nominated frequently for
campus teaching awards.
Paul A. Reid Distinguished Service
Award—Staff
Steve White will retire at the end of
June as WCU's sports information
director. A 1967 graduate of WCU,
White has also served as associate
Distinguished Teacher Katie Ray, University Scholar David Butcher, and Reid Service honoree
Gordon Mercer receive their awards from Chancellor John Bardo.
White will head the Catamount Sports
Network, Inc., the radio broadcaster of
WCU's football and basketball games.
University Scholar Award
David Butcher joined WCU's faculty
in 1990. An analytical chemist, he has
received several external grants and
has published numerous articles on his
scientific research, which includes the
atomic absorption spectroscopy, atomic
fluorescence spectrometry, and the
search for potential
chemical causes for
Sports Information Director Steve White receives the Reid
Service Award for Staff from Chancellor John Bardo.
of Excellence, Bardo
praised the
Department of
English for its
innovative work to
studied composition in the electronic
environment.
Other major awards recognized at
the convocation were the Beyond the
Classroom Teaching Award and the
Scholarly Development Assignment
Program Awards.
The Beyond the Classroom
Teaching Award is given to an
academic teaching unit that excels
in enhancing students' learning
through such activities outside the
classroom as mentoring programs,
effective academic advising or
cooperative learning experience. The
1998 winner of the award, which is
funded by the UNC Board of
Governors, is the Department of
Health Sciences.
Recipients in the Scholarly
Development Assignment
Program are Richard Boyer
(English); Barbara Lovin (head,
Health Sciences); and Dan Pittillo
(Biology). The Scholarly Develop­ment
Assignment Program awards
provide full-time tenured faculty
members who have a minimum of six
the decline of Fraser
fir trees in the
Southern Appala­chian
Mountains.
Academic Award
of Exc ellence
In presenting
the Academic Award
June 24,1998 • The Reporter • page 3
Bruce Henderson Receives UNC System Teaching Award
Bruce B. Henderson, professor
of psychology, was among
sixteen recipients of the fourth
annual Awards for Excellence
in Teaching, presented by the
UNC Board of Governors.
Henderson accepted the award
at a special academic convoca­tion
held at N.C. Central
University in conjunction with
the inauguration of Molly
Corbett Broad as president of the
University of North Carolina.
Winners from
each campus
received a
bronze medallion
and a $7,500
cash prize.
Recipients were
nominated by
special commit­tees
from each of
the sixteen UNC
campuses and selected by the Board
of Governors Committee on Teach­ing
Awards.
Established by the Board of
Governors in 1994 to underscore
the importance of teaching and to
reward good teaching across the
university system, the awards are
given annually to a tenured faculty
member from each UNC campus.
Winners must have taught at their
present institutions at least seven
years, and no one may receive the
award more than once.
Henderson, on WCU's faculty
since 1978 and former head of the
psychology department, received
Western's Botner Superior
Teaching Award in 1988. He co-edited
the book Curiosity and
Exploration, focusing on how
intrinsic rewards affect behavior
in children. Henderson received
his bachelor's and master's degrees
from Bucknell University and
his doctorate from the University
of Minnesota.
WCU Colleges Present Awards
Awards for teaching, service, and scholarship were presented on a college-level
at the end of spring semester. The following is a listing of award
winners by college:
College of Arts and Sciences
• Curtis Wood (History) received
the Creighton Sossomon Professor­ship
for outstanding teacher-scholars
in American, English or European
history. Appointment to the
professorship is for a three-year term.
• Richard Bruce (Biology and
director, Highlands Biological
Station) received the H.F. and
Katherine P. Robinson Professorship.
• Robert Holquist (Music and
director of choral activities) received
the James Dooley Excellence in
Music Teaching Award, which carries
a $500 cash stipend.
• Betty Farmer (Communication
and Theatre Arts) received the Board
of Governors College of Arts and
Sciences Teaching Award, which
carries a $1,000 prize.
• Faculty members in the College of
Arts and Sciences also presented
acting dean J.C. Alexander Jr. with
a ""lifetime achievement award"" in
appreciation for service as acting
dean of the college.
The College of Business
• Roger Lirely (Accounting and
Information Systems) received the
Jay I. Kneedler Professor of
Excellence Award, which includes a
$1,000 cash prize and a plaque.
• Board of Governors Creative and
Innovative Teaching Awards went to
Julie Johnson (Business Adminis­tration,
Law and Marketing); Susan
Kask (Economics); Reagan
McLaurin (Business Administration,
Law and Marketing); and Max
Schreiber (Economics, Finance and
International Business). Each award
carries a $250 stipend.
The College of Education and
Allied Professions
• Carol Burton (director, Teaching
Fellows) received the annual Taft B.
Botner Award for Superior Teaching,
which includes a $750 cash prize and
a plaque.
• Board of Governors Awards for
Superior Teaching and $250 stipends
went to Barbara Bell (Elementary
and Middle Grades Education and
director, Reading Center); Cindy
Cavanaugh (Health and Human
Performance); Richard Haynes
(Administration, Curriculum and
Instruction and director of field
experiences and teacher education);
and Hedy White (Psychology).
The College of Applied Sciences
• The Board of Governors Innovation
in Teaching Award, which carries a
$1,000 stipend, went to Walter
Floreani (Health Sciences).
Trustees Approve Appointments and
Campus Name Changes
WCU's board of trustees approved a number of administrative appoint­ments
for the coming year at its quarterly meeting June 10.
• Terry L. Ballman, assistant professor of Hispanic studies at the
University of Northern Colorado, as associate professor and head of the
Department of Modern Foreign Languages.
• Paul F. Brandt as head of the Department of Chemistry and Physics.
• James A. Lewis as head of the Department of History.
• Carol C. Stephens as director of the Master of Science in Nursing ~
Program.
• Paul Wright as head of the Department of Biology.
• Kathleen S. Wright as head of the Department of Communication
and Theatre Arts.
• Jerry L. Kinard to continue as head of the Department of Manage­ment
through spring 1999.
• John A. Wade III to continue as head of the Department of Econom­ics,
Finance and International Business through spring 2000.
The trustees also approved several administrative and departmental
changes within the College of Business. The department ","The Reporter, June 1998",,,"Hunter Library Digital Collections, Western Carolina University, Cullowhee, NC 28723;",,core
322490272,2018-09-17T00:00:00,"© 2018 The Author(s).This paper presents a methodology to monitor the fatigue life of aerospace structures and hence the remaining allowable fatigue life. In fatigue clearance, conservative load assumptions are made. However, in reality, a structure may see much lower loads and so would be usable for much longer. An example ofthis is air carried guided missiles. In the UK, missiles must be decommissioned after a period of carriage. The implementation of a system that can monitor the usage of a missile during its time in service is advantageous to the military customer and provides a competitive advantage for the missile manufacture inexport markets where reduced through-life costs, longer in-service lives and increased safety are desired. The proposed methodology provides a means to monitor the service life of a missile. This paper describes how machine learning algorithms can be used with accelerometers to determine loads on a missile structure which would then be used to predict how long the missile has left in service",A methodology using health and usage monitoring system data for payload life prediction,,https://core.ac.uk/download/322490272.pdf,"KU Leuven, Faculty of Arts",,core
275647974,2018-01-01T00:00:00,"[EN] Affective Computing has emerged as an important field of study that aims to develop systems that can automatically recognize emotions. Up to the present, elicitation has been carried out with nonimmersive stimuli. This study, on the other hand, aims to develop an emotion recognition system for affective states evoked through Immersive Virtual Environments. Four alternative virtual rooms were designed to elicit four possible arousal-valence combinations, as described in each quadrant of the Circumplex Model of Affects. An experiment involving the recording of the electroencephalography (EEG) and electrocardiography (ECG) of sixty participants was carried out. A set of features was extracted from these signals using various state-of-the-art metrics that quantify brain and cardiovascular linear and nonlinear dynamics, which were input into a Support Vector Machine classifier to predict the subject's arousal and valence perception. The model's accuracy was 75.00% along the arousal dimension and 71.21% along the valence dimension. Our findings validate the use of Immersive Virtual Environments to elicit and automatically recognize different emotional states from neural and cardiac dynamics; this development could have novel applications in fields as diverse as Architecture, Health, Education and Videogames.This work was supported by the Ministerio de Economia y Competitividad. Spain (Project TIN2013-45736-R).Marín-Morales, J.; Higuera-Trujillo, JL.; Greco, A.; Guixeres Provinciale, J.; Llinares Millán, MDC.; Scilingo, EP.; Alcañiz Raya, ML.... (2018). Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors. Scientific Reports. 8:1-15. https://doi.org/10.1038/s41598-018-32063-4S1158Picard, R. W. Affective computing. (MIT press, 1997).Picard, R. W. Affective Computing: Challenges. Int. J. Hum. Comput. Stud. 59, 55–64 (2003).Jerritta, S., Murugappan, M., Nagarajan, R. & Wan, K. Physiological signals based human emotion Recognition: a review. Signal Process. its Appl. (CSPA), 2011 IEEE 7th Int. Colloq. 410–415, https://doi.org/10.1109/CSPA.2011.5759912 (2011).Harms, M. B., Martin, A. & Wallace, G. L. Facial emotion recognition in autism spectrum disorders: A review of behavioral and neuroimaging studies. Neuropsychol. Rev. 20, 290–322 (2010).Koolagudi, S. G. & Rao, K. S. Emotion recognition from speech: A review. Int. J. Speech Technol. 15, 99–117 (2012).Gross, J. J. & Levenson, R. W. Emotion elicitation using films. Cogn. Emot. 9, 87–108 (1995).Lindal, P. J. & Hartig, T. Architectural variation, building height, and the restorative quality of urban residential streetscapes. J. Environ. Psychol. 33, 26–36 (2013).Ulrich, R. View through a window may influence recovery from surgery. Science (80-.). 224, 420–421 (1984).Fernández-Caballero, A. et al. Smart environment architecture for emotion detection and regulation. J. Biomed. Inform. 64, 55–73 (2016).Ekman, P. Basic Emotions. Handbook of cognition and emotion 45–60, https://doi.org/10.1017/S0140525X0800349X (1999).Posner, J., Russell, J. A. & Peterson, B. S. The circumplex model of affect: an integrative approach to affective neuroscience, cognitive development, and psychopathology. Dev. Psychopathol. 17, 715–34 (2005).Russell, J. A. & Mehrabian, A. Evidence for a three-factor theory of emotions. J. Res. Pers. 11, 273–294 (1977).Calvo, R. A. & D’Mello, S. Affect detection: An interdisciplinary review of models, methods, and their applications. IEEE Trans. Affect. Comput. 1, 18–37 (2010).Valenza, G. et al. Combining electroencephalographic activity and instantaneous heart rate for assessing brain–heart dynamics during visual emotional elicitation in healthy subjects. Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 374, 20150176 (2016).Valenza, G., Lanata, A. & Scilingo, E. P. The role of nonlinear dynamics in affective valence and arousal recognition. IEEE Trans. Affect. Comput. 3, 237–249 (2012).Valenza, G., Citi, L., Lanatá, A., Scilingo, E. P. & Barbieri, R. Revealing real-time emotional responses: a personalized assessment based on heartbeat dynamics. Sci. Rep. 4, 4998 (2014).Valenza, G. et al. Wearable monitoring for mood recognition in bipolar disorder based on history-dependent long-term heart rate variability analysis. IEEE J. Biomed. Heal. Informatics 18, 1625–1635 (2014).Piwek, L., Ellis, D. A., Andrews, S. & Joinson, A. The Rise of Consumer Health Wearables: Promises and Barriers. PLoS Med. 13, 1–9 (2016).Xu, J., Mitra, S., Van Hoof, C., Yazicioglu, R. & Makinwa, K. A. A. Active Electrodes for Wearable EEG Acquisition: Review and Electronics Design Methodology. IEEE Rev. Biomed. Eng. 3333, 1–1 (2017).Kumari, P., Mathew, L. & Syal, P. Increasing trend of wearables and multimodal interface for human activity monitoring: A review. Biosens. Bioelectron. 90, 298–307 (2017).He, C., Yao, Y. & Ye, X. An Emotion Recognition System Based on Physiological Signals Obtained by Wearable Sensors. In Wearable Sensors and Robots: Proceedings of International Conference on Wearable Sensors and Robots 2015 (eds Yang, C., Virk, G. S. & Yang, H.) 15–25. https://doi.org/10.1007/978-981-10-2404-7_2 (Springer Singapore, 2017).Nakisa, B., Rastgoo, M. N., Tjondronegoro, D. & Chandran, V. Evolutionary computation algorithms for feature selection of EEG-based emotion recognition using mobile sensors. Expert Syst. Appl. 93, 143–155 (2018).Kory Jacqueline, D. & Sidney, K. Affect Elicitation for Affective Computing. In The Oxford Handbook of Affective Computing 371–383 (2014).Ekman, P. The directed facial action task. In Handbook of emotion elicitation and assessment 47–53 (2007).Harmon-Jones, E., Amodio, D. M. & Zinner, L. R. Social psychological methods of emotion elicitation. Handb. Emot. elicitation Assess. 91–105, https://doi.org/10.2224/sbp.2007.35.7.863 (2007)Roberts, N. A., Tsai, J. L. & Coan, J. A. Emotion elicitation using dyadic interaction task. Handbook of Emotion Elicitation and Assessment 106–123 (2007).Nardelli, M., Valenza, G., Greco, A., Lanata, A. & Scilingo, E. P. Recognizing emotions induced by affective sounds through heart rate variability. IEEE Trans. Affect. Comput. 6, 385–394 (2015).Kim, J. Emotion Recognition Using Speech and Physiological Changes. Robust Speech Recognit. Underst. 265–280 (2007).Soleymani, M., Pantic, M. & Pun, T. Multimodal emotion recognition in response to videos (Extended abstract). 2015 Int. Conf. Affect. Comput. Intell. Interact. ACII 2015 3, 491–497 (2015).Baños, R. M. et al. Immersion and Emotion: Their Impact on the Sense of Presence. CyberPsychology Behav. 7, 734–741 (2004).Giglioli, I. A. C., Pravettoni, G., Martín, D. L. S., Parra, E. & Raya, M. A. A novel integrating virtual reality approach for the assessment of the attachment behavioral system. Front. Psychol. 8, 1–7 (2017).Marín-Morales, J., Torrecilla, C., Guixeres, J. & Llinares, C. Methodological bases for a new platform for the measurement of human behaviour in virtual environments. DYNA 92, 34–38 (2017).Vince, J. Introduction to virtual reality. (Media, Springer Science & Business, 2004).Alcañiz, M., Baños, R., Botella, C. & Rey, B. The EMMA Project: Emotions as a Determinant of Presence. PsychNology J. 1, 141–150 (2003).Vecchiato, G. et al. Neurophysiological correlates of embodiment and motivational factors during the perception of virtual architectural environments. Cogn. Process. 16, 425–429 (2015).Slater, M. & Wilbur, S. A Framework for Immersive Virtual Environments (FIVE): Speculations on the Role of Presence in Virtual Environments. Presence Teleoperators Virtual Environ. 6, 603–616 (1997).Riva, G. et al. Affective Interactions Using Virtual Reality: The Link between Presence and Emotions. CyberPsychology Behav. 10, 45–56 (2007).Baños, R. M. et al Changing induced moods via virtual reality. In International Conference on Persuasive Technology (ed. Springer, Berlin, H.) 7–15, https://doi.org/10.1007/11755494_3 (2006).Baños, R. M. et al. Positive mood induction procedures for virtual environments designed for elderly people. Interact. Comput. 24, 131–138 (2012).Gorini, A. et al. Emotional Response to Virtual Reality Exposure across Different Cultures: The Role of the AttributionProcess. CyberPsychology Behav. 12, 699–705 (2009).Gorini, A., Capideville, C. S., De Leo, G., Mantovani, F. & Riva, G. The Role of Immersion and Narrative in Mediated Presence: The Virtual Hospital Experience. Cyberpsychology, Behav. Soc. Netw. 14, 99–105 (2011).Chirico, A. et al. Effectiveness of Immersive Videos in Inducing Awe: An Experimental Study. Sci. Rep. 7, 1–11 (2017).Blascovich, J. et al. Immersive Virtual Environment Technology as a Methodological Tool for Social Psychology. Psychol. Inq. 7965, 103–124 (2012).Peperkorn, H. M., Alpers, G. W. & Mühlberger, A. Triggers of fear: Perceptual cues versus conceptual information in spider phobia. J. Clin. Psychol. 70, 704–714 (2014).McCall, C., Hildebrandt, L. K., Bornemann, B. & Singer, T. Physiophenomenology in retrospect: Memory reliably reflects physiological arousal during a prior threatening experience. Conscious. Cogn. 38, 60–70 (2015).Hildebrandt, L. K., Mccall, C., Engen, H. G. & Singer, T. Cognitive flexibility, heart rate variability, and resilience predict fine-grained regulation of arousal during prolonged threat. Psychophysiology 53, 880–890 (2016).Notzon, S. et al. Psychophysiological effects of an iTBS modulated virtual reality challenge including participants with spider phobia. Biol. Psychol. 112, 66–76 (2015).Amaral, C. P., Simões, M. A., Mouga, S., Andrade, J. & Castelo-Branco, M. A novel Brain Computer Interface for classification of social joint attention in autism and comparison of 3 experimental setups: A feasibility study. J. Neurosci. Methods 290, 105–115 (2017).Eudave, L. & Valencia, M. Physiological response while driving in an immersive virtual environment. 2017 IEEE 14th Int. Conf. Wearable Implant. Body Sens. Networks 145–148, https://doi.org/10.1109/BSN.2017.7936028 (2017).Sharma, G. et al. Influence of landmarks on wayfinding and brain connectivity in immersive virtual reality environment. Front. Psychol. 8, 1–12 (2017).Bian, Y. et al. A framework for physiological indicators of flow in VR games: construction and preliminary evaluation. Pers. Ubiquitous Comput. 20, 821–832 (2016).Egan, D. et al. An evaluation of Heart Rate and Electrodermal Activity as an Objective QoE Evaluation method for Immersive Virtual Reality Environments. 3–8, https://doi.org/10.1109/QoMEX.2016.7498964 (2016).Meehan, M., Razzaque, S., Insko, B., Whitton, M. & Brooks, F. P. Review of four studies on the use of physiological reaction as a measure of presence in stressful virtual environments. Appl. Psychophysiol. Biofeedback 30, 239–258 (2005).Higuera-Trujillo, J. L., López-Tarruella Maldonado, J. & Llinares Millán, C. Psychological and physiological human responses to simulated and real environments: A comparison between Photographs, 360° Panoramas, and Virtual Reality. Appl. Ergon. 65, 398–409 (2016).Felnhofer, A. et al. Is virtual reality emotionally arousing? Investigating five emotion inducing virtual park scenarios. Int. J. Hum. Comput. Stud. 82, 48–56 (2015).Anderson, A. P. et al. Relaxation with Immersive Natural Scenes Presented Using Virtual Reality. Aerosp. Med. Hum. Perform. 88, 520–526 (2017).Higuera, J. L. et al. Emotional cartography in design: A novel technique to represent emotional states altered by spaces. In D and E 2016: 10th International Conference on Design and Emotion 561–566 (2016).Kroenke, K., Spitzer, R. L. & Williams, J. B. W. The PHQ-9: Validity of a brief depression severity measure. J. Gen. Intern. Med. 16, 606–613 (2001).Bradley, M. M. & Lang, P. J. Measuring emotion: The self-assessment manikin and the semantic differential. J. Behav. Ther. Exp. Psychiatry 25, 49–59 (1994).Lang, P. J., Bradley, M. M. & Cuthbert, B. N. International Affective Picture System (IAPS): Technical Manual and Affective Ratings. NIMH Cent. Study Emot. Atten. 39–58, https://doi.org/10.1027/0269-8803/a000147 (1997).Nanda, U., Pati, D., Ghamari, H. & Bajema, R. Lessons from neuroscience: form follows function, emotions follow form. Intell. Build. Int. 5, 61–78 (2013).Russell, J. A. A circumplex model of affect. J. Pers. Soc. Psychol. 39, 1161–1178 (1980).Sejima, K. Kazuyo Sejima. 1988–1996. El Croquis 15 (1996).Ochiai, H. et al. Physiological and Psychological Effects of Forest Therapy on Middle-Aged Males with High-NormalBlood Pressure. Int. J. Environ. Res. Public Health 12, 2532–2542 (2015).Noguchi, H. & Sakaguchi, T. Effect of illuminance and color temperature on lowering of physiological activity. Appl. Hum. Sci. 18, 117–123 (1999).Küller, R., Mikellides, B. & Janssens, J. Color, arousal, and performance—A comparison of three experiments. Color Res. Appl. 34, 141–152 (2009).Yildirim, K., Hidayetoglu, M. L. & Capanoglu, A. Effects of interior colors on mood and preference: comparisons of two living rooms. Percept. Mot. Skills 112, 509–524 (2011).Hogg, J., Goodman, S., Porter, T., Mikellides, B. & Preddy, D. E. Dimensions and determinants of judgements of colour samples and a simulated interior space by architects and non‐architects. Br. J. Psychol. 70, 231–242 (1979).Jalil, N. A., Yunus, R. M. & Said, N. S. Environmental Colour Impact upon Human Behaviour: A Review. Procedia - Soc. Behav. Sci. 35, 54–62 (2012).Jacobs, K. W. & Hustmyer, F. E. Effects of four psychological primary colors on GSR, heart rate and respiration rate. Percept. Mot. Skills 38, 763–766 (1974).Jin, H. R., Yu, M., Kim, D. W., Kim, N. G. & Chung, A. S. W. Study on Physiological Responses to Color Stimulation. In International Association of Societies of Design Research (ed. Poggenpohl, S.) 1969–1979 (Korean Society of Design Science, 2009).Vartanian, O. et al. Impact of contour on aesthetic judgments and approach-avoidance decisions in architecture. Proc. Natl. Acad. Sci. 110, 1–8 (2013).Tsunetsugu, Y., Miyazaki, Y. & Sato, H. Visual effects of interior design in actual-size living rooms on physiological responses. Build. Environ. 40, 1341–1346 (2005).Stamps, A. E. Physical Determinants of Preferences for Residential Facades. Environ. Behav. 31, 723–751 (1999).Berlyne, D. E. Novelty, Complexity, and Hedonic Value. Percept. Psychophys. 8, 279–286 (1970).Krueger, R. A. & Casey, M. Focus groups: a practical guide for applied research. (Sage Publications, 2000).Acharya, U. R., Joseph, K. P., Kannathal, N., Lim, C. M. & Suri, J. S. Heart rate variability: A review. Med. Biol. Eng. Comput. 44, 1031–1051 (2006).Tarvainen, M. P., Niskanen, J. P., Lipponen, J. A., Ranta-aho, P. O. & Karjalainen, P. A. Kubios HRV - Heart rate variability analysis software. Comput. Methods Programs Biomed. 113, 210–220 (2014).Pan, J. & Tompkins, W. J. A real-time QRS detection algorithm. Biomed. Eng. IEEE Trans. 1, 230–236 (1985).Tarvainen, M. P., Ranta-aho, P. O. & Karjalainen, P. A. An advanced detrending method with application to HRV analysis. IEEE Trans. Biomed. Eng. 49, 172–175 (2002).Valenza, G. et al. Predicting Mood Changes in Bipolar Disorder Through HeartbeatNonlinear Dynamics. IEEE J. Biomed. Heal. Informatics 20, 1034–1043 (2016).Pincus, S. & Viscarello, R. Approximate Entropy A regularity measure for fetal heart rate analysis. Obstet. Gynecol. 79, 249–255 (1992).Richman, J. & Moorman, J. Physiological time-series analysis using approximate entropy and sample entropy. Am J Physiol Hear. Circ Physiol 278, H2039–H2049 (2000).Peng, C.-K., Havlin, S., Stanley, H. E. & Goldberger, A. L. Quantification of scaling exponents and crossover phenomena in nonstationary heartbeat time series. Chaos 5, 82–87 (1995).Grassberger, P. & Procaccia, I. Characterization of strange attractors. Phys. Rev. Lett. 50, 346–349 (1983).Delorme, A. & Makeig, S. EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis. J. Neurosci. Methods 134, 9–21 (2004).Colomer Granero, A. et al. A Comparison of Physiological Signal Analysis Techniques and Classifiers for Automatic Emotional Evaluation of Audiovisual Contents. Front. Comput. Neurosci. 10, 1–14 (2016).Kober, S. E., Kurzmann, J. & Neuper, C. Cortical correlate of spatial presence in 2D and 3D interactive virtual reality: An EEG study. Int. J. Psychophysiol. 83, 365–374 (2012).Hyvärinen, A. & Oja, E. Independent component analysis: Algorithms and applications. Neural Networks 13, 411–430 (2000).Welch, P. D. The Use of Fast Fourier Transform for the Estimation of Power Spectra: A Method Based on Time Aver. aging Over Short, Modified Periodograms. IEEE Trans. AUDIO Electroacoust. 15, 70–73 (1967).Mormann, F., Lehnertz, K., David, P. & Elger, E. C. Mean phase coherence as a measure for phase synchronization and its application to the EEG of epilepsy patients. Phys. D Nonlinear Phenom. 144, 358–369 (2000).Jolliffe, I. T. Principal Component Analysis, Second Edition. Encycl. Stat. Behav. Sci. 30, 487 (2002).Schöllkopf, B., Smola, A. J., Williamson, R. C. & Bartlett, P. L. New support vector algorithms. Neural Comput 12, 1207–1245 (2000).Yan, K. & Zhang, D. Feature selection and analysis on correlated gas sensor data with recursive feature elimination. Sensors Actuators, B Chem. 212, 353–363 (2015).Chang, C.-C. & Lin, C.-J. Libsvm: A Library for Support Vector Machines. ACM Trans. Intell. Syst. Technol. 2, 1–27 (2011).Lewis, P. A., Critchley, H. D., Rotshtein, P. & Dolan, R. J. Neural correlates of processing valence and arousal in affective words. Cereb. Cortex 17, 742–748 (2007).McCall, C., Hildebrandt, L. K., Hartmann, R., Baczkowski, B. M. & Singer, T. Introducing the Wunderkammer as a tool for emotion research: Unconstrained gaze and movement patterns in three emotionally evocative virtual worlds. Comput. Human Behav. 59, 93–107 (2016).Blake, J. & Gurocak, H. B. Haptic glove with MR brakes for virtual reality. IEEE/ASME Trans. Mechatronics 14, 606–615 (2009).Heydarian, A. et al. Immersive virtual environments versus physical built environments: A benchmarking study for building design and user-built environment explorations. Autom. Constr. 54, 116–126 (2015).Kuliga, S. F., Thrash, T., Dalton, R. C. & Hölscher, C. Virtual reality as an empirical research tool - Exploring user experience in a real building and a corresponding virtual model. Comput. Environ. Urban Syst. 54, 363–375 (2015).Yeom, D., Choi, J.-H. & Zhu, Y. Investigation of the Physiological Differences between Immersive Virtual Environment and Indoor Enviorment in a Building. Indoor adn Built Enviornment 0, Accept (2017).Combrisson, E. & Jerbi, K. Exceeding chance level by chance: The caveat of theoretical chance levels in brain signal classification and statistical assessment of decoding accuracy. J. Neurosci. Methods 250, 126–136 (2015).He, C., Yao, Y. & Ye, X. An Emotion Recognition System Based on Physiological Signals Obtained by Wearable Sensors. In Wearable Sensors and Robots: Proceedings of International Conference on Wearable Sensors and Robots 2015 (eds. Yang, C., Virk, G. S. & Yang, H.) 15–25, https://doi.org/10.1007/978-981-10-2404-7_2 (Springer Singapore, 2017)",Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors,10.1038/s41598-018-32063-4,https://riunet.upv.es/bitstream/handle/10251/123197/Affective%20computing%20in%20virtual%20reality%20emotion%20recognition%20from%20brain%20and%20heartbeat%20dynamics%20using%20wearable%20sensors.pdf?sequence=1&isAllowed=y,'Springer Science and Business Media LLC',,core
160762897,2018-10-07T00:00:00,"We have seen much recent progress in rigid object manipulation, but
interaction with deformable objects has notably lagged behind. Due to the large
configuration space of deformable objects, solutions using traditional
modelling approaches require significant engineering work. Perhaps then,
bypassing the need for explicit modelling and instead learning the control in
an end-to-end manner serves as a better approach? Despite the growing interest
in the use of end-to-end robot learning approaches, only a small amount of work
has focused on their applicability to deformable object manipulation. Moreover,
due to the large amount of data needed to learn these end-to-end solutions, an
emerging trend is to learn control policies in simulation and then transfer
them over to the real world. To-date, no work has explored whether it is
possible to learn and transfer deformable object policies. We believe that if
sim-to-real methods are to be employed further, then it should be possible to
learn to interact with a wide variety of objects, and not only rigid objects.
In this work, we use a combination of state-of-the-art deep reinforcement
learning algorithms to solve the problem of manipulating deformable objects
(specifically cloth). We evaluate our approach on three tasks --- folding a
towel up to a mark, folding a face towel diagonally, and draping a piece of
cloth over a hanger. Our agents are fully trained in simulation with domain
randomisation, and then successfully deployed in the real world without having
seen any real deformable objects.Comment: Published at the Conference on Robot Learning (CoRL) 201",Sim-to-Real Reinforcement Learning for Deformable Object Manipulation,,http://arxiv.org/abs/1806.07851,,,core
162514979,2018-10-16T00:00:00,"International audienceContext. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.Results. The development and implementation of this method in a five-dimensional space (l, b, ϖ, μα*, μδ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",A new method for unveiling open clusters in Gaia New nearby open clusters confirmed by DR2,10.1051/0004-6361/201833390,,'EDP Sciences',,core
157511312,2018-04-04T00:00:00,"International audienceIn this demonstration we showcase a new spatial augmented reality device (interactive projection) with three applications: education and experimentation of color models, map exploration for visually impaired people and scientific vulgarization of machine learning. The first exhibition is an interactive exploration about the nature of light. Visitors can experiment with additive subtractive color models. We engage them with questions, and they have to reply using cards to find out answers. This exhibit is suitable for children. The second exhibition is about map exploration and creation for Visually Impaired Persons (VIP). VIP generally use tactile maps with braille to learn about an unknown environment. However, these maps are not accessible to the 80% of VIP who don't read braille. Our prototype augments raised-line maps with audio output. The third exhibition is destined to be used for scientific outreach. It enables the creation of artificial neural networks (ANN) using tangible interfaces. Neurons are represented by laser-cut diamond shaped tokens, and the data to learn is printed on cards. The ANN learns to differentiate shapes, and the whole learning process is made visible and interactive. These three applications demonstrate the capabilities of our hardware and software development kit in different scenarios. At ReVo, each demonstration will have its own setup and interactive space",Nectar: Multi-user Spatial Augmented Reality for everyone: Three live demonstrations of educative applications,10.1145/3234253.3234317,https://core.ac.uk/download/157511312.pdf,'Association for Computing Machinery (ACM)',,core
275648583,2018-01-01T00:00:00,"[EN] In program analysis, the synthesis of models of logical theories representing the program semantics is often useful to prove program properties. We use order-sorted first- order logic as an appropriate framework to describe the semantics and properties of programs as given theories. Then we investigate the automatic synthesis of models for such theories. We use convex polytopic domains as a flexible approach to associate different domains to different sorts. We introduce a framework for the piecewise definition of functions and predicates. We develop its use with linear expressions (in a wide sense, including linear transformations represented as matrices) and inequalities to specify functions and predicates. In this way, algorithms and tools from linear algebra and arithmetic constraint solving (e.g., SMT) can be used as a backend for an efficient implementation.Partially supported by the EU (FEDER), projects TIN2015-69175-C4-1-R, and GV PROMETEOII/2015/ 013. R. Gutiérrez also supported by Juan de la Cierva Fellowship JCI-2012-13528.Lucas Alba, S.; Gutiérrez Gil, R. (2018). Automatic Synthesis of Logical Models for Order-Sorted First-Order Theories. Journal of Automated Reasoning. 60(4):465-501. https://doi.org/10.1007/s10817-017-9419-3S465501604Alarcón, B., Gutiérrez, R., Lucas, S., Navarro-Marset, R.: Proving termination properties with MU-TERM. In: Proceedings of AMAST’10. LNCS, vol. 6486, pp. 201–208 (2011)Alarcón, B., Lucas, S., Navarro-Marset, R.: Using matrix interpretations over the reals in proofs of termination. In: Proceedings of PROLE’09, pp. 255–264 (2009)Albert, E., Genaim, S., Gutiérrez, R.: A Transformational Approach to Resource Analysis with Typed-Norms. Revised Selected Papers from LOPSTR’13. LNCS, vol. 8901, pp 38–53 (2013)de Angelis, E., Fioravante, F., Pettorossi, A., Proietti, M.: Proving correctness of imperative programs by linearizing constrained Horn clauses. Theory Pract. Log. Program. 15(4–5), 635–650 (2015)de Angelis, E., Fioravante, F., Pettorossi, A., Proietti, M.: Semantics-based generation of verification conditions by program specialization. In: Proceedings of PPDP’15, pp. 91–102. ACM Press, New York (2015)Aoto, T.: Solution to the problem of zantema on a persistent property of term rewriting systems. J. Funct. Log. Program. 2001(11), 1–20 (2001)Barwise, J.: An Introduction to First-Order Logic. In: Barwise, J. (ed.) Handbook of Mathematical Logic. North-Holland, Amsterdam (1977)Barwise, J.: Axioms for Abstract Model Theory. Ann. Math. Log. 7, 221–265 (1974)Bochnak, J., Coste, M., Roy, M.-F.: Real Algebraic Geometry. Springer, Berlin (1998)Birkhoff, G., Lipson, J.D.: Heterogeneous algebras. J. Comb. Theory 8, 115–133 (1970)Bofill, M., Nieuwenhuis, R., Oliveras, A., Rodríguez-Carbonell, E., Rubio, A.: The Barcelogic SMT Solver. In: Proceedings of CAV’08. LNCS, vol. 5123, pp. 294–298 (2008)Bjørner, N., Gurfinkel, A., McMillan, K., Rybalchenko, A.: Horn-clause solvers for program verification. In: Fields of Logic and Computation II—Essays Dedicated to Yuri Gurevich on the Occasion of His 75th Birthday. LNCS, vol. 9300, pp. 24–51 (2015)Bjørner, N., McMillan, K., Rybalchenko, A.: On solving universally quantified horn-clauses. In: Proceedings of SAS’13. LNCS vol. 7935, pp. 105–125 (2013)Bjørner, N., McMillan, K., Rybalchenko, A.: Program verification as satisfiability modulo theories. In: Proceedings of SMT’12, EPiC Series in Computing, vol. 20, pp. 3–11 (2013)Bliss, G.A.: Algebraic Functions. Dover (2004)Bonfante, G., Marion, J.-Y., Moyen, J.-Y.: On Lexicographic Termination Ordering With Space Bound Certifications. Revised Papers from PSI 2001. LNCS, vol. 2244, pp. 482–493 (2001)Boolos, G.S., Burgess, J.P., Jeffrey, R.C.: Computability and Logic, 4th edn. Cambridge University Press, Cambridge (2002)Borralleras, C., Lucas, S., Oliveras, A., Rodríguez, E., Rubio, A.: SAT modulo linear arithmetic for solving polynomial constraints. J. Autom. Reason. 48, 107–131 (2012)Bürckert, H.-J., Hollunder, B., Laux, A.: On Skolemization in constrained logics. Ann. Math. Artif. Intell. 18, 95–131 (1996)Burstall, R.M., Goguen, J.A.: Putting Theories together to make specifications. In: Proceedings of IJCAI’77, pp. 1045–1058. William Kaufmann (1977)Caplain, M.: Finding invariant assertions for proving programs. In: Proceedings of the International Conference on Reliable Software, pp. 165–171. ACM Press, New York (1975)Chang, C.L., Lee, R.C.: Symbolic Logic and Mechanical Theorem Proving. Academic Press, Orlando (1973)Clavel, M., Durán, F., Eker, S., Lincoln, P., Martí-Oliet, N., Meseguer, J., Talcott, C.: All About Maude—A High-Performance Logical Framework. LNCS 4350, (2007)Cohn, A.G.: Improving the expressiveness of many sorted logic. In: Proceedings of the National Conference on Artificial Intelligence, pp. 84–87. AAAI Press, Menlo Park (1983)Contejean, E., Marché, C., Tomás, A.-P., Urbain, X.: Mechanically proving termination using polynomial interpretations. J. Autom. Reason. 34(4), 325–363 (2006)Cooper, D.C.: Programs for mechanical program verification. Mach. Intell. 6, 43–59 (1971). Edinburgh University PressCooper, D.C.: Theorem proving in arithmetic without multiplication. Mach. Intell. 7, 91–99 (1972)Courtieu, P., Gbedo, G., Pons, O.: Improved matrix interpretations. In: Proceedings of SOFSEM’10. LNCS, vol. 5901, pp. 283–295 (2010)Cousot, P., Cousot, R., Mauborgne, L.: Logical abstract domains and interpretations. In: The Future of Sofware Engineering, pp. 48–71. Springer, New York (2011)Cousot, P., Halbwachs, N.: Automatic Discovery of linear restraints among variables of a program. In: Conference Record of POPL’78, pp. 84–96. ACM Press, New York (1978)Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order. Cambridge University Press, Cambridge (1990)Elspas, B., Levitt, K.N., Waldinger, R.J., Waksman, A.: An assessment of techniques for proving program correctness. Comput. Surv. 4(2), 97–147 (1972)van Emdem, M.H., Kowalski, R.A.: The semantics of predicate logic as a programming language. J. ACM 23(4), 733–742 (1976)Endrullis, J., Waldmann, J., Zantema, H.: Matrix interpretations for proving termination of term rewriting. In: Proceedings of IJCAR’06. LNCS, vol. 4130, pp. 574–588 (2006)Endrullis, J., Waldmann, J., Zantema, H.: Matrix interpretations for proving termination of term rewriting. J. Autom. Reason. 40(2–3), 195–220 (2008)Floyd, R.W.: Assigning meanings to programs. Math. Asp. Comput. Sci. 19, 19–32 (1967)Fuhs, C., Giesl, J., Middeldorp, A., Schneider-Kamp, P., Thiemann, R., Zankl, H.: Maximal termination. In: Proceedings of RTA’08. LNCS, vol. 5117, pp. 110–125 (2008)Fuhs, C., Giesl, J., Parting, M., Schneider-Kamp, P., Swiderski, S.: Proving termination by dependency pairs and inductive theorem proving. J. Autom. Reason. 47, 133–160 (2011)Fuhs, C., Kop, C.: Polynomial interpretations for higher-order rewriting. In: Proceedings of RTA’12. LIPIcs, vol. 15, pp. 176–192 (2012)Futatsugi, K., Diaconescu, R.: CafeOBJ Report. World Scientific, AMAST Series, (1998)Gaboardi, M., Péchoux, R.: On bounding space usage of streams using interpretation analysis. Sci. Comput. Program. 111, 395–425 (2015)Giesl, J., Mesnard, F., Rubio, A., Thiemann, R., Waldmann, J.: Termination competition (termCOMP 2015). In: Proceedings of CADE’15. LNCS, vol. 9195, pp. 105–108 (2015)Giesl, J., Ströder, T., Schneider-Kamp, P., Emmes, F., Fuhs, C.: Symbolic evaluation graphs and term rewriting—a general methodology for analyzing logic programs. In: Proceedings of the PPDP’12, pp. 1–12. ACM Press (2012)Giesl, J., Raffelsieper, M., Schneider-Kamp, P., Swiderski, S., Thiemann, R.: Automated termination proofs for haskell by term rewriting. ACM Trans. Program. Lang. Syst. 33(2), 7 (2011)Gnaedig, I.: Termination of Order-sorted Rewriting. In: Proceedings of ALP’92. LNCS, vol. 632, pp. 37–52 (1992)Goguen, J.A.: Order-Sorted Algebra. Semantics and Theory of Computation Report 14, UCLA (1978)Goguen, J.A., Burstall, R.M.: Some fundamental algebraic tools for the semantics of computation. Part 1: comma categories, colimits, signatures and theories. Theoret. Comput. Sci. 31, 175–209 (1984)Goguen, J.A., Burstall, R.M.: Some fundamental algebraic tools for the semantics of computation. Part 2 signed and abstract theories. Theoret. Comput. Sci. 31, 263–295 (1984)Goguen, J., Meseguer, J.: Models and equality for logical programming. In: Proceedings of TAPSOFT’87. LNCS, vol. 250, pp. 1–22 (1987)Goguen, J.A., Thatcher, J.W., Wagner, E.G.: An initial algebra approach to the specification, correctness and implementation of abstract data types. In: Current Trends in Programming Methodology, pp. 80–149. Prentice Hall (1978)Goguen, J.A., Meseguer, J.: Remarks on remarks on many-sorted equational logic. Sigplan Notices 22(4), 41–48 (1987)Goguen, J., Meseguer, J.: Order-sorted algebra I: equational deduction for multiple inheritance, overloading, exceptions and partial operations. Theoret. Comput. Sci. 105, 217–273 (1992)Goguen, J.A., Winkler, T., Meseguer, J., Futatsugi, K., Jouannaud, J.-P.: Introducing OBJ. In: Goguen, J., Malcolm, G. (eds.) Software Engineering with OBJ: Algebraic Specification in Action. Kluwer, Boston (2000)Grebenshikov, S., Lopes, N.P., Popeea, C., Rybalchenko, A.: Synthesizing software verifiers from proof rules. In: Proceedings of PLDI’12, pp. 405–416. ACM Press (2012)Gulwani, S., Tiwari, A.: Combining Abstract Interpreters. In: Proceedings of PLDI’06, pp. 376–386. ACM Press (2006)Gurfinkel, A., Kahsai, T., Komuravelli, A., Navas, J.A.: The seahorn verification framework. In: Proceedings of CAV’15, Part I. LNCS, vol. 9206, pp. 343–361 (2015)Gutiérrez, R., Lucas, S., Reinoso, P.: A tool for the automatic generation of logical models of order-sorted first-order theories. In: Proceedings of PROLE’16, pp. 215–230 (2016). http://zenon.dsic.upv.es/ages/Hantler, S.L., King, J.C.: An introduction to proving the correctness of programs. ACM Comput. Surv. 8(3), 331–353 (1976)Hayes, P.: A logic of actions. Mach. Intell. 6, 495–520 (1971). Edinburgh University Press, EdinburghHeidergott, B., Olsder, G.J., van der Woude, J.: Max plus at work. A course on max-plus algebra and its applications. In: Modeling and Analysis of Synchronized Systems, Princeton University Press (2006)Hirokawa, N., Moser, G.: Automated complexity analysis based on the dependency pair method. In: Proceedings of IJCAR 2008. LNCS, vol. 5195, pp. 364–379 (2008)Hoare, C.A.R.: An axiomatic basis for computer programming. Commun. ACM 12(10), 576–583 (1969)Hodges, W.: Elementary Predicate Logic. Handbook of Philosophical Logic, vol. 1, pp. 1–131. Reidel Publishing Company (1983)Hodges, W.: A Shorter Model Theory. Cambridge University Press, Cambridge (1997)Hofbauer, D.: Termination proofs by context-dependent interpretation. In: Proceedings of RTA’01. LNCS, vol. 2051, pp. 108–121 (2001)Hofbauer, D.: Termination proofs for ground rewrite systems. interpretations and derivational complexity. Appl. Algebra Eng. Commun. Comput. 12, 21–38 (2001)Hofbauer, D., Lautemann, C.: Termination proofs and the length of derivations. In: Proceedings of RTA’89. LNCS, vol. 355, pp. 167–177 (1989)Hull, T.E., Enright, W.H., Sedgwick, A.E.: The correctness of numerical algorithms. In: Proceedings of PAAP’72, pp. 66–73 (1972)Igarashi, S., London, R.L., Luckham, D.: Automatic program verification I: a logical basis and its implementation. Acta Inform. 4, 145–182 (1975)Iwami, M.: Persistence of termination of term rewriting systems with ordered sorts. In: Proceedings of 5th JSSST Workshop on Programming and Programming Languages, Shizuoka, Japan, pp. 47–56. (2003)Iwami, M.: Persistence of termination for non-overlapping term rewriting systems. In: Proceedings of Algebraic Systems, Formal Languages and Conventional and Unconventional Computation Theory, Kokyuroku RIMS, University of Kyoto, vol. 1366, pp. 91–99 (2004)Katz, S., Manna, Z.: Logical analysis of programs. Commun. ACM 19(4), 188–206 (1976)Langford, C.H.: Review: Über deduktive Theorien mit mehreren Sorten von Grunddingen. J. Symb. Log. 4(2), 98 (1939)Lankford, D.S.: Some approaches to equality for computational logic: a survey and assessment. Memo ATP-36, Automatic Theorem Proving Project, University of Texas, Austin, TXLondon, R.L.: The current state of proving programs correct. In: Proceedings of ACM’72, vol. 1, pp. 39–46. ACM (1972)Lucas, S.: Polynomials over the reals in proofs of termination: from theory to practice. RAIRO Theor. Inform. Appl. 39(3), 547–586 (2005)Lucas, S.: Synthesis of models for order-sorted first-order theories using linear algebra and constraint solving. Electron. Proc. Theor. Comput. Sci. 200, 32–47 (2015)Lucas, S.: Use of logical models for proving operational termination in general logics. In: Selected Papers from WRLA’16. LNCS, vol. 9942, pp. 1–21 (2016)Lucas, S., Marché, C., Meseguer, J.: Operational termination of conditional term rewriting systems. Inform. Proces. Lett. 95, 446–453 (2005)Lucas, S., Meseguer, J.: Models for logics and conditional constraints in automated proofs of termination. In: Proceedings of AISC’14. LNAI, vol. 8884, pp. 7–18 (2014)Lucas, S., Meseguer, J.: Order-sorted dependency pairs. In: Proceedings of PPDP’08 , pp. 108–119. ACM Press (2008)Lucas, S., Meseguer, J.: Proving operational termination of declarative programs in general logics. In: Proceedings of PPDP’14, pp. 111–122. ACM Digital Library (2014)Lucas, S., Meseguer, J.: Dependency pairs for proving termination properties of conditional term rewriting systems. J. Log. Algebr. Methods Program. 86, 236–268 (2017)Manna, Z.: The correctness of programs. J. Comput. Syst. Sci. 3, 119–127 (1969)Manna, Z.: Properties of programs and the first-order predicate calculus. J. ACM 16(2), 244–255 (1969)Manna, Z.: Termination of programs represented as interpreted graphs. In: Proceedings of AFIPS’70, pp. 83–89 (1970)Manna, Z., Ness, S.: On the termination of Markov algorithms. In: Proceedings of the Third Hawaii International Conference on System Science, pp. 789–792 (1970)Manna, Z., Pnueli, A.: Formalization of properties of functional programs. J. ACM 17(3), 555–569 (1970)Marion, Y.-I., Péchoux, R.: Sup-interpretations, a semantic method for static analysis of program resources. ACM Trans. Comput. Log. 10(4), 27 (2009)Martí-Oliet, N., Meseguer, J., Palomino, M.: Theoroidal maps as algebraic simulations. Revised Selected Papers from WADT’04. LNCS, vol. 3423, pp. 126–143 (2005)McCarthy, J.: Recursive functions of symbolic expressions and their computation by machine. Part I. Commun. ACM 3(4), 184–195 (1960)Meseguer, J.: General logics. In: Ebbinghaus, H.D., et al. (eds.) Logic Colloquium’87, pp. 275–329. North-Holland (1989)Meseguer, J., Skeirik, S.: Equational formulas and pattern operations in initial order-sorted algebras. Revised Selected Papers from LOPSTR’15. LNCS, vol. 9527, pp. 36–53 (2015)Middeldorp, A.: Matrix interpretations for polynomial derivational complexity of rewrite systems. In: Proceedings of LPAR’12. LNCS, vol. 7180, p. 12 (2012)Monin, J.-F.: Understanding Formal Methods. Springer, London (2003)Montenegro, M., Peña, R., Segura, C.: Space consumption analysis by abstract interpretation: inference of recursive functions. Sci. Comput. Program. 111, 426–457 (2015)de Moura, L., Bjørner, N.: Satisfiability modulo theories: introduction and applications. Commun. ACM 54(9), 69–77 (2011)Naur, P.: Proof of algorithms by general snapshots. Bit 6, 310–316 (1966)Neurauter, F., Middeldorp, A.: Revisiting matrix interpretations for proving termination of term rewriting. In: Proceedings of RTA’11. LIPICS, vol. 10, pp. 251–266 (2011)Ohlebusch, E.: Advanced Topics in Term Rewriting. Springer, New York (2002)Ölveczky, P.C., Lysne, O.: Order-sorted termination: the unsorted way. In: Proceedings of ALP’96. LNCS, vol. 1139, pp. 92–106 (1996)Otto, C., Brockschmidt, M., von Essen, C., Giesl, J.: Automated termination analysis of java bytecode by term rewriting. In: Proceedings of RTA’10. LIPICS, vol. 6, pp. 259–276 (2010)Péchoux, R.: Synthesis of sup-interpretations: a survey. Theoret. Comput. Sci. 467, 30–52 (2013)Podelski, A., Rybalchenko, A.: Transition invariants. In: IEEE Computer Society Proceedings of LICS’04, pp. 32–41 (2004)Prestel, A., Delzell, C.N.: Positive Polynomials. From Hilbert’s 17th Problem to Real Algebra. Springer, Berlin (2001)Robinson, D.J.S.: A Course in Linear Algebra with Applications, 2nd edn. World Scientific Publishing, Co, Singapore (2006)Rümmer, P., Hojjat, H., Kuncak, V.: Disjunctive interpolants for horn-clause verification. In: Proceedings of CAV’13, vol. 8044, pp. 347–363 (2013)Schrijver, A.: Theory of Linear and Integer Programming. Wiley, Amsterdam (1986)Schmidt, A.: Über deduktive Theorien mit mehreren Sorten von Grunddingen. Matematische Annalen 115(4), 485–506 (1938)Schmidt-Schauss, M.: Computational Aspects Of An Order-Sorted Logic With Term Declarations. PhD Thesis, Fachbereich Informatik der Universität Kaiserslautern (1988)Shapiro, S.: Foundations without Foundationalism: A Case for Second-Order Logic. Clarendon Press, New York (1991)Shostak, R.E.: A practical decision procedure for arithmetic with function symbols. J. ACM 26(2), 351–360 (1979)Smullyan, R.M.: Theory of Formal Systems. Princeton University Press, Princeton (1961)Tarski, A.: A Decision Method for Elementary Algebra and Geometry, 2nd edn. University of California Press, Berkeley (1951)Toyama, Y.: Counterexamples to termination for the direct sum of term rewriting systems. Inform. Process. Lett. 25, 141–143 (1987)Turing, A.M.: Checking a large routine. In: Report of a Conference on High Speed Automatic Calculating Machines, University Mathematics Laboratory, Cambridge, pp. 67–69 (1949)Urban, C.: The abstract domain of segmented ranking functions. In: Proceeding of SAS’13. LNCS, vol. 7935, pp. 43–62 (2013)Urban, C., Gurfinkel, A., Kahsai, T.: Synthesizing ranking functions from bits and pieces. In: Proceedings of TACAS’16. LNCS, vol. 9636, pp. 54–70 (2016)Waldmann, J.: Matrix interpretations on polyhedral domains. In: Proceedings of RTA’15. LIPICS, vol. 26, pp. 318–333 (2015)Waldmann, J., Bau, A., Noeth, E.: Matchbox termination prover. http://github.com/jwaldmann/matchbox/ (2014)Walther, C.: A mechanical solution of schubert’s steamroller by many-sorted resolution. Aritif. Intell. 26, 217–224 (1985)Wang, H.: Logic of many-sorted theories. J. Symb. Logic 17(2), 105–116 (1952)Zantema, H.: Termination of term rewriting: interpretation and type elimination. J. Symb. Comput. 17, 23–50 (1994",Automatic Synthesis of Logical Models for Order-Sorted First-Order Theories,10.1007/s10817-017-9419-3,https://riunet.upv.es/bitstream/10251/124228/7/Lucas%3bGuti%c3%a9rrez%20-%20Automatic%20Synthesis%20of%20Logical%20Models%20for%20Order-Sorted%20First-Order%20Theories.pdf,'Springer Science and Business Media LLC',,core
161953335,2018-05-01T00:00:00,"Distributed artificial intelligence (DAI) and multi-agent system (MAS) has recently gained increasing interest due to its vast applications in real-world problems. Inspired by the natural MAS, this thesis primarily focuses on the study of the collective intelligence and the joint behavior of MAS, which are typically generated by a group of intelligent agents applied with autonomous controls. In particular, the collective intelligence is described as geometric group patterns, stationary distribution and cooperative motions in this thesis.

As the size of the group increases, it is essential to exploit simple and distributed controls to achieve the desired collective intelligence of the system with robustness and minimal cost. Therefore, this thesis proposes two bio-inspired applications of MAS to illustrate that simple controls can obtain stable and robust limiting collective behaviors. Besides, topological configuration space is introduced to describe the admissible collective behaviors and design the cooperative controls.

In the first part of the thesis, cyclic pursuit as a periodic joint behavior of the MAS is studied. With prescribed deployments and controls of the agents, the limiting group geometric formation varies from regular polygons to an eight-shaped graph. The rotation number of the graph is a geometric invariant during the evolution. In the second application, the cyclic collective intelligence is generalized to a swarm concentration problem with desired gathering and drifting group behaviors. Randomized algorithms are used to localize the agents and generate stationary distributions of the swarm. In the last part of the thesis, topological configuration space is implemented to assist the design of hybrid controls for a multi-agent coverage problem",Control and collective intelligence of multi-agent system,,https://core.ac.uk/download/161953335.pdf,,,core
395005287,2018-09-01T00:00:00,"The design of cyber-physical systems (CPSs) requires methods and tools that can efficiently reason about the interaction between discrete models, e.g., representing the behaviors of 'cyber' components, and continuous models of physical processes. Boolean methods such as satisfiability (SAT) solving are successful in tackling large combinatorial search problems for the design and verification of hardware and software components. On the other hand, problems in control, communications, signal processing, and machine learning often rely on convex programming as a powerful solution engine. However, despite their strengths, neither approach would work in isolation for CPSs. In this paper, we present a new satisfiability modulo convex programming (SMC) framework that integrates SAT solving and convex optimization to efficiently reason about Boolean and convex constraints at the same time. We exploit the properties of a class of logic formulas over Boolean and nonlinear real predicates, termed monotone satisfiability modulo convex formulas, whose satisfiability can be checked via a finite number of convex programs. Following the lazy satisfiability modulo theory (SMT) paradigm, we develop a new decision procedure for monotone SMC formulas, which coordinates SAT solving and convex programming to provide a satisfying assignment or determine that the formula is unsatisfiable. A key step in our coordination scheme is the efficient generation of succinct infeasibility proofs for inconsistent constraints that can support conflict-driven learning and accelerate the search. We demonstrate our approach on different CPS design problems, including spacecraft docking mission control, robotic motion planning, and secure state estimation. We show that SMC can handle more complex problem instances than state-of-the-art alternative techniques based on SMT solving and mixed integer convex programming",SMC: Satisfiability Modulo Convex Programming,,,"eScholarship, University of California",,core
275646428,2018-01-01T00:00:00,"[EN] Traditional interaction mechanisms in distributed digital spaces often fail to consider the intrinsic properties of action, perception, and communication among workgroups, which may affect access to the common resources used to mutually organize information. By developing suitable spatial geometries and natural interaction mechanisms, distributed spaces can become blended where the physical and virtual boundaries of local and remote spaces merge together to provide the illusion of a single unified space. In this paper, we discuss the importance of blended interaction in distributed spaces and the particular challenges faced when designing accessible technology. We illustrate this discussion through a new tangible interaction mechanism for collaborative spaces based on tabletop system technology implemented with optical frames. Our tangible elements facilitate the exchange of digital information in distributed collaborative settings by providing a physical manifestation of common digital operations. The tangibles are designed as passive elements that do not require the use of any additional hardware or external power while maintaining a high degree of accuracy.This work was supported by the Spanish Ministry of Economy and Competitiveness and the European Regional Development Fund, through the ANNOTA Project (Ref. TIN2013-46036-C3-1-R).Salvador-Herranz, G.; Camba, J.; Contero, M.; Naya Sanchis, F. (2018). Accessibility and tangible interaction in distributed workspaces based on multi-touch surfaces. Universal Access in the Information Society. 17(2):247-256. https://doi.org/10.1007/s10209-017-0563-7S247256172Arkin, E.M., Chew, L.P., Huttenlocher, D.P., Kedem, K., Mitchell, J.S.B.: An efficiently computable metric for comparing polygonal shapes. IEEE Trans. Acoust. Speech Signal Process. 13(3), 209–216 (1991)Benyon, D.: Presence in blended spaces. Interact. Comput. 24(4), 219–226 (2012)Bhalla, M.R., Bhalla, A.V.: Comparative study of various touchscreen technologies. Int. J. Comput. Appl. 6(8), 12–18 (2010)Bradski, G., Kaehler, A.: Learning OpenCV: Computer Vision with the OpenCV Library. O’Reilly Media Inc., Newton (2008)Candela, E.S., Pérez, M.O., Romero, C.M., López, D.C.P., Herranz, G.S., Contero, M., Raya, M.A.: Humantop: a multi-object tracking tabletop. Multimed. Tools Appl. 70(3), 1837–1868 (2014)Cohen, J., Withgott, M., Piernot, P.: Logjam: a tangible multi-person interface for video logging. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 128–135. ACM (1999)Couture, N., Rivière, G., Reuter, P.: Geotui: a tangible user interface for geoscience. In: Proceedings of the 2nd International Conference on Tangible and Embedded Interaction, pp. 89–96. ACM (2008)de la Guía, E., Lozano, M.D., Penichet, V.R.: Cognitive rehabilitation based on collaborative and tangible computer games. In: 2013 7th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth), pp. 389–392. IEEE (2013)Dietz, P., Leigh, D.: Diamondtouch: a multi-user touch technology. In: Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, pp. 219–226. ACM (2001)Falcão, T.P., Price, S.: What have you done! the role of ‘interference’ in tangible environments for supporting collaborative learning. In: Proceedings of the 9th International Conference on Computer Supported Collaborative Learning-Volume 1, pp. 325–334. International Society of the Learning Sciences (2009)Fallman, D.: Wear, point and tilt. In: Proceedings of the Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques, pp. 293–302. ACM Press (2002)Fishkin, K.P., Gujar, A., Harrison, B.L., Moran, T.P., Want, R.: Embodied user interfaces for really direct manipulation. Commun. ACM 43(9), 74–80 (2000)Fitzmaurice, G.W., Buxton, W.: An empirical evaluation of graspable user interfaces: towards specialized, space-multiplexed input. In: Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, pp. 43–50. ACM (1997)Fitzmaurice, G.W., Ishii, H., Buxton, W.A.: Bricks: laying the foundations for graspable user interfaces. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 442–449. ACM Press (1995)Graham, R.L., Yao, F.F.: Finding the convex hull of a simple polygon. J. Algorithms 4(4), 324–331 (1983)Hartigan, J.A., Wong, M.A.: Algorithm as 136: a k-means clustering algorithm. J. R. Stat. Soc.: Ser. C (Appl. Stat.) 28(1), 100–108 (1979)Higgins, S.E., Mercier, E., Burd, E., Hatch, A.: Multi-touch tables and the relationship with collaborative classroom pedagogies: a synthetic review. Int. J. Comput. Support. Collab. Learn. 6(4), 515–538 (2011)Hinckley, K., Pausch, R., Goble, J.C., Kassell, N.F.: Passive real-world interface props for neurosurgical visualization. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 452–458. ACM (1994)Hinske, S.: Determining the position and orientation of multi-tagged objects using RFID technology. In: 5th Annual IEEE International Conference on Pervasive Computing and Communications Workshops, 2007. PerCom Workshops’07, pp. 377–381. IEEE (2007)Hornecker, E.: A design theme for tangible interaction: embodied facilitation. In: ECSCW 2005, pp. 23–43. Springer (2005)Hoshi, K., Öhberg, F., Nyberg, A.: Designing blended reality space: conceptual foundations and applications. In: Proceedings of the 25th BCS Conference on Human–Computer Interaction, pp. 217–226. British Computer Society (2011)Ishii, H.: Tangible User Interfaces. CRC Press, Boca Raton (2007)Ishii, H., Ullmer, B.: Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, pp. 234–241. ACM (1997)Jacob, R.J., Girouard, A., Hirshfield, L.M., Horn, M.S., Shaer, O., Solovey, E.T., Zigelbaum, J.: Reality-based interaction: a framework for post-wimp interfaces. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 201–210. ACM (2008)Jetter, H.C., Dachselt, R., Reiterer, H., Quigley, A., Benyon, D., Haller, M.: Blended Interaction: Envisioning Future Collaborative Interactive Spaces. ACM, New York (2013)Jin, X., Han, J.: Quality threshold clustering. In: Sammut, C., Webb, G.I. (eds.) Encyclopedia of Machine Learning, pp. 820–820. Springer, Boston, MA (2011)Jordà, S., Geiger, G., Alonso, M., Kaltenbrunner, M.: The reactable: exploring the synergy between live music performance and tabletop tangible interfaces. In: Proceedings of the 1st International Conference on Tangible and Embedded Interaction, pp. 139–146. ACM (2007)Kaltenbrunner, M., Bovermann, T., Bencina, R., Costanza, E.: Tuio: a protocol for table-top tangible user interfaces. In: Proceedings of the 6th International Workshop on Gesture in Human–Computer Interaction and Simulation, pp. 1–5 (2005)Kirk, D., Sellen, A., Taylor, S., Villar, N., Izadi, S.: Putting the physical into the digital: issues in designing hybrid interactive surfaces. In: Proceedings of the 23rd British HCI Group Annual Conference on People and Computers: Celebrating People and Technology, pp. 35–44. British Computer Society (2009)Marques, T., Nunes, F., Silva, P., Rodrigues, R.: Tangible interaction on tabletops for elderly people. In: International Conference on Entertainment Computing, pp. 440–443. Springer (2011)Müller, D.: Mixed reality systems. iJOE 5(S2), 10–11 (2009)Newton-Dunn, H., Nakano, H., Gibson, J.: Block jam: a tangible interface for interactive music. In: Proceedings of the 2003 Conference on New Interfaces for Musical Expression, pp. 170–177. National University of Singapore (2003)Patten, J., Recht, B., Ishii, H.: Audiopad: a tag-based interface for musical performance. In: Proceedings of the 2002 Conference on New Interfaces for Musical Expression, pp. 1–6. National University of Singapore (2002)Patten, J., Recht, B., Ishii, H.: Interaction techniques for musical performance with tabletop tangible interfaces. In: Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology, p. 27. ACM (2006)PQLabs: Inc. http://multitouch.com/ . Retrieved on 16 October 2016Ryokai, K., Marti, S., Ishii, H.: I/o brush: drawing with everyday objects as ink. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI’04, pp. 303–310. ACM, New York (2004). doi: 10.1145/985692.985731Salvador, G., Bañó, M., Contero, M., Camba, J.: Evaluation of a distributed collaborative workspace as a creativity tool in the context of design education. In: 2014 IEEE Frontiers in Education Conference (FIE) Proceedings, pp. 1–7. IEEE (2014)Salvador-Herranz, G., Contero, M., Camba, J.: Use of tangible marks with optical frame interactive surfaces in collaborative design scenarios based on blended spaces. In: International Conference on Cooperative Design, Visualization and Engineering, pp. 253–260. Springer (2014)Salvador-Herranz, G., Camba, J.D., Naya, F., Contero, M.: On the integration of tangible elements with multi-touch surfaces for the collaborative creation of concept maps. In: International Conference on Learning and Collaboration Technologies, pp. 177–186. Springer (2016)Schöning, J., Hook, J., Bartindale, T., Schmidt, D., Oliver, P., Echtler, F., Motamedi, N., Brandl, P., von Zadow, U.: Building interactive multi-touch surfaces. In: Müller-Tomfelde, C. (ed.) Tabletops-Horizontal Interactive Displays, pp. 27–49. Springer, London, UK (2010)Shaer, O., Hornecker, E.: Tangible user interfaces: past, present, and future directions. Found. Trends Hum. Comput. Interact. 3(1–2), 1–137 (2010)Shen, C., Everitt, K., Ryall, K.: Ubitable: Impromptu face-to-face collaboration on horizontal interactive surfaces. In: International Conference on Ubiquitous Computing, pp. 281–288. Springer (2003)Suzuki, H., Kato, H.: Algoblock: a tangible programming language, a tool for collaborative learning. In: Proceedings of 4th European Logo Conference, pp. 297–303 (1993)Suzuki, H., Kato, H.: Interaction-level support for collaborative learning: Algoblockan open programming language. In: The 1st International Conference on Computer Support for Collaborative Learning, pp. 349–355. L. Erlbaum Associates Inc. (1995)Terrenghi, L., Kirk, D., Richter, H., Krämer, S., Hilliges, O., Butz, A.: Physical handles at the interactive surface: exploring tangibility and its benefits. In: Proceedings of the Working Conference on Advanced Visual Interfaces, pp. 138–145. ACM (2008)Veltkamp, R.C.: Shape matching: similarity measures and algorithms. In: SMI 2001 International Conference on Shape Modeling and Applications, pp. 188–197. IEEE (2001)Weinberg, G., Gan, S.L.: The squeezables: Toward an expressive and interdependent multi-player musical instrument. Comput. Music J. 25(2), 37–45 (2001)Weiser, M.: Some computer science issues in ubiquitous computing. Commun. ACM 36(7), 75–84 (1993)Wilson, F.: The hand: how its use shapes the brain, language, and human culture. Vintage Series. Vintage Books (1998). https://books.google.es/books?id=l_Boy_-NkwUCZuckerman, O., Arida, S., Resnick, M.: Extending tangible interfaces for education: digital montessori-inspired manipulatives. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 859–868. ACM (2005",Accessibility and tangible interaction in distributed workspaces based on multi-touch surfaces,10.1007/s10209-017-0563-7,https://riunet.upv.es/bitstream/10251/120351/9/Postprint%20UAIS%202018.pdf,'Springer Science and Business Media LLC',,core
296662226,2015-11-26T15:37:01Z,"The productivity and quality of a continuous caster depend mainly on process parameters, i.e. casting speed, casting temperature, steel composition and cleanliness of the melt, water flow rates in the different cooling zones, etc. This work presents the development of an algorithm, which incorporates heuristic search techniques for direct application in metallurgical industries, particularly those using continuous casting process for the production of steel billets and slabs. This is done to determine the casting objectives of maximum casting rate as a function of casting constraints. These constraints are evaluated with the aid of a heat transfer and solidification model based on the finite difference technique, which has been developed and integrated with a genetic algorithm. The essential parts of continuous casting equipment, which must be subjected to monitoring, as well as a methodology of mathematical model and physical settlements in each cooling region, are presented. The efficiency of the intelligent system is assured by the optimisation of the continuous casting operation by maximum casting rate and defect-free products. This approach is applied to the real dimension of a steel continuous caster, in real conditions of operation, demonstrating that good results can be attained by using heuristic search, such as: smaller temperature gradients between sprays zones, reduction in water consumption and an increase in casting speed. © 2002 Elsevier Science Inc. All rights reserved.261110771092Williamson, C.Q., Process control in continuous casting a trend or must (1988) Continuous Casting, 4, pp. 281-287Irving, W.R., On line quality control for continuously cast semis (1990) Ironmaking and Steelmaking, 17 (3), pp. 197-202Kumar, S., Samarasekera, I.V., Brimacombe, J.K., Mould thermal response and formation of defects in the continuous casting of steel billets-laps and bleeds (1997) Iron and Steelmaker, pp. 53-69Samarasekera, I.V., Brimacombe, J.K., Wilder, K., The pursuit of steel billet quality (1994) Iron and Steelmaker, pp. 53-63Kumar, S., Walker, B.N., Samarasekera, I.V., Brimacombe, J.K., Chaos at the meniscus-the genesis of defects in continuously cast steel billets (1993) 13th PTD Conference Proceeding, pp. 119-141Brimacombe, J.K., Empowerment with knowledge--toward the intelligent mould for the continuous casting of steel billets (1993) Iron and Steelmaker, pp. 35-47Kumar, S., Meech, J.A., Samarasekera, I.V., Brimacombe, J.K., Knowledge engineering an expert systems to troubleshoot quality problems in the continuous casting of steel billets (1993) Iron and Steelmaker, pp. 29-36Larreq, M., Birat, J.P., Optimization of casting and cooling conditions on steel continuous casters--implementation of optimal strategies on slab and bloom casters (1982) Application of Mathematical and Physical Models in the Iron and Steel Industry, Iron and Steel Society of ASMEFilipic, B., Sarler, B., Continuous casting simulator--a tool for improved quality and productivity (1997) Proceedings of the 2nd International Metallurgical Conference Continuous Casting of Billets, pp. 161-168. , Trinec, Czech RepublicFilipic, B., Sarler, B., Evolving parameter setting for continuous casting of steel (1998) Proceedings of the 6th European Congress on Intelligent Techniques and Soft Computing--EUFIT'98, 1, pp. 444-449. , Aachen, Germany, Sept 7-10Lally, B., Biegler, L.T., Henein, H., Optimisation and continuous casting: Part I. Problem formulation and solution strategy (1991) Metallurgical Transactions B, 22 B, pp. 641-648Cheung, N., Garcia, A., The use of a heuristic search technique for the optimization of quality of steel billets produced by continuous casting (2001) Engineering Applications of Artificial Intelligence, 14, pp. 229-238Osman, I.H., Kelly, J.P., Meta-heuristics: An overview (1996) Meta-Heuristics: Theory - Applications, , Kluwer Academic PublishersYagiura, M., Ibaraki, T., Genetic and local search algorithms as robust and simple optimization tools (1996) Meta-Heuristics: Theory - Applications, , Kluwer Academic PublishersRasheed, K., Hirsh, H., Gelsey, A., A genetic algorithm for continuous design space search (1997) Artificial Intelligence in Engineering, 11, pp. 295-305Spim, J.A., Garcia, A., An optimisation of the finite difference method for modeling solidification of complex shapes (1997) Journal of the Brazilian Society of Mechanical Sciences, 19, pp. 392-409Brown, D.E., White, C.C., (1990) Operations Research and Artificial Intelligence: The Integration of Problem-solving Strategies, , Kluwer Academic PublisherApelian, D., Meysel, A., Intelligent processing of materials (1990) The Minerals, pp. 427-434. , H. N. Wadley, & W. E. Eckhart (Eds.), Metals - Materials SocietyBohmer, J.R., Fett, F.N., Modelling of casting welding and advanced solidification process V (1991) The Minerals, p. 337. , M. Rappaz, M. R. Ozgu, & K. Mahin (Eds.), Metals - Materials SocietyMizikar, E.A., Sprays cooling investigation for continuous casting of billets and blooms (1970) Iron and Steel Institute, pp. 53-60Bolle, E., Moureau, J.C., Sprays cooling of hot surfaces: A description of the dispersed phase and a parametric study of heat transfer results (1946) Proceedings of Two Phase Flows and Heat Transfer, 3, pp. 1327-1346. , NATO Advanced Study InstituteKominami, H., Neural network system for breakout prediction in continuous casting process (1991) Nippon Steel Technical Report, 49, pp. 34-38Voller, V.R., Swaminathan, C.R., General source-based method for solidification phase change (1991) Numerical Heat Transfer Part B, 19, p. 175Welty, J.R., (1976) Engineering Heat Transfer, , New York: J. Wiley and Sons In",The Use Of Artificial Intelligence Technique For The Optimisation Of Process Parameters Used In The Continuous Casting Of Steel,10.1016/S0307-904X(02)00062-8,,,,core
212998283,2016-04-25T07:00:00,"Off-the-shelf Reinforcement Learning (RL) algorithms suffer from slow learning performance, partly because they are expected to learn a task from scratch merely through an agent\u27s own experience. In this thesis, we show that learning from scratch is a limiting factor for the learning performance, and that when prior knowledge is available RL agents can learn a task faster. We evaluate relevant previous work and our own algorithms in various experiments.  Our first contribution is the first implementation and evaluation of an existing interactive RL algorithm in a real-world domain with a humanoid robot. Interactive RL was evaluated in a simulated domain which motivated us for evaluating its practicality on a robot. Our evaluation shows that guidance reduces learning time, and that its positive effects increase with state space size.  A natural follow up question after our first evaluation was, how do some other previous works compare to interactive RL. Our second contribution is an analysis of a user study, where na ive human teachers demonstrated a real-world object catching with a humanoid robot. We present the first comparison of several previous works in a common real-world domain with a user study.  One conclusion of the user study was the high potential of RL despite poor usability due to slow learning rate. As an effort to improve the learning efficiency of RL learners, our third contribution is a novel human-agent knowledge transfer algorithm. Using demonstrations from three teachers with varying expertise in a simulated domain, we show that regardless of the skill level, human demonstrations can improve the asymptotic performance of an RL agent.  As an alternative approach for encoding human knowledge in RL, we investigated the use of reward shaping. Our final contributions are Static Inverse Reinforcement Learning Shaping and Dynamic Inverse Reinforcement Learning Shaping algorithms that use human demonstrations for recovering a shaping reward function. Our experiments in simulated domains show that our approach outperforms the state-of-the-art in cumulative reward, learning rate and asymptotic performance.  Overall we show that human demonstrators with varying skills can help RL agents to learn tasks more efficiently",Reinforcement Learning from Demonstration,,https://core.ac.uk/download/212998283.pdf,Digital WPI,,core
73369227,2016-09-07T00:00:00,"Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments","A modular architecture for transparent computation in Recurrent Neural
  Networks",10.1016/j.neunet.2016.09.001,http://arxiv.org/abs/1609.01926,'Elsevier BV',,core
108382732,2016-09-23,"Abstract — This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates. Keywords-component; Multi-FPGA; Embedded Syseim",A Multi-FPGA Distributed Embedded System for the Emulation of Multi-Layer CNNs in Real Time Video Applications,,,,,core
104117398,2016-08-14,"We present a new similarity measure based on information theoretic measures which is superior than Normalized Com-pression Distance for clustering problems and inherits the useful properties of conditional Kolmogorov complexity. We show that Normalized Compression Dictionary Size and Normalized Compression Dictionary Entropy are com-putationally more efficient, as the need to perform the com-pression itself is eliminated. Also they scale linearly with ex-ponential vector size growth and are content independent. We show that normalized compression dictionary distance is compressor independent, if limited to lossless compres-sors, which gives space for optimizations and implementation speed improvement for real-time and big data applications. The introduced measure is applicable for machine learn-ing tasks of parameter-free unsupervised clustering, super-vised learning such as classification and regression, feature selection, and is applicable for big data problems with order of magnitude speed increase. Index Terms — dissimilarity, distance function, normal-ized compression distance, time-series clustering, parameter-free data-mining, heterogenous data analysis, Kolmogorov complexity, information theory, machine learning, big data 1",GENERALIZED COMPRESSION DICTIONARY DISTANCE AS UNIVERSAL SIMILARITY MEASURE,,,,,core
83226635,2015-12-01T00:00:00,"Google Glass is a recently designed wearable device capable of displaying information in a smartphone-like hands-free format by wireless communication. The Glass also provides convenient control over remote devices, primarily enabled by voice recognition commands. These unique features of the Google Glass make it useful for medical and biomedical applications where hands-free experiences are strongly preferred. Here, we report for the first time, an integral set of hardware, firmware, software, and Glassware that enabled wireless transmission of sensor data onto the Google Glass for on-demand data visualization and real-time analysis. Additionally, the platform allowed the user to control outputs entered through the Glass, therefore achieving bi-directional Glass-device interfacing. Using this versatile platform, we demonstrated its capability in monitoring physical and physiological parameters such as temperature, pH, and morphology of liver- and heart-on-chips. Furthermore, we showed the capability to remotely introduce pharmaceutical compounds into a microfluidic human primary liver bioreactor at desired time points while monitoring their effects through the Glass. We believe that such an innovative platform, along with its concept, has set up a premise in wearable monitoring and controlling technology for a wide variety of applications in biomedicine.United States. Defense Threat Reduction Agency (Space and Naval Warfare Systems Center Pacific (SSC PACIFIC) Contract No. N66001-13-C-2027)United States. Office of Naval Research (Young National Investigator Award)National Institutes of Health (U.S.) (EB012597)National Institutes of Health (U.S.) (AR057837)National Institutes of Health (U.S.)  (DE021468)National Institutes of Health (U.S.) (HL099073)National Institutes of Health (U.S.) (R56AI105024)United States. Office of Naval Research. Presidential Early Career Award for Scientists and Engineer",Google Glass-Directed Monitoring and Control of Microfluidic Biosensors and Actuators,10.1038/srep22237,https://core.ac.uk/download/83226635.pdf,'Springer Science and Business Media LLC',"[{'title': 'Scientific Reports', 'identifiers': ['2045-2322', 'issn:2045-2322']}]",core
79844017,2016-10-21,"Space Shuttle Main Engine fault detection systems typically rely on sensor data analysis via redundant rule-based expert systems along with visual observations for the real-time assessment of engine health. A novel alternative to the traditional health monitoring approach is predicated upon the acquisition and subsequent neural network processing of electromagnetic plume emissions. Spectrometric examination of an emission spectrum provides a means for the identification and quantification of metallic species indigenous to the main engine plume flow. Knowledge of the metallic species eroding could pinpoint the specific location of component degradation within the engine as well as identify serious component failures at an early stage. Such an approach is advantageous because it allows for the detection of numerous internal failures that would otherwise go unnoticed by traditional monitoring methods. This paper details a radial basis function neural network architecture that is capable of inferring metallic state from a given plume spectrum. Specifically, a comprehensive discussion of the methodologies necessary for the development and implementation of the neural network approach are provided. The resulting neural networks are validated with actual test-stand data from an actual Space Shuttle Main Engine at NASA’s Stennis Space Center. Nomenclature y = desired function for neural approximation x = neural network input feature pattern gj = Gaussian kernel function µj = kernel function center position σj = kernel function spread constant w = optimal array of network weighting coefficients G = array of Gaussian kernel responses to input patterns Ii = vector of spectral intensities, (Watts•cm-2•str-1•nm-1",Analyzing Rocket Plume Spectral Data with Neural Networks for Condition Monitoring,,,,,core
195551950,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob","Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",,,HAL CCSD,,core
296636960,2015-11-26T14:50:22Z,"Usability and accessibility have been guiding interaction designers in their endeavor for better user interfaces. Recently, emotions and human values are gaining space, demanding new directions for the design of systems. As an emerging subject, the need for models that associate emotions and affective quality with human-computer interaction design is still to be addressed. This paper presents a literature review and critical analysis about existing frameworks and models, organizing the state of the art in emotional quality, towards educational system design. This review puts in evidence the missing core in the subject. We then present the concept of Affectibility as the guiding point from which the use and design of system-user interaction could be treated. Similar to the concepts of Learnability and Playability, Affectibility refers to the aspects that make the system of good-or bad-affective, emotional and hedonic qualities, potentially evoking certain affective responses in the users. © 2011 IEEE.7279Alsmeyer, M., Luckin, R., Good, J., Developing a novel interface for capturing self reports of affect Proc. of CHI 08, pp. 2883-2888. , Florence, ItalyAntle, A.N., The CTI framework: Informing the design of tangible systems for children (2007) TEI'07: First International Conference on Tangible and Embedded Interaction, pp. 195-202. , DOI 10.1145/1226969.1227010, TEI'07: First International Conference on Tangible and Embedded Interaction - Conference ProceedingsBarendregt, W., Bekker, M.M., Towards a framework for design guidelines for young children's computer games (2004) Proc. of the 2004 ICEC Conference, pp. 365-376. , Eindhoven, The NetherlandsBoehner, K., DePaula, R., Dourish, P., Sengers, P., How emotion is made and measured (2007) International Journal of Human Computer Studies, 65 (4), pp. 275-291. , DOI 10.1016/j.ijhcs.2006.11.016, PII S1071581906001844Bødker, S., When second wave HCI meets third wave challenges Proc. of the 4th Nordic Conference on Human-Computer Interaction: Changing Roles, pp. 1-8. , ACM, New York, NYCastells, M., (2000) The Rise of the Network Society, the Information Age: Economy, Society and Culture, 1. , Cambridge, MAOxford, UK: BlackwellChorianopoulos, K., Spinellis, D., User interface evaluation of interactive TV: A media studies perspective (2006) Universal Access in the Information Society, 5 (2), pp. 209-218. , Springer, Heildelberghttp://dictionary.reference.com, Accessed in May, 2011Drucker, P., Sociedade P6s-capitalista (1993) Pi One Ira, Sao Paulo. Translated from the Original ""the Post-Capitalist SocietyDruin, A., The role of children in the design of new technology (2002) Behaviour and Information Technology, 21 (1), pp. 1-25Friedman, B., Kahn Jr., P.H., Borning, A., Value sensitive design and information systems (2006) Human-Computer Interaction and Management Information Systems: Foundations, pp. 348-372. , M.E. Sharpe, Armonk, N.YFurtado, E., Furtado, V., Vaconcelos, E., A conceptual framework for the design and evaluation of affective usability in educational geosimulation systems (2007) Proc. of INTERACT 2007, Rio de Janeiro, pp. 497-510Gardner, H., (1983) Frames of Mind, , New York, New York, NY: Basic BooksGelderblom, H., Kotze, P., Designing technology for young children: What we can learn from theories of cognitive development (2008) Proc. of SAICSIT'08, pp. 66-75. , Wilderness, South AfricaGorguen, J.A., Semiotics, compassion and value-centered design (2005) Virtual, Distributed and Flexible Organisations-Studies in Organisational Semiotics, , SpringerHarrison, S., Tatar, D., Senger, P., The three paradigms of HCI (2007) Extended Abstracts CHIHassenzahl, M., Beu, A., Burmester, M., Engineering joy IEEE Software, 18 (1), pp. 70-76Hayashi, E.C.S., Neris, V.P.A., Martins, M.C., Baranauskas, M.C.C., Piccolo, L.S., Costa, R., Avaliando a qualidade afetiva de sistemas computacionais interativos no cemirio brasileiro (2009) Usabilidade, Acessibilidade e Inteligibilidade Aplicadas em Interfaces Para Analfabetos, Idosos e Pessoas Com Deficiencia-Resultados Do Workshop. Campinas: Funda9ao CPqD, 1, pp. 55-62Höök, K., User-centered design and evaluation of affective interfaces (2004) From Brows to Trust, pp. 127-160. , Z. Ruttkay and C. Pelachaud (eds.)Hoonhout, J., Let's start to create a fun product: Where is my toolbox? (2008) User Experience Evaluation Methods in Product Development Workshop in CH1'08, , Florence, Italy, AprilIp, H.H.S., Kwong, B., A conceptual framework of affective context-aware interactive learning media (2007) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 4469, pp. 391-400. , Technologies for E-Learning and Digital Entertainment - Second International Conference, Edutainment 2007, ProceedingsIsbister, K., Hook, K., Sharp, M., Laaksolahti, J., The sensual evaluation instrument: Developing an affective evaluation tool (2006) Conference on Human Factors in Computing Systems - Proceedings, 2, pp. 1163-1172. , CHI 2006: Conference on Human Factors in Computing Systems, Conference Proceedings SIGCHIKhan, M.M., Ward, R.D., Ingleby, M., Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature (2009) ACM Trans. Appl. Percpt., 6 (1). , Article 6 (February 2009), 22 pagesKleinginna, P.R., Kleinginna, A.M., A categorized list of emotion definitions, with suggestions for a consensual definition (1981) Motivatioin and Emotion, 5, pp. 345-379Kort, B., Reilly, R., Picard, R.W., An Affective Model of Interplay between Emotions and Learning: Reengineering Educational Pedagogy-Building a Learning Companion (2001) Proc. of ICALT 2001, Madison WisconsinLang, P.J., Bradley, M.M., Cuthbert, B.N., (2005) International Affective Picture System (LAPS): Instruction Manual and Affective Ratings, , Technical Report A-6, University of FloridaLeung, E., Underwood, J., Abstract affordance and affect in promotional websites (2007) Proc. of ACIS, pp. 1120-1131Liu, H., Lieberman, H., Selker, T., A model of textual affect sensing using real-world knowledge (2003) ACM Conference on Intelligent User Interfaces, pp. 125-132. , January 2003, MiamiLim, C.P., A theoretical frarnework for the study of ICT in schools: A proposal (2002) British Journal of Educational Technology, 33 (4), pp. 411-421Mandryk, R.L., Atkins, M.S., Inkpen, K.M., A continuous and objective evaluation of emotional experience with interactive play environments (2006) Conference on Human Factors in Computing Systems - Proceedings, 2, pp. 1027-1036. , CHI 2006: Conference on Human Factors in Computing Systems, Conference Proceedings SIGCHIMelo, A.M., Baranauskas, M.C.C., Soares, Design com crianvas: Da pnitica a urn modelo de processo (2008) Revista Brasileira de Informatica Na Educav1io, 16 (1), pp. 43-55Miranda, L., Arniel, T., Arantes, F., Martins, F., Baranauskas, M.C.C., Formação de professores em cenarios contextualizados no âmbito do projeto XO da UNICAMP (2010) Memo NIED UNICAMP N., 1. , in portugueseNeris, V.P.A., Almeida, L.D., Miranda, L.C., Hayashi, E.C.S., Baranauskas, M.C.C., Collective construction of meaning and system for an inclusive social network Approved to Be Published in the International Journal of Information Systems and Social Change (LJISSC). ISSN: 1941-868X, , to be publishedNorman, D., (2004) Emotional Design: Why We Love (Or Hate) Everyday Things, , New York: Basic BooksO'Brien, H.L., Toms, E.G., Engagement as process in computer-mediated environments (2005) Proc. of ASIS&T. Charlotte, North Carolinahttp://one.laptop.org, Accessed in May, 2011Picard, R.W., (1997) Affective Computing, , MIT Press, CarnbridgePicard, R.W., Bender, W., Blumberg, B., Breazeal, C., Cavallo, D., MacHover, T., Resnick, M., Strohecker, C., Affective learning-A manifesto (2004) BT Technology Journal, 22 (4). , OctoberPiccolo, L.S.G., Hayashi, E., Baranauskas, M.C.C., The evaluation of affective quality in social software: Preliminary thoughts (2010) 2nd WAIHCWSPlutchik, R., Emotions and life: Perspectives from psychology, biology, and evolution (2002) M: APA American Psychological Association), , I. ed., novhttp://www.premo-online.com, last access: Oct., 2010Ravaja, N., Saari, T., Turpeinen, M., Laarni, J., Salminen, M., Kivikangas, M., Spatial presence and emotions during video game playing: Does it matter with whom you play? (2006) Presence: Teleoperators and Virtual Environments, 15 (4), pp. 381-392. , http://www.mitpressjournals.org/doi/pdf/10.1162/pres.15.4.381, DOI 10.1162/pres.15.4.381Romani, R., Baranauskas, M.C.C., GWIDO-Games with interaction design objective Proc. of IADIS Internetional Journal on WWW/lnternet, 8, p. 1Russell, A.J., Affect grid: A single-item scale of pleasure and arousal (1989) Journal of Personality and Social Psychology, 57 (3), pp. 493-502Scherer, K., What are emotions? and how can they be measured? (2005) Social Science Information, 44 (1), p. 695. , SAGE PublicationsSellen, A., Rogers, Y., Harper, R., Rodden, T., Reflecting human values in the digital age (2009) Commun ACM, 52 (3), pp. 58-66. , MarSluis-Thiescheffer, R.J.W., Bekker, M.M., Eggen, J.H., Vermeeren, A.P.O.S., Ridder, H., Development and application of a framework for comparing early design methods for young children (2011) Interacting with Computers, 23, p. 1Zhang, P., Li, N.A., The importance of affective quality (2005) Communications of the ACM, 48 (9), pp. 105-108. , DOI 10.1145/1081992.1081997Zheng, S., Bromage, A., Adam, M., Scrivener, S.A.R., Surprising creativity: A cognitive framework for interactive exhibits designed for children (2007) Proc. of the 6th ACM SIGCHI Conference on Creativity & Cognition, , Washington, DC, USAZaman, B., Abeele, V.V., Towards a Likeability Framework that meets Child-Computer Interaction & Communication Sciences (2007) Proc. of the 6th International Conference on Interaction Design and ChildrenGiannakopoulos, T., Pikrakis, A., Theodoridis, S., A dimensional approach to emotion recognition of speech from movies (2009) Proc. of ICASSP IEEE International Conference, Taipei, pp. 65-6",Towards A Framework For The Affective And Emotional Faces Of Usability,10.1145/1081992.1081997,,,,core
296641560,2015-11-26T14:58:13,"With the growing use of biometric authentication systems in the past years, spoof fingerprint detection has become increasingly important. In this work, we implement and evaluate two different feature extraction techniques for software-based fingerprint liveness detection: Convolutional Networks with random weights and Local Binary Patterns. Both techniques were used in conjunction with a Support Vector Machine (SVM) classifier. Dataset Augmentation was used to increase classifier's performance and a variety of preprocessing operations were tested, such as frequency filtering, contrast equalization, and region of interest filtering. The experiments were made on the datasets used in The Liveness Detection Competition of years 2009, 2011 and 2013, which comprise almost 50,000 real and fake fingerprints' images. Our best method achieves an overall rate of 95.2% of correctly classified samples - an improvement of 35% in test error when compared with the best previously published results.2229IEEE Italy SectionGalbally, J., Alonso-Fernandez, F., Fierrez, J., Ortega-Garcia, J., A high performance fingerprint liveness detection method based on quality related features (2012) Future Generation Computer Systems, 28 (1), pp. 311-321Wiehe, A., Søndrol, T., Olsen, O.K., Skarderud, F., (2004) Attacking Fingerprint Sensors, , Gjøvik University CollegeChen, Y., Jain, A., Fingerprint deformation for spoof detection (2005) Proc. IEEE Biometric Symposium, pp. 19-21Tan, B., Schuckers, S., Comparison of ridge-and intensitybased perspiration liveness detection methods in fingerprint scanners (2006) Defense and Security Symposium International Society for Optics and Photonics, pp. 62020A-62020AColi, P., Marcialis, G.L., Roli, F., Fingerprint silicon replicas: Static and dynamic features for vitality detection using an optical capture device (2008) International Journal of Image and Graphics, 8 (4), pp. 495-512Lapsley, P., Lee, J., Pare, D., Hoffman, N., (1998) Anti-fraud Biometric Scanner That Accurately Detects Blood FlowAntonelli, A., Cappelli, R., Maio, D., Maltoni, D., Fake finger detection by skin distortion analysis (2006) Information Forensics and Security, 1 (3), pp. 360-373Baldisserra, D., Franco, A., Maio, D., Maltoni, D., Fake fingerprint detection by odor analysis (2005) Advances in Biometrics, pp. 265-272. , Berlin Heidelberg, SpringerMarcialis, G.L., Lewicke, A., Tan, B., Coli, P., Grimberg, D., Congiu, A., Schuckers, S., First international fingerprint liveness detection competition-livdet 2009 (2009) Image Analysis and Processing-ICIAP 2009, pp. 12-23Jain, A.K., Chen, Y., Demirku, M., Pores and ridges: Highresolution fingerprint matching using level 3 features (2007) Pattern Analysis and Machine Intelligence, 29 (1), pp. 15-27Gragnaniello, D., Poggi, G., Sansone, C., Verdoliva, L., Fingerprint liveness detection based on weber local image descriptor (2013) IEEE Workshop on Biometric Measurements and Systems for Security and Medical ApplicationsGragnaniello, D., Poggi, G., Sansone, C., Verdoliva, L., Local contrast phase descriptor for fingerprint liveness detection (2014) Pattern Recognition, , 9 JunJia, X., Yang, X., Cao, K., Zang, Y., Zhang, N., Dai, R., Tian, J., Multi-scale local binary pattern with filters for spoof fingerprint detection (2013) Information SciencesGhiani, L., Marcialis, G.L., Roli, F., Fingerprint liveness detection by local phase quantization (2012) Proc. IEEE Int. Conf. on Pattern Recognition (ICPR)Nikam, S.B., Agarwal, S., (2008) Local Binary Pattern and Wavelet-based Spoof Fingerprint Detection, 1, pp. 141-159Ghiani, L., Hadid, A., Marcialis, G.L., Roli, F., Fingerprint liveness detection using binarized statistical image features (2013) Proc. IEEE Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS)Simard, P.Y., Steinkraus, D., Platt, J.C., Best practices for convolutional neural networks applied to visual document analysis (2013) ICDAR, 3, pp. 958-962Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2012) NIPS, 1 (2)Dietterich, T.G., Approximate statistical tests for comparing supervised classification learning algorithms (1998) Neural Computation, 10 (7), pp. 1895-1923Zimmerman, J.B., Pizer, S.M., Staab, E.V., Perry, J.R., McCartney, W., Brenton, B.C., An evaluation of the effectiveness of adaptive histogram equalization for contrast enhancement (1988) Medical Imaging, IEEE Transactions on, 7 (4), pp. 304-312Pizer, S.M., Amburn, E.P., Austin, J.D., Cromartie, R., Geselowitz, A., Greer, T., Zuiderveld, K., Adaptive histogram equalization and its variations (1987) Computer Vision, Graphics, and Image Processing, pp. 355-368Lecun, Y., Generalization and network design strategies (1989) Connections in PerspectiveWan, L., Zeiler, M., Zhang, S., Cun, Y.L., Fergus, R., Regularization of neural networks using dropconnect (2013) Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 1058-1066Goodfellow, I.J., Warde-Farley, D., Mirza, M., Courville, A., Bengio, Y., (2013) Maxout Networks, , arXiv preprint arXiv: 1302. 4389Nair, V., Hinton, G.E., Rectified linear units improve restricted Boltzmann machines (2010) Proceedings of the 27th International Conference on Machine Learning (ICML-10)Boureau, Y.-L., Ponce, J., Lecun, Y., A theoretical analysis of feature pooling in visual recognition (2010) Proceedings of the 27th International Conference on Machine Learning (ICML-10)Zeiler, M.D., Fergus, R., (2013) Stochastic Pooling for Regularization of Deep Convolutional Neural Networks, , arXiv preprint arXiv: 1301. 3557Hinton, G.E., Osindero, S., Teh, Y.-W., A fast learning algorithm for deep belief nets (2006) Neural Computation, 18 (7), pp. 1527-1554Lecun, Y., Kavukcuoglu, K., Farabet, C., Convolutional networks and applications in vision (2010) Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pp. 253-256. , MayJarrett, K., Kavukcuoglu, K., Ranzato, M., Lecun, Y., What is the best multi-stage architecture for object recognition? (2009) Computer Vision, 2009 IEEE 12th International Conference on, pp. 2146-2153. , SeptemberSaxe, A., Koh, P.W., Chen, Z., Bhand, M., Suresh, B., Ng, A.Y., On random weights and unsupervised feature learning (2011) Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 1089-1096Lyu, S., Simoncelli, E.P., Nonlinear image representation using divisive normalization (2008) Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference On. IEEEPinto, N., Cox, D.D., Dicarlo, J.J., Why is real-world visual object recognition hard? (2008) PLoS Computational Biology, 4 (1-27)Lyu, S., Divisive normalization: Justification and effectiveness as efficient coding transform (2010) NIPSHadid, A., Pietikainen, M., Ahonen, T., A discriminative feature space for detecting and recognizing faces (2004) Computer Vision and Pattern RecognitionOjala, T., Pietikäinen, M., Mäenpää, T., Multiresolution gray scale and rotation invariant texture analysis with local binary patterns (2002) IEEE Trans. Pattern Anal. Mach. Intell, 24 (7), pp. 971-987. , JulAhonen, T., Hadid, A., Pietikäinen, M., Face recognition with local binary patterns (2004) Computer Vision-eccv, pp. 469-481Halko, N., Martinsson, P.-G., Tropp, J.A., Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions (2011) SIAM Review, 53 (2), pp. 217-288Martinsson, P.-G., Rokhlin, V., Tygert, M., A randomized algorithm for the decomposition of matrices (2011) Applied and Computational Harmonic Analysis, 30 (1), pp. 47-68Hyvärinen, A., Hurri, J., Hoyer, P., Principal components and whitening (2009) Natural Image Statistics, pp. 93-130. , London, Springerhttp://ufldl.stanford.edu/wiki/index.php/Whitening, [Accessed 08 03 2014]Coates, A., Ng, A.Y., Lee, H., An analysis of single-layer networks in unsupervised feature learning (2011) International Conference on Artificial Intelligence and StatisticsCao, L.J., Chua, K.S., Chong, W.K., Lee, H.P., Gu, Q.M., A comparison of pca, kpca and ica for dimensionality reduction in support vector machine (2003) Neurocomputing, 55 (1), pp. 321-336Lei, H., Govindaraju, V., Speeding up multi-class svm evaluation by pca and feature selection (2005) Feature Selection for Data Mining, p. 72Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2012) NIPS, 1 (2)Cireşan, D.C., Meier, U., Masci, J., Gambardella, L.M., Schmidhuber, J., (2011) High-performance Neural Networks for Visual Object Classification, , arXiv: 1102. 0183Ciresan, D., Meier, U., Schmidhuber, J., Multi-column deep neural networks for image classification (2012) Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3642-3649. , Junehttps://github.com/giovanichiachia/convnet-rfw, G. Chiachia. [Accessed 17 05 2014]Yambay, D., Ghiani, L., Denti, P., Marcialis, G.L., Roli, F., Schuckers, S., LivDet 2011-fingerprint liveness detection competition 2011 (2012) Biometrics (ICB), 2012 5th IAPR International Conference on, pp. 208-215Ghiani, L., Yambay, D., Mura, V., Tocco, S., Marcialis, G.L., Roli, F., Schuckcrs, S., LivDet 2013 fingerprint liveness detection competition 2013 (2013) Biometrics (ICB), 2013 International Conference on, pp. 1-",Evaluating Software-based Fingerprint Liveness Detection Using Convolutional Networks And Local Binary Patterns,10.1109/BIOMS.2014.6951531,,'Institute of Electrical and Electronics Engineers (IEEE)',,core
296199376,2016,"A key issue in designing robotics systems is the cost of an integrated camera sensor that meets the bandwidth/processing requirement for many advanced robotics applications, especially lightweight robotics applications, such as visual surveillance or SLAM in autonomous aerial vehicles. There is currently much work going on to adapt smartphones to provide complete robot vision systems, as the phone is so exquisitely integrated having camera(s), inertial sensing, sound I/O and excellent wireless connectivity. Mass market production makes this a very low-cost platform and manufacturers from quadrotor drone suppliers to children’s toys, such as the Meccanoid robot, employ a smartphone to provide a vision system/control system.



Accordingly, many research groups are attempting to optimise image analysis, computer vision and machine learning libraries for the smartphone platform. However current approaches to robot vision remain highly demanding for mobile processors such as the ARM, and while a number of algorithms have been developed, these are very stripped down, i.e. highly compromised in function or performance For example, the semi-dense visual odometry implementation of [1] operates on images of only 320x240pixels.



In our research we have been developing biologically motivated foveated vision algorithms, potentially some 100 times more efficient than their conventional counterparts, based on a model of the mammalian retina we have developed. Vision systems based on the foveated architectures found in mammals have the potential to reduce bandwidth and processing requirements by about x100 - it has been estimated that our brains would weigh ~60Kg if we were to process all our visual input at uniform high resolution. We have reported a foveated visual architecture that implements a functional model of the retina-visual cortex to produce feature vectors that can be matched/classified using conventional methods, or indeed could be adapted to employ Deep Convolutional Neural Nets for the classification/interpretation stage, [2,3,4]. 



We are now at the early stages of investigating how best to port our foveated architecture onto a smartphone platform. To achieve the required levels of performance we propose to optimise our retina model to the ARM processors utilised in smartphones, in conjunction with their integrated GPUs, to provide a foveated smart vision system on a smartphone. Our current goal is to have a foveated system running in real-time to serve as a front-end robot sensor for tasks such as general purpose object recognition and reliable dense SLAM using a commercial off-the-shelf smartphone which communicates with conventional hardware performing back-end visual classification/interpretation. We believe that, as in Nature, space-variance is the key to achieving the necessary data reduction to be able to implement the complete visual processing chain on the smartphone itself",A Biologically Motivated Software Retina for Robotic Sensors Based on Smartphone Technology,,,,,core
102706186,2015-09-04,"Comparative dependency network learning is a growing field of research, especially in systems biology. Domain scientists would like to discover patterns of variable dependency that are conserved across conditions or discover pathways that are disrupted due to disease. In machine learning, multitask graphical structure learning algorithms have been developed to help solve this problem by learning network models from multiple related datasets. These algorithms typically have regularization hyper-parameters that have the effect of re-ducing the number of spurious edges learned and the num-ber of spurious differences learned. We propose a mecha-nism to allow the end-user to control these regularization hyper-parameters in real-time to interactively explore the huge space of potential dependency network solutions. This is a critical element of a visualization system that enables domain scientists to discover interesting patterns in mul-tivariate data. Yet, this is a computationally challenging endeavor as complex models must be learned in real-time and, additionally the number of differences learned in each network and the number of differences between them must be translated by the machine learning algorithm into the correct change in the setting of the hyper-parameters. This paper introduces a general framework for interactively ex-ploring the similarities and differences among a set of depen-dency networks and demonstrates our work-in-progress on a specific implementation for multiple Bayesian networks. 1",Interactive Exploration of Comparative Dependency Network Learning,,,,,core
140329993,2016-03-06,"2016-03-07Various types of controllers have been studied and implemented to mitigate the effects of excitations from natural hazards. Linear control laws are most often applied, for active devices as well as part of the controller for semiactive devices, primarily due to their simple design and implementation. Yet, linear structural control strategies generally cannot focus on different objectives in different excitations, a goal that has real meaning in many structural control applications. For example, minimizing structural drift is necessary during strong earthquakes to mitigate damage to the structure, yet occupant comfort and safety of building contents in the much-more-frequent moderate earthquakes demands reductions in absolute accelerations in the structure. Minimizing drift and acceleration in different excitations are often somewhat competing objectives and cannot be achieved with conventional linear strategies. Therefore, it is imperative to use nonlinear control strategies instead. To design an optimal nonlinear controller for a linear structure, it is well established that minimization of a nonquadratic cost function is required, which will lead to solving the Hamilton-Jacobi-Bellman (HJB) partial differential equation. However, finding the exact analytical solution of the HJB equation is very difficult, and may only have a solution when the cost function is cast in a particular form. ❧ The first part of this study presents a comparison of some of the analytical and numerical methods for finding optimal nonlinear controllers, particularly for cost functions that are even-order powers of the states and quadratic in the control. A scalar model (i.e., a scalar state-space equation) is used for the comparisons since analytical solutions exist for some of the methods. ❧ As most of the HJB-derived optimal nonlinear controllers proposed by other researchers are focused on the single objective of reducing structural drift, the next part of this study presents an analytical approach to solve the HJB equation and find the optimal nonlinear control law for a nonquadratic cost function with higher-order polynomials. The resulting optimal nonlinear control law can be formulated as the summation of a linear term, which is related to a linear quadratic regulator (LQR) problem designed to be effective in reducing one performance metric in small excitations, and some nonlinear terms that are dominant to reduce the other performance metric when the excitation level is large. Thus, it can achieve significant improvements in both serviceability (i.e., acceleration mitigation) in weak and moderate excitations and life safety goals (i.e., drift mitigation) in extreme events—all in one general control law. ❧ Next, the application of gain scheduling to achieve the two aforementioned objectives is investigated. Two different optimal linear controls are designed, each to reduce a different response metric; scheduling and switching between the two controllers, depending on the strength of earthquake, is used to minimize both objectives. Numerical simulations show that this approach can successfully achieve both life safety (drift reduction) and serviceability (acceleration reduction) objectives at different excitation levels. ❧ The next part of this study presents a nonlinear control strategy, based on model predictive control (MPC), to minimize a nonquadratic cost function with higher-order polynomials. The nonquadratic term in the cost function is defined such that it becomes negligible in weak excitations, resulting the same performance as a LQR problem designed for acceleration reduction, but dominant in strong excitations, resulting in a superior drift reduction compared with that of corresponding LQR problem. ❧ For systems that exhibit dynamics that are most naturally expressed with a hybrid system model containing both continuous and discrete states, such as structures controlled with smart dampers, hybrid model predictive control is a useful design strategy. Further, this study investigates an offline implementation of hybrid MPC that is based on neural networks, which may be generalizable to controllable damping of more complex structures. The aim is to examine whether nonlinear regression using neural networks is suitable for modeling the control laws obtained from hybrid MPC. Intensive numerical simulations for both SDOF and 2DOF structural models show that a NN with many neurons can indeed accurately replicate the MPC control function. However, a NN with only a few neurons in the hidden layer can only grossly approximate the MPC control function; nevertheless, the few-neuron NN is still capable of achieving a cost function similar to, and even slightly better than, that provided by the MPC. ❧ Finally, the last part of this dissertation presents concluding remarks on the various methods developed in this dissertation for obtaining optimal nonlinear control strategies that can achieve different objectives in different levels of excitation to provide, in one control strategy, both optimal serviceability performance in the frequent weak-to-moderate events and optimal life safety in the infrequent extreme events. It has been demonstrated via intensive numerical simulations that all nonlinear control strategies studied in this research (i.e., HJB, GS and MPC) can successfully achieve both life safety (drift reduction) and serviceability (acceleration reduction) objectives at different excitation levels; however, it should be noted that, due to different optimization techniques to find the nonlinear controller, their results are not identical. The last chapter also presents avenues for future work and exploration of the subject, and highlights the areas needing further investigation and study to improve and complete the results of this research. The first steps to extend this study are to choose the weak and strong excitation levels and the desired building performance (e.g., drift, acceleration, etc) based on characteristics of the building and its contents, the history of previous excitations at the location, the building code and so forth. Moreover, all of the nonlinear control approaches must be evaluated based on the ease of their application to the variety of models that civil engineers use, so it is absolutely essential to verify the performance of the nonlinear control strategy in a laboratory experiments. Considering more than one control device in different levels of the structure is also necessary to extend the results of this study",Structural nonlinear control strategies to provide life safety and serviceability,,,University of Southern California. Libraries,,core
73394660,2016-10-16T00:00:00,"With the advancement of huge data generation and data handling capability,
Machine Learning and Probabilistic modelling enables an immense opportunity to
employ predictive analytics platform in high security critical industries
namely data centers, electricity grids, utilities, airport etc. where downtime
minimization is one of the primary objectives. This paper proposes a novel,
complete architecture of an intelligent predictive analytics platform, Fault
Engine, for huge device network connected with electrical/information flow.
Three unique modules, here proposed, seamlessly integrate with available
technology stack of data handling and connect with middleware to produce online
intelligent prediction in critical failure scenarios. The Markov Failure module
predicts the severity of a failure along with survival probability of a device
at any given instances. The Root Cause Analysis model indicates probable
devices as potential root cause employing Bayesian probability assignment and
topological sort. Finally, a community detection algorithm produces correlated
clusters of device in terms of failure probability which will further narrow
down the search space of finding route cause. The whole Engine has been tested
with different size of network with simulated failure environments and shows
its potential to be scalable in real-time implementation.Comment: Accepted in 4th International Conference on Business Analytics and
  Intelligence (ICBAI 2016","Fault Detection Engine in Intelligent Predictive Analytics Platform for
  DCIM",,http://arxiv.org/abs/1610.04872,,,core
101081333,2015-01-09,"Abstract—Intelligent systems based on machine learning techniques, such as classification, clustering, are gaining wide spread popularity in real world applications. This paper presents work on developing a software system for predicting crop yield, for example oil-palm yield, from climate and plantation data. At the core of our system is a method for unsupervised partitioning of data for finding spatio-temporal patterns in climate data using kernel methods which offer strength to deal with complex data. This work gets inspiration from the notion that a non-linear data transformation into some high dimensional feature space increases the possibility of linear separability of the patterns in the transformed space. Therefore, it simplifies exploration of the associated structure in the data. Kernel methods implicitly perform a non-linear mapping of the input data into a high dimensional feature space by replacing the inner products with an appropriate positive definite function. In this paper we present a robust weighted kernel k-means algorithm incorporating spatial constraints for clustering the data. The proposed algorithm can effectively handle noise, outliers and auto-correlation in the spatial data, for effective and efficient data analysis by exploring patterns and structures in the data, and thus can be used for predicting oil-palm yield by analyzing various factors affecting the yield. Keywords—Pattern analysis, clustering, kernel methods, spatial data, crop yiel",A Framework for Predicting Oil-Palm Yield from Climate Data,,,,,core
101409309,2015-02-06,"Abstract: This paper describes parallel histogram modification techniques with embedded morphological preprocessing methods within the CNN-UM framework. The procedure is formulated in terms of nonlinear partial differential equations (PDE) and approximated through finite differences in space, resulting in coupled nonlinear ordinary differential equations (ODE). The I/O mapping of the system (containing both local and global couplings) can be calculated by a complex analogic (analog and logic) algorithm executed on a stored program nonlinear array processor, called the cellular nonlinear network universal machine (CNN-UM, [3]). We describe and illustrate how the implementation of the algorithm results in an adaptive multi-thresholding scheme when histogram modification is combined with embedded morphological processing at a finite (low) number of gray-scale levels. This has obvious advantages if the further processing steps are segmentation and/or recognition. Experimental results processing real-life and echocardiography images are measured on different hardware/software platforms, including a 64x64 CNN-UM chip (ACE4k, [6], [17]). Key words: cellular nonlinear/neural networks (CNN), analogic algorithms, CNN Universal Machine, histogram modification, morphological processing, partial differentia",2PDE BASED HISTOGRAM MODIFICATION WITH EMBEDDED MORPHOLOGICAL,,,,,core
103319765,2015-11-14,"This paper deals with the design and implementation of Maximum Likelihood (ML) Estimation method for a MIMO system utilising Space-time block coding (STBC)  based on Alamouti transmitter diversity scheme. The design is modelled using MATLAB to derive the required specifications. The code is simulated, synthesized and implemented on Test bed for real-time analysis. The performance of STBC- MIMO is evaluated on the basis of Bit Error Rate (BER) and Symbol Error Rate (SER) for different types of modulations",Test-Bed,,,,,core
103551285,2016-01-06,"Abstract—Fine-grained real-time metering is a fundamental service of wireless energy auditing networks, where metering data is transmitted from embedded power meters to gateways for centralized processing, storage, and forwarding. Due to limited meter capability and wireless bandwidth, the increasing sampling rates and network scales needed to support new energy auditing applications pose significant challenges to metering data fidelity and secrecy. This paper exploits the compression and encryption properties of compressive sensing (CS) to design a joint data compression and encryption (JICE) approach that addresses these two challenges simultaneously. Compared with a conventional signal processing pipeline that compresses and encrypts data sequentially, JICE reduces computation and storage complexities due to its simple design. It thus leaves more processor time and available buffer space for handling lossy wireless transmissions. Moreover, JICE features a machine-learning-based reconfigu-ration mechanism that adapts its signal representation basis to changing power patterns autonomously. On a smart plug platform, we implemented JICE and several baseline approaches including downsampling, lossless compression, and the pipeline approach. Extensive testbed experiments show that JICE achieves higher data delivery ratios and lower recovery distortions under a range of realistic settings. In particular, JICE increases the number of meters supported by a gateway by 50%, compared with the pipeline approach, while keeping a distortion rate lower than 5%. I",JICE: Joint Data Compression and Encryption for Wireless Energy Auditing Networks,,,,,core
103532217,2016-01-06,"Abstract—Modern simulations feature crowds of AI-controlled agents moving through dynamic environments, with obstacles appearing or disappearing at run-time. A dynamic navigation mesh can represent the traversable space of such en-vironments. The A * algorithm computes optimal paths through the dual graph of this mesh. When an obstacle is inserted or deleted, the mesh changes and agents should re-plan their paths. Many existing re-planning algorithms are too memory-intensive for crowds, or they cannot easily be used on graphs where vertices and edges are added or removed. In this paper, we present Dynamically Pruned A * (DPA*), an extension of A * for re-planning optimal paths in dynamic navi-gation meshes. DPA * has similarities to adaptive algorithms that make the A * heuristic more informed based on previous queries. However, DPA * prunes the search using only the previous path and its relation to the dynamic event. We describe the four re-planning scenarios that can occur; DPA * uses different rules in each scenario. Our algorithm is memory-friendly and robust against structural changes in the graph, which makes it suitable for crowds in dynamic navigation meshes. Experiments show that DPA * performs particularly well in complex environments and when the dynamic event is visible to the agent. We integrate the algorithm into crowd simulation software to model large crowds in dynamic environments in real-time. I",Dynamically Pruned A * for Re-planning in Navigation Meshes,,,,,core
103828307,2016-01-20,"In real-world applications, “what you saw ” during training is often not “what you get ” during deployment: the distribution and even the type and dimensionality of fea-tures can change from one dataset to the next. In this pa-per, we address the problem of visual domain adaptation for transferring object models from one dataset or visual domain to another. We introduce ARC-t, a flexible model for supervised learning of non-linear transformations be-tween domains. Our method is based on a novel theoreti-cal result demonstrating that such transformations can be learned in kernel space. Unlike existing work, our model is not restricted to symmetric transformations, nor to features of the same type and dimensionality, making it applicable to a significantly wider set of adaptation scenarios than pre-vious methods. Furthermore, the method can be applied to categories that were not available during training. We demonstrate the ability of our method to adapt object recog-nition models under a variety of situations, such as differing imaging conditions, feature types and codebooks. 1",What you saw is not what you get: Domain adaptation using asymmetric kernel transforms,,,,,core
56698369,2016-01-01T08:00:00,"Yearly increases in computer performance have diminished as of late, mostly due to the inability of transistors, the building blocks of computers, to deliver the same rate of performance seen in the 1980’s and 90’s. Shifting away from traditional CPU design, accelerator architectures have been shown to offer a potentially untapped solution. These architectures implement unique, custom hardware to increase the speed of certain tasking, such as graphics processing. The studies undertaken for this dissertation examine the ability of unique accelerator hardware to provide improved power and speed performance over traditional means, with an emphasis on classification tasking.
In the first study, the compression algorithm Lempel-Ziv-Oberhumer (LZO) 1x-1-15 is analyzed and documented. This algorithm family has seen widespread use and can be found in the NASA mars space rover and the B-tree Linux file system. A thorough analysis of the algorithm is seen to yield x86 vector and other CPU parallelization improvements that can be utilized for acceleration. Real-world datasets are used to concretely benchmark the improved performance.
The second study shifts the focus from CPU instruction acceleration to optimized hardware acceleration. A real-world embedded application of machine learning involving Support Vector Machine (SVM) accelerated hardware is examined. Prior work developed by URI’s Biomedical Engineering department investigated the use of a state-of-the-art SVM-based algorithm to control an artificial limb in real-time. Evaluation of the algorithm was performed using general processing means, using a Core i7 CPU and an Intel ATOM mobile CPU. This study builds on the prior work, investigating the performance advantages imparted by implementing the SVM decision function in hardware and combining it with a hardware-based feature extractor on a Field Programmable Gate Array (FPGA). The design is evaluated for both accuracy and real-time response to determine if the FPGA implementation is a better choice for implementation in a power-limited cyber physical system.
The third study examines the SVM classification portion of the FPGA design that was constructed for use in the artificial limb in further detail. A general purpose hardware architecture for fast, accurate SVM classification, R2SVM, is proposed. While several similar architectures have been published, our architecture is shown to be superior in a several ways. To prove the performance, accuracy, and power consumption of the architecture, a prototype is constructed and multiple machine learning datasets are run and analyzed.
The final study takes a look at the creation of a smart city architecture. A novel multi-tiered hierarchical architecture, Reflex Tree, is proposed as a solution to automated city management in the future. The four layers of the architecture are able to perform massive parallel sensing, pattern recognition, spatial-temporal association, and system-wide behavioral analysis. Like the human nervous system, each layer in the hierarchy is able to detect specific events and inject feedback without the need for higher level intervention. Simulations of the architecture are performed in two scenarios: a gas pipeline and a city power supply network",Research Into Computer Hardware Acceleration of Data Reduction and SVMS,,https://core.ac.uk/download/56698369.pdf,DigitalCommons@URI,,core
211484820,2016-01-01T00:00:00,"Concretely, guidance entails not navigation alone, but mainly the utilisation of the navigation solution for the maintenance of a moving vehicle course from one location to another.  Technically, it depends heavily on the trajectory determination in 3D space which in effect, it concerns with two aspects; firstly, location information expressed at a suitable coordinate system and secondly, geometric information that shapes the path of the moving vehicle in a formation of a sequence of mathematical curves.  Location information results directly from the navigation solution in a form of individual, consecutively distributed point fixes (position estimates), whereas geometric information can be extracted using various modelling approaches.  Notwithstanding position estimation can be performed in real time, trajectory geometry is essentially determined post-hoc, as a real time procedure would infer trajectory geometry identification requiring prior knowledge of the curvature along the path nonetheless.  In this frame, the following research venture discusses questions upon guidance, an interdisciplinary subject also known as the art of navigation.More specifically, it proposes a methodology to perform geometry identification of a vehicle trajectory in real time.  It is a work of synthesis of supervised machine learning and optimal state estimation for the real time classification of point fixes into appropriate geometric classes.  On the basis of a categorisation axiom derived from attitude estimation, a Least Squares Support Vector Machine (LS-SVM) for pattern recognition classifies sequential position estimates into linear elements (such as straight lines, simple circular curves or lines of a curvature variation).  Thus, the geometrical profile of the trajectory path is being identified while an Extended Kalman filter aboard a tightly coupled GNSS/INS system estimates vehicle kinematics.  The elemental outcome from the application of the proposed methodology is the statistical estimation with which consecutive point fixes are constrained to belong to the same class of a linear pattern beforehand.  Thence, the parametric equation of such a geometrically recognised line is simultaneously updated, as its fitting parameters are being recalculated after the classification of a successive point fix at the same class.The training scheme is designed on learning features created from the position-to-position time-series of attitude delta values.  Given that the value of the trajectory curvature is a priori unknown, the conception of this simple trick offers an implicit quantification of the curvature rate of change, delivering an intuitive solution to tackle this point at issue.  Inherently, a great single asset of the proposed methodology is centred upon the learning features the classifier is trained on; thus classification prediction is less influenced by the quality of the navigation solution.  Furthermore, prior knowledge of the geometric formation of a vehicle path implies knowledge of its trajectory smooth analytical expression; it implies a real time positioning smoothing, in other words.In order to verify generalisation performance potentialities for the learning algorithm, a point classification application was implemented using real-world navigational data collected along a 120 km suburban railway track connecting Athens with Kiato at the north of Peloponnesus (Greece).  According to the experimental results, two classifiers of different training length have both carried out a fine generalisation performance along unknown routes, recognising the linear pattern on any of the unseen point fixes at a level of 95%.  Moreover, the implementation of the “real time positioning smoothing” in conditions of bad perception/outage of satellite signal delivered some significant corrections with regard to a post processed reference trajectory.Η παρούσα εργασία πραγματεύεται το διεπιστημονικό ζήτημα του ελέγχου ενός δυναμικού συστήματος, όπως αυτό εμφανίζεται κατά την κίνηση επίγειου οχήματος. Αναπτύσσεται αλγόριθμος που συνθέτει μεθόδους επιβλεπόμενης μηχανικής μάθησης και βελτιστοποίησης της εκτίμησης κατάστασης συστήματος, για την πλοήγηση οχήματος σε κατάσταση καθοδήγησης.  Συγκεκριμένα, προτείνεται πρωτότυπη μεθοδολογία ταξινόμησης της θέσης εντοπισμού σε γραμμικά γεωμετρικά πρότυπα,  όπου με την εφαρμογή- κατά την εξέλιξη της κίνησης- ενός LS SVM μοντέλου για αναγνώριση προτύπου (pattern recognition), ταυτοποιείται το γεωμετρικό προφίλ του ίχνους της τροχιάς του οχήματος, σε πραγματικό χρόνο.Με βάση μια σειρά αξιωμάτων κατηγοριοποίησης, η προτεινόμενη μηχανή εφαρμόζει ένα σχέδιο ταξινόμησης επί των παρεχόμενων εκτιμήσεων των παραμέτρων της περιστροφικής συμπεριφοράς του σώματος του οχήματος.  Αυτές προκύπτουν από βελτιστοποίηση μετρήσεων αδρανειακής μονάδας τακτικού βαθμού, σε σχεδιασμό αυστηρής ολοκλήρωσης συστήματος GNSS/INS (tightly coupled integration).  Το αποτέλεσμα είναι η ταξινόμηση των διαδοχικών σημείων-εκτίμηση θέσης που συνθέτουν το ίχνος της τροχιάς, σε κατάλληλα γεωμετρικά πρότυπα.  Αυτά είναι γραμμικά στοιχεία όπως ευθείες γραμμές, κυκλικά τόξα και καμπύλες σύνθετου τύπου.  Η επιτυχία της μεθοδολογίας έγκειται στην αξιοποίηση των ιδιοτήτων που προκύπτουν από το συνδυασμό των εκτιμήσεων θέσης και γωνιακής συμπεριφοράς.  Βασικό αξίωμα κατηγοριοποίησης είναι ο ρυθμός αλλαγής της καμπυλότητας της τροχιάς, της οποίας η τιμή είναι άγνωστη κατά τη διάρκεια της κίνησης.  Η μεθοδολογία αξιολογήθηκε με χρήση πραγματικών δεδομένων, που προέκυψαν από πείραμα κατά μήκος της σιδηροδρομικής γραμμής Αθήνα-Κιάτο.  Με βάση τα πειραματικά αποτελέσματα, η προτεινόμενη μηχανή παρουσίασε πολύ υψηλά επίπεδα γενικευμένης απόδοσης κατά την εφαρμογή της σε άγνωστη διαδρομή, αναγνωρίζοντας το γραμμικό πρότυπο της τροχιάς σε επίπεδο 95% επί του συνόλου των σημείων-εκτίμηση θέσης που ταξινομήθηκαν σε κάποια γραμμική κλάση.Από την πρωτοτυπία της ταξινόμησης της θέσης εντοπισμού σε γραμμικά γεωμετρικά πρότυπα, προκύπτουν επιπλέον δυνατότητες καινοτομίας.Καθώς η μηχανή ταξινομεί τα σημεία εντοπισμού θέσης σε γραμμές γνωστής γεωμετρίας, ο αναδρομικός υπολογισμός της αναλυτικής έκφρασης του ίχνους της τροχιάς προσφέρει εκτιμήσεις γεωμετρικής εξομάλυνσης θέσης (position estimation smoothing) σε πραγματικό χρόνο.  Ως εξομάλυνση θέσης πραγματικού χρόνου, προτείνεται ο προσδιορισμός του σημείου επί της τρέχουσας γραμμής ταξινόμησης που απέχει την ελάχιστη απόσταση από την αντίστοιχη εκτίμηση θέσης.  Η εφαρμογή αυτής της καινοτομίας κάτω από συνθήκες με μεγάλο ερευνητικό ενδιαφέρον για την περιοχή της Πλοήγησης, όπως κακής λήψης δορυφορικού σήματος ή/και ολικής απώλειάς του, έδωσε κάποια πολύ ενδιαφέροντα αποτελέσματα, ως προς τις διορθώσεις που παρείχε στις εκτιμήσεις θέσης σε πραγματικό χρόνο.Τέλος, η προτεινόμενη μεθοδολογία βρήκε ουσιαστική εφαρμογή και στην αυτόματη εξαγωγή as-built γεωμετρικών στοιχείων χάραξης οδοποιίας, σε πραγματικό χρόνο, με ακριβή προσδιορισμό των σημείων εναλλαγής τους.  Με βάση την εκπαίδευσή της, η μηχανή απέδωσε το ελάχιστο εύρος εντοπισμού του - υλοποιημένου στο έδαφος - σημείου συνένωσης δύο διαδοχικών στοιχείων χάραξης σε ποσοστό 60%",Positioning state classification into linear patterns for guidance applications,10.12681/eadd/39181,,'National Documentation Centre (EKT)',,core
296755707,2016-06-03T20:15:36,"Nowadays, e-mail spam is not a novelty, but it is still an important problem with a high impact on the economy. Spam filtering poses a special problem in text categorization, in which the defining characteristic is that filters face an active adversary, which constantly attempts to evade filtering. In this paper, we present a novel approach to spam filtering based on a compression-based model. We have conducted an empirical experiment on eight public and real non-encoded datasets. The results indicate that the proposed filter is fast to construct, is incrementally updateable, and clearly outperforms established spam classifiers. © 2016 John Wiley & Sons, Ltd.94327335Cormack, G., Email spam filtering: a systematic review (2008) Foundations and Trends in Information Retrieval, 1 (4), pp. 335-455Joachims, T., A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization (1997) Proceedings of 14th International Conference on Machine Learning, pp. 143-151. , Nashville, TN, USASchapire, R., Singer, Y., Singhal, A., Boosting and Rocchio applied to text filtering (1998) Proceedings of the 21st Annual International Conference on Information Retrieval, pp. 215-223. , Melbourne, AustraliaCarreras, X., Marquez, L., Boosting trees for anti-spam email filtering (2001) Proceedings of the 4th International Conference on Recent Advances in Natural Language Processing, pp. 58-64. , Tzigov Chark, BulgariaReddy, C., Park, J.H., Multi-Resolution Boosting for Classification and Regression Problems (2011) Knowledge and Information Systems, 29 (2), pp. 435-456Sahami, M., Dumais, S., Hecherman, D., Horvitz, E., A bayesian approach to filtering junk e-mail (1998) Proceedings of the 15th National Conference on Artificial Intelligence, pp. 55-62. , Madison, WI, USAAndroutsopoulos, I., Koutsias, J., Chandrinos, K., Paliouras, G., Spyropoulos, C., An evalutation of Naive Bayesian anti-spam filtering (2000) Proceedings of the 11st European Conference on Machine Learning, pp. 9-17. , Barcelona, SpainSong, Y., Kolcz, A., Gilez, C., Better Naive Bayes classification for high-precision spam detection (2009) Software - Practice and Experience, 39 (11), pp. 1003-1024Almeida, T., Yamakami, A., Almeida, J., Evaluation of approaches for dimensionality reduction applied with Naive Bayes anti-spam filters (2009) Proceedings of the 8th IEEE International Conference on Machine Learning and Applications, pp. 517-522. , Miami, FL, USAAlmeida, T., Yamakami, A., Almeida, J., Probabilistic anti-spam filtering with dimensionality reduction (2010) Proceedings of the 25th ACM Symposium On Applied Computing, pp. 1804-1808. , Sierre, SwitzerlandAlmeida, T., Almeida, J., Yamakami, A., Spam filtering: how the dimensionality reduction affects the accuracy of Naive Bayes classifiers (2011) Journal of Internet Services and Applications, 1 (3), pp. 183-200Drucker, H., Wu, D., Vapnik, V., Support vector machines for spam categorization (1999) IEEE Transactions on Neural Networks, 10 (5), pp. 1048-1054. , SeptemberKolcz, A., Alspector, J., SVM-based filtering of e-mail spam with content-specific misclassification costs (2001) Proceedings of the 1st International Conference on Data Mining, pp. 1-14. , San Jose, CA, USAHidalgo, J., Evaluating cost-sensitive unsolicited bulk email categorization (2002) Proceedings of the 17th ACM Symposium on Applied Computing, pp. 615-620. , Madrid, SpainForman, G., Scholz, M., Rajaram, S., Feature shaping for linear SVM classifiers (2009) Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 299-308. , Paris, FranceAlmeida, T., Yamakami, A., Content-based spam filtering (2010) Proceedings of the 23rd IEEE International Joint Conference on Neural Networks, pp. 1-7. , Barcelona, SpainCarpinter, J., Hunt, R., Tightening the net: a review of current and next generation spam filtering tools (2006) Computers and Security, 25 (8), pp. 566-578Blanzieri, E., Bryl, A., A survey of learning-based techniques of email spam filtering (2008) Artificial Intelligence Review, 29 (1), pp. 335-455Guzella, T., Caminhas, W., A review of machine learning approaches to spam filtering (2009) Expert Systems with Applications, 36 (7), pp. 10 206-10 222. , SeptemberCaruana, G., Li, M., A survey of emerging approaches to spam filtering (2012) ACM Computing Surveys, 44 (2), pp. 1-27Rissanen, J., Modeling by shortest data description (1978) Automatica, 14, pp. 465-471Barron, A., Rissanen, J., Yu, B., The minimum description length principle in coding and modeling (1998) IEEE Transactions on Information Theory, 44 (6), pp. 2743-2760Grünwald, P., A tutorial introduction to the minimum description length principle (2005) Advances in Minimum Description Length: Theory and Applications, pp. 3-81. , Grünwald P, Myung I, Pitt M (eds). MIT Press: Cambridge, MA, USAAlmeida, T., Yamakami, A., Almeida, J., Filtering spams using the minimum description length principle (2010) Proceedings of the 25th ACM Symposium On Applied Computing, pp. 1856-1860. , Sierre, SwitzerlandAndroutsopoulos, I., Paliouras, G., Michelakis, E., Learning to filter unsolicited commercial e-mail (2004) National Centre for Scientific Research ""Demokritos"", , Athens, Greece March, Report 2004/2Teahan, W., Harper, D., Using compression-based language models for text categorization (2001) Proceedings of the 2001 Workshop on Language Modeling and Information Retrieval, pp. 1-5. , Pittsburgh, PA, USAFrank, E., Chui, C., Witten, I., Text categorization using compression models (2000) Proceedings of the 10th Data Compression Conference, pp. 555-565. , Snowbird, UT, USASiefkes, C., Assis, F., Chhabra, S., Yerazunis, W., Combining winnow and orthogonal sparse bigrams for incremental spam filtering (2004) Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases, pp. 410-421. , Pisa, ItalyMetsis, V., Androutsopoulos, I., Paliouras, G., Spam filtering with Naive Bayes - which Naive Bayes? (2006) Proceedings of the 3rd International Conference on Email and Anti-Spam, pp. 1-5. , Mountain View, CA, USABordes, A., Ertekin, S., Weston, J., Bottou, L., Fast kernel classifiers with online and active learning (2005) Journal of Machine Learning Research, 6, pp. 1579-1619Matthews, B., Comparison of the predicted and observed secondary structure of T4 phage lysozyme (1975) Biochimica et Biophysica Acta, 405 (2), pp. 442-451. , OctoberBaldi, P., Brunak, S., Chauvin, Y., Andersen, C., Nielsen, H., Assessing the accuracy of prediction algorithms for classification: an overview (2000) Bioinformatics, 16 (5), pp. 412-424. , MayMcCallum, A., Nigam, K., A comparison of event models for Naive Bayes text classification (1998) Proceedings of the 15th AAAI Workshop on Learning for Text Categorization, pp. 41-48. , Menlo Park, CA, USASchneider, K., On word frequency information and negative evidence in Naive Bayes text classification (2004) Proceedings of the 4th International Conference on Advances in Natural Language Processing, pp. 474-485. , Alicante, SpainLosada, D., Azzopardi, L., Assessing multivariate Bernoulli models for information retrieval (2008) ACM Transactions on Information Systems, 26 (3), pp. 1-46. , JuneJohn, G., Langley, P., Estimating continuous distributions in Bayesian classifiers (1995) Proceedings of the 11st International Conference on Uncertainty in Artificial Intelligence, pp. 338-345. , Montreal, CanadaSculley, D., Wachman, G., Brodley, C., Spam filtering using inexact string matching in explicit feature space with on-line linear classifiers (2006) Proceedings of the 15th Text Retrieval Conference, pp. 1-10. , Gaithersburg, MD, US",Compression-based Spam Filter,10.1002/sec.639,,'Wiley',,core
224462449,2016-01-01T00:00:00,"The rapid growth of camera and storage capabilities, over the past decade, has resulted in an exponential growth in the size of video repositories, such as YouTube. In 2015, 400 hours of videos are uploaded to YouTube every minute. At the same time, massive amount of images/videos are generated from monitoring cameras for elderly, sick assistance, satellites for earth science research, and telescopes for space exploration. Human annotation and manual manipulation of such videos are infeasible. Computer vision technology plays an essential role in automating the indexing, sorting, tagging, searching and analyzing huge amount of video data. Object detection and activity recognition in general are some of the most challenging topics in computer vision today. While the detection/recognition accuracy has increased dramatically over the past few years, it has not kept up with the complexity of detection/recognition tasks nor with the increased resolution of the video/image sources. As a result, the computation speed, and power consumption, of computer vision applications have become a major impediment to their wider use. Thus applications relying on real-time monitoring/feedback are not possible under current speeds. This thesis focuses on the use of Field Programmable Gate Arrays (FPGAs) to accelerate computer vision applications for embedded/real time applications while maintaining similar detection/recognition accuracy as the original processing. FPGAs are electronic devices on which an arbitrary digital circuit can be (re) configured under software control. To leverage the computational parallelism on FPGAs, fixed-point arithmetic is used for all implementations. The benefit of using fixed-point representation over floating point is the reduced bit-width, but the range and sometimes the precision are limited. Comprehensive studies are performed in this study to show that the classification system has some degree of tolerance to the reduced precision data representation. Hence FPGA programs are implemented accordingly in low bit-width fixed-point to achieve high computation throughput, low power consumption, and accurate classification.As a first step, the impact of reduced precision is studied for Viola-Jones face detection algorithm: whereas the reference OpenCV code uses double precision floating-point values, by using only five decimal digit (17 bits) fixed-point representation, the detection can achieve the same rates of false positives and false negatives as the reference OpenCV code. By reducing the necessary precision by a factor of 3X to 4X, the size of the circuit on FPGA is reduced by a factor of 12X; hence increasing the number of feature classifiers that can be fit on a single FPGA. A hybrid CPU-FPGA processing pipeline is proposed to reduce CPU work-load. As a second step, Histogram of Oriented Gradients (HOG), one of the most popular object detection algorithms, is evaluated by using the full-image evaluation methodology to explore the FPGA implementation of HOG using reduced bit-width. This approach lessens the required area resources on the FPGA and increases the clock frequency and hence the throughput per device through increased parallelism. Detection accuracy of the fixed-point HOG is evaluated  by applying state-of-the-art computer vision pedestrian detection evaluation metrics. The reduced precision detection performs as well as the original floating-point code from OpenCV. This work then shows the single FPGA implementation achieves a 68.7x higher throughput than a high-end CPU, 5.1x higher than a high-end GPU, and 7.8x higher than the same implementation using floating-point on the same FPGA. A power consumption comparison for different platforms shows our fixed-point FPGA implementation uses 130x less power than CPU, and 31x less energy than GPU to process one image. In addition to object detection algorithms, this thesis also investigates the acceleration of action recognition, specifically a human action recognition (HAR) algorithm. In HAR, pedestrian detection is normally used as a pre-processing step to locate human in stream video. In this work, the possibility to perform feature extraction under reduced precision fixed-point arithmetic is evaluated to ease hardware resource requirements. The Histogram of Oriented Gradient in 3D (HOG3D) feature extraction is then compared with state-of-the-art Convolutional Neural Networks (CNNs) methods and result shows that the later is 75X slower than the former. The experiment shows that by re-training the classifier with reduced data precision, the classification performs as well as the original double-precision floating-point. Based on this result, an FPGA-based HAR feature extraction is implemented for near camera processing using fixed-point data representation and arithmetic. This implementation, using a single Xilinx Virtex 6 FPGA, achieves about 70x speedup over multicore CPU.Furthermore, a GPU implementation of HAR is introduced with 80x speedup over CPU (on an Nvidia Tesla K20)",Optimizing FPGA Design For Real Time Video Content Analysis,,,"eScholarship, University of California",,core
79586725,2016-01-01T00:00:00,"abstract: There is a concerted effort in developing robust systems health monitoring/management (SHM) technology as a means to reduce the life cycle costs, improve availability, extend life and minimize downtime of various platforms including aerospace and civil infrastructure. The implementation of a robust SHM system requires a collaborative effort in a variety of areas such as sensor development, damage detection and localization, physics based models, and prognosis models for residual useful life (RUL) estimation. Damage localization and prediction is further complicated by geometric, material, loading, and environmental variabilities. Therefore, it is essential to develop robust SHM methodologies by taking into account such uncertainties. In this research, damage localization and RUL estimation of two different physical systems are addressed: (i) fatigue crack propagation in metallic materials under complex multiaxial loading and (ii) temporal scour prediction near bridge piers.  With little modifications, the methodologies developed can be applied to other systems. 

 Current practice in fatigue life prediction is based on either physics based modeling or data-driven methods, and is limited to predicting RUL for simple geometries under uniaxial loading conditions. In this research, crack initiation and propagation behavior under uniaxial and complex biaxial fatigue loading is addressed.  The crack propagation behavior is studied by performing extensive material characterization and fatigue testing under in-plane biaxial loading, both in-phase and out-of-phase, with different biaxiality ratios. A hybrid prognosis model, which combines machine learning with physics based modeling, is developed to account for the uncertainties in crack propagation and fatigue life prediction due to variabilities in material microstructural characteristics, crack localization information and environmental changes. The methodology iteratively combines localization information with hybrid prognosis models using sequential Bayesian techniques. The results show significant improvements in the localization and prediction accuracy under varying temperature. 

For civil infrastructure, especially bridges, pier scour is a major failure mechanism. Currently available techniques are developed from a design perspective and provide highly conservative scour estimates. In this research, a fully probabilistic scour prediction methodology is developed using machine learning to accurately predict scour in real-time under varying flow conditions.Dissertation/ThesisDoctoral Dissertation Mechanical Engineering 201",Systems Health Management and Prognosis using Physics Based Modeling and Machine Learning,,https://core.ac.uk/download/79586725.pdf,,,core
379207905,2016-06-01T00:00:00,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arises in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments",Real-time data mining of massive data streams from synoptic sky surveys,,https://core.ac.uk/download/379207905.pdf,'Elsevier BV',,core
101731706,2015-03-11,"In this paper, we investigate the problem of detecting depres-sion from recordings of subjects ’ speech using speech pro-cessing and machine learning. There has been considerable interest in this problem in recent years due to the potential for developing objective assessments from real-world behaviors, which may provide valuable supplementary clinical informa-tion or may be useful in screening. The cues for depression may be present in ”what is said ” (content) and ”how it is said” (prosody). Given the limited amounts of text data, even in this relatively large study, it is difficult to employ standard method of learning models from n-gram features. Instead, we learn models using word representations in an alternative feature space of valence and arousal. This is akin to embed-ding words into a real vector space albeit with manual rat-ings instead of those learned with deep neural networks [1]. For extracting prosody, we employ standard feature extractors such as those implemented in openSMILE and compare them with features extracted from harmonic models that we have been developing in recent years. Our experiments show that our features from harmonic model improve the performance of detecting depression from spoken utterances than other al-ternatives. The context features provide additional improve-ments to achieve an accuracy of about 74%, sufficient to be useful in screening applications. Index Terms — Depression, Speech analysis, Telemedicine 1",INFERRING CLINICAL DEPRESSION FROM SPEECH AND SPOKEN UTTERANCES,,,,,core
108597206,2016-09-28,"Assuring quality of service (QoS) requirements is critical when assembling a distributed real-time and embedded (DRE) system from a repository of existing software and hardware components. This paper presents a two-level approach for assuring satisfaction of QoS requirements in the context of a reduced design space for DRE systems. Techniques from artificial intelligence and statistics are used to fulfill these collective objectives at system assembly time. The result not only lessens the overhead of validation of QoS require-ments at run-time, but also reduces the development and integration cost of DRE systems",Two-Level Assurance of QoS Requirements for Distributed Real-time and Embedded Systems ∗,,,,,core
79182396,2016-02-29T00:00:00,"Accurate and timely traffic flow prediction is crucial to proactive traffic management and control in data-driven intelligent transportation systems (D2ITS), which has attracted great research interest in the last few years. In this paper, we propose a Spatial-Temporal Weighted K-Nearest Neighbor model, named STW-KNN, in a general MapReduce framework of distributed modeling on a Hadoop platform, to enhance the accuracy and efficiency of short-term traffic flow forecasting. More specifically, STW-KNN considers the spatial-temporal correlation and weight of traffic flow with trend adjustment features, to optimize the search mechanisms containing state vector, proximity measure, prediction function, and K selection. urthermore, STW-KNN is implemented on a widely adopted Hadoop distributed computing platform with the MapReduce parallel processing paradigm, for parallel prediction of traffic flow in real time. inally, with extensive experiments on real-world big taxi trajectory data, STW-KNN is compared with the state-of-the-art prediction models including conventional K-Nearest Neighbor (KNN), Artificial Neural Networks (ANNs), Na&iuml;ve Bayes (NB), Random orest (R), and C4.. The results demonstrate that the proposed model is superior to existing models on accuracy by decreasing the mean absolute percentage error (MAPE) value more than 11.9% only in time domain and even achieves 89.71% accuracy improvement with the MAPEs of between 4% and 6.% in both space and time domains, and also significantly improves the efficiency and scalability of short-term traffic flow forecasting over existing approaches",A distributed spatial-temporal weighted model on MapReduce for short-term traffic flow forecasting,10.1016/j.neucom.2015.12.013,http://hdl.handle.net/10536/DRO/DU:30082156,'Elsevier BV',,core
296645546,2015-11-26T15:06:52,"A major problem in image processing and analysis is the segmentation of its components. Many computer vision tasks process image regions after segmentation, and the minimization of errors is then crucial for a good automatic inspection system. This paper presents an applied work on automatic segmentation of cell nuclei in digital noisy images. One of the major problems when using morphological watersheds is oversegmentation. By using an efficient homotopy image modification module, we prevent oversegmentation. This module utilizes diverse operations, such as sequential filters, distance transforms, opening by reconstruction, top hat, etc., some in parallel, some in cascade form, leading to a new set of internal and external cell nuclei markers. Very good results have been obtained and the proposed technique should facilitate better analysis of visual perception of cell nuclei for human and computer vision. All steps are presented, as well as the associated images. Implementations were done in the Khoros system using the MMach toolbox.3164314324Costa, J.A.F., Andrade Netto, M.L., Parts classification in assembly lines using multilayer feedforward neural networks Proc. of the 1997 IEEE International Conference on Systems, Man., and Cybernetics, , Orlando, Florida, October 12-15Gonzaga, A., Costa, J.A.F., Moment invariants applied to the recognition of objects using neural networks (1996) Proceedings of SPIE, 2847, pp. 223-233. , Applications of Digital Image Processing XIX, Andrew G. Tescher, EditorMascarenhas, N.D.A., Velasco, F.R.D., (1989) Processamento digital de imagens. 2a. Ed., , I Escola Brasileiro-Argentina de Informática. Buenos Aires: Ed. KapeluszSilver, D., Object-oriented visualization (1995) IEEE Camputer Graphics and Applications, 15 (3), pp. 54-62. , MayBallard, D., Brown, C., (1982) Computer Vision, , Prentice-Hall, Englewood Cliffs, N.JKass, M., Witkin, A., Terzopoulos, D., Snakes: Active contour models (1988) Int'l. J. Computer Vision, 1 (4), pp. 321-331Choi, C., Jennings, A., Learning to segment using fuzzy boundary cell features (1996) Proc. of Complex Systems ConferenceHasegawa, A., Cullen, K.J., Man, S.K., Segmentation and analysis of breast cancer pathological images by na adaptive-sized hybrid neural network (1996) Proc. SPIE, 2710, pp. 752-762. , Medical Imaging: Image Processing, Murray H. LoewKenneth M. HansonEdsBarrera, J., Banon, G.J.F., Lotufo, R.A., Mathematical morphology toolbox for the KHOROS system (1994) Conf. on Image Algebra and Morphological Image Processing V, Intl. Symposium on Optics, Imaging and Instrumentation, , SPIE's Annual Meeting, 24-29 July. San Diego, USAMatheron, G., (1975) Random Sets and Integral Geometry, , John Wiley and Sons, New YorkSerra, J., (1982) Image Analysis and Mathematical Morphology, , Academic Press, LondonGiardina, C.R., Dougherty, E.R., (1988) Morphological Methods in Image and Signal Processing, , Prentice-Hall, Englewood Cliffs, NJHaralick, R.M., Sternberg, S.R., Zhuang, X., Image analysis using mathematical morphology (1987) IEEE Transactions on Pattern Analysis and Machine Intelligence, 9, pp. 532-550Dougherty, E.R., An introduction to morphological image processing (1992) SPIE Tutorial Text, TT09Heijmans, H.J.A.M., (1994) Morphological Image Operators, , Academic Press, BostonBeucher, S., Lantuéjoul, C., Use of watersheds in contour detection (1979) Proc. Int'l Workshop Image Processing, Real-Time Edge and Motion Detection/Estimation, , Rennes, France, Sept. 17-21Najman, L., Schmitt, M., Geodesic saliency of watershed contours and hierarchical segmentation (1996) IEEE Trans. Pattern Anal. Machine Intell., 18 (12), pp. 1163-1173Vincent, L., Sollie, P., Watersheds in digital space: An efficient algorithm based on immersion simulations (1991) IEEE Trans. on Pattern Anal and Machine Intell., 13 (6), pp. 583-598Michael, W.H., William, E.H., Watershed-driven relaxation labeling for image segmentation (1994) Proceedings ICIP-94, 3, pp. 460-463. , IEEE International Conference on Image ProcessingPerry, S., (1996) Fast Interactive Segmentation for Content Based Retrieval and Navigation, , mini-thesis submited for transfer of registration from MPhil to PhD., University of Southampton, UK, OctoberMeyer, F., Color image segmentation (1992) 4th International Conference on Image Processing and its Applications, pp. 303-306. , IEE, Conference Publication No. 354Beucher, S., Segmentation tools in mathematical morphology (1990) Proceedings SPIE, 1350, pp. 70-84. , Image Algebra and Morphological Image Processing(P.D. Gader, ed.)Haris, K., Efstratiadis, S.N., Maglaveras, N., Pappas, C., Hybrid image segmentation using watersheds (1996) Proc. SPIE Vol. 2727, Visual Communications and Image Processing '96, 2727, pp. 1140-1151. , Rashid AnsariMark J. SmithEdsMeyer, F., Beucher, S., Morphological segmentation (1990) J. Visual Comm. & Img. Repr., 1, pp. 21-46Lotufo, R., Trettel, E., Image segmentation by mathematical morphology - Laboratory notes (1996) Brazilian Workshop'96 on Mathematical Morphology, , São Paulo, Feb 27 - March 1Barrera, J., Banon, G.J.F., Lotufo, R.A., Hirata R., Jr., (1997) MMach: A Mathematical Morphology Toolbox for the KHOROS System, , Tech. Report RT-MAC-9704. IME/University of São Paulo, São Paulo, Brazil. MayRasure, J., Jordán, R., Lotufo, R., Teaching image processing with khoros (1994) 1994 IEEE Conf. on Image Processing, , ftp://ftp.dca.fee.unicamp.brRasure, J., Williams, C., An integrated data flow visual language and software development environment (1991) Journal of Visual Languages and Computing, pp. 217-246Konstatinides, K., Rasure, J., The khoros software development environment for image and signal processing (1994) IEEE Trans. on Image Processing, 3 (3), pp. 243-252. , MayJordan, R., Lotufo, R., (1994) Digital Image Processing with Khoros 2.0, , World Wide Web (WWW) CoursewareRasure, J., Kubica, S., Tutorial: The Khoros application development environment (1993) VI Simpósio Brasileiro de Computação Gráfica e Processamento de Imagens, , Recife, P",Cell Nuclei Segmentation In Noisy Images Using Morphological Watersheds,10.1117/12.292759,https://core.ac.uk/download/296645546.pdf,'SPIE-Intl Soc Optical Eng',,core
296721711,2015-11-26T17:46:34Z,"Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP)Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES)For decades, photographs have been used to document space-time events and they have often served as evidence in courts. Although photographers are able to create composites of analog pictures, this process is very time consuming and requires expert knowledge. Today, however, powerful digital image editing software makes image modifications straightforward. This undermines our trust in photographs and, in particular, questions pictures as evidence for real-world events. In this paper, we analyze one of the most common forms of photographic manipulation, known as image composition or splicing. We propose a forgery detection method that exploits subtle inconsistencies in the color of the illumination of images. Our approach is machine-learning-based and requires minimal user interaction. The technique is applicable to images containing two or more people and requires no expert interaction for the tampering decision. To achieve this, we incorporate information from physics-and statistical-based illuminant estimators on image regions of similar material. From these illuminant estimates, we extract texture-and edge-based features which are then provided to a machine-learning approach for automatic decision-making. The classification performance using an SVM meta-fusion classifier is promising. It yields detection rates of 86% on a new benchmark dataset consisting of 200 images, and 83% on 50 images that were collected from the Internet.8711821194UnicampUnicamp FaepexConselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP)Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES)IF Sudeste MGMicrosoftConselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP)Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES",Exposing Digital Image Forgeries by Illumination Color Classification,10.1109/TIFS.2013.2265677,,EUA,"[{'title': None, 'identifiers': ['issn:1556-6013', '1556-6013']}]",core
45450607,2016-06-16T10:10:00,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.  While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.  The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg-Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg-Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg-Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.  Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H2O, O3, HNO3, CH4, N2O, NO2, CFC-11, CFC-12, N2O5 and ClONO2. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements. © Author(s) 2013.This work has been performed under the ESA study >Support to MIPAS Level 2 product validation>, contract ESA-ESRIN no. 21719/08/I-OL. The authors are grateful to the Astrium team that developed the industrial prototype ML2PP using the ORM code as reference, to Thorsten Fehr and Rolf von Kulhmann for the coordination of the MIPAS Quality Working Group activities and to Michael Kiefer for the work done within the MIPAS Quality Working Group.Peer Reviewe",Ten years of MIPAS measurements with ESA Level 2 processor V6 – Part 1: Retrieval algorithm and diagnostics of the products,10.5194/amt-6-2419-2013,https://core.ac.uk/download/45450607.pdf,'Copernicus GmbH',"[{'title': None, 'identifiers': ['issn: 1867-1381', ' 1867-1381']}]",core
293724665,2016-01-01T00:00:00,"The ability of robots to perform tasks in human environments has 
largely been limited to rather simple and specific tasks, such as lawn mowing 
and vacuum cleaning. As such, current robots are far away from the robot butlers, assistants, 
and housekeepers that are depicted in science fiction movies. Part of this gap can be 
explained by the fact that human environments are hugely varied, complex and unstructured. 
For example, the homes that a domestic robot might end up in are hugely varied. Since 
every home has a different layout with different objects and furniture, it is impossible for 
a human designer to anticipate all challenges a robot might 
face, and equip the robot a priori with all the necessary perceptual and manipulation skills.


Instead, robots could be programmed in a way that allows them to adapt to any 
environment that they are in. In that case, the robot designer would not 
need to precisely anticipate such environments. The ability to adapt can be provided by 
robot learning techniques, which can be applied to learn skills for perception and manipulation.
Many of the current
robot learning techniques,
however, rely on human supervisors to provide annotations or demonstrations, and to fine-tuning the methods parameters and heuristics. As such, 
it can require a significant amount of human time investment to 
make a robot perform a task in a novel environment, even if statistical learning techniques are used.

In this thesis, I focus on another way of obtaining the data a robot needs to 
learn about the environment and how to successfully 
perform skills in it. By exploring the environment using its own sensors and actuators, rather than 
passively waiting for annotations or demonstrations, a 
robot can obtain this data by itself. I investigate multiple approaches that allow a robot 
to explore its environment autonomously, while trying to minimize the design effort 
required to deploy such algorithms in different situations.

First, I consider an unsupervised robot with minimal prior knowledge
about its environment. It can only learn through observed
sensory feedback obtained though interactive exploration of its
environment. In a bottom-up, probabilistic approach, the robot tries to segment
the objects in its environment through clustering with minimal prior knowledge. This clustering is
based on static visual scene features and observed movement. Information theoretic principles are used to autonomously select actions that maximize
the expected information gain, and thus learning speed. Our evaluations 
on a real robot system equipped with an on-board camera show that the proposed 
method handles noisy inputs better than previous methods, and that 
action selection according to the information gain criterion does increase the learning speed.

Often, however, the goal of a robot is not just to learn the structure of the environment, but to learn 
how to perform a task encoded by a reward signal. 
 In addition to the weak feedback provided by reward signals, the robot has access to rich sensory data, that, even for
simple tasks, is often non-linear and high-dimensional. Sensory data can be
leveraged to learn a system model, but in high-dimensional sensory spaces this
step often requires manually designing features. I propose a robot
reinforcement learning algorithm with learned non-parametric models, value
functions, and policies that can deal with high-dimensional state representations.
As such, the proposed algorithm is well-suited to deal with high-dimensional signals 
such as camera images. To avoid that the robot converges prematurely  to a sub-optimal solution, 
the information loss of policy updates is limited. This constraint makes sure the robot keeps exploring the effects
of its behavior on the environment. The experiments show that the proposed non-parametric 
relative entropy policy search algorithm performs better than prior methods that either do not employ bounded updates, 
or that try to cover the state-space with general-purpose radial basis functions. Furthermore, 
the method is validated on a 
real-robot setup with high-dimensional camera image inputs.

One problem with typical exploration strategies is that the behavior is perturbed independently 
in each time step, for example through selecting a random action or random policy parameters. 
As such, the resulting exploration behavior might be incoherent. Incoherence causes 
inefficient random walk behavior, makes the system less robust, and causes wear and tear on the robot. 
A typical solution is to perturb the policy parameters directly, and use the same perturbation for an entire episode. However, this
strategy
tends to increase the number of episodes needed, since only a single perturbation can be evaluated per episode. I introduce a
strategy that can make a more balanced trade-off between the advantages of these two approaches.
The experiments show that intermediate trade-offs, rather than independent or episode-based exploration, 
is beneficial across different tasks and learning algorithms. 

This thesis thus addresses how robots can learn autonomously by exploring the world through
unsupervised learning and reinforcement learning. Throughout the thesis, new approaches 
and algorithms are introduced: a probabilistic interactive segmentation approach, the non-parametric 
relative entropy policy search algorithm, and a framework for generalized exploration. 
To allow the learning algorithms to be applied in different and unknown environments, 
the  design effort and supervision required from human designers or users is minimized. 
These approaches and algorithms contribute 
towards the capability of robots to autonomously learn useful skills in human environments in a practical manner",Machine Learning through Exploration for Perception-Driven Robotics,,,,,core
77952810,2016-01-01T08:00:00,"Visual intelligence is the ability to recognize objects with their internal representations. These representations are used to produce semantics or understand relations among the objects. Human visual cortex has been known to be a superior system to any kinds of artificial vision systems and many researchers have improved machine vision systems by taking inspiration from it. Recently, bio-inspired vision such as convolutional neural networks (CNNs) have shown great promise in visual understanding. The algorithms become a record holder on many visual understanding tasks, but it is challenging to deploy them on embedded platforms where processing and power budget are limited. This proposal describes three methods to improve CNNs in terms of accuracy and runtime, which facilitate their applicability on embedded devices. Firstly, we improve robustness of CNNs against strong noise by utilizing uncertainty information. With the uncertainty noise added to the networks, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model accordingly. Since the proposed model makes decision based on the uncertainty region instead of point-wise prediction, it alleviates the need for averaging multiple model predictions used to stabilize output. Secondly, we propose a filter separation technique for CNNs in order to accelerate their runtime as well as to reduce their memory usage. The redundancy of the parameters, especially weights of the convolutional filters in CNNs has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. We propose a network architecture that consists of consecutive sequence of one-dimensional filters across all directions in 3D space. The proposed network not only achieves comparable performance as conventional CNNs, but also significantly reduces the redundancy of the parameters in CNNs. The proposed convolution pipeline provides around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, it does not require additional efforts in manual tuning or post processing once the model is trained. Lastly, a hardware implementation for CNNs is proposed to accelerate its computation. While CNNs produce hundreds of intermediate results whose constant memory accesses result in inefficient use of general purpose processor hardware, the custom hardware with efficient routing strategy maximizes hardware utilization and obtains high performance in real world applications. The proposed coprocessor is capable of a peak performance of 40 G-ops/s while consuming less than 4 W of power. All of those methods optimize the algorithm for embedded devices thereby making it feasible to use in various mobile platforms",Fast and robust convolutional neural networks optimized for embedded platforms,,,'Purdue University (bepress)',,core
77521365,2016-08-09T21:15:00,"With support from DARPA and the Operationally Responsive Space (ORS) Office a new approach to conventional spacecraft mission assurance has been developed that has the potential to revolutionize current practices. The goal of this new approach, coined Digital Assurance (DA), is to provide decision makers with real-time, quantified information at any level of the program, including continuous, live custody of its comprising components. Digital Assurance will enable reduction in program costs and time associated with conventional mission assurance approaches. DA represents the intelligent integration of the digital design environment with the digital manufacturing environment to achieve unprecedented levels of knowledge about the physical configuration of a satellite.
The foundation for the successful implementation of DA is the concept of Continuous Custody and a Graph Database. Continuous Custody takes advantage of the physical reality that the majority of spacecraft Assembly, Integration, and Test (AI&T) takes place in a very well-defined and well-controlled physical environment, allowing capture of a broad scope of environmental factors",Digital Assurance: Empowering Decision Makers in the Digital Age,,https://core.ac.uk/download/77521365.pdf,DigitalCommons@USU,,core
378966948,2015-01-01T08:00:00,"Writing English research article (RA) abstracts is a difficult but mandatory task for Taiwanese engineering graduate students (Feng, 2013). Understanding the current situation and needs of Taiwanese engineering graduate students, this dissertation aimed to develop and evaluate an automated writing evaluation (AWE) tool to assist their research article (RA) abstract writing in English by following a Design-Based Research (DBR) approach as the methodological framework. DBR was chosen because it strives to solve real-world problems through multiple iterations of development and building on results from each iteration to advance the project.
Six design iterations were undertaken to develop and to evaluate the AWE tool in this dissertation, including (1) corpus compilation of engineering RAs, (2) genre analysis of engineering abstracts, (3) machine learning of move classification in abstracts, (4) analysis of lexical bundles used to express moves, (5) analysis of the choice of verb categories associated with moves, and finally, (6) AWE tool development based on previous findings, classroom implementation, and evaluation of the AWE tool following Chapelle’s (2001) computer-assisted language learning (CALL) framework.
To begin with, I collected a corpus of 480 engineering RAs (Corpus-480) to extract appropriate linguistic properties as pedagogical materials to be implemented in the AWE tool. A sub-corpus (Corpus-72) was compiled with 72 RAs randomly chosen from Corpus-480 for manual and automated analyses. Next, to seek the best descriptive framework for the structure of engineering RA abstracts, two move schemata were compared: (1) IMRD (Introduction, Methodology, Results, and Discussion) and (2) CARS (Create-A-Research-Space, Swales, 1990). Abstracts in Corpus-72 were annotated and these two schemas were evaluated according to three quantitative metrics devised specifically for this comparison.
Applying a statistical natural language processing (StatNLP) approach, a Support Vector Machine (SVM) was trained for automated move classification in abstracts. Formulaic language in engineering RA sections was used as linguistic features to automatically classify moves in abstracts. Additionally, four-word lexical bundles and verb categories were identified from Corpus-480 and Corpus-72, respectively. Four-word lexical bundles associated with moves in abstracts were extracted automatically. Additionally, verb categories (i.e., tense, aspect, and voice) in moves of abstracts were identified using CyWrite::Analyzer, a hybrid (statistical and rule-based) NLP software.
Finally, the AWE tool was developed, based on the findings from the previous iterations, and implemented in an English-as-a-foreign-language (EFL) classroom setting. Through analyzing students’ drafts before and after using the tool, and responses to a questionnaire and a semi-structured interview, the AWE tool was evaluated based on Chapelle’s (2001) CALL evaluation framework. The findings showed that students attempted to improve their abstracts by adding, deleting, or changing the sequences of their sentences, lexical bundles, and verb categories in their abstracts. Their attitudes toward the effectiveness and appropriateness of the tool were quite positive. Overall, the AWE tool drew students’ attention to the use of lexical bundles and verb categories to achieve the communicative purposes of each move in their abstracts.
In conclusion, this dissertation started from Taiwanese engineering students’ needs to improve their English abstract writing, and attempted to develop and evaluate an AWE tool for assisting them. Following DBR, the findings from this dissertation are discussed to improve the next generation of the AWE tools. Having these iterations in place, future studies can focus on developing pedagogical materials from genre-based analysis in different disciplines to fulfill learners’ needs","Designing, implementing, and evaluating an automated writing evaluation tool for improving EFL graduate students’ abstract writing: a case in Taiwan",,,Iowa State University Digital Repository,,core
234828572,2015,"Mastering the finest art of ‘mechatronics’ currently looks one of the most attractive task of modern engineering technology and

science. Many are the applications which resort to the interdisciplinary approach of mechatronics to enhance the performance, quality

and safety of either product or process. Some are very traditional, like hard disk drives, biomedical, automotive and aerospace

systems, other are fairly new like micro and nano electromechanical systems, unmanned air vehicles, intelligent machining and

manufacturing systems or bioinspired devices. A first generation of mechatronic products was conceived to embed a suitable

‘smartness’ to improve the skill of self–adapting to any abrupt variation of operating conditions, by resorting to the ‘synergistic

integration of mechanical engineering with electronics and control in the design and manufacturing of product process’ as

mechatronics was brightly defined. Nowadays, a mechatronic design is surely based on its interdisciplinary nature, but its real

meaning was harmonized with an effective contamination among different application domains, methodologies and technologies,

being smartly applied to reach the highest result in any product, system and process development. A recent experience within the

frame of the EMEA District of the American Society of Mechanical Engineers (ASME) was a chance to get an impression of the

scientific and industrial research activity performed in some fields of mechatronics. Some exciting examples describing how different

competences, disciplines, technologies met in an innovative mechatronic system are herein exposed by some researchers of the EMEA

area of the world. They deal with several domains, like the hard disk drive technology, biomedical prostheses, fluidic automation,

UAV Vision System, vibration monitoring and suppression in steelmaking plants, materials machining and smart composites. These

examples will narrate to the reader who is still looking for the meaning of mechatronics how some approaches, as neural network

positioning control, chaos prevention, myoelectric stimulation of prosthesis, human detection by vision system, multi-physics

modeling and control of dynamics are currently implemented in a sort of artificial intelligence in small scale device, as in a finger of a

biotronic hand or in a large equipment like an electric arc furnace. Moreover, the reader will realize how intensively this goal is

achieved by exploiting the available technologies as additive manufacturing or fiber optics embedded into composite structures to

reduce the cost, weight or volume of the product or to improve the quality and accuracy of a material processing like in rolling or in

turning against the risk of self–excited chatter vibration. This scenario is covering a wide range of mechatronic applications, although

many others are currently developed in several fileds of engineering","Mechatronics: Principles, Technologies and  Applications",,,"NOVA Science Publishers, Inc",,core
102415566,2015-08-23,"This is a review of the new information-theoretic Process Physics. This new modelling of reality brings physics very much into accord with the general concepts of Process Phi-losophy. The fundamental assumption is that reality is to be modelled as self-organising semantic or relational information using a self-referentially limited neural network model, where the information-theoretic limitations are implemented via self-referential noise. This modelling was motivated by the discovery that such stochastic neural networks are foundational to known quantum field theories. In Process Physics time is a distinct non-geometric process while space and quantum physics are emergent and unified. Quantum phenomena are caused by fractal topological defects embedded in and forming a grow-ing three-dimensional fractal process-space, which is essentially a quantum foam. Other features are the emergence of quantum field theory with flavour and confined colour, limited causality and the Born quantum measurement metarule, inertia, time-dilation effects, gravity and the equivalence principle, a growing universe with a cosmological constant, black holes and event horizons, and the emergence of classicality. The unifica",Process Studies Supplement URL:,,,,,core
103754366,2016-01-16,"Abstract—Many research efforts propose the use of flow-level features (e.g., packet sizes and inter-arrival times) and machine learning algorithms to solve the traffic classification problem. However, these statistical methods have not made the anticipated impact in the real world. We attribute this to two main reasons: (a) training the classifiers and bootstrapping the system is cumbersome, (b) the resulting classifiers have limited ability to adapt gracefully as the traffic behavior changes. In this paper, we propose an approach that is easy to bootstrap and deploy, as well as robust to changes in the traffic, such as the emergence of new applications. The key novelty of our classifier is that it learns to identify the traffic of each application in isolation, instead of trying to distinguish one application from another. This is a very challenging task that hides many caveats and subtleties. To make this possible, we adapt and use subspace clustering, a powerful technique that has not been used before in this context. Subspace clustering allows the profiling of applications to be more precise by automatically eliminating irrelevant features. We show that our approach exhibits very high accuracy in classifying each application on five traces from different ISPs captured between 2005 and 2011. This new way of looking at application classification could generate powerful and practical solutions in the space of traffic monitoring and network management. I",SubFlow: Towards Practical Flow-Level Traffic Classification,,,,,core
101139094,2015-01-13,"Abstract:- This paper describes a two-rate stochastic control system as state-space (SS) type decomposed and discretized models of stochastic subsystems with the &quot;fast &quot; and &quot;slow &quot; artificial neural networks (NNs). These NNs are used as the dynamic subsystems controllers. This is because such neuromorphic controllers are especially suitable to control complex systems. An illustrative example – two-rate NN hybrid control of decomposed stochastic model of a rigid guided missile over different operating conditions – was carried out using the proposed two-rate SS decomposition technique. This example demonstrates that this research technique results in simplified low-order autonomous control subsystems with various discretization periods and with various speeds of actuation, and shows the quality of the proposed technique. The obtained results show that the control tasks for the autonomous subsystems can be solved more qualitatively than for the original system. The simulation and animation results with use of software package Simulink demonstrate that this research technique would work for real-time stochastic systems",Simulation of Two-Rate Neural Network Control for Stochastic Model of Missile Autopilot,,,,,core
275623422,2016-03-01T00:00:00,"This is an Accepted Manuscript of an article published by Taylor & Francis Group in 
Enterprise Information Systems on 23/03/2016, available online: http://www.tandfonline.com/10.1080/17517575.2015.1036927.[EN] There is an increasing interest on developing virtual enterprises in order to deal with
the globalisation of the economy, the rapid growth of information technologies and the
increase of competitiveness. In this paper we deal with the development of normative
open virtual enterprises (NOVEs). They are systems with a global objective that are
composed of a set of heterogeneous entities and enterprises that exchange services
following a specific normative context. In order to analyse and design systems of this
kind the multi-agent paradigm seems suitable because it offers a specific solution for
supporting the social and contractual relationships between enterprises and for formalising
their business processes. This paper presents how the Regulated Open Multiagent
systems (ROMAS) methodology, an agent-oriented software methodology, can
be used to analyse and design NOVEs. ROMAS offers a complete development
process that allows identifying and formalising of the structure of NOVEs, their
normative context and the interactions among their members. The use of ROMAS is
exemplified by means of a case study that represents an automotive supply chain.This work was partially supported by the projects [PROMETEOII/2013/019], [TIN2012-36586-C03-01], [FP7-29493], [TIN2011-27652-C03-00] and [CSD2007-00022], and the CASES project within the 7th European Community Framework Programme [grant agreement number 294931].Garcia Marques, ME.; Giret Boggino, AS.; Botti Navarro, VJ. (2016). Designing normative open virtual enterprises. Enterprise Information Systems. 10(3):303-324. https://doi.org/10.1080/17517575.2015.1036927S303324103Cardoso, H. L., Urbano, J., Brandão, P., Rocha, A. P., & Oliveira, E. (2012). ANTE: Agreement Negotiation in Normative and Trust-Enabled Environments. Advances on Practical Applications of Agents and Multi-Agent Systems, 261-264. doi:10.1007/978-3-642-28786-2_33Chu, X. N., Tso, S. K., Zhang, W. J., & Li, Q. (2002). Partnership Synthesis for Virtual Enterprises. The International Journal of Advanced Manufacturing Technology, 19(5), 384-391. doi:10.1007/s001700200028Davidsson, P., & Jacobsson, A. (s. f.). Towards Norm-Governed Behavior in Virtual Enterprises. Studies in Computational Intelligence, 35-55. doi:10.1007/978-3-540-88071-4_3DeLoach, S. A., & Ojeda, J. C. G. (2010). O-MaSE: a customisable approach to designing and building complex, adaptive multi-agent systems. International Journal of Agent-Oriented Software Engineering, 4(3), 244. doi:10.1504/ijaose.2010.036984DI MARZO SERUGENDO, G., GLEIZES, M.-P., & KARAGEORGOS, A. (2005). Self-organization in multi-agent systems. The Knowledge Engineering Review, 20(2), 165-189. doi:10.1017/s0269888905000494Dignum, V. 2003. “A Model for Organizational Interaction: Based on Agents, Founded in Logic.” PhD diss., Utrecht University.Dignum, V., and F. Dignum. 2006.A Landscape of Agent Systems for the Real World. Technical Report 44-CS-2006-061. Utrecht: Institute of Information and Computing Sciences, Utrecht University.Dignum, V., Meyer, J.-J. C., Dignum, F., & Weigand, H. (2003). Formal Specification of Interaction in Agent Societies. Lecture Notes in Computer Science, 37-52. doi:10.1007/978-3-540-45133-4_4Garcia, E. 2013. “Engineering Regulated Open Multiagent Systems.” PhD diss., Universitat Politecnica de Valencia.Garcia, E., Giret, A., & Botti, V. (s. f.). Software Engineering for Service-Oriented MAS. Lecture Notes in Computer Science, 86-100. doi:10.1007/978-3-540-85834-8_9Garcia, E., Giret, A., & Botti, V. (2013). A Model-Driven CASE tool for developing and verifying regulated open MAS. Science of Computer Programming, 78(6), 695-704. doi:10.1016/j.scico.2011.10.009Garcia, E., Giret, A., & Botti, V. (2011). Evaluating software engineering techniques for developing complex systems with multiagent approaches. Information and Software Technology, 53(5), 494-506. doi:10.1016/j.infsof.2010.12.012Garcia, E., Giret, A., & Botti, V. (2011). Regulated Open Multi-Agent Systems Based on Contracts. Information Systems Development, 243-255. doi:10.1007/978-1-4419-9790-6_20Garcia, E., Giret, A., & Botti, V. (2014). ROMAS Methodology. Handbook on Agent-Oriented Design Processes, 331-369. doi:10.1007/978-3-642-39975-6_11Hollander, C. D., & Wu, A. S. (2011). The Current State of Normative Agent-Based Systems. Journal of Artificial Societies and Social Simulation, 14(2). doi:10.18564/jasss.1750HORLING, B., & LESSER, V. (2004). A survey of multi-agent organizational paradigms. The Knowledge Engineering Review, 19(4), 281-316. doi:10.1017/s0269888905000317Julian, V., Rebollo, M., Argente, E., Botti, V., Carrascosa, C., & Giret, A. (2009). Using THOMAS for Service Oriented Open MAS. Lecture Notes in Computer Science, 56-70. doi:10.1007/978-3-642-10739-9_5Luck, M., Barakat, L., Keppens, J., Mahmoud, S., Miles, S., Oren, N., … Taweel, A. (2011). Flexible Behaviour Regulation in Agent Based Systems. Lecture Notes in Computer Science, 99-113. doi:10.1007/978-3-642-22427-0_8Meneguzzi, F., Modgil, S., Oren, N., Miles, S., Luck, M., & Faci, N. (2012). Applying electronic contracting to the aerospace aftercare domain. Engineering Applications of Artificial Intelligence, 25(7), 1471-1487. doi:10.1016/j.engappai.2012.06.004Presley, A., Sarkis, J., Barnett, W., & Liles, D. (2001). International Journal of Flexible Manufacturing Systems, 13(2), 145-162. doi:10.1023/a:1011131417956Saeki, M., & Kaiya, H. (2008). Supporting the Elicitation of Requirements Compliant with Regulations. Active Flow and Combustion Control 2018, 228-242. doi:10.1007/978-3-540-69534-9_18Such, J. M., García-Fornes, A., Espinosa, A., & Bellver, J. (2013). Magentix2: A privacy-enhancing Agent Platform. Engineering Applications of Artificial Intelligence, 26(1), 96-109. doi:10.1016/j.engappai.2012.06.009Telang, P. R., & Singh, M. P. (2009). Enhancing Tropos with Commitments. Lecture Notes in Computer Science, 417-435. doi:10.1007/978-3-642-02463-4_22Wooldridgey, M., & Ciancarini, P. (2001). Agent-Oriented Software Engineering: The State of the Art. Lecture Notes in Computer Science, 1-28. doi:10.1007/3-540-44564-1_",Designing normative open virtual enterprises,10.1080/17517575.2015.1036927,https://riunet.upv.es/bitstream/10251/80335/3/ROMAS_final_version.pdf,'Informa UK Limited',,core
73400730,2016-10-25T00:00:00,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI",Intelligence in Artificial Intelligence,,http://arxiv.org/abs/1610.07862,,,core
301250191,2016-11-01T00:00:00,"We present a reachability graph-based search optimization tool for scheduling.Motivated by the lack of tool support for optimization of TCPNs.Implements an event-driven timed state space with AI heuristic search algorithms.Aimed at supporting flexible decision making process with algorithm portfolio.Comparative study of nine search algorithms on real system demonstrates tool efficiency. The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.Peer Reviewe",TIMSPAT - Reachability graph search-based optimization tool for colored Petri net-based scheduling,10.1016/j.cie.2016.07.031,,'Elsevier BV',"[{'title': 'Computers & Industrial Engineering', 'identifiers': ['issn:0360-8352', '0360-8352']}]",core
163024812,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob","Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",,https://core.ac.uk/download/163024812.pdf,HAL CCSD,,core
91751447,2015-12-01T00:00:00Z,"Application of Dynamic Control Systems (DCS) in detecting and

diagnosing car engine Starter Cranks is continuously being implemented to

serve different cases of real life problems such as Control of MEMS-based

scanning-probe data-storage devices, track-follow control for tape storage,

probe-based ultrahigh-density storage technology, a review of feed forward

control approaches in nanopositioning for high-speed SPM and so on. Car

engine Starter Cranks faults can be detected by sequence of diagnostic

processes which brings about the deployment of an Expert System. An Expert

System is one of the leading Artificial Intelligence techniques that have been

adopted to handle such task. This paper presents the imperatives for an Expert

System in developing Dynamic Control Systems for detecting and diagnosing

car engine Starter Crank faults through input and output requirements of

constructing successful Knowledge-Based Systems. Furthermore, diagnosis of

car engine Starter Cranks faults requires high technical skills and experience.

thus, DCS provides input and output equations in form of Matrix/Vector State

Space Representation (MSSR) which is useful in assisting mechanics for car

engine Starter Cranks fault detection and diagnosis via DCS and mathematical

Differential Equations (DE’s)",Expert system for detecting and diagnosing car engine starter cranks fault using dynamic control system,,,AcademicDirect,"[{'title': None, 'identifiers': ['1583-0233', 'issn:1583-0233']}]",core
103027738,2015-10-31,"Abstract—Autotuning systems intelligently navigate a search space of possible implementations of a computation to find the implementation(s) that best meets a specific optimization criteria, usually performance. This paper describes Nitro, a programmer-directed autotuning framework that facilitates tuning of code variants, or alternative implementations of the same computation. Nitro provides a library interface that permits programmers to express code variants along with meta-information that aids the system in selecting among the set of variants at run time. Machine learning is employed to build a model through training on this meta-information, so that when a new input is presented, Nitro can consult the model to select the appropriate variant. In experiments with five real-world irregular GPU benchmarks from sparse numerical methods, graph computations and sorting, Nitro-tuned variants achieve over 93 % of the performance of variants selected through exhaustive search. Further, we describe optimizations and heuristics in Nitro that substantially reduce training time and other overheads. Keywords-Autotuning; Performance Optimization; GPUs; I",Nitro: A Framework for Adaptive Code Variant Tuning,,,,,core
81580238,2016-11-01T00:00:00,"We present a reachability graph-based search optimization tool for scheduling.Motivated by the lack of tool support for optimization of TCPNs.Implements an event-driven timed state space with AI heuristic search algorithms.Aimed at supporting flexible decision making process with algorithm portfolio.Comparative study of nine search algorithms on real system demonstrates tool efficiency. The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.Peer ReviewedPostprint (author's final draft",TIMSPAT - Reachability graph search-based optimization tool for colored Petri net-based scheduling,10.1016/j.cie.2016.07.031,https://core.ac.uk/download/81580238.pdf,'Elsevier BV',"[{'title': 'Computers & Industrial Engineering', 'identifiers': ['issn:0360-8352', '0360-8352']}]",core
288335785,2015-01-01T00:00:00,"WOS: 000370402900001PubMed ID: 26321943In this article, we introduce the Bioinspired Neuroprosthetic Design Environment (BNDE) as a practical platform for the development of novel brain-machine interface (BMI) controllers, which are based on spiking model neurons. We built the BNDE around a hard real-time system so that it is capable of creating simulated synapses from extra-cellularly recorded neurons to model neurons. In order to evaluate the practicality of the BNDE for neuroprosthetic control experiments, a novel, adaptive BMI controller was developed and tested using real-time closed-loop simulations. The present controller consists of two in silico medium spiny neurons, which receive simulated synaptic inputs from recorded motor cortical neurons. In the closed-loop simulations, the recordings from the cortical neurons were imitated using an external, hardware-based neural signal synthesizer. By implementing a reward-modulated spike timing-dependent plasticity rule, the controller achieved perfect target reach accuracy for a two-target reaching task in one-dimensional space. The BNDE combines the flexibility of software-based spiking neural network (SNN) simulations with powerful online data visualization tools and is a low-cost, PC-based, and all-in-one solution for developing neurally inspired BMI controllers. We believe that the BNDE is the first implementation, which is capable of creating hybrid biological/in silico neural networks for motor neuroprosthetic control and utilizes multiple CPU cores for computationally intensive real-time SNN simulations.Bogazici University BAP Grants [10XD3]; Bogazici University Life Sciences and Technologies Research Center [09K120520]This research was supported by Bogazici University BAP Grants #10XD3 and Bogazici University Life Sciences and Technologies Research Center #09K120520",Toward building hybrid biological/in silico neural networks for motor neuroprosthetic control,10.3389/fnbot.2015.00008,https://core.ac.uk/download/288335785.pdf,'Frontiers Media SA',"[{'title': 'Frontiers in Neurorobotics', 'identifiers': ['issn:1662-5218', '1662-5218']}]",core
250573840,2015-08-01T00:00:00,"International audienceNetwork anomalies and attacks represent a serious challenge to ISPs, who need to cope with an increasing number of unknown events that put their networks' integrity at risk. Most of the network anomaly detection systems proposed so far employ a supervised strategy to accomplish their task, using either signature-based detection methods or supervised-learning techniques. The former fails to detect unknown anomalies, exposing the network to severe consequences; the latter requires labeled traffic, which is difficult and expensive to produce. In this paper we introduce a powerful unsupervised approach to detect and characterize network anomalies in the dark, i.e., without relying on signatures or labeled traffic. Unsupervised detection is accomplished by means of robust clustering techniques , combining sub-space clustering with correlation analysis to blindly identify anomalies. To alleviate network operator's post-processing tasks and to speed-up the deployment of effective countermeasures, anomaly ranking and characterization are automatically performed on the detected events. The system is extensively tested with real traffic from the WIDE backbone network, spanning six years of flows captured from a trans-pacific link between Japan and the US, using the MAWILab framework for ground-truth generation. We additionally evaluate the proposed approach with synthetic data, consisting of traffic from an operational network with synthetic attacks. Finally, we compare the performance of the unsupervised detection against different previously used unsupervised detection techniques, as well as against multiple anomaly detectors used in MAWILab",Hunting Attacks in the Dark: Clustering and Correlation Analysis for Unsupervised Anomaly Detection,10.1002/nem.1903,,'Wiley',,core
84396420,2016-01-01T00:00:00,"Import 13/01/2017The main objective of this thesis is to investigate and tackle urgent practical problems involving Vietnamese agriculture. In Vietnam, agriculture is one of the major industries and contributes significantly to the national Gross Domestic Product (GDP). Thus it is necessary to drastically improve Vietnamese agriculture in many aspects, such as national policies, advanced agriculture technologies, applications of computer science and so on. Two problems that are investigated in this thesis are river runoff prediction and boiler efficiency optimization. Since neural networks have proven to be effective methods for modeling, characterizing and predicting several types of sophisticated data, they are chosen as key methods in this thesis.
For the first problem, we investigate some appropriate methods for predicting river runoff. The Srepok River is chosen as a case study. The task of prediction is divided into two cases: long-term and short-term prediction. To deal with the task of long-term prediction, three methods are utilized, such as recurrent fuzzy neural networks (RFNN), a hybrid of RFNN and genetic algorithms, and a physical-based method called SWAT. The experimental results show that the hybrid of RFNN and genetic algorithm is the most effective method.
To predict short-term river runoff, we propose a hybrid of chaotic expressions, RFNN and clustering algorithms consisting of K-means and DBSCAN. Chaotic expressions are used to transform the river runoff data into new data, called phase space, containing much temporal information. Whereas the combination of RFNN and clustering algorithms, which is based on the principle of mixture of experts, is trained and tested with the phase space. The experimental results are conducted with many combinations of RFNN, K-means, DBSCAN, Eulid distance, and Dynamic Time Warping (DTW). The experimental results indicate that the combination of RFNN, DBSCAN and DTW is superior to others.
For the second problem, RFNN and clustering algorithms are used to simulate boiler efficiency. The module of boiler simulation is an important component of a sophisticated soft sensor, namely BEO, which has been deployed at Phu My Fertilizer Plant since 2013. Then the boiler efficiency is forecasted multi-step-ahead and real-time. This task is tackled by using three methods including RFNN, a hybrid of RFNN and stochastic exploration, and RFNN improved by a reinforcement learning algorithm. The experimental results show that BEO is effective and can bring increased benefits to the plant.The main objective of this thesis is to investigate and tackle urgent practical problems involving Vietnamese agriculture. In Vietnam, agriculture is one of the major industries and contributes significantly to the national Gross Domestic Product (GDP). Thus it is necessary to drastically improve Vietnamese agriculture in many aspects, such as national policies, advanced agriculture technologies, applications of computer science and so on. Two problems that are investigated in this thesis are river runoff prediction and boiler efficiency optimization. Since neural networks have proven to be effective methods for modeling, characterizing and predicting several types of sophisticated data, they are chosen as key methods in this thesis.
For the first problem, we investigate some appropriate methods for predicting river runoff. The Srepok River is chosen as a case study. The task of prediction is divided into two cases: long-term and short-term prediction. To deal with the task of long-term prediction, three methods are utilized, such as recurrent fuzzy neural networks (RFNN), a hybrid of RFNN and genetic algorithms, and a physical-based method called SWAT. The experimental results show that the hybrid of RFNN and genetic algorithm is the most effective method.
To predict short-term river runoff, we propose a hybrid of chaotic expressions, RFNN and clustering algorithms consisting of K-means and DBSCAN. Chaotic expressions are used to transform the river runoff data into new data, called phase space, containing much temporal information. Whereas the combination of RFNN and clustering algorithms, which is based on the principle of mixture of experts, is trained and tested with the phase space. The experimental results are conducted with many combinations of RFNN, K-means, DBSCAN, Eulid distance, and Dynamic Time Warping (DTW). The experimental results indicate that the combination of RFNN, DBSCAN and DTW is superior to others.
For the second problem, RFNN and clustering algorithms are used to simulate boiler efficiency. The module of boiler simulation is an important component of a sophisticated soft sensor, namely BEO, which has been deployed at Phu My Fertilizer Plant since 2013. Then the boiler efficiency is forecasted multi-step-ahead and real-time. This task is tackled by using three methods including RFNN, a hybrid of RFNN and stochastic exploration, and RFNN improved by a reinforcement learning algorithm. The experimental results show that BEO is effective and can bring increased benefits to the plant.460 - Katedra informatikyvyhově",Bio-inspirované výpočty,,,Vysoká škola báňská - Technická univerzita Ostrava,,core
33224868,,"[[abstract]]人群監控為視訊監控系統中一項重要應用，主要目的在避免因人潮眾多所產生之 推擠與碰撞意外發生，傳統上視訊人群監控方式，主要由具經驗之專業人員對所監控 畫面，評估人群聚集數量以及擁擠程度，其主要缺點為評估數據較為主觀，無法作為 定量分析與滿足實際應用之需求。因此，自動分析公共空間中人群密集程度與統計人 群數量，為當前視訊監控系統中重要課題之一。因此，本計畫主要在開發一套基於電 腦視覺之人群切割 DSP 系統平台，並預期達成下列三項目標:1) 能適用於多種場景與 環境、2) 能有效偵測與切割人群以及 3) 能執行於 DSP硬體平台。 上述之人群切割 DSP 系統，工作時程初步規畫為三年，第一年計畫(去年度)已審 核通過基於目前初步執行成果，此計畫內容主要說明第二年(今年度)與第三年(明年度) 之規劃。去年度主要著重於開發基於背景相減之前景切割與頭部偵測技術與演算法。 今年度(第二年)工作重點主要分為兩部分，第一部分為建構整合全域行人樣板與區域 HOG特徵之行人分類器，於行人樣板建構部分，有別於文獻中人工標註方式，再給定 一組包含正樣版與負樣板之訓練資料庫，本計畫提出一個自動建構一組行人輪廓權重 樣板之演算法，除有效表示人體於影像中之外觀輪廓外，並同時給予輪廓特徵點不同 權重，以提升樣板比對之鑑別度，接著透過 Biased Boosting概念，將樣板比對結果整 合於 HOG區域分類器 Adaboost選取架構中，以學習出一組有效之 HOG區域行人分類 器。第二部分為將第一年所獲得之可能為行人頭部以下區域切割為數個區塊，透過群 眾透過期望值最大化(Expectation Maximization)演算法，依據第二年所學習之行人分類 器定義出區塊與行人之可能性機率，並估算出所有區塊與頭部之對應關係，以達到群 眾切割之目的。 明年度(第三年)工作重點為將所有人群切割模組實現於 DSP 硬體平台，需考量影 像顯示與儲存格式、記憶體管理及影像函數模組移植相關議題，另一方面，為提升 DSP 平台系統執行效能，程式移植過程中須考量影像資料讀取、色彩空間轉換、迴圈架構 以及資料平行化計算等事項。最後透過先前所拍攝之影片，對系統進行大量測試，最 後分析系統之人群切割準確率與執行效能，最後期望能夠開發出一個能實際應用於視 訊監控領域之人群切割雛型產品。[[abstract]]In tradition, vision-based crowd surveillance is based on experienced person to estimate the number of crowd. However, this way suffers from the drawback that the estimated result is not accurate for further analysis in real application. Therefore, it is critical issue in vision-based surveillance area of how to automatically estimate the crowd number in public area. Currently, vision-based crowd surveillance is still challenging due to occlusion, complex background, pose variance and view difference. Besides, most systems for crowd surveillance are performed on PC-based platform so that it is hard to deploy to various environment. Accordingly, the main objective of this project is to develop a vision-based system for crowd surveillance performed on DSP platform which meets three requirements. It can easily be deployed to various environments; it can effectively detect and segment crowd from videos; it can perform on DSP platform. The time period of this project for developing the proposed system is three years. The first-year project is in progress and its objective is to segment the foreground objects from observed image as well as detect all the head candidates. The objectives in the second-year (this year) project include the learning of pedestrian detector and crowd segmentation. Different from the works in the literature, given a training dataset consisting of positive and negative samples, this project proposes an algorithm for automatically constructing a set of weighted edge templates. A more effective pedestrian detector is learned by imposing the matching results from a classifier based on a set of constructed weighted templates to the boosting framework. The integration of global contour templates and local HOGs is through the adjustment of the hyperplane from the support vector machine. The concept behind is to bias the hyperplane and make it consistent with the template-based classifier at each round of boosting stage. Next, the regions below the detected head candidates are divided into several patches. The problem of crowd segmentation is then formulated as a maximum likelihood estimate (ML) and expectation maximization algorithm is applied to simultaneously achieve patch assignment and human segmentation. The porting of the developed crowd surveillance system from PC to DSP platform is the main task of the third-year project. The main issues to be taken into account are image format and processing, memory arrangement, code transferring. In addition to performance efficiency consideration, the code ported on DSP platform should be optimized in four aspects: image data accessing, color space transformation, loop re-organization, and instruction parallelization. The proposed system on DSP platform will be intensively validated by many videos collected from various scenarios.[[note]]MOST103-2221-E327-03",DSP-Platform Development of Vision-Based Crowd Segmentation Using Expectation Maximization (II),,,科技部,,core
101037806,2015-01-07,"This paper presents the results that we have been achieving with our research involving educational environments based on Intelligent Tutoring Systems (ITS) architecture using a MAS (Multi-Agent System approach. A general guideline was idealized to be use as a reference for our research group to design and to implement intelligent educational software. Specially, interactive educational software modeled as a game. We believe that now we have the agent’s technology that makes possible to build interesting solving problems environments. However, it is very important to remember that educational process is closely connected with country culture. Educational environments that work very well in a specific context will not necessarily perform in the same way in a different place. It can be viewed as a restriction in many ways: as applicability for other educational reality, pedagogical paradigm, and so on. However, we argue that educational process has a “kernel ” of necessities and aspects to be observed. When you intend to work considering learner as central person in the educational process, under the “learn-to-learn ” paradigm. So, it causes an immediately reflection in the way we design and model an educational software. We discuss our ideas and guidelines used to model and to implement our systems under pedagogical viewpoint. We explore the possibilities of designing such systems using a MAS (Muti-agent) approach in order to explore the possibilities of this technique. In our proposal the domain is modeled with reactive agents and some of them we applied techniques of Machine Learning (Reinforcement Learning). The students and the set of pedagogical agents (task agents, assistants, and tutors) were modeled as cognitive agents using BDI architecture (belief, desire, and intention). Due to the space we are not describing the systems in details, but exemplifying how the guidelines were used. 1",Modeling and Implementing Intelligent Educational Environments Using an Interdisciplinary Approach,,,,,core
101291135,2015-01-30,"The Satellite Anomaly Analysis and Prediction System (SAAPS) is a software containing a database of space weather data and satellite anomaly data, tools for plotting and analysis, and models for the prediction of anomalies. The system uses real-time data and can run stand-alone on a computer or remotely over the Internet. The anomaly prediction models use neural networks that have been trained on specific anomaly sets that are related to either surface or internal ESDs. The predictions range from nowcasting up to one-day forecasts. Generally, the predictions are correct in slightly more than 70 % of the events. The model also gives the probability that a specific forecast is correct, which range from poor (50%) to very good (90%). A user can also submit anomaly data for further analysis. Visi",ANALYSIS AND PREDICTION OF SATELLITE ANOMALIES,,,,,core
105056134,2016-08-25,"which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. By introducing the conflicting effects of dynamic changes in blood flow, volume, and blood oxygenation, Balloon model provides a biomechanical compelling interpretation of the BOLD signal. In order to obtain optimal estimates for both the states and parameters involved in this model, a joint filtering (estimate) method has been widely used. However, it is flawed in several aspects (i) Correlation or interaction between the states and parameters is incorporated despite its nonexistence in biophysical reality. (ii) A joint representation for states and parameters necessarily means the large dimension of state space and will in turn lead to huge numerical cost in implementation. Given this knowledge, a dual filtering approach is proposed and demonstrated in this paper as a highly competent alternative, which can not only provide more reliable estimates, but also in a more efficient way. The two approaches in our discussion will be based on unscented Kalman filter, which has become the algorithm of choice in numerous nonlinear estimation and machine learning applications. 1",Research Article Reliable and Efficient Approach of BOLD Signal with Dual Kalman Filtering,,,,,core
144097128,2015-11-25T00:00:00,"A pesquisa em redes neurais artificiais (RNAs) está atualmente experimentando um crescente interesse por modelos que utilizem a variável tempo como um grau de liberdade extra a ser explorado nas representações neurais. Esta ênfase na codificação temporal (temporal coding) tem provocado debates inflamados nas neurociências e áreas correlatas, mas nos últimos anos o surgimento de um grande volume de dados comportamentais e fisiológicos vêm dando suporte ao papel-chave desempenhado por este tipo de representação no cérebro [BALLARD et al. (1998)]. Contribuições ao estudo da representação temporal em redes neurais vêm sendo observadas nos mais diversos tópicos de pesquisa, tais como sistemas dinâmicos não-lineares, redes oscilatórias, redes caóticas, redes com neurônios pulsantes e redes acopladas por pulsos. Como conseqüência, várias tarefas de processamento da informação têm sido investigada via codificação temporal, a saber: classificação de padrões, aprendizagem, memória associativa, controle sensório-motor, identificação de sistemas dinâmicos e robótica. Freqüentemente, porém, não fica muito claro até que ponto a modelagem dos aspectos temporais de uma tarefa contribui para aumentar a capacidade de processamento da informação de modelos neurais. Esta tese busca apresentar, de uma maneira clara e abrangente, os principais conceitos e resultados referentes à proposição de dois modelos de redes neurais não-supervisionadas (RNATs), e como estas lançam mão da codificação temporal para desempenhar melhor a tarefa que lhes é confiada. O primeiro modelo, chamado rede competitiva Hebbiana temporal (competitive temporal Hebbian - CTH), é aplicado especificamente em tarefas de aprendizagem e reprodução de trajetórias do robô manipulador PUMA 560. A rede CTH é uma rede neural competitiva cuja a principal característica é o aprendizado rápido, em apenas uma época de treinamento, de várias trajetórias complexas contendo ) elementos repetidos. As relações temporais da tarefa, representadas pela ordem temporal da trajetória, são capturadas por pesos laterais treinados via aprendizagem hebbiana. As propriedades computacionais da rede CTH são avaliadas através de simulações, bem como através da implementação de um sistema de controle distribuído para o robô PUMA 560 real. O desempenho da rede CTH é superior ao de métodos tabulares (look-up table) tradicionais de aprendizagem de trajetórias robóticas e ao de outras técnicas baseadas em redes neurais, tais como redes recorrentes supervisionadas e modelos de memória associativa bidirecional (BAM). O segundo modelo, chamado rede Auto-Organizável NARX (Self-Organizing NARX-SONARX), é baseado no conhecido algoritmo SOM, proposto por KOHONEN (1997). Do ponto de vista computacional, as propriedades de rede SONARX são avaliadas em diferentes domínios de aplicação, tais como predição de séries temporais caóticas, identificação de um atuador hidráulico e no controle preditivo de uma planta não-linear. Do ponto de vista teórico, demonstra-se que a rede SONARX pode ser utilizada como aproximador assintótico de mapeamentos dinâmicos não-lineares, graças a uma nova técnica de modelagem neural, chamada Memória Associativa Temporal via Quantização Vetorial (MATQV). A MATQV, assim como a aprendizagem hebbiana da rede CTH, é uma técnica de aprendizado associativo temporal. A rede SONARX é comparada com modelos NARX supervisionados, implementados a partir das redes MLP e RBF. Em todos os testes realizados para cada uma das tarefas citadas no parágrafo anterior, a rede SONARX tem desempenho similar ou melhor do que o apresentado por modelos supervisionados tradicionais, com um custo computacional consideravelmente menor. A rede SONARX é também comparada com a rede CTH na parendizagem de trajetórias robóticas complexas, com o intuito de destacar as principais diferenças entre os dois ) tipos de aprendizado associativo. Esta tese também propõe uma taxonomia matemática, baseada na representação por espaço de estados da teoria de sistemas, que visa classificar redes neurais não-supervisionadas temporais com ênfase em suas propriedades computacionais. Esta taxonomia tem como principal objetivo unificar a descrição de modelos neurais dinâmicos, facilitando a análise e a comparação entre diferentes arquiteturas, contrastando suas características representacionais e operacionais. Como exemplo, as redes CTH e a SONARX serão descritas usando a taxonomia proposta.Neural network research is currently witnessing a significant shift of emphasis towards temporal coding, which uses time as an extra degree of freedom in neural representations. Temporal coding is passionately debated in neuroscience and related fields, but in the last few years a large volume of physiological and behavioral data has emerged that supports a key role for temporal coding in the brain [BALLARD et al. (1998)]. In neural networks, a great deal of research is undertaken under the topics of nonlinear dynamics, oscillatory and chaotic networks, spiking neurons, and pulse-coupled networks. Various information processing tasks are investigated using temporal coding, including pattern classification, learning, associative memory, inference, motor control, dynamical systems identification and control, and robotics. Progress has been made that substantially advances the state-of-the-art of neural computing. In many instances, however, it is unclear whether, and to what extent, the temporal aspects of the models contribute to information processing capabilities. This thesis seeks to present, in a clear and collective way, the main issues and results regarding the proposal of two unsupervised neural models, emphasizing how these networks make use of temporal coding to perform better in the task they are engaged in. The first model, called Competitive Temporal Hebbian (CTH) network, is applied specifically to learning and reproduction of trajectories of a PUMA 560 robot. The CTH model is a competitive neural network whose main characteristic is the fast learning, in just one training epoch, of multiple trajectories containing repeated elements. The temporal relationships within the task, represented by the temporal order of the elements of a given trajectory, are coded in lateral synaptic trained with hebbian learning. The computational properties of the CTH network are assessed through simulations, as well ) as through the practical implementation of a distributed control system for the real PUMA 560 robot. The CTH performs better than conventional look-up table methods for robot trajectory learning, and better than other neural-based techniques, such as supervised recurrent networks and bidirectional associative memory models. The second model, called Self-Organizing NARX (SONARX) network, is based on the well-known SOM algorithm by KOHONEN (1997). From the computational view-point, the properties of the SONARX model are evaluated in different application domains, such as prediction of chaotic time series, identification of an hydraulic actuator and predictive control of a non-linear plant. From the theoretic viewpoint, it is shown that the SONARX model can be seen as an asymptotic approximator for nonlinear dynamical mappings, thanks to a new neural modelling technique, called Vector-Quantized Temporal Associative Memory (VQTAM). This VQTAM, just like the hebbian learning rule of the CTH network, is a temporal associative memory techniques. The SONARX network is compared with supervised NARX models which based on the MLP and RBF networks. For all simulations, in each one of the forementioned application domains, the SONARX network had a similar and sometimes better performance than those observed for standard supervised models, with the additional advantage of a lower computational cost. The SONARX model is also compared with the CTH network in trajectory reproduction tasks, in order to contrast the main differences between these two types of temporal associative learning models. In this thesis, it is also proposed a mathematical taxonomy, based on the state-space representation of dynamical systems, for classification of unsupervised temporal neural networks with emphasis in their computational properties. The main goal of this taxonomy is to unify the description of dynamic neural models, ) facilitating the analysis and comparison of different architectures by constrasting their representational and operational characteristics. Is is shown how the CTH and SONARX models can be described using the proposed taxonomy",Temporal unsupervised neural networks for identification and control of dynamical systems,10.11606/T.18.2003.tde-25112015-115752,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,core
79584235,2016-01-01T00:00:00,"abstract: Feature learning and the discovery of nonlinear variation patterns in high-dimensional data is an important task in many problem domains, such as imaging, streaming data from sensors, and manufacturing. This dissertation presents several methods for learning and visualizing nonlinear variation in high-dimensional data. First, an automated method for discovering nonlinear variation patterns using deep learning autoencoders is proposed. The approach provides a functional mapping from a low-dimensional representation to the original spatially-dense data that is both interpretable and efficient with respect to preserving information. Experimental results indicate that deep learning autoencoders outperform manifold learning and principal component analysis in reproducing the original data from the learned variation sources.

A key issue in using autoencoders for nonlinear variation pattern discovery is to encourage the learning of solutions where each feature represents a unique variation source, which we define as distinct features. This problem of learning distinct features is also referred to as disentangling factors of variation in the representation learning literature. The remainder of this dissertation highlights and provides solutions for this important problem.

An alternating autoencoder training method is presented and a new measure motivated by orthogonal loadings in linear models is proposed to quantify feature distinctness in the nonlinear models. Simulated point cloud data and handwritten digit images illustrate that standard training methods for autoencoders consistently mix the true variation sources in the learned low-dimensional representation, whereas the alternating method produces solutions with more distinct patterns. 

Finally, a new regularization method for learning distinct nonlinear features using autoencoders is proposed. Motivated in-part by the properties of linear solutions, a series of learning constraints are implemented via regularization penalties during stochastic gradient descent training. These include the orthogonality of tangent vectors to the manifold, the correlation between learned features, and the distributions of the learned features. This regularized learning approach yields low-dimensional representations which can be better interpreted and used to identify the true sources of variation impacting a high-dimensional feature space. Experimental results demonstrate the effectiveness of this method for nonlinear variation pattern discovery on both simulated and real data sets.Dissertation/ThesisDoctoral Dissertation Industrial Engineering 201",Distinct Feature Learning and Nonlinear Variation Pattern Discovery Using Regularized Autoencoders,,https://core.ac.uk/download/79584235.pdf,,,core
85123517,2016-08-01T00:00:00,"This paper investigates three different technologies for solving a planning and scheduling problem of deploying multiple robots in a retirement home environment to assist elderly residents. The models proposed make use of standard techniques and solvers developed in AI planning and scheduling, with two primary motivations. First, to find a planning and scheduling solution that we can deploy in our real-world application. Second, to evaluate planning and scheduling technology in terms of the ``model-and-solve'' functionality that forms a major research goal in both domain-independent planning and constraint programming. Seven variations of our application are studied using the following three technologies: PDDL-based planning, time-line planning and scheduling, and constraint-based scheduling. The variations address specific aspects of the problem that we believe can impact the performance of the technologies while also representing reasonable abstractions of the real world application. We evaluate the capabilities of each technology and conclude that a constraint-based scheduling approach, specifically a decomposition using constraint programming, provides the most promising results for our application. PDDL-based planning is able to find mostly low quality solutions while the timeline approach was unable to model the full problem without alterations to the solver code, thus moving away from the model-and-solve paradigm. It would be misleading to conclude that constraint programming is ``better'' than PDDL-based planning in a general sense, both because we have examined a single application and because the approaches make different assumptions about the knowledge one is allowed to embed in a model. Nonetheless, we believe our investigation is valuable for AI planning and scheduling researchers as it highlights these different modelling assumptions and provides insight into avenues for the application of AI planning and scheduling for similar robotics problems. In particular, as constraint programming has not been widely applied to robot planning and scheduling in the literature, our results suggest significant untapped potential in doing so.California Institute of Technology. Keck Institute for Space Studie",Robots in Retirement Homes: Applying Off-the-Shelf Planning and Scheduling to a Team of Assistive Robots,10.1613/jair.5306,https://core.ac.uk/download/85123517.pdf,'AI Access Foundation',"[{'title': 'Journal of Artificial Intelligence Research', 'identifiers': ['issn:1943-5037', 'issn:1076-9757', '1076-9757', '1943-5037']}]",core
129712012,2016-07-01T00:00:00,"This research work presents a novel Cognitive Task Planning framework for Smart Industrial Robots. The framework makes an industrial mobile manipulator robot Cognitive by applying Semantic Web Technologies. It also introduces a novel Navigation Among Movable Obstacles algorithm for robots navigating and manipulating inside a ﬁrm. 



The objective of Industrie 4.0 is the creation of Smart Factories: modular ﬁrms provided with cyber-physical systems able to strong customize products under the condition of highly ﬂexible mass-production. Such systems should real-time communicate and cooperate with each other and with humans via the Internet of Things. They should intelligently adapt to the changing surroundings and autonomously navigate inside a ﬁrm while moving obstacles that occlude free paths, even if seen for the ﬁrst time. At the end, in order to accomplish all these tasks while being eﬃcient, they should learn from their actions and from that of other agents. 



Most of existing industrial mobile robots navigate along pre-generated trajectories. They follow ectriﬁed wires embedded in the ground or lines painted on th eﬂoor. When there is no expectation of environment changes and cycle times are critical, this planning is functional. When workspaces and tasks change frequently, it is better to plan dynamically: robots should autonomously navigate without relying on modiﬁcations of their environments. Consider the human behavior: humans reason about the environment and consider the possibility of moving obstacles if a certain goal cannot be reached or if moving objects may signiﬁcantly shorten the path to it. This problem is named Navigation Among Movable Obstacles and is mostly known in rescue robotics. This work transposes the problem on an industrial scenario and tries to deal with its two challenges: the high dimensionality of the state space and the treatment of uncertainty. 



The proposed NAMO algorithm aims to focus exploration on less explored areas. For this reason it extends the Kinodynamic Motion Planning by Interior-Exterior Cell Exploration algorithm. The extension does not impose obstacles avoidance: it assigns an importance to each cell by combining the eﬀorts necessary to reach it and that needed to free it from obstacles. The obtained algorithm is scalable because of its independence from the size of the map and from the number, shape, and pose of obstacles. It does not impose restrictions on actions to be performed: the robot can both push and grasp every object. Currently, the algorithm assumes full world knowledge but the environment is reconﬁgurable and the algorithm can be easily extended in order to solve NAMO problems in unknown environments. The algorithm handles sensor feedbacks and corrects uncertainties. 



Usually Robotics separates Motion Planning and Manipulation problems. NAMO forces their combined processing by introducing the need of manipulating multiple objects, often unknown, while navigating. Adopting standard precomputed grasps is not suﬃcient to deal with the big amount of existing diﬀerent objects. A Semantic Knowledge Framework is proposed in support of the proposed algorithm by giving robots the ability to learn to manipulate objects and disseminate the information gained during the fulﬁllment of tasks. The Framework is composed by an Ontology and an Engine. The Ontology extends the IEEE Standard Ontologies for Robotics and Automation and contains descriptions of learned manipulation tasks and detected objects. It is accessible from any robot connected to the Cloud. It can be considered a data store for the eﬃcient and reliable execution of repetitive tasks; and a Web-based repository for the exchange of information between robots and for the speed up of the learning phase. No other manipulation ontology exists respecting the IEEE Standard and, regardless the standard, the proposed ontology diﬀers from the existing ones because of the type of features saved and the eﬃcient way in which they can be accessed: through a super fast Cascade Hashing algorithm. The Engine lets compute and store the manipulation actions when not present in the Ontology. It is based on Reinforcement Learning techniques that avoid massive trainings on large-scale databases and favors human-robot interactions. 



The overall system is ﬂexible and easily adaptable to diﬀerent robots operating in diﬀerent industrial environments. It is characterized by a modular structure where each software block is completely reusable. Every block is based on the open-source Robot Operating System. Not all industrial robot controllers are designed to be ROS-compliant. This thesis presents the method adopted during this research in order to Open Industrial Robot Controllers and create a ROS-Industrial interface for them",Cognitive Task Planning for Smart Industrial Robots,,https://core.ac.uk/download/129712012.pdf,,,core
188165729,2016-01-01T00:00:00,"Artificial Intelligence without Using Ontological Data about a Presupposed Reality. This paper introduces an original model to provide software agents and robots with the capacity of learning by interpreting regularities in their stream of sensorimotor experience rather than by exploiting data that would give them ontological information about a predefined domain. Specifically, this model pulls inspiration from : a) the movement of embodied cognition, b) the philosophy of knowledge, c) constructivist epistemology, and d) the theory of enaction. Respectively to these four influences : a) Our agents discover their environment through their body’s active capacity of experimentation. b) They do not know their environment “ as such” but only “ as they can experience it”. c) They construct knowledge from regularities of sensorimotor experience. d) They have some level of constitutive autonomy. Technically, this model differs from the traditional perception / cognition/ action model in that it rests upon atomic sensorimotor experiences rather than separating percepts from actions. We present algorithms that implement this model, and we describe experiments to validate these algorithms. These experiments show that the agents exhibit a certain form of intelligence through their behaviors, as they construct proto-ontological knowledge of the phenomena that appear to them when they observe persistent possibilities of sensorimotor experiences in time and space. These results promote a theory of artificial intelligence without ontological data about a presupposed reality. An application includes a more robust way of creating robots capable of constructing their own knowledge and goals in the real world, which could be initially unknown to them and un-modeled by their designers.Cet article propose un modèle original pour doter des agents informatiques ou des robots de la capacité d’apprendre en interprétant des régularités dans leur flux d’expériences sensorimotrices plutôt qu’en exploitant des données qui leur apporteraient des informations ontologiques sur un domaine prédéfini. Ce modèle s’inspire en particulier de : a) le courant de la cognition incarnée, b) la philosophie de la connaissance, c) l’épistémologie constructiviste, et d) la théorie de l’énaction. Respectivement à ces quatre influences : a) Nos agents découvrent leur environnement à travers les capacités expérimentales actives de leur corps. b) Ils ne connaissent pas leur environnement «en soi » mais uniquement «en ce qu’ils peuvent en faire l’expérience » . c) Ils construisent leurs connaissances à partir de régularités d’expériences sensorimotrices. d) Ils disposent d’une certaine autonomie constitutive.
Techniquement, ce modèle se distingue du modèle perception/cognition/action classique par le fait qu’il considère des expériences sensorimotrices atomiques au lieu de séparer les percepts et les actions. Nous présentons des algorithmes qui implémentent ce modèle, et décrivons des expérimentations permettant de les valider. Les expérimentations montrent que les agents exhibent une certaine forme d’intelligence dans leurs comportements en construisant une connaissance protoontologique des phénomènes qui apparaissent à eux quand ils constatent des possibilités d’expériences sensorimotrices persistantes dans l’espace et le temps. Ces résultats promeuvent une théorie de l’intelligence artificielle sans données ontologiques sur une réalité présupposée, avec, comme perspectives applicatives, des robots capables de construire leurs propres connaissances et objectifs dans le monde réel, initialement inconnu d’eux et non modélisé par leur concepteur.Georgeon Olivier, Mille Alain, Gay Simon. Intelligence artificielle sans données ontologiques sur une réalité présupposée. In: Intellectica. Revue de l'Association pour la Recherche Cognitive, n°65, 2016/1. Nouvelles approches en Robotique Cognitive. pp. 143-168",Intelligence artificielle sans données ontologiques sur une réalité présupposée,10.3406/intel.2016.1793,,'PERSEE Program',,core
103802348,2016-01-18,"Brain-Computer Interfaces (BCIs) usually propose typing strategies to restore communication for paralyzed and aphasic people. A more natural way would be to use speech BCI directly controlling a speech synthesizer. Toward this goal, a prerequisite is the development a synthesizer that should i) produce intelligible speech, ii) run in real time, iii) depend on as few parameters as possible, and iv) be robust to error fluctuations on the control parameters. In this context, we describe here an articulatory-to-acoustic mapping approach based on deep neural network (DNN) trained on electromagnetic articulography (EMA) data recorded synchronously with produced speech sounds. On this corpus, the DNN-based model provided a speech synthesis quality (as assessed by automatic speech recognition and behavioral testing) comparable to a state-of-the-art Gaussian mixture model (GMM), yet showing higher robustness when noise was added to the EMA coordinates. Moreover, to envision BCI applications, this robustness was also assessed when the space covered by the 12 original articulatory parameters was reduced to 7 parameters using deep auto-encoders (DAE). Given that this method can be implemented in real time, DNN-based articulatory speech synthesis seems a good candidate for speech BCI applications. Index Terms: articulatory speech synthesis, brain computer interface (BCI), deep neural networks, deep auto-encoder",Robust Articulatory Speech Synthesis using Deep Neural Networks for BCI Applications,,,,,core
82915343,2017-04-25T00:00:00,"Complex networks are ubiquitous in nature. Numerous neurological diseases, such as

Alzheimer's, Parkinson's, epilepsy are caused by the abnormal collective behaviour of

neurons in the brain. In particular, there is a strong evidence that Parkinson's disease is

caused by the synchronisation of neurons, and understanding how and why such synchronisation

occurs will bring scientists closer to the design and implementation of appropriate

control to support desynchronisation required for the normal functioning of the brain. In

order to study the emergence of (de)synchronisation, it is necessary first to understand

how the dynamical behaviour of the system under consideration depends on the changes

in systems parameters. This can be done using a powerful mathematical method, called

bifurcation analysis, which allows one to identify and classify different dynamical regimes,

such as, for example, stable/unstable steady states, Hopf and fold bifurcations, and find

periodic solutions by varying parameters of the nonlinear system.

In real-world systems, interactions between elements do not happen instantaneously

due to a finite time of signal propagation, reaction times of individual elements, etc.

Moreover, time delays are normally non-constant and may vary with time. This means

that it is vital to introduce time delays in any realistic model of neural networks. In

this thesis, I consider four different models. First, in order to analyse the fundamental

properties of neural networks with time-delayed connections, I consider a system of four

coupled nonlinear delay differential equations. This model represents a neural network,

where one subsystem receives a delayed input from another subsystem. The exciting

feature of this model is the combination of both discrete and distributed time delays, where

distributed time delays represent the neural feedback between the two sub-systems, and the

discrete delays describe neural interactions within each of the two subsystems. Stability

properties are investigated for different commonly used distribution kernels, and the results

are compared to the corresponding stability results for networks with no distributed delays.

It is shown how approximations to the boundary of stability region of an equilibrium point

can be obtained analytically for the cases of delta, uniform, and gamma delay distributions.

Numerical techniques are used to investigate stability properties of the fully nonlinear

system and confirm our analytical findings.

In the second part of this thesis, I consider a globally coupled network composed of

active (oscillatory) and inactive (non-oscillatory) oscillators with distributed time delayed

coupling. Analytical conditions for the amplitude death, where the oscillations are quenched,

are obtained in terms of the coupling strength, the ratio of inactive oscillators, the width

of the uniformly distributed delay and the mean time delay for gamma distribution. The

results show that for uniform distribution, by increasing both the width of the delay distribution

and the ratio of inactive oscillators, the amplitude death region increases in the

mean time delay and the coupling strength parameter space. In the case of the gamma

distribution kernel, we find the amplitude death region in the space of the ratio of inactive

oscillators, the mean time delay for gamma distribution, and the coupling strength for

both weak and strong gamma distribution kernels.

Furthermore, I analyse a model of the subthalamic nucleus (STN)-globus palidus (GP)

network with three different transmission delays. A time-shift transformation reduces the

model to a system with two time delays, for which the existence of a unique steady

state is established. Conditions for stability of the steady state are derived in terms of

system parameters and the time delays. Numerical stability analysis is performed using

traceDDE and DDE-BIFTOOL in Matlab to investigate different dynamical regimes in

the STN-GP model, and to obtain critical stability boundaries separating stable (healthy)

and oscillatory (Parkinsonian-like) neural ring. Direct numerical simulations of the fully

nonlinear system are performed to confirm analytical findings, and to illustrate different

dynamical behaviours of the system.

Finally, I consider a ring of n neurons coupled through the discrete and distributed

time delays. I show that the amplitude death occurs in the symmetric (asymmetric) region

depending on the even (odd) number of neurons in the ring neural system. Analytical

conditions for linear stability of the trivial steady state are represented in a parameter space

of the synaptic weight of the self-feedback and the coupling strength between the connected

neurons, as well as in the space of the delayed self-feedback and the coupling strength

between the neurons. It is shown that both Hopf and steady-state bifurcations may occur

when the steady state loses its stability. Stability properties are also investigated for

different commonly used distribution kernels, such as delta function and weak gamma

distributions. Moreover, the obtained analytical results are confirmed by the numerical

simulations of the fully nonlinear system",Dynamics of neural systems with time delays,,https://core.ac.uk/download/82915343.pdf,,,core
148025514,2016-11-09T08:27:53,"In cities, urban places provide a socio-cultural habitat for people to counterbalance the daily grind of urban life, an environment away from home and work. Places provide an environment for people to communicate, share perspectives, and in the process form new social connections. Due to the active role of places to the social fabric of city life, it is important to understand how people perceive and experience places. One fundamental construct that relates place and experience is ambiance, i.e., the impressions we ubiquitously form when we go out. Young people are key actors of urban life, specially at night, and as such play an equal role in co-creating and appropriating the urban space. Understanding how places and their youth inhabitants interact at night is a relevant urban issue. Until recently, our ability to assess the visual and perceptual qualities of urban spaces and to study the dynamics surrounding youth experiences in those spaces have been limited partly due to the lack of quantitative data. However, the growth of computational methods and tools including sensor-rich mobile devices, social multimedia platforms, and crowdsourcing tools have opened ways to measure urban perception at scale, and to deepen our understanding of nightlife as experienced by young people. In this thesis, as a first contribution, we present the design, implementation and computational analysis of four mobile crowdsensing studies involving youth populations from various countries to understand and infer phenomena related to urban places and people. We gathered a variety of explicit and implicit crowdsourced data including mobile sensor data and logs, survey responses, and multimedia content (images and videos) from hundreds of crowdworkers and thousands of users of mobile social networks. Second, we showed how crowdsensed images can be used for the computational characterization and analysis of urban perception in indoor and outdoor places. For both place types, urban perception impressions were elicited for several physical and psychological constructs using online crowdsourcing. Using low-level and deep learning features extracted from images, we automatically inferred crowdsourced judgments of indoor ambiance with a maximum R2 of 0.53 and outdoor perception with a maximum R2 of 0.49. Third, we demonstrated the feasibility to collect rich contextual data to study the physical mobility, activities, ambiance context, and social patterns of youth nightlife behavior. Fourth, using supervised machine learning techniques, we automatically classified drinking behavior of young people in an urban, real nightlife setting. Using features extracted from mobile sensor data and application logs, we obtained an overall accuracy of 76.7%. While this thesis contributes towards understanding urban perception and youth nightlife patterns in specific contexts, our research also contributes towards the computational understanding of urban places at scale with high spatial and temporal resolution, using a combination of mobile crowdsensing, social media, machine learning, multimedia analysis, and online crowdsourcing",Computational Analysis of Urban Places Using Mobile Crowdsensing,10.5075/epfl-thesis-7243,https://core.ac.uk/download/148025514.pdf,"Lausanne, EPFL",,core
141703177,2017-06-16T00:00:00,"Part 1: AlgorithmsInternational audienceThe size, complexity and dimensionality of data collections are ever increasing from the beginning of the computer era. Clustering is used to reveal structures and to reduce large amounts of raw data. There are two main issues when clustering based on unsupervised learning, such as Growing Neural Gas (GNG) [9], is performed on vast high dimensional data collection – the fast growth of computational complexity with respect to growing data dimensionality, and the specific similarity measurement in a high-dimensional space. These two factors reduce the effectiveness of clustering algorithms in many real applications. The growth of computational complexity can be partially solved using the parallel computation facilities, such as High Performance Computing (HPC) cluster with MPI. An effective parallel implementation of GNG is discussed in this paper, while the main focus is on minimizing of interprocess communication. The achieved speed-up was better than previous approach and the results from the standard and parallel version of GNG are same",Optimalization of Parallel GNG by Neurons Assigned to Processes,10.1007/978-3-319-59105-6_6,,'Springer Science and Business Media LLC',,core
84145810,2016-09-21T00:00:00,"Recent work has attempted to deliver optimized distributed energy resource management, including the use of demand side management through smart homes. This aims to reduce power transmission losses, increase the generation share of renewable energy sources and create new markets through peak shaving and flexibility markets. Further, this leverages the development of product models at the device, building, and network level within the operational lifecycle stage, beyond the conventional role of BIM between design and construction stages. However, the management of heterogeneous software entities, incompatible data models and domain perspectives, across systems of systems of significant complexity, represent critical barriers to sustainable urban energy solutions and leads to a highly challenging problem space. The presented work describes a systemic approach based on the concept of holonic systems, which exemplify the role of autonomy, belonging, connectivity, diversity and emergence across entities. This reduces the decision complexity of the problem and facilitates the implementation of optimized solutions in real power systems in a scalable and robust manner. Further, the concept of a flexibility market is introduced, whereby smart appliance owners are able to sell load curtailment and deferment to a local aggregator, which interfaces between a small number of homes and a distribution system operator. Artificial intelligence is present at each of the entities in order to express constraints, trade energy and flexibility, and optimize the network management decisions within that entity’s scope. Specifically, this paper focuses on enabling interoperability between system entities such as smart homes, local load aggregators, and last mile network operators. This interoperability is achieved through ontological modelling of the domain, based on the existing standards of CIM, OpenADR, and energy@home. The produced ontology utilizes description logic to formalize the concepts, relationships and properties of the domain. A use case is presented of applying the ontology within a multi-agent system, which enables the optimization of day-ahead markets, load balancing, and stochastic renewable generation, and closely aligns with the holonic approach to deliver a holonic multi-agent system. The use case assumes a scenario in line with the emerging energy landscape of a district of domestic prosumers, with a high penetration of micro-generation, energy storage and electric vehicles. Initial results demonstrate interoperability between heterogeneous agents through ontological modelling based on an integration and extension of existing standards, which acts as a proof of concept for the approach",Semantic interoperability for holonic energy optimization of connected smart homes and distributed energy resources,10.1201/9781315386904,,'Informa UK Limited',,core
61691505,2016-08-26T07:00:00,"Commercial and residential buildings are responsible for a substantial portion of total global energy consumption and as a result make a significant contribution to global carbon emissions.  Hence, energy-saving goals that target buildings can have a major impact in reducing environmental damage. During building operation, a significant amount of energy is wasted due to equipment and human-related faults. To reduce waste, today\u27s smart buildings monitor energy usage with the aim of identifying abnormal consumption behaviour and notifying the building manager to implement appropriate energy-saving procedures. To this end, this research proposes the \textit{ensemble anomaly detection} (EAD) framework. The EAD is a generic framework that combines several anomaly detection classifiers using majority voting. This anomaly detection classifiers are formed using existing machine learning algorithm. It is assumed that each anomaly classifier has equal weight. More importantly, to ensure diversity of anomaly classifiers, the EAD is implemented by combining pattern-based and prediction-based anomaly classifiers. For this reason, this research also proposes a new pattern-based anomaly classifier, the \textit{collective contextual anomaly detection using sliding window} (CCAD-SW) framework. The CCAD-SW, which is also a machine leaning-based framework that identifies anomalous consumption patterns using overlapping sliding windows. The EAD framework combines the CCAD-SW, which is implemented using autoencoder, with two prediction-based anomaly classifiers that are implemented using the support vector regression and random forest machine-learning algorithms. In addition, it determines an ensemble threshold that yields an anomaly classifier with optimal anomaly detection capability and false positive minimization. Results show that the EAD performs better than the individual anomaly detection classifiers. In the EAD framework, the optimal ensemble anomaly classifier is not attained by combining the individual learners at their respective optimal performance levels. Instead, an ensemble threshold combination that yields the optimal anomaly classifier was identified by searching through the ensemble threshold space. The research was evaluated using real-world data provided by Powersmiths, located in Brampton, Ontario, Canada",Collective Contextual Anomaly Detection for Building Energy Consumption,,https://core.ac.uk/download/61691505.pdf,Scholarship@Western,,core
144821921,2017-09-19,"‘In faith, I do not love thee with mine eyes, For they in thee a thousand errors note; But ‘tis my heart that loves what they despise …’ 1 This sonnet and the ancient Japanese notion of wabi-sabi view aesthetics or beauty as imperfect, impermanent and incomplete. Rather than celebrating the human diversity created by our ‘imperfections’, today's society increasingly focuses on them as ‘areas for improvement’, often via a doctor’s scalpel or the latest gadget. Developments in science, technology, engineering, mathematics and medicine (STEMM) promise a tomorrow where ‘errors’ or ‘deficiencies’ in an organism’s genetic and/or phenotypic makeup can be modulated, enhanced, corrected, redefined or eradicated by, for instance, networks of biological nanomachines. Upgraded organisms will be convolutions of organic parts, electronic components, microchips, and biomechanotronic devices. Humans 1.0, Humans 2.0 and transhumans will live in new fully immersive worlds (virtual reality), inhabit a modified real world (augmented reality), and exist with an altered body schema (mixed-reality). This future world could be a place of total technological convergence, where it may not be possible to ensure privacy of an individual’s thoughts. It could also be a place where people can be subjected to Social Engineering and manipulation, including the potential for viruses and malware infecting the brain or body, as well as new forms of external control of individuals by third parties. In this discussion paper, we will explore the potential privacy, security, and ethical issues raised by humanmachine mergers. The focus is on research, development and products at the intersection of robotics, artificial intelligence, Big Data, and smart computing. We suggest that there is a need for a more holistic approach to the assessment of technology and its governance. Additionally, we suggest that in order to determine how the law will need to respond to this particular future space, it is necessary to understand the full impacts of human-machine mergers on societies and our planet – to go beyond these three issues. Since STEMM-related activities are promising a cornucopia of future spaces, we will propose that the problems of governance and assessment require a new conception of ‘responsible research and innovation’, one that is fulfilled by our recently proposed FLE5 SH framework.2 To some extent the FLE5 SH framework can be seen as allowing the formation of a social contract, whereby all stakeholders are required to engage in a review of this wider spectrum of the possible impacts of technologies. We suggest that a Precautionary Principle approach may be of assistance in considering the impacts of technologies, remembering that especially in the context of software based systems, it is always useful to think first and bugfix later",Governance and Assessment of Future Spaces: A Discussion of Some Issues Raised by the Possibilities of Human-Machine Mergers,10.5281/zenodo.896109,https://core.ac.uk/download/pdf/144821921.pdf,,,core
82916324,2016-01-01T00:00:00,"Major natural or man-made disasters such as Hurricane Katrina or the 9/11 terror attacks pose significant challenges for emergency responders. First, they have to develop an understanding of the unfolding event either using their own resources or through third-parties such as the local population and agencies. Second, based on the information gathered, they need to deploy their teams in a flexible manner, ensuring that each team has the best available capabilities to perform tasks successfully as they navigate a space that may have significantly changed in structure from its pre-disaster state. Third, given the dynamic nature of a disaster space, and the uncertainties involved in performing rescue missions, information about the disaster space and the actors within it needs to be stored and verified to ensure that responders are always acting on up-to-date and trusted information. Against this background, this paper proposes a novel disaster response system called HAC-ER that addresses some of the situational awareness and coordination challenges faced by emergency responders in real-world disaster environments. Informed by focus groups with domain experts and real-world trials with volunteers from a number of organisations, HAC-ER interweaves humans and agents, both robotic and software, in social relationships that augment their individual and collective capabilities. HAC-ER thus demonstrates how such Human-Agent Collectives (HACs) can address key challenges in disaster response. Specifically, we show how HAC-ER utilises crowdsourcing combined with machine learning to gather most important situational awareness from large streams of reports posted by members of the public and trusted organisations. We then show how this information can inform human-agent teams in coordinating multi-UAV deployments, as well as task planning for responders on the ground. Finally, HAC-ER incorporates a provenance infrastructure for tracking the provenance of information shared across the entire system to ensure its accountability. A Provenance Agent was also developed to monitor recorded provenance data to help stakeholders in the system to react to changes during an operation in a timely manner. We individually validate each of these elements of HAC-ER and show how they perform against standard (non-HAC) baselines. In summary, this paper describes a prototype system, validated by real-world emergency responders, that combines several state-of-the-art techniques for integrating humans and agents, and illustrates, for the first time, how such an approach can enable more effective disaster response operations",A disaster response system based on human-agent collectives,10.1613/jair.5098,,'AI Access Foundation',,core
84503725,2017-01-01T00:00:00,"– Nel corso di questo lavoro ho inteso approfondire e discutere le seguenti tesi: 1. il code, ossia il

software e l’hardware che costituiscono il cyberspazio, impone un assetto normativo sul comportamento individuale

e collettivo nel Web. Con ciò non si nega la funzione regolativa del diritto sul cyberspazio; si pensi ad

esempio alle sanzioni previste dalle leggi sul copyright, sul diritto contrattuale, sulla diffamazione e sull’oscenità.

Pur tuttavia, Internet rappresenta un universo di flussi e di attriti restio all’imposizione top-down di qualsivoglia

governance estranea ai propri utenti. Il code è performativo: “ciò che dice fa”; 2. il cyberspazio è un luogo ambivalente,

ove molte attività si sovrappongono alle attività del mondo reale e molte attività gli sono peculiari, denotando

la plasticità che gli è propria. Il tasso di crescita dell’innovazione tecnologica continua ad aumentare ed è

accompagnato da nuove insidiose forme di invasione della sfera privata delle persone. È forte la tentazione di rinunciare

ai propri diritti per godere del paradiso tecnologico che ci viene offerto. Nell’ecosistema digitale compare

una nuova entità – la persona digitale – quale esito tecnologico della riconfigurazione della nozione classica di

persona; 3. lo sviluppo delle applicazioni 2.0 permette la connessione tra persone e loro corrispondenti identità

digitali: soggetti che formano legami senza vincoli di spazio e di compresenza fisica diventano parte di uno sciame

digitale. La nostra privacy (e in particolar modo quella dei minori, nativi digitali) è progressivamente erosa dalla

nostra crescente accondiscendenza, apatia, indifferenza o supporto esplicito a misure che ci sono presentate

come indispensabili o innocue. Se è ancora prematuro affermare che la privacy è ormai morta, le giovani generazioni

sono protagoniste, volenti o no, di una prassi culturale e di una socializzazione primaria lontane dal concetto

di privacy; 4. la quarta tesi, con cui concludo questo contributo, approfondisce la questione se si possono 

ancora ricavare spazi per forme di governance efficaci a tutelare la persona digitale: il suo diritto alla dignità,

all’habeas data e alla riservatezza dei dati personali in Internet.– The purpose of this work is to discuss and investigate the following theses: 1. the code, that is to say

the software and the hardware cyberspace is made of, imposes a normative set-up both on Web collective and

individual conduct. By that, law’s regulative task on Web can’t be denied; for instance, think about sanctions imposed

by laws about copyright, contracts, slander and obscenity. Yet Internet represents a universe made of

flows and frictions, unwilling to whatever top-down governance alien to its users. Code is performative: “what it

says, it does”; 2. cyberspace is an ambivalent place, where many activities are laid on top of real world’s ones

and many others are peculiar to it, showing its own plasticity. Technological innovation’s growth rate is still increasing,

together with new insidious forms of invasion of people’s private life. It is still strong the temptation to

waive one’s own rights only to enjoy the technological paradise we are offered. A new entity – digital person –

makes its appearance on digital ecosystem, as technological outcome of classical person concept’s reconfiguration;

3. the development of 2.0 applications allows to put into connection people and their digital identities: individuals

get involved in a digital swarm, forming links free from any territorial and simultaneous physical presence’s

chains. Our privacy (in particular, that of the minors, born in the digital era) is progressively eaten away by

our rising indulgency, apathy, unconcern and explicit support to measures, shown us as necessary and harmless.

If it is yet untimely to say that privacy is almost death, new generations, whether they like it or not, are playing

a leading role in a cultural praxis and in a primary socialization which is far from the concept of privacy; 4. the

fourth thesis, summing up this note, investigates if it is yet possible or not to give space to new effective forms of

governance, appointed to defend the digital person: its rights to dignity, habeas data and personal data privacy","Il minore come persona digitale.

Regole, tutele e privacy dei minori sul Web",10.15160/2038-1034/1417,,,,core
84067306,2017-06-23T00:00:00,"Vortex coronagraphs are among the most promising solutions to perform high contrast imaging at small angular separations from bright stars. They enhance the dynamic range at very small inner working angle (down to the diffraction limit of the telescope) and provide a clear 360 degree discovery space for high-contrast direct imaging of exoplanets. In 2013, we installed and commissioned an L-band coronagraph in LBTI/LMIRCam and obtained outstanding images of the four planets around HR8799 during the first hours on sky. In this presentation, we will present the results of the latest data reduction performed with the VIP software that is developed at the University of Liège and that features state-of-the-art image processing algorithms inherited from the field of background subtraction in computer vision (including machine learning algorithms and low rank modeling algorithms). We will also present the results obtained with the second L- and M-band coronagraph that was recently installed in LMIRCam to enable binocular Vortex observations. During the first observations (October 2016), we tested and validated a new real-time post-coronagraphic tip-tilt sensing technique (called QACITS) to quickly align each beam on the center of their respective Vortex coronagraph and obtained observations of a young star showing disk features near the resolution limit of each aperture. Finally, we will present some exciting prospects for the Vortex coronagraph that will be installed on VISIR and ELT/METIS","Latest results with LBTI's Vortex coronagraph: real-time tip/tilt sensing, new data reduction algorithms, and YSO observations",,https://orbi.uliege.be/handle/2268/212293,,,core
162017603,2016-01-01T00:00:00,"Robust multi-user detection (MUD) methods based on space division multiple access (SDMA) techniques are essential to efficiently exploit the electromagnetic spectrum. In this paper, an adaptive Genetic Algorithm-based tool for SDMA-OFDM Systems (GASOS) is developed to improve the performance and computational complexity in cases of fully-loaded and overloaded multi-user scenarios. The data flow in GASOS is appropriate in pipelining and parallelization to reduce operational time. A new GASOS-based MUD hardware design for SDMA-OFDM systems is proposed using FPGA architecture. The design details are presented together with their planned operational modules. Resource utilization is optimized, and the total number of clock cycles required is found to be 15 initially, in addition to one clock cycle per member of algorithm population. A clock frequency of 100 MHz is used and implementation is carried out on Xilinx® Virtex-6 FPGA, built in the development platform ML605 edition with JTAG Hardware Co-simulation. According to the results obtained from the developed algorithm and implementation tools, a high number of users can be physically possible and provided with support. Real-time based implementation of MUD systems has the potential to play a major role in next-generation communication systems",FPGA Implementation of Multi-User Detection Genetic Algorithm Tool for SDMA-OFDM Systems,10.1007/s11277-015-2986-x,,'Springer Science and Business Media LLC',,core
77229041,2016-04-01T00:00:00,"Optical and radar satellite remote sensing have proven to provide essential crisis information in case of natural disasters, humanitarian relief activities and civil security issues in a growing number of cases through mechanisms such as the Copernicus Emergency Management Service (EMS) of the European Commission or the International Charter ‘Space and Major Disasters’.

The aforementioned programs and initiatives make use of satellite-based rapid mapping services aimed at delivering reliable and accurate crisis information after natural hazards.

Although these services are increasingly operational, they need to be continuously updated and improved through research and development (R&D) activities. The principal objective of ASAPTERRA (Advancing SAR and Optical Methods for Rapid Mapping), the ESA-funded R&D project being described here, is to improve, automate and, hence, speed-up geo-information extraction procedures in the context of natural hazards response. This is performed through the development, implementation, testing and validation of novel image processing methods using optical and Synthetic Aperture Radar (SAR) data. The methods are mainly developed based on data of the German radar satellites TerraSAR-X and TanDEM-X, the French satellite missions Pléiades-1A/1B as well as the ESA missions Sentinel-1/2 with the aim to better characterize the potential and limitations of these sensors and their synergy. The resulting algorithms and techniques are evaluated in real case applications during rapid mapping activities. 

The project is focussed on three types of natural hazards: floods, landslides and fires. Within this presentation an overview of the main methodological developments in each topic is given and demonstrated in selected test areas. The following developments are presented in the context of flood mapping: a fully automated Sentinel-1 based processing chain for detecting open flood surfaces, a method for the improved detection of flooded vegetation in Sentinel-1data using Entropy/Alpha decomposition, unsupervised Wishart Classification, and object-based post-classification as well as semi-automatic approaches for extracting inundated areas and flood traces in rural and urban areas from VHR and HR optical imagery using machine learning techniques. Methodological developments related to fires are the implementation of fast and robust methods for mapping burnt scars using change detection procedures using SAR (Sentinel-1, TerraSAR-X) and HR optical (e.g. SPOT, Sentinel-2) data as well as the extraction of 3D surface and volume change information from Pléiades stereo-pairs. In the context of landslides, fast and transferable change detection procedures based on SAR (TerraSAR-X) and optical (SPOT) data as well methods for extracting the extent of landslides only based on polarimetric VHR SAR (TerraSAR-X) data are presented","Improving the extraction of crisis information in the context of flood, fire, and landslide rapid mapping using SAR and optical remote sensing data",,,,,core
478869500,2017-01-01T00:00:00,"2017 annual report for the Blue Waters ProjectNSF OCI-0725070NSF ACI-12389932017 Blue Waters Annual Report Table of Contents
18.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence in Star-Forming Molecular Clouds on the Blue Waters System
20.	Tiziana Di Matteo, Supermassive Black Holes at the Cosmic Frontier
22.	Gilbert Holder, Theoretical Astrophysics and Data Analysis
24.	Eliu Huerta, Detection of Gravitational Wave Sources in Dense Stellar Environments
26.	Eliu Huerta, Deep Neural Networks to Enable Real-Time Multimessenger Astrophysics
28.	Thomas W. Jones, Toward Robust Magnetohydrodynamic Simulations of Galaxy Cluster Formation
30.	Eric J. Lentz, Exploring the Nature of Exploding Massive Stars with High Resolution
32.	Deborah Levin, Modeling Plasma Flows with Kinetic Approaches using Hybrid CPU-GPU Computing
34.	Yi-Hsin Liu, Three-Dimensional Nature of Collisionless Magnetic Reconnection at Earth’s Magnetopause
36.	Warren B. Mori, Transformative Petascale Particle-in-Cell Simulations
38.	Scott C. Noble, Mini-disk Dynamics about Supermassive Black Holes Binaries 
40.	Michael L. Norman, Realistic Simulations of the Intergalactic Medium:  The Search for Missing Physics
42.	Brian W. O’Shea, Simulating Galaxy Formation Across Cosmic Time
44.	Christian D. Ott, 3D General-Relativistic Radiation-Hydrodynamic Simulations of Core-Collapse Supernovae
46.	Nikolai Pogorelov, Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with a Multiscale Fluid-Kinetic Simulations Suite
48.	Thomas Quinn, Unified Modeling of Galaxy Populations in Clusters
50.	Vadim Roytershteyn, Kinetic Simulations of Large-Scale Plasma Turbulence
52.	Hsi-Yu Schive, GPU-Accelerated Adaptive Mesh Refinement
54.	Stuart L. Shapiro, Magnetorotational Collapse of Supermassive Stars:  Black Hole Formation, Gravitational Waves, and Jets
56.	Alexander Tchekhovskoy, GPU-Accelerated Simulations:  Black Holes, Spaghettified Stars, and Tilted Disks
58.	Gabor Toth, Advanced Space Weather Modeling
60.	Paul R. Woodward, 3D Simulations of i-Process Nucleosynthesis
64.	Marin Clark, High-Resolution Digital Surface Models of the 2016 Mw7.8 Kaikoura Earthquake, New Zealand
66.	Jennifer Corcoran, Image Processing to Build a Multi-Temporal Vegetation Elevation Model(MTVEM) of the Great Lakes Basin(GLB)
68.	Larry Di-Girolamo, The Terra Data Fusion Project
70.	Marcelo H. Garcia, Large-Eddy Simulation of Sediment Transport and Hydrodynamics at River Bifurcations:  Using a Highly Scalable Spectral Element-Based CFD Solver
72.	Ian Howat, The Reference Elevation Model of Antarctica
74.	Sonia Lasher-Trapp, Untangling Entrainment and Precipitation in Convective Clouds
76.	Lijun Liu, Understanding the 4-D Evolution of the Solid Earth Using Geodynamic Models with Data Assimilation
78.	Philip J. Maechling, Physics-Based Modeling of High-Frequency Ground Motions and Probabilistic Seismic Hazard Analysis
80.	Paul Morin, Enhanced Digital Elevation Model for the Artic
82.	Leigh Orf, Simulating the Most Devastating Tornadoes Embedded Within Supercell Thunderstorms
84.	Robert Rauber, Don Wuebbles, High-Resolution Earth System Modeling Using Blue Waters’ Capabilities
86.	Jamesina J. Simpson, Location-Specific Space Weather Hazards to Electric Power Grids Calculated on a Global Scale
88.	Ryan L. Sriver, Impact of Ocean Coupling on Simulated Tropical Cyclone Activity in the High-Resolution Community Earth System Mode
90.	Robert J. Trapp, Petascale Modeling of Convective Storms Under Climate Change and Variability
92.	Junshik Um, Impacts of Orientation and Morphology of Small Atmospheric Ice Crystals on in-situ Aircraft Measurements:  Scattering Calculations
94.	Albert J. Valocchi, Pore-Scale Simulation of Multiphase Flow in Porous Media with Applications to Geological Sequestration of Carbon Dioxide
96.	Matthew West, 3D Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
98.	Donald J. Wuebbles, Particulate Matter Prediction and Source Attribution for U.S. Air Quality Management in a Changing World
102.	David Ackerman, Exploring Confinement vs. Orientation Effects in Rigid and Semi-Flexible Polymers Using a Massively Parallel Framework
104.	Ange-Therese Akono, Multi-Scale and Multi-Physics Modeling of the Strength of Geopolymer Composites
106.	Jean Paul Allain, Harnessing Petascale Computing to Explain Fundamental Mechanisms Driving Nanopatterning of Multicomponent Surfaces by Directed Irradiation Synthesis
108.	Narayana R. Aluru, Study of DIBs with Functional Channels
110.	Jerzy Bernholc, Petaflops Simulation and Design of Nanoscale Materials and Devices
112.	Daniel Bodony, Reducing Jet Aircraft Noise
114.	David Ceperley, Properties of Dense Hydrogen
116.	Huck Beng Chew, Scalable Nanopatterning of Graphene by Hydrogen-Plasma Etching
118.	Davide Curreli, hPIC:  A Scalable Electrostatic Particle-In-Cell for Plasma-Material Interactions
120.	J. P. Draayer, Innovative Ab Initio Symmetry-Adapted No-Core Shell Model for Advancing Fundamental Physics and Astrophysics
122.	Lian Duan, DNS of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
124.	Said Elghobashi, Dispersion of Fully Resolved Liquid Droplets in Isotropic Turbulent Flow
126.	Elif Ertekin, QMCBD:  A Living Database to Accelerate Worldwide Development and use of Quantum Monte Carlo Methods
128.	Paul Fischer, Numerical Methods and Software for Computational Fluid Dynamics, NEK5000
130.	Marcelo H. Garcia, Direct Numerical Simulation of Turbulence and Sediment Transport in Oscillatory Boundary Layer Flows
132.	Paolo Gardoni, 3D Probabilistic Physics-Based Seismic Hazard Maps for Regional Risk Analysis
134.	Mattia Gazzola, Optimal Bio-Locomotion Strategies in Fluids
136.	Kathryn Huff, Coupled Multi-Physics of Advanced Molten Salt Nuclear Reactors
138.	Sohrab Ismail-Beigi, Understanding Hydrogen Storage in Metal Organic Frameworks using Massively-Parallel Electronic Structure Calculations
140.	Prashant K. Jain, Atomistic Modeling of Transformations in Nanocrystals
142.	Eric Johnsen, Numerical Simulations of Collapsing Cavitation Bubbles on Blue Waters
144.	Gerhard Klimeck, Leading Future Electronics into the Nano Regime Using Quantum Atomistic Simulations in NEMO5
146.	Deborah A. Levin, Kinetic Simulations of Unsteady Shock-Boundary Layer Interactions
148.	Paul Mackenzie, High Energy Physics on Blue Waters
150.	Burkhard Militzer, First-Principles Computer Simulations of Hydrocarbons Under Fusion Conditions
152.	Sarma L. Rani, Direct Numerical Simulations of the Relative Motion of High-Inertia Particles in Isotropic Turbulence
154.	Caroline Riedl, Mapping Proton Quark Structure Using Petabytes of COMPASS Data
156.	Andre Schleife, Optical Determination of Crystal Phase in Semiconductor Nanocrystals
158.	Ahmed Taha, Advanced Digital Technology for Materials and Manufacturing
160.	Brian G. Thomas, Transient Multiphase Flow Phenomena and Defect Formation in Steel Continuous Casting
162.	Rafael Tinoco Lopez, High Resolution Numerical Simulation of Oscillatory Flow and Sediment Transport through Aquatic Vegetation
164.	Lucas K. Wagner, Quantum Monte Carlo Simulations of Magnetism and Models in Condensed Matter
166.	P.K. Yeung, Intermittency, Resolution Effects and High Schmidt Number Mixing in Turbulence
170.	Donna Cox, CADENS NSF Project:  Digital Literacy, Data Visualization, and the Cinematic Presentation of Science
172.	William Gropp, Algorithms for Extreme-Scale Systems
174.	Levent Gurel, Parallelization of the Multilevel Fast Multipole Algorithm(MLFMA) on Heterogeneous CPU-GPU Architectures
176.	Ravishankar K. Iyer, Predicting Performance Degradation and Failure of Applications through System Activity Monitoring
178.	Rakesh Nagi, Parallel Algorithms for Solving Large Assignment Problems
180.	Luke Olson, Localizing Communication in Sparse Matrix Operations
182.	Edgar Solomonik, Performance Evaluation of New Algebraic Algorithms and Libraries
184.	Tandy Warnow, Parallel Algorithms for Big Data Phylogenomics, Proteomics, and Metagenomics
186.	Tao Xie, Hardware Acceleration of Deep Learning
188.	Kevin Olson, A Critical Evaluation of the OP2/OPS Parallel Meshing and Code Generation Software
192.	Aleksei Aksimentiev, DNA Origami Membrane Channels
194.	Aleksei Aksimentiev, Molecular Mechanism of Nuclear Transports
196.	Gregory Bauer, Improving NWChem Scalability Using the DataSpaces Framework
198.	Gustavo Caetano-Anolles, How Function Shapes Dynamics in Protein Evolution
200.	Isaac Cann, Cellulosome Structure Determination by Atomistic Simulations Combined with Experimental Assays
202.	Vincenzo Carnevale, Mechanism of Temperature Sensitivity in TRPV1 Channel
204.	Thomas E. Cheatham III, Exploring the Structure and Dynamics of Converged Ensembles of DNA and RNA Through Molecular Dynamics Simulations
206.	Christina C.H. Cheng, Structural Basis for Extreme Cold Tolerance in the Eye Lenses of Teleost Fishes
208.	Ken A. Dill, Predicting Protein Structures with Physical Petascale Molecular Simulations
210.	Ahmed Elbanna, Multiscale Modeling of Biofilm Dynamics in Drinking Water Distribution Systems:  Toward Predictive Modeling of Pathogen Outbreaks
212.	Peter Freddolino, Comprehensive in silico Mapping of DNA-Binding Protein Affinity Landscapes
214.	Sharon Hammes-Schiffer, Non-Born-Oppenheimer Effects Between Electrons and Protons
216.	So Hirata, Brueckner-Goldstone Quantum Monte Carlo
218.	Peter Kasson, How Membrane Organization Controls Influenza Infections
220.	Zaida Luthey-Schulten, A Hybrid Stochastic-Deterministic Simulation Method Enables Fast Simulation of Cellular Processes in Eukaryotes
222.	Nancy Makri, Quantum-Classical Path Integral Simulation of Charge Transfer Reactions
224.	Arif Masud, Patient-Specific HPC Models and Simulation-Based Imaging for Cardiovascular Surgical Planning
226.	James W. Mazzuca, Quantum Effects of Proton Transfer in Biological Systems
228.	Mahmoud Moradi, Thermodynamic Characterization of Conformational Landscape in Proton-Coupled Oligopeptide Transporters
230.	Vijay Pande, Machine Learning Reveals Ligand-Directed Conformational Change of u Opioid Receptor
232.	Benoit Roux, Elucidating the Molecular Mechanism of C-type Inactivation in Potassium Channels
234.	Klaus Schulten, Studying Cellular Processes through the Computational Microscope
236.	Diwakar Shukla, Understanding the Protein Allostery in Kinanses and GPCRs
238.	Ivan Soltesz, Data-Driven, Biologically Constrained Computational Model of the Hippocampal Network at Full Scale
240.	Marcos Sotomayor, Stretching the Cadherin Molecular Velcro of Cell-Cell Junctions
242.	Ahsok Srinivasan, Simulation of Viral Infection Propagation through Air Travel
244. 	Brad Sutton, High-Resolution Magnetic Resonance Imaging of Mechanical Properties of the Brain
246.	Ilias Tagkopoulos, A Crystal Ball of Bacterial Behavior:  from Data to Prediction using Genome-Scale Models
248.	Greg A. Voth, Large-Scale Coarse-Grained Molecular Simulations of the Viral Lifecycle of HIV-1
252.	Lars Hansen, Yongyang Cai, Policy Responses to Climate Change in a Dynamic Stochastic Economy
254.	Wendy K. Tam Cho, Enabling Redistricting Reform:  A Computational Study of Zoning Optimization
258.	Elizabeth Agee, Resolving Plant Functional Biodiversity to Quantify Forest Drought Resistance in the Amazon
260.	Maureen T. Brooks, Modeling Nonlinear Physical-Biological Interactions:  Eddies and Sargassum in the North Atlantic
262.	Iryna Butsky, The Role of Cosmic Rays in Isolated Disk Galaxies
264.	Jon Calhoun, Analyzing the Propagation of Soft Error Corruption in HPC Applications
266.	Justin Drake, Toward Developing a Thermodynamic Model of Binding-Induced Conformational Transitions in Short, Disordered Protein Regions
268. 	Paul Hime, Genomic Perspectives on the Amphibian Tree of Life
270.	Michael P. Howard, Multiscale Simulations of Complex Fluid Rheology
272.	Alexandra L. Jones, High Accuracy Radiative Transfer in Cloudy Atmospheres
274.	Andrew Kirby, High-Fidelity Blade-Resolved Wind Farm Simulations
276.	Sara Kokkila Schumacher, Reducing the Computational Cost of Coupled Clustery Theory
278.	Larisa Reames, Simulated Effects of Urban Environments on the Dynamics of a Supercell Thunderstorm
280.	Sherwood Richers, Monte Carlo Neutrino Closures in 3D GRMHD Simulations of Core-Collapse Supernovae and Neutron Star Mergers
282.	Sean L. Seyler, Understanding the Role of Hydrodynamic Fluctuations in Biomacromolecular Dynamics through the Development of Hybrid Atomistic-Continuum Simulation
284.	Ronald Stenz, The Impacts of Hydrometeor Centrifuging on Tornado Dynamics
286.	Erin Teich, Glassy Dynamics and Identity Crises in Hard-Particle Systems
288.	Samuel Totorica, Magnetic Reconnection in Laser-Driven Plasmas:  from Astrophysics to the Laboratory in silico






2017 Blue Waters Annual Report Table of Contents
18.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence in Star-Forming Molecular Clouds on the Blue Waters System
20.	Tiziana Di Matteo, Supermassive Black Holes at the Cosmic Frontier
22.	Gilbert Holder, Theoretical Astrophysics and Data Analysis
24.	Eliu Huerta, Detection of Gravitational Wave Sources in Dense Stellar Environments
26.	Eliu Huerta, Deep Neural Networks to Enable Real-Time Multimessenger Astrophysics
28.	Thomas W. Jones, Toward Robust Magnetohydrodynamic Simulations of Galaxy Cluster Formation
30.	Eric J. Lentz, Exploring the Nature of Exploding Massive Stars with High Resolution
32.	Deborah Levin, Modeling Plasma Flows with Kinetic Approaches using Hybrid CPU-GPU Computing
34.	Yi-Hsin Liu, Three-Dimensional Nature of Collisionless Magnetic Reconnection at Earth’s Magnetopause
36.	Warren B. Mori, Transformative Petascale Particle-in-Cell Simulations
38.	Scott C. Noble, Mini-disk Dynamics about Supermassive Black Holes Binaries 
40.	Michael L. Norman, Realistic Simulations of the Intergalactic Medium:  The Search for Missing Physics
42.	Brian W. O’Shea, Simulating Galaxy Formation Across Cosmic Time
44.	Christian D. Ott, 3D General-Relativistic Radiation-Hydrodynamic Simulations of Core-Collapse Supernovae
46.	Nikolai Pogorelov, Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with a Multiscale Fluid-Kinetic Simulations Suite
48.	Thomas Quinn, Unified Modeling of Galaxy Populations in Clusters
50.	Vadim Roytershteyn, Kinetic Simulations of Large-Scale Plasma Turbulence
52.	Hsi-Yu Schive, GPU-Accelerated Adaptive Mesh Refinement
54.	Stuart L. Shapiro, Magnetorotational Collapse of Supermassive Stars:  Black Hole Formation, Gravitational Waves, and Jets
56.	Alexander Tchekhovskoy, GPU-Accelerated Simulations:  Black Holes, Spaghettified Stars, and Tilted Disks
58.	Gabor Toth, Advanced Space Weather Modeling
60.	Paul R. Woodward, 3D Simulations of i-Process Nucleosynthesis
64.	Marin Clark, High-Resolution Digital Surface Models of the 2016 Mw7.8 Kaikoura Earthquake, New Zealand
66.	Jennifer Corcoran, Image Processing to Build a Multi-Temporal Vegetation Elevation Model(MTVEM) of the Great Lakes Basin(GLB)
68.	Larry Di-Girolamo, The Terra Data Fusion Project
70.	Marcelo H. Garcia, Large-Eddy Simulation of Sediment Transport and Hydrodynamics at River Bifurcations:  Using a Highly Scalable Spectral Element-Based CFD Solver
72.	Ian Howat, The Reference Elevation Model of Antarctica
74.	Sonia Lasher-Trapp, Untangling Entrainment and Precipitation in Convective Clouds
76.	Lijun Liu, Understanding the 4-D Evolution of the Solid Earth Using Geodynamic Models with Data Assimilation
78.	Philip J. Maechling, Physics-Based Modeling of High-Frequency Ground Motions and Probabilistic Seismic Hazard Analysis
80.	Paul Morin, Enhanced Digital Elevation Model for the Artic
82.	Leigh Orf, Simulating the Most Devastating Tornadoes Embedded Within Supercell Thunderstorms
84.	Robert Rauber, Don Wuebbles, High-Resolution Earth System Modeling Using Blue Waters’ Capabilities
86.	Jamesina J. Simpson, Location-Specific Space Weather Hazards to Electric Power Grids Calculated on a Global Scale
88.	Ryan L. Sriver, Impact of Ocean Coupling on Simulated Tropical Cyclone Activity in the High-Resolution Community Earth System Mode
90.	Robert J. Trapp, Petascale Modeling of Convective Storms Under Climate Change and Variability
92.	Junshik Um, Impacts of Orientation and Morphology of Small Atmospheric Ice Crystals on in-situ Aircraft Measurements:  Scattering Calculations
94.	Albert J. Valocchi, Pore-Scale Simulation of Multiphase Flow in Porous Media with Applications to Geological Sequestration of Carbon Dioxide
96.	Matthew West, 3D Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
98.	Donald J. Wuebbles, Particulate Matter Prediction and Source Attribution for U.S. Air Quality Management in a Changing World
102.	David Ackerman, Exploring Confinement vs. Orientation Effects in Rigid and Semi-Flexible Polymers Using a Massively Parallel Framework
104.	Ange-Therese Akono, Multi-Scale and Multi-Physics Modeling of the Strength of Geopolymer Composites
106.	Jean Paul Allain, Harnessing Petascale Computing to Explain Fundamental Mechanisms Driving Nanopatterning of Multicomponent Surfaces by Directed Irradiation Synthesis
108.	Narayana R. Aluru, Study of DIBs with Functional Channels
110.	Jerzy Bernholc, Petaflops Simulation and Design of Nanoscale Materials and Devices
112.	Daniel Bodony, Reducing Jet Aircraft Noise
114.	David Ceperley, Properties of Dense Hydrogen
116.	Huck Beng Chew, Scalable Nanopatterning of Graphene by Hydrogen-Plasma Etching
118.	Davide Curreli, hPIC:  A Scalable Electrostatic Particle-In-Cell for Plasma-Material Interactions
120.	J. P. Draayer, Innovative Ab Initio Symmetry-Adapted No-Core Shell Model for Advancing Fundamental Physics and Astrophysics
122.	Lian Duan, DNS of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
124.	Said Elghobashi, Dispersion of Fully Resolved Liquid Droplets in Isotropic Turbulent Flow
126.	Elif Ertekin, QMCBD:  A Living Database to Accelerate Worldwide Development and use of Quantum Monte Carlo Methods
128.	Paul Fischer, Numerical Methods and Software for Computational Fluid Dynamics, NEK5000
130.	Marcelo H. Garcia, Direct Numerical Simulation of Turbulence and Sediment Transport in Oscillatory Boundary Layer Flows
132.	Paolo Gardoni, 3D Probabilistic Physics-Based Seismic Hazard Maps for Regional Risk Analysis
134.	Mattia Gazzola, Optimal Bio-Locomotion Strategies in Fluids
136.	Kathryn Huff, Coupled Multi-Physics of Advanced Molten Salt Nuclear Reactors
138.	Sohrab Ismail-Beigi, Understanding Hydrogen Storage in Metal Organic Frameworks using Massively-Parallel Electronic Structure Calculations
140.	Prashant K. Jain, Atomistic Modeling of Transformations in Nanocrystals
142.	Eric Johnsen, Numerical Simulations of Collapsing Cavitation Bubbles on Blue Waters
144.	Gerhard Klimeck, Leading Future Electronics into the Nano Regime Using Quantum Atomistic Simulations in NEMO5
146.	Deborah A. Levin, Kinetic Simulations of Unsteady Shock-Boundary Layer Interactions
148.	Paul Mackenzie, High Energy Physics on Blue Waters
150.	Burkhard Militzer, First-Principles Computer Simulations of Hydrocarbons Under Fusion Conditions
152.	Sarma L. Rani, Direct Numerical Simulations of the Relative Motion of High-Inertia Particles in Isotropic Turbulence
154.	Caroline Riedl, Mapping Proton Quark Structure Using Petabytes of COMPASS Data
156.	Andre Schleife, Optical Determination of Crystal Phase in Semiconductor Nanocrystals
158.	Ahmed Taha, Advanced Digital Technology for Materials and Manufacturing
160.	Brian G. Thomas, Transient Multiphase Flow Phenomena and Defect Formation in Steel Continuous Casting
162.	Rafael Tinoco Lopez, High Resolution Numerical Simulation of Oscillatory Flow and Sediment Transport through Aquatic Vegetation
164.	Lucas K. Wagner, Quantum Monte Carlo Simulations of Magnetism and Models in Condensed Matter
166.	P.K. Yeung, Intermittency, Resolution Effects and High Schmidt Number Mixing in Turbulence
170.	Donna Cox, CADENS NSF Project:  Digital Literacy, Data Visualization, and the Cinematic Presentation of Science
172.	William Gropp, Algorithms for Extreme-Scale Systems
174.	Levent Gurel, Parallelization of the Multilevel Fast Multipole Algorithm(MLFMA) on Heterogeneous CPU-GPU Architectures
176.	Ravishankar K. Iyer, Predicting Performance Degradation and Failure of Applications through System Activity Monitoring
178.	Rakesh Nagi, Parallel Algorithms for Solving Large Assignment Problems
180.	Luke Olson, Localizing Communication in Sparse Matrix Operations
182.	Edgar Solomonik, Performance Evaluation of New Algebraic Algorithms and Libraries
184.	Tandy Warnow, Parallel Algorithms for Big Data Phylogenomics, Proteomics, and Metagenomics
186.	Tao Xie, Hardware Acceleration of Deep Learning
188.	Kevin Olson, A Critical Evaluation of the OP2/OPS Parallel Meshing and Code Generation Software
192.	Aleksei Aksimentiev, DNA Origami Membrane Channels
194.	Aleksei Aksimentiev, Molecular Mechanism of Nuclear Transports
196.	Gregory Bauer, Improving NWChem Scalability Using the DataSpaces Framework
198.	Gustavo Caetano-Anolles, How Function Shapes Dynamics in Protein Evolution
200.	Isaac Cann, Cellulosome Structure Determination by Atomistic Simulations Combined with Experimental Assays
202.	Vincenzo Carnevale, Mechanism of Temperature Sensitivity in TRPV1 Channel
204.	Thomas E. Cheatham III, Exploring the Structure and Dynamics of Converged Ensembles of DNA and RNA Through Molecular Dynamics Simulations
206.	Christina C.H. Cheng, Structural Basis for Extreme Cold",Blue Waters 2017 Annual Report,,,,,core
296197060,2017-11,"A robot agent designed to engage in real-world human--robot joint action must be able to understand the social states of the human users it interacts with in order to behave appropriately. In particular, in a dynamic public space, a crucial task for the robot is to determine the needs and intentions of all of the people in the scene, so that it only interacts with people who intend to interact with it. We address the task of estimating the engagement state of customers for a robot bartender based on the data from audiovisual sensors. We begin with an offline experiment using hidden Markov models, confirming that the sensor data contains the information necessary to estimate user state. We then present two strategies for online state estimation: a rule-based classifier based on observed human behaviour in real bars, and a set of supervised classifiers trained on a labelled corpus. These strategies are compared in offline cross-validation, in an online user study, and through validation against a separate test corpus. These studies show that while the trained classifiers are best in a cross-validation setting, the rule-based classifier performs best with novel data; however, all classifiers also change their estimate too frequently for practical use. To address this issue, we present a final classifier based on Conditional Random Fields: this model has comparable performance on the test data, with increased stability. In summary, though, the rule-based classifier shows competitive performance with the trained classifiers, suggesting that for this task, such a simple model could actually be a preferred option, providing useful online performance while avoiding the implementation and data-scarcity issues involved in using machine learning for this task",Automatically classifying user engagement for dynamic multi-party human–robot interaction,10.1007/s12369-017-0414-y,,Springer,,core
103890051,2016-01-25,"Abstract Spectral analysis approaches have been actively studied in machine learning and data mining areas, due to their generality, efficiency, and rich theoretical foundations. As a natural non-linear generalization of Graph Laplacian, p-Laplacian has recently been pro-posed, which interpolates between a relaxation of normalized cut and the Cheeger cut. How-ever, the relaxation can only be applied to two-class cases. In this paper, we propose full eigenvector analysis of p-Laplacian and obtain a natural global embedding for multi-class clustering problems, instead of using greedy search strategy implemented by previous re-searchers. An efficient gradient descend optimization approach is introduced to obtain the p-Laplacian embedding space, which is guaranteed to converge to feasible local solutions. Empirical results suggest that the greedy search method often fails in many real-world appli-cations with non-trivial data structures, but our approach consistently gets robust clustering results. Visualizations of experimental results also indicate our embedding space preserves the local smooth manifold structures existing in real-world data",DOI 10.1007/s10994-010-5201-z On the eigenvectors of p-Laplacian,,,,,core
95856733,"June 27, 2017","Although communications-based cognitive engines have been proposed, very few have been implemented in a full system, especially in a space communications system. In this paper, we detail the implementation of a multi-objective reinforcement-learning algorithm and deep artificial neural networks for the use as a radio-resource-allocation controller. The modular software architecture presented encourages re-use and easy modification for trying different algorithms. Various trade studies involved with the system implementation and integration are discussed. These include the choice of software libraries that provide platform flexibility and promote reusability, choices regarding the deployment of this cognitive engine within a system architecture using the DVB-S2 standard and commercial hardware, and constraints placed on the cognitive engine caused by real-world radio constraints. The implemented radio-resource allocation-management controller was then integrated with the larger spaceground system developed by NASA Glenn Research Center (GRC)",Implementation of a Space Communications Cognitive Engine,,https://core.ac.uk/download/pdf/95856733.pdf,,,core
217606912,2017-01-01T00:00:00,"This paper investigates the use of convolutional neural networks for the purpose of image foreground extraction from a dynamic environment. The proposed solution utilises the latest developments in image segmentation using pixel-wise classification to produce foreground target extraction for real-time operations. A collection of spacecraft images were assembled for network training and evaluation. The proposed technique takes advantage of transfer learning for the stable training of a convolutional neural network classifier. The image extraction software was applied to a thermal camera video, taken by an undocking spacecraft from the International Space Station. The results show the proposed deep learning-based image extraction has advantages over traditional background subtraction methods. This investigation provides evidence that semantic segmentation using convolutional neural network can be an effective tool for spacecraft image isolation and extraction from a dynamically cluttered scene",International space station image extraction from a dynamic environment using deep learning,10.11159/cdsr17.131,,'Avestia Publishing',,core
79586779,2016-01-01T00:00:00,"abstract: Robotic systems are outmatched by the abilities of the human hand to perceive and manipulate the world. Human hands are able to physically interact with the world to perceive, learn, and act to accomplish tasks. Limitations of robotic systems to interact with and manipulate the world diminish their usefulness. In order to advance robot end effectors, specifically artificial hands, rich multimodal tactile sensing is needed. In this work, a multi-articulating, anthropomorphic robot testbed was developed for investigating tactile sensory stimuli during finger-object interactions. The artificial finger is controlled by a tendon-driven remote actuation system that allows for modular control of any tendon-driven end effector and capabilities for both speed and strength. The artificial proprioception system enables direct measurement of joint angles and tendon tensions while temperature, vibration, and skin deformation are provided by a multimodal tactile sensor. Next, attention was focused on real-time artificial perception for decision-making. A robotic system needs to perceive its environment in order to make decisions. Specific actions such as “exploratory procedures” can be employed to classify and characterize object features. Prior work on offline perception was extended to develop an anytime predictive model that returns the probability of having touched a specific feature of an object based on minimally processed sensor data. Developing models for anytime classification of features facilitates real-time action-perception loops. Finally, by combining real-time action-perception with reinforcement learning, a policy was learned to complete a functional contour-following task: closing a deformable ziplock bag. The approach relies only on proprioceptive and localized tactile data. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards within a finite time period by balancing exploration versus exploitation of the action space. Performance of the C-MAB learner was compared to a benchmark Q-learner that eventually returns the optimal policy. To assess robustness and generalizability, the learned policy was tested on variations of the original contour-following task. The work presented contributes to the full range of tools necessary to advance the abilities of artificial hands with respect to dexterity, perception, decision-making, and learning.Dissertation/ThesisDoctoral Dissertation Mechanical Engineering 201","Haptic Perception, Decision-making, and Learning  for Manipulation with Artificial Hands",,https://core.ac.uk/download/79586779.pdf,,,core
286710687,2016-08-26T00:00:00,"Currently, high dimensional data processing confronts two main difficulties: inefficient similarity measure and high computational complexity in both time and memory space. Common methods to deal with these two difficulties are based on dimensionality reduction and feature selection. In this paper, we present a different way to solve high dimensional data problems by combining the ideas of Random Forests and Anchor Graph semi-supervised learning. We randomly select a subset of features and use the Anchor Graph method to construct a graph. This process is repeated many times to obtain multiple graphs, a process which can be implemented in parallel to ensure runtime efficiency. Then the multiple graphs vote to determine the labels for the unlabeled data. We argue that the randomness can be viewed as a kind of regularization. We evaluate the proposed method on eight real-world data sets by comparing it with two traditional graph-based methods and one state-of-the-art semi-supervised learning method based on Anchor Graph to show its effectiveness. We also apply the proposed method to the subject of face recognition",Random Multi-Graphs: A semi-supervised learning framework for classification of high dimensional data.,10.1016/j.imavis.2016.08.006,https://core.ac.uk/download/286710687.pdf,'Elsevier BV',,core
42698812,"July 12, 2016","The ISS provides an excellent opportunity for pioneering artificial intelligence software to meet the challenges of real-time communications (comm) link management. This opportunity empowers the ISS Program to forge a testbed for developing cognitive communications systems for the benefit of the ISS mission, manned Low Earth Orbit (LEO) science programs and future planetary exploration programs. In November, 1998, the Flight Operations Directorate (FOD) started the ISS Antenna Manager (IAM) project to develop a single processor supporting multiple comm satellite tracking for two different antenna systems. Further, the processor was developed to be highly adaptable as it supported the ISS mission through all assembly stages. The ISS mission mandated communications specialists with complete knowledge of when the ISS was about to lose or gain comm link service. The current specialty mandated cognizance of large sun-tracking solar arrays and thermal management panels in addition to the highly-dynamic satellite service schedules and rise/set tables. This mission requirement makes the ISS the ideal communications management analogue for future LEO space station and long-duration planetary exploration missions. Future missions, with their precision-pointed, dynamic, laser-based comm links, require complete autonomy for managing high-data rate communications systems. Development of cognitive communications management systems that permit any crew member or payload science specialist, regardless of experience level, to control communications is one of the greater benefits the ISS can offer new space exploration programs. The IAM project met a new mission requirement never previously levied against US space-born communications systems management: process and display the orientation of large solar arrays and thermal control panels based on real-time joint angle telemetry. However, IAM leaves the actual communications availability assessment to human judgment, which introduces unwanted variability because each specialist has a different core of experience with comm link performance. Because the ISS utilizes two different frequency bands, dynamic structure can be occasionally translucent at one frequency while it can completely interdict service at the other frequency. The impact of articulating structure on the comm link can depend on its orientation at the time it impinges on the link. It can become easy for a human specialist to cross-associate experience at one frequency with experience at the other frequency. Additionally, the specialist's experience is incremental, occurring one nine-hour shift at a time. Only the IAM processor experiences the complete 24x7x365 communications link performance for both communications links but, it has no ""learning capability."" If the IAM processor could be endowed with a cognitive ability to remember past structure-induced comm link outages, based on its knowledge of the ISS position, attitude, communications gear, array joint angles and tracking accuracy, it could convey such experience to the human operator. It could also use its learned communications link behaviors to accurately convey the availability of future communications sessions. Further, the tool could remember how accurately or inaccurately it predicted availability and correct future predictions based on past performance. The IAM tool could learn frequency-specific impacts due to spacecraft structures and pass that information along as ""experience."" Such development would provide a single artificial intelligence processor that could provide two different experience bases. If it also ""knew"" the satellite service schedule, it could distinguish structure blockage from schedule or planet blockage and then quickly switch to another satellite. Alternatively, just as a human operator could judge, a cognizant comm system based on the IAM model could ""know"" that the blockage is not going to last very long and continue tracking a comm satellite, waiting for it to track away from structure. Ultimately, once this capability was fully developed and tested in the Mission Control Center, it could be transferred on-orbit to support development of operations concepts that include more advanced cognitive communications systems. Future applications of this capability are easily foreseen because even more dynamic satellite constellations with more nodes and greater capability are coming. Currently, the ISS fully employs a 300 million bit-per-second (Mbps) return link for harvesting payload science. In the coming eighteen months, it will step up to 600 Mbps. Already there is talk of a 1.2 billion bit-per-second (Gbps) upgrade for the ISS and laser comm links have already been tested from the ISS. Every data rate upgrade mandates more complicated and sensitive communications equipment which implies greater expertise invested in the human operator. Future on-orbit cognizant comm systems will be needed to meet greater performance demands aboard larger, far more complicated spacecraft. In the LEO environment, the old-style one-satellite-per-spacecraft operations concept will give way to a new concept of a single customer spacecraft simultaneously using multiple comm satellites. Much more highly-dynamic manned LEO missions with decades of crew members potentially increase the demand for communications link performance. A cognizant on-board communications system will meet advanced communications demands from future LEO missions and future planetary missions. The ISS has fledgling components of future exploration programs, both LEO and planetary. Further, the Flight Operations Directorate, through the IAM project, has already begun to develop a communications management system that attempts to solve advanced problems ideally represented by dynamic structure impacting scheduled satellite service. With an earnest project to integrate artificial intelligence into the IAM processor, the ISS Program could develop a cognizant communications system that could be adapted and transferred to future on-orbit avionics designs",Utilizing the ISS Mission as a Testbed to Develop Cognitive Communications Systems,,https://core.ac.uk/download/pdf/42698812.pdf,,,core
42694627,"July 12, 2016","The ISS provides an excellent opportunity for pioneering artificial intelligence software to meet the challenges of real-time communications (comm) link management. This opportunity empowers the ISS Program to forge a testbed for developing cognitive communications systems for the benefit of the ISS mission, manned Low Earth Orbit (LEO) science programs and future planetary exploration programs. In November, 1998, the Flight Operations Directorate (FOD) started the ISS Antenna Manager (IAM) project to develop a single processor supporting multiple comm satellite tracking for two different antenna systems. Further, the processor was developed to be highly adaptable as it supported the ISS mission through all assembly stages. The ISS mission mandated communications specialists with complete knowledge of when the ISS was about to lose or gain comm link service. The current specialty mandated cognizance of large sun-tracking solar arrays and thermal management panels in addition to the highly-dynamic satellite service schedules and rise/set tables. This mission requirement makes the ISS the ideal communications management analogue for future LEO space station and long-duration planetary exploration missions. Future missions, with their precision-pointed, dynamic, laser-based comm links, require complete autonomy for managing high-data rate communications systems. Development of cognitive communications management systems that permit any crew member or payload science specialist, regardless of experience level, to control communications is one of the greater benefits the ISS can offer new space exploration programs. The IAM project met a new mission requirement never previously levied against US space-born communications systems management: process and display the orientation of large solar arrays and thermal control panels based on real-time joint angle telemetry. However, IAM leaves the actual communications availability assessment to human judgement, which introduces unwanted variability because each specialist has a different core of experience with comm link performance. Because the ISS utilizes two different frequency bands, dynamic structure can be occasionally translucent at one frequency while it can completely interdict service at the other frequency. The impact of articulating structure on the comm link can depend on its orientation at the time it impinges on the link. It can become easy for a human specialist to cross-associate experience at one frequency with experience at the other frequency. Additionally, the specialist's experience is incremental, occurring one nine-hour shift at a time. Only the IAM processor experiences the complete 24x7x365 communications link performance for both communications links but, it has no ""learning capability."" If the IAM processor could be endowed with a cognitive ability to remember past structure-induced comm link outages, based on its knowledge of the ISS position, attitude, communications gear, array joint angles and tracking accuracy, it could convey such experience to the human operator. It could also use its learned communications link behaviors to accurately convey the availability of future communications sessions. Further, the tool could remember how accurately or inaccurately it predicted availability and correct future predictions based on past performance. The IAM tool could learn frequency-specific impacts due to spacecraft structures and pass that information along as ""experience."" Such development would provide a single artificial intelligence processor that could provide two different experience bases. If it also ""knew"" the satellite service schedule, it could distinguish structure blockage from schedule or planet blockage and then quickly switch to another satellite. Alternatively, just as a human operator could judge, a cognizant comm system based on the IAM model could ""know"" that the blockage is not going to last very long and continue tracking a comm satellite, waiting for it to track away from structure. Ultimately, once this capability was fully developed and tested in the Mission Control Center, it could be transferred on-orbit to support development of operations concepts that include more advanced cognitive communications systems. Future applications of this capability are easily foreseen because even more dynamic satellite constellations with more nodes and greater capability are coming. Currently, the ISS fully employs its high-data-rate return link for harvesting payload science. In the coming months, it will double that data rate and is forecast to fully utilize that capability. Already there is talk of an upgrade that quadruples the current data rate allocated to ISS payload science before the end of its mission and laser comm links have already been tested from the ISS. Every data rate upgrade mandates more complicated and sensitive communications equipment which implies greater expertise invested in the human operator. Future on-orbit cognizant comm systems will be needed to meet greater performance demands aboard larger, far more complicated spacecraft. In the LEO environment, the old-style one-satellite-per-spacecraft operations concept will give way to a new concept of a single customer spacecraft simultaneously using multiple comm satellites. Much more highly-dynamic manned LEO missions with decades of crew members potentially increase the demand for communications link performance. A cognizant on-board communications system will meet advanced communications demands from future LEO missions and future planetary missions. The ISS has fledgling components of future exploration programs, both LEO and planetary. Further, the Flight Operations Directorate, through the IAM project, has already begun to develop a communications management system that attempts to solve advanced problems ideally represented by dynamic structure impacting scheduled satellite service. With an earnest project to integrate artificial intelligence into the IAM processor, the ISS Program could develop a cognizant communications system that could be adapted and transferred to future on-orbit avionics designs",Utilizing the ISS Mission as a Testbed to Develop Cognitive Communications Systems,,https://core.ac.uk/download/pdf/42694627.pdf,,,core
146472430,2017-12-17T00:00:00,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals",Railway Track Specific Traffic Signal Selection Using Deep Learning,,http://arxiv.org/abs/1712.06107,,,core
154803302,2017-01-01T00:00:00,"Many tasks involve the fine manipulation of objects despite limited visual feedback. In such scenarios, tactile and proprioceptive feedback can be leveraged for task completion. We present an approach for real-time haptic perception and decision-making for a haptics-driven, functional contour-following task: the closure of a ziplock bag. This task is challenging for robots because the bag is deformable, transparent, and visually occluded by artificial fingertip sensors that are also compliant. A deep neural net classifier was trained to estimate the state of a zipper within a robot&#x0027;s pinch grasp. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards by balancing exploration versus exploitation of the state-action space. The C-MAB learner outperformed a benchmark Q-learner by more efficiently exploring the state-action space while learning a hard-to-code task. The learned C-MAB policy was tested with novel ziplock bag scenarios and contours (wire, rope). Importantly, this work contributes to the development of reinforcement learning approaches that account for limited resources such as hardware life and researcher time. As robots are used to perform complex, physically interactive tasks in unstructured or unmodeled environments, it becomes important to develop methods that enable efficient and effective learning with physical testbeds. IEE",Functional Contour-following via Haptic Perception and Reinforcement Learning,10.1109/TOH.2017.2753233,,'Institute of Electrical and Electronics Engineers (IEEE)',,core
42673613,2016-01-17T00:00:00,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.Comment: 14 pages, an invited paper for a special issue of Future Generation
  Computer Systems, Elsevier Publ. (2015). This is an expanded version of a
  paper arXiv:1407.3502 presented at the IEEE e-Science 2014 conf., with some
  new conten",Real-Time Data Mining of Massive Data Streams from Synoptic Sky Surveys,10.1016/j.future.2015.10.013,http://arxiv.org/abs/1601.04385,'Elsevier BV',,core
73403155,2016-10-27T00:00:00,"Neural networks augmented with external memory have the ability to learn
algorithmic solutions to complex tasks. These models appear promising for
applications such as language modeling and machine translation. However, they
scale poorly in both space and time as the amount of memory grows --- limiting
their applicability to real-world domains. Here, we present an end-to-end
differentiable memory access scheme, which we call Sparse Access Memory (SAM),
that retains the representational power of the original approaches whilst
training efficiently with very large memories. We show that SAM achieves
asymptotic lower bounds in space and time complexity, and find that an
implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less
physical memory than non-sparse models. SAM learns with comparable data
efficiency to existing models on a range of synthetic tasks and one-shot
Omniglot character recognition, and can scale to tasks requiring $100,\!000$s
of time steps and memories. As well, we show how our approach can be adapted
for models that maintain temporal associations between memories, as with the
recently introduced Differentiable Neural Computer.Comment: in 30th Conference on Neural Information Processing Systems (NIPS
  2016), Barcelona, Spai",Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes,,http://arxiv.org/abs/1610.09027,,,core
80311918,2016-01-01T00:00:00,"Big data from floating cars supply a frequent, ubiquitous sampling of traffic conditions on the road network and provide great opportunities for enhanced short-term traffic predictions based on real-time information on the whole network. Two network-based machine learning models, a Bayesian network and a neural network, are formulated with a double star framework that reflects time and space correlation among traffic variables and because of its modular structure is suitable for an automatic implementation on large road networks. Among different mono-dimensional time-series models, a seasonal autoregressive moving average model (SARMA) is selected for comparison. The time-series model is also used in a hybrid modeling framework to provide the Bayesian network with an a priori estimation of the predicted speed, which is then corrected exploiting the information collected on other links. A large floating car data set on a sub-area of the road network of Rome is used for validation. To account for the variable accuracy of the speed estimated from floating car data, a new error indicator is introduced that relates accuracy of prediction to accuracy of measure. Validation results highlighted that the spatial architecture of the Bayesian network is advantageous in standard conditions, where a priori knowledge is more significant, while mono-dimensional time series revealed to be more valuable in the few cases of non-recurrent congestion conditions observed in the data set. The results obtained suggested introducing a supervisor framework that selects the most suitable prediction depending on the detected traffic regimes",Short-term speed predictions exploiting big data on large urban road networks,10.1016/j.trc.2016.10.019,,'Elsevier BV',,core
127587458,2017-01-01T00:00:00,"Il ruolo della cultura tecnologica del progetto rispetto alle tre grandi sfide poste dalle questioni ambientali ed energetiche del nostro tempo: cambiamenti climatici, limitatezza delle risorse e eccessivo consumo di energia, è centrale e imprescindibile per affrontare consapevolmente la ricerca di una rinnovata dimensione delle condizioni dell'Abitare che nei differenti contesti europei e mondiali porti con sé i concetti - diversi terminologicamente ma affini nelle ampie accezioni che essi racchiudono e negli obiettivi che sottendono - di Sustainable Development, Nachhaltige Entwicklung, Dévéloppement Durable.

Nelle tre differenti aggettivazioni è racchiuso uno dei mandati della Tecnologia e della relativa Cultura tecnologica progettuale da sviluppare nel prossimo futuro: la richiesta di sostenibilità del fare umano per le generazioni future (Sustainable), di solidità e affidabilità dei comportamenti e delle prestazioni (Nachhaltige), di durabilità nel tempo dei prodotti delle trasformazioni (Durable).

L'altro grande mandato della Cultura tecnologica nella nostra epoca è quello di offrire non solo risposte 'dinamiche' nella dimensione temporale di medio-lungo termine alle crescenti esigenze di sostenibilità/affidabilità/durabilità, ma anche risposte 'dinamiche' nello spazio reale e nel tempo presente e di breve termine supportando l'Architettura nella sua altrettanto assoluta necessità di essere 'adattiva' e 'resiliente' ai cambiamenti già in atto sul piano climatico e ambientale.

Cultura tecnologica significa dunque profonda consapevolezza degli obiettivi da perseguire progettualmente, indissolubilmente legata ad un'attitudine alla visione sistemica dei problemi, ad un'impostazione metodologica delle strategie da tracciare e promuovere, ad un'intima sapienza degli aspetti di fattibilità e realizzabilità delle azioni da sperimentare, monitorare, consolidare nel tempo.

D'altra parte tutti i maggiori centri di ricerca e sperimentazione dell'area dell'Architettura nei Paesi avanzati pongono quale nodo focale dei processi di concezione, progettazione e realizzazione di qualsiasi tipo di intervento trasformativo delle nostre realtà quello caratterizzato dall'approccio tecnologico, in cui nei diversi contesti la Architectural Technology, la Baukonstruktion, la Technologie de l'Architecture, la Construcción en Arquitectura non rappresentano solo un ambito disciplinare (pealtro da sempre a vocazione fortemente interdisciplinare) ma, di più, la dimensione logica e culturale nella quale si coordinano e ruotano le complesse declinazioni e i differenti caratteri del progetto.

Nel rinvenire gli elementi di innovazione propri della Cultura tecnologica nell'approcciare e sviluppare ricerca e sperimentazione sui temi dell'ambiente e dell'energia vi è in primis la capacità di perseguire al contempo tre categorie di obiettivi ""alti"" prestazionali: efficienza delle azioni nel controllarne rendimenti e risultati; efficacia complessiva delle strategie nel verificarne il rapporto tra efficienza conseguita e quantità di risorse coinvolte e impiegate nei processi per raggiungere quei risultati; soddisfazione degli utenti nel vivere quelle condizioni di efficienza e nel percepire o addirittura esser coinvolti in quelle dimensioni di efficacia.

La concezione innovativa di scenario che è aperta dall'evoluzione del significato di efficacia pone in primo piano la questione della 'scarsità di risorse' e tiene in massima considerazione -fino al punto di farla sua nei recenti sviluppi a più ancora in quelli a venire nel prossimo futuro - l'accezione di 'Economia circolare' (asse portante della visione della Green Economy) che, secondo il recentissimo Piano di Azione Globale della Comunità Europea, attiene ad un sistema complesso in cui ""il valore dei prodotti, dei materiali, dell'energia e delle risorse è mantenuto quanto più a lungo possibile e la produzione di emissioni, inquinamento, scarti e rifiuti è ridotta al massimo"" (European Commission, 2017) e in cui, secondo il report della Agenzia Europea per l'Ambiente, centrale è ""contrastare il depauperamento delle risorse naturali, re-immettere nel mercato le risorse in dismissione ed agevolare il recupero delle risorse di valore"" (European Environment Agency, 2016).

In questo senso protagonista assoluto di tale innovazione è quell'approccio ecosistemico ai problemi, alle questioni in gioco e alle strategie ed azioni per risolverli, che fa fondamentalmente riferimento alla visione di tipo 'Life Cycle' strutturante il senso stesso di 'sostenibilità' di uno sviluppo capace di sostanziare la 'circolarità' del sistema economico, la 'mitigabilità' della crisi climatica, l''efficientabilità' della questione energetica, la 'capitalizzabilità' del patrimonio naturale, la 'inclusività' del benessere e la 'rigenerabilità' delle città; e di includere il concetto di 'Costo ambientale' in tutte le strategie e azioni da questi assi sottese.The role of technological culture in planning, seen in light of the three major challenges raised by the environmental and energy-related issues of our time, meaning climate change, limited resources and excessive energy consumption, is a key role, and one that we ignore at our peril when it comes to seeking out, in informed fashion, a renewal in the conditions of inhabiting the contexts of Europe and the rest of the world, a quest that necessarily brings into play concepts which differ with respect to their terminology, but yet prove quite similar when it comes to the board range of concepts covered and the wealth of their underlying objectives: Sustainable Development, Nachhaltige Entwicklung and Dévéloppement Durable.

Ensconced within each of these descriptors is one of the vital tasks that technology and the related cultural of technology must fulfil in the near future: responding to the request for sustainability in human  endeavours in future generations (Sustainable), as well as for integrity and reliability in terms of conduct and performance (Nachhaltige), together with the durability over time of the products of transformations (Durable).

The other major task of cultural technology in our time is to provide responses that prove 'dynamic' not only over the medium-long term, with respect to meeting the growing need for sustainability/reliability /durability, but that are also 'dynamic' within real space and at the present time, in the short term, supporting architecture’s equally absolute need to be 'adaptable' and 'resilient' in response to changes currently affecting the climate and the environment.

Technological culture, therefore, means possessing a thoroughgoing knowledge of the objectives to be pursued through planning, irrevocably linked to a leaning towards a systemic vision of issues, as well as a methodological approach to the strategies to be outline and promoted, plus an intimate familiarity with the considerations of feasibility and practicality tied to the initiatives to be tested, monitored and built up over time.

For that matter, all the major centres of research and experimentation in the architectural sectors of the developed countries posit the technological approach as a vital factor in the processes of conceptualisation, planning and implementation of any type of initiative meant to transform our existing realties, with the result that the various contexts of Architectural Technology, Baukonstruktion, Technologie de l'Architecture and Construcción en Arquitectura represent not only a disciplinary realm (albeit one with a marked interdisciplinary bent) but, even more to the point, the logical and cultural dimension within which the various manifestations and characteristics of planning are coordinated and brought into focus.

In identifying the innovative features displayed by technological culture as it addresses and develops research and experimentation on environmental and energy-related topics, the first observation to be made is its ability to simultaneously pursue three different categories of ""advanced"" objectives involving performance: the efficiency of initiatives in terms of controlling yields and results; the overall effectiveness of strategies in light of assessments of the ratio between the efficiency achieved and the quantities of resources involved and utilised in the processes enacted to reach those results; the satisfaction of the users of those conditions of efficiency as they perceive, or even become involved in, the new dimensions of effectiveness.

The innovative approach to conceptualisation of scenarios brought about by the evolution in the meaning of effectiveness highlights the issue of 'scarce resources' while placing the utmost importance – to the point of absorbing it in recent developments, a trend that shall become even more pronounced in the near future – on the framework of the 'Circular Economy' outlook (a cornerstone of the vision of a Green Economy), an outlook that, according to the recent Global Action Plan of the European  Community, points to a complex system in which, ""The value of products, materials, energy and resources is maintained for as long as possible, while the production of emissions, pollution, scrap and waste is reduced to the greatest possible extent"" (European Commission, 2017), with key importance placed on, ""Contrasting the impoverishment of natural resources, reintroducing discarded resources onto the market and facilitating the recovery of resources of value "" (European Environment Agency, 2016).

Seen in this light, the key ingredient to a similar innovation is the eco-systemic approach taken to the problems and issues addressed, as well as to the strategies and initiatives enacted to resolve them, an outlook that essentially consists of a 'Life Cycle' vision able to provide the structure for the fundamental 'sustainability' of a mode of development that succeeds in supporting the 'circularity' of the economic system, together with the potential for mitigating the climate crisis, for increasing energy efficiency, for capitalising on natural resources, for making wellbeing more inclusive while regenerating cities, with the concept of 'environmental cost' becoming an integral part of all the strategies and actions underlying such efforts","Cultura tecnologica, ambiente, energia: prospettive della ricerca e della sperimentazione | Technological Culture, the Environment and Energy: the outlook for research and experimentation",,,'Firenze University Press',,core
74233965,2017-09-01T00:00:00,"Bulk body motion may randomly occur during PET acquisitions introducing blurring, attenuation-emission mismatches and, in dynamic PET, discontinuities in the measured time activity curves between consecutive frames. Meanwhile, dynamic PET scans are longer, thus increasing the probability of bulk motion. In this study, we propose a streamlined 3D PET motion-compensated image reconstruction (3D-MCIR) framework, capable of robustly deconvolving intra-frame motion from a static or dynamic 3D sinogram. The presented 3D-MCIR methods need not partition the data into multiple gates, such as 4D MCIR algorithms, or access list-mode (LM) data, such as LM MCIR methods, both associated with increased computation or memory resources. The proposed algorithms can support compensation for any periodic and non-periodic motion, such as cardio-respiratory or bulk motion, the latter including rolling, twisting or drifting. Inspired from the widely adopted point-spread function (PSF) deconvolution 3D PET reconstruction techniques, here we introduce an image-based 3D generalized motion deconvolution method within the standard 3D maximum-likelihood expectation-maximization (ML-EM) reconstruction framework. In particular, we initially integrate a motion blurring kernel, accounting for every tracked motion within a frame, as an additional MLEM modeling component in the image space (integrated 3D-MCIR). Subsequently, we replaced the integrated model component with a nested iterative Richardson-Lucy (RL) image-based deconvolution method to accelerate the MLEM algorithm convergence rate (RL-3D-MCIR). The final method was evaluated with realistic simulations of whole-body dynamic PET data employing the XCAT phantom and real human bulk motion profiles, the latter estimated from volunteer dynamic MRI scans. In addition, metabolic uptake rate Ki parametric images were generated with the standard Patlak method. Our results demonstrate significant improvement in contrast-to-noise ratio (CNR) and noise-bias performance in both dynamic and parametric images. The proposed nested RL-3D-MCIR method is implemented on the Software for Tomographic Image Reconstruction (STIR) open-source platform and is scheduled for public release",Quantitative PET image reconstruction employing nested expectation-maximization deconvolution for motion compensation,10.1016/j.compmedimag.2016.11.006,https://core.ac.uk/download/74233965.pdf,'Elsevier BV',,core
480623003,2017-05-18T00:00:00,"In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable functions. This network, called additive neural network, is very suitable for mobile computing. The neural structure is based on a novel vector product definition, called ef-operator, that permits a multiplier-free implementation. In ef-operation, the ""product"" of two real numbers is defined as the sum of their absolute values, with the sign determined by the sign of the product of the numbers. This ""product"" is used to construct a vector product in n-dimensional Euclidean space. The vector product induces the lasso norm. The proposed additive neural network successfully solves the XOR problem. The experiments on MNIST dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron",An Energy Efficient Additive Neural Network,,,,,core
49322915,2017-01-15T00:00:00,"International audienceComputation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Gödelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments",A modular architecture for transparent computation in recurrent neural networks,10.1016/j.neunet.2016.09.001,,'Elsevier BV',,core
323445918,2017-12-28T00:00:00,"BACKGROUND: In metagenomics, the separation of nucleotide sequences belonging to an individual or closely matched populations is termed binning. Binning helps the evaluation of underlying microbial population structure as well as the recovery of individual genomes from a sample of uncultivable microbial organisms. Both supervised and unsupervised learning methods have been employed in binning; however, characterizing a metagenomic sample containing multiple strains remains a significant challenge. In this study, we designed and implemented a new workflow, Coverage and composition based binning of Metagenomes (CoMet), for binning contigs in a single metagenomic sample. CoMet utilizes coverage values and the compositional features of metagenomic contigs. The binning strategy in CoMet includes the initial grouping of contigs in guanine-cytosine (GC) content-coverage space and refinement of bins in tetranucleotide frequencies space in a purely unsupervised manner. With CoMet, the clustering algorithm DBSCAN is employed for binning contigs. The performances of CoMet were compared against four existing approaches for binning a single metagenomic sample, including MaxBin, Metawatt, MyCC (default) and MyCC (coverage) using multiple datasets including a sample comprised of multiple strains. RESULTS: Binning methods based on both compositional features and coverages of contigs had higher performances than the method which is based only on compositional features of contigs. CoMet yielded higher or comparable precision in comparison to the existing binning methods on benchmark datasets of varying complexities. MyCC (coverage) had the highest ranking score in F1-score. However, the performances of CoMet were higher than MyCC (coverage) on the dataset containing multiple strains. Furthermore, CoMet recovered contigs of more species and was 18 - 39% higher in precision than the compared existing methods in discriminating species from the sample of multiple strains. CoMet resulted in higher precision than MyCC (default) and MyCC (coverage) on a real metagenome. CONCLUSIONS: The approach proposed with CoMet for binning contigs, improves the precision of binning while characterizing more species in a single metagenomic sample and in a sample containing multiple strains. The F1-scores obtained from different binning strategies vary with different datasets; however, CoMet yields the highest F1-score with a sample comprised of multiple strains",CoMet: a workflow using contig coverage and composition for binning a metagenomic sample with high precision,10.1186/s12859-017-1967-3,https://core.ac.uk/download/323445918.pdf,'Springer Science and Business Media LLC',"[{'title': 'BMC Bioinformatics', 'identifiers': ['1471-2105', 'issn:1471-2105']}]",core
74508578,2016-01-01T08:00:00,"Subspace estimation appears in a wide variety of signal processing applications such as radar, communications, and underwater acoustics, often as a prelude to high resolution parameter estimation. As with any estimation problem the availability of statistical benchmarks on estimator accuracy is key to developing and understanding algorithm performance. The parameter space in general subspace/basis estimation problems is naturally described as a Riemannian quotient manifold. The concept of a manifold is central to many parts of geometry and modern mathematical physics because it allows more complicated structures to be described and understood in terms of the properties of Euclidean spaces. This identification permits the well-developed tools of differential geometry to be brought to bear on the analysis of subspace/basis estimation problems. Classical Cramér-Rao bounds (CRB), originally formulated for standard Euclidean spaces, have recently been generalized to Riemannian quotient manifolds and the particular case of the standard real linear statistical model with the Grassmann manifold (set of subspaces) as the parameter space has been analyzed. The present works applies this differential geometric approach to the analysis of the complex linear model estimation problem. We consider both the general unconstrained signal model and, most importantly, the parametrically constrained signal model. First, we show that the appropriate parameter space for the most general unconstrained signal model is a modified Stiefel manifold, termed here the Basis manifold. Elements of the Basis manifold are semi-unitary matrices grouped into equivalence classes dictated by the model invariances. Using this formulation we derive the full Fisher Information Matrix (FIM) and, from this, intrinsic CRBs on the both subspace and rotation estimation accuracy. Among the corollaries that flow from this Basis manifold formulation is a CRB on the individual columns of the eigen vector matrix, say Y (an N × p semi-unitary matrix), of the true signal covariance. The eigen decomposition of the sample covariance matrix (SCM) yields estimate of Y; the analytical expression for the corresponding estimation error covariance is well known from statistics. We show the relationship between this existing expression and the CRB developed here using the geometric formulation. Secondly, we consider situation when the signal matrix is constrained, and in particular, the important special case when the constraint arises due to some underlying parametric model. Here the set of semi-unitary matrices representing the signal matrix are constrained to a submanifold of the Basis manifold. In this geometric approach these underlying parameters are viewed as a set of natural coordinates on the submanifold; the set of values that describe the related N × p matrix representation is an alternate (extrinsic coordinate representation. Using this differential geometric framework, we derive the FIM with respect to a set of normal coordinates, and the resultant intrinsic CRB on subspace and rotational estimation accuracies for this constrained model. The relationship between this new bound on constrained subspace estimation accuracy and the existing bounds developed for system parameters (i.e., natural coordinates) is established and its relevance to the estimation problem is explored in detail. The CRB development for the constrained model naturally suggests an asymptotically ML estimation approach that leverages the estimate given by the standard EVD of the SCM (ML for the unconstrained signal matrix assumption). This estimate, referenced to an algorithmically convenient extrinsic coordinate set, may be transformed to the natural coordinates defined by the underlying parametric model. By the maximum likelihood (ML) invariance principle, this estimation approach yields an asymptotically ML estimator of the desired parameters. An efficient implementation of this general theoretical approach is developed for the important special case of the uniform multi-dimensional array and complex exponential waveform model. Utilizing the shift-invariant properties that define the constraint in this setting, we derive a closed-formed estimator of the signal matrix Y. This new estimation approach is applied to the challenging 2-D Direction-of-Arrival (DOA) estimation problem and a set specific scenarios drawn from the literature are evaluated to demonstrate performance",Bounds and algorithms for subspace estimation on Riemannian quotient submanifolds,,,DigitalCommons@URI,,core
87950546,2017-01-25T00:00:00,"This demonstration presents an open-source hardware and software platform which allows non-roboticists researchers to conduct machine learning experiments to benchmark algorithms for autonomous exploration and active learning. In particular, in addition to showing the general properties of the platform such as its modularity and usability, it demonstrates the online functioning of a particular algorithm which allows efficient learning of multiple forward and inverse models and can leverage information from human guidance. A first aspect of the demonstration is to illustrate the ease of use of the 3D printed low-cost Poppy humanoid robotic platform, that allows non-roboticists to quickly set up and program robotic experiments. A second aspect is to show how the Explauto library allows systematic comparison and evaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API to select already implemented exploration algorithms. The third idea is to showcase Active Model Babbling, an efficient exploration algorithm dynamically choosing which task/goal space to explore and particular goals to reach, and integrating social guidance from humans in real time to drive exploration towards particular objects or actions. Contact us for a CC-BY version of the video ![Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery of tool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea. [Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer, P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Education and Art. In Digital Intelligence 2014, page 6, Nantes, France. [Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open- source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-International Conference on Development and Learning, Epirob",Intrinsically Motivated Multi-Task Reinforcement LearningWith Open-Source Explauto Library and Poppy Humanoid Robot,,,HAL CCSD,,core
103638864,2016-01-09,"Kernels – which implicitly achieve rich feature space representations – are one of the most widely deployed tools in machine learning, spanning much of the field: from supervised to unsupervised learning, classification to regression, frequentist to Bayesian, etc. One particularly fruitful application of this general class of techniques is the use of gaussian processes for nonparametric regression. This course will explore the cutting edge of the modern literature on gaussian processes, covering their basic use, the theory underlying them, kernel choices, speed and scaling issues and techniques, approximate inference, their use for optimization and active learning, and more. We will also spend some weeks covering non-probabilistic kernel techniques, including the basic theories of reproducing kernel hilbert spaces, kernel mean embeddings, kernel two sample tests, and more. This is a PhD-level course, and its focal point will be a substantial project by the student (or group of two students). Students have substantial freedom in choosing these projects, from creating novel research-grade methods, to contributing to open source machine learning projects, to analyzing data of interest, to exploring a theoretical topic. Real data analysis problems and problems that connect to the student’s ongoing research agenda are particularly welcome",Instructor,,,,,core
275624161,2016-12-19T00:00:00,"The final publication is available at Springer via  http://dx.doi.org/10.1007/s11219-016-9350-6The virtue of quality is not itself a subject; it depends on a subject. In the software engineering field, quality means good software products that meet customer expectations, constraints, and requirements. Despite the numerous approaches, methods, descriptive models, and tools, that have been developed, a level of consensus has been reached by software practitioners. However, in the model-driven engineering (MDE) field, which has emerged from software engineering paradigms, quality continues to be a great challenge since the subject is not fully defined. The use of models alone is not enough to manage all of the quality issues at the modeling language level. In this work, we present the current state and some relevant considerations regarding quality in MDE, by identifying current categories in quality conception and by highlighting quality issues in real applications of the model-driven initiatives. We identified 16 categories in the definition of quality in MDE. From this identification, by applying an adaptive sampling approach, we discovered the five most influential authors for the works that propose definitions of quality. These include (in order): the OMG standards (e.g., MDA, UML, MOF, OCL, SysML), the ISO standards for software quality models (e.g., 9126 and 25,000), Krogstie, Lindland, and Moody. We also discovered families of works about quality, i.e., works that belong to the same author or topic. Seventy-three works were found with evidence of the mismatch between the academic/research field of quality evaluation of modeling languages and actual MDE practice in industry. We demonstrate that this field does not currently solve quality issues reported in industrial scenarios. The evidence of the mismatch was grouped in eight categories, four for academic/research evidence and four for industrial reports. These categories were detected based on the scope proposed in each one of the academic/research works and from the questions and issues raised by real practitioners. We then proposed a scenario to illustrate quality issues in a real information system project in which multiple modeling languages were used. For the evaluation of the quality of this MDE scenario, we chose one of the most cited and influential quality frameworks; it was detected from the information obtained in the identification of the categories about quality definition for MDE. We demonstrated that the selected framework falls short in addressing the quality issues. Finally, based on the findings, we derive eight challenges for quality evaluation in MDE projects that current quality initiatives do not address sufficiently.F.G, would like to thank COLCIENCIAS (Colombia) for funding this work through the Colciencias Grant call 512-2010. This work has been supported by the Gene-ralitat Valenciana Project IDEO (PROMETEOII/2014/039), the European Commission FP7 Project CaaS (611351), and ERDF structural funds.Giraldo-Velásquez, FD.; España Cubillo, S.; Pastor López, O.; Giraldo, WJ. (2016). Considerations about quality in model-driven engineering. Software Quality Journal. 1-66. https://doi.org/10.1007/s11219-016-9350-6S166(1985). Iso information processing—documentation symbols and conventions for data, program and system flowcharts, program network charts and system resources charts. ISO 5807:1985(E) (pp. 1–25).(2011). Iso/iec/ieee systems and software engineering – architecture description. ISO/IEC/IEEE 42010:2011(E) (Revision of ISO/IEC 42010:2007 and IEEE Std 1471-2000) (pp. 1–46).Abran, A., Moore, J.W., Bourque, P., Dupuis, R., & Tripp, L.L. (2013). Guide to the Software Engineering Body of Knowledge (SWEBOK) version 3 public review. IEEE. ISO Technical Report ISO/IEC TR 19759.Agner, L.T.W., Soares, I.W., Stadzisz, P.C., & Simão, J.M. (2013). A brazilian survey on {UML} and model-driven practices for embedded software development. Journal of Systems and Software, 86(4), 997–1005. {SI} : Software Engineering in Brazil: Retrospective and Prospective Views.Amstel, M.F.V. (2010). The right tool for the right job: assessing model transformation quality. pages 69–74. Affiliation: Eindhoven University of Technology, P.O. Box 513, 5600 MB, Eindhoven, Netherlands. Cited By (since 1996):1.Aranda, J., Damian, D., & Borici, A. (2012). Transition to model-driven engineering: what is revolutionary, what remains the same?. In Proceedings of the 15th international conference on model driven engineering languages and systems, MODELS’12 (pp. 692–708). Berlin, Heidelberg: Springer.Arendt, T., & Taentzer, G. (2013). A tool environment for quality assurance based on the eclipse modeling framework. Automated Software Engineering, 20(2), 141–184.Atkinson, C., Bunse, C., & Wüst, J. (2003). Driving component-based software development through quality modelling, volume 2693. Cited By (since 1996):3.Baker, P., Loh, S., & Weil, F. (2005). Model-driven engineering in a large industrial context—motorola case study. In Briand, L., & Williams, C. (Eds.) Model Driven Engineering Languages and Systems, volume 3713 of Lecture Notes in Computer Science (pp. 476–491). Berlin, Heidelberg: Springer.Barišić, A., Amaral, V., Goulão, M., & Barroca, B. (2011). Quality in use of domain-specific languages: a case study. In Proceedings of the 3rd ACM SIGPLAN workshop on evaluation and usability of programming languages and tools, PLATEAU ’11 (pp. 65–72). New York: ACM.Becker, J., Bergener, P., Breuker, D., & Rackers, M. (2010). Evaluating the expressiveness of domain specific modeling languages using the bunge-wand-weber ontology. In 2010 43rd Hawaii international conference on system sciences (HICSS) (pp. 1–10).Bertrand Portier, L.A. (2009). Model driven development misperceptions and challenges.Bézivin, J., & Kurtev, I. (2005). Model-based technology integration with the technical space concept. In Proceedings of the Metainformatics Symposium: Springer.Brambilla, M. (2016). How mature is of model-driven engineering as an engineering discipline @ONLINE.Brambilla, M., & Fraternali, P. (2014). Large-scale model-driven engineering of web user interaction: The webml and webratio experience. Science of Computer Programming, 89 Part B(0), 71 – 87. Special issue on Success Stories in Model Driven Engineering.Brown, A. (2009). Simple and practical model driven architecture (mda) @ONLINE.Bruel, J.-M., Combemale, B., Ober, I., & Raynal, H. (2015). Mde in practice for computational science. Procedia Computer Science, 51, 660–669.Budgen, D., Burn, A.J., Brereton, O.P., Kitchenham, B.A., & Pretorius, R. (2011). Empirical evidence about the uml: a systematic literature review. Software: Practice and Experience, 41(4), 363–392.Burden, H., Heldal, R., & Whittle, J. (2014). Comparing and contrasting model-driven engineering at three large companies. In Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM ’14 (pp. 14:1–14:10). New York: ACM.Cabot, J. Has mda been abandoned (by the omg)?Cabot, J. (2009). Modeling will be commonplace in three years time @ONLINE.Cachero, C., Poels, G., Calero, C., & Marhuenda, Y. (2007). Towards a Quality-Aware Engineering Process for the Development of Web Applications. Working Papers of Faculty of Economics and Business Administration, Ghent University, Belgium 07/462, Ghent University, Faculty of Economics and Business Administration.Challenger, M., Kardas, G., & Tekinerdogan, B. (2015). A systematic approach to evaluating domain-specific modeling language environments for multi-agent systems. Software Quality Journal, 1–41.Chaudron, M.V., Heijstek, W., & Nugroho, A. (2012). How effective is uml modeling? Software & Systems Modeling, 11(4), 571–580. J2: Softw Syst Model.Chenouard, R., Granvilliers, L., & Soto, R. (2008). Model-driven constraint programming. pages 236–246. Affiliation: CNRS, LINA, Universit de Nantes, France; Affiliation: Pontificia Universidad Catlica de, Valparaiso, Chile. Cited By (since 1996):8.Clark, T., & Muller, P.-A. (2012). Exploiting model driven technology: a tale of two startups. Software and Systems Modeling, 11(4), 481–493.Corneliussen, L. (2008). What do you think of model-driven software development?Costal, D., Gómez, C., & Guizzardi, G. (2011). Formal semantics and ontological analysis for understanding subsetting, specialization and redefinition of associations in uml. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 6998 LNCS:189–203. cited By (since 1996)3.Cruz-Lemus, J.A., Maes, A., Género, M., Poels, G., & Piattini, M. (2010). The impact of structural complexity on the understandability of uml statechart diagrams. Information Sciences, 180(11), 2209–2220. Cited By (since 1996):14.Cuadrado, J.S., Izquierdo, J.L.C., & Molina, J.G. (2014). Applying model-driven engineering in small software enterprises. Science of Computer Programming, 89 Part B(0), 176 – 198. Special issue on Success Stories in Model Driven Engineering.Da Silva, A.R. (2015). Model-driven engineering: a survey supported by the unified conceptual model. Computer Languages Systems and Structures, 43, 139–155.Da Silva Teixeira, D.G.M., Quirino, G.K., Gailly, F., De Almeida Falbo, R., Guizzardi, G., & Perini Barcellos, M. (2016). PoN-S: a Systematic Approach for Applying the Physics of Notation (PoN), (pp. 432–447). Cham: Springer International Publishing.Davies, I., Green, P., Rosemann, M., Indulska, M., & Gallo, S. (2006). How do practitioners use conceptual modeling in practice? Data and Knowledge Engineering, 58(3), 358 – 380. Including the special issue : {ER} 2004ER 2004.Davies, J., Milward, D., Wang, C.-W., & Welch, J. (2015). Formal model-driven engineering of critical information systems. Science of Computer Programming, 103(0), 88 – 113. Selected papers from the First International Workshop on Formal Techniques for Safety-Critical Systems (FTSCS 2012).De Oca, I.M.-M., Snoeck, M., Reijers, H.A., & Rodríguez-Morffi, A. (2015). A systematic literature review of studies on business process modeling quality. Information and Software Technology, 58, 187–205.DenHaan, J. (2009). 8 reasons why model driven development is dangerous @ONLINE.DenHaan, J. (2010). Model driven engineering vs the commando pattern @ONLINE.DenHaan, J. (2011a). Why aren’t we all doing model driven development yet @ONLINE.DenHaan, J. (2011b). Why there is no future model driven development @ONLINE.Di Ruscio, D., Iovino, L., & Pierantonio, A. (2013). Managing the coupled evolution of metamodels and textual concrete syntax specifications. cited By (since 1996)0.Dijkman, R.M., Dumas, M., & Ouyang, C. (2008). Semantics and analysis of business process models in {BPMN}. Information and Software Technology, 50(12), 1281–1294.Domínguez-Mayo, F.J., Escalona, M.J., Mejías, M., Ramos, I., & Fernández, L. (2011). A framework for the quality evaluation of mdwe methodologies and information technology infrastructures. International Journal of Human Capital and Information Technology Professionals, 2(4), 11–22.Domínguez-Mayo, F.J., Escalona, M.J., Mejías, M., & Torres, A.H. (2010). A quality model in a quality evaluation framework for mdwe methodologies. pages 495–506. Affiliation: Departamento de Lenguajes y Sistemas Informíticos, University of Seville, Seville, Spain., Cited By (since 1996):1.Dubray, J.-J. (2011). Why did mde miss the boat?.Escalona, M.J., Gutiérrez, J.J., Pérez-Pérez, M., Molina, A., Domínguez-Mayo, E., & Domínguez-Mayo, F.J. (2011). Measuring the Quality of Model-Driven Projects with NDT-Quality, (pp. 307–317). New York: Springer.Espinilla, M., Domínguez-Mayo, F.J., Escalona, M.J., Mejías, M., Ross, M., & Staples, G. (2011). A Method Based on AHP to Define the Quality Model of QuEF (Vol. 123, pp. 685–694). Berlin, Heidelberg: Springer.Fabra, J., Castro, V.D., Álvarez, P., & Marcos, E. (2012). Automatic execution of business process models: exploiting the benefits of model-driven engineering approaches. Journal of Systems and Software, 85(3), 607–625. Novel approaches in the design and implementation of systems/software architecture.Falkenberg, E.D., Hesse, W., Lindgreen, P., Nilsson, B.E., Oei, J.L.H., Rolland, C., Stamper, R.K., Assche, F.J.M.V., Verrijn-Stuart, A.A., & Voss, K. (1996). Frisco: a framework of information system concepts. Technical report, The IFIP WG 8. 1 Task Group FRISCO.Fettke, P., Houy, C., Vella, A.-L., & Loos, P. (2012). Towards the Reconstruction and Evaluation of Conceptual Model Quality Discourses – Methodical Framework and Application in the Context of Model Understandability, volume 113 of Lecture Notes in Business Information Processing, chapter 28, pages 406–421, Springer, Berlin, Heidelberg.Finnie, S. (2015). Modeling community: Are we missing something?Fournier, C. (2008). Is uml practical?@ONLINE.France, R., & Rumpe, B. (2007). Model-driven development of complex software: a research roadmap. In Future of Software Engineering, 2007, FOSE ’07 (pp. 37–54).Gallego, M., Giraldo, F.D., & Hitpass, B. (2015). Adapting the pbec-otss software selection approach for bpm suites: an application case. In 2015 34th International Conference of the Chilean Computer Science Society (SCCC) (pp. 1–10).Galvão, I., & Goknil, A. (2007). Survey of traceability approaches in model-driven engineering. cited By (since 1996)22.Giraldo, F., España, S., Giraldo, W., & Pastor, O. (2015). Modelling language quality evaluation in model-driven information systems engineering: a roadmap. In 2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS) (pp. 64–69).Giraldo, F., España, S., & Pastor, O. (2014). Analysing the concept of quality in model-driven engineering literature: a systematic review. In 2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS) (pp. 1–12).Giraldo, F.D., España, S., & Pastor, O. (2016). Evidences of the mismatch between industry and academy on modelling language quality evaluation. arXiv: 1606.02025 .González, C., & Cabot, J. (2014). Formal verification of static software models in mde: a systematic review. Information and Software Technology, 56(8), 821–838. cited By (since 1996)0.González, C.A., Büttner, F., Clarisó, R., & Cabot, J. (2012). Emftocsp: a tool for the lightweight verification of emf models. pages 44–50. Affiliation: cole des Mines de Nantes, INRIA, LINA, Nantes, France; Affiliation: Universitat Oberta de Catalunya, Barcelona, Spain. Cited By (since 1996):1.Gorschek, T., Tempero, E., & Angelis, L. (2014). On the use of software design models in software development practice: an empirical investigation. Journal of Systems and Software, 95(0), 176– 193.Goulão, M., Amaral, V., & Mernik, M. (2016). Quality in model-driven engineering: a tertiary study. Software Quality Journal, 1–33.Grobshtein, Y., & Dori, D. (2011). Generating sysml views from an opm model: design and evaluation. Systems Engineering, 14(3), 327–340.Haan, J.d. (2008). 8 reasons why model-driven approaches (will) fail.Harel, D., & Rumpe, B. (2000). Modeling languages: Syntax, semantics and all that stuff, part i: The basic stuff, Israel. Technical report Jerusalem Israel.Harel, D., & Rumpe, B. (2004). Meaningful modeling: what’s the semantics of semantics? Computer, 37(10), 64–72.Hebig, R., & Bendraou, R. (2014). On the need to study the impact of model driven engineering on software processes. In Proceedings of the 2014 International Conference on Software and System Process, ICSSP 2014 (pp. 164–168). New York: ACM.Heidari, F., & Loucopoulos, P. (2014). Quality evaluation framework (qef): modeling and evaluating quality of business processes. International Journal of Accounting Information Systems, 15(3), 193–223. Business Process Modeling.Heymans, P., Schobbens, P.Y., Trigaux, J.C., Bontemps, Y., Matulevicius, R., & Classen, A. (2008). Evaluating formal properties of feature diagram languages. Software, IET, 2(3), 281–302. ID 2.Hindawi, M., Morel, L., Aubry, R., & Sourrouille, J.-L. (2009). Description and Implementation of a UML Style Guide (Vol. 5421, pp. 291–302). Berlin: Springer.Hoang, D. (2012). Current limitations of mdd and its implications @ONLINE.Hodges, W. (2013). Model theory Zalta, E.N. (Ed.) The Stanford Encyclopedia of Philosophy. Fall 2013 edition.Hutchinson, J., Rouncefield, M., & Whittle, J. (2011a). Model-driven engineering practices in industry. In Proceedings of the 33rd International Conference on Software Engineering, ICSE’11 (pp. 633–642). New York: ACM.Hutchinson, J., Whittle, J., & Rouncefield, M. (2014). Model-driven engineering practices in industry: social, organizational and managerial factors that lead to success or failure. Science of Computer Programming, 89 Part B(0), 144–161. Special issue on Success Stories in Model Driven Engineering.Hutchinson, J., Whittle, J., Rouncefield, M., & Kristoffersen, S. (2011b). Empirical assessment of mde in industry. In Proceedings of the 33rd International Conference on Software Engineering, ICSE’11 (pp. 471–480). New York: ACM.Igarza, I.M.H., Boada, D.H.G., & Valdés, A.P. (2012). Una introducción al desarrollo de software dirigido por modelos. Serie Científica, 5(3).ISO/IEC (2001). ISO/IEC 9126. Software engineering—Product quality. ISO/IEC.Izurieta, C., Rojas, G., & Griffith, I. (2015). Preemptive management of model driven technical debt for improving software quality. In Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures, QoSA’15 (pp. 31–36). New York: ACM.Jalali, S., & Wohlin, C. (2012). Systematic literature studies: Database searches vs. backward snowballing. In Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM’12 (pp. 29–38). New York: ACM.Kahraman, G., & Bilgen, S. (2013). A framework for qualitative assessment of domain-specific languages. Software & Systems Modeling, 1–22.Kessentini, M., Langer, P., & Wimmer, M. (2013). Searching models, modeling search: On the synergies of sbse and mde (pp. 51–54).Kitchenham, B., & Charters, S. (2007). Guidelines for performing Systematic Literature Reviews in Software Engineering. Technical Report EBSE 2007-001, Keele University and Durham University Joint Report.Kitchenham, B., Pfleeger, S., Pickard, L., Jones, P., Hoaglin, D., El Emam, K., & Rosenberg, J. (2002). Preliminary guidelines for empirical research in software engineering. IEEE Transactions on Software Engineering, 28(8), 721–734.Klinke, M. (2008). Do you use mda/mdd/mdsd, any kind of model-driven approach? Will it be the future?Köhnlein, J. (2013). Eclipse diagram editors from a user’s perspective.Kolovos, D.S., Paige, R.F., & Polack, F.A. (2008). The grand challenge of scalability for model driven engineering. In Models in Software Engineering (pp. 48–53): Springer.Kolovos, D.S., Rose, L.M., Matragkas, N., Paige, R.F., Guerra, E., Cuadrado, J.S., De Lara, J., Ráth, I., Varró, D., Tisi, M., & Cabot, J. (2013). A research roadmap towards achieving scalability in model driven engineering. In Proceedings of the Workshop on Scalability in Model Driven Engineering, BigMDE’13 (pp. 2:1–2:10). New York: ACM.Krill, P. (2016). Uml to be ejected from microsoft visual studio (infoworld).Krogstie, J. (2012a). Model-based development and evolution of information systems: a quality approach, Springer Publishing Company, Incorporated.Krogstie, J. (2012b). Quality of modelling languages, (pp. 249–280). London: Springer.Krogstie, J. (2012c). Quality of models, (pp. 205–247). London: Springer.Krogstie, J. (2012d). Specialisations of SEQUAL, (pp. 281–326). London: Springer.Krogstie, J., Lindland, O.I., & Sindre, G. (1995). Defining quality aspects for conceptual models. In Proceedings of the IFIP International Working Conference on Information System Concepts: Towards a Consolidation of Views (pp. 216–231). London: Chapman & Hall, Ltd.Kruchten, P. (2000). The rational unified process: an introduction, 2nd edn. Boston: Addison-Wesley Longman Publishing Co., Inc.Kruchten, P., Nord, R., & Ozkaya, I. (2012). Technical debt: from metaphor to theory and practice. Software, IEEE, 29(6), 18–21.Kulkarni, V., Reddy, S., & Rajbhoj, A. (2010). Scaling up model driven engineering – experience and lessons learnt. In Petriu, D., Rouquette, N., & Haugen, y. (Eds.) Model Driven Engineering Languages and Systems, volume 6395 of Lecture Notes in Computer Science (pp. 331–345). Berlin, Heidelberg: Springer.Laguna, M.A., & Marqués, J.M. (2010). Uml support for designing software product lines: the package merge mechanism, 16(17), 2313–2332.Lange, C. (2007a). Model size matters. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 4364 LNCS:211–216. cited By (since 1996)1.Lange, C., & Chaudron, M. (2005). Managing Model Quality in UML-Based Software Development. In 13th IEEE International Workshop on Technology and Engineering Practice, 2005 (pp. 7–16).Lange, C., Chaudron, M.R.V., Muskens, J., Somers, L.J., & Dortmans, H.M. (2003). An empirical investigation in quantifying inconsistency and incompleteness of uml designs. In Incompleteness of UML Designs, Proceedings Workshop on Consistency Problems in UML-based Software Development, 6th International Conference on Unified Modeling Language, UML, 2003.Lange, C., DuBois, B., Chaudron, M., & Demeyer, S. (2006). An experimental investigation of uml modeling conventions. In Nierstrasz, O., Whittle, J., Harel, D., & Reggio, G. (Eds.) Model Driven Engineering Languages and Systems, volume 4199 of Lecture Notes in Computer Science (pp. 27–41). Berlin, Heidelberg: Springer.Lange, C.F.J., & Chaudron, M.R.V. (2006). Effe",Considerations about quality in model-driven engineering,10.1007/s11219-016-9350-6,https://riunet.upv.es/bitstream/10251/84549/2/Author%20version%20-%20Considerations%20about%20quality%20in%20model-driven%20engineering.pdf,'Springer Science and Business Media LLC',,core
225647572,2016-01-01T00:00:00,"Artificial Intelligence without Using Ontological Data about a Presupposed Reality. This paper introduces an original model to provide software agents and robots with the capacity of learning by interpreting regularities in their stream of sensorimotor experience rather than by exploiting data that would give them ontological information about a predefined domain. Specifically, this model pulls inspiration from : a) the movement of embodied cognition, b) the philosophy of knowledge, c) constructivist epistemology, and d) the theory of enaction. Respectively to these four influences : a) Our agents discover their environment through their body’s active capacity of experimentation. b) They do not know their environment “ as such” but only “ as they can experience it”. c) They construct knowledge from regularities of sensorimotor experience. d) They have some level of constitutive autonomy. Technically, this model differs from the traditional perception / cognition/ action model in that it rests upon atomic sensorimotor experiences rather than separating percepts from actions. We present algorithms that implement this model, and we describe experiments to validate these algorithms. These experiments show that the agents exhibit a certain form of intelligence through their behaviors, as they construct proto-ontological knowledge of the phenomena that appear to them when they observe persistent possibilities of sensorimotor experiences in time and space. These results promote a theory of artificial intelligence without ontological data about a presupposed reality. An application includes a more robust way of creating robots capable of constructing their own knowledge and goals in the real world, which could be initially unknown to them and un-modeled by their designers.Cet article propose un modèle original pour doter des agents informatiques ou des robots de la capacité d’apprendre en interprétant des régularités dans leur flux d’expériences sensorimotrices plutôt qu’en exploitant des données qui leur apporteraient des informations ontologiques sur un domaine prédéfini. Ce modèle s’inspire en particulier de : a) le courant de la cognition incarnée, b) la philosophie de la connaissance, c) l’épistémologie constructiviste, et d) la théorie de l’énaction. Respectivement à ces quatre influences : a) Nos agents découvrent leur environnement à travers les capacités expérimentales actives de leur corps. b) Ils ne connaissent pas leur environnement «en soi » mais uniquement «en ce qu’ils peuvent en faire l’expérience » . c) Ils construisent leurs connaissances à partir de régularités d’expériences sensorimotrices. d) Ils disposent d’une certaine autonomie constitutive.
Techniquement, ce modèle se distingue du modèle perception/cognition/action classique par le fait qu’il considère des expériences sensorimotrices atomiques au lieu de séparer les percepts et les actions. Nous présentons des algorithmes qui implémentent ce modèle, et décrivons des expérimentations permettant de les valider. Les expérimentations montrent que les agents exhibent une certaine forme d’intelligence dans leurs comportements en construisant une connaissance protoontologique des phénomènes qui apparaissent à eux quand ils constatent des possibilités d’expériences sensorimotrices persistantes dans l’espace et le temps. Ces résultats promeuvent une théorie de l’intelligence artificielle sans données ontologiques sur une réalité présupposée, avec, comme perspectives applicatives, des robots capables de construire leurs propres connaissances et objectifs dans le monde réel, initialement inconnu d’eux et non modélisé par leur concepteur.Georgeon Olivier, Mille Alain, Gay Simon. Intelligence artificielle sans données ontologiques sur une réalité présupposée. In: Intellectica. Revue de l'Association pour la Recherche Cognitive, n°65, 2016/1. Nouvelles approches en Robotique Cognitive. pp. 143-168",Intelligence artificielle sans données ontologiques sur une réalité présupposée,10.3406/intel.2016.1793,,'PERSEE Program',,core
74508563,2016-01-01T08:00:00,"Cyber-physical systems (CPS) are the new generation of engineered systems integrated with computation and physical processes. The integration of computation, communication and control adds new capabilities to the systems being able to interact with physical world. The uncertainty in physical environment makes future CPS to be more reliant on machine learning algorithms which can learn and accumulate knowledge from historical data to support intelligent decision making. Such CPS with the incorporation of intelligence or smartness are termed as intelligent CPS which are safer, more reliable and more efficient. This thesis studies fundamental machine learning algorithms in supervised and unsupervised manners and examines new computing architecture for the development of next generation CPS. Two important applications of CPS, including smart pipeline and smart grid, are also studied in this thesis. Particularly, regarding supervised machine learning algorithms, several generative learning and discriminative learning methods are proposed to improve learning performance. For the generative learning, we build novel classification methods based on exponentially embedded families (EEF), a new probability density function (PDF) estimation method, when some of the sufficient statistics are known. For the discriminative learning, we develop an extended nearest neighbor (ENN) method to predict patterns according to the maximum gain of intra-class coherence. The new method makes a prediction in a “two-way communication  style: it considers not only who are the nearest neighbors of the test sample, but also who consider the test sample as their nearest neighbors. By exploiting the generalized class-wise statistics from all training data, the proposed ENN is able to learn from the global distribution, therefore improving pattern recognition performance and providing a powerful technique for a wide range of data analysis applications. Based on the concept of ENN, an anomaly detection method is also developed in an unsupervised manner. CPS usually have high-dimensional data, such as text, video, and other multi-modal sensor data. It is necessary to reduce feature dimensions to facilitate the learning. We propose an optimal feature selection framework which aims to select feature subsets with maximum discrimination capacity. To further address the information loss issue in feature reduction, we develop a novel learning method, termed generalized PDF projection theorem (GPPT), to reconstruct the distribution in high-dimensional raw data space from the low-dimensional feature subspace. To support the distributed computations throughout the CPS, it needs a novel computing architecture to offer high-performance computing over multiple spatial and temporal scales and to support Internet of Things for machine-to-machine communications. We develop a hierarchical distributed Fog computing architecture for the next generation CPS. A prototype of such architecture for smart pipeline monitoring is implemented to verify its feasibility in real world applications. Regarding the applications, we examine false data injection detection in smart grid. False data injection is a type of malicious attack which can threaten the security of energy systems. We examine the observability of false data injection and develop statistical models to estimate underlying system states and detect false data injection attacks under different scenarios to enhance the security of power systems","Toward intelligent cyber-physical systems: Algorithms, architectures, and applications",,,DigitalCommons@URI,,core
161281539,2016-08-01T00:00:00,"Multi-label classification (MLC) is a supervised learning problem in which a particular example can be associated with a set of labels instead of a single one as in traditional classification. Many real-world applications, such as Web page classification or resource tagging on the Social Web, are challenging for existing MLC algorithms, because the label space grows exponentially as instance space increases. Under the problem transformation approach, the most common alternative for MLC, multi-label problems are transformed into several single label problems, whose outputs are then aggregated into a prediction to the whole classification problem. Feature selection techniques become crucial in large-scale MLC problems to help reducing dimensionality. However, the impact of feature selection in multi-label setting has not been as extensively studied as in the case of single-label data. In this paper, we present an empirical evaluation of feature selection techniques in the context of the three main problem transformation MLC methods: Binary Relevance, Pair-wise and Label power-set. Experimentation was performed across a number of benchmark datasets for multi-label classification exhibiting varied characteristics, which allows observing the behavior of techniques and assessing their impact according to multiple metrics.Fil: Rodriguez, Juan Manuel. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Instituto Superior de Ingeniería del Software. Universidad Nacional del Centro de la Provincia de Buenos Aires. Instituto Superior de Ingeniería del Software; ArgentinaFil: Godoy, Daniela Lis. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Instituto Superior de Ingeniería del Software. Universidad Nacional del Centro de la Provincia de Buenos Aires. Instituto Superior de Ingeniería del Software; ArgentinaFil: Zunino Suarez, Alejandro Octavio. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Instituto Superior de Ingeniería del Software. Universidad Nacional del Centro de la Provincia de Buenos Aires. Instituto Superior de Ingeniería del Software; Argentin",An empirical comparison of feature selection methods in problem transformation multi-label classification,10.1109/TLA.2016.7786364,,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': 'IEEE Latin America Transactions', 'identifiers': ['issn:1548-0992', '1548-0992']}]",core
149937767,2016-03-01T13:58:49Z,"<div>1. Overview</div><div><br></div><div>HTRU2 is a data set which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey (South) [1]. </div><div><br></div><div></div><div>Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter (see [2] for more uses). </div><div><br></div><div>As pulsars rotate, their emission beam sweeps across the sky, and when this crosses our line of sight, produces a detectable pattern of broadband radio emission. As pulsars</div><div>rotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking for periodic radio signals with large radio telescopes.</div><div><br></div><div></div><div>Each pulsar produces a slightly different emission pattern, which varies slightly with each rotation (see [2] for an introduction to pulsar astrophysics to find out why). Thus a  potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of an observation. In the absence of additional info, each candidate could potentially describe a real pulsar. However in practice almost all detections are caused by radio frequency interference (RFI) and noise, making legitimate signals hard to find.</div><div><br></div><div></div><div>Machine learning tools are now being used to automatically label pulsar candidates to facilitate rapid analysis. Classification systems in particular are being widely adopted,</div><div>(see [4,5,6,7,8,9]) which treat the candidate data sets  as binary classification problems. Here the legitimate pulsar examples are a minority positive class, and spurious examples the majority negative class. At present multi-class labels are unavailable, given the costs associated with data annotation.</div><div><br></div><div></div><div>The data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators. Each candidate is described by 8 continuous variables. The first four are simple statistics obtained from the integrated pulse profile (folded profile). This is an array of continuous variables that describe a longitude-resolved version of the signal that has been averaged in both time and frequency (see [3] for more details). The remaining four variables are similarly obtained from the DM-SNR curve (again see [3] for more details). These are summarised below:</div><div><br></div><div></div><div>1. Mean of the integrated profile.</div><div>2. Standard deviation of the integrated profile.</div><div>3. Excess kurtosis of the integrated profile.</div><div>4. Skewness of the integrated profile.</div><div>5. Mean of the DM-SNR curve.</div><div>6. Standard deviation of the DM-SNR curve.</div><div>7. Excess kurtosis of the DM-SNR curve.</div><div>8. Skewness of the DM-SNR curve.</div><div><br></div><div></div><div>HTRU 2 Summary</div><div></div><div>17,898 total examples.</div><div>1,639 positive examples.</div><div>16,259 negative examples.</div><div><br></div><div></div><div></div><div>The data is presented in two formats: CSV and ARFF (used by the WEKA data mining tool). Candidates are stored in both files in separate rows. Each row lists the variables first, and the class label is the final entry. The class labels used are 0 (negative) and 1 (positive).</div><div><br></div><div></div><div>Please note that the data contains no positional information or other astronomical details. It is simply feature data extracted from candidate files using the PulsarFeatureLab tool (see [10]).</div><div><br></div><div>2. Citing our work</div><div><br></div><div></div><div>If you use the dataset in your work please cite us using the DOI of the dataset, and the paper: </div><div><br></div><div>R. J. Lyon, B. W. Stappers, S. Cooper, J. M. Brooke, J. D. Knowles, Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach MNRAS, 2016.</div><div><br></div><div></div><div>3. Acknowledgements</div><div><br></div><div>This data was obtained with the support of grant EP/I028099/1 for the University of Manchester  Centre for Doctoral Training in Computer Science, from the UK Engineering and Physical Sciences Research Council (EPSRC). The raw observational data was collected by the High Time Resolution Universe Collaboration using the Parkes Observatory, funded by the Commonwealth of Australia and managed by the CSIRO.</div><div><br></div><div></div><div>4. References</div><div><br></div><div>[1] M.~J. Keith et al., ""The High Time Resolution Universe Pulsar Survey - I. System Configuration and Initial Discoveries"",2010, Monthly Notices of the Royal Astronomical Society, vol. 409,  pp. 619-627. DOI: 10.1111/j.1365-2966.2010.17325.x</div><div><br></div><div></div><div>[2] D. R. Lorimer and M. Kramer, ""Handbook of Pulsar Astronomy"", Cambridge University Press, 2005.</div><div><br></div><div></div><div>[3] R. J. Lyon, ""Why Are Pulsars Hard To Find?"", PhD Thesis, University of Manchester, 2015.</div><div><br></div><div></div><div>[4] R. J. Lyon et al., ""Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach"", Monthly Notices of the Royal Astronomical Society, submitted.</div><div><br></div><div></div><div>[5] R. P. Eatough et al., ""Selection of radio pulsar candidates using artificial neural networks"", Monthly Notices of the Royal Astronomical Society, vol. 407, no. 4, pp. 2443-2450, 2010.</div><div><br></div><div></div><div>[6] S. D. Bates et al., ""The high time resolution universe pulsar survey vi. an artificial neural network and timing of 75 pulsars"", Monthly Notices of the Royal Astronomical Society, vol. 427, no. 2, pp. 1052-1065, 2012.</div><div><br></div><div>[7] D. Thornton, ""The High Time Resolution Radio Sky"", PhD thesis, University of Manchester, Jodrell Bank Centre for Astrophysics School of Physics and Astronomy, 2013.</div><div><br></div><div></div><div>[8] K. J. Lee et al., ""PEACE: pulsar evaluation algorithm for candidate extraction a software package for post-analysis processing of pulsar survey candidates"", Monthly Notices of the Royal Astronomical Society, vol. 433, no. 1, pp. 688-694, 2013.</div><div><br></div><div></div><div>[9] V. Morello et al., ""SPINN: a straightforward machine learning solution to the pulsar candidate selection problem"", Monthly Notices of the Royal Astronomical Society, vol. 443, no. 2, pp. 1651-1662, 2014.</div><div><br></div><div></div><div>[10] R. J. Lyon, ""PulsarFeatureLab"", 2015, https://dx.doi.org/10.6084/m9.figshare.1536472.v1.</div><div><br></div",HTRU2,10.6084/m9.figshare.3080389.v1,,,,core
84591553,2017-11-01T00:00:00,"A robot agent designed to engage in real-world human--robot joint action must be able to understand the social states of the human users it interacts with in order to behave appropriately. In particular, in a dynamic public space, a crucial task for the robot is to determine the needs and intentions of all of the people in the scene, so that it only interacts with people who intend to interact with it. We address the task of estimating the engagement state of customers for a robot bartender based on the data from audiovisual sensors. We begin with an offline experiment using hidden Markov models, confirming that the sensor data contains the information necessary to estimate user state. We then present two strategies for online state estimation: a rule-based classifier based on observed human behaviour in real bars, and a set of supervised classifiers trained on a labelled corpus. These strategies are compared in offline cross-validation, in an online user study, and through validation against a separate test corpus. These studies show that while the trained classifiers are best in a cross-validation setting, the rule-based classifier performs best with novel data; however, all classifiers also change their estimate too frequently for practical use. To address this issue, we present a final classifier based on Conditional Random Fields: this model has comparable performance on the test data, with increased stability. In summary, though, the rule-based classifier shows competitive performance with the trained classifiers, suggesting that for this task, such a simple model could actually be a preferred option, providing useful online performance while avoiding the implementation and data-scarcity issues involved in using machine learning for this task",Automatically classifying user engagement for dynamic multi-party human–robot interaction,10.1007/s12369-017-0414-y,https://core.ac.uk/download/84591553.pdf,'Springer Science and Business Media LLC',,core
132448913,2016-08-26T00:00:00,"This thesis proposes a hybrid neuro-evolutive algorithm (NEA) that uses a compact indirect
encoding scheme (IES) for representing its genotypes (a set of ten production rules of a
Lindenmayer System with memory), moreover has the ability to reuse the genotypes and
automatically build modular, hierarchical and recurrent neural networks. A genetic algorithm
(GA) evolves a Lindenmayer System (L-System) that is used to design the neural network’s
architecture. This basic neural codification confers scalability and search space reduction in
relation to other methods. Furthermore, the system uses a parallel genome scan engine that
increases both the implicit parallelism and convergence of the GA. The fitness function of the
NEA rewards economical artificial neural networks (ANNs) that are easily implemented. The
NEA was tested on five real-world classification datasets and three well-known datasets for
time series forecasting (TSF). The results are statistically compared against established stateof-
the-art algorithms and various forecasting methods (ADANN, ARIMA, UCM, and Forecast
Pro®). In most cases, our NEA outperformed the other methods, delivering the most accurate
classification and time series forecasting with the least computational effort. These superior
results are attributed to the improved effectiveness and efficiency of NEA in the decisionmaking
process. The result is an optimized neural network architecture for solving
classification problems and simulating dynamical systems.Essa tese propõe um algoritmo neuro-evolutivo (ANE) que utiliza um esquema de codificação indireto compacto para representar seus genótipos (um conjunto de dez regras de produção de um sistema de lindenmayer com memória), além disso, possui a habilidade de reuso dos genótipos e automaticamente construir redes neurais modulares, hierárquicas e recorrentes. Um algoritmo genético evolui um sistema de lindenmayer (sistema-l) que é usado para projetar a arquitetura de redes neurais. Essa codificação neural proporciona redução de escalabilidade e do espaço de busca em relação a outros métodos, possibilitando uma busca mais eficiente no espaço infinito de arquiteturas de redes neurais. Em adição, o sistema usa um mecanismo de checagem paralelo do genoma que aumenta o paralelismo implícito e a convergência do AG. A função fitness do ANE recompensa redes neurais que são facilmente implementadas. Essa é a primeira tentativa de gerar redes recorrentes a partir dessa combinação de metáforas. O ANE foi testado utilizando cinco bancos de dados do mundo real para classificação e três bens conhecidos para predição de séries temporais (PST). Os resultados são estatisticamente comparados com algoritmos proeminentes citados no estado da arte e com vários métodos de predição (ADANN, ARIMA, UCM e Forecast Pro®). Na maioria dos casos, o ANE superou os outros métodos produzindo classificação e predição de séries temporais mais precisas com um menor esforço computacional. Esses resultados são atribuídos a melhoria da eficácia e eficiência no processo de tomada de decisão. O resultado é uma arquitetura de rede neural otimizada para resolver problemas de classificação e simular problemas dinâmicos",Uma metodologia biologicamente inspirada para projeto automático de redes neurais artificiais usando Sistemas-L paramétricos com memória,,,Programa de Pós-Graduação em Engenharia Elétrica,,core
23483067,2013-12-03,"Abstract. We construct an innovative SVP(CVP) solver for ideal lattices in case of any relative extension of number fields L/K of degree n where L is real(contained in R). The solver, by exploiting the relationships between the so-called local and global number fields, reduces solving SVP(CVP) of the input ideal A in field L to solving a set of (at most n) SVP(CVP) of the ideals Ai in field Li with relative degree 1 ≤ ni &lt; n and ∑ i ni = n. The solver’s space-complexity is polynomial and its time-complexity’s explicit dependence on the dimension (relative extension degree n) is also polynomial. More precisely, our solver’s time-complexity is poly(n, |S|, NP G, NP T, Nd, Nl) where |S | is bit-size of the input data and NP G, NP T, Nd, Nl are the number of calls to some oracles for relatively simpler problems (some of which are decisional). This feature implies that if such oracles can be implemented by efficient algorithms (with time-complexity polynomial in n), which is indeed possible in some situations, our solver will perform in this case with timecomplexity polynomial in n. Even if there is no efficient implementations for these oracles, this solver’s time-complexity may still be significantly lower than those for general lattices, because these oracles may be implemented by algorithms with sub-exponential time-complexity Keywords:Shortest Vector Problem(SVP); Closest Vector Problem(CVP); ideal lattices; field valuations; non-Archimedean valuations; local field; global field. ",A Local-Global Approach to Solving Ideal Lattice Problems,,,,,core
31292035,2014-09-19T00:00:00,"In this manuscript, I describe my research work concerning signal and image processing, and instrumentation for astronomy. I also present the projectsI intend to develop in the next few years. The thematic areas I dealt with were numerous and diverse, and the applications concerned, among others,visual and spectroscopic double stars, tidal effects in Am-type stars, shell galaxies, atmospheric turbulence and wind profiles with the SCIDAR technique.Among the main astrophysical results to which I contributed, are the validation of merging models to explain the origin of shells around elliptical galaxies,the evidence of the presence of dark matter in those galaxies, the detailed study of many spectroscopic binaries with the constitution of representative samples, including long period orbit systems containing FGKM, Am or composite spectrum stars. In particular, a thorough study of a sample of a hundred Am-type stars highlighted the influence of tidal effects, which contributed to circularizing the orbits and to synchronizing the spin and orbital motion in many systems. The fractional critical radii for circularisation and synchronisation that we have determined for our sample were found to be compatible with Zahn's theoretical models. We have also compared the estimated ages of our systems with the theoretical circularisation and synchronisation characteristic times, also predicted by Zahn. We found a fair agreement for the synchronism, but for circularisation, the tidal effects we have observed are more important than what was expected by the theory, for Am-type stars with a radiative envelope.As regards the visual binary stars, in collaboration with a group of European researchers, we have obtained a few thousand measurementswith the PISCO and PISCO2 instruments, by using the speckle interferometry technique. Those accurate measurements have been calibratedin an absolute way with a diffraction grating. Their spatial resolution often reached the diffraction limit of the telescope. They have already led to the revision of a few hundred orbits. As their orbital periods are very long, and generally exceed a few hundred years, the observation of the visual double stars is a substantial program, which requires reliability and regularity over a very long period of time. This program needs an international collaboration that has been already active since several generations. As soon as they have been published, our measures were incorporated into the double star data base of the US. Navy in Washington DC, which is open to the whole scientific community.With regard to instrumentation, my activities mainly concerned the designing, construction and operation of the PISCO and PISCO2 focal instruments.A substantial part was devoted to real-time software developments for the remote control of the instruments and the detectors, and the subsequent data processing. The main selected criteria were ``performance and reliability'', both for hardware and software developments. Indeed those instruments and programs have been used during nearly every clear night, for many years (more than 20 years for PISCO, and already five years for PISCO2).The processing of PISCO data also lead me to developing programs for atmospheric turbulence profile inversion from SCIDAR observations, that can be operated in real-time and in a non-supervised mode. Those programs allow the restoration of turbulence profiles ($C_N^2$ parameter,seeing, coherence radius and coherence time) and of the wind parameters (velocity and direction) in the turbulent layers. When the observations are obtained in ``generalized SCIDAR'' mode, we can determine those parameters in all the turbulent layers that have been crossed by the incoming light,from the inside of the dome to altitudes up to 20--23 km. I have also written specialized software for computing orbits from radial velocities formultiple stellar systems made of two or three components. The main difficulty of this problem rested in the irregular sampling of the measurements.Finally, over the last few years, I have been working on the problems encountered for the phase self-calibration for aperture synthesis inastronomy, and for the calibration of GNSS (Global Navigation Satellite System) networks in geodesy, to which my principal collaborator, A.~Lannes (L2S, Paris), has proposed an elegant and original solution.For the coming years, I would like to work in collaboration with colleagues, but also if possible with some students (graduate trainees or PhD students) on the following projects:- ""Instrumentation'' projects concerning (i) transfer of PISCO to the new Epsilon 1 m telescope of OCA (Obs. Côte d'Azur), with the automation of the data acquisition and processing, (ii) automation of PISCO2 on the L76 refractor of OCA, and (iii) software development for the ``Shack-Hartmann'' modeof PISCO.- ""Signal processing"" projects about (i) the implementation of the calibration methods proposed by A.~Lannes for radio-astronomy (ALMA) and for GNSS networks, and (ii) non-supervised classification of X sources that have been detected by the XMM-Newton space telescope.- ""Astrophysical projects'' concerning the observation of double stars belonging to the Pre-Main Sequence (PMS) of the HR diagram, and of red dwarfs of the solar neighbourhood, with PISCO and PISCO2, to determine the orbits and the masses of those stars, which are still poorly known.Dans ce manuscrit, je décris mes activités de recherche en traitement du signal, des images et en l'instrumentation pour l'astronomie et je présente les projets que je compte développer dans les prochaines années. Les thématiques que j'ai abordées sont très variées, et les applications ont concerné à la fois les étoiles doubles visuelles et spectroscopiques, les galaxies elliptiques à coquilles et la turbulence atmosphérique et de profils de vitesse du ventpar la technique SCIDAR.Parmi les principaux résultats astrophysiques auxquels j'ai contribué, on peut mentionner la validation des modèles de fusion pourexpliquer l'origine des coquilles, la mise en évidence de matière noire dans les galaxies elliptiques, la constitution d'échantillons représentatifset l'étude détaillée de nombreuses binaires spectroscopiques, dont certaines à longue période, de type FGKM, Am et d'étoiles à spectre composite. En particulier une étude approfondie d'un échantillon d'une centaine d'étoiles Am a permis de mettre en évidence des effets de marées, semanifestant par une circularisation des orbites et une synchronisation entre la rotation axiale des étoiles et leur mouvement orbital. Notreétude a conduit à préciser la valeur de paramètres importants pour la modélisation (rayons fractionnaires critiques) et nousavons pu comparer les temps caractéristiques  théoriques de circularisation et synchronisation avec les âges estimés pour cessystèmes. L'accord est convenable pour le synchronisme mais moins bon pour la circularisation. Les effets de marée observés semblentplus importants que ceux prévus par la théorie, pour les étoiles de type Am, à enveloppe radiative.En ce qui concerne les étoiles doubles visuelles, en collaboration avec un groupe de chercheurs européens, nous avons obtenu plusieurs milliersde mesures avec PISCO et PISCO2, en utilisant des techniques d'interférométrie des tavelures. Ces mesures très précises ont été calibrées de façonabsolue avec un réseau de diffraction, et ont une résolution atteignant la limite de diffraction du télescope. Elles ont déjà conduit à la révision de quelques centaines d'orbites. Les périodes orbitales étant très longues, souvent supérieures à des centaines d'années, l'observation des étoiles doubles est un programme de fond, qui nécessite régularité et durée dans le temps. Ce programme ne peut se faire que dans le cadre d'une collaboration internationale et inter-générationnelle. Au fur et à mesure des publications, nos mesures ont été intégrées dans la base de donnéesspécialisée de l'observatoire de l'US. Navy de Washington (USA), qui est ouverte à toute la communauté.Sur le plan instrumental mes activités ont concerné principalement la conception, la réalisation et le suivi de l'exploitation d'instruments comme PISCO et PISCO2, avec une part importante consacrée au développement de logiciels temps réel pour le contrôle des instruments, des détecteurs et letraitement des données. Les principaux critères que nous avons retenus ont été ``performance et fiabilité'', puisque ces instruments et logiciels sont utilisés, pratiquement toutes les nuits de beau temps, depuis de nombreuses années (plus de 20 ans pour PISCO).Le traitement des données de PISCO m'a aussi conduit à développer des programmes d'inversion de profils de turbulence atmosphérique à partird'observation SCIDAR, qui peuvent fonctionner en mode non supervisé et en temps réel. Ils permettent de restaurer des profils de la turbulence (paramètre $C_N^2$, seeing, rayon et temps de cohérence) et des paramètres du vent en altitude (vitesse et direction) dans les couches turbulentes.Lorsque les observations ont été obtenues en mode ``SCIDAR généralisé'', ils permet de caractériser la turbulence dans toutes les couches traversées,depuis l'intérieur de la coupole jusqu'à des altitudes de 20 à 23 km. J'ai aussi écrit des programmes de calcul d'orbites à partir de mesures de vitesses radiales pour des systèmes à deux ou trois composantes. La difficulté de ce problème résidait principalement dans l'irrégularité de l'échantillonnage des mesures. Enfin depuis quelques années, je travaille sur des problèmes d'auto-calibration de phase pour la synthèse d'ouverture en astronomie,et de calibration des réseaux GNSS (Global Navigation Satellite System) en géodésie, auxquels mon principal collaborateur, A. Lannes (L2S, Paris), a apporté une solution élégante et originale.Pour les prochaines années, je voudrais travailler en collaboration avec des collègues, mais aussi si possible avec la participation d'étudiants (stages niveau ""master'' ou ""thèse'') sur les projets suivants:- projets ""Instrumentaux'' concernant (i) le transfert de PISCO sur le nouveau télescope Epsilon de 1 m de l'OCA (Obs. Côte d'Azur), avec une automatisation de l'acquisition et du traitement des données. (ii) l'automatisation des fonctions de PISCO2 sur la lunette L76 de Nice,et (iii) le développement d'un logiciel de traitement pour le mode ``Shack-Hartmann'' de PISCO.- projets ""Traitement du signal"" portant sur (i) la mise en oeuvre des méthodes de calibration proposées par A. Lannes pour la radio-astronomie et pour les réseaux GNSS et (ii) sur  la classification non supervisée de sources détectées par le télescope spatial XMM-Newton.- projets ""Astrophysique'' concernant l'observation d'étoiles doubles pré-séquence principale du diagramme HR et des naines rouges du voisinage solaire avec PISCO et PISCO2, pour déterminer les orbites et les masses de ces étoiles, encore mal connues","Contribution en Signal, Image et Instrumentation pour l'Astronomie",,HAL CCSD,,,core
87661950,2014-01-01T00:00:00Z,"Artificial intelligence methodologies, as the core of discrete control and decision support systems, have been extensively applied in the industrial production sector. The resulting tools produce excellent results in certain cases; however, the NP-hard nature of many discrete control or decision making problems in the manufacturing area may require unaffordable computational resources, constrained by the limited available time required to obtain a solution. With the purpose of improving the efficiency of a control methodology for discrete systems, based on a simulation-based optimization and the Petri net (PN) model of the real discrete event dynamic system (DEDS), this paper presents a strategy, where a transformation applied to the model allows removing the redundant information to obtain a smaller model containing the same useful information. As a result, faster discrete optimizations can be implemented. This methodology is based on the use of a formalism belonging to the paradigm of the PN for describing DEDS, the disjunctive colored PN. Furthermore, the metaheuristic of genetic algorithms is applied to the search of the best solutions in the solution space. As an illustration of the methodology proposal, its performance is compared with the classic approach on a case study, obtaining faster the optimal solution",Control of Discrete Event Systems by Means of Discrete Optimization and Disjunctive Colored PNs: Application to Manufacturing Facilities,,Hindawi Limited,"[{'title': None, 'identifiers': ['issn:1085-3375', '1687-0409', 'issn:1687-0409', '1085-3375']}]",10.1155/2014/821707,core
104676264,2014,"In brain imaging, solving learning problems in multi-subjects settings is difficult because of the differences that exist across individuals. Here we introduce a novel classification framework based on group-invariant graphical representations, allowing to overcome the inter-subject variability present in functional magnetic resonance imaging (fMRI) data and to perform multivariate pattern analysis across subjects. Our contribution is twofold: first, we propose an unsupervised representation learning scheme that encodes all relevant characteristics of distributed fMRI patterns into attributed graphs; second, we introduce a custom-designed graph kernel that exploits all these characteristics and makes it possible to perform supervised learning (here, classification) directly in graph space. The well-foundedness of our technique and the robustness of the performance to the parameter setting are demonstrated through inter-subject classification experiments conducted on both artificial data and a real fMRI experiment aimed at characterizing local cortical representations. Our results show that our framework produces accurate inter-subject predictions and that it outperforms a wide range of state-of-the-art vector- and parcel-based classification methods. Moreover, the genericity of our method makes it is easily adaptable to a wide range of potential applications. The dataset used in this study and an implementation of our framework are available a",Graph-Based Inter-Subject Pattern Analysis of fMRI Data,,,,,core
29723989,2013-01-01T00:00:00,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.  While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.  The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg-Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg-Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg-Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.  Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H2O, O3, HNO3, CH4, N2O, NO2, CFC-11, CFC-12, N2O5 and ClONO2. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements. © Author(s) 2013",Ten years of MIPAS measurements with ESA Level 2 processor V6-Part 1: Retrieval algorithm and diagnostics of the products,,,,,core
49608797,2014-07-01T00:00:00,"International audienceIn brain imaging, solving learning problems in multi-subjects settings is difficult because of the differences that exist across individuals. Here we introduce a novel classification framework based on group-invariant graphical representations, allowing to overcome the inter-subject variability present in functional magnetic resonance imaging (fMRI) data and to perform multivariate pattern analysis across subjects. Our contribution is twofold: first, we propose an unsupervised representation learning scheme that encodes all relevant characteristics of distributed fMRI patterns into attributed graphs; second, we introduce a custom-designed graph kernel that exploits all these characteristics and makes it possible to perform supervised learning (here, classification) directly in graph space. The well-foundedness of our technique and the robustness of the performance to the parameter setting are demonstrated through inter-subject classification experiments conducted on both artificial data and a real fMRI experiment aimed at characterizing local cortical representations. Our results show that our framework produces accurate inter-subject predictions and that it outperforms a wide range of state-of-the-art vector- and parcel-based classification methods. Moreover, the genericity of our method makes it is easily adaptable to a wide range of potential applications. The dataset used in this study and an implementation of our framework are available at http://dx.doi.org/10.6084/m9.figshare.1086317",Graph-based inter-subject pattern analysis of fMRI data,https://core.ac.uk/download/49608797.pdf,'Public Library of Science (PLoS)',,10.1371/journal.pone.0104586,core
481314527,2013-09-13T00:00:00,"This  paper  demonstrates  a  new  alternative  way  in  estimating seismically thin-bed (below-tuning) thickness. Initial thickness is built by bandpass filtering the amplitude display of a zero-phase seismic. The filter removes the  non  minimum  and  or  non  maximum  and  left  the  maximum  and  or  the minimum of seismic amplitude. The unresolved below-tuning thickness is then corrected  by  zero-INTENS-difference  (z-i-d)  attribute.  INTENS  is  integrated energy  spectra,  an  attribute  which  can  be  derived  from  spectral  analysis.  z-i-d attribute is zero difference of INTENS between the seismic and its synthetic. The method  generates  INTENS  difference  profile  by  subtracting  seismic  INTENS and its synthetic INTENS iteratively. The iteration is controlled by dipole space shifting from  distance to closer or  vice  versa.  The true thickness is derived  by locating z-i-d which laid in INTENS different profile. It has found that, for free noise  true  seismic  and  perfect-wavelet  (a  wavelet  which  only  approximately similar  with  wavelet  which  constructing  the  true  seismic)  synthetic  seismic,  in INTENS  different  profile,  the  z-i-d  location  always  corresponds  to  true  dipole space or thickness. The method could resolve all thickness of a wedge-modeled seismic with three different dominant frequencies. When the synthetic seismic is constructed with imperfect wavelet, slightly different analysis is needed to locate z-i-d  attribute  and  the  result  is  not  as  perfect  as  when  perfect  wavelet constructing synthetic seismic. A quiet similar result is got when the method is implemented  for  noisy  wedge-modeled  seismic.  Bad  thickness  estimation  is resulted  for  20%  noise  seismic.  The  method  algorithm  is  extended  for  similar dipole polarity model and multilayer model to bring the method to real seismic data  nearer.  The  extension  is  done  by  estimating  thickness  of  every  layer  of  a stacked-wedge-modeled  seismic. The algorithm then generalized for estimating layers  thickness  with  several  thickness  combinations.  The  method  was  able  to delineate shallow channel of Stratton Field by providing good pseudo-acousticimpedance (pseudo AI) map",Combination of Minimum-Maximum (m-m) Attribute and Zero-INTENS-Difference (z-i-d) Attribute for Estimating Seismically Thin-Bed Thickness,https://core.ac.uk/download/481314527.pdf,"Institute for Research and Community Services, Institut Teknologi Bandung",,,core
22974905,2013-08-21,"Abstract – In Virtual-Environment (VE) Applications, where virtual objects are presented in a head-mounted display, virtual images must be continuously stabilized in space against the user’s head motion. Latencies in head-motion compensation cause virtual objects to swim around instead of being stable in space. This results in an unnatural feel, disorientation, and simulation sickness in addition to errors in fitting/matching of virtual and real objects. Visual update delays are a critical technical obstacle for implementation of head-mounted displays in a wide variety of applications. To address this problem, we propose to use machine learning techniques to define a forward model of head movement based on angular velocity information. In particular, we utilize recurrent neural network to capture the temporal pattern of pitch and yaw motion. Our results demonstrate an ability to predict head motion up to 40 ms. ahead thus eliminating the main source of latencies. The accuracy of the system is tested for conditions akin to those encountered in virtual environments. These results demonstrate successful generalization by the learning system. that the virtual object is displayed in the same location in space as it was prior to the motion. Fig. 2 shows that the point on the HMD representing a stationary point in space needs to be shifted on the next display update in order to compensate for the user’s head motion. That can be accomplished by using the linear and angular location, velocity, and acceleration measurements out of the head tracker to compute and render the next updated image (henceforth, we will omit the word angular). Currently these operations take an overall minimum of about 25 ms. I",Prediction of Pitch and Yaw Head Movements via Recurrent Neural Networks,,,,,core
23967888,2014-08-27,"Over the years, demand on space systems has increased tremendously and this trend will continue for the near future. Enhanced capabilities of space systems, however, can only be met with increased complexity and sophistication of onboard and ground systems. Artificial intelligence and expert system techniques have great potential in space applications. Expert systems could facilitate autonomous decision making, improve in-orbit fault diagnosis and repair, enhance performance and reduce reliance on ground support. However, real-time expert systems, unlike conventional off-line consultative systems, have to satisfy certain special stringent requirements before they could be used for onboard space applications. Challenging and interesting new environments are faced while developing expert system space applications. This paper discusses the special characteristics, requirements and typical life cycle issues for onboard expert systems. Further, it also describes considerations in design, development, and implementation which are particulary important to real-time expert systems for space applications. 1",Considerations in Development of Expert Systems for Real-Time Space Applications,,,,,core
427084953,2013-09-13T00:00:00,"This  paper  demonstrates  a  new  alternative  way  in  estimating seismically thin-bed (below-tuning) thickness. Initial thickness is built by bandpass filtering the amplitude display of a zero-phase seismic. The filter removes the  non  minimum  and  or  non  maximum  and  left  the  maximum  and  or  the minimum of seismic amplitude. The unresolved below-tuning thickness is then corrected  by  zero-INTENS-difference  (z-i-d)  attribute.  INTENS  is  integrated energy  spectra,  an  attribute  which  can  be  derived  from  spectral  analysis.  z-i-d attribute is zero difference of INTENS between the seismic and its synthetic. The method  generates  INTENS  difference  profile  by  subtracting  seismic  INTENS and its synthetic INTENS iteratively. The iteration is controlled by dipole space shifting from  distance to closer or  vice  versa.  The true thickness is derived  by locating z-i-d which laid in INTENS different profile. It has found that, for free noise  true  seismic  and  perfect-wavelet  (a  wavelet  which  only  approximately similar  with  wavelet  which  constructing  the  true  seismic)  synthetic  seismic,  in INTENS  different  profile,  the  z-i-d  location  always  corresponds  to  true  dipole space or thickness. The method could resolve all thickness of a wedge-modeled seismic with three different dominant frequencies. When the synthetic seismic is constructed with imperfect wavelet, slightly different analysis is needed to locate z-i-d  attribute  and  the  result  is  not  as  perfect  as  when  perfect  wavelet constructing synthetic seismic. A quiet similar result is got when the method is implemented  for  noisy  wedge-modeled  seismic.  Bad  thickness  estimation  is resulted  for  20%  noise  seismic.  The  method  algorithm  is  extended  for  similar dipole polarity model and multilayer model to bring the method to real seismic data  nearer.  The  extension  is  done  by  estimating  thickness  of  every  layer  of  a stacked-wedge-modeled  seismic. The algorithm then generalized for estimating layers  thickness  with  several  thickness  combinations.  The  method  was  able  to delineate shallow channel of Stratton Field by providing good pseudo-acousticimpedance (pseudo AI) map",Combination of Minimum-Maximum (m-m) Attribute and Zero-INTENS-Difference (z-i-d) Attribute for Estimating Seismically Thin-Bed Thickness,https://core.ac.uk/download/427084953.pdf,'The Institute for Research and Community Services (LPPM) ITB',,10.5614/itbj.eng.sci.2011.43.2.1,core
199617527,2013-01-01T00:00:00,"Wings, control surfaces and rotor blades subject to aerodynamic forces may exhibit aeroelastic instabilities such as flutter, divergence and limit cycle oscillations which generally reduce their life and functionality. This possibility of instability must be taken into account during the design process and numerical simulation models may be used to predict aeroelastic stability. Aeroelastic stability is a design requirement that encompasses several difficulties also found in other areas of design. For instance, the large computational time associated with stability analysis is also found in computational fluid dynamics (CFD) models. It is a major hurdle in numerical optimization and reliability analysis, which generally require large numbers of call to the simulation code. Similarly, the presence of bifurcations and discontinuities is also encountered in structural impact analysis based on nonlinear dynamic simulations and renders traditional approximation techniques such as Kriging ineffective. Finally, for a given component or system, aeroelastic instability is only one of multiple failure modes which must be accounted for during design and reliability studies. To address the above challenges, this dissertation proposes a novel algorithm to predict, over a range of parameters, the qualitative outcomes (pass/fail) of simulations based on relatively few, classified (pass/fail) simulation results. This is different from traditional approximation techniques that seek to predict simulation outcomes quantitatively, for example by fitting a response surface. The predictions of the proposed algorithm are based on the theory of support vector machines (SVM), a machine learning method originated in the field of pattern recognition. This process yields an analytical function that explicitly defines the boundary between feasible and infeasible regions of the parameter space and has the ability to reproduce nonlinear, disjoint boundaries in n dimensions. Since training the SVM only requires classification of training samples as feasible or infeasible, the presence of discontinuities in the simulation results does not affect the proposed algorithm. For the same reason, multiple failure modes such as aeroelastic stability, maximum stress or geometric constraints, may be represented by a single SVM predictor. Often, multiple models are available to simulate a given design at different levels of fidelity and small improvements in accuracy may increase simulation cost by an order of magnitude. In many cases, a lower-fidelity model will classify a case correctly as feasible or infeasible. Therefore a multi-fidelity algorithm is proposed that takes advantage of lower-fidelity models when appropriate to minimize the overall computational burden of training the SVM. To this end, the algorithm combines the concepts of adaptive sampling and multi-fidelity analysis to iteratively select not only the training samples, but also the appropriate level of fidelity for evaluation. The proposed algorithm, referred to as multi-fidelity explicit design space decomposition (MF-EDSD), is demonstrated on various models of aeroelastic stability to either build the stability boundary and/or to perform design optimization. The aeroelastic models range from linear and nonlinear analytical models to commercial software (ZAERO) and represent divergence, flutter, and limit cycle oscillation instabilities. Additional analytical test problems have the advantage that the accuracy of the SVM predictor and the convergence to optimal designs are more easily verified. On the other hand the more sophisticated models demonstrate the applicability to real aerospace applications where the solutions are not known a priori. In conclusion, the presented MF-EDSD algorithm is well suited for approximating stability boundaries associated with aeroelastic instabilities in high-dimensional parameter spaces. The adaptive selection of training samples and use of multi-fidelity models leads to large reductions of simulation cost without sacrificing accuracy. The SVM representation of the boundary of the feasible design space provides a single differentiable constraint function with negligible evaluation cost, ideal for numerical optimization and reliability quantification",Multi-Fidelity Construction of Explicit Boundaries: Application to Aeroelasticity,,The University of Arizona.,,,core
55628774,2014-03-01T00:00:00,"Education is being revolutionized by the introduction, of mobile technologies in the teaching and learning process. However, studies that focus in the application of mobile technologies to informal
learning environments is scarce and not systematized [1]. This is the reason for conducting a research
project that involved a urban game MobiGeo, designed in to take better advantage of the flexibility and
ubiquity offered by the Mobile Learning (ML) but also taking into account the importance of motivation
and interaction to enhance students learning.
The definition of ML has been a complicated task for researchers, but there are assumptions that can
not be neglected: the mobility, portability and ubiquity [2], these are features that will drive new learning spaces and thus motivate students. This idea is supported by [2] that introduces the concepts
of ""just in time"", ""just enough"" and ""just-for-me"" and [4] that speaks of the triad ""location
independence”, ""independence time"" and ""meaningful content”.
These principles of ""anytime"" and ""anywhere"" consolidated by mobile technologies came to renew the
variety of educational activities available to teachers and in this context arises the concept of mobile
location-based games. According to [5] ""these games are played in physical space, but at the same
time, they are supported by actions and events in an interconnected virtual space"", which can be
classified into three categories: ludic, pedagogic and hybrid. By being in direct contact with the
contents to assimilate and move in a real context, students will have a more significant learning [6]
and this will result in the mobilization of knowledge in different contexts. To make the connection
between the physical and the virtual world, our research has made use of Qr codes as these devices provide information in real time and in a dynamically way.
For this research was idealized an urban game called “MobiGeo”, that respect the principles
suggested by [7] and that has as common thread the history of the European Union. To measure
results the researchers developed a questionnaire that was adapted from a proposal of [8] which
created a ""Model to evaluate Educational Games”, so our proposal was built taking into account the
motivational model of Kirkpatrick (level1) and encompassing three major dimensions:
Motivation/Interest, Interaction and Perceived Learning. To assess the Motivation/Interest was used
the Model ARCS (Relevance, Confidence and Satisfaction) and items of Fun, Immersion and
Challenge of “Game User Experience”. On the other hand the interaction was evaluated by items of
the Social Interaction dimension of the “Game of User Experience”, the Learning Perceptions were
evaluated by Bloom's Taxonomy (Knowledge category).
In this paper we present the design and implementation of the MobiGeo outdoor learning activity with
a group of 173 students from the 7th grade of a basic school in the north of Portugal. Initial results
show that this urban game with Qr codes was an adequate activity to use in informal learning
environments that could engage students in gaming with high degrees of motivation and interaction in order to solve the tasks presented to them and so consolidate and acquired new knowledge about the European Union.CIEC – Research Centre on Child Studies, UM (FCT R&D 317",The implementation of mobile location based-games and Qr codes : the case of MobiGeo,https://core.ac.uk/download/55628774.pdf,"'Associated Management Consultants,  PVT., Ltd.'","[{'title': None, 'identifiers': ['2340-1079', 'issn:2340-1079']}]",,core
43350722,2014-06-01T00:00:00,"Background: Pleural manometry can predict the presence of trapped lung and guide large-volume thoracentesis. The current technique for pleural manometry transduces pressure from the needle or intercostal catheter, necessitating intermittent cessation of fluid drainage at the time of pressure recordings. Objectives: To develop and validate a technique for performing continuous pleural manometry, where pressure is transduced from an epidural catheter that is passed through the drainage tube to sit within the pleural space. Methods: Pleural manometry was performed on 10 patients undergoing thoracentesis of at least 500 ml, using the traditional intermittent and new continuous technique simultaneously, and pleural pressures were recorded after each drainage of 100 ml. The pleural elastance (PEL) curves and their 95% confidence intervals (CIs), derived using measurements from each technique, were compared using the analysis of covariance and Student's paired t test, respectively. Results: There was no significant difference in PEL calculated using each method (p > 0.1); however, there was a trend towards the CI for the PEL derived from the continuous method being narrower (p = 0.08). Fully automated measurement of drainage volume and pleural pressure, with real-time calculation and display of PEL, was achieved by connecting the system to a urodynamics machine. Conclusions: Pleural manometry can be transduced from an epidural catheter passed through the drainage tube into the pleural space, which gives continuous recording of the pleural pressure throughout the procedure. This allows for automated calculation and display of the pleural pressure and PEL in real time, if the system is connected to a computer with appropriate software. (C) 2014 S. Karger AG, Base",A new method for performing continuous manometry during pleural effusion drainage,,'S. Karger AG',,10.1159/000358842,core
344307851,2014-09-19T00:00:00,"In this manuscript, I describe my research work concerning signal and image processing, and instrumentation for astronomy. I also present the projectsI intend to develop in the next few years. The thematic areas I dealt with were numerous and diverse, and the applications concerned, among others,visual and spectroscopic double stars, tidal effects in Am-type stars, shell galaxies, atmospheric turbulence and wind profiles with the SCIDAR technique.Among the main astrophysical results to which I contributed, are the validation of merging models to explain the origin of shells around elliptical galaxies,the evidence of the presence of dark matter in those galaxies, the detailed study of many spectroscopic binaries with the constitution of representative samples, including long period orbit systems containing FGKM, Am or composite spectrum stars. In particular, a thorough study of a sample of a hundred Am-type stars highlighted the influence of tidal effects, which contributed to circularizing the orbits and to synchronizing the spin and orbital motion in many systems. The fractional critical radii for circularisation and synchronisation that we have determined for our sample were found to be compatible with Zahn's theoretical models. We have also compared the estimated ages of our systems with the theoretical circularisation and synchronisation characteristic times, also predicted by Zahn. We found a fair agreement for the synchronism, but for circularisation, the tidal effects we have observed are more important than what was expected by the theory, for Am-type stars with a radiative envelope.As regards the visual binary stars, in collaboration with a group of European researchers, we have obtained a few thousand measurementswith the PISCO and PISCO2 instruments, by using the speckle interferometry technique. Those accurate measurements have been calibratedin an absolute way with a diffraction grating. Their spatial resolution often reached the diffraction limit of the telescope. They have already led to the revision of a few hundred orbits. As their orbital periods are very long, and generally exceed a few hundred years, the observation of the visual double stars is a substantial program, which requires reliability and regularity over a very long period of time. This program needs an international collaboration that has been already active since several generations. As soon as they have been published, our measures were incorporated into the double star data base of the US. Navy in Washington DC, which is open to the whole scientific community.With regard to instrumentation, my activities mainly concerned the designing, construction and operation of the PISCO and PISCO2 focal instruments.A substantial part was devoted to real-time software developments for the remote control of the instruments and the detectors, and the subsequent data processing. The main selected criteria were ``performance and reliability'', both for hardware and software developments. Indeed those instruments and programs have been used during nearly every clear night, for many years (more than 20 years for PISCO, and already five years for PISCO2).The processing of PISCO data also lead me to developing programs for atmospheric turbulence profile inversion from SCIDAR observations, that can be operated in real-time and in a non-supervised mode. Those programs allow the restoration of turbulence profiles ($C_N^2$ parameter,seeing, coherence radius and coherence time) and of the wind parameters (velocity and direction) in the turbulent layers. When the observations are obtained in ``generalized SCIDAR'' mode, we can determine those parameters in all the turbulent layers that have been crossed by the incoming light,from the inside of the dome to altitudes up to 20--23 km. I have also written specialized software for computing orbits from radial velocities formultiple stellar systems made of two or three components. The main difficulty of this problem rested in the irregular sampling of the measurements.Finally, over the last few years, I have been working on the problems encountered for the phase self-calibration for aperture synthesis inastronomy, and for the calibration of GNSS (Global Navigation Satellite System) networks in geodesy, to which my principal collaborator, A.~Lannes (L2S, Paris), has proposed an elegant and original solution.For the coming years, I would like to work in collaboration with colleagues, but also if possible with some students (graduate trainees or PhD students) on the following projects:- ""Instrumentation'' projects concerning (i) transfer of PISCO to the new Epsilon 1 m telescope of OCA (Obs. Côte d'Azur), with the automation of the data acquisition and processing, (ii) automation of PISCO2 on the L76 refractor of OCA, and (iii) software development for the ``Shack-Hartmann'' modeof PISCO.- ""Signal processing"" projects about (i) the implementation of the calibration methods proposed by A.~Lannes for radio-astronomy (ALMA) and for GNSS networks, and (ii) non-supervised classification of X sources that have been detected by the XMM-Newton space telescope.- ""Astrophysical projects'' concerning the observation of double stars belonging to the Pre-Main Sequence (PMS) of the HR diagram, and of red dwarfs of the solar neighbourhood, with PISCO and PISCO2, to determine the orbits and the masses of those stars, which are still poorly known.Dans ce manuscrit, je décris mes activités de recherche en traitement du signal, des images et en l'instrumentation pour l'astronomie et je présente les projets que je compte développer dans les prochaines années. Les thématiques que j'ai abordées sont très variées, et les applications ont concerné à la fois les étoiles doubles visuelles et spectroscopiques, les galaxies elliptiques à coquilles et la turbulence atmosphérique et de profils de vitesse du ventpar la technique SCIDAR.Parmi les principaux résultats astrophysiques auxquels j'ai contribué, on peut mentionner la validation des modèles de fusion pourexpliquer l'origine des coquilles, la mise en évidence de matière noire dans les galaxies elliptiques, la constitution d'échantillons représentatifset l'étude détaillée de nombreuses binaires spectroscopiques, dont certaines à longue période, de type FGKM, Am et d'étoiles à spectre composite. En particulier une étude approfondie d'un échantillon d'une centaine d'étoiles Am a permis de mettre en évidence des effets de marées, semanifestant par une circularisation des orbites et une synchronisation entre la rotation axiale des étoiles et leur mouvement orbital. Notreétude a conduit à préciser la valeur de paramètres importants pour la modélisation (rayons fractionnaires critiques) et nousavons pu comparer les temps caractéristiques  théoriques de circularisation et synchronisation avec les âges estimés pour cessystèmes. L'accord est convenable pour le synchronisme mais moins bon pour la circularisation. Les effets de marée observés semblentplus importants que ceux prévus par la théorie, pour les étoiles de type Am, à enveloppe radiative.En ce qui concerne les étoiles doubles visuelles, en collaboration avec un groupe de chercheurs européens, nous avons obtenu plusieurs milliersde mesures avec PISCO et PISCO2, en utilisant des techniques d'interférométrie des tavelures. Ces mesures très précises ont été calibrées de façonabsolue avec un réseau de diffraction, et ont une résolution atteignant la limite de diffraction du télescope. Elles ont déjà conduit à la révision de quelques centaines d'orbites. Les périodes orbitales étant très longues, souvent supérieures à des centaines d'années, l'observation des étoiles doubles est un programme de fond, qui nécessite régularité et durée dans le temps. Ce programme ne peut se faire que dans le cadre d'une collaboration internationale et inter-générationnelle. Au fur et à mesure des publications, nos mesures ont été intégrées dans la base de donnéesspécialisée de l'observatoire de l'US. Navy de Washington (USA), qui est ouverte à toute la communauté.Sur le plan instrumental mes activités ont concerné principalement la conception, la réalisation et le suivi de l'exploitation d'instruments comme PISCO et PISCO2, avec une part importante consacrée au développement de logiciels temps réel pour le contrôle des instruments, des détecteurs et letraitement des données. Les principaux critères que nous avons retenus ont été ``performance et fiabilité'', puisque ces instruments et logiciels sont utilisés, pratiquement toutes les nuits de beau temps, depuis de nombreuses années (plus de 20 ans pour PISCO).Le traitement des données de PISCO m'a aussi conduit à développer des programmes d'inversion de profils de turbulence atmosphérique à partird'observation SCIDAR, qui peuvent fonctionner en mode non supervisé et en temps réel. Ils permettent de restaurer des profils de la turbulence (paramètre $C_N^2$, seeing, rayon et temps de cohérence) et des paramètres du vent en altitude (vitesse et direction) dans les couches turbulentes.Lorsque les observations ont été obtenues en mode ``SCIDAR généralisé'', ils permet de caractériser la turbulence dans toutes les couches traversées,depuis l'intérieur de la coupole jusqu'à des altitudes de 20 à 23 km. J'ai aussi écrit des programmes de calcul d'orbites à partir de mesures de vitesses radiales pour des systèmes à deux ou trois composantes. La difficulté de ce problème résidait principalement dans l'irrégularité de l'échantillonnage des mesures. Enfin depuis quelques années, je travaille sur des problèmes d'auto-calibration de phase pour la synthèse d'ouverture en astronomie,et de calibration des réseaux GNSS (Global Navigation Satellite System) en géodésie, auxquels mon principal collaborateur, A. Lannes (L2S, Paris), a apporté une solution élégante et originale.Pour les prochaines années, je voudrais travailler en collaboration avec des collègues, mais aussi si possible avec la participation d'étudiants (stages niveau ""master'' ou ""thèse'') sur les projets suivants:- projets ""Instrumentaux'' concernant (i) le transfert de PISCO sur le nouveau télescope Epsilon de 1 m de l'OCA (Obs. Côte d'Azur), avec une automatisation de l'acquisition et du traitement des données. (ii) l'automatisation des fonctions de PISCO2 sur la lunette L76 de Nice,et (iii) le développement d'un logiciel de traitement pour le mode ``Shack-Hartmann'' de PISCO.- projets ""Traitement du signal"" portant sur (i) la mise en oeuvre des méthodes de calibration proposées par A. Lannes pour la radio-astronomie et pour les réseaux GNSS et (ii) sur  la classification non supervisée de sources détectées par le télescope spatial XMM-Newton.- projets ""Astrophysique'' concernant l'observation d'étoiles doubles pré-séquence principale du diagramme HR et des naines rouges du voisinage solaire avec PISCO et PISCO2, pour déterminer les orbites et les masses de ces étoiles, encore mal connues","Contribution en Signal, Image et Instrumentation pour l'Astronomie",,HAL CCSD,,,core
299803410,2014-09-19T00:00:00,"In this manuscript, I describe my research work concerning signal and image processing, and instrumentation for astronomy. I also present the projectsI intend to develop in the next few years. The thematic areas I dealt with were numerous and diverse, and the applications concerned, among others,visual and spectroscopic double stars, tidal effects in Am-type stars, shell galaxies, atmospheric turbulence and wind profiles with the SCIDAR technique.Among the main astrophysical results to which I contributed, are the validation of merging models to explain the origin of shells around elliptical galaxies,the evidence of the presence of dark matter in those galaxies, the detailed study of many spectroscopic binaries with the constitution of representative samples, including long period orbit systems containing FGKM, Am or composite spectrum stars. In particular, a thorough study of a sample of a hundred Am-type stars highlighted the influence of tidal effects, which contributed to circularizing the orbits and to synchronizing the spin and orbital motion in many systems. The fractional critical radii for circularisation and synchronisation that we have determined for our sample were found to be compatible with Zahn's theoretical models. We have also compared the estimated ages of our systems with the theoretical circularisation and synchronisation characteristic times, also predicted by Zahn. We found a fair agreement for the synchronism, but for circularisation, the tidal effects we have observed are more important than what was expected by the theory, for Am-type stars with a radiative envelope.As regards the visual binary stars, in collaboration with a group of European researchers, we have obtained a few thousand measurementswith the PISCO and PISCO2 instruments, by using the speckle interferometry technique. Those accurate measurements have been calibratedin an absolute way with a diffraction grating. Their spatial resolution often reached the diffraction limit of the telescope. They have already led to the revision of a few hundred orbits. As their orbital periods are very long, and generally exceed a few hundred years, the observation of the visual double stars is a substantial program, which requires reliability and regularity over a very long period of time. This program needs an international collaboration that has been already active since several generations. As soon as they have been published, our measures were incorporated into the double star data base of the US. Navy in Washington DC, which is open to the whole scientific community.With regard to instrumentation, my activities mainly concerned the designing, construction and operation of the PISCO and PISCO2 focal instruments.A substantial part was devoted to real-time software developments for the remote control of the instruments and the detectors, and the subsequent data processing. The main selected criteria were ``performance and reliability'', both for hardware and software developments. Indeed those instruments and programs have been used during nearly every clear night, for many years (more than 20 years for PISCO, and already five years for PISCO2).The processing of PISCO data also lead me to developing programs for atmospheric turbulence profile inversion from SCIDAR observations, that can be operated in real-time and in a non-supervised mode. Those programs allow the restoration of turbulence profiles ($C_N^2$ parameter,seeing, coherence radius and coherence time) and of the wind parameters (velocity and direction) in the turbulent layers. When the observations are obtained in ``generalized SCIDAR'' mode, we can determine those parameters in all the turbulent layers that have been crossed by the incoming light,from the inside of the dome to altitudes up to 20--23 km. I have also written specialized software for computing orbits from radial velocities formultiple stellar systems made of two or three components. The main difficulty of this problem rested in the irregular sampling of the measurements.Finally, over the last few years, I have been working on the problems encountered for the phase self-calibration for aperture synthesis inastronomy, and for the calibration of GNSS (Global Navigation Satellite System) networks in geodesy, to which my principal collaborator, A.~Lannes (L2S, Paris), has proposed an elegant and original solution.For the coming years, I would like to work in collaboration with colleagues, but also if possible with some students (graduate trainees or PhD students) on the following projects:- ""Instrumentation'' projects concerning (i) transfer of PISCO to the new Epsilon 1 m telescope of OCA (Obs. Côte d'Azur), with the automation of the data acquisition and processing, (ii) automation of PISCO2 on the L76 refractor of OCA, and (iii) software development for the ``Shack-Hartmann'' modeof PISCO.- ""Signal processing"" projects about (i) the implementation of the calibration methods proposed by A.~Lannes for radio-astronomy (ALMA) and for GNSS networks, and (ii) non-supervised classification of X sources that have been detected by the XMM-Newton space telescope.- ""Astrophysical projects'' concerning the observation of double stars belonging to the Pre-Main Sequence (PMS) of the HR diagram, and of red dwarfs of the solar neighbourhood, with PISCO and PISCO2, to determine the orbits and the masses of those stars, which are still poorly known.Dans ce manuscrit, je décris mes activités de recherche en traitement du signal, des images et en l'instrumentation pour l'astronomie et je présente les projets que je compte développer dans les prochaines années. Les thématiques que j'ai abordées sont très variées, et les applications ont concerné à la fois les étoiles doubles visuelles et spectroscopiques, les galaxies elliptiques à coquilles et la turbulence atmosphérique et de profils de vitesse du ventpar la technique SCIDAR.Parmi les principaux résultats astrophysiques auxquels j'ai contribué, on peut mentionner la validation des modèles de fusion pourexpliquer l'origine des coquilles, la mise en évidence de matière noire dans les galaxies elliptiques, la constitution d'échantillons représentatifset l'étude détaillée de nombreuses binaires spectroscopiques, dont certaines à longue période, de type FGKM, Am et d'étoiles à spectre composite. En particulier une étude approfondie d'un échantillon d'une centaine d'étoiles Am a permis de mettre en évidence des effets de marées, semanifestant par une circularisation des orbites et une synchronisation entre la rotation axiale des étoiles et leur mouvement orbital. Notreétude a conduit à préciser la valeur de paramètres importants pour la modélisation (rayons fractionnaires critiques) et nousavons pu comparer les temps caractéristiques  théoriques de circularisation et synchronisation avec les âges estimés pour cessystèmes. L'accord est convenable pour le synchronisme mais moins bon pour la circularisation. Les effets de marée observés semblentplus importants que ceux prévus par la théorie, pour les étoiles de type Am, à enveloppe radiative.En ce qui concerne les étoiles doubles visuelles, en collaboration avec un groupe de chercheurs européens, nous avons obtenu plusieurs milliersde mesures avec PISCO et PISCO2, en utilisant des techniques d'interférométrie des tavelures. Ces mesures très précises ont été calibrées de façonabsolue avec un réseau de diffraction, et ont une résolution atteignant la limite de diffraction du télescope. Elles ont déjà conduit à la révision de quelques centaines d'orbites. Les périodes orbitales étant très longues, souvent supérieures à des centaines d'années, l'observation des étoiles doubles est un programme de fond, qui nécessite régularité et durée dans le temps. Ce programme ne peut se faire que dans le cadre d'une collaboration internationale et inter-générationnelle. Au fur et à mesure des publications, nos mesures ont été intégrées dans la base de donnéesspécialisée de l'observatoire de l'US. Navy de Washington (USA), qui est ouverte à toute la communauté.Sur le plan instrumental mes activités ont concerné principalement la conception, la réalisation et le suivi de l'exploitation d'instruments comme PISCO et PISCO2, avec une part importante consacrée au développement de logiciels temps réel pour le contrôle des instruments, des détecteurs et letraitement des données. Les principaux critères que nous avons retenus ont été ``performance et fiabilité'', puisque ces instruments et logiciels sont utilisés, pratiquement toutes les nuits de beau temps, depuis de nombreuses années (plus de 20 ans pour PISCO).Le traitement des données de PISCO m'a aussi conduit à développer des programmes d'inversion de profils de turbulence atmosphérique à partird'observation SCIDAR, qui peuvent fonctionner en mode non supervisé et en temps réel. Ils permettent de restaurer des profils de la turbulence (paramètre $C_N^2$, seeing, rayon et temps de cohérence) et des paramètres du vent en altitude (vitesse et direction) dans les couches turbulentes.Lorsque les observations ont été obtenues en mode ``SCIDAR généralisé'', ils permet de caractériser la turbulence dans toutes les couches traversées,depuis l'intérieur de la coupole jusqu'à des altitudes de 20 à 23 km. J'ai aussi écrit des programmes de calcul d'orbites à partir de mesures de vitesses radiales pour des systèmes à deux ou trois composantes. La difficulté de ce problème résidait principalement dans l'irrégularité de l'échantillonnage des mesures. Enfin depuis quelques années, je travaille sur des problèmes d'auto-calibration de phase pour la synthèse d'ouverture en astronomie,et de calibration des réseaux GNSS (Global Navigation Satellite System) en géodésie, auxquels mon principal collaborateur, A. Lannes (L2S, Paris), a apporté une solution élégante et originale.Pour les prochaines années, je voudrais travailler en collaboration avec des collègues, mais aussi si possible avec la participation d'étudiants (stages niveau ""master'' ou ""thèse'') sur les projets suivants:- projets ""Instrumentaux'' concernant (i) le transfert de PISCO sur le nouveau télescope Epsilon de 1 m de l'OCA (Obs. Côte d'Azur), avec une automatisation de l'acquisition et du traitement des données. (ii) l'automatisation des fonctions de PISCO2 sur la lunette L76 de Nice,et (iii) le développement d'un logiciel de traitement pour le mode ``Shack-Hartmann'' de PISCO.- projets ""Traitement du signal"" portant sur (i) la mise en oeuvre des méthodes de calibration proposées par A. Lannes pour la radio-astronomie et pour les réseaux GNSS et (ii) sur  la classification non supervisée de sources détectées par le télescope spatial XMM-Newton.- projets ""Astrophysique'' concernant l'observation d'étoiles doubles pré-séquence principale du diagramme HR et des naines rouges du voisinage solaire avec PISCO et PISCO2, pour déterminer les orbites et les masses de ces étoiles, encore mal connues","Contribution en Signal, Image et Instrumentation pour l'Astronomie",,HAL CCSD,,,core
236258092,2013-01-01T08:00:00,"Life long learning is a machine learning technique that deals with learning sequential tasks over time. It seeks to transfer knowledge from previous learning tasks to new learning tasks in order to increase generalization performance and learning speed. Real-time learning environments in which many agents are participating may provide learning opportunities but they are spread out in time and space outside of the geographical scope of a single learning agent. This research seeks to provide an algorithm and framework for life long learning among a network of agents in a sparse real-time learning environment. This work will utilize the robust knowledge representation of neural networks, and make use of both functional and representational knowledge transfer to accomplish this task. A new generative life long learning algorithm utilizing cascade correlation and reverberating pseudo-rehearsal and incorporating a method for merging divergent life long learning paths will be implemented",Life Long Learning In Sparse Learning Environments,https://core.ac.uk/download/236258092.pdf,'Information Bulletin on Variable Stars (IBVS)',,,core
29479131,2014-10-21T00:00:00,"We present a new similarity measure based on information theoretic measures
which is superior than Normalized Compression Distance for clustering problems
and inherits the useful properties of conditional Kolmogorov complexity. We
show that Normalized Compression Dictionary Size and Normalized Compression
Dictionary Entropy are computationally more efficient, as the need to perform
the compression itself is eliminated. Also they scale linearly with exponential
vector size growth and are content independent. We show that normalized
compression dictionary distance is compressor independent, if limited to
lossless compressors, which gives space for optimizations and implementation
speed improvement for real-time and big data applications. The introduced
measure is applicable for machine learning tasks of parameter-free unsupervised
clustering, supervised learning such as classification and regression, feature
selection, and is applicable for big data problems with order of magnitude
speed increase.Comment: 2014 Conference on Big Data from Space (BiDS 14","Generalized Compression Dictionary Distance as Universal Similarity
  Measure",http://arxiv.org/abs/1410.5792,,,,core
302854302,2013-01-01T00:00:00,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.  While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.  The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg-Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg-Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg-Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.  Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H2O, O3, HNO3, CH4, N2O, NO2, CFC-11, CFC-12, N2O5 and ClONO2. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements. © Author(s) 2013",Ten years of MIPAS measurements with ESA Level 2 processor V6-Part 1: Retrieval algorithm and diagnostics of the products,,'Copernicus GmbH',"[{'title': None, 'identifiers': ['1867-1381', 'issn:1867-1381']}]",10.5194/amt-6-2419-2013,core
56611312,2014,"Artículo de publicación ISIThe capability to generate complex geometry features
at tight tolerances and fine surface roughness is a key
element in implementation of Creep Feed grinding process in
specialist applications such as the aerospace manufacturing
environment. Based on the analysis of 3D cutting forces,
this paper proposes a novel method of predicting the profile
deviations of tight geometrical features generated using
Creep Feed grinding. In this application, there are several
grinding passes made at varying depths providing an incremental
geometrical change with the last cut generating the
final complex feature. With repeatable results from coordinate
measurements, both the radial and tangential forces can
be gauged versus the accuracy of the ground features. The
results of the tangential force were found more sensitive to
the deviation of actual cut depth from the theoretical one.
However, to make a more robust prediction on the profile
deviation, its values were considered as a function of both
force components. In addition, the power signals were obtained
as these signals are also proportional to force and
deviation measurements. Genetic programming (GP), an
evolutionary programming technique, has been used to
compute the prediction rules of part profile deviations based
on the extracted radial and tangential force and correlated
with the initial “gauging” methodology. It was found that
using this technique, complex rules can be achieved and
used online to dynamically control the geometrical accuracy
of the ground features. The GP complex rules are based on
the correlation between the measured forces and recorded
deviation of the theoretical profile. The mathematical rules
are generated from Darwinian evolutionary strategy which
provides the mapping between different output classes. GP works from crossover recombination of different rules, and
the best individual is evaluated in terms of the given best
fitness value so far which closes on an optimal solution.
Once the best rule has been generated, this can be further
used independently or in combination with other close-tobest
rules to control the evolution of output measures of
machining processes. The best GP terminal sets will be
realised in rule-based embedded coded systems which will
finally be implemented into a real-time Simulink simulation.
This realisation gives a view of how such a control
regime can be utilised within an industrial capacity. Neural
networks were also used for GP rule verification.The experimental work was carried out at The University of
Nottingham funded by EPSRC",The prediction of profile deviations when Creep Feed grinding complex geometrical features by use of neural networks and genetic programming with real-time simulation,,Springer-Verlag,,10.1007/s00170-014-5829-0,core
235648223,2013-06-26T00:00:00,"In healthcare organization, the primary objective of rostering is to plan and deploy the available

skills and resources to best meet patient needs. This must be accomplishes with the aim to deliver high levels of staff

satisfaction and worklife balance. The principle focus is to

study the potential of Information Technology generally and

Artificial Intelligence specifically to find the best solution for two specific problems in nurse rostering: shift design and satisfaction constraints problem. The challenges of nurse rostering problem are hard to modulate, heavily constrained, large search space and a large number of possible solution. Nurse rostering problem in real world is categorised as NP-hard. In this paper there have three parts.

First section, a brief review about nurse  demands and nurse

rostering formulation. Second section, the recent related

works in nurse rostering search technique in order to  find a better exploitation and exploration of the search space nurse rostering problem and last section conclusion remarks from this investigation","An Investigation of Intelligent Search Techniques For Nurse Scheduling

Improvement in Healthcare Organization",,,,,core
322723744,2013-09-23T00:00:00,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.

While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.

The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg–Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg–Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg–Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.

Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H[subscript: 2]O, O[subscript: 3], HNO[subscript: 3], CH[subscript: 4], N[subscript: 2]O, NO[subscript: 2], CFC-11, CFC-12, N[subscript: 2]O[subscript: 5] and ClONO[subscript: 2]. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements",Ten years of MIPAS measurements with ESA Level 2 processor V6-Part 1: Retrieval algorithm and diagnostics of the products,http://hdl.handle.net/2381/32625,,,,core
79144566,2013-01-01T00:00:00,"A plant disease model is a simplification of a real pathosystem (i.e., the relationships between a pathogen, a host plant, and the environment) that determine whether and how an epidemic develops over time and / or space. Different approaches have been used for the development of plant disease models, with relevant improvements in recent years. 
Empirical models have been elaborated using data collected under variable field conditions since the second half of the last century. The so called 3 10 rules for predicting first seasonal infection of grape downy mildew is a precursor of this empirical approach for understanding relationships between pathogens, plants and the environment. By using this approach, the model is developed by searching mathematical or statistical relationships between field collected data and these relationships do not necessarily have cause-effect meaning. Lack of knowledge, accuracy and, especially, robustness are the main weaknesses of these models, which impose accurate validation and, usually, proper calibration when these models are used in different environments or under changing climate. Recent methods of data analysis, like for instance neural networks, improve the capability of searching the mathematical structure of the model but they do not overcome the above mentioned weaknesses.
Mechanistic models are a new class of models based on knowledge of biological and epidemiological behaviour of the system under study. These models (also referred to as explanatory, theoretical, or fundamental) explain the pathosystem on the basis of what is known about how the system works in relation to the influencing variables. Mechanistic models are dynamic, because they analyse the changes over time of the components of an epidemic due to the external, influencing variables. Dynamic modelling is based on the assumption that the state of the pathosystem in every moment can be quantitatively characterised and that changes in the system can be described by mathematical equations. These models overcome the weakness of the empirical models. Compared to the 3 10 rule, a mechanistic model for grape downy mildew increased the overall accuracy of the predictions from ~60% to ~90%.
Complexity of mechanistic models has been regarded as a problem for the implementation of models in practical disease control, compared to the simplicity of the empirical models. This is a false problem, because confusing complexity of the mathematical framework of the model with complexity of model output is misleading. Indeed, it is possible to use complex models, able to depict the complexity of the biological systems, to produce simply, easy-to-use output for growers. Implementation of the above mentioned mechanistic model for grape downy mildew in a Decision Support System used by viticulturists clearly demonstrates the inconsistency of the \u201ccomplexity paradigm\u201d",Plant disease models: from field observations to biological mechanisms,,place:Riva del Garda,,,core
33116415,2014-10-01T00:00:00,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments",Automated Real-Time Classification and Decision Making in Massive Data Streams from Synoptic Sky Surveys,https://core.ac.uk/download/33116415.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/escience.2014.7,core
100051638,2014-11-28,"Mobile learning or m-learning (ML) is a new form of e-learning where students use wireless networks to exchange data (questions, notes, courses, demos …) with their teachers. It is accomplished with the use of mobile devices such as palmtops, PDA, smart phones, tablet PCs or any other handheld devices. The main contribution of ML is the large mobility it offers to the different actors of the training. We believe that the comprehension of this mobility is crucial in order to better design, deploy, and manage future wireless networks in a training context. In this article we propose an approach to more understand and analyze learners &apos; behavior in wireless environments. Our idea is to integrate and visualize statically (e.g. infrastructure, Access Points position) and dynamically (e.g. users &apos; itineraries, accessed data) extracted data in unique dashboard interface. This approach makes it possible to track real learners by providing, in addition to their graph of mobility, various views on their space-time behaviors. Moreover, it allows the comparison of behaviors evolution over several periods of time",TOWARD A MODELIZATION OF MOBILE LEARNERS BEHAVIOR FOR THE DESIGN AND THE EVALUATION OF ADVANCED TRAINING SYSTEMS,,,,,core
275610847,2014-08-01T00:00:00,"Colloidal or heterogeneous nanocatalysts can improve the range and diversity of Cu(I)catalysed click reactions and facilitate catalyst separation and reuse. Catalysis by metal nanoparticles raises the question as to whether heterogeneous catalysts may cause homogeneous catalysis through metal ion leaching, since the catalytic process could be mediated by the particle, or by metal ions released from it. The question is critical as unwanted homogeneous processes could offset the benefits of heterogeneous catalysis. Here, we combine standard bench scale techniques with single-molecule spectroscopy to monitor single catalytic events in real time and demonstrate that click catalysis occurs directly at the surface of copper nanoparticles; this general approach could be implemented in other systems. We use 'from the mole to the molecule' to describe this emerging idea in which mole scale reactions can be optimized through an intimate understanding of the catalytic process at the single-molecule-single catalytic nanoparticle level.The Natural Sciences and Engineering Research Council of Canada supported this work through its Discovery and CREATE programs, while the Canadian Foundation for Innovation enabled the purchase of the instrumentation used in this work. M. L. M. thanks the Spanish Ministerio de Educacion, Cultura y Deporte (Programa Salvador de Madariaga) for its financial support.Decan, MR.; Impellizzeri, S.; Marín García, ML.; Scaiano, J. (2014). Copper nanoparticle heterogeneous catalytic 'click' cycloaddition confirmed by single-molecule spectroscopy. Nature Communications. 5:4612-4615. doi:10.1038/ncomms5612S461246155Rostovtsev, V. V., Green, L. G., Fokin, V. V. & Sharpless, K. B. A stepwise Huisgen cycloaddition process: copper(I)-catalyzed regioselective ‘ligation’ of azides and terminal alkynes. Angew. Chem. Int. Ed. 41, 2596–2599 (2002).Tornøe, C. W., Christensen, C. & Meldal, M. Peptidotriazoles on solid phase: [1,2,3]-triazoles by regiospecific copper(I)-catalyzed 1,3-dipolar cycloadditions of terminal alkynes to azides. J. Org. Chem. 67, 3057–3064 (2002).Hein, J. & Fokin, V. Copper-catalyzed azide–alkyne cycloaddition (CuAAC) and beyond: new reactivity of copper(I) acetylides. Chem. Soc. Rev. 39, 1302–1315 (2010).Kolb, H. C., Finn, M. G. & Sharpless, K. B. Click chemistry: diverse chemical function from a few good reactions. Angew. Chem. Int. Ed. 40, 2004 (2001).Meldal, M. & Tornoe, C. W. Cu-catalyzed azide-alkyne cycloaddition. Chem. Rev. 108, 2952–3015 (2008).Moses, J. E. & Moorhouse, A. D. The growing applications of click chemistry. Chem. Soc. Rev. 36, 1249–1262 (2007).Thirumurugan, P., Matosiuk, D. & Jozwiak, K. Click chemistry for drug development and diverse chemical—biology applications. Chem. Rev. 113, 4905–4979 (2013).Cintas, P., Barge, A., Tagliapietra, S., Boffa, L. & Cravotto, G. Alkyne–azide click reaction catalyzed by metallic copper under ultrasound. Nat. Protoc. 5, 607–616 (2010).Hong, V., Presolski, S., Ma, C. & Finn, M. Analysis and optimization of copper-catalyzed azide-alkyne cycloaddition for bioconjugation. Angew. Chem. Int. Ed. 48, 9879–9883 (2009).Kappe, C. & Van Der Eycken, E. Click chemistry under non-classical reaction conditions. Chem. Soc. Rev. 39, 1280–1290 (2010).Pachón, L., Van Maarseveen, J. & Rothenberg, G. Click chemistry: copper clusters catalyse the cycloaddition of azides with terminal alkynes. Adv. Synth. Catal. 347, 811–815 (2005).Adzima, B. et al. Spatial and temporal control of the alkyne–azide cycloaddition by photoinitiated Cu(II) reduction. Nat. Chem. 3, 258–261 (2011).Jin, T., Yan, M. & Yamamoto, Y. Click chemistry of alkyne-azide cycloaddition using nanostructured copper catalysts. ChemCatChem 4, 1217–1229 (2012).Woo, H. et al. Azide-alkyne Huisgen [3+2] cycloaddition using CuO nanoparticles. Molecules 17, 13235–13252 (2012).Rance, G., Solomonsz, W. & Khlobystov, A. Click chemistry in carbon nanoreactors. Chem. Commun. 49, 1067–1069 (2013).Alonso, F., Moglie, Y., Radivoy, G. & Yus, M. Unsupported copper nanoparticles in the 1,3-dipolar cycloaddition of terminal alkynes and azides. Eur. J. Org. Chem. 10, 1875–1884 (2010).Alonso, F., Moglie, Y., Radivoy, G. & Yus, M. Multicomponent synthesis of 1,2,3-triazoles in water catalyzed by copper nanoparticles on activated carbon. Adv. Synth. Catal. 352, 3208–3214 (2010).Raut, D. et al. Copper nanoparticles in ionic liquids: Recyclable and efficient catalytic system for 1,3-dipolar cycloaddition reaction. Catal. Commun. 10, 1240–1243 (2009).Kumar, B. S. P. A., Reddy, K. H. V., Madhav, B., Ramesh, K. & Nageswar, Y. V. D. Magnetically separable CuFe2O4 nano particles catalyzed multicomponent synthesis of 1,4-disubstituted 1,2,3-triazoles in tap water using ‘click chemistry’. Tetrahedron Lett. 53, 4595–4599 (2012).Hudson, R., Li, C. & Moores, A. Magnetic copper–iron nanoparticles as simple heterogeneous catalysts for the azide–alkyne click reaction in water. Green Chem. 14, 622–624 (2012).Sarkar, A., Mukherjee, T. & Kapoor, S. PVP-stabilized copper nanoparticles: a reusable catalyst for ‘click’ reaction between terminal alkynes and azides in nonaqueous solvents. J. Phys. Chem. C 112, 3334–3340 (2008).Pacioni, N. L., Filippenko, V., Presseau, N. & Scaiano, J. C. Oxidation of copper nanoparticles in water: mechanistic insights revealed by oxygen uptake and spectroscopic methods. Dalton Trans. 42, 5832–5838 (2013).Davies, I. W., Matty, L., Hughes, D. L. & Reider, P. J. Are heterogeneous catalysts precursors to homogeneous catalysts? J. Am.Chem. Soc. 123, 10139–10140 (2001).Witham, C. A. et al. Converting homogeneous to heterogeneous in electrophilic catalysis using monodisperse metal nanoparticles. Nat. Chem. 2, 36–41 (2010).Schmidt, A. F. & Kurokhtina, A. A. Distinguishing between the homogeneous and heterogeneous mechanisms of catalysis in the Mizoroki-Heck and Suzuki-Miyaura reactions: problems and prospects. Kinet. Catal. 53, 714–730 (2012).Nishina, Y., Miyata, J., Kawai, R. & Gotoh, K. Recyclable Pd-graphene catalyst: mechanistic insights into heterogeneous and homogeneous catalysis. RSC Advances 2, 9380–9382 (2012).Esfandiari, N. M. & Blum, S. A. Homogeneous vs heterogeneous polymerization catalysis revealed by single-particle fluorescence microscopy. J. Am.Chem. Soc. 133, 18145–18147 (2011).Hensle, E. M. & Blum, S. A. Phase separation polymerization of dicyclopentadiene characterized by in operando fluorescence microscopy. J. Am.Chem. Soc. 135, 12324–12328 (2013).De Cremer, G. et al. High-resolution single-turnover mapping reveals intraparticle diffusion limitation in Ti-MCM-41-catalyzed epoxidation. Angew. Chem. Int. Ed. 49, 908–911 (2010).Roeffaers, M. B. J. et al. Super-resolution reactivity mapping of nanostructured catalyst particles. Angew. Chem. Int. Ed. 48, 9285–9289 (2009).Roeffaers, M. B. J. et al. Spatially resolved observation of crystal-face-dependent catalysis by single turnover counting. Nature 439, 572–575 (2006).Xu, W., Kong, J. S., Yeh, Y.-T.E. & Chen, P. Single-molecule nanocatalysis reveals heterogeneous reaction pathways and catalytic dynamics. Nat. Mater. 7, 992–996 (2008).Zhou, X., Xu, W., Liu, G., Panda, D. & Chen, P. Size-dependent catalytic activity and dynamics of gold nanoparticles at the single-molecule level. J. Am.Chem. Soc. 132, 138–146 (2010).Zhou, X. et al. Quantitative super-resolution imaging uncovers reactivity patterns on single nanocatalysts. Nat. Nanotechnol. 7, 237–241 (2012).Chen, P. et al. Single-molecule fluorescence imaging of nanocatalytic processes. Chem. Soc. Rev. 39, 4560–4570 (2010).Chen, P. et al. Spatiotemporal catalytic dynamics within single nanocatalysts revealed by single-molecule microscopy. Chem. Soc. Rev. 43, 1107–1117 (2014).Buurmans, I. L. C. & Weckhuysen, B. M. Heterogeneities of individual catalyst particles in space and time as monitored by spectroscopy. Nat. Chem. 4, 873–886 (2012).Cordes, T. & Blum, S. A. Opportunities and challenges in single-molecule and single-particle fluorescence microscopy for mechanistic studies of chemical reactions. Nat. Chem. 5, 993–999 (2013).Zhou, X., Choudhary, E., Andoy, N. M., Zou, N. & Chen, P. Scalable parallel screening of catalyst activity at the single-particle level and subdiffraction resolution. ACS Catal. 3, 1448–1453 (2013).Esfandiari, N. M. et al. Single-molecule imaging of platinum ligand exchange reaction reveals reactivity distribution. J. Am. Chem. Soc. 132, 15167–15169 (2010).Wee, T., Schmidt, L. C. & Scaiano, J. C. Photooxidation of 9-anthraldehyde catalyzed by gold nanoparticles: solution and single nanoparticle studies using fluorescence lifetime imaging. J. Phys. Chem. C 116, 24373–24379 (2012).Fiolka, R., Belyaev, Y., Ewers, H. & Stemmer, A. Even illumination in total internal reflection fluorescence microscopy using laser light. Microsc. Res. Tech. 71, 45–50 (2007).Canham, S. M. et al. Toward the single-molecule investigation of organometallic reaction mechanisms: single-molecule imaging of fluorophore-tagged palladium(II) complexes. Organometallics 27, 2172–2175 (2008).Jares-Erijman, E. & Jovin, T. FRET imaging. Nat. Biotechnol. 21, 1387–1395 (2003).Roy, R., Hohng, S. & Ha, T. A practical guide to single-molecule FRET. Nat. Methods 5, 507–516 (2008).Kasper, L. et al. Probing the free-energy surface for protein folding with single-molecule spectroscopy. Nature 419, 743–747 (2002).Chung, H., Louis, J. & Eaton, W. Distinguishing between protein dynamics and dye photophysics in single-molecule FRET experiments. Biophys. J. 98, 696–706 (2010).Hohlbein, J., Craggs, T. & Cordes, T. Alternating-laser excitation: single-molecule FRET and beyond. Chem. Soc. Rev. 43, 1156–1171 (2014).Greenleaf, W. J., Woodside, M. T. & Block, S. M. High-resolution, single-molecule measurements of biomolecular motion. Annu. Rev. Biophys. Biomol. Struct. 36, 171–190 (2007).Di Fiori, N. & Meller, A. The effect of dye-dye interactions on the spatial resolution of single-molecule FRET measurements in nucleic acids. Biophys. J. 98, 2265–2272.Ahlquist, M. r. & Fokin, V. Enhanced reactivity of dinuclear copper(I) acetylides in dipolar cycloadditions. Organometallics 26, 4389–4391 (2007).Straub, B. μ-Acetylide and μ-alkenylidene ligands in ‘click’ triazole syntheses. Chem. Commun. 37, 3868–3870 (2007)",Copper nanoparticle heterogeneous catalytic 'click' cycloaddition confirmed by single-molecule spectroscopy,https://riunet.upv.es/bitstream/10251/61400/2/ncomms5612.pdf,'Springer Science and Business Media LLC',,10.1038/ncomms5612,core
17354658,2013-05-01T00:00:00,"Continuous increases in traffic volume and limited available capacity in the roadway system have created a need for improved traffic control. From traditional pre-timed isolated signals to actuated and coordinated corridors, traffic control for urban networks has evolved into more complex adaptive signal control systems. However, unexpected traffic fluctuations, rapid changes in traffic demands, oversaturation, the occurrence of incidents, and adverse weather conditions, among others, significantly impact the traffic network operation in ways that current control systems cannot always cope with.
On the other hand, strategies for traffic control based on developments from the field of machine learning can provide promising alternative solutions, particularly those that make use of unsupervised learning such as reinforcement learning (RL) - also referred as approximate dynamic programming (ADP) in some research communities. For the traffic control problem, examples of convenient RL algorithms are the off-policy Q-learning and the ADP using a post decision state variable, since they address processes with sequential decision making, do not need to compute transition probabilities, and are well suited for high dimensional spaces. 
A series of benefits are expected from these algorithms in the traffic control domain: 1) no need of prediction models to transition traffic over time and estimate the best actions; 2) availability of cost-to-go estimates at any time (appropriate for real-time applications); 3) self-evolving policies; and 4) flexibility to make use of new sources of information part of emergent Intelligent Transportation Systems (ITS) such as mobile vehicle detectors (Bluetooth and GPS vehicle locators).   
Given the potential benefits of these strategies, this research proposes MASTraf: a decentralized Multi-Agent System for network-wide Traffic signal control with dynamic coordination. MASTraf is designed to capture the behavior of the environment and take decisions based on situations directly observed by RL agents. Also, agents can communicate with each other, exploring the effects of temporary coalitions or subgroups of intersections as a mechanism for coordination. 
Separate MASTraf implementations with similar state and reward functions using Q-learning and ADP were tested using a microscopic traffic simulator (VISSIM) and real-time manipulation of the traffic signals through the software’s COM interface. Testing was conducted to determine the performance of the agents in scenarios with increasing complexity, from a single intersection, to arterials and networks, both in undersaturated and oversaturated conditions. 
Results show that the multi-agent system provided by MASTraf improves its performance as the agents accumulate experience, and the system was able to efficiently manage the traffic signals of simple and complex scenarios. Exploration of the policies generated by MASTraf showed that the agents followed expected behavior by providing green to greater vehicle demands and accounting for the effects of blockages and lost time. The performance of MASTraf was on par with current state of practice tools for finding signal control settings, but MASTraf can also adapt to changes in demands and driver behavior by adjusting the signal timings in real-time, thus improving coordination and preventing queue spillbacks and green starvation.  
A strategy for signal coordination was also tested in one of the MASTraf implementations, showing increased throughput and reduced number of stops, as expected. The coordination employed a version of the max-plus algorithm embedded in the reward structure, acting as a bias towards improved coordination. The response of the system using imprecise detector data, in the form of coarse aggregation, showed that the system was able to handle oversaturation under such conditions. Even when the data had only 25% of the resolution of the original implementation, the system throughput was only reduced by 5% and the number of stops per vehicle was increased by 8%. 
The state and reward formulations allowed for a simple function approximation method in order to reduce the memory requirements for storing the state space, and also to create a form of generalization for states that have not been visited or that have not been experienced enough. Given the discontinuities in the reward function generated by penalties for blockages and lost times, the value approximation was conducted through a series of functions for each action and each of the conditions before and after a discontinuity. 
The policies generated using MASTraf with a function approximation were analyzed for different intersections in the network, showing agent behavior that reflected the principles formulated in the original problem using lookup tables, including right of way assignment based on expected rewards with consideration of penalties such as lost time. In terms of system performance, MASTraf with function approximation resulted in average reductions of 1% in the total system throughput and 3.6% increases in the number of stops per vehicle, when compared to the implementation using lookup tables on a congested network of 20 intersections",MASTraf: a decentralized multi-agent system for network-wide traffic signal control with dynamic coordination,https://core.ac.uk/download/17354658.pdf,,,,core
29047200,2014-06-01T00:00:00,"Design optimisation of electromagnetic and electromechanical devices is usually aided by numerical simulations, such as the finite element method, which often carry high computational costs, especially if three-dimensional transient modelling is required. Thus in addition to the task of finding the global optimum, while avoiding local minima traps, there is the additional requirement of achieving the final solution efficiently with as few objective function evaluations as possible. With this in mind several surrogate modelling techniques have been developed to replace, under controlled environment, the computationally expensive accurate field modelling by fast approximate substitutes. This thesis looks at a particular technique known as kriging, which in other applications has been demonstrated to provide accurate representations, even of complicated responses, based on a limited set of observations whilst providing an error estimate of the predictions and hence increasing the confidence in the answer. In the iterative optimisation process the critical issue is where to position the next point for evaluation to find a sensible compromise between conflicting goals to explore the search space thoroughly but at the same time exploit information already available. This thesis proposes several novel algorithms based on reinforcement learning theory using the concept of rewards for balancing exploration and exploitation automatically and adaptively. The performance of these algorithms has been assessed comprehensively using carefully selected test functions and real engineering problems (taken from TEAM workshops) and compared with the results published in literature. The kriging approach has generally been found to outperform significantly other available methods. One of the practical limitations, however, was found to be large-scale multi-dimensional or multi-objective tasks because of the need to create special correlation matrices for the kriging predictions to work. Several techniques have been developed and implemented to alleviate such problems and control the memory space occupied by such matrices. Finally, in practical design problems, the issue of robustness of the design has to be considered – related to manufacturing tolerances, material variability, etc – which requires the designer not only to find the theoretical optimum but also assess its quality (sensitivity) within specified uncertainties of variables. Several strategies for evaluation of design robustness assisted by kriging modelling have been developed and implemented in combination with commercial electromagnetic design software",Balancing exploration and exploitation in robust multiobjective electromagnetic design optimisation,,,,,core
346413338,2013-07-16T07:00:00,"Traditionally, animal species diversity and abundance is assessed using a variety of methods that are generally costly, limited in space and time, and most importantly, they rarely include a permanent record. Given the urgency of climate change and the loss of habitat, it is vital that we use new technologies to improve and expand global biodiversity monitoring to thousands of sites around the world. In this article, we describe the acoustical component of the Automated Remote Biodiversity Monitoring Network (ARBIMON), a novel combination of hardware and software for automating data acquisition, data management, and species identification based on audio recordings. The major components of the cyberinfrastructure include: a solar powered remote monitoring station that sends 1-min recordings every 10 min to a base station, which relays the recordings in real-time to the project server, where the recordings are processed and uploaded to the project website (arbimon.net). Along with a module for viewing, listening, and annotating recordings, the website includes a species identification interface to help users create machine learning algorithms to automate species identification. To demonstrate the system we present data on the vocal activity patterns of birds, frogs, insects, and mammals from Puerto Rico and Costa Rica",Real-time bioacoustics monitoring and automated species identification,,WBI Studies Repository,,,core
287791798,2014-09-15T00:00:00,"Computational models of neural networks can be based on a variety of different parameters. These parameters include, for example, the 3d shape of neuron layers, the neurons' spatial projection patterns, spiking dynamics and neurotransmitter systems. While many well-developed approaches are available to model, for example, the spiking dynamics, there is a lack of approaches for modeling the anatomical layout of neurons and their projections. We present a new method, called Parametric Anatomical Modeling (PAM), to fill this gap. PAM can be used to derive network connectivities and conduction delays from anatomical data, such as the position and shape of the neuronal layers and the dendritic and axonal projection patterns. Within the PAM framework, several mapping techniques between layers can account for a large variety of connection properties between pre- and post-synaptic neuron layers. PAM is implemented as a Python tool and integrated in the 3d modeling software Blender. We demonstrate on a 3d model of the hippocampal formation how PAM can help reveal complex properties of the synaptic connectivity and conduction delays, properties that might be relevant to uncover the function of the hippocampus. Based on these analyses, two experimentally testable predictions arose: (i) the number of neurons and the spread of connections is heterogeneously distributed across the main anatomical axes, (ii) the distribution of connection lengths in CA3-CA1 differ qualitatively from those between DG-CA3 and CA3-CA3. Models created by PAM can also serve as an educational tool to visualize the 3d connectivity of brain regions. The low-dimensional, but yet biologically plausible, parameter space renders PAM suitable to analyse allometric and evolutionary factors in networks and to model the complexity of real networks with comparatively little effort",Parametric anatomical modeling,,,,,core
78067765,2014-11-01T00:00:00,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].Hertz FoundationMIT Energy InitiativeMassachusetts Institute of Technology. Computer Science and Artificial Intelligence LaboratoryUnited States. Defense Advanced Research Projects Agency (Robotics Challenge",Footstep planning on uneven terrain with mixed-integer convex optimization,,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/HUMANOIDS.2014.7041373,core
22952652,2013-08-18,"This paper introduces Tabu Search in analytical chemistry by applying it to wavelength selection. Tabu Search is a deterministic global optimization technique loosely based on concepts from artificial intelligence. Wavelength selection is a method which can be used for improving the quality of calibration models. Tabu Search uses basic, problem-specific operators to explore a search space, and memory to keep track of parts already visited. Several implementational aspects of wavelength selection with Tabu Search will be discussed. Two ways of memorizing the search space are investigated: storing the actual solutions and storing the steps necessary to create them. Parameters associated with Tabu Search are configured with a Plackett–Burman design. In addition, two extension schemes for Tabu Search, intensification and diversification, have been implemented and are applied with good results. Eventually, two implementations of wavelength selection with Tabu Search are tested, one which searches for a solution with a constant number of wavelengths and one with a variable number of wavelengths. Both implementations are compared with results obtained by wavelength selection methods based on simulated annealing (SA) and genetic algorithms (GAs). It is demonstrated with three real-world data sets that Tabu Search performs equally well as and can be a valuable alternative to SA and GAs. The improvements in predictive abilities increased by a factor of 20 for data set 1 and by a factor of 2 for data sets 2 and 3. In addition, when the number of wavelengths in a solution is variable, measurements on the coverage of the search space show that the coverage is usually higher for Tabu Search compared with SA and GAs",Published online in Wiley InterScience (www.interscience.wiley.com). DOI: 10.1002/cem.782 Wavelength selection with Tabu Search,,,,,core
52437422,2014-07-01T00:00:00,"International audienceIn brain imaging, solving learning problems in multi-subjects settings is difficult because of the differences that exist across individuals. Here we introduce a novel classification framework based on group-invariant graphical representations, allowing to overcome the inter-subject variability present in functional magnetic resonance imaging (fMRI) data and to perform multivariate pattern analysis across subjects. Our contribution is twofold: first, we propose an unsupervised representation learning scheme that encodes all relevant characteristics of distributed fMRI patterns into attributed graphs; second, we introduce a custom-designed graph kernel that exploits all these characteristics and makes it possible to perform supervised learning (here, classification) directly in graph space. The well-foundedness of our technique and the robustness of the performance to the parameter setting are demonstrated through inter-subject classification experiments conducted on both artificial data and a real fMRI experiment aimed at characterizing local cortical representations. Our results show that our framework produces accurate inter-subject predictions and that it outperforms a wide range of state-of-the-art vector- and parcel-based classification methods. Moreover, the genericity of our method makes it is easily adaptable to a wide range of potential applications. The dataset used in this study and an implementation of our framework are available at http://dx.doi.org/10.6084/m9.figshare.1086317",Graph-based inter-subject pattern analysis of fMRI data,,'Public Library of Science (PLoS)',,10.1371/journal.pone.0104586,core
48090360,2013-01-01T00:00:00,"With the unprecedented fast growth of data, we have better opportunities to understand our complex world, and simultaneously face pervasive challenges in efficiently inferring the meaning behind these vast amounts of data. It is particularly important to explore the intrinsic structures in data to increase our rational understanding of the latent mechanisms that generate them. In modeling, structures are features used to characterize the underlying systems, such as the rank of a system, the number of clusters, the levels of hierarchy, and the order of spatio-temporal correlations in multiple measurements.  In this thesis, we present our research contributions on utilizing structures and sparsity in observed data to improve estimation and prediction of trajectories of system states for two systems: the highway traffic system and the human physiology systems. Both systems exhibit features that are seen in many other applications.  For the traffic problem, it is useful to know the near--term traffic conditions after the occurrence of some events which have noticeable impact on the road traffic. Often used macroscopic models, which view road traffic as fluid flowing in pipes, suffer from various inaccuracies, which could be mitigated by incorporating past observations to correct  predictions. However, we often have limited observation and computing resources (e.g. probe vehicles, smartphones, bandwidth, sensors) to gather and process past observations. We describe a novel low-overhead strategy to adaptively select observation sites in real-time by using the density of the mesh of the numerical solution of  the underlying mathematical model to capture the variability of that solution. We show that our proposed strategy improves the numerical accuracy of near--term traffic forecasting with limited observation resources as compared with with uniform deployment of the observation resources. In addition to deploying limited observation resources, one  is often concerned with detecting special traffic events. To this end, we propose a novel method to decompose traffic observations into normal background and sparse events. Our method couples multiple traffic datastreams so that they share a certain sparse spatio--temporal structure.  We also study the utility of sparseness and structure in physiological datastreams. Missing values hinder the use of many machine learning methods. We show how to incorporate ideas from compressive sensing into handling the missing values problem in continuous intracranial pressure (ICP) datastreams from patients with traumatic brain injury. We  experimentally evaluate the proposed method in experiments where randomly selected ICP values are marked as missing. We find our method gives estimated missing values that are in better agreement with the true values as compared with k--nearest neighbor and expectation maximization data imputation methods.  Moreover, predicting the near--term intracranial pressure for traumatic brain injury patients is of great importance to clinicians. Traditional regression methods, need an explicit parametric form of the model to fit. However, due to our limited knowledge of the complex brain physiology, it is difficult to specify an accurate parametric model. To overcome this  difficulty, our model uses Gaussian processes to quantify our prior beliefs on the smoothness of the regression model, and performs regression in an infinite dimensional space. We show that the proposed Gaussian process regression model shows predicts ICP changes in clinically useful timeframes and may support future development of minimally-invasive ICP monitoring systems, earlier intervention strategies, and better patient outcomes",On Prediction and Estimation for Datastreams Utilizing Sparsity and Structure,,"University of Maryland, Baltimore County (UMBC)",,,core
22963097,2013-08-20,"Second Life is a 3D virtual environment that is created by Linden Labs. It is currently continuing to grow and is widely used all over the world. It has already been adopted by organisations such as IBM, Sun Microsystems, ABC, Sony, and many others. The Sony AIBO (short for Artificial Intelligence roBOt) is an artificial intelligence robotic pet dog designed and manufactured by Sony. The AIBO is considered to be an autonomous robot, which is able to gain information about the environment and be able to function without human intervention. The main aim of this project is to use the Second Life online virtual space (virtual world) to simulate and control the movements of the Sony AIBO robot (real world) in a wireless environment using sound software engineering principles. This paper details the design of the teleoperation system and the rationale behind the design as well as proving that the aim of the project can be successfully met. The outcomes of the experiments performed to measure the success of the system are detailed in this paper. These outcomes include how successful the system was in terms of teleoperating the robot using the Second Life virtual world as well as some of the issues faced while performing the experiments and a dicussion of the possible solutions on how to solve the problems. This project could lead to future work in various application domains such as the smart home concept or in the mining industry",Connecting the Real World with the Virtual World,,,,,core
380812268,2014-03-01T00:00:00,"Education is being revolutionized by the introduction, of mobile technologies in the teaching and learning process. However, studies that focus in the application of mobile technologies to informal
learning environments is scarce and not systematized [1]. This is the reason for conducting a research
project that involved a urban game MobiGeo, designed in to take better advantage of the flexibility and
ubiquity offered by the Mobile Learning (ML) but also taking into account the importance of motivation
and interaction to enhance students learning.
The definition of ML has been a complicated task for researchers, but there are assumptions that can
not be neglected: the mobility, portability and ubiquity [2], these are features that will drive new learning spaces and thus motivate students. This idea is supported by [2] that introduces the concepts
of ""just in time"", ""just enough"" and ""just-for-me"" and [4] that speaks of the triad ""location
independence”, ""independence time"" and ""meaningful content”.
These principles of ""anytime"" and ""anywhere"" consolidated by mobile technologies came to renew the
variety of educational activities available to teachers and in this context arises the concept of mobile
location-based games. According to [5] ""these games are played in physical space, but at the same
time, they are supported by actions and events in an interconnected virtual space"", which can be
classified into three categories: ludic, pedagogic and hybrid. By being in direct contact with the
contents to assimilate and move in a real context, students will have a more significant learning [6]
and this will result in the mobilization of knowledge in different contexts. To make the connection
between the physical and the virtual world, our research has made use of Qr codes as these devices provide information in real time and in a dynamically way.
For this research was idealized an urban game called “MobiGeo”, that respect the principles
suggested by [7] and that has as common thread the history of the European Union. To measure
results the researchers developed a questionnaire that was adapted from a proposal of [8] which
created a ""Model to evaluate Educational Games”, so our proposal was built taking into account the
motivational model of Kirkpatrick (level1) and encompassing three major dimensions:
Motivation/Interest, Interaction and Perceived Learning. To assess the Motivation/Interest was used
the Model ARCS (Relevance, Confidence and Satisfaction) and items of Fun, Immersion and
Challenge of “Game User Experience”. On the other hand the interaction was evaluated by items of
the Social Interaction dimension of the “Game of User Experience”, the Learning Perceptions were
evaluated by Bloom's Taxonomy (Knowledge category).
In this paper we present the design and implementation of the MobiGeo outdoor learning activity with
a group of 173 students from the 7th grade of a basic school in the north of Portugal. Initial results
show that this urban game with Qr codes was an adequate activity to use in informal learning
environments that could engage students in gaming with high degrees of motivation and interaction in order to solve the tasks presented to them and so consolidate and acquired new knowledge about the European Union.CIEC – Research Centre on Child Studies, UM (FCT R&D 317",The implementation of mobile location based-games and Qr codes : the case of MobiGeo,,"'Associated Management Consultants,  PVT., Ltd.'","[{'title': None, 'identifiers': ['2340-1079', 'issn:2340-1079']}]",,core
211472495,2013-01-01T00:00:00,"As robots’ working environments become more and more complex, new pattern recognition challenges emerge. Apart from the conventional question ”What is the object?"", nowadays, scholars should answer to other appealing queries such as ""Where is it?"" or ""How to manipulate it?"". State of the art vision methods consist of advanced recognition subroutines that are fed into machine learning algorithms so as to reach assumptions, regarding the current world state, akin to the ones of a human mind. One of the most challenging tasks in machine vision is the estimation of the 3D pose of an object, due to its practical implication and its augmentative importance in scene interpretation processes. A plethora of diverse applications impose upon the accurate assessment of objects’ geometrical configuration, relative to a given coordinate system, in order to achieve their goals. In robotics, autonomous object manipulation can efficiently be accomplished only in cases where the full pose (six Degrees of Freedom (DoF)) of the testing target is known. Despite the affluent research endeavors and the achievements reached so far, an advanced vision system characterized by low complexity and sufficient generalization capabilities has yet to be built. This PhD thesis is inspired by the remarkable skills of humans in the particular task of estimating the relative pose of objects given an initial hypothesis. Regardless the lighting conditions and common visual disturbances, such as partial occlusions, humans excel in interpreting the 3D geometrical configuration of arbitrary placed objects. Among all operations realized by human beings, the majority is directly related to object manipulation. On the other hand, the realization of target manipulation by a robotic arm puts strict constraints regarding the exact speed of execution and the ""nature"" of the testing objects. Contemporary solutions to this problem require high storage capabilities whilst failing to generalize to unknown testing targets, i.e. objects that are not included into the training set. The main objective of this PhD thesis is the development of innovative vision algorithms tackling the 3D object pose estimation problem and that are to be realized on robotic platforms. It is apparent that, the designed architectures should initially, deal with the position estimation problem -i.e. mainly the depth estimation one- and gradually solve for the remainder three orientation DoFs. The relevant literature was surveyed to reveal the current state of the art and to highlight open research issues that are to be addressed. Afterwards, novel manifold modeling approaches, resulting in low dimensional and high discriminative feature vectors, were designed and implemented. Lastly, the developed algorithms were adopted by robotic platforms to carry out autonomous object grasping. In the first research endeavor of this thesis, the construction of a simple and easy-to-build framework for location assignment of an object in a scene, is investigated. The developed method is based on the observation that features extracted from any two-part (detector-descriptor) algorithm correspond to spots on the object’s surface and their center of mass is related to the one of the objects. Thus, by extracting these features at known positions of the sought object, one can estimate its distance from the camera. Comparing to the contemporary solutions for depth estimation, the employed approach is computationally inexpressive whilst requiring only a single sensor opposed to stereo vision architectures. Moreover, a neural network-based framework that is able to calculate the three rotational DoFs of any object is presented. The network is trained with numerous targets contained in several available datasets with the training process been guided by a fuzzy extraction of the centers of the abstracted features through the Fuzzy c-means algorithm. The 3D pose of an object in a training instance is considered as the distances of the fuzzy centers from one particular cluster. The proposed method involves a new input-output mapping that reduces the dimensionality of the input vectors with good performance. The contribution of this study entails the formalization of this new input-output method that outperforms the conventional dimensionality reduction techniques widely used in image processing applications. Following the intuition that (a) one object viewed under varying perspectives lays on a well-projected subspace and (b) different objects captured under similar viewpoints share identical poses, a sophisticated framework capable of both recognizing objects and estimating their pose in the 3D working space was coined. It encompasses a manifold modeling procedure that depends on the attributes of mutual information and a constellation-based structure, respectively. The employed recognition module incorporates a modified approach of a known dimensionality reduction technique that constructs a similarity matrix based on mutual information among objects and then seek a low-dimensional representation that preserves the local structure of the objects in the initial high-dimensional space. Once the data are projected into the sub-space a Support Vector Machine (SVM) classifier provides accurate recognition. The 3D pose estimation module is based on a manifold modeling algorithm that constructs feature vectors of low dimensionality and high discriminative capabilities. Then a Sparse Grasping Manifolds (SGM) method is proposed in this thesis, which aims at solving the problem of manipulation of unknown objects by unifying 3D pose manifolds and grasping points into a cohesive framework for robot grasping. Unlike contemporary systems that crave extensive supervision and large repositories of images of objects, meticulous emphasis is given on providing a ground solution with large generalization capacities founded on unsupervised learning. Additionally, the visual data available are processed in a way that their projection onto the corresponding subspaces is sparse, compact and highly representative. Grasping manifolds depend on a novel bunch-based architecture, here introduced for the first time that, unlike previous works, bypasses the part selection process using unsupervised clustering, while by extracting local patches encapsulates both appearance and geometrical attributes of the objects. Contrary to earlier works the presented method offers higher generalization capacities mainly due to the efficient learning of the employed function that is based on a large a priori training set containing numerous examples of real and artificial data. Moreover, through the minimization of the l₁ norm, the presented approach builds sparse and compact manifolds that are highly representative, tolerant to common imaging disturbances (e.g. noise) and possess superior discrimination capabilities. In the last study of this thesis, the goal was to design a unified architecture for autonomous manipulation of unknown objects, which is capable of answering and addressing the constraints of all the following questions: ""What is the object?"", ""Where is it?"" and ""How to grasp it?"". The recognition problem is tackled from a shape-based perspective, whilst obtaining accurate detection decisions via a Bag-of-Visual Words classification scheme. Grasping points are found through an ontology-based knowledge acquisition where recognized objects inherit the grasping points assigned to the respective class. The presented ontologies include: a) object-class related data, b) pose-manifolds assigned to each instance of the object-class conceptual model and c) information about the grasping points of every trained instance. This work represents the first integrated research endeavor in concept ontologization focusing on the liaison between image understanding algorithms and the corresponding motor commands in the particular task of unknown object grasping.Καθώς το περιβάλλον εργασίας των ρομπότ γίνεται όλο και πιο πολύπλοκο, αναδύονται νέες προκλήσεις αναγνώρισης προτύπων. Εκτός από τη συμβατική ερώτηση «Ποιο είναι το αντικείμενο», στις μέρες μας, οι επιστήμονες καλούνται να απαντήσουν και σε άλλες εξίσου ενδιαφέρουσες ερωτήσεις, όπως «Πού είναι αυτό;» ή «Πώς να το πιάσω;». Οι τεχνικές αιχμής περιλαμβάνουν προηγμένες υπορουτίνες αναγνώρισης προτύπων που συνεργάζονται με αλγόριθμους μηχανικής μάθησης, έτσι ώστε να παρέχουν πορίσματα, σχετικά με την τρέχουσα κατάσταση του περιβάλλοντος, παρόμοια με εκείνα που εξάγει ο ανθρώπινος νους. Ένα ενδιαφέρον πρόβλημα στον τομέα της υπολογιστικής όρασης είναι η εκτίμηση του τρισδιάστατου διανύσματος θέσης ενός αντικειμένου, λόγω της πρακτικής ιδιαιτερότητας και της θεμελιώδους σημασίας του, σε διαδικασίες κατανόησης σκηνικών. Μια πληθώρα διαφορετικών εφαρμογών απαιτούν την ακριβή εκτίμηση της γεωμετρικής τοποθέτησης των αντικειμένων, ως προς ένα δεδομένο σύστημα αναφοράς, για την επίτευξη των στόχων τους. Στη ρομποτική, ο αυτόνομος χειρισμός αντικειμένων είναι   εφικτός μόνο σε περιπτώσεις, κατά τις οποίες οι έξι βαθμοί ελευθερίας του στόχου είναι γνωστοί. Παρά τις άφθονες ερευνητικές προσπάθειες και τα μέχρι στιγμής επιτεύγματα, δεν έχει ακόμη κατασκευαστεί ένα προηγμένο σύστημα όρασης, το οποίο να χαρακτηρίζεται από χαμηλή πολυπλοκότητα και επαρκείς δυνατότητες γενίκευσης. Η παρούσα διδακτορική διατριβή είναι εμπνευσμένη από τις αξιοσημείωτες έμφυτες ικανότητες των ανθρώπων, που εντοπίζονται στην εκτίμηση της σχετικής θέσης των αντικειμένων, δεδομένης μιας αρχικής υπόθεσης. Ανεξάρτητα από τις συνθήκες φωτισμού κα τις πιθανές κοινές  διαταραχές της όρασης, όπως η μερική επικάλυψη, η υπεροχή των ανθρώπων σε ότι αφορά στην ερμηνεία της τρισδιάστατης γεωμετρικής τοποθέτησης των αυθαίρετα τοποθετημένων αντικειμένων είναι αδιαμφισβήτητη. Από όλες τις εργασίες που πραγματοποιούνται από ανθρώπινα όντα, η πλειοψηφία έχει άμεση σχέση με το χειρισμό αντικειμένων, είτε κατά τη διάρκεια της βρώσης και της πόσης, είτε για τον χειρισμό ενός εργαλείου. Από την άλλη πλευρά, η υλοποίηση του χειρισμού στόχου από ένα ρομποτικό βραχίονα τοποθετεί αυστηρούς περιορισμούς ως προς την ακριβή ταχύτητα εκτέλεσης και την «φύση» των υπό εξέταση αντικειμένων. Σύγχρονες λύσεις για το πρόβλημα αυτό προϋποθέτουν υψηλές δυνατότητες αποθήκευσης ενώ παράλληλα αποτυγχάνουν να γενικεύσουν σε άγνωστους στόχους, δηλαδή σε αντικείμενα που δεν περιλαμβάνονται στο σύνολο εκπαίδευσης. Ο κύριος στόχος αυτής της διδακτορικής διατριβής είναι η ανάπτυξη καινοτόμων αλγόριθμων όρασης, για την επίλυση του προβλήματος εύρεσης του διανύσματος θέσης αντικειμένων, οι οποίοι θα μπορούν να υιοθετηθούν από ρομποτικές πλατφόρμες. Είναι προφανές ότι, οι αρχιτεκτονικές που θα σχεδιαστούν θα πρέπει σε πρώτο στάδιο, να αντιμετωπίσουν το πρόβλημα εκτίμησης της θέσης (δηλαδή κυρίως του υπολογισμού του βάθους) και σταδιακά, να υπολογίζουν τους υπόλοιπους τρεις περιστροφικούς βαθμούς ελευθερίας. Αρχικά, ερευνήθηκε η σχετική βιβλιογραφία προκειμένου να αποκαλυφθούν οι τρέχουσες τεχνικές στην αιχμή της τεχνολογίας και να αναδειχθούν τα ανοικτά ζητήματα έρευνας που πρέπει να αντιμετωπιστούν. Στη συνέχεια σχεδιάστηκαν και υλοποιήθηκαν, καινοτόμες προσεγγίσεις μοντελοποίησης πολύπτυχων μορφωμάτων, που αντιστοιχούν σε διανύσματα χαμηλών διαστάσεων και υψηλών διακριτικών ικανοτήτων. Τέλος, οι κατασκευασμένοι αλγόριθμοι τοποθετήθηκαν σε προηγμένες ρομποτικές πλατφόρμες για αυτόνομο χειρισμό αντικειμένων. Αρχικά αυτή η διατριβή, ερευνά το σχεδιασμό ενός απλού και εύκολου στην υλοποίηση συστήματος, για την εύρεση της τοποθεσίας ενός αντικειμένου σε ένα σκηνικό. Η μέθοδος που αναπτύχθηκε εδράζεται στην παρατήρηση ότι, τα χαρακτηριστικά γνωρίσματα, που εξάγονται από οποιοδήποτε αλγόριθμο δύο μερών (ανιχνευτή-περιγραφέα), αντιστοιχούν σε σημεία στην επιφάνεια του αντικειμένου, το κέντρο μάζας των οποίων σχετίζεται με εκείνο του αντικειμένου. Επομένως, εξάγοντας χαρακτηριστικά γνωρίσματα σε γνωστές θέσεις του αναζητούμενου αντικειμένου, είναι εφικτός ο υπολογισμός της απόστασής του από την κάμερα. Σε σύγκριση με τις σύγχρονες λύσεις στο πρόβλημα της εκτίμησης βάθους, η προσέγγιση που προτείνεται απαιτεί μικρό υπολογιστικό κόστος, ενώ χρησιμοποιεί μόνο ένα αισθητήρα όρασης, σε αντίθεση με αρχιτεκτονικές που βασίζουν τη λειτουργία τους σε στερεοσκοπικές κάμερες. Στη συνέχεια παρουσιάζεται ένα πλαίσιο το οποίο, βασισμένο στις ιδιότητες των νευρωνικών δικτύων, είναι σε θέση να υπολογίζει τους 3 περιστροφικούς βαθμούς ελευθερίας οποιουδήποτε αντικειμένου. Το δίκτυο χρησιμοποιεί πολυάριθμα δεδομένα εκπαίδευσης, που περιέχονται σε διαθέσιμες βάσεις δεδομένων, ενώ τα χαρακτηριστικά γνωρίσματα εξάγονται μέσω του αλγορίθμου Fuzzy c-means. Λαμβάνοντας υπόψη ότι (α) ένα αντικείμενο φωτογραφισμένο υπό διαφορετικές γωνίες μπορεί να προβληθεί σε ένα συγκεκριμένο υποχώρο και (β) εντελώς διαφορετικά αντικείμενα φωτογραφισμένα υπό παρόμοιες οπτικές γωνίες μοιράζονται ίδια διανύσματα θέσης, υλοποιήθηκε ένα εξελιγμένο πλαίσιο, ικανό τόσο να αναγνωρίζει στόχους όσο και να υπολογίζει τη γεωμετρική τους τοποθέτηση. Αυτό περιλαμβάνει μια διαδικασία μοντελοποίησης πολύπτυχων μορφωμάτων βασισμένη στις ιδιότητες της αμοιβαίας πληροφορίας και σε μια αρχιτεκτονική μερών, αντίστοιχα. Η προτεινόμενη μέθοδος αναγνώρισης προτύπων ενσωματώνει μια τροποποιημένη προσέγγιση μιας γνωστής τεχνικής μείωσης διαστάσεων, που κατασκευάζει έναν πίνακα ομοιότητας με βάση την αμοιβαία πληροφορία μεταξύ των αντικειμένων και στη συνέχεια αναζητεί ένα αντιπροσωπευτικό διάνυσμα χαμηλών διαστάσεων, που να διατηρεί την τοπική δομή των αντικειμένων. Μόλις τα δεδομένα προβληθούν στον υποχώρο, ένας ταξινομητής βασισμένος σε μηχανές υποστήριξης διανυσμάτων παρέχει ακριβή πορίσματα αναγνώρισης. Όσον αφορά στο υποσύστημα τρισδιάστατου εντοπισμού θέσης, δημιουργούνται πολύπτυχα μορφώματα τα οποία, παρόλο που είναι χαμηλής διάστασης, παρέχουν αξιοσημείωτες διακριτικές ικανότητες. Η προτεινόμενη μέθοδος συνιστά μια εναλλακτική προσέγγιση στο πρόβλημα του υπολογισμού θέσης αντικειμένων καθώς είναι ικανή να προσφέρει αυξημένη γενίκευση, εν αντιθέσει με άλλες τεχνικές του τομέα που περιορίζονται στην εκτίμηση των γεωμετρικών τοποθετήσεων αυτοκινήτων. Η μέθοδος Αραιών Πολύπτυχων Μορφωμάτων Λαβής στοχεύει στην επίλυση του προβλήματος του χειρισμού άγνωστων αντικειμένων, με την ενοποίηση μορφωμάτων θέσης και σημείων λαβής σε ένα συνεκτικό πλαίσιο. Αντίθετα με τα σύγχρονα συστήματα που απαιτούν εκτεταμένη εποπτεία και αποθήκευση πολυάριθμων εικόνων των αντικειμένων, δίνεται ιδιαίτερη έμφαση στην παροχή μιας λύσης με μεγάλες δυνατότητες γενίκευσης, βασισμένη στις ιδιότητες της εκμάθησης χωρίς επίβλεψη. Επιπλέον, τα οπτικά στοιχεία, που είναι διαθέσιμα, υφίστανται επεξεργασία, τέτοια ώστε η προβολή τους στους αντίστοιχους υποχώρους να είναι αραιή, συμπαγής και επαρκώς αντιπροσωπευτική. Τα πολύπτυχα μορφώματα χειρισμού εξαρτώνται από μια αρχιτεκτονική μερών, η οποία παρουσιάζεται για πρώτη φορά και σε αντίθεση με   προηγούμενες τεχνικές, παρακάμπτει τη διαδικασία επιλογής χαρακτηριστικών. Συγκεκριμένα, χρησιμοποιώντας τεχνικές εκμάθησης χωρίς επίβλεψη, η αρχιτεκτονική μερών είναι σε θέση να εξάγει νέα γνωρίσματα, που φέρουν τοπικές πληροφορίες γειτνίασης και υφής. Συγκρινόμενη με αλγορίθμους, που ανήκουν στην αιχμή της τεχνολογίας, η μέθοδος προσφέρει μεγαλύτερη ικανότητα γενίκευσης, κυρίως λόγω του ακριβούς υπολογισμού των παραμέτρων της προτεινόμενης συνάρτησης και της εκπαίδευσής της με μεγάλο αριθμό δεδομένων. Επιπροσθέτως, μέσω της ελαχιστοποίησης της L₁ νόρμας, δημιουργεί αραιά και συμπαγή πολύπτυχα μορφώματα, τα οποία είναι ανεκτικά σε κοινές διαταραχές απεικόνισης (π.χ. θόρυβος) και κατέχουν ανώτερες διακριτικές ικανότητες. Στην τελευταία μέθοδο της παρούσας διατριβής, δίνεται ιδιαίτερη έμφαση στο σχεδιασμό μιας ενοποιημένης αρχιτεκτονικής για αυτόνομο χειρισμό άγνωστων αντικειμένων, η οποία είναι σε θέση να απαντήσει στα ακόλουθα ερωτήματα: «Ποιο είναι το αντικείμενο;», «Πού είναι;» και «Πώς να το πιάσω;». Το πρόβλημα της αναγνώρισης ερευνάται υπό το πρίσμα των μορφολογικών ιδιοτήτων των στόχων, ενώ ταυτόχρονα, εξάγονται ακριβή πορίσματα για την ταυτότητα των αντικειμένων με τη χρήση του ταξινομητή Bag-of-Visual Words. Τα σημεία λαβής υπολογίζονται με τη χρήση οντολογιών, σύμφωνα με τις οποίες τα αντικείμενα που αναγνωρίζονται, κληρονομούν σημεία λαβής παρόμοια εκείνων της κλάσης στην οποία ανήκουν. Οι εν λόγω οντολογίες περιλαμβάνουν: α) δεδομένα που σχετίζονται με την κλάση του αντικειμένου, β) πολύπτυχα μορφώματα γεωμετρικής τοποθέτησης για κάθε στιγμιότυπο του αντικειμένου και γ) πληροφορίες σχετικά με τα σημεία λαβής του κάθε παραδείγματος εκπαίδευσης. Ο προτεινόμενος αλγόριθμος συνιστά την πρώτη ολοκληρωμένη ερευνητική προσπάθεια αξιοποίησης των δυνατοτήτων των οντολογιών και επικεντρώνεται στη δημιουργία σχέσεων μεταξύ τεχνικών κατανόησης εικόνας και μεθόδων αυτόνομου χειρισμού αντικειμένων",Νέες τεχνικές αναγνώρισης προτύπων για συστήματα τεχνητής όρασης,,'National Documentation Centre (EKT)',,10.12681/eadd/29283,core
46819332,2014-09-19T00:00:00,"In this manuscript, I describe my research work concerning signal and image processing, and instrumentation for astronomy. I also present the projectsI intend to develop in the next few years. The thematic areas I dealt with were numerous and diverse, and the applications concerned, among others,visual and spectroscopic double stars, tidal effects in Am-type stars, shell galaxies, atmospheric turbulence and wind profiles with the SCIDAR technique.Among the main astrophysical results to which I contributed, are the validation of merging models to explain the origin of shells around elliptical galaxies,the evidence of the presence of dark matter in those galaxies, the detailed study of many spectroscopic binaries with the constitution of representative samples, including long period orbit systems containing FGKM, Am or composite spectrum stars. In particular, a thorough study of a sample of a hundred Am-type stars highlighted the influence of tidal effects, which contributed to circularizing the orbits and to synchronizing the spin and orbital motion in many systems. The fractional critical radii for circularisation and synchronisation that we have determined for our sample were found to be compatible with Zahn's theoretical models. We have also compared the estimated ages of our systems with the theoretical circularisation and synchronisation characteristic times, also predicted by Zahn. We found a fair agreement for the synchronism, but for circularisation, the tidal effects we have observed are more important than what was expected by the theory, for Am-type stars with a radiative envelope.As regards the visual binary stars, in collaboration with a group of European researchers, we have obtained a few thousand measurementswith the PISCO and PISCO2 instruments, by using the speckle interferometry technique. Those accurate measurements have been calibratedin an absolute way with a diffraction grating. Their spatial resolution often reached the diffraction limit of the telescope. They have already led to the revision of a few hundred orbits. As their orbital periods are very long, and generally exceed a few hundred years, the observation of the visual double stars is a substantial program, which requires reliability and regularity over a very long period of time. This program needs an international collaboration that has been already active since several generations. As soon as they have been published, our measures were incorporated into the double star data base of the US. Navy in Washington DC, which is open to the whole scientific community.With regard to instrumentation, my activities mainly concerned the designing, construction and operation of the PISCO and PISCO2 focal instruments.A substantial part was devoted to real-time software developments for the remote control of the instruments and the detectors, and the subsequent data processing. The main selected criteria were ``performance and reliability'', both for hardware and software developments. Indeed those instruments and programs have been used during nearly every clear night, for many years (more than 20 years for PISCO, and already five years for PISCO2).The processing of PISCO data also lead me to developing programs for atmospheric turbulence profile inversion from SCIDAR observations, that can be operated in real-time and in a non-supervised mode. Those programs allow the restoration of turbulence profiles ($C_N^2$ parameter,seeing, coherence radius and coherence time) and of the wind parameters (velocity and direction) in the turbulent layers. When the observations are obtained in ``generalized SCIDAR'' mode, we can determine those parameters in all the turbulent layers that have been crossed by the incoming light,from the inside of the dome to altitudes up to 20--23 km. I have also written specialized software for computing orbits from radial velocities formultiple stellar systems made of two or three components. The main difficulty of this problem rested in the irregular sampling of the measurements.Finally, over the last few years, I have been working on the problems encountered for the phase self-calibration for aperture synthesis inastronomy, and for the calibration of GNSS (Global Navigation Satellite System) networks in geodesy, to which my principal collaborator, A.~Lannes (L2S, Paris), has proposed an elegant and original solution.For the coming years, I would like to work in collaboration with colleagues, but also if possible with some students (graduate trainees or PhD students) on the following projects:- ""Instrumentation'' projects concerning (i) transfer of PISCO to the new Epsilon 1 m telescope of OCA (Obs. Côte d'Azur), with the automation of the data acquisition and processing, (ii) automation of PISCO2 on the L76 refractor of OCA, and (iii) software development for the ``Shack-Hartmann'' modeof PISCO.- ""Signal processing"" projects about (i) the implementation of the calibration methods proposed by A.~Lannes for radio-astronomy (ALMA) and for GNSS networks, and (ii) non-supervised classification of X sources that have been detected by the XMM-Newton space telescope.- ""Astrophysical projects'' concerning the observation of double stars belonging to the Pre-Main Sequence (PMS) of the HR diagram, and of red dwarfs of the solar neighbourhood, with PISCO and PISCO2, to determine the orbits and the masses of those stars, which are still poorly known.Dans ce manuscrit, je décris mes activités de recherche en traitement du signal, des images et en l'instrumentation pour l'astronomie et je présente les projets que je compte développer dans les prochaines années. Les thématiques que j'ai abordées sont très variées, et les applications ont concerné à la fois les étoiles doubles visuelles et spectroscopiques, les galaxies elliptiques à coquilles et la turbulence atmosphérique et de profils de vitesse du ventpar la technique SCIDAR.Parmi les principaux résultats astrophysiques auxquels j'ai contribué, on peut mentionner la validation des modèles de fusion pourexpliquer l'origine des coquilles, la mise en évidence de matière noire dans les galaxies elliptiques, la constitution d'échantillons représentatifset l'étude détaillée de nombreuses binaires spectroscopiques, dont certaines à longue période, de type FGKM, Am et d'étoiles à spectre composite. En particulier une étude approfondie d'un échantillon d'une centaine d'étoiles Am a permis de mettre en évidence des effets de marées, semanifestant par une circularisation des orbites et une synchronisation entre la rotation axiale des étoiles et leur mouvement orbital. Notreétude a conduit à préciser la valeur de paramètres importants pour la modélisation (rayons fractionnaires critiques) et nousavons pu comparer les temps caractéristiques  théoriques de circularisation et synchronisation avec les âges estimés pour cessystèmes. L'accord est convenable pour le synchronisme mais moins bon pour la circularisation. Les effets de marée observés semblentplus importants que ceux prévus par la théorie, pour les étoiles de type Am, à enveloppe radiative.En ce qui concerne les étoiles doubles visuelles, en collaboration avec un groupe de chercheurs européens, nous avons obtenu plusieurs milliersde mesures avec PISCO et PISCO2, en utilisant des techniques d'interférométrie des tavelures. Ces mesures très précises ont été calibrées de façonabsolue avec un réseau de diffraction, et ont une résolution atteignant la limite de diffraction du télescope. Elles ont déjà conduit à la révision de quelques centaines d'orbites. Les périodes orbitales étant très longues, souvent supérieures à des centaines d'années, l'observation des étoiles doubles est un programme de fond, qui nécessite régularité et durée dans le temps. Ce programme ne peut se faire que dans le cadre d'une collaboration internationale et inter-générationnelle. Au fur et à mesure des publications, nos mesures ont été intégrées dans la base de donnéesspécialisée de l'observatoire de l'US. Navy de Washington (USA), qui est ouverte à toute la communauté.Sur le plan instrumental mes activités ont concerné principalement la conception, la réalisation et le suivi de l'exploitation d'instruments comme PISCO et PISCO2, avec une part importante consacrée au développement de logiciels temps réel pour le contrôle des instruments, des détecteurs et letraitement des données. Les principaux critères que nous avons retenus ont été ``performance et fiabilité'', puisque ces instruments et logiciels sont utilisés, pratiquement toutes les nuits de beau temps, depuis de nombreuses années (plus de 20 ans pour PISCO).Le traitement des données de PISCO m'a aussi conduit à développer des programmes d'inversion de profils de turbulence atmosphérique à partird'observation SCIDAR, qui peuvent fonctionner en mode non supervisé et en temps réel. Ils permettent de restaurer des profils de la turbulence (paramètre $C_N^2$, seeing, rayon et temps de cohérence) et des paramètres du vent en altitude (vitesse et direction) dans les couches turbulentes.Lorsque les observations ont été obtenues en mode ``SCIDAR généralisé'', ils permet de caractériser la turbulence dans toutes les couches traversées,depuis l'intérieur de la coupole jusqu'à des altitudes de 20 à 23 km. J'ai aussi écrit des programmes de calcul d'orbites à partir de mesures de vitesses radiales pour des systèmes à deux ou trois composantes. La difficulté de ce problème résidait principalement dans l'irrégularité de l'échantillonnage des mesures. Enfin depuis quelques années, je travaille sur des problèmes d'auto-calibration de phase pour la synthèse d'ouverture en astronomie,et de calibration des réseaux GNSS (Global Navigation Satellite System) en géodésie, auxquels mon principal collaborateur, A. Lannes (L2S, Paris), a apporté une solution élégante et originale.Pour les prochaines années, je voudrais travailler en collaboration avec des collègues, mais aussi si possible avec la participation d'étudiants (stages niveau ""master'' ou ""thèse'') sur les projets suivants:- projets ""Instrumentaux'' concernant (i) le transfert de PISCO sur le nouveau télescope Epsilon de 1 m de l'OCA (Obs. Côte d'Azur), avec une automatisation de l'acquisition et du traitement des données. (ii) l'automatisation des fonctions de PISCO2 sur la lunette L76 de Nice,et (iii) le développement d'un logiciel de traitement pour le mode ``Shack-Hartmann'' de PISCO.- projets ""Traitement du signal"" portant sur (i) la mise en oeuvre des méthodes de calibration proposées par A. Lannes pour la radio-astronomie et pour les réseaux GNSS et (ii) sur  la classification non supervisée de sources détectées par le télescope spatial XMM-Newton.- projets ""Astrophysique'' concernant l'observation d'étoiles doubles pré-séquence principale du diagramme HR et des naines rouges du voisinage solaire avec PISCO et PISCO2, pour déterminer les orbites et les masses de ces étoiles, encore mal connues","Contribution en Signal, Image et Instrumentation pour l'Astronomie",,HAL CCSD,,,core
464689506,2014-01-01T00:00:00,"Artificial intelligence methodologies, as the core of discrete control and decision support systems, have been extensively applied in the industrial production sector. The resulting tools produce excellent results in certain cases; however, the NP-hard nature of many discrete control or decision making problems in the manufacturing area may require unaffordable computational resources, constrained by the limited available time required to obtain a solution. With the purpose of improving the efficiency of a control methodology for discrete systems, based on a simulation-based optimization and the Petri net (PN) model of the real discrete event dynamic system (DEDS), this paper presents a strategy, where a transformation applied to the model allows removing the redundant information to obtain a smaller model containing the same useful information. As a result, faster discrete optimizations can be implemented. This methodology is based on the use of a formalism belonging to the paradigm of the PN for describing DEDS, the disjunctive colored PN. Furthermore, the metaheuristic of genetic algorithms is applied to the search of the best solutions in the solution space. As an illustration of the methodology proposal, its performance is compared with the classic approach on a case study, obtaining faster the optimal solution. © 2014 Juan-Ignacio Latorre-Biel et al",Control of discrete event systems by means of discrete optimization and disjunctive colored PNs: Application to manufacturing facilities,,'Hindawi Limited',,10.1155/2014/821707,core
22577025,2013-07-17,"Backtracking search algorithms are useful in many domains, from SAT solvers to artificial intelligences for playing games such as chess. Searching disjoint branches can, inherently be done in parallel though it can considerably increase the amount of work that the algorithm does. Such parallelism is speculative, once a solution is found additional work is irrelevant, but the individual branches each have their own potential to find the solution. In sequential algorithms, heuristics are used to prune regions of the search space. In parallel implementations this pruning often corresponds to aborting existing computations that can be shown to be pursuing dead-ends. While some systems provide native support for aborting work, Intel’s current parallel extensions to C++, implemented in the cilk++ compiler [Lei09], do not. In this work, we show several methods for implementing abort as a library in the cilk++ system. We compare our implementations to each other and quantify the benefit of abort in a real game AI. We derive a mostly mechanical translation to convert programs with native abort into cilk++ using continuation passing style",Speculative Parallelism in Cilk++,,,,,core
36862045,2014,"In today's world, an optimal and intelligent problem solving approaches are required in every field, regardless of simple or complex problems. Researches and developers are trying to make machines and software's more efficient and intelligent. This is where the Artificial Intelligence plays its role in developing efficient and optimal searching algorithm solutions. Genetic algorithm is one of most pervasive and advanced developed heuristic search technique in AI. Genetic algorithm (GA) is developed to find the most optimized solution for a given problem based on inheritance, mutation, selection and some other techniques. It was proved that genetic algorithms are the most powerful unbiased optimization techniques for sampling a large solution space. In this paper, we have used GA for the image optimization and Knapsack Problems, which are commonly found in a real world scenario. Furthermore, a research based on a tool that uses Genetic Algorithm, called the GA Playground is done to demonstrate the capability of solving the Knapsack Problem with the fitness function and a case study on how images can be reproduced using the optimal parameters. Lastly, a few methods such as the Hash Table and the Taguchi Method are suggested to improve the performance of the Genetic Algorithm",A genetic algorithm analysis towards optimization solutions,,Society of Digital Information and Wireless Communications,,,core
226204753,2014-01-01,"A novel hybrid co-design for implementing high-resolution reconstruction algorithms, for near real time implementation of remote sensing (RS) imagery, is addressed in this paper. In the proposed codesign scheme, the inverse square root and the matrix operations of the robust adaptive space filter algorithm are implemented as accelerators units in a Field Programmable Gate Array (FPGA) using piecewise polynomial approximations and systolic array (SA) techniques. Then, the FPGA based accelerator is integrated with an ARM processor in a HW/SW co-design paradigm that meets the (near) real time imaging systems requirements in spite of conventional computations. Finally, we report and discuss the results of the hybrid FPGA/ARM co-design implementation in a Xilinx Virtex-5 XC5VFX70TFFG1136 for reconstruction of real world RS images. "" Springer International Publishing Switzerland 2014."",,,,,,,,,""http://hdl.handle.net/20.500.12104/41993"",""http://www.scopus.com/inward/record.url?eid=2-s2.0-84921687571&partnerID=40&md5=09b8437322b32b3fc3c9b4a1ea73bb1c"",,,,,,,,""Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"",,""103",One-frame two-dimensional deflectometry for phase retrieval by addition of orthogonal fringe patterns,,'The Optical Society',,10.1364/AO.52.006537,core
102670227,2015-09-03,"Abstract. Off-road autonomous navigation is one of the most difficult automation challenges from the point of view of constraints on mobility, speed of motion, lack of environmental structure, density of hazards, and typical lack of prior information. This paper describes an autonomous navigation software system for outdoor vehicles which includes perception, mapping, obstacle detection and avoidance, and goal seeking. It has been used on sev-eral vehicle testbeds including autonomous HMMWV’s and planetary rover prototypes. To date, it has achieved speeds of 15 km/hr and excursions of 15 km. We introduce algorithms for optimal processing and computational stabilization of range imagery for terrain map-ping purposes. We formulate the problem of trajectory generation as one of predictive control searching trajectories in command space. We also formulate the problem of goal arbitration in local autonomous mobility as an optimal control problem. We emphasize the modeling of vehicles in state space form. The resulting high fidelity models sta-bilize coordinated control of a high speed vehicle for both obstacle avoidance and goal seeking purposes. An intermediate predictive control layer is introduced between the typical high-level strategic or artificial intelli-gence layer and the typical low-level servo control layer. This layer incorporates some deliberation, and some envi-ronmental mapping as do deliberative AI planners, yet it also emphasizes the real-time aspects of the problem as d",An Approach to Rough Terrain Autonomous Mobility,,,,,core
230280740,2013-01-01T00:00:00,"With the unprecedented fast growth of data, we have better opportunities to understand our complex world, and simultaneously face pervasive challenges in efficiently inferring the meaning behind these vast amounts of data. It is particularly important to explore the intrinsic structures in data to increase our rational understanding of the latent mechanisms that generate them. In modeling, structures are features used to characterize the underlying systems, such as the rank of a system, the number of clusters, the levels of hierarchy, and the order of spatio-temporal correlations in multiple measurements.  In this thesis, we present our research contributions on utilizing structures and sparsity in observed data to improve estimation and prediction of trajectories of system states for two systems: the highway traffic system and the human physiology systems. Both systems exhibit features that are seen in many other applications.  For the traffic problem, it is useful to know the near--term traffic conditions after the occurrence of some events which have noticeable impact on the road traffic. Often used macroscopic models, which view road traffic as fluid flowing in pipes, suffer from various inaccuracies, which could be mitigated by incorporating past observations to correct  predictions. However, we often have limited observation and computing resources (e.g. probe vehicles, smartphones, bandwidth, sensors) to gather and process past observations. We describe a novel low-overhead strategy to adaptively select observation sites in real-time by using the density of the mesh of the numerical solution of  the underlying mathematical model to capture the variability of that solution. We show that our proposed strategy improves the numerical accuracy of near--term traffic forecasting with limited observation resources as compared with with uniform deployment of the observation resources. In addition to deploying limited observation resources, one  is often concerned with detecting special traffic events. To this end, we propose a novel method to decompose traffic observations into normal background and sparse events. Our method couples multiple traffic datastreams so that they share a certain sparse spatio--temporal structure.  We also study the utility of sparseness and structure in physiological datastreams. Missing values hinder the use of many machine learning methods. We show how to incorporate ideas from compressive sensing into handling the missing values problem in continuous intracranial pressure (ICP) datastreams from patients with traumatic brain injury. We  experimentally evaluate the proposed method in experiments where randomly selected ICP values are marked as missing. We find our method gives estimated missing values that are in better agreement with the true values as compared with k--nearest neighbor and expectation maximization data imputation methods.  Moreover, predicting the near--term intracranial pressure for traumatic brain injury patients is of great importance to clinicians. Traditional regression methods, need an explicit parametric form of the model to fit. However, due to our limited knowledge of the complex brain physiology, it is difficult to specify an accurate parametric model. To overcome this  difficulty, our model uses Gaussian processes to quantify our prior beliefs on the smoothness of the regression model, and performs regression in an infinite dimensional space. We show that the proposed Gaussian process regression model shows predicts ICP changes in clinically useful timeframes and may support future development of minimally-invasive ICP monitoring systems, earlier intervention strategies, and better patient outcomes",On Prediction and Estimation for Datastreams Utilizing Sparsity and Structure,,"University of Maryland, Baltimore County (UMBC)",,,core
76990583,2014-06-27T00:00:00,"It is difficult or impossible to separate the performance of an optimization solver  from the architecture of the computing system on which the algorithm is implemented. This is particularly true if measurements from a physical system are used to update and solve a sequence of mathematical optimization problems in real-time, such as in control, automation, signal processing and machine learning. In these  real-time optimization applications the designer has to trade off computing time, space and energy against each other, while satisfying constraints on the performance and robustness of the resulting cyber-physical system. This paper is an informal introduction to the issues involved when designing the computing hardware and a real-time optimization algorithm at the same time, which can result in  systems with efficiencies and performances that are unachievable when designing the  sub-systems independently. The co-design process can, in principle, be formulated as a sequence of uncertain and non-smooth optimization problems. In other words, optimizers might be used to design optimizers. Before this can become a reality, new systems theory and numerical  methods will have to be developed  to solve these co-design problems effectively and reliably",Co-design of Hardware and Algorithms for Real-time Optimization,,,,10.1109/ecc.2014.6862630,core
215313918,2015-12-01T00:00:00,"Today�s electric power grids face serious challenges due to overstressed networks, need to assimilate variable generation, strict environmental regulations and widespread weather-caused outages. There is an urgent need to improve grid resilience and system security more than ever before. Amidst such challenges, the best approach would be to focus primarily on the grid intelligence rather than implementing redundant preventive measures. The foundation of any intelligent operational strategy would be on the ability of the grid to assess its current dynamic state instantaneously. Traditional forms of real-time power system security assessment consist mainly of methods based on power flow analyses. Such methods do not consider the dynamics inherent in the system and hence, are static in nature. However, in order to capture the nonlinear dynamics present in the system, it is necessary to carry out time-domain simulations (TDS) that are computationally too involved to be performed in real-time. Machine learning (ML) techniques have the capability to organize data gathered from such simulations and thereby extract useful information in order to better assess the system security instantaneously. This dissertation presents a framework that would enable implementation of machine learning techniques for real-time assessment of grid resilience. An IEEE 14-bus test system is used in this work for simulation purposes. Firstly, a set of multiple steady-state operating points is generated by performing a SSA on the base case of the power system. Secondly, a TDS is performed on each operating point to assess the grid resilience against a specific disturbance, thus generating a database for this work. This work highlights the importance and need for selecting a few operating points as �landmarks� in the operational space under consideration for prediction of power system security. Further, a few heuristics are developed so as to rank all the operating points of the system. The proposed ranking methodologies are used to select the best landmarks in order to improve prediction accuracy on the original database, thereby enhancing the ability to assess grid resilience instantaneously.Electrical Engineerin",Approach to Assess the Resiliency of Electric Power Grids,https://core.ac.uk/download/215313918.pdf,'Oklahoma State University Library',,,core
22599427,2013-07-22,"In this paper, an online methodology for the detection of unsafe driving states while driving is presented. The detection is based on the multi-sensor approaches, including gyrometer, accelerometer, radar, video and so on. Various information comes from both the ego vehicle and its surroundings are fused to gain a comprehensive understanding of driving situations. Using subspace modeling techniques, we propose an unsupervised learning algorithm to perform the unsafe states detection. The feature space are decomposed into the normal and anomalous subspace, where the normal space are assumed as the major components of the driving patterns, and significant deviations from the modeled normal subspace are signaled as unsafe states. In addition, the algorithm works in a real-time way incorporating a implementation of sliding window, which enable the method to adapt over time to address changes in the new emerged driving situations. We have implemented our algorithm with a prototype system installed in a transit bus, validations are performed in real driving situations. Our experimental results demonstrate the effectiveness of the approach on forward risk predication. We gain a timely predication while with a low false positive when there occurs conflicts between the ego vehicle and front vehicles","JIE MA,",,,,,core
47136616,2013-04-15T14:49:47,"Artificial intelligence techniques are being widely used to face the new reality and to provide solutions that can make power systems undergo all the changes while assuring high quality power. In this way, the agents that act in the power industry are gaining access to a generation of more intelligent applications, making use of a wide set of AI techniques. Knowledge-based systems and decision-support systems have been applied in the power and energy industry. This article is intended to offer an updated overview of the application of artificial intelligence in power systems. This article paper is organized in a way so that readers can easily understand the problems and the adequacy of the proposed solutions. Because of space constraints, this approach can be neither complete nor sufficiently deep to satisfy all readers’ needs. As this is amultidisciplinary area, able to attract both software and computer engineering and power system people, this article tries to give an insight into themost important concepts involved in these applications. Complementary material can be found in the reference list, providing deeper and more specific approaches",Intelligent power system,,'Wiley',,10.1002/9780470050118.ecse196,core
159327263,2014-07-01T00:00:00,"International audienceIn brain imaging, solving learning problems in multi-subjects settings is difficult because of the differences that exist across individuals. Here we introduce a novel classification framework based on group-invariant graphical representations, allowing to overcome the inter-subject variability present in functional magnetic resonance imaging (fMRI) data and to perform multivariate pattern analysis across subjects. Our contribution is twofold: first, we propose an unsupervised representation learning scheme that encodes all relevant characteristics of distributed fMRI patterns into attributed graphs; second, we introduce a custom-designed graph kernel that exploits all these characteristics and makes it possible to perform supervised learning (here, classification) directly in graph space. The well-foundedness of our technique and the robustness of the performance to the parameter setting are demonstrated through inter-subject classification experiments conducted on both artificial data and a real fMRI experiment aimed at characterizing local cortical representations. Our results show that our framework produces accurate inter-subject predictions and that it outperforms a wide range of state-of-the-art vector- and parcel-based classification methods. Moreover, the genericity of our method makes it is easily adaptable to a wide range of potential applications. The dataset used in this study and an implementation of our framework are available at http://dx.doi.org/10.6084/m9.figshare.1086317",Graph-based inter-subject pattern analysis of fMRI data,,'Public Library of Science (PLoS)',,10.1371/journal.pone.0104586,core
55632960,2014-01-01T00:00:00,"Education is being revolutionized by the introduction, of mobile technologies in the teaching and
learning process. However, studies that focus in the application of mobile technologies to informal
learning environments is scarce and not systematized [1]. This is the reason for conducting a research
project that involved a urban game MobiGeo, designed in to take better advantage of the flexibility and
ubiquity offered by the Mobile Learning (ML) but also taking into account the importance of motivation
and interaction to enhance students learning.
The definition of ML has been a complicated task for researchers, but there are assumptions that can
not be neglected: the mobility, portability and ubiquity [2], these are features that will drive new
learning spaces and thus motivate students. This idea is supported by [2] that introduces the concepts
of ""just in time"", ""just enough"" and ""just-for-me"" and [4] that speaks of the triad ""location
independence”, ""independence time"" and ""meaningful content”.
These principles of ""anytime"" and ""anywhere"" consolidated by mobile technologies came to renew the
variety of educational activities available to teachers and in this context arises the concept of mobile
location-based games. According to [5] ""these games are played in physical space, but at the same
time, they are supported by actions and events in an interconnected virtual space"", which can be
classified into three categories: ludic, pedagogic and hybrid. By being in direct contact with the
contents to assimilate and move in a real context, students will have a more significant learning [6]
and this will result in the mobilization of knowledge in different contexts. To make the connection
between the physical and the virtual world, our research has made use of Qr codes as these devices
provide information in real time and in a dynamically way.
For this research was idealized an urban game called “MobiGeo”, that respect the principles
suggested by [7] and that has as common thread the history of the European Union. To measure
results the researchers developed a questionnaire that was adapted from a proposal of [8] which
created a ""Model to evaluate Educational Games”, so our proposal was built taking into account the
motivational model of Kirkpatrick (level1) and encompassing three major dimensions:
Motivation/Interest, Interaction and Perceived Learning. To assess the Motivation/Interest was used
the Model ARCS (Relevance, Confidence and Satisfaction) and items of Fun, Immersion and
Challenge of “Game User Experience”. On the other hand the interaction was evaluated by items of
the Social Interaction dimension of the “Game of User Experience”, the Learning Perceptions were
evaluated by Bloom's Taxonomy (Knowledge category).
In this paper we present the design and implementation of the MobiGeo outdoor learning activity with
a group of 173 students from the 7th grade of a basic school in the north of Portugal. Initial results
show that this urban game with Qr codes was an adequate activity to use in informal learning
environments that could engage students in gaming with high degrees of motivation and interaction in
order to solve the tasks presented to them and so consolidate and acquired new knowledge about the
European Union.CIEC – Research Centre on Child Studies, IE, UMinho (FCT R&D unit 317), Portuga",The implementation of mobile location based-games and QR-codes: the case of MobiGeo,https://core.ac.uk/download/55632960.pdf,"'Associated Management Consultants,  PVT., Ltd.'","[{'title': None, 'identifiers': ['2340-1079', 'issn:2340-1079']}]",,core
275615215,2014-06-01T00:00:00,"This work describes an approach devised by the authors for time series classification. In our approach genetic programming is used in combination with a serial processing of data, where the last output is the result of the classification. The use of genetic programming for classification, although still a field where more research in needed, is not new. However, the application of genetic programming to classification tasks is normally done by considering the input data as a feature vector. That is, to the best of our knowledge, there are not examples in the genetic programming literature of approaches where the time series data are processed serially and the last output is considered as the classification result. The serial processing approach presented here fills a gap in the existing literature. This approach was tested in three different problems. Two of them are real world problems whose data were gathered for online or conference competitions. As there are published results of these two problems this gives us the chance to compare the performance of our approach against top performing methods. The serial processing of data in combination with genetic programming obtained competitive results in both competitions, showing its potential for solving time series classification problems. The main advantage of our serial processing approach is that it can easily handle very large datasets.Alfaro Cid, E.; Sharman, KC.; Esparcia Alcázar, AI. (2014). Genetic programming and serial processing for time series classification. Evolutionary Computation. 22(2):265-285. doi:10.1162/EVCO_a_00110S265285222Adeodato, P. J. L., Arnaud, A. L., Vasconcelos, G. C., Cunha, R. C. L. V., Gurgel, T. B., & Monteiro, D. S. M. P. (2009). The role of temporal feature extraction and bagging of MLP neural networks for solving the WCCI 2008 Ford Classification Challenge. 2009 International Joint Conference on Neural Networks. doi:10.1109/ijcnn.2009.5178965Alfaro-Cid, E., Merelo, J. J., de Vega, F. F., Esparcia-Alcázar, A. I., & Sharman, K. (2010). Bloat Control Operators and Diversity in Genetic Programming: A Comparative Study. Evolutionary Computation, 18(2), 305-332. doi:10.1162/evco.2010.18.2.18206Alfaro-Cid, E., Sharman, K., & Esparcia-Alcazar, A. I. (s. f.). Evolving a Learning Machine by Genetic Programming. 2006 IEEE International Conference on Evolutionary Computation. doi:10.1109/cec.2006.1688316Arenas, M. G., Collet, P., Eiben, A. E., Jelasity, M., Merelo, J. J., Paechter, B., … Schoenauer, M. (2002). A Framework for Distributed Evolutionary Algorithms. Lecture Notes in Computer Science, 665-675. doi:10.1007/3-540-45712-7_64Blankertz, B., Muller, K.-R., Curio, G., Vaughan, T. M., Schalk, G., Wolpaw, J. R., … Birbaumer, N. (2004). The BCI Competition 2003: Progress and Perspectives in Detection and Discrimination of EEG Single Trials. IEEE Transactions on Biomedical Engineering, 51(6), 1044-1051. doi:10.1109/tbme.2004.826692Borrelli, A., De Falco, I., Della Cioppa, A., Nicodemi, M., & Trautteur, G. (2006). Performance of genetic programming to extract the trend in noisy data series. Physica A: Statistical Mechanics and its Applications, 370(1), 104-108. doi:10.1016/j.physa.2006.04.025Eads, D. R., Hill, D., Davis, S., Perkins, S. J., Ma, J., Porter, R. B., & Theiler, J. P. (2002). Genetic Algorithms and Support Vector Machines for Time Series Classification. Applications and Science of Neural Networks, Fuzzy Systems, and Evolutionary Computation V. doi:10.1117/12.453526Eggermont, J., Eiben, A. E., & van Hemert, J. I. (1999). A Comparison of Genetic Programming Variants for Data Classification. Lecture Notes in Computer Science, 281-290. doi:10.1007/3-540-48412-4_24Holladay, K. L., & Robbins, K. A. (2007). Evolution of Signal Processing Algorithms using Vector Based Genetic Programming. 2007 15th International Conference on Digital Signal Processing. doi:10.1109/icdsp.2007.4288629Kaboudan, M. A. (2000). Computational Economics, 16(3), 207-236. doi:10.1023/a:1008768404046Kishore, J. K., Patnaik, L. M., Mani, V., & Agrawal, V. K. (2000). Application of genetic programming for multicategory pattern classification. IEEE Transactions on Evolutionary Computation, 4(3), 242-258. doi:10.1109/4235.873235Kishore, J. K., Patnaik, L. M., Mani, V., & Agrawal, V. K. (2001). Genetic programming based pattern classification with feature space partitioning. Information Sciences, 131(1-4), 65-86. doi:10.1016/s0020-0255(00)00081-5Langdon, W. B., McKay, R. I., & Spector, L. (2010). Genetic Programming. International Series in Operations Research & Management Science, 185-225. doi:10.1007/978-1-4419-1665-5_7Yi Liu, & Khoshgoftaar, T. (s. f.). Reducing overfitting in genetic programming models for software quality classification. Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings. doi:10.1109/hase.2004.1281730Luke, S. (2000). Two fast tree-creation algorithms for genetic programming. IEEE Transactions on Evolutionary Computation, 4(3), 274-283. doi:10.1109/4235.873237Luke, S., & Panait, L. (2006). A Comparison of Bloat Control Methods for Genetic Programming. Evolutionary Computation, 14(3), 309-344. doi:10.1162/evco.2006.14.3.309Mensh, B. D., Werfel, J., & Seung, H. S. (2004). BCI Competition 2003—Data Set Ia: Combining Gamma-Band Power With Slow Cortical Potentials to Improve Single-Trial Classification of Electroencephalographic Signals. IEEE Transactions on Biomedical Engineering, 51(6), 1052-1056. doi:10.1109/tbme.2004.827081Muni, D. P., Pal, N. R., & Das, J. (2006). Genetic programming for simultaneous feature selection and classifier design. IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics), 36(1), 106-117. doi:10.1109/tsmcb.2005.854499Oltean, M., & Dioşan, L. (2009). An autonomous GP-based system for regression and classification problems. Applied Soft Computing, 9(1), 49-60. doi:10.1016/j.asoc.2008.03.008Otero, F. E. B., Silva, M. M. S., Freitas, A. A., & Nievola, J. C. (2003). Genetic Programming for Attribute Construction in Data Mining. Genetic Programming, 384-393. doi:10.1007/3-540-36599-0_36Poli, R. (2010). Genetic programming theory. Proceedings of the 12th annual conference comp on Genetic and evolutionary computation - GECCO  ’10. doi:10.1145/1830761.1830905Tsakonas, A. (2006). A comparison of classification accuracy of four genetic programming-evolved intelligent structures. Information Sciences, 176(6), 691-724. doi:10.1016/j.ins.2005.03.012Wolpaw, J. R., Birbaumer, N., Heetderks, W. J., McFarland, D. J., Peckham, P. H., Schalk, G., … Vaughan, T. M. (2000). Brain-computer interface technology: a review of the first international meeting. IEEE Transactions on Rehabilitation Engineering, 8(2), 164-173. doi:10.1109/tre.2000.84780",Genetic programming and serial processing for time series classification,https://riunet.upv.es/bitstream/handle/10251/67540/Alfaro%3bSHARMAN%3bEsparcia%20-%20Genetic%20programming%20and%20serial%20processing%20for%20time%20series%20classification.pdf?sequence=1&isAllowed=y,'MIT Press - Journals',,10.1162/EVCO_a_00110,core
150531781,2014-08-15T04:04:42Z,"<div><p>In brain imaging, solving learning problems in multi-subjects settings is difficult because of the differences that exist across individuals. Here we introduce a novel classification framework based on group-invariant graphical representations, allowing to overcome the inter-subject variability present in functional magnetic resonance imaging (fMRI) data and to perform multivariate pattern analysis across subjects. Our contribution is twofold: first, we propose an unsupervised representation learning scheme that encodes all relevant characteristics of distributed fMRI patterns into attributed graphs; second, we introduce a custom-designed graph kernel that exploits all these characteristics and makes it possible to perform supervised learning (here, classification) directly in graph space. The well-foundedness of our technique and the robustness of the performance to the parameter setting are demonstrated through inter-subject classification experiments conducted on both artificial data and a real fMRI experiment aimed at characterizing local cortical representations. Our results show that our framework produces accurate inter-subject predictions and that it outperforms a wide range of state-of-the-art vector- and parcel-based classification methods. Moreover, the genericity of our method makes it is easily adaptable to a wide range of potential applications. The dataset used in this study and an implementation of our framework are available at <a href=""http://dx.doi.org/10.6084/m9.figshare.1086317"" target=""_blank"">http://dx.doi.org/10.6084/m9.figshare.1086317</a>.</p></div",Graph-Based Inter-Subject Pattern Analysis of fMRI Data,,,,10.1371/journal.pone.0104586,core
291738743,2013-05,"Continuous increases in traffic volume and limited available capacity in the roadway system have created a need for improved traffic control. From traditional pre-timed isolated signals to actuated and coordinated corridors, traffic control for urban networks has evolved into more complex adaptive signal control systems. However, unexpected traffic fluctuations, rapid changes in traffic demands, oversaturation, the occurrence of incidents, and adverse weather conditions, among others, significantly impact the traffic network operation in ways that current control systems cannot always cope with.
On the other hand, strategies for traffic control based on developments from the field of machine learning can provide promising alternative solutions, particularly those that make use of unsupervised learning such as reinforcement learning (RL) - also referred as approximate dynamic programming (ADP) in some research communities. For the traffic control problem, examples of convenient RL algorithms are the off-policy Q-learning and the ADP using a post decision state variable, since they address processes with sequential decision making, do not need to compute transition probabilities, and are well suited for high dimensional spaces. 
A series of benefits are expected from these algorithms in the traffic control domain: 1) no need of prediction models to transition traffic over time and estimate the best actions; 2) availability of cost-to-go estimates at any time (appropriate for real-time applications); 3) self-evolving policies; and 4) flexibility to make use of new sources of information part of emergent Intelligent Transportation Systems (ITS) such as mobile vehicle detectors (Bluetooth and GPS vehicle locators).   
Given the potential benefits of these strategies, this research proposes MASTraf: a decentralized Multi-Agent System for network-wide Traffic signal control with dynamic coordination. MASTraf is designed to capture the behavior of the environment and take decisions based on situations directly observed by RL agents. Also, agents can communicate with each other, exploring the effects of temporary coalitions or subgroups of intersections as a mechanism for coordination. 
Separate MASTraf implementations with similar state and reward functions using Q-learning and ADP were tested using a microscopic traffic simulator (VISSIM) and real-time manipulation of the traffic signals through the software’s COM interface. Testing was conducted to determine the performance of the agents in scenarios with increasing complexity, from a single intersection, to arterials and networks, both in undersaturated and oversaturated conditions. 
Results show that the multi-agent system provided by MASTraf improves its performance as the agents accumulate experience, and the system was able to efficiently manage the traffic signals of simple and complex scenarios. Exploration of the policies generated by MASTraf showed that the agents followed expected behavior by providing green to greater vehicle demands and accounting for the effects of blockages and lost time. The performance of MASTraf was on par with current state of practice tools for finding signal control settings, but MASTraf can also adapt to changes in demands and driver behavior by adjusting the signal timings in real-time, thus improving coordination and preventing queue spillbacks and green starvation.  
A strategy for signal coordination was also tested in one of the MASTraf implementations, showing increased throughput and reduced number of stops, as expected. The coordination employed a version of the max-plus algorithm embedded in the reward structure, acting as a bias towards improved coordination. The response of the system using imprecise detector data, in the form of coarse aggregation, showed that the system was able to handle oversaturation under such conditions. Even when the data had only 25% of the resolution of the original implementation, the system throughput was only reduced by 5% and the number of stops per vehicle was increased by 8%. 
The state and reward formulations allowed for a simple function approximation method in order to reduce the memory requirements for storing the state space, and also to create a form of generalization for states that have not been visited or that have not been experienced enough. Given the discontinuities in the reward function generated by penalties for blockages and lost times, the value approximation was conducted through a series of functions for each action and each of the conditions before and after a discontinuity. 
The policies generated using MASTraf with a function approximation were analyzed for different intersections in the network, showing agent behavior that reflected the principles formulated in the original problem using lookup tables, including right of way assignment based on expected rewards with consideration of penalties such as lost time. In terms of system performance, MASTraf with function approximation resulted in average reductions of 1% in the total system throughput and 3.6% increases in the number of stops per vehicle, when compared to the implementation using lookup tables on a congested network of 20 intersections",MASTraf: a decentralized multi-agent system for network-wide traffic signal control with dynamic coordination,,,,,core
389907284,2015-12-17T00:00:00,"Objective. This paper presents the selective particle swarm optimization and results of it applying to solve a problem of the optimal capacitor placement and sizing, configuration and conductor sizing simultaneously for a real distribution system with 3 substations, 37 feeders, 274 buses, 284 branches and 11 normally open switches located in the eastern part of Mariupol city. Technique. Selective particle swarm optimization as a simple modification of the binary particle swarm optimization to search in a selected decision space was used for solving the aforesaid problem formulated as a multi-objective mixed-integer combinatorial nonlinear optimization problem with equality and inequality constraints. To verify the effectiveness of the proposed algorithm, a plenty of test systems and real schemes with the different number of buses were used and selective particle swarm optimization was compared to another modern heuristic and artificial intelligence techniques (e.g., simulated annealing, genetic algorithms, ant colony search algorithm, fuzzy-reasoning approach, etc.). MATLAB R2010a was applied as a computational tool to realize the proposed algorithm for modeling of the real large-scale distribution system. Before starting simulation, the long-term measurements (during one week) of the active and reactive power, power factor, voltage distortion and unbalance were made by Fluke 435 power quality analyzer on the four feeders with the highest active power losses and voltage drop. Results. Simulation results before and after optimization demonstrated an effectiveness of the selective particle swarm optimization in reducing the both active power and energy losses, improving the voltage quality and receiving the total annual saving. A comparative study of the three abovementioned techniques (capacitor placement, reconfiguration and reconductoring) to optimize a performance of the large-scale distribution system was executed. Scientific novelty. The proposed algorithm has been implemented for an optimization procedure of the real large-scale distribution system to find the optimal capacitor placement and sizing, configuration and conductor sizing simultaneously accounting for technical constraints (maximum permissible branch current, maximum and minimum voltage limits, maximum total harmonic distortion limit and maximum permissible total size of capacitors) and operational constraints (load connectivity and radial network structure). Practical value. For investigated distribution system, applying of all three optimization techniques simultaneously with THD constraint would be reduce the active power losses from 7.4 % to 3.5 % and energy losses – from 5 % to 2.38 % (approximately twice). In this case, the total annual saving would be about $700000 and the total saving for remaining period – about $4.8 million.Цель. В работе приведены селективный метод роя частиц и результаты его применения для решения проблемы совместной оптимизации мест установки и мощности батарей конденсаторов, конфигурации и сечений проводников реальной распределительной сети, которая содержит 3 подстанции, 37 фидеров, 274 узла, 284 ветви и 11 нормально разомкнутых коммутационных аппаратов, и расположена в восточной части города Мариуполь. Методика. Селективный метод роя частиц, представляющий собой простую модификацию бинарного метода роя частиц, предназначенную для поиска в выбранном пространстве решений, был использован для решения вышеуказанной проблемы, сформулированной как проблема многокритериальной частично целочисленной комбинаторной нелинейной оптимизации с ограничениями в виде равенств и неравенств. Для подтверждения эффективности предложенного алгоритма были использованы многочисленные тестовые и реальные схемы с различным количеством узлов, а селективный метод роя частиц сравнили с другими современными эвристическими методами и методами искусственного интеллекта (например, имитация отжига, генетические алгоритмы, алгоритм колонии муравьёв, нечёткая логика и др.). Чтобы реализовать предложенный алгоритм для моделирования реальной разветвлённой сети, в качестве вычислительного средства была использована среда MATLAB R2010a. Перед началом моделирования с помощью анализатора качества электроэнергии Fluke 435 на четырёх фидерах с наибольшими потерями активной мощности и падением напряжения были проведены длительные (в течение одной недели) измерения активной и реактивной мощности, коэффициента мощности, несинусоидальности и несимметрии напряжений. Результаты. Результаты моделирования до и после оптимизации продемонстрировали эффективность применения селективного метода роя частиц для снижения потерь активной мощности и электроэнергии, улучшения качества напряжения и получения суммарной годовой экономии. Выполнено сравнительное исследование трёх приведенных выше методов оптимизации параметров разветвлённой распределительной сети (установка батарей конденсаторов, реконфигурация и замена сечений проводников). Научная новизна. Предложенный алгоритм был реализован для оптимизации параметров реальной разветвлённой распределительной сети с целью совместного определения оптимальных мест установки и мощности батарей конденсаторов, конфигурации и сечений проводников, учитывая технические ограничения (максимально допустимый ток, максимально и минимально допустимые напряжения, максимальное значение суммарного коэффициента гармонических составляющих по напряжению и максимально допустимое значение суммарной установленной мощности батарей конденсаторов) и эксплуатационные ограничения (отсутствие отключенных нагрузок и радиальная структура сети). Практическая значимость. Применение для исследуемой распределительной сети всех трёх методов оптимизации одновременно с учётом ограничения на максимальное значение суммарного коэффициента гармонических составляющих по напряжению позволило бы снизить потери активной мощности с 7,4 % до 3,5 %, а потери электроэнергии – с 5 % до 2,38 % (примерно в 2 раза). В этом случае суммарная годовая экономия составила бы около 700000 $, а суммарная экономия за остаточный период – около 4,8 млн. $.Мета. В роботі наведені селективний метод рою частинок та результати його застосування щодо вирішення проблеми спільної оптимізації місць встановлення та потужності батарей конденсаторів, конфігурації та перерізів провідників реальної розподільної мережі, яка містить 3 підстанції, 37 фідерів, 274 вузли, 284 гілки та 11 нормально розімкнених комутаційних апаратів і розташована у східній частині міста Маріуполь.Методика. Селективний метод рою частинок, який є простою модифікацією бінарного методу рою частинок, призначеною для пошуку у обраному просторі розв'язань, був застосований щодо вирішення зазначеної вище проблеми, сформульованої як проблема багатокритеріальної частково цілочислової комбінаторної нелінійноїоптимізації з обмеженнями у вигляді рівностей та нерівностей. Для підтвердження ефективності запропонованого алгоритму були використані численні тестові та реальні схеми з різною кількістю вузлів, а селективний метод рою частинок був порівняний з іншими сучасними евристичними методами та методами штучного інтелекту (наприклад, імітація відпалу, генетичні алгоритми, алгоритм колонії мурашів, нечітка логіка та ін.). Щоби реалізувати запропонований алгоритм для моделювання реальної розгалуженої розподільної мережі, у якості обчислювального засобу було застосовано середовище MATLAB R2010a. Перед початком моделювання за допомогою аналізатору якості електроенергії Fluke 435 на чотирьох фідерах з найбільшими втратами активної потужності та спадом напруги були виконані тривалі (протягом одного тижня) вимірювання активної та реактивної потужності, коефіцієнту потужності, несинусоїдності та несиметрії напруг. Результати. Результати моделювання до та після оптимізації продемонстрували ефективність застосування селективного методу рою частинок для зниження втрат активної потужності та електроенергії, покращення якості напруги та отримання сумарної річної економії. Виконано порівняльне дослідження трьох зазначених вище методів оптимізації параметрів розгалуженої розподільної мережі (встановлення батарей конденсаторів, реконфігурація та заміна перерізів провідників). Наукова новизна. Запропонований алгоритм був реалізований для оптимізації параметрів реальної розгалуженої розподільної мережі з метою спільного знаходження оптимальних місць встановлення та потужності батарей конденсаторів, конфігурації та перерізів провідників, враховуючи технічні обмеження (максимально припустимий струм, максимально та мінімально припустимі напруги, максимальне значення сумарного коефіцієнту гармонічних складових за напругою та максимально припустиме значення сумарної встановленої потужності батарей конденсаторів) та експлуатаційні обмеження (відсутність вимкнених навантажень та радіальна структура мережі). Практична значимість. Застосування для досліджуваної розподільної мережі всіх трьох методів оптимізації разом з урахуванням обмеження на максимальне значення сумарного коефіцієнту гармонічних складових за напругою дозволило би знизити втрати активної потужності з 7,4 % до 3,5 %, а втрати електроенергії – з 5 % до 2,38 % (приблизно у 2 рази). У цьому випадку сумарна щорічна економія склала би приблизно 700000 $, а сумарна економія за залишковий період – приблизно 4,8 млн. $",Застосування селективного методу рою частинок для оптимізації режимів та структури реальної розгалуженої розподільної мережі з метою зниження втрат електроенергії та покращення якості напруги,,Dnipropetrovsk National University of Railway Transport named after Acad. V.  Lazaryan,,,core
27009758,2013-07-01T00:00:00Z,"Traditionally, animal species diversity and abundance is assessed using a variety of methods that are generally costly, limited in space and time, and most importantly, they rarely include a permanent record. Given the urgency of climate change and the loss of habitat, it is vital that we use new technologies to improve and expand global biodiversity monitoring to thousands of sites around the world. In this article, we describe the acoustical component of the Automated Remote Biodiversity Monitoring Network (ARBIMON), a novel combination of hardware and software for automating data acquisition, data management, and species identification based on audio recordings. The major components of the cyberinfrastructure include: a solar powered remote monitoring station that sends 1-min recordings every 10 min to a base station, which relays the recordings in real-time to the project server, where the recordings are processed and uploaded to the project website (arbimon.net). Along with a module for viewing, listening, and annotating recordings, the website includes a species identification interface to help users create machine learning algorithms to automate species identification. To demonstrate the system we present data on the vocal activity patterns of birds, frogs, insects, and mammals from Puerto Rico and Costa Rica",Real-time bioacoustics monitoring and automated species identification,,PeerJ Inc.,"[{'title': None, 'identifiers': ['issn:2167-8359', '2167-8359']}]",10.7717/peerj.103,core
33162260,2013-03-26T00:00:00,"This thesis deals with Statistical Relational Learning (SRL), a research area combining principles and ideas from three important subfields of Artificial Intelligence: machine learn- ing,  knowledge representation and reasoning on uncertainty.  Machine learning is the study of systems that improve their behavior over time with experience; the learning process typi- cally involves a search through various generalizations of the examples, in order to discover regularities or classification rules. A wide variety of machine learning techniques have been developed in the past fifty years, most of which used propositional logic as a (limited) represen- tation language. Recently, more expressive knowledge representations have been considered, to cope with a variable number of entities as well as the relationships that hold amongst them. These representations are mostly based on logic that, however, has limitations when reason- ing on uncertain domains. These limitations have been lifted allowing a multitude of different formalisms combining probabilistic reasoning with logics,  databases or logic programming, where probability theory provides a formal basis for reasoning on uncertainty.



In this thesis we consider in particular the proposals for integrating probability in Logic Programming, since the resulting probabilistic logic programming languages present very in- teresting computational properties.  In Probabilistic Logic Programming, the so-called ""dis- tribution  semantics"" has gained a wide popularity.  This semantics was introduced for the PRISM language (1995) but is shared by many other languages: Independent Choice Logic, Stochastic Logic Programs, CP-logic, ProbLog and Logic Programs with Annotated Disjunc- tions (LPADs).  A program in one of these languages defines a probability distribution over normal logic programs called worlds.  This distribution is then extended to queries and the probability of a query is obtained by marginalizing the joint distribution of the query and the programs. The languages following the distribution semantics differ in the way they define the distribution over logic programs.

The first part of this dissertation presents techniques for learning probabilistic logic pro- grams  under the distribution semantics.  Two problems are considered:  parameter learning and structure learning, that is, the problems of inferring values for the parameters or both the structure and the parameters of the program from data.  This work contributes an algorithm for parameter learning, EMBLEM, and two algorithms for structure learning (SLIPCASE and SLIPCOVER) of probabilistic logic programs (in particular LPADs). EMBLEM is based on the Expectation Maximization approach and computes the expectations directly on the Binary De- cision Diagrams that are built for inference. SLIPCASE performs a beam search in the space of LPADs while SLIPCOVER performs a beam search in the space of probabilistic clauses and a greedy search in the space of LPADs, improving SLIPCASE performance. All learning approaches have been evaluated in several relational real-world domains.



The second part of the thesis concerns the field of Probabilistic Description Logics, where we consider a logical framework suitable for the Semantic Web. Description Logics (DL) are a family of formalisms for representing knowledge. Research in the field of knowledge repre- sentation and reasoning is usually focused on methods for providing high-level descriptions of the world that can be effectively used to build intelligent applications.

Description Logics have been especially effective as the representation language for for- mal ontologies. Ontologies model a domain with the definition of concepts and their properties and relations.   Ontologies are the structural frameworks for organizing information and are used in artificial intelligence, the Semantic Web, systems engineering, software engineering, biomedical informatics, etc. They should also allow to ask questions about the concepts and in- stances described, through inference procedures. Recently, the issue of representing uncertain information in these domains has led to probabilistic extensions of DLs.



The contribution of this dissertation is twofold: (1) a new semantics for the Description Logic SHOIN(D) , based on the distribution semantics for probabilistic logic programs, which embeds probability; (2) a probabilistic reasoner for computing the probability of queries from uncertain knowledge bases following this semantics. The explanations of queries are encoded in Binary Decision Diagrams, with the same technique employed in the learning systems de- veloped for LPADs. This approach has been evaluated on a real-world probabilistic ontology",Integration of Logic and Probability in Terminological and Inductive Reasoning,https://core.ac.uk/download/33162260.pdf,,,10.5072//843,core
33802013,2015-01-26,"In video surveillance, decision support systems rely more and more on face recognition (FR) to rapidly determine if facial regions captured over a network of cameras correspond to individuals of interest. Systems for FR in video surveillance are applied in a range of scenarios, for instance in watchlist screening, face re-identification, and search and retrieval. The focus of this Thesis is video-to-video FR, as found in face re-identification applications, where facial models are designed on reference data, and update is archived on operational captures from video streams. Several challenges emerge from the task of recognizing individuals of interest from faces captured with video cameras. Most notably, it is often assumed that the facial appearance of target individuals do not change over time, and the proportions of faces captured for target and non-target individuals are balanced, known a priori and remain fixed. However, faces captured during operations vary due to several factors, including illumination, blur, resolution, pose expression,  and camera interoperability. In addition, facial models used matching are commonly not representative since they are designed a priori, with a limited amount of reference samples that are collected and labeled at a high cost. Finally, the proportions of target and non-target individuals continuously change during operations.



In literature, adaptive multiple classifier systems (MCSs) have been successfully applied to video-to-video FR, where  the facial model for each target individual is designed using an ensemble of 2-class classifiers (trained using target vs. non-target reference samples). Recent approaches employ ensembles of 2-class Fuzzy ARTMAP classifiers, with a DPSO strategy to generate a pool of classifiers with optimized hyperparameters, and Boolean combination to merge their responses in the ROC space. Besides, the skew-sensitive ensembles were recently proposed to adapt the fusion function of an ensemble according to class imbalance measured on operational data. These active approaches estimate target vs. non-target proportions periodically during operations distance, and the fusion of classifier ensembles are adapted to such imbalance. Finally, face tracking can be used to regroup the system responses linked to a facial trajectory (facial captures from a single person in the scene) for robust spatio-temporal recognition, and to update facial models over time using operational data.



In this Thesis, new techniques are proposed to adapt the facial models for individuals enrolled to a video-to-video FR system. Trajectory-based self-updating is proposed to update the system, considering gradual and abrupt changes in the classification environment. Then, skew-sensitive ensembles are proposed to adapt the system to the operational imbalance.



In Chapter 2, an adaptive framework is proposed for partially-supervised learning of facial models over time based on facial trajectories. During operations, information from a face tracker and individual-specific ensembles is integrated for robust spatio-temporal recognition and for self-update of facial models. The tracker defines a facial trajectory for each individual in video. Recognition of a target individual is done if the positive predictions accumulated along a trajectory surpass a detection threshold for an ensemble. If the accumulated positive predictions surpass a higher update threshold, then all target face samples from the trajectory are combined with non-target samples (selected from the cohort and universal models) to update the corresponding facial model. A learn-and-combine strategy is employed to avoid knowledge corruption during self-update of ensembles. In addition, a memory management strategy based on Kullback-Leibler divergence is proposed to rank and select the most relevant target and non-target reference samples to be stored in memory as the ensembles evolves. The proposed system was validated with synthetic data and real videos from Face in Action dataset, emulating a passport checking scenario. Initially, enrollment trajectories were used for supervised learning of ensembles, and videos from three capture sessions were presented to the system for FR and self-update.  Transaction-level analysis shows that the proposed approach outperforms baseline systems that do not adapt to new trajectories, and provides comparable performance to ideal systems that adapt to all relevant target trajectories, through supervised learning. Subject-level analysis reveals the existence of individuals for which self-updated ensembles provide a considerable benefit. Trajectory-level analysis indicates that the proposed system allows for robust spatio-temporal video-to-video FR.



In Chapter 3, an extension and a particular implementation of the ensemble-based system for spatio-temporal FR is proposed, and is characterized in scenarios with gradual and abrupt changes in the classification environment. Transaction-level results show that the proposed system allows to increase AUC accuracy by about 3% in scenarios with abrupt changes, and by about 5% in scenarios with gradual changes. Subject-based analysis reveals the difficulties of FR with different poses, affecting more significantly the lamb- and goat-like individuals. Compared to reference spatio-temporal fusion approaches, the proposed accumulation scheme produces the highest discrimination.



In Chapter 4, adaptive skew-sensitive ensembles are proposed to combine classifiers trained by selecting data with varying levels of imbalance and complexity, to sustain a high level the performance for video-to-video FR. During operations, the level of imbalance is periodically estimated from the input trajectories using the HDx quantification method, and pre-computed histogram representations of imbalanced data distributions. Ensemble scores are accumulated of trajectories for robust skew-sensitive spatio-temporal recognition. Results on synthetic data show that adapting the fusion function with the proposed approach can significantly improve performance. Results on real data show that the proposed method can outperform reference techniques in imbalanced video surveillance environments",Adaptive multi-classifier systems for face re-identification applications,https://core.ac.uk/download/pdf/33802013.pdf,École de technologie supérieure,,,core
38941469,2015-01-01T08:00:00,"Writing English research article (RA) abstracts is a difficult but mandatory task for Taiwanese engineering graduate students (Feng, 2013). Understanding the current situation and needs of Taiwanese engineering graduate students, this dissertation aimed to develop and evaluate an automated writing evaluation (AWE) tool to assist their research article (RA) abstract writing in English by following a Design-Based Research (DBR) approach as the methodological framework. DBR was chosen because it strives to solve real-world problems through multiple iterations of development and building on results from each iteration to advance the project.
Six design iterations were undertaken to develop and to evaluate the AWE tool in this dissertation, including (1) corpus compilation of engineering RAs, (2) genre analysis of engineering abstracts, (3) machine learning of move classification in abstracts, (4) analysis of lexical bundles used to express moves, (5) analysis of the choice of verb categories associated with moves, and finally, (6) AWE tool development based on previous findings, classroom implementation, and evaluation of the AWE tool following Chapelle’s (2001) computer-assisted language learning (CALL) framework.
To begin with, I collected a corpus of 480 engineering RAs (Corpus-480) to extract appropriate linguistic properties as pedagogical materials to be implemented in the AWE tool. A sub-corpus (Corpus-72) was compiled with 72 RAs randomly chosen from Corpus-480 for manual and automated analyses. Next, to seek the best descriptive framework for the structure of engineering RA abstracts, two move schemata were compared: (1) IMRD (Introduction, Methodology, Results, and Discussion) and (2) CARS (Create-A-Research-Space, Swales, 1990). Abstracts in Corpus-72 were annotated and these two schemas were evaluated according to three quantitative metrics devised specifically for this comparison.
Applying a statistical natural language processing (StatNLP) approach, a Support Vector Machine (SVM) was trained for automated move classification in abstracts. Formulaic language in engineering RA sections was used as linguistic features to automatically classify moves in abstracts. Additionally, four-word lexical bundles and verb categories were identified from Corpus-480 and Corpus-72, respectively. Four-word lexical bundles associated with moves in abstracts were extracted automatically. Additionally, verb categories (i.e., tense, aspect, and voice) in moves of abstracts were identified using CyWrite::Analyzer, a hybrid (statistical and rule-based) NLP software.
Finally, the AWE tool was developed, based on the findings from the previous iterations, and implemented in an English-as-a-foreign-language (EFL) classroom setting. Through analyzing students’ drafts before and after using the tool, and responses to a questionnaire and a semi-structured interview, the AWE tool was evaluated based on Chapelle’s (2001) CALL evaluation framework. The findings showed that students attempted to improve their abstracts by adding, deleting, or changing the sequences of their sentences, lexical bundles, and verb categories in their abstracts. Their attitudes toward the effectiveness and appropriateness of the tool were quite positive. Overall, the AWE tool drew students’ attention to the use of lexical bundles and verb categories to achieve the communicative purposes of each move in their abstracts.
In conclusion, this dissertation started from Taiwanese engineering students’ needs to improve their English abstract writing, and attempted to develop and evaluate an AWE tool for assisting them. Following DBR, the findings from this dissertation are discussed to improve the next generation of the AWE tools. Having these iterations in place, future studies can focus on developing pedagogical materials from genre-based analysis in different disciplines to fulfill learners’ needs","Designing, implementing, and evaluating an automated writing evaluation tool for improving EFL graduate students’ abstract writing: a case in Taiwan",https://core.ac.uk/download/38941469.pdf,Iowa State University Digital Repository,,,core
43897902,2015-08-01T00:00:00Z,"In this article, we introduce the Bioinspired Neuroprosthetic Design Environment (BNDE) as a practical platform for the development of novel brain machine interface (BMI) controllers which are based on spiking model neurons. We built the BNDE around a hard real-time system so that it is capable of creating simulated synapses from extracellularly recorded neurons to model neurons. In order to evaluate the practicality of the BNDE for neuroprosthetic control experiments, a novel, adaptive BMI controller was developed and tested using real-time closed-loop simulations. The present controller consists of two in silico medium spiny neurons which receive simulated synaptic inputs from recorded motor cortical neurons. In the closed-loop simulations, the recordings from the cortical neurons were imitated using an external, hardware-based neural signal synthesizer. By implementing a reward-modulated spike timing-dependent plasticity rule, the controller achieved perfect target reach accuracy for a two target reaching task in one dimensional space. The BNDE combines the flexibility of software-based spiking neural network (SNN) simulations with powerful online data visualization tools and is a low-cost, PC-based and all-in-one solution for developing neurally-inspired BMI controllers. We believe the BNDE is the first implementation which is capable of creating hybrid biological/in silico neural networks for motor neuroprosthetic control and utilizes multiple CPU cores for computationally intensive real-time SNN simulations",Towards building hybrid biological/in silico neural networks for motor neuroprosthetic control,,Frontiers Media S.A.,"[{'title': None, 'identifiers': ['issn:1662-5218', '1662-5218']}]",10.3389/fnbot.2015.00008/full,core
56782979,2014,"In contemporary society the appearance becomes more and more important. Accordingly, inappropriate color characteristics of dentures are critical. In practice, shade is usually determined by using color rings. But this is highly error-prone. This work deals with the development of a method for automatic determination of tooth color using spectral data and its integration in the spectral Smart Sensor VITA Easyshade. The core element is the implementation of a multi-stage predictive model for the tooth color on the basis of artificial neural networks. The model is trained by means of real measurement data. As preliminary process a method has been developed to evaluate the real data also regarding their suitability for training the model. Moreover an automatic eject rejection of inconsistent data can be done. To increase the prediction quality a method was designed and implemented to perform a plausibility check through neighborhoods in the color space. For the evaluation of the selected solutions almost 4000 spectral measurements of teeth from volunteers have been acquired. The validation of the method showed that the prediction accuracy could be raised by almost 40%, compared to conventional methods",Entwicklung eines robusten Verfahrens zur Farbbestimmung von Zähnen auf Basis spektraler Daten,,"Fraunhofer Verlag, Stuttgart",,,core
100889040,2015-01-01,"Assuring quality of service (QoS) requirements is critical when assembling a distributed real-time and embedded (DRE) system from a repository of existing software and hardware components. This paper presents a two-level approach for assuring satisfaction of QoS requirements in the context of a reduced design space for DRE systems. Techniques from artificial intelligence and statistics are used to fulfill these collective objectives at system assembly time. The result not only lessens the overhead of validation of QoS require-ments at run-time, but also reduces the development and integration cost of DRE systems",Poster Paper Two-Level Assurance of QoS Requirements for Distributed Real-time and Embedded Systems ∗,,,,,core
100217941,2014-12-08,"ABSTRACT: Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic architectural environments to be implemented and tested in the last decade in virtual and physical prototypes. These prototypes are incorporating sensing-actuating mechanisms that enable interaction with their users and surroundings in real-time. While these prototypes obviously point towards a paradigm shift from inanimate towards animate architecture, they do not operate at building but at building component scale and do not address socio-economical or environmental aspects that affect architecture and society at large. This paper, on the one hand, critically discusses robotic prototypes built in the last decade at Delft University of Technology, on the other hand, it proposes a framework for future research envisioning robotic environments, as resizable, able to spatially expand or contract as well as move or be moved as needed. Such reconfigurable environments aim to validate the assumption that robotics incorporated in architecture improve efficiency of use due to multiple use of built space in condensed timeframes, while at the same time they advance technology for distributed autonomous robotic systems exhibiting collective behavior as well as test their application to sustainable architecture",ROBOTIC ENVIRONMENTS,,,,,core
158142979,2014-03-01,"This paper describes the design of high performance packet filtering firewall using embedded system. An FPGA (field programmable gate array) platform has been used for implementation and analysing the network firewall. It is capable of accepting real time changes. This network security application has an ability to perform powerful protection against unwanted data packets such as virus attack, spam in e-mails, hackers, worms, spyware unauthorized contents. However the firewalls don’t address the difficulty of unwanted data packets intrusion. The ultimate aim of this work is to create a systematic way of approach for unwanted packets discard in a network system. We use a specially trained algorithms such as Wu-manber algorithms (high performance, multi-pattern matching), bloom filter algorithm (space efficient data structure for testing an element in the set.Our design is mainly based on machine learning and artificial intelligence. This gives a high efficiency, improved performance and high ability of packet detection with less contribution of time in an effective way",FPGA Based Firewall using Embedded Processor for Vulnarability Packet Detection,,Institute of Advanced Engineering and Science,,,core
197914177,2015-08-01T00:00:00,"International audienceNetwork anomalies and attacks represent a serious challenge to ISPs, who need to cope with an increasing number of unknown events that put their networks' integrity at risk. Most of the network anomaly detection systems proposed so far employ a supervised strategy to accomplish their task, using either signature-based detection methods or supervised-learning techniques. The former fails to detect unknown anomalies, exposing the network to severe consequences; the latter requires labeled traffic, which is difficult and expensive to produce. In this paper we introduce a powerful unsupervised approach to detect and characterize network anomalies in the dark, i.e., without relying on signatures or labeled traffic. Unsupervised detection is accomplished by means of robust clustering techniques , combining sub-space clustering with correlation analysis to blindly identify anomalies. To alleviate network operator's post-processing tasks and to speed-up the deployment of effective countermeasures, anomaly ranking and characterization are automatically performed on the detected events. The system is extensively tested with real traffic from the WIDE backbone network, spanning six years of flows captured from a trans-pacific link between Japan and the US, using the MAWILab framework for ground-truth generation. We additionally evaluate the proposed approach with synthetic data, consisting of traffic from an operational network with synthetic attacks. Finally, we compare the performance of the unsupervised detection against different previously used unsupervised detection techniques, as well as against multiple anomaly detectors used in MAWILab",Hunting Attacks in the Dark: Clustering and Correlation Analysis for Unsupervised Anomaly Detection,,'Wiley',,10.1002/nem.1903,core
144069861,2013-02-18T00:00:00,"Dentre as técnicas de posicionamento utilizando os sistemas de navegação por satélite globais (GNSS - Global Navigation Satellite Systems), merece destaque a que utiliza dados de uma rede de estações GNSS para gerar estações de referência virtuais. Desde que as estações da rede não estejam separadas por mais de 100 km e o receptor do usuário esteja dentro da região interna à rede de referência, esta técnica de posicionamento pode proporcionar posicionamento com precisão melhor que 10 cm a usuários de receptores de uma frequência. No entanto, o posicionamento em tempo-real pode ser inviabilizado caso ocorra problema de comunicação com as estações da rede de referência. Tendo em vista a relação do conteúdo total de elétrons (TEC - Total Electron Content) com o atraso ionosférico de primeira ordem, esta pesquisa apresenta uma forma de se prever 72 horas do TEC na direção vertical (VTEC - Vertical Total Electron Content)  regionalmente com a arquitetura de redes neurais artificiais (RNA) denominada  perceptrons de múltiplas camadas (MLP  MultiLayer Perceptrons). A metodologia de previsão do VTEC proposta foi empregada na geração de estações de referência virtuais, onde arquivos de previsão do atraso  troposférico zenital, produzidos pelo Instituto Nacional de Pesquisas Espaciais (INPE), foram utilizados para considerar o atraso provocado pela atmosfera neutra e as efemérides preditas pelo serviço internacional do GNSS (IGS - International GNSS Service) foram empregadas para calcular a posição dos satélites. As RNA foram treinadas e avaliadas com dados de VTEC extraídos dos mapas da ionosfera globais (GIM - Global Ionospheric Map) produzidos pelo IGS e dos arquivos produzidos com o software Mod_Ion, ambos no formato IONEX (IONosphere Map EXchange), mostrando que o VTEC pode ser previsto por 72 horas com diferença média quadrática (RMS  Root Mean Square) que varia de 1,2 unidades de TEC (TECU - TEC Units) a 12,5 TECU, em baixa e alta atividade solar, respectivamente. Dezoito linhas de base, localizadas no oeste do Estado de São Paulo, foram calculadas utilizando estações de referência virtuais e estações de referência reais, verificando-se que o posicionamento relativo tridimensional empregando a metodologia proposta apresentou RMS de aproximadamente 46 cm. Quando avaliada no posicionamento absoluto preciso (PPP  Precise Point Positioning), o RMS relacionado com o posicionamento tridimensional foi de 26 cm.The positioning technique that uses data from a network of GNSS reference stations to generate virtual reference stations should be detached among the Global Navigation Satellite Systems (GNSS) positioning techniques. Since the inter reference station distances are up to 100 km and the user receiver is within the internal region of the network, this technique can provide single frequency receiver users positioning with better accuracy than 10 cm. However, real-time positioning can be impracticable if communication breakdown involving such reference stations occurs. Given the relation between the Total Electron Content (TEC) and the first-order ionospheric delay, this research presents a way to predict 72 hours of vertical TEC (VTEC) regionally using the Artificial Neural Networks (ANN) architecture called MultiLayer Perceptorns (MLP). The proposed VTEC prediction methodology was employed in the generation of virtual reference stations, where files of prediction of zenithal tropospheric delay, produced by the National Institute For Space Research (INPE  Instituto de Pesquisas Espaciais), were used to take the neutral atmospheric delay into account and the precise ephemeris predicted by the GNSS International Service (GNSS) were employed to compute satellites positioning. ANN were trained and assessed using VTEC data from the Global Ionospheric Maps (GIM) produced by IGS and the files produced by Mod_Ion software, both in IONEX (IONosphere Map EXchange) format, showed VTEC can be predicted for 72 hours with Root Mean Square difference (RMS) of about 1.2 TEC units (TECU) and 12.5 TECU, respectively, in low solar activity and high solar activity. Eighteen baselines, in the west region of Sao Paulo State, were computed using virtual reference stations and real reference stations, verifying that the three-dimensional relative positioning using the proposed methodology showed RMS of 46 cm. When assessed by precise point positioning (PPP), the three-dimensional RMS positioning was of 26 cm",Use of artiificial neural networks to predict VTEC aiming to generate virtual reference stations in real-time.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,10.11606/T.3.2012.tde-08022013-155136,core
296643354,2015-11-26T15:02:15,"Euclidean distance geometry is the study of Euclidean geometry based on the concept of distance. This is useful in several applications where the input data consist of an incomplete set of distances and the output is a set of points in Euclidean space realizing those given distances. We survey the theory of Euclidean distance geometry and its most important applications, with special emphasis on molecular conformation problems. © 2014 Society for Industrial and Applied Mathematics.561369Alexandrov, A., (1950) Convex Polyhedra, Gosudarstv. Izdat. Tekhn.-Theor. Lit., , MoscowAlfakih, A., Khandani, A., Wolkowicz, H., Solving Euclidean distance matrix completion problems via semidefinite programming (1999) Comput. Optim. Appl., 12, pp. 13-30Alves, R., Cassioli, A., Mucherino, A., Lavor, C., Liberti, L., Adaptive branching in iBP with Clifford algebra (2013) Proceedings of the Workshop on Distance Geometry and Applications, pp. 65-69. , A. Andrioni, C. Lavor, L. Liberti, A. Mucherino, N. Maculan, and R. Rodriguez, eds., Universidade Federal do Amazonas, ManausAnderson, B., Belhumeur, P., Eren, T., Goldenberg, D., Morse, S., Whiteley, W., Yang, R., Graphical properties of easily localizable sensor networks (2009) Wireless Networks, 15, pp. 177-191Arabie, P., Was Euclid an unnecessarily sophisticated psychologist? (1991) Psychometrika, 56, pp. 567-587Asimow, L., Roth, B., The rigidity of graphs (1978) Trans. Amer. Math. Soc., 245, pp. 279-289Asimow, L., Roth, B., The rigidity of graphs II (1979) J. Math. Anal. Appl., 68, pp. 171-190Aspnes, J., Eren, T., Goldenberg, D., Morse, S., Whiteley, W., Yang, R., Anderson, B., Belhumeur, P., A theory of network localization (2006) IEEE Trans. Mobile Comput., 5, pp. 1663-1678Aspnes, J., Goldenberg, D., Yang, R., On the computational complexity of sensor network localization (2004) Algorithmic Aspects of Wireless Sensor Networks, pp. 32-44. , S. Nikoletseas and J. Rolim, eds., Lecture Notes in Comput. Sci. 3121, Springer, BerlinAuslander, L., Mackenzie, R., (1977) Introduction to Differentiable Manifolds, , Dover, New YorkBahr, A., Leonard, J., Fallon, M., Cooperative localization for autonomous underwater vehicles (2009) Internat. J. Robotics Res., 28, pp. 714-728Barvinok, A., Problems of distance geometry and convex properties of quadratic maps (1995) Discrete Comput. Geom., 13, pp. 189-202Belkin, M., Niyogi, P., Laplacian eigenmaps for dimensionality reduction and data representation (2003) Neural Comput., 15, pp. 1373-1396Belotti, P., Lee, J., Liberti, L., Margot, F., Wächter, A., Branching and bounds tightening techniques for non-convex MINLP (2009) Optim. Methods Softw., 24, pp. 597-634Ben-Israel, A., Mond, B., What is invexity? (1986) J. Aust. Math. Soc. Ser. B, 28, pp. 1-9Benedetti, R., Risler, J.-J., (1990) Real Algebraic and Semi-algebraic Sets, , Hermann, ParisBerger, B., Kleinberg, J., Leighton, T., Reconstructing a three-dimensional model with arbitrary errors (1999) J. ACM, 46, pp. 212-235Berman, H., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov, I., Bourne, P., The protein data bank (2000) Nucleic Acid Res., 28, pp. 235-242Biggs, N., (1974) Algebraic Graph Theory, , Cambridge University Press, Cambridge, UKBiggs, N., Lloyd, E., Wilson, R., (1976) Graph Theory 1736-1936, , Oxford University Press, OxfordBiswas, P., (2007) Semidefinite Programming Approaches to Distance Geometry Problems, , Ph.D. thesis, Stanford University, Stanford, CABiswas, P., Lian, T., Wang, T., Ye, Y., Semidefinite programming based algorithms for sensor network localization (2006) ACM Trans. Sensor Networks, 2, pp. 188-220Biswas, P., Liang, T.-C., Toh, K.-C., Wang, T.-C., Ye, Y., Semidefinite programming approaches for sensor network localization with noisy distance measurements (2006) IEEE Trans. Automation Sci. Engrg., 3, pp. 360-371Biswas, P., Toh, K.-C., Ye, Y., A distributed SDP approach for large-scale noisy anchorfree graph realization with applications to molecular conformation (2008) SIAM J. Sci. Comput., 30, pp. 1251-1277Biswas, P., Ye, Y., Semidefinite programming for ad hoc wireless sensor network localization (2004) Proceedings of the 3rd International Symposium on Information Processing in Sensor Networks (IPSN04), pp. 46-54. , ACM, New YorkBiswas, P., Ye, Y., A distributed method for solving semidefinite programs arising from ad hoc wireless sensor network localization (2006) Multiscale Optimization Methods and Applications, pp. 69-84. , W. Hager et al., eds., Nonconvex Optim. Appl. 82, Springer, New YorkBjörner, A., Las Vergnas, M., Sturmfels, B., White, N., Ziegler, G., (1993) Oriented Matroids, , Cambridge University Press, Cambridge, UKBlumenthal, L., (1953) Theory and Applications of Distance Geometry, , Oxford University Press, OxfordBohr, H., Brunak, S., (1996) Protein Folds: A Distance Based Approach, , CRC Press, Boca Raton, FLBokowski, J., Sturmfels, B., On the coordinatization of oriented matroids (1986) Discrete Comput. Geom., 1, pp. 293-306Borg, I., Groenen, P., (2010) Modern Multidimensional Scaling, , 2nd ed., Springer, New YorkBoyd, S., El Ghaoui, L., Feron, E., Balakrishnan, V., (1994) Linear Matrix Inequalities in System and Control Theory, , SIAM, PhiladelphiaBreu, H., Kirkpatrick, D., Unit disk graph recognition is NP-hard (1998) Comput. Geom., 9, pp. 3-24Canny, J., Emiris, I., A subdivision-based algorithm for the sparse resultant (2000) J. ACM, 47, pp. 417-451Carroll, J., Chang, J., Analysis of individual differences in multidimensional scaling via an n-way generalization of ""eckart-Young"" decomposition (1970) Psychometrika, 35, pp. 283-319Carvalho, R., Lavor, C., Protti, F., Extending the geometric build-up algorithm for the molecular distance geometry problem (2008) Inform. Process. Lett., 108, pp. 234-237Cauchy, A.-L., Sur les polygones et les polyèdres (1813) J. Ecole Polytechnique, 16, pp. 87-99Cayley, A., A theorem in the geometry of position (1841) Cambridge Math. J., 2, pp. 267-271Chen, H., (2012) Distance Geometry for Kissing Balls, , preprint, arXiv:1203.2131v2Chevalley, C., (1955) The Construction and Study of Certain Important Algebras, , The Mathematical Society of Japan, TokyoClark, B., Colburn, C., Johnson, D., Unit disk graphs (1990) Discrete Math., 86, pp. 165-177Clore, G., Gronenborn, A., Determination of three-dimensional structures of proteins and nucleic acids in solution by nuclear magnetic resonance spectroscopy (1989) Critical Reviews in Biochemistry and Molecular Biology, 24, pp. 479-564Connelly, R., A counterexample to the rigidity conjecture for polyhedra (1978) Inst. Hautes Études Sci. Publ. Math., 47, pp. 333-338Connelly, R., On generic global rigidity (1991) Applied Geometry and Discrete Mathematics, DIMACS Ser. Discrete Math. Theoret. Comput. Sci. 4, pp. 147-155. , AMS, Providence, RIConnelly, R., Generic global rigidity (2005) Discrete Comput. Geom., 33, pp. 549-563Conway, J., Sloane, N., (1993) Sphere Packings, , Lattices and Groups, Springer, BerlinCoope, I., Reliable computation of the points of intersection of n spheres in Rn (2000) Aust. N. Z. Indust. Appl. Math. J., 42, pp. C461-C477Costa, V., Lavor, C., Mucherino, A., Cassioli, A., Carvalho, L., Maculan, N., Discretization orders for protein side chains J. Global Optim., , to appearCremona, L., (1872) Le Figure Reciproche Nella Statica Grafica, , G. Bernardoni, MilanoCremona, L., (1874) Elementi di Calcolo Grafico, , Paravia, TorinoCrippen, G., Distance geometry for realistic molecular conformations (2013) Distance Geometry: Theory, Methods, and Applications, pp. 315-328. , A. Mucherino, C. Lavor, L. Liberti, and N. Maculan, eds., Springer, New YorkCrippen, G., Havel, T., (1988) Distance Geometry and Molecular Conformation, , Wiley, New YorkCrum Brown, A., On the theory of isomeric compounds (1864) Trans. Roy. Soc. Edinburgh, 23, pp. 707-719Cucuringu, M., Lipman, Y., Singer, A., Sensor network localization by eigenvector synchronization over the Euclidean group (2012) ACM Trans. Sensor Networks, 8, pp. 1-42Cucuringu, M., Singer, A., Cowburn, D., Eigenvector synchronization, graph rigidity and the molecule problem (2012) Inform. Inference, 1, pp. 21-67Dattorro, J., (2005) Convex Optimization and Euclidean Distance Geometry, , Mß oo, Palo AltoDattorro, J., Equality relating Euclidean distance cone to positive semidefinite cone (2008) Linear Algebra Appl., 428, pp. 2597-2600De Leeuw, J., Heiser, W., Theory of multidimensional scaling (1982) Classification Pattern Recognition and Reduction of Dimensionality, pp. 285-316. , P. Krishnaiah and L. Kanal, eds., Handbook of Statist. 2, ElsevierDemaine, E., Gomez-Martin, F., Meijer, H., Rappaport, D., Taslakian, P., Toussaint, G., Winograd, T., Wood, D., The distance geometry of music (2009) Comput. Geom., 42, pp. 429-454Deza, M., Deza, E., (2009) Encyclopedia of Distances, , Springer, BerlinDiestel, R., (2005) Graph Theory, , Springer, New YorkDirac, G., On rigid circuit graphs (1961) Abh. Math. Sem. Univ. Hamburg, 25, pp. 71-76Doherty, L., Pister, K., El Ghaoui, L., Convex position estimation in wireless sensor networks (2001) Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies, Vol. 3 of INFOCOM, IEEE, pp. 1655-1663Donald, B., (2011) Algorithms in Structural Molecular Biology, , MIT Press, BostonDong, Q., Wu, Z., A linear-time algorithm for solving the molecular distance geometry problem with exact inter-atomic distances (2002) J. Global Optim., 22, pp. 365-375Dong, Q., Wu, Z., A geometric build-up algorithm for solving the molecular distance geometry problem with sparse distance data (2003) J. Global Optim., 26, pp. 321-333Dress, A., Havel, T., Distance geometry and geometric algebra (1993) Found. Phys., 23, pp. 1357-1374Dzemyda, G., Kurasova, O., Žilinskas, J., (2013) Multidimensional Data Visualiation: Methods and Applications, , Springer, New YorkDzhafarov, E., Colonius, H., Reconstructing distances among objects from their discriminability (2006) Psychometrika, 71, pp. 365-386Eaton, J., (2002) GNU Octave Manual, , Network Theory LimitedEckart, C., Young, G., The approximation of one matrix by another of lower rank (1936) Psychometrika, 1, pp. 211-218Emiris, I., Mourrain, B., Computer algebra methods for studying and computing molecular conformations (1999) Algorithmica, 25, pp. 372-402Eren, T., Goldenberg, D., Whiteley, W., Yang, Y., Morse, A., Anderson, B., Belhumeur, P., Rigidity, computation, and randomization in network localization (2004) IEEE Infocom Proc., 4, pp. 2673-2684Everitt, B., Rabe-Hesketh, S., (1997) The Analysis of Proximity Data, , Arnold, LondonFeferman, S., Dawson, J., Kleene, S., Moore, G., Solovay, R., Van Heijenoort, J., (1986) Kurt Gödel: Collected Works, 1. , Oxford University Press, OxfordFekete, Z., Jordán, T., Uniquely localizable networks with few anchors (2006) Algorithmic Aspects of Wireless Sensor Networks, pp. 176-183. , S. Nikoletseas and J. Rolim, eds., Lecture Notes in Comput. Sci. 4240, Springer, BerlinForman, G., Zahorjan, J., The challenges of mobile computing (1994) IEEE Comput., 27, pp. 38-47Fudos, I., Hoffmann, C., A graph-constructive approach to solving systems of geometric constraints (1997) ACM Trans. Graphics, 16, pp. 179-216Garey, M., Johnson, D., (1979) Computers and Intractability: A Guide to the Theory of NPCompleteness, , Freeman and Company, New YorkGibson, K., Scheraga, H., Energy minimization of rigid-geometry polypeptides with exactly closed disulfide loops (1997) J. Comput. Chem., 18, pp. 403-415Gluck, H., Almost all simply connected closed surfaces are rigid (1975) Geometric Topology, pp. 225-239. , A. Dold and B. Eckmann, eds., Lecture Notes in Math. 438, Springer, BerlinGlunt, W., Hayden, T.L., Hong, S., Wells, J., An alternating projection algorithm for computing the nearest Euclidean distance matrix (1990) SIAM J. Matrix Anal. Appl., 11, pp. 589-600Gortler, S., Healy, A., Thurston, D., Characterizing generic global rigidity (2010) Amer. J. Math., 132, pp. 897-939Gower, J., Some distance properties of latent root and vector methods in multivariate analysis (1966) Biometrika, 53, pp. 325-338Gower, J., Euclidean distance geometry (1982) Math. Sci., 7, pp. 1-14Gramacho, W., Mucherino, A., Lavor, C., Maculan, N., A parallel BP algorithm for the discretizable distance geometry problem (2012) Proceedings of the Workshop on Parallel Computing and Optimization, Shanghai, pp. 1756-1762. , IEEEGraver, J., Rigidity matroids (1991) SIAM J. Discrete Math., 4, pp. 355-368Graver, J., Servatius, B., Servatius, H., (1993) Combinatorial Rigidity, , AMS, Providence, RIGrippo, L., Sciandrone, M., On the convergence of the block nonlinear Gauss-Seidel method under convex constraints (2000) Oper. Res. Lett., 26, pp. 127-136Grone, R., Johnson, C., De Sá, E., Wolkowicz, H., Positive definite completions of partial Hermitian matrices (1984) Linear Algebra Appl., 58, pp. 109-124Grosso, A., Locatelli, M., Schoen, F., Solving molecular distance geometry problems by global optimization algorithms (2009) Comput. Optim. Appl., 43, pp. 23-27Havel, T., Metric matrix embedding in protein structure calculations (2003) Magnetic Resonance Chem., 41, pp. 537-550Havel, T., Kuntz, I., Crippen, G., The theory and practice of distance geometry (1983) Bull. Math. Biol., 45, pp. 665-720Hendrickson, B., Conditions for unique graph realizations (1992) SIAM J. Comput., 21, pp. 65-84Hendrickson, B., The molecule problem: Exploiting structure in global optimization (1995) SIAM J. Optim., 5, pp. 835-857Henneberg, L., (1886) Statik der Starren Systeme, , Bergstræsser, DarmstadtHenneberg, L., (1911) Die Graphische Statik der Starren Systeme, , Teubner, LeipzigHoai An, L., Solving large scale molecular distance geometry problems by a smoothing technique via the Gaussian transform and D.C. Programming (2003) J. Global Optim., 27, pp. 375-397Hoai An, L.T., Dinh Tao, P., Large-scale molecular optimization from distance matrices by a d.c. Optimization approach (2003) SIAM J. Optim., 14, pp. 77-114Huang, H.-X., Liang, Z.-A., Pardalos, P., Some properties for the Euclidean distance matrix and positive semidefinite matrix completion problems (2003) J. Global Optim., 25, pp. 3-21Hunt, K., (1990) Kinematic Geometry of Mechanisms, , Oxford University Press, OxfordIzrailev, S., Zhu, F., Agrafiotis, D., A distance geometry heuristic for expanding the range of geometries sampled during conformational search (2006) J. Comput. Chem., 26, pp. 1962-1969Jackson, B., Jordán, T., Connected rigidity matroids and unique realization of graphs (2005) J. Combin. Theory Ser. B, 94, pp. 1-29Jackson, B., Jordán, T., On the rigidity of molecular graphs (2008) Combinatorica, 28, pp. 645-658Jackson, B., Jordán, T., Graph theoretic techniques in the analysis of uniquely localizable sensor networks (2009) Localization Algorithms and Strategies for Wireless Sensor Networks: Monitoring and Surveillance Techniques for Target Tracking, pp. 146-173. , G. Mao and B. Fidan, eds., IGI GlobalJohnson, C., Kroschel, B., Wolkowicz, H., An interior-point method for approximate positive semidefinite completions (1998) Comput. Optim. Appl., 9, pp. 175-190Jolliffe, I., (2010) Principal Component Analysis, , 2nd ed., Springer, BerlinKostrowicki, J., Piela, L., Diffusion equation method of global minimization: Performance for standard test functions (1991) J. Optim. Theory Appl., 69, pp. 269-284Krishnaiah, P., Kanal, L., (1982) Theory of Multidimensional Scaling, 2. , North-HollandKrislock, N., (2010) Semidefinite Facial Reduction for Low-Rank Euclidean Distance Matrix Completion, , Ph.D. thesis, University of WaterlooKrislock, N., Wolkowicz, H., Explicit sensor network localization using semidefinite representations and facial reductions (2010) SIAM J. Optim., 20, pp. 2679-2708Kruskal, J., Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis (1964) Psychometrika, 29, pp. 1-27Kruskal, J., Nonmetric multidimensional scaling: A numerical method (1964) Psychometrika, 29, pp. 115-129Kucherenko, S., Belotti, P., Liberti, L., Maculan, N., New formulations for the kissing number problem (2007) Discrete Appl. Math., 155, pp. 1837-1841Kucherenko, S., Sytsko, Y., Application of deterministic low-discrepancy sequences in global optimization (2004) Comput. Optim. Appl., 30, pp. 297-318Laman, G., On graphs and rigidity of plane skeletal structures (1970) J. Engrg. Math., 4, pp. 331-340Laurent, M., Cuts, matrix completions and graph rigidity (1997) Math. Program., 79, pp. 255-283Laurent, M., Polynomial instances of the positive semidefinite and Euclidean distance matrix completion problems (2000) SIAM J. Matrix Anal. Appl., 22, pp. 874-894Laurent, M., Matrix completion problems (2009) Encyclopedia of Optimization, pp. 1967-1975. , 2nd ed., C. Floudas and P. Pardalos, eds., Springer, New YorkLavor, C., On generating instances for the molecular distance geometry problem (2006) Global Optimization: From Theory to Implementation, pp. 405-414. , L. Liberti and N.Maculan, eds., Springer, BerlinLavor, C., Lee, J., Lee-St. John, A., Liberti, L., Mucherino, A., Sviridenko, M., Discretization orders for distance geometry problems (2012) Optim. Lett., 6, pp. 783-796Lavor, C., Liberti, L., Maculan, N., Grover's algorithm applied to the molecular distance geometry problem (2005) Proceedings of the 7th Brazilian Congress of Neural Networks, Natal, BrazilLavor, C., Liberti, L., Maculan, N., Computational experience with the molecular distance geometry problem (2006) Global Optimization: Scientific and Engineering Case Studies, pp. 213-225. , J. Pintér, ed., Springer, BerlinLavor, C., Liberti, L., Maculan, N., (2006) The Discretizable Molecular Distance Geometry Problem, , preprint, arXiv:q-bio/0608012Lavor, C., Liberti, L., Maculan, N., Molecular distance geometry problem (2009) Encyclopedia of Optimization, pp. 2305-2311. , 2nd ed., C. Floudas and P. Pardalos, eds., Springer, New YorkLavor, C., Liberti, L., Maculan, N., A note on ""a Branch-and-Prune Algorithm for the Molecular Distance Geometry Problem"" (2011) Internat. Trans. Oper. Res., 18, pp. 751-752Lavor, C., Liberti, L., Maculan, N., Mucherino, A., The discretizable molecular distance geometry problem (2012) Comput. Optim. Appl., 52, pp. 115-146Lavor, C., Liberti, L., Maculan, N., Mucherino, A., Recent advances on the discretizable molecular distance geometry problem (2012) European J. Oper. Res., 219, pp. 698-706Lavor, C., Liberti, L., Mucherino, A., The interval branch-and-prune algorithm for the discretizable molecular distance geometry problem with inexact distances (2013) J. Global Optim., 56, pp. 855-871Lavor, C., Liberti, L., Mucherino, A., Maculan, N., On a discretizable subclass of instances of the molecular distance geometry problem (2009) Proceedings of the 24th Annual ACM Symposium on Applied Computing, pp. 804-805. , D. Shin, ed., ACM, New YorkLavor, C., Mucherino, A., Liberti, L., Maculan, N., An artificial backbone of hydrogens for finding the conformation of protein molecules (2009) Proceedings of the Computational Structural Bioinformatics Workshop, pp. 152-155. , Washington D.C., IEEELavor, C., Mucherino, A., Liberti, L., Maculan, N., Computing artificial backbones of hydrogen atoms in order to discover protein backbones (2009) Proceedings of the International Multiconference on Computer Science and Information Technology, Mragowo, Poland, IEEE, pp. 751-756Lavor, C., Mucherino, A., Liberti, L., Maculan, N., Discrete approaches for solving molecular distance geometry problems using NMR data (2010) Internat. J. Comput. Biosci., 1, pp. 88-94Lavor, C., Mucherino, A., Liberti, L., Maculan, N., On the solution of molecular distance geometry problems with interval data (2010) Proceedings of the International Workshop on Computational Proteomics, Hong Kong, IEEE, pp. 77-82Lavor, C., Mucherino, A., Liberti, L., Maculan, N., On the computation of protein backbones by using artificial backbones of hydrogens (2011) J. Global Optim., 50, pp. 329-344Lavor, C., Mucherino, A., Liberti, L., Maculan, N., Finding low-energy homopolymer conformations by a discrete approach (2012) Proceedings of the Global Optimization Workshop, , D. Aloise, P. Hansen, and C. Rocha, eds., Universidade Federal do Rio Grande do Norte, NatalLe Grand, S., Elofsson, A., Eisenberg, D., The effect of distance-cutoff on the performance of the distance matrix error when used as a potential function to drive conformational search (1996) Protein Folds: A Distance Based Approach, pp. 105-113. , H. Bohr and S. Brunak, eds., CRC Press, Boca Raton, FLLee, J., Verleysen, M., (2010) Nonlinear Dimensionality Reduction, , Springer, BerlinLeung, N.-H.Z., Toh, K.-C., An SDP-based divide-and-conquer algorithm for large-scale noisy anchor-free graph realization (2009) SIAM J. Sci. Comput., 31, pp. 4351-4372Liberti, L., (2004) Reformulation and Convex Relaxation Techniques for Global Optimization, , Ph.D. thesis, Imperial College London, LondonLiberti, L., Reformulations in mathematical programming: Definitions and systematics (2009) RAIRO Oper. Res., 43, pp. 55-85Liberti, L., Dražic, M., Variable neighbourhood search for the global optimization of constrained NLPs (2005) Proceedings of GO Workshop, Almeria, SpainLiberti, L., Kucherenko, S., (2004) Comparison of Deterministic and Stochastic Approaches to Global Optimization, , Tech. Rep. 2004.25, DEI, Politecnico di MilanoLiberti, L., Lavor, C., On a relationship between graph realizability and distance matrix completion (2013) Optimization Theory, Decision Making, and Operational Research Applications, pp. 39-48. , A. Migdalas, A. Sifaleras, C. Georgiadis, J. Papathanaiou, and E. Stiakakis, eds., Proc. Math. Statist. 31, Springer, BerlinLiberti, L., Lavor, C., Maculan, N., A branch-and-prune algorithm for the molecular distance geometry problem (200",Euclidean Distance Geometry And Applications,,'Society for Industrial & Applied Mathematics (SIAM)',,10.1137/120875909,core
100600196,2014-12-23,"Reinforcement learning is an important family of algo-rithms that have been extremely effective in fields such as robotics, economics, and artificial intelligence. Current al-gorithms become increasingly expensive as the state space of the problem increases in size. Additionally, computer ar-chitectures are becoming increasingly parallelized, and ex-isting algorithms need to be reworked to fit within this par-allelization paradigm. We will present a general framework for parallelizing such algorithms. Many reinforcement learning problems (such as robot navigation) have state-spaces that correspond to real, phys-ical spaces, in which states are physical locations and ac-tions transition between neighboring locations. We will demonstrate how problems of this nature can be decom-posed into smaller subproblems that can solved in parallel. We built two implementations of our framework, a MATLAB implementation for rapid prototyping (in which all paral-lelism is simulated) and a Java multicore implementation for which our reported runtimes are actual parallel runtime. Our framework improves the runtime of policy iteration by up to 3 × on an simulated 8 core processor (MATLAB), and up to 203 × on an actual 8 core processor (Java). 1",Parallelizing Reinforcement Learning,,,,,core
197747512,2015-01-01T00:00:00,"Particularly interesting group consists of algorithms that implement co-evolution or co-operation in natural environments, giving much more powerful implementations. The main aim is to obtain the algorithm which operation is not influenced by the environment. An unusual look at optimization algorithms made it possible to develop a new algorithm and its metaphors define for two groups of algorithms. These studies concern the particle swarm optimization algorithm as a model of predator and prey. New properties of the algorithm resulting from the co-operation mechanism that determines the operation of algorithm and significantly reduces environmental influence have been shown. Definitions of functions of behavior scenarios give new feature of the algorithm. This feature allows self controlling the optimization process. This approach can be successfully used in computer games. Properties of the new algorithm make it worth of interest, practical application and further research on its development. This study can be also an inspiration to search other solutions that implementing co-operation or co-evolution.Angeline, P. (1998). Using selection to improve particle swarm optimization. In Proceedings of the IEEE congress on evolutionary computation, Anchorage (pp. 84–89).Arquilla, J., & Ronfeldt, D. (2000). Swarming and the future of conflict, RAND National Defense Research Institute, Santa Monica, CA, US.Bessaou, M., & Siarry, P. (2001). A genetic algorithm with real-value coding to optimize multimodal continuous functions. Structural and Multidiscipline Optimization, 23, 63–74.Bird, S., & Li, X. (2006). Adaptively choosing niching parameters in a PSO. In Proceedings of the 2006 genetic and evolutionary computation conference (pp. 3–10).Bird, S., & Li, X. (2007). Using regression to improve local convergence. In Proceedings of the 2007 IEEE congress on evolutionary computation (pp. 592–599).Blackwell, T., & Bentley, P. (2002). Dont push me! Collision-avoiding swarms. In Proceedings of the IEEE congress on evolutionary computation, Honolulu (pp. 1691–1696).Brits, R., Engelbrecht, F., & van den Bergh, A. P. (2002). Solving systems of unconstrained equations using particle swarm optimization. In Proceedings of the 2002 IEEE conference on systems, man, and cybernetics (pp. 102–107).Brits, R., Engelbrecht, A., & van den Bergh, F. (2002). A niching particle swarm optimizer. In Proceedings of the fourth asia-pacific conference on simulated evolution and learning (pp. 692–696).Chelouah, R., & Siarry, P. (2000). A continuous genetic algorithm designed for the global optimization of multimodal functions. Journal of Heuristics, 6(2), 191–213.Chelouah, R., & Siarry, P. (2000). Tabu search applied to global optimization. European Journal of Operational Research, 123, 256–270.Chelouah, R., & Siarry, P. (2003). Genetic and Nelder–Mead algorithms hybridized for a more accurate global optimization of continuous multiminima function. European Journal of Operational Research, 148(2), 335–348.Chelouah, R., & Siarry, P. (2005). A hybrid method combining continuous taboo search and Nelder–Mead simplex algorithms for the global optimization of multiminima functions. European Journal of Operational Research, 161, 636–654.Chen, T., & Chi, T. (2010). On the improvements of the particle swarm optimization algorithm. Advances in Engineering Software, 41(2), 229–239.Clerc, M., & Kennedy, J. (2002). The particle swarm-explosion, stability, and convergence in a multidimensional complex space. IEEE Transactions on Evolutionary Computation, 6(1), 58–73.Fan, H., & Shi, Y. (2001). Study on Vmax of particle swarm optimization. In Proceedings of the workshop particle swarm optimization, Indianapolis.Gao, H., & Xu, W. (2011). Particle swarm algorithm with hybrid mutation strategy. Applied Soft Computing, 11(8), 5129–5142.Gosciniak, I. (2008). Immune algorithm in non-stationary optimization task. In Proceedings of the 2008 international conference on computational intelligence for modelling control & automation, CIMCA ’08 (pp. 750–755). Washington, DC, USA: IEEE Computer Society.He, Q., & Wang, L. (2007). An effective co-evolutionary particle swarm optimization for constrained engineering design problems. Engineering Applications of Artificial Intelligence, 20(1), 89–99.Higashitani, M., Ishigame, A., & Yasuda, K., (2006). Particle swarm optimization considering the concept of predator–prey behavior. In 2006 IEEE congress on evolutionary computation (pp. 434–437).Higashitani, M., Ishigame, A., & Yasuda, K. (2008). Pursuit-escape particle swarm optimization. IEEJ Transactions on Electrical and Electronic Engineering, 3(1), 136–142.Hu, X., & Eberhart, R. (2002). Multiobjective optimization using dynamic neighborhood particle swarm optimization. In Proceedings of the evolutionary computation on 2002. CEC ’02. Proceedings of the 2002 congress (Vol. 02, pp. 1677–1681). Washington, DC, USA: IEEE Computer Society.Hu, X., Eberhart, R., & Shi, Y. (2003). Engineering optimization with particle swarm. In IEEE swarm intelligence symposium, SIS 2003 (pp. 53–57). Indianapolis: IEEE Neural Networks Society.Jang, W., Kang, H., Lee, B., Kim, K., Shin, D., & Kim, S. (2007). Optimized fuzzy clustering by predator prey particle swarm optimization. In IEEE congress on evolutionary computation, CEC2007 (pp. 3232–3238).Kennedy, J. (2000). Stereotyping: Improving particle swarm performance with cluster analysis. In Proceedings of the 2000 congress on evolutionary computation (pp. 1507–1512).Kennedy, J., & Mendes, R. (2002). Population structure and particle swarm performance. In IEEE congress on evolutionary computation (pp. 1671–1676).Kuo, H., Chang, J., & Shyu, K. (2004). A hybrid algorithm of evolution and simplex methods applied to global optimization. Journal of Marine Science and Technology, 12(4), 280–289.Leontitsis, A., Kontogiorgos, D., & Pange, J. (2006). Repel the swarm to the optimum. Applied Mathematics and Computation, 173(1), 265–272.Li, X. (2004). Adaptively choosing neighborhood bests using species in a particle swarm optimizer for multimodal function optimization. In Proceedings of the 2004 genetic and evolutionary computation conference (pp. 105–116).Li, C., & Yang, S. (2009). A clustering particle swarm optimizer for dynamic optimization. In Proceedings of the 2009 congress on evolutionary computation (pp. 439–446).Liang, J., Suganthan, P., & Deb, K. (2005). Novel composition test functions for numerical global optimization. In Proceedings of the swarm intelligence symposium [Online]. Available: .Liang, J., Qin, A., Suganthan, P., & Baskar, S. (2006). Comprehensive learning particle swarm optimizer for global optimization of multimodal functions. IEEE Transactions on Evolutionary Computation, 10(3), 281–295.Lovbjerg, M., & Krink, T. (2002). Extending particle swarm optimizers with self-organized criticality. In Proceedings of the congress on evolutionary computation, Honolulu (pp. 1588–1593).Lung, R., & Dumitrescu, D. (2007). A collaborative model for tracking optima in dynamic environments. In Proceedings of the 2007 congress on evolutionary computation (pp. 564–567).Mendes, R., Kennedy, J., & Neves, J. (2004). The fully informed particle swarm: simpler, maybe better. IEEE Transaction on Evolutionary Computation, 8(3), 204–210.Miranda, V., & Fonseca, N. (2002). New evolutionary particle swarm algorithm (EPSO) applied to voltage/VAR control. In Proceedings of the 14th power systems computation conference, Seville, Spain [Online] Available: .Parrott, D., & Li, X. (2004). A particle swarm model for tracking multiple peaks in a dynamic environment using speciation. In Proceedings of the 2004 congress on evolutionary computation (pp. 98–103).Parrott, D., & Li, X. (2006). Locating and tracking multiple dynamic optima by a particle swarm model using speciation. In IEEE transaction on evolutionary computation (Vol. 10, pp. 440–458).Parsopoulos, K., & Vrahatis, M. (2004). UPSOA unified particle swarm optimization scheme. Lecture Series on Computational Sciences, 868–873.Passaroand, A., & Starita, A. (2008). Particle swarm optimization for multimodal functions: A clustering approach. Journal of Artificial Evolution and Applications, 2008, 15 (Article ID 482032).Peram, T., Veeramachaneni, K., & Mohan, C. (2003). Fitness-distance-ratio based particle swarm optimization. In Swarm intelligence symp. (pp. 174–181).Sedighizadeh, D., & Masehian, E. (2009). Particle swarm optimization methods, taxonomy and applications. International Journal of Computer Theory and Engineering, 1(5), 1793–8201.Shi, Y., & Eberhart, R. (2001). Particle swarm optimization with fuzzy adaptive inertia weight. In Proceedings of the workshop particle swarm optimization, Indianapolis (pp. 101–106).Shi, Y., & Eberhart, R. (1998). A modified particle swarm optimizer. In Proceedings of IEEE International Conference on Evolutionary Computation (pp. 69–73). Washington, DC, USA: IEEE Computer Society.Thomsen, R. (2004). Multimodal optimization using crowding-based differential evolution. In Proceedings of the 2004 congress on evolutionary computation (pp. 1382–1389).Trojanowski, K., & Wierzchoń, S. (2009). Immune-based algorithms for dynamic optimization. Information Sciences, 179(10), 1495–1515.Tsoulos, I., & Stavrakoudis, A. (2010). Enhancing PSO methods for global optimization. Applied Mathematics and Computation, 216(10), 2988–3001.van den Bergh, F., & Engelbrecht, A. (2004). A cooperative approach to particle swarm optimization. IEEE Transactions on Evolutionary Computation, 8, 225–239.Wolpert, D., & Macready, W. (1997). No free lunch theorems for optimization. IEEE Transaction on Evolutionary Computation, 1(1), 67–82.Xie, X., Zhang, W., & Yang, Z. (2002). Dissipative particle swarm optimization. In Proceedings of the congress on evolutionary computation (pp. 1456–1461).Yang, S., & Li, C. (2010). A clustering particle swarm optimizer for locating and tracking multiple optima in dynamic environments. In IEEE Trans. on  evolutionary computation (Vol. 14, pp. 959–974).Kuo, H., Chang, J., & Liu, C. (2006). Particle swarm optimization for global optimization problems. Journal of Marine Science and Technology, 14(3), 170–181",A new approach to particle swarm optimization algorithm,https://core.ac.uk/download/197747512.pdf,'Elsevier BV',"[{'title': 'Expert Systems with Applications', 'identifiers': ['0957-4174', 'issn:0957-4174']}]",10.1016/j.eswa.2014.07.034,core
211485892,2013-01-01T00:00:00,"The twelve papers included in this special issue represent a selection of extended contributions presented at the Sixth International Conference on Soft Computing Models in Industrial and Environmental Applications, held in Salamanca, Spain, 6–8th April, 2011. Papers were selected on the basis of fundamental ideas and concepts rather than the direct usage of well-established techniques. This special issue is then aimed at practitioners, researchers and post-graduate students, who are engaged in developing and applying advanced Soft Computing Models to solving real-world problems in the Industrial and Environmental fields. The papers are organized as follows. In the first contribution, Graña and Gonzalez-Acuña develop a formulation of dendritic classifiers based on lattice kernels and train them using a direct Monte Carlo approach and a Sparse Bayesian Learning. The results of both kinds of training are compared with the Relevance Vector Machines on a collection of benchmark datasets. In the second contribution by Irigoyen and Miñano, the authors present the results of the identification of the relationship in time, between the required exercise (machine resistance) and the heart rate of the patient in medical effort tests, using a NARX neural network model. In the experimental stage, test data have been obtained by exercising with a cyclo-ergometer in two different tests: Power Step Response and Conconi. Carneiro et al. in the third contribution present a biologically inspired method to deal with the problem in which genetic algorithms are used to create possible solutions for a given dispute. The approach presented is able to generate a broad number of diverse solutions that cover virtually the whole search space for a given problem. The results of this work are being applied in a negotiation tool that is part of the UMCourt conflict resolution platform. In the fourth contribution by Donate et al., they propose a novel Evolutionary Artificial Neural Networks (EANN) approach, where a weighted n-fold validation fitness scheme is used to build an ensemble of neural networks, under four different combination methods: mean, median, softmax and rank-based combinations. Several experiments were held, using six real-world time series with different characteristics and from distinct domains. Overall, the proposed approach achieved competitive results when compared with non weighted n-fold EANN ensembles, the simpler 0-fold EANN and also the popular Holt–Winters statistical method. Dan Burdescu et al. in the fifth contribution, present a system used in the medical domain for three distinct tasks: image annotation, semantic based image retrieval and content based image retrieval. An original image segmentation algorithm based on a hexagonal structure was used to perform the segmentation of medical images. Image's regions are described using a vocabulary of blobs generated from image features using the K-means clustering algorithm. The annotation and semantic based retrieval task is evaluated for two annotation models: Cross Media Relevance Model and Continuous-space Relevance Model. Semantic based image retrieval is performed using the methods provided by the annotation models. The ontology used by the annotation process was created in an original manner starting from the information content provided by the Medical Subject Headings (MeSH). The experiments were made using a database containing colour images retrieved from medical domain using an endoscope and related to digestive diseases. In sixth paper by Pedraza et al., they develop a face recognition system based on soft computing techniques, which complies with privacy-by-design rules and defines a set of principles that context-aware applications (including biometric sensors) should contain to conform to European and US law. This research deals with the necessity to consider legal issues concerning privacy or human rights in the development of biometric identification in ambient intelligence systems. Clearly, context-based services and ambient intelligence (and the most promising research area in Europe, namely ambient assisted living, ALL) call for a major research effort on new identification procedures. The aim of the research by Redel-Macías et al. in paper seven is to develop a novel model which can be used in pass-by noise test in vehicles based on ensembles of hybrid Evolutionary Product Unit or Radial Basis function Neural Networks (EPUNN or ERBFNNs) at high frequencies. Statistical models and ensembles of hybrid EPUNN and ERBFNN approaches have been used to develop different noise identification models. The results obtained using different ensembles of hybrid EPUNNs and ERBFNNs show that the functional model and the hybrid algorithms proposed provide a very accurate identification compared to other statistical methodologies used to solve this regression problem. In the eighth paper, Wu et al. analyse the existence criterion of loop strategies, and then present some corollaries and theorems, by which the loop strategies and chain strategies can be found, also superfluous strategies and inconsistent strategies. It presents a ranking model that indicates the weak node in strategy set and it also introduces a probability-based model which is the basis of evaluation of strategy. Additionally, this research proposes a method to generate offensive strategy, and the statistic results of simulation game prove the validity of the method. Pop et al. in the ninth paper present an efficient hybrid heuristic algorithm obtained by combining a genetic algorithm (GA) with a local–global approach to the generalized vehicle routing problem (GVRP) and a powerful local search procedure. The computational experiments on several benchmarks instances show that the hybrid algorithm is competitive to all of the known heuristics published to date. In the tenth paper Kramer et al. illustrate how methods from neural computation can serve as forecasting, and monitoring techniques, contributing to a successful integration of wind into sustainable, and smart energy grids. The study is based on the application of kernel methods like support vector regression and kernel density estimation as prediction methods. Furthermore, dimension reduction techniques like self-organizing maps for monitoring of high-dimensional wind time series are applied. The methods are briefly introduced, related work is presented, and experimental case studies are exemplarily described. The experimental parts are based on real wind energy time series data from the NREL western wind resource dataset. Vera et al. in the eleventh contribution present a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. The novel proposed approach was tested under real dental milling processes using a high precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study. The final contribution, by Sakalauskas and Kriksciuniene, presents a research about financial market efficiency and to recognize major reversal points of long-term trend of stock market index, which could indicate forthcoming crisis or market raise periods. The study suggests a computational model of financial time series analysis, which combines several approaches of soft computing, including information efficiency evaluation methods (Shannon's entropy, Hurst exponent), neural networks and sensitivity analysis. The model aims to derive the aggregated measure for evaluating efficiency of the financial market and to find its interrelationships with the reversal of long-term trend. The radial basis function neural network was designed for forecasting moments of cardinal changes in stock market behaviour, expressed by its entropy values derived from the symbolized time series of stock market index. The performance of neural network model is explored by applying sensitivity analysis and resulted in selecting smoothing parameters of the input variables. The experimental research investigates behaviour of the long-term trend of the three emerging financial markets within NASDAQ OMX Baltic stock exchange. Introduction of information efficiency measures improve ability of the model to recognize the approaching reversal of long-term trend from temporary market “nervousness” and can be useful for calibrating stock trading strategy. First, we would like to thank all the authors for their valuable contributions, which made this special issue possible. We also like to thank our peer-reviewers for their timely diligent work and efficient efforts. We are also grateful to the Editor-in-Chief of Neurocomputing Journal, Prof. Tom Heskes, for his continued support for the SOCO series of conferences and for this Special Issue on this prestigious journal. Finally, we hope the reader will share our joy and find this special issue very useful",New trends on soft computing models in industrial and environmental applications,https://core.ac.uk/download/211485892.pdf,'Elsevier BV',,,core
55247422,2014-01-01T00:00:00,"A hazard detection and landing map generator based on a single camera acquisition, sufficiently light to run onboard during the landing phase of a planetary exploration mission, is presented. Autonomous, precise and safe landing capability is a key feature for next generation space missions: scientifically relevant places may be associated with hazardous terrain features or confined in very limited areas; in other cases there is no possibility to completely characterize a predefined landing area with the required accuracy. The short duration of the landing phase together with telecommunications delays require a high level of onboard autonomy in the GNC, coupled with light computational mechanisms. In such a scenario, the ability to distinguish hazardous from safe landing areas and consequently correct the landing trajectory becomes crucial. Algorithms development is made more difficult by uncertainties in the knowledge of the morphological structures to be encountered during the landing phase, being the environment not perfectly known in advance. The well-known generalization properties of Artificial Neural Networks (ANN) is here exploited to build maps online, flexible with respect to the conditions exploited on ground to settle the classification mechanism. The calibration of the hazardous regions classification is based on is a quite long and complex operation: once more, ANN are simple to be implemented and computationally efficient, even during their training phase. A set of self-organizing maps are exploited to characterize different terrain features at different scales. Each activated neuron corresponds to a class of combinations of morphological properties, such as shadows, roughness and slopes. Then, a set of feedforward ANN interprets these parameters to produce a unique hazard index for each pixel of the original image. Finally, the map is filtered in order to correlate each index to the surrounding elements. Once the hazard map is available, the target landing site is updated, considering merit parameters such as hazard index, extension of safe landing areas and proximity to nominal target. The algorithm requires less than 20 ms to analyze a 1024 Ã— 1024 image, on an AMD A10-7700K processor, written in C++. Different training methods are investigated: the exploitation of both artificial and real images is considered. Results for different scenarios in a Lunar landing case are shown and discussed, in order to highlight the effectiveness of the proposed system. Sensitivity to environmental parameters, such as light conditions, trajectory inclination and camera attitude is investigated. Finally, possible future improvements are suggested",Autonomous Vision-Based Hazard Map Generator for Planetary Landing Phases,,"Curran Associates, Inc.",,,core
76525985,2015,"Mastering the finest art of ‘mechatronics' currently looks one of the most attractive task of modern engineering technology and science. Many are the applications which resort to the interdisciplinary approach of mechatronics to enhance the performance, quality and safety of either product or process. Some are very traditional, like hard disk drives, biomedical, automotive and aerospace systems, other are fairly new like micro and nano electromechanical systems, unmanned air vehicles, intelligent machining and manufacturing systems or bioinspired devices. A first generation of mechatronic products was conceived to embed a suitable ‘smartness' to improve the skill of self-adapting to any abrupt variation of operating conditions, by resorting to the ‘synergistic integration of mechanical engineering with electronics and control in the design and manufacturing of product process' as mechatronics was brightly defined. Nowadays, a mechatronic design is surely based on its interdisciplinary nature, but its real meaning was harmonized with an effective contamination among different application domains, methodologies and technologies, being smartly applied to reach the highest result in any product, system and process development. A recent experience within the frame of the EMEA District of the American Society of Mechanical Engineers (ASME) was a chance to get an impression of the scientific and industrial research activity performed in some fields of mechatronics. Some exciting examples describing how different competences, disciplines, technologies met in an innovative mechatronic system are herein exposed by some researchers of the EMEA area of the world. They deal with several domains, like the hard disk drive technology, biomedical prostheses, fluidic automation, UAV Vision System, vibration monitoring and suppression in steelmaking plants, materials machining and smart composites. These examples will narrate to the reader who is still looking for the meaning of mechatronics how some approaches, as neural network positioning control, chaos prevention, myoelectric stimulation of prosthesis, human detection by vision system, multi-physics modeling and control of dynamics are currently implemented in a sort of artificial intelligence in small scale device, as in a finger of a biotronic hand or in a large equipment like an electric arc furnace. Moreover, the reader will realize how intensively this goal is achieved by exploiting the available technologies as additive manufacturing or fiber optics embedded into composite structures to reduce the cost, weight or volume of the product or to improve the quality and accuracy of a material processing like in rolling or in turning against the risk of self-excited chatter vibration. This scenario is covering a wide range of mechatronic applications, although many others are currently developed in several fileds of engineerin","Mechatronics: Principles, Technologies and Applications",,"NOVA Science Publishers, Inc",,,core
378966947,2015-01-01T08:00:00,"Writing English research article (RA) abstracts is a difficult but mandatory task for Taiwanese engineering graduate students (Feng, 2013). Understanding the current situation and needs of Taiwanese engineering graduate students, this dissertation aimed to develop and evaluate an automated writing evaluation (AWE) tool to assist their research article (RA) abstract writing in English by following a Design-Based Research (DBR) approach as the methodological framework. DBR was chosen because it strives to solve real-world problems through multiple iterations of development and building on results from each iteration to advance the project.
Six design iterations were undertaken to develop and to evaluate the AWE tool in this dissertation, including (1) corpus compilation of engineering RAs, (2) genre analysis of engineering abstracts, (3) machine learning of move classification in abstracts, (4) analysis of lexical bundles used to express moves, (5) analysis of the choice of verb categories associated with moves, and finally, (6) AWE tool development based on previous findings, classroom implementation, and evaluation of the AWE tool following Chapelle’s (2001) computer-assisted language learning (CALL) framework.
To begin with, I collected a corpus of 480 engineering RAs (Corpus-480) to extract appropriate linguistic properties as pedagogical materials to be implemented in the AWE tool. A sub-corpus (Corpus-72) was compiled with 72 RAs randomly chosen from Corpus-480 for manual and automated analyses. Next, to seek the best descriptive framework for the structure of engineering RA abstracts, two move schemata were compared: (1) IMRD (Introduction, Methodology, Results, and Discussion) and (2) CARS (Create-A-Research-Space, Swales, 1990). Abstracts in Corpus-72 were annotated and these two schemas were evaluated according to three quantitative metrics devised specifically for this comparison.
Applying a statistical natural language processing (StatNLP) approach, a Support Vector Machine (SVM) was trained for automated move classification in abstracts. Formulaic language in engineering RA sections was used as linguistic features to automatically classify moves in abstracts. Additionally, four-word lexical bundles and verb categories were identified from Corpus-480 and Corpus-72, respectively. Four-word lexical bundles associated with moves in abstracts were extracted automatically. Additionally, verb categories (i.e., tense, aspect, and voice) in moves of abstracts were identified using CyWrite::Analyzer, a hybrid (statistical and rule-based) NLP software.
Finally, the AWE tool was developed, based on the findings from the previous iterations, and implemented in an English-as-a-foreign-language (EFL) classroom setting. Through analyzing students’ drafts before and after using the tool, and responses to a questionnaire and a semi-structured interview, the AWE tool was evaluated based on Chapelle’s (2001) CALL evaluation framework. The findings showed that students attempted to improve their abstracts by adding, deleting, or changing the sequences of their sentences, lexical bundles, and verb categories in their abstracts. Their attitudes toward the effectiveness and appropriateness of the tool were quite positive. Overall, the AWE tool drew students’ attention to the use of lexical bundles and verb categories to achieve the communicative purposes of each move in their abstracts.
In conclusion, this dissertation started from Taiwanese engineering students’ needs to improve their English abstract writing, and attempted to develop and evaluate an AWE tool for assisting them. Following DBR, the findings from this dissertation are discussed to improve the next generation of the AWE tools. Having these iterations in place, future studies can focus on developing pedagogical materials from genre-based analysis in different disciplines to fulfill learners’ needs","Designing, implementing, and evaluating an automated writing evaluation tool for improving EFL graduate students’ abstract writing: a case in Taiwan",,Iowa State University Digital Repository,,,core
23795726,2014-01-22,"Abstract. Spectral analysis approaches have been actively studied in machine learning and data mining areas, due to their generality, efficiency, and rich theoretical foundations. As a natural non-linear generalization of Graph Laplacian, p-Laplacian has recently been proposed, which interpolates between a relaxation of normalized cut and the Cheeger cut. However, the relaxation can only be applied to two-class cases. In this paper, we propose full eigenvector analysis of p-Laplacian and obtain a natural global embedding for multi-class clustering problems, instead of using greedy search strategy implemented by previous researchers. An efficient gradient descend optimization approach is introduced to obtain the p-Laplacian embedding space, which is guaranteed to converge to feasible local solutions. Empirical results suggest that the greedy search method often fails in many real-world applications with non-trivial data structures, but our approach consistently gets robust clustering results. Visualizations of experimental results also indicate our embedding space preserves the local smooth manifold structures existing in real-world data. ",On the eigenvectors of p-Laplacian,,,,,core
187722098,2014-01-01T00:00:00,"Artificial intelligence methodologies, as the core of discrete control and decision support systems, have been extensively applied
in the industrial production sector. The resulting tools produce excellent results in certain cases; however, the NP-hard nature of
many discrete control or decision making problems in the manufacturing area may require unaffordable computational resources,
constrained by the limited available time required to obtain a solution. With the purpose of improving the efficiency of a control
methodology for discrete systems, based on a simulation-based optimization and the Petri net (PN) model of the real discrete
event dynamic system (DEDS), this paper presents a strategy, where a transformation applied to the model allows removing the
redundant information to obtain a smaller model containing the same useful information. As a result, faster discrete optimizations
can be implemented.This methodology is based on the use of a formalism belonging to the paradigmof thePNfor describingDEDS,
the disjunctive colored PN. Furthermore, the metaheuristic of genetic algorithms is applied to the search of the best solutions in
the solution space. As an illustration of the methodology proposal, its performance is compared with the classic approach on a case
study, obtaining faster the optimal solution",Control of discrete event systems by means of discrete optimization and disjunctive colored PNs: application to manufacturing facilities,https://core.ac.uk/download/187722098.pdf,'Hindawi Limited',,10.1155/2014/821707,core
42383835,2014-06-01T00:00:00,"When performing a piece, a pianist's interpretation is communicated both through the sound produced and through body gestures. We present PiaF (Piano Follower), a prototype for augmenting piano performance by measuring gesture variations. We survey other augmented piano projects, several of which focus on gestural recognition, and present our prototype which uses machine learning techniques for

gesture classification and estimation of gesture variations in

real-time. Our implementation uses the Kinect depth sensor to track body motion in space, which is used as input data. During an initial learning phase, the system is taught a set of reference gestures, or templates. During performance, the live gesture is classi�ed in real-time, and variations with respect to the recognized template are computed. These values can then be mapped to audio processing parameters, to control digital effects which are applied to the acoustic output of the piano in real-time. We discuss initial tests using PiaF with a pianist, as well as potential applications beyond live performance, including pedagogy and embodiment of recorded performance",PiaF: A Tool for Augmented Piano Performance Using Gesture Variation Following,https://core.ac.uk/download/42383835.pdf,,,,core
42412712,2015-10-01T00:00:00,"© Emerald Group Publishing Limited. Purpose - The focus of this work is on the client-designer interface where decisions have significant impact over the lifecycle of the project. Therefore, the briefing stage is examined in the context of clients needs which is divided into project-based strategy and broader clients strategy. The purpose of this paper is to address the pitfalls in the briefing process which has been attributed to the shortcomings in the client-designer communication interfaces. This will be achieved by developing an automated brief generation framework. The research examines the efficiency of standard approaches to modelling and design, and the benefits that these methodologies have offered to the computer industry. The work reviews the similarities between the two industries and argues in support of the potential benefits in adopting a standard methodology in the construction industry. The structure upon which the framework is developed is based on system analysis and design methodology (SSADM) which has proven to be an effective platform used within the software development industry. Design/methodology/approach - SSADM is an established methodology within the software development industry. The paper will demonstrate that due to fundamental similarities between the construction and software development industries, SSADM is likely to offer a viable platform upon which an automated enhanced brief generation model is developed for use in the construction industry. The construction design and construction process will be mapped on SSADM high-level definition before focusing and honing on the design phase. The methodology for the development of the framework will be based on the rationalist approach of generating knowledge through reasoning leading to model-building. Findings - A model that is based on SSADM is proposed for the design development phase of construction projects. In order to shape the project strategy, the model considers the combined role of clients requirements with organisation strategy and environmental factors. The paper has shown that it is feasible to increase the automation of the briefing process and enhanced the briefing output. The model here does not diminish the importance of direct communication between the client and the design team. It provides a more structured way of doing so, while taking advantage of vast array of data and technology in order to improve the brief outcome. Research limitations/implications - From practical perspective, the proposed framework is in its formative stage, thus requiring incremental refinement through several case studies. This is particularly true about the AI components of the system which typically rely on extensive data representing the real-case scenarios. Therefore, the work invites further research into the examination of various parts as well as the overall system. Practical implications - There are several ways by which construction projects are procured. There may be fluctuation in their rate of usage, but while there is no indication of any procurement option fading, new ones such as PPP and PFI are periodically introduced. The existence of this diversity is indicative of the fact that the industry tends to respond to problems rather than attempting to instigate a measured solution supported by theoretical underpinning. Subsequently, there have been suggestions of a communication and information discourse between actors and within processes involved in project lifecycle. This project is aimed at addressing the gap in the client-designer communication. The automated approach to brief generation will lead to better briefs while reducing ambiguities as well as the overhead associated with brief generation. Social implications - The quality of project brief has a significant impact on decisions at the design stage. In turn, these decisions will influence all phases of construction project lifecycle. The briefing session and requirement analysis of a construction project can be very difficult for inexperienced clients particularly for complex projects. Therefore, there is potential for the process of client-requirement-analysis to be optimised. The work promises to improve the quality of the briefing process, thus helping clients to realise their intended objectives and minimise resource waste. Originality/value - The work builds on the commonalities of the construction and software development industries and takes advantage of the advancements in the latter. In doing so, project quality is defined quantitatively which is used to develop project strategy in a three-dimensional space. The development of the model was also contingent upon enhancement of artificial neural network structure",Enhanced project brief: Structured approach to client-designer interface,https://core.ac.uk/download/42412712.pdf,'Emerald',,10.1108/ECAM-10-2014-0128,core
42656576,2015-11-19T00:00:00,"Binary representation is desirable for its memory efficiency, computation
speed and robustness. In this paper, we propose adjustable bounded rectifiers
to learn binary representations for deep neural networks. While hard
constraining representations across layers to be binary makes training
unreasonably difficult, we softly encourage activations to diverge from real
values to binary by approximating step functions. Our final representation is
completely binary. We test our approach on MNIST, CIFAR10, and ILSVRC2012
dataset, and systematically study the training dynamics of the binarization
process. Our approach can binarize the last layer representation without loss
of performance and binarize all the layers with reasonably small degradations.
The memory space that it saves may allow more sophisticated models to be
deployed, thus compensating the loss. To the best of our knowledge, this is the
first work to report results on current deep network architectures using
complete binary middle representations. Given the learned representations, we
find that the firing or inhibition of a binary neuron is usually associated
with a meaningful interpretation across different classes. This suggests that
the semantic structure of a neural network may be manifested through a guided
binarization process.Comment: Under review as a conference paper at ICLR 201",Adjustable Bounded Rectifiers: Towards Deep Binary Representations,http://arxiv.org/abs/1511.06201,,,,core
296637354,2015-11-26T14:50:53,"Although grid technologies have been embraced by academia and industry as a viable solution to build integrated systems out of heterogeneous resources, a number of challenges still hamper their widespread acceptance and use. One particular challenge has proven difficult to overcome: the transition of legacy code into grid environments. The ability to adapt legacy applications to benefit from grid resources is vital to the success of grid technologies, since re-writing them is not a practical solution in many settings. Financial institutions are perhaps the most clear case, since they use complex, sensitive and resource-demanding software that can greatly benefit from grid technologies but cannot afford to be significantly re-written. This paper describes the efforts conducted to modify legacy financial applications to be executed under a commercial grid middleware named Sparsi Maestro. We describe the steps involved in the transition of a legacy application into the grid environment and present two examples of actual financial applications that have undergone that process. © 2011 ACM.3945ACM SIGARCH,IEEE Computer Society (IEEE CS)Amdahl, G., Validity of the single-processor approach to achieving large scale computing capabilities (1967) AFIPS Conference, pp. 483-485Baduel, L., Baude, F., Caromel, D., Contes, A., Huet, F., Morel, M., Quilici, R., Programming, deploying, composing, for the grid (2006) Grid Computing: Software Environments and Tools, , Springer LondonChandra, R., Dagum, L., Kohr, D., Maydan, D., Mcdonald, J., Menon, R., (2001) Parallel Programming in OpenMP, , Morgan Kaufmann Publishers Inc., San Francisco, CA, USAChien, A., Calder, B., Elbert, S., Bhatia, K., Entropia: Architecture, performance of an enterprise desktop grid system (2003) Journal of Parallel and Distributed Computing, 63 (5)Childers, L., Disz, T., Olson, R., Papka, M.E., Stevens, R., Udeshi, T., Access grid: Immersive group-to-group collaborative visualization Proceedings of the Fourth International Immersive Projection Technology Workshop, , June 19-20Delaitre, T., Kiss, T., Goyeneche, A., Terstyanszky, G., Winter, S., Kacsuk, P., GEMLCA: Running legacy code applications as Grid services (2005) Journal of Grid Computing, 3 (1-2), pp. 75-90. , DOI 10.1007/s10723-005-9002-8Foster, I., Globus toolkit version 4: Software for service-oriented systems (2006) Journal of Computer Science and Technology, 21 (4)Ho, Q., Ong, Y., Cai, W., Gridifying aerodynamic design problem using GridRPC Grid and Cooperative Computing(Lecture Notes in Computer Science, 3032, pp. 83-90. , Springer Berlin / HeidelbergKommineni, J., Abramson, D., GriddLeS enhancements and building virtual applications for the GRID with legacy components (2005) Lecture Notes in Computer Science, 3470, pp. 961-971. , Advances in Grid Computing - EGC 2005: European Grid Conference, Revised Selected PapersMateos, C., Zunino, A., Campo, M., A survey on approaches to gridification (2008) Software - Practice and Experience, (38)Natrajan, A., Humphrey, M.A., Grimshaw, A.S., The legion support for advanced parameter space studies on a grid (2002) Future Generation Computer Systems, 18 (8)Project, B.B.J., (2011) Reliable Multicasting with the Jgroups Toolkit, , http://www.jgroups.org/manual/, SeptemberSnir, M., Otto, S., Huss-Lederman, S., Walker, D., Dongarra, J., (1998) MPI - The Complete Reference, Volume 1: The MPI Core, , MIT Press, Cambridge, MA, USA, 2nd. (revised) editionSynapse, D., (2010) Gridserver: High Performance Application Infrastructure for Your Business Critical Applications, , http://www.datasynapse.com/gridserver, MayThain, D., Tanenbaum, T., Livny, M., Condor and the grid (2003) Grid Computing: Making the Global Infrastructure a Reality, , John Wiley & SonsVadhiyar, S., Dongarra, J., Grads gridification of numeric applications based on globus and mpi (2005) Self Adaptability in Grid Computing. Concurrency and Computation: Practice and Experience, 17 (2-4). , Special Issue on Grid PerformanceWang, B., Xu, Z., Xu, C., Yin, Y., Ding, W., Yu, H., A study of gridifying scientific computing legacy codes (2004) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 3251, pp. 404-412Willard, C.G., (2010) Univa UD's Strategy for Subduing the Complex Software Monster, , http://www.univaud.com/about/resources/files/wp-tabor-research.pdf, Ma",Practical Experiences On The Gridification Of Financial Applications,,'Association for Computing Machinery (ACM)',,10.1145/2088256.2088269,core
296653223,2015-11-26T15:19:15,"This paper presents an evolving neuro-fuzzy network approach (eNFN) to model a twin rotor MIMO system (TRMS) with two degrees of freedom in real-time. The TRMS is a fast, nonlinear, open loop unstable time-varying dynamic system, with cross coupling between the rotors. Modeling and control of TRMS require high sampling rates, typically in the order of milliseconds. Actual laboratory implementation shows that eNFN is fast, effective, and accurately models the TRMS in real-time. The eNFN captures the TRMS system dynamics quickly, and develops precise low cost models from the point of view of time and space complexity. The results suggest eNFN as a potential candidate to model complex, fast time-varying dynamic systems in real-time.Afruz, J., Alam, M., Non-linear modeling of a twin rotor system using particle swarm optimization (2010) Proceedings of the International Computer Symposium, ICS'10, pp. 1026-1032Toha, S., Tokhi, M., ANFIS modelling of a twin rotor system using particle swarm optimization and RLS (2010) Proceedings of the IEEE 9th International Conference on Cybernetic Intelligent Systems, CIS'10, pp. 1-6Angelov, P., Filev, D., Kasabov, N., Guest editorial evolving fuzzy systems: Preface to the special section (2008) IEEE Transactions on Fuzzy Systems, 16 (6), pp. 1390-1392Lemos, A., Caminhas, W., Gomide, F., Adaptive fault detection and diagnosis using an evolving fuzzy classifier (2013) Information Sciences, 220 (0), pp. 64-85Venkatasubramanian, V., Rengaswamy, R., Yin, K., Kavuri, S.N., A review of process fault detection and diagnosis: Part I: Quantitative model-based methods (2003) Computers & Chemical Engineering, 27 (3), pp. 293-311Feedback, I., (2006) Twin Rotor MIMO System Control Experiments, pp. 33-942s. , UKToha, S., Tokhi, M., Dynamic nonlinear inverse-model based control of a twin rotor system using adaptive neuro-fuzzy inference system (2009) Proceedings of the Third UKSim European Symposium on Computer Modeling and Simulation, EMS'09, pp. 107-111Nejjari, F., Rotondo, D., Puig, V., Innocenti, M., Quasi-LPV modelling and non-linear identification of a twin rotor system (2012) Proceedings of the 20th Mediterranean Conference on Control Automation, pp. 229-234Subudhi, B., Jena, D., Nonlinear system identification of a twin rotor MIMO system (2009) Proceedings of the IEEE Region 10 Conference, TENCON'09, pp. 1-6Rahideh, A., Shaheed, M., Dynamic modelling of a twin rotor MIMO system using grey box approach (2008) Proceedings of the 5th International Symposium on Mechatronics and Its Applications, ISMA'08, pp. 1-6Aldebrez, F., Darus, I., Tokhi, M., Dynamic modelling of a twin rotor system in hovering position (2004) Proceedings of the First International Symposium on Control, Communications and Signal Processing, pp. 823-826Maciel, L., Lemos, A., Gomide, F., Ballini, R., Evolving fuzzy systems for pricing fixed income options (2012) Evolving Systems, 3 (1), pp. 5-18Lughofer, E., On-line assurance of interpretability criteria in evolving fuzzy systems-achievements, new concepts and open issues (2013) Information Sciences, 251 (0), pp. 22-46Pratama, M., Anavatti, S., Lughofer, E., Evolving fuzzy rule-based classifier based on GENEFIS (2013) Proceedings of the IEEE International Conference on Fuzzy Systems, FUZZ-IEEE'13, pp. 1-8Tung, S., Quek, C., Guan, C., ET2FIS: An evolving type-2 neural fuzzy inference system (2013) Information Sciences, 220 (0), pp. 124-148Cernuda, C., Lughofer, E., Marzinger, W., Kasberger, J., NIRbased quantification of process parameters in polyetheracrylat (PEA) production using flexible non-linear fuzzy systems (2011) Chemometrics and Intelligent Laboratory Systems, 109 (1), pp. 22-33Cernuda, C., Lughofer, E., Suppan, L., Roder, T., Schmuch, R., Hintenaus, P., Marzinger, W., Kasberger, J., Evolving chemometric models for predicting dynamic process parameters in viscose production (2012) Analytica Chimica Acta, 725 (0), pp. 22-38Smith, F., Tighe, A., Adapting in an uncertain world (2005) Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, 6, pp. 5958-5963Barros, J., Dexter, A., Evolving fuzzy model-based adaptive control (2007) Proceedings of the IEEE International Conference on Fuzzy Systems, FUZZ-IEEE'07, pp. 1-5Angelov, P., Zhou, X., Filev, D., Lughofer, E., Architectures for evolving fuzzy rule-based classifiers (2007) Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, pp. 2050-2055Lughofer, E., On-line incremental feature weighting in evolving fuzzy classifiers (2011) Fuzzy Sets Systems, 163 (1), pp. 1-23Iglesias, J., Angelov, P., Ledezma, A., Sanchis, A., Modelling evolving user behaviours (2009) Proceedings of the IEEE Workshop on Evolving and Self-Developing Intelligent Systems, ESDIS'09, pp. 16-23Lemos, A., Caminhas, W., Gomide, F., Fuzzy multivariable gaussian evolving approach for fault detection and diagnosis (2010) Computational Intelligence for Knowledge-Based Systems Design, Ser. Lecture Notes in Computer Science, 6178, pp. 360-369Lughofer, E., Macian, V., Guardiola, C., Klement, E., Identifying static and dynamic prediction models for NOx emissions with evolving fuzzy systems (2011) Applied Soft Computing, 11 (2), pp. 2487-2500Leite, D., Ballini, R., Costa, P., Gomide, F., Evolving fuzzy granular modeling from nonstationary fuzzy data streams (2012) Evolving Systems, 3, pp. 65-79Rahideh, A., Shaheed, M., Robust model predictive control of a twin rotor MIMO system (2009) Proceedings of IEEE International Conference on the Mechatronics, ICM'09, pp. 1-6Rahideh, A., Shaheed, M., Huijberts, H., Stable adaptive model predictive control for nonlinear systems (2008) Proceedings of the American Control Conference, pp. 1673-1678Rahideh, A., Bajodah, A., Shaheed, M., Real time adaptive nonlinear model inversion control of a twin rotor MIMO system using neural networks (2012) Engineering Applications of Artificial Intelligence, 25 (6), pp. 1289-1297Toha, S., Tokhi, M., Inverse model based control for a twin rotor system (2010) Proceedings of the IEEE 9th International Conference on Cybernetic Intelligent Systems, CIS'10, pp. 1-5Silva, A., Caminhas, W., Lemos, A., Gomide, F., A fast learning algorithm for evolving neo-fuzzy neuron (2014) Applied Soft Computing, Part B, 14 (0), pp. 194-209Yamakawa, T., Uchino, E., Miki, T., Kusabagi, H., A neo fuzzy neuron and its applications to system identification and predictions to system behavior (1992) Proceedings of the International Conference on Fuzzy Logic and Neural Networks, 1, pp. 477-484Caminhas, W., Gomide, F., A fast learning algorithm for neofuzzy networks (2000) Proceedings of the Information Processing and Management of Uncertainty in Knowledge Based Systems, IPMU'00, 1 (1), pp. 1784-1790Lemos, A., Caminhas, W., Gomide, F., Fuzzy evolving linear regression trees (2011) Evolving Systems, 2, pp. 1-14Mathworks, I., (2009) Real-time Workshop 7 Users Guide, , Natick, MA, USAKasabov, N., Song, Q., Denfis: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction (2002) IEEE Transactions on Fuzzy Systems, 10 (2), pp. 144-154Angelov, P., Filev, D., An approach to online identification of takagi-sugeno fuzzy models (2004) IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, 34 (1), pp. 484-498Angelov, P., Zhou, X., Evolving fuzzy systems from data streams in real-time (2006) Proceedings of the International Symposium on Evolving Fuzzy Systems, pp. 29-3",Real-time Nonlinear Modeling Of A Twin Rotor Mimo System Using Evolving Neuro-fuzzy Network,,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/CICA.2014.7013229,core
216204075,2014-10-01T00:00:00,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments",Automated Real-Time Classification and Decision Making in Massive Data Streams from Synoptic Sky Surveys,https://core.ac.uk/download/216204075.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
102579671,2015-08-26,"Abstract—For decades, photographs have been used to docu-ment space-time events and they have often served as evidence in courts. Although photographers are able to create composites of analog pictures, this process is very time consuming and re-quires expert knowledge. Today, however, powerful digital image editing software makes image modifications straightforward. This undermines our trust in photographs and, in particular, questions pictures as evidence for real-world events. In this paper, we ana-lyze one of the most common forms of photographic manipulation, known as image composition or splicing. We propose a forgery detection method that exploits subtle inconsistencies in the color of the illumination of images. Our approach is machine-learning-based and requires minimal user interaction. The technique is ap-plicable to images containing two or more people and requires no expert interaction for the tampering decision. To achieve this, we incorporate information from physics- and statistical-based illumi-nant estimators on image regions of similar material. From these illuminant estimates, we extract texture- and edge-based features which are then provided to a machine-learning approach for au-tomatic decision-making. The classification performance using an SVM meta-fusion classifier is promising. It yields detection rates of 86 % on a new benchmark dataset consisting of 200 images, and 83 % on 50 images that were collected from the Internet. Index Terms—Color constancy, illuminant color, image foren-sics, machine learning, spliced image detection, texture and edge descriptors. I",Exposing Digital Image Forgeries by Illumination Color Classification,,,,,core
102675782,2015-09-03,"Formal design and analysis of embedded control software re-lies on mathematical models of dynamical systems, and such models can be hard to obtain. In this paper, we focus on au-tomatic construction of piecewise affine models from input-output data. Given a set of examples, where each example consists of a d-dimensional real-valued input vector mapped to a real-valued output, we want to compute a set of affine functions that covers all the data points up to a specified degree of accuracy, along with a disjoint partitioning of the space of all inputs defined using a Boolean combination of affine inequalities with one region for each of the learnt func-tions. While traditional machine learning algorithms such as linear regression can be adapted to learn the set of affine functions, we develop new techniques based on automatic construction of interpolants to derive precise guards defin-ing the desired partitioning corresponding to these functions. We report on a prototype tool, Mosaic, implemented in Matlab. We evaluate its performance using some synthetic data, and compare it against known techniques using data-sets modeling electronic placement process in pick-and-place machines. 1",Precise Piecewise Affine Models from Input-Output Data,,,,,core
31021561,2015-01-01T00:00:00,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were - ntegrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods",Utilizing Artificial Intelligence for Achieving a Robust Architecture for Future Robotic Spacecraft,https://core.ac.uk/download/31021561.pdf,,,,core
275626576,2015-02-01T00:00:00,"The final publication is available at Springer via http://dx.doi.org/10.1007/s11227-014-1307-6.Control architectures based on Emotions are becoming promising solutions for the implementation of future robotic agents. The basic controllers of the architecture are the emotional processes that decide which behaviors of the robot must activate to fulfill the objectives. The number of emotional processes increases (hundreds of millions/s) with the complexity level of the application, reducing the processing capacity of the main processor to solve complex problems (millions of decisions in a given instant). However, the potential parallelism of the emotional processes permits their execution in parallel on FPGAs or Multicores, thus enabling slack computing in the main processor to tackle more complex dynamic problems. In this paper, an emotional architecture for mobile robotic agents is presented. The workload of the emotional processes is evaluated. Then, the main processor is extended with FPGA co-processors through Ethernet link. The FPGAs will be in charge of the execution of the emotional processes in parallel. Different Stratix FPGAs are compared to analyze their suitability to cope with the proposed mobile robotic agent applications. The applications are set up taking into account different environmental conditions, robot dynamics and emotional states. Moreover, the applications are run also on Multicore processors to compare their performance in relation to the FPGAs. Experimental results show that Stratix IV FPGA increases the performance in about one order of magnitude over the main processor and solves all the considered problems. Quad-Core increases the performance in 3.64 times, allowing to tackle about 89 % of the considered problems. Quad-Core has a lower cost than a Stratix IV, so more adequate solution but not for the most complex application. Stratix III could be applied to solve problems with around the double of the requirements that the main processor could support. Finally, a Dual-Core provides slightly better performance than stratix III and it is relatively cheaper.This work was supported in part under Spanish Grant PAID/2012/325 of ""Programa de Apoyo a la Investigacion y Desarrollo. Proyectos multidisciplinares"", Universitat Politecnica de Valencia, Spain.Domínguez Montagud, CP.; Hassan Mohamed, H.; Crespo, A.; Albaladejo Meroño, J. (2015). Multicore and FPGA implementations of emotional-based agent architectures. Journal of Supercomputing. 71(2):479-507. doi:10.1007/s11227-014-1307-6S479507712Malfaz M, Salichs MA (2010) Using MUDs as an experimental platform for testing a decision making system for self-motivated autonomous agents. Artif Intell Simul Behav J 2(1):21–44Damiano L, Cañamero L (2010) Constructing emotions. Epistemological groundings and applications in robotics for a synthetic approach to emotions. In: Proceedings of international symposium on aI-inspired biology, The Society for the Study of Artificial Intelligence, pp 20–28Hawes N, Wyatt J, Sloman A (2009) Exploring design space for an integrated intelligent system. Knowl Based Syst 22(7):509–515Sloman A (2009) Some requirements for human-like robots: why the recent over-emphasis on embodiment has held up progress. Creat Brain Like Intell 2009:248–277Arkin RC, Ulam P, Wagner AR (2012) Moral decision-making in autonomous systems: enforcement, moral emotions, dignity, trust and deception. In: Proceedings of the IEEE, Mar 2012, vol 100, no 3, pp 571–589iRobot industrial robots website. http://www.irobot.com/gi/ground/ . Accessed 22 Sept 2014Moravec H (2009) Rise of the robots: the future of artificial intelligence. Scientific American, March 2009. http://www.scientificamerican.com/article/rise-of-the-robots/ . Accessed 14 Oct 2014.Thu Bui L, Abbass HA, Barlow M, Bender A (2012) Robustness against the decision-maker’s attitude to risk in problems with conflicting objectives. IEEE Trans Evolut Comput 16(1):1–19Pedrycz W, Song M (2011) Analytic hierarchy process (AHP) in group decision making and its optimization with an allocation of information granularity. IEEE Trans Fuzzy Syst 19(3):527–539Lee-Johnson CP, Carnegie DA (2010) Mobile robot navigation modulated by artificial emotions. IEEE Trans Syst Man Cybern Part B 40(2):469–480Daglarli E, Temeltas H, Yesiloglu M (2009) Behavioral task processing for cognitive robots using artificial emotions. Neurocomputing 72(13):2835–2844Ventura R, Pinto-Ferreira C (2009) Responding efficiently to relevant stimuli using an emotion-based agent architecture. Neurocomputing 72(13):2923–2930Arkin RC, Ulam P, Wagner AR (2012) Moral decision-making in autonomous systems: enforcement, moral emotions, dignity, trust and deception. Proc IEEE 100(3):571–589Salichs MA, Malfaz M (2012) A new approach to modeling emotions and their use on a decision-making system for artificial agents. Affect Comput IEEE Trans 3(1):56–68Altera Corporation (2011) Stratix III device handbook, vol 1–2, version 2.2. http://www.altera.com/literature/lit-stx3.jsp . Accessed 14 Oct 2014.Altera Corporation (2014) Stratix IV device handbook, vol 1–4, version 5.9. http://www.altera.com/literature/lit-stratix-iv.jsp . Accessed 14 Oct 2014.Naouar MW, Monmasson E, Naassani AA, Slama-Belkhodja I, Patin N (2007) FPGA-based current controllers for AC machine drives: a review. IEEE Trans Ind Electr 54(4):1907–1925Intel Corporation (2014) Desktop 4th generation Intel Core Processor Family, Desktop Intel Pentium Processor Family, and Desktop Intel Celeron Processor Family, Datasheet, vol 1, 2March JL, Sahuquillo J, Hassan H, Petit S, Duato J (2011) A new energy-aware dynamic task set partitioning algorithm for soft and hard embedded real-time systems. Comput J 54(8):1282–1294Del Campo I, Basterretxea K, Echanobe J, Bosque G, Doctor F (2012) A system-on-chip development of a neuro-fuzzy embedded agent for ambient-intelligence environments. IEEE Trans Syst Man Cybern Part B 42(2):501–512Pedraza C, Castillo J, Martínez JI, Huerta P, Bosque JL, Cano J (2011) Genetic algorithm for Boolean minimization in an FPGA cluster. J Supercomput 58(2):244–252Orlowska-Kowalska T, Kaminski M (2011) FPGA implementation of the multilayer neural network for the speed estimation of the two-mass drive system. IEEE Trans Ind Inf 7(3):436–445Cassidy AS, Merolla P, Arthur JV, Esser SK, Jackson B, Alvarez-icaza R, Datta P, Sawada J, Wong TM, Feldman V, Amir A, Ben-dayan D, Mcquinn E, Risk WP, Modha DS (2013) Cognitive computing building block: a versatile and efficient digital neuron model for neurosynaptic cores. In: Proceedings of international joint conference on neural networks, IEEE (IJCNN’2013)IBM Cognitive Computing and Neurosynaptic chips website. http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml . Accessed 22 Sept 2014Seo E, Jeong J, Park S, Lee J (2008) Energy efficient scheduling of real-time tasks on multicore processors. IEEE Trans Parallel Distrib Syst 19(11):1540–1552Lehoczky J, Sha L, Ding Y (1989) The rate monotonic scheduling algorithm: exact characterization and average case behavior. In: Proceedings of real time systems symposium, IEEE 1989, pp 166–171Ng-Thow-Hing V, Lim J, Wormer J, Sarvadevabhatla RK, Rocha C, Fujimura K, Sakagami Y (2008) The memory game: creating a human-robot interactive scenario for ASIMO. In: Proceedings of intelligent robots and systems, 2008, IROS 2008, IEEE/RSJ international conference, pp 779–78",Multicore and FPGA implementations of emotional-based agent architectures,https://riunet.upv.es/bitstream/handle/10251/85419/Dom%c3%adnguez%2c%20C.%20et%20al.%20-%20Multicore%20and%20FPGA%20implementations%20of%20emotional-based....pdf?sequence=2&isAllowed=y,'Springer Science and Business Media LLC',,10.1007/s11227-014-1307-6,core
296628806,2015-11-26T14:38:13Z,"In this paper, we discuss the compression of waveforms obtained from measurements of power system quantities and analyze the reasons why its importance is growing with the advent of smart grid systems. While generation and transmission networks already use a considerable number of automation and measurement devices, a large number of smart monitors and meters are to be deployed in the distribution network to allow broad observability and real-time monitoring. This situation creates new requirements concerning the communication interface, computational intelligence and the ability to process data or signals and also to share information. Therefore, a considerable increase in data exchange and in storage is likely to occur. In this context, one must achieve an efficient use of channel communication bandwidth and a reduced need of storage space for power system data. Here, we review the main compression techniques devised for electric signal waveforms providing an overview of the achievements obtained in the past decades. Additionally, we envision some smart grid scenarios emphasizing open research issues regarding compression of electric signal waveforms. We expect that this paper will contribute to motivate joint research efforts between electrical power system and signal processing communities in the area of signal waveform compression. © 2010-2012 IEEE.51291302Gu, I.Y.-H., Styvaktakis, E., Bridge the gap: Signal processing for power quality applications (2003) Signal processing, 66 (1), pp. 83-96. , JulRibeiro, M.V., Szczupak, J., Iravani, M.R., Gu, I.Y.-H., Dash, P.K., Mamishev, A.V., Emerging signal processing techniques for power quality applications (2007) EURASIP J. Adv. Signal Process, 2007 (2), pp. 16-16. , http://dx.doi.org/10.1155/2007/87425, Jun. [Online]. AvailableBollen, M.H.J., Gu, I.Y.-H., Santoso, S., McGranaghan, M.F., Crossley, P.A., Ribeiro, M.V., Ribeiro, P.F., Bridging the gap between signal and power (2009) IEEE Signal Process. Mag., 26 (4), pp. 12-31. , JulBollen, M.H.J., Ribeiro, P.F., Gu, I.Y.-H., Duque, C.A., Trends, challenges and opportunities in power quality research (2009) Eur. Trans. Electr. Power, 4 (1), pp. 2-18Bollen, M.H.J., (2000) Understanding Power Quality Problems-Voltage Sags and Interruptions, , Piscataway, NJ USA: IEEE PressBollen, M.H.J., Gu, I.Y.-H., (2006) Signal Processing of Power Quality Disturbances, , New York: Wiley-IEEE PressMehta Ketan, Data compression for digital data from power systems disturbances: Requirements and technique evaluation (1989) IEEE Transactions on Power Delivery, 4 (3), pp. 1683-1688. , DOI 10.1109/61.32659Toivonen, L., Morsky, J., Measurement and processing of distortion quantities in a portable, multi-purpose analyzer (1993) IEEE Trans. Power Del., 8 (4), pp. 1736-1746. , Oct(2011) Smart Grid, , http://www.oe.energy.gov/smartgrid.htm, U.S. Department of Energy, Jun. [Online]. AvailableAmin, S.M., Wollenberg, B., Toward a smart grid: Power delivery for the 21st century (2005) IEEE Power Energy Mag., 3 (5), pp. 34-41. , Sep.-OctVu, K., Begouic, M.M., Novosel, D., Grids get smart protection and control (1997) IEEE Comput. Apps. Power, 10 (4), pp. 40-44. , OctVojdani, A., Smart integration (2008) IEEE Power Energy Mag., 6 (6), pp. 71-79. , Nov.-DecIpakchi, A., Albuyeh, F., Grid of the future (2009) IEEE Power Energy Mag., 7 (2), pp. 52-62. , Mar.-Apr(2011) Smart Grid Demonstration-integration of Distributed Energy Resources, , http://www.smartgrid.epri.com/Demo.aspx, EPRI [Online]. Available JunAlbu, M.M., Neurohr, R., Apetrei, D., Silvas, I., Federenciuc, D., Monitoring voltage and frequency in smart distribution grids. A case study on data compression and accessibility (2010) Proc. IEEE PES Gen. Meet., pp. 1-6Abart, A., Lugmair, A., Schenk, A., Smart metering features for managing low voltage distribution grids (2009) Proc. 20th CIRED-Part, 2, p. 1Zhang, D., Bi, Y., Zhao, J., A new data compression algorithm for power quality online monitoring (2009) Proc. SUPERGEN'09, pp. 1-4Chicco, G., Challenges for smart distribution systems: Data representation and optimization objectives (2010) Proc. 12th OPTIM, pp. 1236-1244Albu, M.M., Neurohr, R., Apetrei, D., Silvas, I., Federenciuc, D., Monitoring voltage and frequency in smart distribution grids. A case study on data compression and accessibility (2010) Proc. PES Gen. Meet., pp. 1-6Ning, J., Wang, J., Gao, W., Liu, C., A wavelet-based data compression technique for smart grid (2011) IEEE Trans. Smart Grid, 2 (1), pp. 212-218. , MarDas, S., Rao, P., Principal component analysis based compression scheme for power system steady state operational data (2011) IEEE PES Innov. Smart Grid Technol.-India, pp. 95-100Parseh, R., Acevedo, S.S., Kansanen, K., Molinas, M., Ramstad, T.A., Real-time compression of measurements in distribution grids (2012) Proc. IEEE 3rd Int. Conf. Smart Grid Commun. (SmartGridComm.), pp. 223-228Recommended practice on monitoring electric power quality (2009) Ieee p1159/p6Bingham, R.P., Kreiss, D., Santoso, S., Advances in data reduction techniques for power quality instrumentation (1995) Proc. 3rd Eur. Power Quality Conf., , Bremen, GermanyWilkinson, W.A., Cox, M.D., Discrete wavelet analysis of power system transients (1996) IEEE Transactions on Power Systems, 11 (4), pp. 2038-2044Pillay, P., Bhattacharjee, A., Application of wavelets to model short-term power system disturbances (1996) IEEE Trans. Power Syst., 11 (4), pp. 2031-2037. , NovToivonen, L., Morsky, J., Measurement and processing of distortion quantities in a portable, multi-purpose analyzer (1993) IEEE Trans. Power Del., 8 (4), pp. 1736-1746. , OctKhan, A.K., Monitoring power for the future (2001) Power Engineering Journal, 15 (2), pp. 81-85Heydt, G.T., Gunther, E., Post-measurement processing of electric power quality data (1996) IEEE Trans. Power Del., 11 (4), pp. 1853-1859. , OctSantoso, S., Powers, E.J., Grady, W.M., Power quality disturbance data compression using wavelet transform methods (1997) IEEE Trans. Power Del., 12 (3), pp. 1250-1257. , JulHsieh, C.-T., Huang, S.-J., Huang, C.-L., Data reduction of power quality disturbances - A wavelet transform approach (1998) Electric Power Systems Research, 47 (2), pp. 79-86. , PII S0378779698000431Chung, J., Powers, E.J., Grady, W., Bhatt, S.C., Variable rate power disturbance signal compression using embedded zerotree wavelet transform coding (1999) Proc. 1999 IEEE Power Eng. Soc. Winter Meet., 2, pp. 1305-1309. , Jan.-4 FebLittler, T.B., Morrow, D.J., Wavelets for the analysis and compression of power system disturbances (1999) IEEE Trans. Power Del., 14 (2), pp. 358-364. , AprHeydt, G.T., Bhatt, S.C., Present and future trends and needs in electric power quality sensors and instrumentation (1999) Elect. Mach. Power Syst., 27 (7), pp. 691-700Hamid, E.Y., Kawasaki, Z.-I., Wavelet-based data compression of power system disturbances using the minimum description length criterion (2002) IEEE Transactions on Power Delivery, 17 (2), pp. 460-466. , DOI 10.1109/61.997918, PII S0885897702032776Panda, G., Dash, P.K., Pradhan, A.K., Meher, S.K., Data compression of power quality events using the slantlet transform (2002) IEEE Transactions on Power Delivery, 17 (2), pp. 662-667. , DOI 10.1109/61.997957, PII S0885897702011755Hsieh, C.-T., Huang, S.-J., Disturbance data compression of a power system using the huffman coding approach with wavelet transform enhancement (2003) IEE Proc. Gener., Transm., Distrib., 150 (1), pp. 7-14. , JanDash, P.K., Panigrahi, B.K., Sahoo, D.K., Panda, G., Power quality disturbance data compression, detection, and classification using integrated spline wavelet and s-transform (2003) IEEE Trans. Power Del., 18 (2), pp. 595-600. , AprWu, C.-J., Fu, T.-H., Huang, C.-P., Data compression technique in recording electric arc furnace voltage and current waveforms for tracking power quality (2003) Proc. IEEE PES Transm. Distrib. Conf. Expo., 1, pp. 383-388Shang, L., Krebs, J.J.R., Efficiency analysis of data compression of power system transients using wavelet transform (2003) Proc. IEEE Bologna Power Tech. Conf., 4, p. 6Meher, S.K., Pradhan, A.K., Panda, G., An integrated data compression scheme for power quality events using spline wavelet and neural network (2004) Elect. Power Syst. Res., 69 (2-3), pp. 213-220Gerek, O.N., Ece, D., 2-d analysis and compression of power-quality event data (2004) IEEE Trans. Power Del., 19 (2), pp. 791-798. , AprHuang, S.-J., Jou, M.-J., Application of arithmetic coding for electric power disturbance data compression with wavelet packet enhancement (2004) IEEE Trans. Power Syst., 19 (3), pp. 1334-1341. , AugLorio, F., Magnago, F., Analysis of data compression methods for power quality events (2004) 2004 IEEE Power Engineering Society General Meeting, 1, pp. 504-509. , 2004 IEEE Power Engineering Society General MeetingYuan, Y., Yu, X., Du, H., Power system fault data compression using the wavelet transform and vector quantification (2006) Proc. IEEE POWERCOM, Oct., pp. 1-6Gerek, O.N., Ece, D.G., Compression of power quality event data using 2D representation (2008) Electric Power Systems Research, 78 (6), pp. 1047-1052. , DOI 10.1016/j.epsr.2007.08.006, PII S0378779607001769Qing, A., Hongtao, Z., Zhikun, H., Zhiwen, C., A compression approach of power quality monitoring data based on two-dimension dct (2011) Proc. ICMTMA'11, 1, pp. 20-24Lovisolo, L., Da Silva, E.A.B., Rodrigues, M.A.M., Diniz, P.S.R., Efficient coherent adaptive representations of monitored electric signals in power systems using damped sinusoids (2005) IEEE Transactions on Signal Processing, 53 (10), pp. 3831-3846. , DOI 10.1109/TSP.2005.855400Tcheou, M.P., Lovisolo, L., Da Silva, E.A.B., Rodrigues, M.A.M., Diniz, P.S.R., Optimum rate-distortion dictionary selection for compression of atomic decompositions of electric disturbance signals (2007) IEEE Signal Processing Letters, 14 (2), pp. 81-84. , DOI 10.1109/LSP.2006.882117Lovisolo, L., Tcheou, M.P., Da Silva, E.A.B., Rodrigues, M.A.M., Diniz, P.S.R., Modeling of electric disturbance signals using damped sinusoids via atomic decompositions and its applications (2007) EURASIP J. Adv. Signal Process, 2007, p. 15. , Article ID 29 507Ribeiro, M.V., Romano, J.M.T., Duque, C.A., An enhanced data compression method for applications in power quality analysis (2001) IECON Proceedings (Industrial Electronics Conference), 3, pp. 676-681Ribeiro, M.V., Duque, C.A., The word length influence on waveform coding techniques based on wavelet transform applied to disturbance compression (2002) Proc. 10th IEEE ICHQP, 1, pp. 139-143Ramos, F.R., Riberto, M.V., Romano, J.M.T., Duque, C.A., On signal processing approach for event detection and compression applied to power quality evaluation (2002) Proc. 10th EEE ICHQP, 1, pp. 133-138Ribeiro, M.V., Romano, J.M.T., Duque, C.A., An improved method for signal processing and compression in power quality evaluation (2004) IEEE Trans. Power Del., 19 (2), pp. 464-471. , AprRibeiro, M.V., Park, S.H., Romano, J.M.T., Mitra, S.K., A novel MDL-based compression method for power quality applications (2007) IEEE Transactions on Power Delivery, 22 (1), pp. 27-36. , DOI 10.1109/TPWRD.2006.887091Yun, Z., Xiaoming, L., Lingxu, A., Jian, S., Lihui, W., Research on encoding/decoding method of electric physical information based on lms-adpcm algorithm (2011) Proc. Int. Conf. Adv. Power Syst. Autom. Protection, 1, pp. 795-800Kraus, J., Tobiska, T., Bubla, V., Looseless encodings and compression algorithms applied on power quality datasets (2009) Proc. 2nd IEEE CIRED-Part 1, pp. 1-4Kraus, J., Stepan, P., Kukacka, L., Optimal data compression techniques for smart grid and power quality trend data (2012) Proc. IEEE ICHQP, pp. 707-712Recommended Practice for the Transfer of Power Quality Data, , IEEEP1159.3/D9Tse, N.C.F., Chan, J.Y.C., Lai, L.L., Development of a smart metering scheme for building smart grid system (2009) Proc. 8th APSCOM, pp. 1-5Zhang, M., Li, K., Hu, Y., A high efficient compression method for power quality applications (2011) IEEE Trans. Instrum. Meas., 60 (6), pp. 1976-1985. , JunXu, W., Component modeling issues for power quality assessment (2001) IEEE Power Eng. Rev., 21 (11), pp. 12-15. , 17 NovRibeiro, M.V., Pereira, J.L.R., Classification of single and multiple disturbances in electric signals (2007) EURASIP J. Adv. Signal Process., 2007 (2), p. 18. , JunInterharmonics: Theory and modeling (2007) IEEE Trans. Power Del., 22 (4), pp. 2335-2348. , IEEE Task Force on Harmonics Modeling and Simulation OctIeee, , http://grouper.ieee.org/1433, IEEE PES Working Group 1433 Power Quality [Online]. AvailableSchweitzer Edmund, O., Hou Daqing, Filtering for protective relays (1993) Communications, Computers and Power in the Modern Environment, pp. 15-23Wiot, D., A new adaptive transient monitoring scheme for detection of power system events (2004) IEEE Trans. Power Del., 19 (1), pp. 42-48Lobos, T., Rezmer, J., Koglin, H.-J., Analysis of power systems transients using wavelets and prony method (2001) Proc. IEEE Porto Power Tech. Conf., 4, pp. 1-4Tawfik, M.M., Morcos, M.M., ANN-based techniques for estimating fault location on transmission lines using prony method (2001) IEEE Transactions on Power Delivery, 16 (2), pp. 219-224. , DOI 10.1109/61.915486, PII S0885897701015394Bujanowski, B.J., Pierre, J.W., Hietpas, S.M., Sharpe, T.L., Pierre, D.A., A comparison of several system identification methods with application to power systems (1993) Proc. 36th MWSCAS, 1, pp. 64-67Galli, A.W., Heydt, G.T., Ribeiro, P.F., Exploring the power of wavelet analysis (1996) IEEE Comput. Appl. Power, 9 (4), pp. 37-41. , OctChung, J., Powers, E.J., Grady, W.M., Bhatt, S.C., Electric power transient disturbance classification using wavelet-based hidden Markov models (2000) Proc. IEEE ICASSP, 6, pp. 3662-3665Pillay, P., Bhattachrjee, A., Application of wavelets to model shortterm power systemdisturbances (1996) IEEE Trans. Power Syst., 11 (4), pp. 2031-2037. , NovPoisson, O., Rioual, P., Meunier, M., Detection and measurement of power quality disturbances using wavelet transforms (2000) IEEE Trans. Power Del., 15 (3), pp. 1039-1044. , JulLiao, H.Y.C., A de-noising scheme for enhancing wavelet-based power quality monitoring systems (2001) IEEE Trans. Power Del., 16 (3), pp. 353-360. , JulSantoso, S., Grady, W.M., Powers, E.J., Lamoore, J., Bhatt, S.C., Characterization of distribution power quality events with fourier and wavelets transforms (2000) IEEE Trans. Power Del., 15 (1), pp. 247-254. , JanKarimi, M., Mokhtari, H., Iravani, M.R., Wavelet based on-line disturbance detection for power quality applications (2000) IEEE Transactions on Power Delivery, 15 (4), pp. 1212-1220. , DOI 10.1109/61.891505Anis Ibrahim, W.R., Morcos, M.M., Artificial intelligence and advanced mathematical tools for power quality applications: A survey (2002) IEEE Transactions on Power Delivery, 17 (2), pp. 668-673. , DOI 10.1109/61.997958, PII S0885897702027474Gosh, A.K., Lubkeman, D.L., The classification of power system disturbance waveforms using a neural network approach (1995) IEEE Trans. Power Del., 10 (1), pp. 109-115. , JanLovisolo, L., Figueiredo, K.T., Laporte Menezes, L.De., Neto, J.A.M., Dos Santos Rocha, J.C., Location of faults generating short duration voltage variations in distribution systems regions from records captured at one point and decomposed into damped sinusoids IET Gener., Transm., Distrib., , Accepted for publicationYang, Q., Wang, J., Sima, W., Chen, L., Yuan, T., Mixed over-voltage decomposition using atomic decompositions based on a damped sinusoids atom dictionary (2011) Energies, 4 (9), pp. 1410-1427Ribeiro, M.V., Marques, C.A.G., Duque, C.A., Cerqueira, A.S., Pereira, J.L.R., Detection of disturbances in voltage signals for power quality analysis using HOS (2007) EURASIP J. Adv. Signal Process., 2007 (2), p. 13. , JunSayood, K., (2000) Introduction to Data Compression, , 2nd ed. San Francisco, CA, USA: Morgan KaufmanBell, T.C., Witten, I.H., Cleary, J.G., (1990) Prentice Hall, , Text Compression/Timothy C. Bell, John G. Cleary, Ian H. Witten. Englewood Cliffs, NJ, USA: Prentice-HallQing, A., Hongtao, Z., Zhikun, H., Zhiwen, C., A compression approach of power quality monitoring data based on two-dimension dct (2011) Proc. 3rd ICMTMA, 1, pp. 20-24Tcheou, M.P., Miranda, A.L., Lovisolo, L., Da Silva, E.A., Rodrigues, M.A., Diniz, P.S., How far can one compress digital fault records analysis of a matching pursuit based algorithm (2012) Digit. Signal Process., 22 (2), pp. 288-297Nascimento, F.A.O., Data compression algorithm for transient recording system (1997) Proc. IEEE ISIE, 3, pp. 1126-1130Mallat, S., (1998) A Wavelet Tour of Signal Processing, , 2nd ed. San Diego, CA, USA: AcademicDaubechies, I., (1991) Ten Lectures on Wavelets, , Philadelphia PA USA: SIAMShapiro, J.M., Embedded image coding using zerotrees of wavelet coefficients (1993) IEEE Trans. Signal Process, 41 (12), pp. 3445-3462. , DecLiu, S., An adaptive Kalman filter for dynamic estimation of harmonic signals (1998) Proc. 8th IEEE ICHQP, 2, pp. 636-640Romano, J.M.T., Bellanger, M., Fast least squares adaptive notch filtering (1988) IEEE Trans. Acoust., Speech, Signal Process, 36 (9), pp. 1536-1540. , SepDiniz, P.S.R., (2008) Adaptive Filtering: Algorithms and Practical Implementations, , 3rd ed. Boston, MA, USA: SpringerCheng, Y.T., TMS320C62x Algorithm: Sine wave generation (2000) Texas Instruments, Dallas, TX, USA, Tech. Rep., , NovSaito, N., Simultaneous noise suppression and signal compression using a library of orthonormal bases and the minimum description length criterion (1994) Wavelets in Geophysics, pp. 299-324. , San Diego, CA, USA: AcademicKrim, H., Tucker, D., Mallat, S., Donoho, D., On denoising and best signal representation (1999) IEEE Transactions on Information Theory, 45 (7), pp. 2225-2238. , DOI 10.1109/18.796365Krim, H., Schick, I.C., Minimax description length for signal denoising and optimized representation (1999) IEEE Trans. Inf. Theory, 45 (3), pp. 898-908. , AprHansen, M., Yu, B., Wavelet thresholding via MDL for natural images (2000) IEEE Trans. Inf. Theory, 46 (5), pp. 1778-1788. , AugChang, S.G., Yu, B., Vitterli, M., Adaptive wavelet thresholding for image denoising and compression (2000) IEEE Trans. Image Process, 9 (9), pp. 1532-1546. , SepRissanen, J., Modeling by shortest data description (1978) Automatica, 14, pp. 465-471Barron, A., Rissanen, J., Yu, B., The minimum description length principle in coding and modeling (1998) IEEE Transactions on Information Theory, 44 (6), pp. 2743-2760. , PII S0018944898052845Mallat, S., Zhang, Z., Matching pursuitswith time-frequency dictionaries (1993) IEEE Trans. Signal Process, 41 (12), pp. 3397-3415. , DecGalli, S., Scaglione, A., Wang, Z., For the grid and through the grid: The role of power line communications in the smart grid (2011) Proc. IEEE, 99 (6), pp. 998-1027. , JunGharavi, H., Hu, B., Multigate communication network for smart grid (2011) Proc. IEEE, 99 (6), pp. 1028-1045. , JunSauter, T., Lobashov, M., End-to-end communication architecture for smart grids (2011) IEEE Trans. Ind. Electron., 58 (4), pp. 1218-1228. , AprGomez-Exposito, A., Abur, A., Jaen Villa A.De, La., Gomez Quiles, C., A multilevel state estimation paradigm for smart grids (2011) Proc. IEEE, 99 (6), pp. 952-976. , JunArnold, G.W., Challenges and opportunities in smart grid: A position article (2011) Proc. IEEE, 99 (6), pp. 922-927. , Junhttp://www.powermonitors.com, Power Monitors Inc. [Online]. Available(1998) Method for Objective Measurements of Perceived Audio Quality, , ITU-R Rec. BS.1387 Geneva, Switzerland ITUWang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: From error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612. , AprKezunovic, M., Rikalo, M., Automating the analysis of faults and power quality (1999) IEEE Comput. Appl. Power, 12 (1), pp. 46-50. , JanStankovic, V., Stankovic, L., Shuang, W., Cheng, S., Distributed compression for condition monitoring of wind farms (2013) IEEE Trans. Sustain. Energy, 4 (1), pp. 174-181. , Ja",The Compression Of Electric Signal Waveforms For Smart Grids: State Of The Art And Future Trends,,,,10.1155/2007/87425,core
379197212,2014-10-01T00:00:00,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments",Automated Real-Time Classification and Decision Making in Massive Data Streams from Synoptic Sky Surveys,https://core.ac.uk/download/379197212.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
100329043,2014-12-12,"Abstract—The notion of agent more and more appears in different contexts of computer science, often with different meanings. The main acceptation is the AI (Artificial Intelli-gence) and Distributed AI one, where agents are essentially exploited as a technique to develop special-purpose systems exhibiting some kind of intelligent behavior. In this paper, we introduce a further perspective, shifting the focus from AI to computer programming and programming languages. In particular, we consider agents and related concepts as general-purpose abstractions useful for programming software systems in general, conceptually extending object-oriented program-ming with features that – we argue – are effective to tackle some main challenges of modern software development. Accordingly, the main contribution of the work is first the definition of a conceptual space framing the basic features that characterize the agent-oriented approach as a programming paradigm, then its validation in practice by using a platform called JaCa, with real-word programming examples. Keywords-agent-oriented programming; multi-agent systems; concurrent programming; distributed programming I",Agent-Oriented Computing: Agents as a Paradigm for Computer Programming and Software Development,,,,,core
275614126,2015-09-07T00:00:00,"The final publication is available at Springer via  http://dx.doi.org/10.1007/978-3-319-23525-7_31Multidimensional data is systematically analysed at multiple granularities by applying aggregate and disaggregate operators (e.g., by the use of OLAP tools). For instance, in a supermarket we may want to predict sales of tomatoes for next week, but we may also be interested in predicting sales for all vegetables (higher up in the product hierarchy) for next Friday (lower down in the time dimension). While the domain and data are the same, the operating context is different. We explore several approaches for multidimensional data when predictions have to be made at different levels (or contexts) of aggregation. One method relies on the same resolution, another approach aggregates predictions bottom-up, a third approach disaggregates predictions top-down and a final technique corrects predictions using the relation between levels. We show how these strategies behave when the resolution context changes, using several machine learning techniques in four application domains.This work was supported by the Spanish MINECO under grants TIN 2010-21062-C02-02 and TIN 2013-45732-C4-1-P, and the REFRAME project, granted by the European Coordinated Research on Longterm Challenges in Information and Communication Sciences Technologies ERA-Net (CHIST-ERA), and funded by MINECO in Spain (PCIN-2013-037) and by Generalitat Valenciana PROMETEOII2015/013.Martínez Usó, A.; Hernández Orallo, J. (2015). Multidimensional Prediction Models When the Resolution Context Changes. En Machine Learning and Knowledge Discovery in Databases. Springer. 509-524. https://doi.org/10.1007/978-3-319-23525-7_31S509524Agrawal, R., Gupta, A., Sarawagi, S.: Modeling multidimensional databases. In: Proceedings of the Thirteenth International Conference on Data Engineering, ICDE 1997, pp. 232–243. IEEE Computer Society (1997)Bella, A., Ferri, C., Hernández-Orallo, J., Ramírez-Quintana, M.: Quantification via probability estimators. In: IEEE ICDM, pp. 737–742 (2010)Bella, A., Ferri, C., Hernández-Orallo, J., Ramírez-Quintana, M.J.: Aggregative quantification for regression. DMKD 28(2), 475–518 (2014)Bickel, R.: Multilevel analysis for applied research: It’s just regression! Guilford Press (2012)Cabibbo, L., Torlone, R.: A logical approach to multidimensional databases. In: Schek, H.-J., Saltor, F., Ramos, I., Alonso, G. (eds.) EDBT 1998. LNCS, vol. 1377, p. 183. Springer, Heidelberg (1998)Chaudhuri, S., Dayal, U.: An overview of data warehousing and OLAP technology. ACM Sigmod Record 26(1), 65–74 (1997)Chen, B.C.: Cube-Space Data Mining. ProQuest (2008)Chen, B.C., Chen, L., Lin, Y., Ramakrishnan, R.: Prediction cubes. In: Proc. of the 31st Intl. Conf. on Very Large Data Bases, pp. 982–993 (2005)Datahub: Car fuel consumptions and emissions 2000–2013 (2013). http://datahub.io/dataset/car-fuel-consumptions-and-emissionsDhurandhar, A.: Using coarse information for real valued prediction. Data Mining and Knowledge Discovery 27(2), 167–192 (2013)Forman, G.: Quantifying counts and costs via classification. Data Min. Knowl. Discov. 17(2), 164–206 (2008)Goldstein, H.: Multilevel Statistical Models, vol. 922. John Wiley & Sons (2011)Golfarelli, M., Maio, D., Rizzi, S.: The dimensional fact model: a conceptual model for data warehouses. Intl. J. of Coop. Information Systems 7, 215–247 (1998)Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: The WEKA data mining software: An update. SIGKDD Explor. 11(1), 10–18 (2009)Hernández-Orallo, J.: Probabilistic reframing for cost-sensitive regression. ACM Transactions on Knowledge Discovery from Data 8(3) (2014)IBM Corporation: Introduction to Aroma and SQL (2006). http://www.ibm.com/developerworks/data/tutorials/dm0607cao/dm0607cao.htmlKamber, M., Jenny, J.H., Chiang, Y., Han, J., Chiang, J.Y.: Metarule-guided mining of multi-dimensional association rules using data cubes. In: KDD, pp. 207–210 (1997)Lin, T., Yao, Y., Zadeh, L.: Data Mining, Rough Sets and Granular Computing. Studies in Fuzziness and Soft Computing. Physica-Verlag HD (2002)Páircéir, R., McClean, S., Scotney, B.: Discovery of multi-level rules and exceptions from a distributed database. In: Proc. of the 6th ACM SIGKDD Intl. Conf. on Knowledge discovery and data mining, pp. 523–532. ACM (2000)Pastor, O., Casamayor, J.C., Celma, M., Mota, L., Pastor, M.A., Levin, A.M.: Conceptual Modeling of Human Genome: Integration Challenges. In: Düsterhöft, A., Klettke, M., Schewe, K.-D. (eds.) Conceptual Modelling and Its Theoretical Foundations. LNCS, vol. 7260, pp. 231–250. Springer, Heidelberg (2012)Perlich, C., Provost, F.: Distribution-based aggregation for relational learning with identifier attributes. Machine Learning 62(1–2), 65–105 (2006)Team, R., et al.: R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria (2012)Ramakrishnan, R., Chen, B.C.: Exploratory mining in cube space. Data Mining and Knowledge Discovery 15(1), 29–54 (2007)Raudenbush, S.W., Bryk, A.S.: Hierarchical linear models: applications and data analysis methods, vol. 1. Sage (2002)UCI Repository: UJIIndoorLoc data set (2014). http://archive.ics.uci.edu/ml/datasets/UJIIndoorLocVassiliadis, P.: Modeling multidimensional databases, cubes and cube operations. In: Proc. of the 10th SSDBM Conference, pp. 53–62 (1998",Multidimensional Prediction Models When the Resolution Context Changes,https://riunet.upv.es/bitstream/10251/65588/4/ecml2015.pdf,'Springer Science and Business Media LLC',,10.1007/978-3-319-23525-7_31,core
78071095,2015-06-01T00:00:00,"We present a wearable system that uses ambient electromagnetic interference (EMI) as a signature to identify electronic devices and support proxemic interaction. We designed a low cost tool, called EMI Spy, and a software environment for rapid deployment and evaluation of ambient EMI-based interactive infrastructure. EMI Spy captures electromagnetic interference and delivers the signal to a user's mobile device or PC through either the device's wired audio input or wirelessly using Bluetooth. The wireless version can be worn on the wrist, communicating with the user;s mobile device in their pocket. Users are able to train the system in less than 1 second to uniquely identify displays in a 2-m radius around them, as well as to detect pointing at a distance and touching gestures on the displays in real-time. The combination of a low cost EMI logger and an open source machine learning tool kit allows developers to quickly prototype proxemic, touch-to-connect, and gestural interaction. We demonstrate the feasibility of mobile, EMI-based device and gesture recognition with preliminary user studies in 3 scenarios, achieving 96% classification accuracy at close range for 6 digital signage displays distributed throughout a building, and 90% accuracy in classifying pointing gestures at neighboring desktop LCD displays. We were able to distinguish 1- and 2-finger touching with perfect accuracy and show indications of a way to determine power consumption of a device via touch. Our system is particularly well-suited to temporary use in a public space, where the sensors could be distributed to support a popup interactive environment anywhere with electronic devices. By designing for low cost, mobile, flexible, and infrastructure-free deployment, we aim to enable a host of new proxemic interfaces to existing appliances and displays","EMI Spy: Harnessing electromagnetic interference for low-cost, rapid prototyping of proxemic interaction",https://core.ac.uk/download/78071095.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/BSN.2015.7299402,core
43661485,2015-02-02T13:13:38,"The next generation of intelligent robots will need to be able to plan reaches. Not just  ballistic point to point reaches, but reaches around things such as the edge of a table,  a nearby human, or any other known object in the robot’s workspace. Planning  reaches may seem easy to us humans, because we do it so intuitively, but it has  proven to be a challenging problem, which continues to limit the versatility of what  robots can do today. In this document, I propose a novel intrinsically motivated RL  system that draws on both Path/Motion Planning and Reactive Control. Through  Reinforcement Learning, it tightly integrates these two previously disparate  approaches to robotics. The RL system is evaluated on a task, which is as yet  unsolved by roboticists in practice. That is to put the palm of the iCub humanoid robot  on arbitrary target objects in its workspace, start- ing from arbitrary initial  configurations. Such motions can be generated by planning, or searching the  configuration space, but this typically results in some kind of trajectory, which must  then be tracked by a separate controller, and such an approach offers a brit- tle  runtime solution because it is inflexible. Purely reactive systems are robust to many  problems that render a planned trajectory infeasible, but lacking the capacity to search,  they tend to get stuck behind constraints, and therefore do not replace motion  planners. The planner/controller proposed here is novel in that it deliberately plans  reaches without the need to track trajectories. Instead, reaches are composed of  sequences of reactive motion primitives, implemented by my Modular Behavioral  Environment (MoBeE), which provides (fictitious) force control with reactive collision  avoidance by way of a realtime kinematic/geometric model of the robot and its  workspace. Thus, to the best of my knowledge, mine is the first reach planning  approach to simultaneously offer the best of both the Path/Motion Planning and  Reactive Control approaches. By controlling the real, physical robot directly, and  feeling the influence of the con- straints imposed by MoBeE, the proposed system  learns a stochastic model of the iCub’s configuration space. Then, the model is  exploited as a multiple query path planner to find sensible pre-reach poses, from which  to initiate reaching actions. Experiments show that the system can autonomously find  practical reaches to target objects in workspace and offers excellent robustness to  changes in the workspace configuration as well as noise in the robot’s sensory-motor  apparatus",Learning to reach and reaching to learn: a unified approach to path planning and reactive control through reinforcement learning,https://core.ac.uk/download/43661485.pdf,,,,core
25042342,2014-07-13T00:00:00,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.Comment: 8 pages, IEEE conference format, to appear in the refereed
  proceedings of the IEEE e-Science 2014 conf., eds. C. Medeiros et al., IEEE,
  in press (2014). arXiv admin note: substantial text overlap with
  arXiv:1209.1681, arXiv:1110.465","Automated Real-Time Classification and Decision Making in Massive Data
  Streams from Synoptic Sky Surveys",http://arxiv.org/abs/1407.3502,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/eScience.2014.7,core
25048869,2014-08-07T00:00:00,"We describe a new paradigm for implementing inference in belief networks,
which relies on compiling a belief network into an arithmetic expression called
a Query DAG (Q-DAG). Each non-leaf node of a Q-DAG represents a numeric
operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG
represents the answer to a network query, that is, the probability of some
event of interest. It appears that Q-DAGs can be generated using any of the
algorithms for exact inference in belief networks --- we show how they can be
generated using clustering and conditioning algorithms. The time and space
complexity of a Q-DAG generation algorithm is no worse than the time complexity
of the inference algorithm on which it is based; that of a Q-DAG on-line
evaluation algorithm is linear in the size of the Q-DAG, and such inference
amounts to a standard evaluation of the arithmetic expression it represents.
The main value of Q-DAGs is in reducing the software and hardware resources
required to utilize belief networks in on-line, real-world applications. The
proposed framework also facilitates the development of on-line inference on
different software and hardware platforms, given the simplicity of the Q-DAG
evaluation algorithm. This paper describes this new paradigm for probabilistic
inference, explaining how it works, its uses, and outlines some of the research
directions that it leads to.Comment: Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996","Query DAGs: A Practical Paradigm for Implementing Belief Network
  Inference",http://arxiv.org/abs/1408.1480,,,,core
44623782,2014-01-01T00:00:00,"Artículo de publicación ISIThe capability to generate complex geometry features
at tight tolerances and fine surface roughness is a key
element in implementation of Creep Feed grinding process in
specialist applications such as the aerospace manufacturing
environment. Based on the analysis of 3D cutting forces,
this paper proposes a novel method of predicting the profile
deviations of tight geometrical features generated using
Creep Feed grinding. In this application, there are several
grinding passes made at varying depths providing an incremental
geometrical change with the last cut generating the
final complex feature. With repeatable results from coordinate
measurements, both the radial and tangential forces can
be gauged versus the accuracy of the ground features. The
results of the tangential force were found more sensitive to
the deviation of actual cut depth from the theoretical one.
However, to make a more robust prediction on the profile
deviation, its values were considered as a function of both
force components. In addition, the power signals were obtained
as these signals are also proportional to force and
deviation measurements. Genetic programming (GP), an
evolutionary programming technique, has been used to
compute the prediction rules of part profile deviations based
on the extracted radial and tangential force and correlated
with the initial “gauging” methodology. It was found that
using this technique, complex rules can be achieved and
used online to dynamically control the geometrical accuracy
of the ground features. The GP complex rules are based on
the correlation between the measured forces and recorded
deviation of the theoretical profile. The mathematical rules
are generated from Darwinian evolutionary strategy which
provides the mapping between different output classes. GP works from crossover recombination of different rules, and
the best individual is evaluated in terms of the given best
fitness value so far which closes on an optimal solution.
Once the best rule has been generated, this can be further
used independently or in combination with other close-tobest
rules to control the evolution of output measures of
machining processes. The best GP terminal sets will be
realised in rule-based embedded coded systems which will
finally be implemented into a real-time Simulink simulation.
This realisation gives a view of how such a control
regime can be utilised within an industrial capacity. Neural
networks were also used for GP rule verification.The experimental work was carried out at The University of
Nottingham funded by EPSRC",The prediction of profile deviations when Creep Feed grinding complex geometrical features by use of neural networks and genetic programming with real-time simulation,,'Springer Science and Business Media LLC',,10.1007/s00170-014-5829-0,core
236803389,2014,"Deployment of smart grid technologies is accelerating. Smart grid enables bidirectional flows of energy and energy-related communications. The future electricity grid will look very different from today's power system. Large variable renewable energy sources will provide a greater portion of electricity, small DERs and energy storage systems will become more common, and utilities will operate many different kinds of energy efficiency. All of these changes will add complexity to the grid and require operators to be able to respond to fast dynamic changes to maintain system stability and security. This thesis investigates advanced control technology for grid integration of renewable energy sources and STATCOM systems by verifying them on real time hardware experiments using two different systems: d SPACE and OPAL RT. Three controls: conventional, direct vector control and the intelligent Neural network control were first simulated using Matlab to check the stability and safety of the system and were then implemented on real time hardware using the d SPACE and OPAL RT systems. The thesis then shows how dynamic-programming (DP) methods employed to train the neural networks are better than any other controllers where, an optimal control strategy is developed to ensure effective power delivery and to improve system stability. Through real time hardware implementation it is proved that the neural vector control approach produces the fastest response time, low overshoot, and, the best performance compared to the conventional standard vector control method and DCC vector control technique. Finally the entrepreneurial approach taken to drive the technologies from the lab to market via ORANGE ELECTRIC is discussed in brief. (Published By University of Alabama Libraries",Real time hardware implementation of power converters for grid integration of distributed generation and statcom systems,,,,,core
296628788,2015-11-26T14:38:11Z,"This study offers help for drawing up a community health orthodontic assistance protocol, especially for the public health system. A preventive and interceptive orthodontic program is suggested through the inclusion of approaches in existing healthcare programs that may help prevent malocclusions, extending the services offered by Basic Healthcare Units to include fitting space retainer appliances and interceptive procedures. Possible occlusion problems are discussed and rated as high or low priority for treatment, stressing occlusal deviations that should not be treated precociously. Preventive and interceptive orthodontic treatment should primarily be offered to children with deciduous and mixed dentition. The management and inclusion of orthodontic procedures can be deployed through the management of physical, human and financial resources, establishing access criteria for these services. The use of an appropriate orthodontic assistance protocol and correct planning of orthodontic actions could pave the way for preventive and interceptive procedures at the Basic Healthcare Units, while more resources could be allocated to setting up Specialty Reference Centers offering more complex corrective orthodontic procedures.12410671078Oliveira CM. Maloclusão no contexto da saúde pública. In: Böneker M, Sheiham A, organizadores. Promovendo saúde bucal na infância e adolescência: conhecimentos e práticas. São Paulo: Livraria e Editora Santos2004. p.55-80Ministério da Saúde, (2004) Diretrizes da política nacional de saúde bucal, , Brasília: Ministério da Saúde;Lang, R., Orthodontic treatment timing. What? When? Who? [editorial] (1989) Oral Health, 79 (9), pp. 7-9Joondeph, D.R., Early orthodontic treatment (1993) Am J Orthod Dentofacial Orthop, 104 (2), pp. 199-200Arvystas, M.G., The rationale for early orthodontic treatment [editorial] (1998) Am J Orthod Dentofacial Orthop, 113 (1), pp. 15-18Dugoni, S.A., Comprehensive mixed dentition treatment (1998) Am J Orthod Dentofacial Orthop, 113 (1), pp. 75-84Sadowsky, P.L., Craniofacial growth and the timing of treatment (1998) Am J Orthod Dentofacial Orthop, 113 (1), pp. 19-23Silva Filho, O.G., Okada, H., Aiello, C.A., Ortodontia interceptiva: Correção precoce de irregularidades na região ântero-superior (1998) Ortodontia, 31 (2), pp. 113-121Tung, A.W., Kiyak, H.A., Psychological influences on the timing of orthodontic treatment (1998) Am J Orthod Dentofacial Orthop, 113 (1), pp. 29-39White, L., Early orthodontic intervention (1998) Am J Orthod Dentofacial Orthop, 113 (1), pp. 24-28Yang, E.Y., Kiyak, A., Orthodontic treatment timing: A survey of orthodontists (1998) Am J Orthod Dentofacial Orthop, 113 (1), pp. 96-103Araújo, M.G.M., Ortodontia para clínicos: Programa pré-ortodôntico (1988) São Paulo: Livraria e Editora, pp. 21-26. , 4a ed, Santos;McDonald RE, Avery DR. Diagnóstico e correção de pequenas irregularidades na dentição em desenvolvimento. In: McDonald RE, Avery DR, organizadores. Odontopediatria. 4a ed. Rio de Janeiro: Guanabara Koogan1983. p.493-96Varela, C.B., A arte de amamentar seu filho (1984) Petrópolis: Editora Vozes/Cidade Nova Editora, , 3a edMartins Filho, J., Como e porque amamentar (1987) São Paulo: Savier Editora, , 2a edVarrela, J., Occurrence of malocclusion in attritive environment: A study of a skull sample from southwest Finland (1990) Scand J Dent Res, 98 (3), pp. 242-247Almeida, R.R., Santos, S.C.B.N., Santos, E.C.A., Insabralde, C.M.B., Almeida, M.R., Mordida aberta anterior - considerações e apresentação de um caso clínico (1998) Rev Dent Press Ortodon Ortop Facial, 3 (2), pp. 17-29Moura, W.V.B., Maia, F.A., Maia, N.G., Avaliação do Modus Operandi dos procedimentos ortodônticos interceptores das más-oclusões na dentadura mista (1998) Rev Dent Press Ortodon Ortop Facial, 3 (6), pp. 53-60Bortolotti, R., Ribeiro, A.T.B., Barros, M.G.L., Spinassé, K.G., Mantenedores de espaço em ortodontia preventiva e interceptiva (1999) Rev Dent Press Ortodon Ortop Facial, 4 (5), pp. 25-33Bastos JRM, Peres SHCS, Ramires I. Educação para a saúde. In: AC Pereira, organizador. Odontologia em saúde coletiva: planejando ações e promovendo saúde. 1 a ed. Porto Alegre: Artmed2003. p. 117-39Almeida, R.R., Garib, D.G., Henriques, J.F.C., Almeida, M.R., Almeida, R.R., Ortodontia preventiva e interceptora: Mito ou realidade? (1999) Rev Dent Press Ortodon Ortop Facial, 4 (6), pp. 87-108Maia, F.A., Cefalometria para o clínico geral e o odontopediatra (1988) São Paulo: Ed, , Santos;Moyers RE, Riolo ML. Tratamento precoce. In: Moyers RE, organizador. Ortodontia. 4a ed. Rio de Janeiro: Guanabara Koogan1991. p. 292-368Ministério da Saúde, (1997) Norma Operacional Básica do Sistema Único de Saúde/NOB-SUS, 96. , Brasília: Ministério da Saúde;Maciel, S.M., (2003) A promoção da eqüidade na triagem de usuários da clínica ortodôntica da UFJF: Elementos para uma política pública e renovadora de saúde bucal, , dissertação, Rio de Janeiro RJ, Instituto de Medicina Social;Cunha, A.C.P.P., (2002) Avaliação da capacidade dos índices DAÍ e IOTN em estabelecer a necessidade de tratamento ortodôntico, , dissertação, Natal RN, Universidade Federal do Rio Grande do Norte, Centro de Ciências da Saúde de Natal;McLain, J.B., Profitt, W.R., Oral health status in the United States: Prevalence in malocclusion (1985) J Dent Educ, 49 (6), pp. 386-396Maia, F.A., Galvão, N.M., Ortodontia preventiva: Programa de odontologia preventiva do C.A.I (1984) O Dentista, p. 2Al Nimri, K., Richardson, A., Interceptive orthodontics in the real world of community dentistry (2000) Int J Paediatr Dent, 10 (2), pp. 99-108Ministério da Saúde, (2004) Relatório Final: 3a Conferência Nacional de Saúde Bucal. Saúde bucal: acesso e qualidade, superando a exclusão social, , Brasília: Ministério da Saúde;Moyers RE. Planejamento do tratamento ortodôntico. In: Moyers RE, organizador. Ortodontia. 4a ed. Rio de Janeiro: Guanabara Koogan1991. p. 281-291Levantamento Epidemiológico Básico de Saúde Bucal: Manual de Instruções (1991) São Paulo: Editora, , Organização Mundial da Saúde, 3a ed, Santos",Remarks On Drawing Up A Community Health Orthodontic Assistance Protocol [considerações Para Elaboração De Protocolo De Assistência Ortodôntica Em Saúde Coletiva],,,,,core
102390751,2015-08-23,"Abstract—This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion. I",,,,,,core
103081551,2015-11-01,"Abstract—More and more the notion of agent appears in differ-ent contexts of computer science, often with different meanings. Main ones are Artificial Intelligence (AI) and Distributed AI, where agents are exploited as a technique to develop systems exhibiting some kind of intelligent behavior. In this paper, we introduce a further perspective, shifting the focus from AI to computer programming and programming languages. In particular, we consider agents and related concepts as general-purpose abstractions useful for programming software systems in general, conceptually extending object-oriented programming with features that – we argue – are effective to tackle some main challenges of modern software development. The main contribution of the work is the definition of a conceptual space framing the basic features that characterize the agent-oriented approach as a programming paradigm, and its validation in practice by using a platform called JaCa, with real-world programming examples. Keywords-agent-oriented programming; multi-agent systems; concurrent programming; distributed programming I",A Programming Paradigm based on Agent-Oriented Abstractions,,,,,core
103691000,2014,"Abstract—Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner’s learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly pro-duced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented. Index Terms—Computer-aided pronunciation training (CAPT), computer-assisted language learning, dialogue game, Markov de-cision process, reinforcement learning. I",A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training,,,,10.1109/taslp.2014.2375572,core
102283206,2015-08-22,"Information Technologies being developed by NASA to assist astronaut-physician in responding to medical emergencies during long space flights are being employed for the improvement of women&apos;s health in the form of &quot;smart surgical probe&quot;. This technology, initially developed for neurosurgery applications, not only has enormous potential for the diagnosis and treatment of breast cancer, but broad applicability to a wide range of medical challenges. For the breast cancer application, the smart surgical probe is being designed to &quot;see &quot; a suspicious lump, determine by its features if it is cancerous, and ultimately predict how the disease may progress. A revolutionary early breast cancer detection tool based on this technology has been developed by a commercial company and is being tested in human clinical trials at the University of California at Davis, School of Medicine. The smart surgical probe technology makes use of adaptive intelligent software (hybrid neural networks/fuzzy logic algorithms) with the most advanced physiologic sensors to provide real-time in vivo tissue characterization for the detection, diagnosis and treatment of tumors, including determination of tumor microenvironment and evaluation of tumor margins. The software solutions and tools from these medical applications will lead to the development of better real-time minimally-invasive smart surgical probes for emergency medical care and treatment of astronauts on long space flights. KEY WORDS Multi-modality microsensors, intelligent medical conditions and treatments database, intelligent virtual interface, smart surgical probe 1",•NASA Smart Surgical Probe Project,,,,,core
100322136,2014-12-12,"1Abstract—The need for intelligent unmanned vehicles has been steadily increasing. These vehicles could be air-, ground-, space-, or sea-based. This paper will review some of the most common software systems and methods that could be used for controlling such vehicles. Early attempts at mobile robots were confined to simple laboratory environments. For vehicles to operate in real-world noisy and uncertain environments, they need to include numerous sensors and they need to include both reactive and deliberative features. The most effective software systems have been hierarchical or multi-layered. Many of these systems mimic biological systems. This paper reviews several software approaches for autonomous vehicles. While there are similarities, there are differences as well. Most of these software systems are very difficult to use, and few of them have the ability to learn. Autonomous vehicles promise remarkable capabilities for both civilian and military applications, but much work remains to develop intelligent systems software which can be used for a wide range of applications. In particular there is a need for reliable open-source software that can be used on inexpensive autonomous vehicles. Index Terms—Mobile robots, autonomous vehicles, intelligent agents, software, and artificial intelligence",,,,,,core
296792594,2015-06-08T00:00:00,"Under a contract with AIRBUS MILITARY (AI-M), an exercise to analyze the potential of optimization techniques to improve the wing performances at cruise conditions has been carried out by using an in-house design code. The original wing was provided by AI-M and several constraints were posed for the redesign. To maximize the aerodynamic efficiency at cruise, optimizations were performed using the design techniques developed internally at INTA under a research program (Programa de Termofluidodinámica). The code is a gradient-based optimizaa tion code, which uses classical finite differences approach for gradient computations. Several techniques for search direction computation are implemented for unconstrained and constrained problems. Techniques for geometry modifications are based on different approaches which include perturbation functions for the thickness and/or mean line distributions and others by Bézier curves fitting of certain degree. It is very e important to afford a real design which involves several constraints that reduce significantly the feasible design space. And the assessment of the code is needed in order to check the capabilities and the possible drawbacks. Lessons learnt will help in the development of future enhancements. In addition, the validation of the results was done using also the well-known TAU flow solver and a far-field drag method in order to determine accurately the improvement in terms of drag counts",Improvement of the cruise performances of a wing by means of aerodynamic optimization. Validation with a Far-Field method,,'EDP Sciences',,10.1051/eucass/201507067,core
216509883,2012-01-01T00:00:00,"The robust popularization of 3D videos noticed along the last decade, allied to the omnipresence of smart mobile devices handling multimedia-capable features, has led to intense development and research focusing on efficient 3D-video encoding techniques, display technologies, and 3D-video capable mobile devices. In this scenario, the Multiview Video Coding (MVC) standard is key enabler of the current 3D-video systems by leading to meaningful data reduction through advanced encoding techniques. However, real-time MVC encoding for high definition videos demands high processing performance and, consequently, high energy consumption. These requirements are attended neither by the performance budget nor by the energy envelope available in the state-of-the-art mobile devices. As a result, the realization of MVC targeting mobile systems has been posing serious challenges to industry and academia. The main goal of this thesis is to propose and demonstrate energy-efficient MVC solutions to enable high-definition 3D-video encoding on mobile battery-powered embedded systems. To expedite high performance under severe energy constraints, this thesis proposes jointly considering energy-efficient optimizations at algorithmic and architectural levels. On the one hand, extensive application knowledge and data analysis was employed to reduce and control the MVC complexity and energy consumption at algorithmic level. On the other hand, hardware architectures specifically designed targeting the proposed algorithms were implemented applying low-power design techniques, dynamic voltage scaling, and application-aware dynamic power management. The algorithmic contribution lies in the MVC energy reduction by shorten the computational complexity of the energy-hungriest encoder blocks, the Mode Decision and the Motion and Disparity Estimation.  The proposed energy-efficient algorithms take advantage of the video properties along with the strong correlation available within the 3D-Neighborhood (spatial, temporal and disparity) space in order to efficiently reduce energy consumption. Our Multi-Level Fast Mode Decision defines two complexity reduction operation modes able to provide, on average, 63% and 71% of complexity reduction, respectively. Additionally, the proposed Fast ME/DE algorithm reduces the complexity in about 83%, for the average case. Considering the run-time variations posed by changing coding parameters and video content, an Energy-Aware Complexity Adaptation algorithm is proposed to handle the energy versus coding efficiency tradeoff while providing graceful quality degradation under severe battery draining scenarios by employing asymmetric video coding. Finally, to cope with eventual video quality losses posed by the energy-efficient algorithms, we define a video quality management technique based on our Hierarchical Rate Control. The Hierarchical Rate Control implements a frame-level rate control based on a Model Predictive Controller able to increase in 0.8dB (Bjøntegaard) the overall video quality. The video quality is increased in 1.9dB (Bjøntegaard) with the integration of the basic unit-level rate control designed using Markov Decision Process and Reinforcement Learning. Even though the energy-efficient algorithms drive to meaningful energy reduction, hardware acceleration is mandatory to reach the energy-efficiency demanded by the MVC. Aware of this requirement, this thesis brings architectural solutions for the Motion and Disparity Estimation unit focusing on energy reduction while attending real-time throughput requirements. To achieve the desired results, as shown along this volume, there is a need to reduce the energy related to the ME/DE computation and related to the intense memory communication.  Therefore, the ME/DE architectures incorporate the Fast ME/DE algorithm in order to reduce the computational complexity while the memory hierarchy was carefully designed to find the optimal energy tradeoff between external memory accesses and on-chip video memory size. Statistical analysis where used to define the size and organization of the on-chip cache memory while avoiding increased memory misses and the consequent data retransmission. A prefetching technique based on search window prediction also supports the reduction of external memory access. Moreover, a memory power gating technique based on dynamic search window formation and an application aware power management were proposed to reduce the static energy consumption related to on-chip video memory. To implement these techniques a SRAM memory featuring multiple power states was used. The architectural contribution contained in this thesis extends the state-of-the-art by achieving real-time ME/DE processing for 4-views HD1080p running at 300MHz and consuming 57mW",Energy-efficient algorithms and architectures for multiview video coding,,,,,core
21417463,2011-04-29,"We apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the highdimensionality of the state space (rendering conventional discretization-based function approximation like tilecoding infeasible), the stochasticity due to noise and multiple learning agents needing to cooperate (meaning that the exact dynamics of the environment are unknown) and real-time learning (meaning that an efficient online implementation is required). We employ the general framework of approximate policy iteration with least-squares-based policy evaluation. As underlying function approximator we consider the family of regularization networks with subset of regressors approximation. The core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions. Simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained with tilecoding by Stone et al. (2005)",Gaussian Processes in Practice Learning RoboCup-Keepaway with Kernels,,,,,core
33219198,,"[[abstract]]本計畫原訂期程為三年，第一年已獲得執行，執行半年至今已經獲得豐碩之成果並發表於國際期刊獲得接受，本次申請依據評審意見修正(謹附於表C012-2 計畫內容中說明)，並將過去半年已完成成果以及已經為未來兩年所作之準備進行詳盡說明，以期研究成果能獲得延續執行。 IEEE 802.16 WiMax 無線網路的發展非常迅速，許多國家列為重要基礎建設之一(台灣也不例外)，就連標準的制定與更新也非常快，新的標準不斷出現，包含802.16e-2005 定義了Mobile WiMax，草擬中的802.16j (draft) 和802.16m (pre-draft)更嘗試在原來的高 throughput 下提昇relay 以及高mobility 的能力，在這些完稿或未確立的標準中，都是在媒體存取層(Media Access Control; MAC)以及實體層(Physical layer; PHY)中進行定義。其中並有對於服務品質(Quality of Service; QoS)的規範，足見QoS 不只是網路層以上的協定必需研究的問題，特別針對WiMax，在MAC 層實作QoS 的重要性更受到許多研究學者之重視。本計劃承接過計畫經驗，這些經驗包含使用跨協定控制方式完成之IEEE 802.11e EDCA QoS 之增進、IEEE 802.11e HCCA QoS 之增進、以及去年成功地完成IEEE 802.16e/j QoS 之增進，欲進行下列的研究，以將研究深入到IEEE 802.16e/j/m 以至於廣義Scalable OFDMA-based 的無線網路的QoS 控制： 1. 階層式跨協定模糊控制(Hierarchical Cross-Layer Control; HCLC) 使用於 802.16e/j/m Mobile 以及Relaying 之QoS 2. 定義於Lattice 上高階模糊宇集(高type 以及高Level 之fuzzy universe)之普適化模糊自動機(Generalized Fuzzy Automata; GFA) 來模仿(mathematically realize)WiMax 無線網路的行為，了解QoS 參數的特性。 3. 根據GFA 自動機理論設計HCLC 控制器，使用高階模糊宇集進行多參數以及多目標控制，以在相容於標準之條件下調整不同QoS 參數。 4. HCLC 之軟硬體共同設計與實作以及MAC 排程器矽智財之電子系統層級 (Electronic System Level; ESL)驗證，開發現今儀器無法量測以及測試的WiMax 驗證平台。 5. 通用於OFDM/Scalable OFDMA-based 之跨協定控制，以利未來延伸到其他之無線網路標準，例如IEEE 802.22 Cognitive Radio，IEEE 802.11n MIMO 之QoS 排程器等。本計畫從過去的研究成果來延伸，在跨協定QoS 控制中，在標準已經定義的Power Sleep Behaviors (Listen/Active)以及Ranging 以外，另加入路由控制、PDU 長度、排程器計算等造成能源消耗的考量，這是IEEE 802.16 標準中缺乏定義的部份。不同於使用最佳化理論之硬式計算(hard computing)演算法，我們說明使用軟式計算 (soft computing)的原因。除了IEEE Computational Intelligence Society (CIS)在CIS 雜誌中所揭出的Emergent Applications 包含了網路控制之外，我們定義了WiMax 問題，說明無線網路無法根據固定的機率模型假設以及固定的環境因素來進行推導，因此無法獲得以 constraints 與 controls 來表示 的objective function 的close-form。無線網路的與變數狀態非常多且動態，無法根據專家知識逐一定義模糊規則或進行最佳化之推導，本計畫使用簡單控制規則，在不同通訊協定層進行控制，每一協定層的模糊規則展開之後，規則數可以隨著所跨協定層數獲得exponential 成長，我們以Homomorphism 來說明並規範上下層之間狀態的關係。針對這樣的網路控制架構，我們稱之為階層式跨協定控制 (Hierarchical Cross-Layer Control; HCLC)。本計畫分析WiMax 在動態無線網路環境下的延遲與QoS 參數的關係，以廣義模糊自動機當作其行為模型，並整合能量消耗的模型。自動機之行為實現(output function)利用過去國科會計畫成果—單一維度HCLC控制架構來延伸。因為WiMax 之OFDMA 與802.16m 的 MIMO 中之特性，跨協定所必須完成的是多目標與多參數之控制，方法是延伸到高階宇集(universe)的HCLC。我們並發現這樣的一個架構是一個比傳統Hierarchical Tagaki-Sugeno Control 還要廣義的Paradigm，本計劃除針對這樣的架構進行穩定性以及效率的理論分析之外，並設計實驗證明這樣的架構即時有效地進行IEEE 802.16e/j 的QoS 控制，本計畫在第一年已經證明HCLC 應用於802.16e/j 的兩種重要的公平性：inter-class fairness 與intra-class fairness，這是大多文獻中未考慮到的部份。在以嵌入式系統以及矽智財設計、實作、驗證WiMax 跨協定排程器方面，我們考慮SoC 設計流程來說明，以往任何通訊網路之矽智財驗證平台無法驗證出在不明確網路下之可行性(特別是無線網路)，必須在花費大筆經費和時間整合到SoC 並且下線(tape out) 之後，再花許多時間進行embedded software 之porting，然後才發現所設計之SIP 是否有瑕疵(bug)，若發現瑕疵，現有之積體電路設計或模擬驗證軟體並無法針對通訊網路的部份進行除錯，即使是昂貴的SoC 驗證工具(例如具有AMBA SoC bus 輸出入介面之 ARM based 平台)也無法立即進行驗證晶片在通訊網路上之行為，僅能提供訊號波形、指令週期等層級之觀察與控制。而對於通訊網路，還必須花費成本複製多台才能夠架構出基本網路拓樸(topology)，也僅能進行基本實驗。即便最後成功，市場之優勢早已不再，對於許多design houses 來說，這是一個難關。本計畫所產出之Electronic System Level (ESL)驗證平台，可以大幅縮短Design House 在嵌入式系統設計驗證之時程，並大幅縮小開發成本。由於我們已經完成WLAN MANET 之ESL 架構，此架構基於HCLC 控制模型，成功驗證了FPGA 以及HCLC 跨協定演算法於MANET 下的行為，因此這樣的一個HCLC ESL Verification 平台， 最適用於Architecture/Algorithm 以及 Hardware/Software Co-design，對於無線網路矽智財以及嵌入式系統初期之開發最具幫助。綜合以上目標以及原因之說明，本計畫擬定接下來兩年之研究方法、步驟以及個別目標，本著研究應該持續且深入的觀念，沿著過去研究成果的脈絡，來規劃未來兩年的研究。第一年已經完成原訂第一年之目標： 高階宇集HCLC 控制模型以及 OFDMA-based 802.16e-2005 Mobile WiMax 以及802.16j (MMR)的跨協定排程器軟體元件；第二年將擴充到OFDMA-based 的802.16j/m，整合PHY 與MIMO 能量消耗模型(特別是mobile node 上行部份)，完成具Relaying 功能且支援MIMO 的IEEE 802.16 e/j/m Mobile WiMax 跨協定控制器；第三年依據WLAN 之ESL 經驗以及第一年與第二年之研究成果，實作OFDMA-based WiMax 排程器矽智財，並進行軟硬體系統整合，以及完成ESL 驗證平台。總結本計畫的創意以及特點即是：1. 所完成之系統以及QoS 矽智財能夠滿足QoS 之外，在透過跨協定的架構下還能同時inter-與intra-class 的公平性；2. 達到節能的目標；3. 完成支援Cross-Layer Design 的WiMAX 通訊網路晶片矽智財；4. 結合網路模擬器與Hierarchical Cross-Layer 控制模型完成低成本之ESL 平台。以未來之擴充性來說，ESL 驗證平台之應用，不僅用於矽智財之設計，也不僅用於MAC 層之協定驗證，對於跨協定設計中任何演算法之驗證、嵌入式系統之演算法驗證、…、等等，均能發揮其功能。而模糊自動機可以應用之範圍甚廣，例如機器學習，以智慧型的方式學習網路的狀態空間，朝實體層之跨協定調變控制以產生最佳頻寬利用等。對於本計畫利用人工智慧與軟式計算理論為工具，應用到通訊網路，並實現在SoC 領域，本著過去計劃之成功經驗，向系統整合方向邁進，成果將非常豐碩。[[abstract]]This project was planned in three-year execution. The first year was granted and till now very good results has been obtained and published in an international journal (SCIE indexed). In this proposal, we revise according to the reviewers’ comments (cf. Table C012-2). We also include the results descriptions of the first year execution and present how/what we have prepared for the following two years wishing that the research can be continued. Since IEEE 802.16 WiMax wireless networks are developed, it becomes the future infrastructure of many developed and developing countries. New related standards and their amendments are fervidly discussed. The IEEE 802.16e-2005 has defined Mobile WiMax and the latest drafting 802.16j and the “pre-drafting” 802.16m even try to respectively promote multi-hop relay and mobility beyond the original throughput. In these finalized, drafting, and pre-drafting standards, most are about Media Access Control (MAC) and PHYsical layers (PHY). What insights are with Quality of Service (QoS) and we see that QoS is a problem relating not only upper layers but also MAC and PHY. Especially for WiMax, QoS realization in MAC and PHY are emphasized by many researches. Inheriting the experiences of previous projects since IEEE 802.11e EDCA, HCCA, and currently the general TDMA-based QoS controls, we propose the following researches to strike into IEEE 802.16e/j Wave 2 and so as into general Scalable OFDMA (SOFDMA) based QoS control: 1. Define Hierarchical Cross-Layer Control (HCLC) for 802.16e/j/m mobile and multi-hop relay QoS 2. Study Generalized Fuzzy Automata (GFA) defined with high-type and high-level fuzzy universes over lattices to mathematically realize WiMax behaviors and characteristics 3. Design HCLC scheduler and allocator (together called controller) according to the GFA theory where high-type fuzzy universes are used for multi-parameter and multi-objective control. In this way, we can adjust QoS parameters while the wireless communications are still compatible with the standards.4. Perform HCLC software-hardware co-design, implement the MAC QoS controller Silicon Intellectual Property (SIP), and its Electronic System Level (ESL) verifications. Develop WiMax verification platform with cross-layer features that modern instruments cannot measure. 5. Research generic cross-layer control for the OFDM/Scalable OFDMA-based wireless such that we can extend the WiMax QoS control research into future standards such as IEEE 802.22 Cognitive Radio and IEEE 802.11n MIMO. This project extends the valuable results of previous executed/executing projects. Beside cross-layer, and beside defined power sleep behaviors (Listen/Active) and ranging, we also consider energy consumption about routing control, PDU length, and scheduling computation. These are what the IEEE 802.16 does not define. Unlike those hard computation algorithms using optimization theory, we explain the feasibility using soft computing. Beside the reason explained in the Computational Intelligence Magazine published by IEEE Computational Intelligence Society (CIS) that one of the emergent applications of soft computing is network control, we additionally and in detail defined problems of WiMax QoS control. In this proposal we explain why wireless networks cannot be modeled by fixed probability model and why their performances cannot be derived using fixed environmental factors. Consequently, optimization using hard computing algorithms can not express objective functions in terms of constraints and control parameters. However, using soft computing we still suffer difficulties. The variables and states of wireless networks forms a large and dynamic space such at that no expertise can define detail fuzzy rules. Therefore, in this project, we try to use simple fuzzy rules distributed in different protocol layers. Expanding the rules in the protocol layers, we found that the number of fuzzy rules exponentially grows along the number of layers. We adopt homomorphism concept to explain and constrain inter-layer states and variables. We called this network control architecture the Hierarchical Cross-Layer Control (HCLC). For WiMax, this project analyses the relationship between delay and QoS parameters when network is highly dynamic. Then, we use generalized fuzzy automata to realize the behavior of the WiMax network and integrate energy consumption model. The realization of automata (output function) inherits previous projects’ results – the one dimensional HCLC control to extend new SOFDMA-based control with multiple objectives and multiple parameters. The method used is adopting high-type and high-level fuzzy universes in the HCLC. We even find out that such HCLC architecture is a more generic paradigm than defined in so-called Hierarchical Tagaki-Sugeno fuzzy control. This project will analyze the stability and efficiency of this fuzzy control architecture. In the first half year, we design the experiment to prove that HCLC efficiently and effectively perform IEEE 802.16e/j QoS control while at the same time keep the fairness that few articles concern. Considering the embedded system design, silicon intellectual property implementation, and verification of the WiMax QoS controller, this project provides a low cost solution. According to known SoC design flow, no SIP verification platform can perform verification of SIP behavior under realistic wireless network environment before accomplishment of SoC integration, tapering, and embedded software porting. It’s a long journey to accomplish all these tasks. If a bug is found in the SIP, no EDA tool can remove it dedicatedly for network communication. Even using costly SoC verification tool such as ARM-based platform with AMBA SoC bus on the PCB, we still cannot observe the SIP behavior under wireless networks. Using costly SoC verification tool, what only we can do is signal wave measurement, observation and control at the instruction and cycle accuracy. For basic networking experiments, we still even have to replicate platforms to construct the basic network topology. After we success eventually, the market opportunity is gone. This is a tuff work for design houses. The verification platform of this project will be superior over traditional ones and obviously shorten the development of networking SIPs. Thus reduce the cost effectively. In previous project, we had accomplished WLAN MANET ESL verification where HCLC control model successfully cooperates with FPGA hardware. This is the ever first verification platform that verify behaviours of real hardware and cross-layer algorithm under MANET. The platform will be extended for WiMAX MMR since it optimally match hardware architecture and cross-layer algorithms and is the best tool for hardware/software co-design. According to the above descriptions, we project three-year research methods, steps, and objectives. Based on the concept that research should be always continuing and burrowing to the deep, we follow the direction of previous research projects. In the first half year, we already accomplished high-type HCLC control model and its cross-layer scheduler software component for OFDMA-based 802.16e/j Mobile Multi-hop Relay (MMR) WiMax. In the second year, we will extend the HCLC controller to 802.16j/m and integrate energy consumption model to accomplish embedded cross-layer scheduler software component for multi-hop relaying and mobile WiMax. In the third year, we will implement the OFDMA-based WiMax scheduler and its electronic system level (ESL) verification platform. Summarizing the originality and features, we have: first, the system and QoS SIP not only guarantee QoS but also intra- and inter-class fairness; sencond, the QoS and fairness guarantees are energy-aware; third, we accomplish HCLC SIP for general WiMax; fourth, we produce low cost ESL verification platform integrating the network simulator and hierarchical cross-layer control. For future extension, fuzzy automata theory applies in many areas such as machine learning, intelligent network state space learning, and cross-layer physical layer modulation for spectrum optimization. The resulting SIP verification platform is also useful in many cases in addition to SIP and MAC layer control. For example, it should be also useful in any cross-layer algorithms, embedded system algorithms, …, etc. This project covers several research fields including artificial intelligence, computing theory, communication networks, and VLSI. With the basis of previous successful projects, we progress toward system integration and the results will also be very plentiful and valuable.[[note]]NSC98-2221-E327-02",Hierarchical Cross-Layer Soft Computation Model for WiMAX Networks and Its Electronic Sytem Level Verification(II),,行政院國家科學委員會,,,core
214601082,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake-holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished[1] Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons (Chichester, England) (2002). ISBN 0 47149691X

[2] Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

[3] Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 10–15. ACM Press (2006)

[4] Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-Architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

[5] Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

[6] Zhang, Z., Thangarajah, J., Padgham, L.: Automated unit testing for agent systems. In: Second International Working Conference on Evaluation of Novel Approaches to Software Engineering (ENASE), pp. 10–18 (2007)

[7] Ekinci, E.E., Tiryaki, A.M., Çetin, Ö.: Goal-oriented agent testing revisited. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 85–96 (2008)

[8] Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 133–144 (2008)

[9] Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 165–176 (2008)

[10] Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

[11] Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

[12] Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

[13] Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

[14] d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, pp. 155–176. Springer-Verlag, LNAI 1365 (1998)

[15] Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

[16] Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

[17] Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

[18] Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), pp. 42–55. Springer Verlag, LNAI 1038 (1996)

[19] Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

[20] Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

[21] Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemel ¨ a (eds.) Proceedings of the 23rd International Conference on Logic Programming (ICLP), pp. 302–316. Springer, LNCS 4670 (2007)

[22] Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http://www.math.upenn.edu/∼wilf/gfology2.pdf

[23] Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

[24] Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), industry track., pp. 37–44. IFAAMAS (2008)

[25] Parunak, H.V.D.: “go to the ant”: Engineering principles from natural multi-agent systems. Annals of Operations Research 75, 69–101 (1997). (Special Issue on Artificial Intelligence and Management Science)

[26] van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

[27] Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: The Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

[28] Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: International Conference on Software Engineering: Future of Software Engineering, pp. 120–136 (2007)

[29] Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

[30] Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

[31] Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007",On the testability of BDI agent systems,,'University of Otago Library',,,core
73879806,2010-06-25,"Artificial neural networks (ANN) offer tremendous promise in classifying electrocardiogram (ECG) for detection and diagnosis of cardiovascular diseases. In this thesis, we propose a reusable neuron architecture (RNA) to enable an efficient and cost-effective ANN-based ECG processing by multiplexing the same physical neurons for both feed-forward and back-propagation stages. RNA further conserves the area and resources of the chip and reduces power dissipation by coalescing different layers of the neural network into a single layer. Moreover, the microarchitecture of each RNA neuron has been optimized to maximize the degree of hardware reusability by fusing multiple two-input multipliers and a multi-input adder into one two-input multiplier and one two-input adder. With RNA, we demonstrated a hardware implementation of a three-layer 51-30-12 artificial neural network using only thirty physical RNA neurons.A quantitative design space exploration in area, power dissipation, and speed between the proposed RNA and three other implementations representative of different reusable hardware strategies is presented and discussed. An RNA ASIC was implemented using 45nm CMOS technology and verified on a Xilinx Virtex-5 FPGA board. Compared with an equivalent software implementation in C executed on a mainstream embedded microprocessor, the RNA ASIC improves both the training speed and the energy efficiency by three orders of magnitude, respectively. The real-time and functional correctness of RNA was verified using real ECG signals from the MIT-BIH arrhythmia database",RNA: REUSABLE NEURON ARCHITECTURE FOR ON-CHIP ELECTROCARDIOGRAM CLASSIFICATION AND MACHINE LEARNING,,,,,core
236402894,2009-11-02T00:00:00,"Modern, intelligent organizations use the full power of information to reach excellent performance. Intelligence is not just about smart decisions, but also about the ability to obtain, model and understand information and processes, the power to adapt to new situations and events in the networked economy, the ability to learn, the capability to analyze, discover and manage knowledge, and the ability to perform effectively according to rules and policies in order to innovate and create value.    Jan VanthienenJan Vanthienen is professor of information systems and management at Katholieke Universiteit Leuven, Faculty of Business and Economics (Belgium) where he is teaching on business intelligence, systems analysis, and information management. His current research interests include information and knowledge management, business rules & processes, business intelligence, e-learning and information productivity. He is a founding member of the Leuven Institute for Research in Information Systems (LIRIS), and president-elect of the Benelux Association for Information Systems (BENAIS). He is chairholder of the PricewaterhouseCoopers Chair on E-Business at K.U.Leuven and co-chairholder of the Microsoft Research Chair on Intelligent Environments.Orkand Chair
University of Maryland
University College
The Intelligent Organization
November 2, 2009 | Adelphi, MD
Prof. dr. Jan Vanthienen
Katholieke Universiteit Leuven (Belgium)
Leuven Institute for Research in Information Systems
jan.vanthienen@econ.kuleuven.be2
Who?3
The world is changing…
•
How about our enterprises, governments, organizations?
–
Information Technology and Systems fundamentally change(d) our organizations
•
How do we handle the change?
–
We love it, as long as we can keep doing things in the same way and there are no big differences
•
Do we handle change in an intelligent way?
–
How to become an intelligent organization?
–
And what is an intelligent organization anyway?4
Some examples from our organizations…
•
Creating information
•
Capturing information
•
Finding information
•
Validating information
•
Classifying information
•
Integrating information
•
Distributing information
•
Understanding information
•
Processing information
•
Using information5
Some examples from our organizations…
•
Creating informationwriting documents -> typing documents
•
Capturing informationpaper forms -> web forms
•
Finding informationlibrary -> search, Google
•
Validating informationprinting and comparing
•
Classifying informationbook shelves -> folders
•
Integrating informationhardly
•
Distributing informationletter -> e-mail
•
Understanding informationreading reports -> electronic paper
•
Processing informationmanual processes -> workflow
•
Using informationmanually
We basically do it in the same way, but using electronic means6
Jan’s e-mail
Not including: spam, publicity, newsletters
Will this be solved by
-
Facebook
-
Twitter
-
LinkedIn
-
Unified Comm
-
???7
•
Megabyte: 1,000,000 or 106bytes
–
5 megabytes: Complete works of Shakespeare
•
Gigabyte: 1,000,000,000 or 109bytes
–
1 gigabyte: Pickup truck filled with paper
•
Terabyte [ 1,000,000,000,000 bytes OR 1012bytes]
–
2 Terabytes: An academic research library
–
10 Terabytes: The printed collection of the Library of Congress
–
200 Terabytes: Worldwide production of office documents (printer/copier);400 million trees (annually)
–
900 Terabytes: Therequired storage space for email (annual)
•
Petabyte [ 1,000,000,000,000,000 bytes OR 1015bytes]
–
2 Petabytes: All US academic research libraries
–
8 Petabytes: All information available on the Web
•
Exabyte [ 1,000,000,000,000,000,000 bytes OR 1018bytes]
–
2 Exabytes: Total volume of information generated worldwide annually
(Lyman, Peter and Hal R. Varian, ""How Much Information"", 2003.
Retrieved from http://www.sims.berkeley.edu/how-much-info-2003)
The Information Tsunami8
Maybe we have to make some changes …9
Adding
a little
intelligence10
Intelligence?11
Main Entry: in·tel·li·gencePronunciation: in-'te-l&-j&n(t)sEtymology: Middle English, from Middle French, from Latin intelligentia, from intelligent-, intelligens intelligent
1 a :INFORMATION:the ability or agency to obtain the necessary information;
1 b : COMPREHENSION: the act of understandingand abstraction;
1 c : COMMUNICATION: the capacity to communicate accurately,
2 :ADAPTABILITY: the power to adapt to the environment, also: CHANGE, INNOVATION;
3 a : LEARNING: the capability to learn;
3 b : REASONING:the ability to reason;;
4 :EFFECTIVENESSthe aptitude to perform functions;
Intelligence12
1 a :INFORMATION:the ability or agency to obtain and process the necessary information;
1 b : COMPREHENSION: the act of understandingand abstraction; handling concepts and models;
1 c : COMMUNICATION: the capacity to communicate accurately, based on meaning and semantics;
2 :ADAPTABILITY: the power to adapt to the environment, reacting to new situations and events, also: CHANGE, INNOVATION:changing the environment or finding an entirely new one;
3 a : LEARNING: the capability to learn from examples or experience; mining data, rules and processes;
3 b : REASONING:the ability to reason; also:the ability to apply knowledge to one's environment for making smartdecisions;
4 :EFFECTIVENESSthe aptitude to perform information functions, according to the business goals and compliantwith policies and rules;
Intelligence (in the organization)13
The Intelligent Organization
•
Information and Comprehension
•
Communication and Semantics
•
Adaptability and Change
•
Learning and Reasoning
•
Effectiveness

Information Ecology, Awareness, Literacy14
Perceived Computer Literacy
(Perceived Computer Literacy Among Different Types Of (Under)graduate Students: Findings Of A Survey, Poelmans S., Truyen F., Deslé R., 2009)15
60 Technology Skills We Should Be Teaching in College …
Basic Web Stuff1. Basics of HTML (bold, underline, italics, special characters)
10. How (and when) to use collaborative documents or spreadsheets
Organization13. How to set up a web-based calendar and use it to manage your time15. How to find a common meeting time (e.g. Doodle)
Communication22. How to summarize your thoughts in 140 characters or less24. How to determine whether you should share it in a public forum
Finding and Managing Information28. How to use web-based bookmarks32. Who writes Wikipedia articles and when can they be trusted?34. When can you trust the information you find?35. How to use article citations to find better references
Privacy, Security, and the Law40. What should you share and how does that change for different audiences?41. How to manage usernames & passwords42. How to find and tweak the privacy settings in common social networking sites
Presentation45. How to determine the audience and appropriate length for your presentation46. Good presentation design principles48. How to share a set of slides on the Internet51. How to find high-quality images that can be used in presentations (with appropriate copyrights)
Ways to Learn56. How to build an interactive mindmap to organize ideas58. How to find good sites, blogs, and other online publications for the topic you are learning about
(Maria H. Andersen, http://teachingcollegemath.com/?p=1498)16
Our information policy17
Key message: Information Attitude
•
We need (and have) good tools, techniques, architectures, …
•
We need digital literacy and computer skills,
•
But this will not solve the quality and productivity issuesif there is no sound information attitude
•
The attitude means:
–
a single version of the truth
–
avoid double work, re-work, creating useless work
–
rethink processes and procedures
–
do not copy the manual procedures
–
understand the difference between push and pull information
–
maintenance and consistency
–
know what can go wrong
–
manage data, information & knowledge
–
separate the know, the flow, the show
A little intelligence will do …18
‘Work smarter, not harder’
(Christopher Thomas)19
Real change: e.g. in e-government
Value
Information,
Interaction
Integration
Transaction
Transformation
Downloadforms
Electronic
forms
Integrated
forms
No more
forms20
The Intelligent Organization
•
Information and Comprehension
•
Communication and Semantics
•
Adaptability and Change
•
Learning and Reasoning

Business-IT Alliance
•
Effectiveness21
‘It’s not what you say,it’s what they hear!’
(Red Auerbach)22
Engineering the Business
Business Strategy… Information Systems
Business Processes
Business Rules
Business
Vocabulary
The 3 musketeers
One for all,
all for one23
System 1
Core Business Rules
Operational Business Rules & Constraints
Work practice Rules
System 1 (new)
Rules are valid across systems
System 3 (outsourced)
System 5 (interorganizational)
System 2 (Web)
System 4 (manual)
•
bringing business rules out of obscurity.
•
bringing business rules out of implementation
•
bringing business rules out of code
•
and into the business management side24
Kim Clijsters' Tennis Ranking
“Clijsters becomes the world's number one if she reaches the final, or If Davenport doesn't reach the final, or Mauresmo doesn't win the tournament.
Lindsay Davenport stays number one if she wins the tournament and Clijsters doesn't reach the final, or she looses the final (against another player than Mauresmo) and Clijsters looses in the semi-finals.
Amélie Mauresmo becomes number one if she wins the tournament and Clijsters looses in the quarter-finals. ”
(translated from www.sporza.be)
(Example provided by Christophe Mues)25
Tennis Ranking26
The Intelligent Organization
•
Information and Comprehension
•
Communication and Semantics
•
Adaptability and Change
•
Learning and Reasoning

Flexibility and Compliance
•
Effectiveness27
Business Agility & Compliance
•
Agilityis the ability of a business to sense changes in its environment and to react (change) effectively on the level of strategy, policy as well as operations
•
Complianceis the ability of a business to prove that its operations are conducted according to applicable regulations, business policy and rules.
•
Solving the trade-off

Business Process Management (BPM) makes us flexible and efficient

Service Oriented Architectures (SOA) allow to realize this

Business Rules Management(BRM) makes us agile and compliant

Enterprise Decision Management (EDM) makes us smarter28
Agile processes
(BPMN 1.0: OMG Final Adopted Specification, 2006)
but also:
•
Timing rules
•
Workflow rules
•
Access rules
•
Audit rules
•
…
Decision rules
Discount rules
Implicit(hard-coded) rule:noadvancepaymentnecessary
Whatifthe rulechanges??29
Cell Phone Bill Payment
Kathy A. Long, ""The KISS Approach to Process,"" Business Rules Journal, Vol. 10, No.1 (Jan.2009)
This is not a process.
Finance
Admin
Manager
Phone Co
Accounting30
When is this rule applied?
–When a customer submits an new order
–When a customer changes an existing order (adds items,changesquantities, substitutes products)
–When a customer’s credit limit is changed
–When product prices are changed (unless prices are frozen at order time)
–… and for any other relevant events the system recognizes
Business Rules influence the Process
“The total value of a customer’s unpaid orders must never exceed his credit limit”
Businessevent
Somethinghappens
process
Businessrule
The total value of a customer’s unpaid orders must never exceed his credit limit
Do something
process
(adapted from: Ron Ross)31
Two sides of the same coin
Business Process Management
–
BP Modelling
•
Analysis
•
Quality/Maintenance
•
Improvement
–
BP Enactment
–
BAM -Business Activity Monitoring
–
Execution
–
BP Maturity
Business Rules Management
–
BR Modelling
•
Analysis
•
Quality/Maintenance
•
Improvement
–
BR Enactment
–
RAM -Rule Activity Monitoring
–
Execution
–
BR Maturity
Rules are from Mars and Processes are from Venus32
The Intelligent Organization
•
Information and Comprehension
•
Communication and Semantics
•
Adaptability and Change
•
Learning and Reasoning

Smart systems
•
Effectiveness33
‘An organization’s ability to learn,and translate that learning into action rapidly,is the ultimate competitive advantage.’
Jack WelchChairman, General Electric34
KNOWLEDGE
INFORMATION
ORGANIZE
ANALYZE
DATA35
•
What do I want to happen?
–
Optimization, Planning
•
What will happen?
–
Forecasting & Prediction
•
Why is it happening?
–
Why have sales gone down?
•
What is happening?
–
Are sales going up or down, trend analysis
•
What happened?
–
What were total sales this month?
(based on: Bill Baker, Microsoft)
Business Intelligence
AnalyticsModeling
AnalysisAlerts
ReportingAccess36
Knowledge Discovery
From data to action
Descriptive
Predictive
Quality
Data, rules &processes
Data
Information
Knowledge
Action
Collection
Decisions
Integration
Analysis37
Knowledge Discovery

Associations, Sequences and Clustering

Cross-selling

Segmentation

Web usage mining

Text clustering

Risk profiles
From data to action
Descriptive
Predictive
Quality
Data, rules &processes38
Knowledge Discovery

Looking forward is important !!!
From data to action
Descriptive
Predictive
Quality
Data, rules &processes39
Knowledge Discovery

Predictive: Classification and Regression

Customer retention

Response Modeling

Bankruptcy

Credit scoring
From data to action
Descriptive
Predictive
Quality
Data, rules &processes
•
Baesens B., Setiono R., Mues C., Vanthienen J., Using Neural Network Rule Extraction and Decision Tables for Credit-Risk Evaluation, Management Science, Volume 49, Number 3, pp. 312-329, March 2003.40
Knowledge Discovery

Comprehensibility
•
Mues C., Huysmans J., Baesens B., Vanthienen J., An empirical investigation into the interpretability of data mining models based on decision trees, tables and rules, 22nd European Conference on Operational Research, Prague, July 8-11, 2007.
•
Martens D., De Backer M., Haesen R., Vanthienen J., Snoeck M., Baesens B., Classification with Ant Colony Optimization, IEEE Transactions on Evolutionary Computation, Vol. 11, No. 5, pp. 651-665, 2007.

Consistency
•
Huysmans J., Baesens B., Vanthienen J., A new approach for measuring rule set consistency, Data & Knowledge Engineering, vol. 63, no. 1, pp. 167-182, 2007.

In line with domain knowledge
•
Martens, D., Baesens, B., Van Gestel, T., Vanthienen, J. (2007). Comprehensible credit scoring models using rule extraction from support vector machines, European journal of operational research, 183(3), 1466-1476.
From data to action
Descriptive
Predictive
Quality
Data, rules &processes41
Knowledge Discovery

Process mining
From data to action
Descriptive
Predictive
Quality
Data, rules &processes
•
Goedertier, S., Martens, D., Vanthienen, J., Baesens, B. (2009), Robust process discovery with artificial negative events, accepted for Journal of Machine Learning Research.42
The Intelligent Organization
•
Information and Comprehension
•
Communication and Semantics
•
Adaptability and Change
•
Learning and Reasoning

Business value
•
Effectiveness43
Different kinds of decisions
High-value, low-volume decisions
Medium-value, medium-volume decisions
Low-value, high-volume decisions
Low High
High
ECONOMIC IMPACT OF INDIVIDUAL DECISION
Low
DECISION VOLUME44
Smart decisions
EDM (Enterprise decision management)
–
Modelling, managing and executing the business decisions
–
Descriptive Analytics, BI
–
Predictive Analytics
–
CEP (Complex Event Processing)45
Business Decision Value
Business event
Action Time
Capturing
Reporting
Decision latency
Business Value
Analysis completed
Action taken
(basedon:RichardHackathorn,BolderTechnology,Inc)
Information delivered
Data captured
Analysis
latency46
Thankyo",Intelligent organization,,University of Maryland University College (UMUC),,,core
236403589,2011-11-01T00:00:00,"The computer gaming industry has begun to export powerful products and technologies from its initial entertainment roots to a number of ""serious"" industries. Game technologies are being adapted to defense, medicine, architecture, education, city planning, and government applications. Each of these industries is already served by an established group of companies that typically do not use computer games to serve their customers. The rapid growth in the power of game technologies and the growing social acceptance of these technologies has created an environment in which these are displacing other industry-specific computer hardware and software tools.

This dissertation proposes four hypotheses concerning the impact and acceptance of virtual reality and computer game technologies in education and training for laparoscopic surgery. It focuses on laparoscopic surgery because of the similarities between that form of surgery and virtual reality systems. The research indicates that the following four hypotheses are supported by the literature published in the field. Hypothesis 1: Training in laparoscopic surgery can be accomplished at a lower cost using virtual reality and game technology-based tools than through existing methods of training. Hypothesis 2: Virtual reality and game technology-based training environments provide better access to representative patient symptoms and allow more repetitive practice than existing forms of training. Hypothesis 3: Virtual reality and game technology-based training environments can reduce the training time required to achieve proficiency in laparoscopic procedures. Hypothesis 4: Virtual reality and game technology-based training can reduce the number of medical errors caused by residents and surgeons learning to perform laparoscopic procedures.

I also proposed a model of medical education in which virtual reality, including game technology, is the next major addition to or transformation of the medical education curriculum. The strong evidence collected in this study indicates that these systems are becoming much more accepted in medical education and that the technical limitations that existed when these devices were first introduced are already being overcome.ABSTRACT
Title of Dissertation: INVESTIGATING THE DISRUPTIVE
EFFECT OF COMPUTER GAME
TECHNOLOGIES ON MEDICAL
EDUCATION AND TRAINING
Roger D. Smith, Doctor of Management, 2008
Dissertation Directed By: Dr. Michael Evanchik, Graduate School of
Management and Technology,
University of Maryland University College
The computer gaming industry has begun to export powerful products and technologies
from its initial entertainment roots to a number of “serious” industries. Game technologies are
being adapted to defense, medicine, architecture, education, city planning, and government
applications. Each of these industries is already served by an established group of companies that
typically do not use computer games to serve their customers. The rapid growth in the power of
game technologies and the growing social acceptance of these technologies has created an
environment in which these are displacing other industry-specific computer hardware and
software tools.
This dissertation proposes four hypotheses concerning the impact and acceptance of
virtual reality and computer game technologies in education and training for laparoscopic
surgery. It focuses on laparoscopic surgery because of the similarities between that form of
surgery and virtual reality systems. The research indicates that the following four hypotheses are
supported by the literature published in the field.
• Hypothesis 1: Training in laparoscopic surgery can be accomplished at a lower cost
using virtual reality and game technology-based tools than through existing methods
of training.
• Hypothesis 2: Virtual reality and game technology-based training environments
provide better access to representative patient symptoms and allow more repetitive
practice than existing forms of training.
• Hypothesis 3: Virtual reality and game technology-based training environments can
reduce the training time required to achieve proficiency in laparoscopic procedures.
• Hypothesis 4: Virtual reality and game technology-based training can reduce the
number of medical errors caused by residents and surgeons learning to perform
laparoscopic procedures.
I also proposed a model of medical education in which virtual reality, including game
technology, is the next major addition to or transformation of the medical education curriculum.
The strong evidence collected in this study indicates that these systems are becoming much more
accepted in medical education and that the technical limitations that existed when these devices
were first introduced are already being overcome.
INVESTIGATING THE DISRUPTIVE EFFECT OF COMPUTER GAME TECHNOLOGIES
ON MEDICAL EDUCATION AND TRAINING
By
Roger D. Smith.
Dissertation submitted to the Faculty of the Graduate School of the
University of Maryland University College, in partial fulfillment
of the requirements for the degree of
Doctor of Management
2008
Advisory Committee:
Dr. Michael Evanchik, Chair
Dr. Monica Bolesta, Member
Dr. Joseph D’Mello, Member
© Copyright by
Roger D. Smith
2008
ii
Acknowledgements
I would like to thank the members of my dissertation committee: Dr. Michael Evanchik,
Dr. Monica Bolesta, and Dr. Joseph D’Mello for their guidance and help in structuring this
research so that it makes a valuable contribution toward understanding current and future
changes in the business of medical education. I also appreciate the opportunity that this degree
program and its faculty have given me to expand my understanding of business, management,
and innovation. This knowledge has been tremendously helpful to me in contributing to the
companies and government organizations with which I have been associated.
I am indebted to my mother and father for the foundations that they established in my
early years and for always believing that I could accomplish great things. I only wish that my
father had lived to see this newest accomplishment. Finally, my gratitude to my wife and
children for allowing me to spend years shut in my office working toward this goal. Your
understanding and support have been greatly appreciated.
iii
Table of Contents
Acknowledgements............................................................................................................. ii
Table of Contents............................................................................................................... iii
List of Tables ...................................................................................................................... v
List of Figures .................................................................................................................... vi
Chapter 1: Introduction and Research Problem.................................................................. 1
Virtual Reality................................................................................................................. 2
Computer Game Technologies ....................................................................................... 3
3D Engine ................................................................................................................... 5
Graphical User Interface ............................................................................................. 7
Physical Models.......................................................................................................... 8
Artificial Intelligence .................................................................................................. 8
Networking ................................................................................................................. 9
Persistence................................................................................................................. 10
Terminology Issues....................................................................................................... 11
Surgical Practice and Education ................................................................................... 12
Research Problem ......................................................................................................... 16
Chapter 2: Literature Review............................................................................................ 20
Medical Education with Virtual Reality ....................................................................... 20
Pioneers in Medical Simulation ................................................................................ 21
Simulation as a Tool for Education .......................................................................... 23
Cost Factors in Medical Education........................................................................... 26
Access to Patient Symptoms and Virtual Reality ..................................................... 28
Simulation and VR Impact on Training Time .......................................................... 31
Potential to Reduce Medical Errors .......................................................................... 33
Game Technology for Non-Entertainment Applications.............................................. 35
Historical Applications ............................................................................................. 35
Educational Applications .......................................................................................... 36
Business Aspects of Games ...................................................................................... 40
Games as Technology Products ................................................................................ 44
Social Acceptance of Games .................................................................................... 45
Dissertations on Computer Games................................................................................ 48
Social Impacts........................................................................................................... 48
Educational Applications .......................................................................................... 51
Business Aspects of Games ...................................................................................... 51
Technology in Games ............................................................................................... 52
Simulation ..................................................................................................................... 53
History of Technology .................................................................................................. 55
Disruptive Innovation and Creative Destruction .......................................................... 59
Chapter 3: Conceptual Framework and Research Method ............................................... 64
Conceptual Framework................................................................................................. 66
Rationale ....................................................................................................................... 72
The Hypotheses............................................................................................................. 74
iv
Research Method .......................................................................................................... 76
Reference Coding.......................................................................................................... 82
Chapter 4: Data Analysis, Results, and Conclusions........................................................ 86
Data Analysis ................................................................................................................ 86
Hypothesis 1: Lower Cost......................................................................................... 86
Hypothesis 2: Better Access ..................................................................................... 95
Hypothesis 3: Reduced Training Time ................................................................... 101
Hypothesis 4: Reduced Errors ................................................................................ 106
Results........................................................................................................................ 112
H1 Lower Cost: Supported ..................................................................................... 113
H2 Better Access: Supported.................................................................................. 113
H3 Reduced Training Time: Supported.................................................................. 114
H4 Reduced Errors: Supported ............................................................................... 115
Model of Medical Education: Supported................................................................ 116
Misleading Domain Assumptions............................................................................... 117
Assumption 1: Didactic Education is Effective ...................................................... 117
Assumption 2: Cost of Systems is Not an Issue ..................................................... 118
Assumption 3: Sufficient Access to Faculty and Patients is Possible .................... 119
Assumption 4: Practicing on Live Patients is Acceptable ...................................... 120
Discussion................................................................................................................... 121
Conclusion .................................................................................................................. 122
Chapter 5: Recommendations for Future Work............................................................. 125
Appendix 1. Medical VR Reference Coding Matrix ...................................................... 127
Appendix 2. Medical VR and Simulation Vendors in the Literature Reviewed ............ 154
Appendix 3. Personal Communication in Support of Dissertation Topic ...................... 158
References...................................................................................................................... 170
VR for Laparoscopic Surgical Education ................................................................... 170
General Medical Education and Virtual Reality......................................................... 176
Game Technology for Non-Entertainment Applications............................................ 185
Dissertations on Computer Games.............................................................................. 188
Simulation ................................................................................................................... 190
History of Technology ................................................................................................ 190
Disruptive Innovation and Creative Destruction ........................................................ 191
v
List of Tables
Table 1. MIST-VR implementations used in the literature............................................... 81
Table 2. Cost/benefit of an AccuTouch laparoscopic simulator....................................... 91
Table 3. Cost categories associated with each method of psychomotor training. ............ 92
Table 4. MIST-VR training program for laparoscopic instrument proficiency.............. 103
vi
List of Figures
Figure 1. Sim One computerized training mannequin in 1967........................................... 2
Figure 2. Six core game technologies that are disruptive to other industries. .................... 5
Figure 3. Visual comparison of 3D scenes from 1992 and 2005........................................ 6
Figure 4. Unique domains of simulations, virtual environments, and computer games..... 7
Figure 5. Denson (left) and Hoffman (right) demonstrate Sim One in 1967.................... 22
Figure 6. Medical education model .................................................................................. 66
Figure 7. Medical education model by example............................................................... 67
Figure 8. Minimally Invasive Surgical Trainer – Virtual Reality (MIST-VR) system .... 81
Figure 9. Medical VR coding matrix. ............................................................................... 83
Figure 10. Medical VR reference coding items. ............................................................... 83
Figure 11. Exercises in MIST-VR skills course ............................................................. 104
1
Chapter 1: Introduction and Research Problem
Medical education has traditionally been conducted on live patients, cadavers, live
animals, collections of tissue and organs, and inanimate mannequins. The “gold standard” for
perfecting operations has been the use of porcine subjects in place of humans. But for over 40
years researchers, surgeons, and scientists have been introducing computerized devices to
augment or replace many of the traditional tools for training. The “Sim One” computerized
mannequin is considered one of the first applications of computers to medical training. This
system was conceived at an aerospace company in 1964, funded with a $272,000 grant from the
Department of Education, and first demonstrated on March 17, 1967. Sim One delivered a
mechanically animated, computer controlled mannequin that could receive and respond to two
forms of gaseous anesthesia and four forms of injection. The “patient” breathed, had a heart beat,
presented temporal and carotid pulse, and maintained blood pressure. The mannequin opened
and closed its mouth, blinked its eyes, and changed these behaviors in response to anesthesia
administered through a mask or a tube (Abrahamson, 1997). The device was enhanced in 1971 to
deliver training in respirator application, endotrachael intubation, intramuscular injection,
recovery room care, and the measurement of pulse and respiration (Hoffman & Abrahamson,
1975). Figure 1 shows the system in a classroom as it would be used for education.
2
Figure 1. Sim One computerized training mannequin in 1967.
Source: Abrahamson, 1997
Hoffman and Abrahamson (1975) summarized the results of 15 different studies into the
effectiveness of the device in improving performance in medical practice. These studies
demonstrated improvements in “learning gain per unit of time, amount of student time required
to reach criterion levels of performance, and investment of faculty time necessary for student
learning.” The educational improvements that were achieved using what would today be
considered primitive computers and animatronics were very impressive and suggest that further
development of these devices could grow these advantages and add others that were not
achievable forty years ago.
Virtual Reality
With Sim One and many later computerized mannequins as a foundation, new computer
technologies have been introduced into medical training with the hope of carrying improvements
3
deeper into the educational curriculum. One group of these technologies includes virtual reality
and the software being created for modern computer games. The first medical virtual reality
system based on a head mounted display and a data glove was introduced by Richard Satava and
Jaron Lanier in 1991 (Satava, 1993). Lanier had coined the term “virtual reality” around 1984 to
refer to the use of electronic devices for immersing humans into a computer generated world (the
head mounted display) and to provide a tool with which to interact with that world (the data
glove). Satava applied these to medical training and demonstrated how such a system might be
employed to teach surgery. Satava’s assessment of that system was that the technology was no
where near good enough to be used in real training. He felt that it would take at least ten more
years for the technology to reach a useful state (R. Satava, personal communication, January 10,
2008).
Early definitions of “virtual reality” required that a system must immerse at least one of
the senses by cutting off access to the outside world and replacing it with a computer generated
stimuli. However, a less strict definition often allows that the visual, audible, or tactile stimuli
can be presented without totally eliminating external, non-computerized stimuli. This latter view
has proven to be more practical and less expensive to develop and to sell to customers. In
medical education, the term virtual reality is usually applied to any system where 3D computer
images are being presented and manipulated. This categorization leads to computer games being
referred to as virtual reality in most of the medical literature.
Computer Game Technologies
The computer gaming industry has begun to export powerful products and technologies
from its initial entertainment roots to a number of “serious” industries. Game technologies are
being adapted in defense, medical, architectural, educational, social, and governmental
4
applications. Each of these industries is already served by an established group of companies that
typically do not use computer games to serve their customers. The rapid growth in the power of
game technologies and the growing social acceptance of these technologies has created an
environment in which these are displacing other industry-specific computer hardware and
software tools.
Computer games provide a rich environment in which to train a wide variety of tasks.
The availability of the necessary computer hardware and game-based software technologies
makes these an attractive alternative to existing methods of training (Lane, 1995; Mayo, 2007).
This attraction is motivated by lower costs, higher effectiveness, and the in",Investigating the disruptive effect of computer game technologies on medical education and training,,University of Maryland University College (UMUC),,,core
226640942,2012-01-01T00:00:00,"More and more the notion of agent appears in different contexts of computer science, often with different meanings.
Main ones are Arti&#64257;cial Intelligence (AI) and Distributed AI,
where agents are exploited as a technique to develop systems
exhibiting some kind of intelligent behavior. In this paper,
we introduce a further perspective, shifting the focus from
AI to computer programming and programming languages. In
particular, we consider agents and related concepts as generalpurpose abstractions useful for programming software systems
in general, conceptually extending object-oriented programming
with features that \u2013 we argue \u2013 are effective to tackle some
main challenges of modern software development. The main
contribution of the work is the de&#64257;nition of a conceptual space
framing the basic features that characterize the agent-oriented
approach as a programming paradigm, and its validation in
practice by using a platform called JaCa, with real-world
programming examples",A Programming Paradigm based on Agent-Oriented Abstractions,,,,,core
360545876,2011-04-07T00:00:00,"Only an extended abstract was published in the proceedings. There is no full text.The process of classifying objects is a fundamental feature of most human pursuits, and the idea that people classify together those things that they find similar is both intuitive and popular across a wide range of disciplines. Therefore, similarity is important for people to make sense of the objects, structures and actions that exist in reality. Furthermore the ability to recognise a similar situation means that experience can be reused to solve problems, alleviating complex situations, save time and allow valuable resources to be used elsewhere.

Various philosophical and psychological theories of similarity have been implemented in information science. Specific information science terms associated with similarity include indexing, sub-setting, retrieval, matching, ranking, solution space, clustering, trees, categorising, equal and equivalence. Information science research in the field of similarity could be grouped under the headings of comparison, retrieval, evaluation and analysis functions. Various researchers from different information science disciplines are studying similarity. The results and ideas between some of these disciplines are interchangeable because of the overlapping interests. The different disciplines include computer vision, graphic design, pattern recognition, image analysis, databases, AI, remote sensing and GI systems.

Spatial similarity can be seen as a subset of similarity and all the entities being compared to each other have spatial components. Research areas that utilise spatial similarity are listed below in Table 1. It is acknowledged that some of the research overlaps, however it was decided to catergorise the general areas of spatial similarity research.PublishedNon Peer Reviewe",Spatial similarity,https://core.ac.uk/download/360545876.pdf,,,,core
160335033,2012-09-04T00:00:00Z,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",Software and System Health Management for Autonomous Robotics Missions,,,10.1184/r1/6710654.v1,,core
155684784,2011,"Several kernel based methods for multi-task learning have been proposed, which leverage relations among tasks as regularization to enhance the overall learning accuracies. These methods assume that the tasks share the same kernel, which could limit their applications because in practice different tasks may need different kernels. The main challenge of introducing multiple kernels into multiple tasks is that models from different Reproducing Kernel Hilbert Spaces (RKHSs) are not comparable, making it difficult to exploit relations among tasks. This paper addresses the challenge by formalizing the problem in the Square Integrable Space (SIS). Specially, it proposes a kernel based method which makes use of a regularization term defined in the SIS to represent task relations. We prove a new representer theorem for the proposed approach in SIS. We further derive a practical method for solving the learning problem and conduct consistency analysis of the method. We discuss the relations between our method and an existing method. We also give an SVM based implementation of our method for multi-label classification. Experiments on two real-world data sets show that the proposed method performs better than the existing method. Copyright ? 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.EI",Multi-task learning in square integrable space,,,,,core
21162874,2010-03-20,"Kernel machines are a popular class of machine learning algorithms that achieve state of the art accuracies on many real-life classification problems. Kernel perceptrons are among the most popular online kernel machines that are known to achieve high-quality classification despite their simplicity. They are represented by a set of B prototype examples, called support vectors, and their associated weights. To obtain a classification, a new example is compared to the support vectors. Both space to store a prediction model and time to provide a single classification scale as O(B). A problem with kernel perceptrons is that the number of support vectors tends to grow without bounds with the number of training examples on noisy data. To reduce the strain at computational resources, budget kernel perceptrons have been developed by upper bounding the number of support vectors. In this work, we proposed a new budget algorithm that upper bounds the number of bits needed to store kernel perceptron. Setting the bitlength constraint could facilitate development of hardware and software implementations of kernel perceptrons on resource-limited devices such as microcontrollers. The proposed compressed kernel perceptron algorithm decides on the optimal tradeoff between number of support vectors and their bit precision. The algorithm was evaluated on several benchmark data sets and the results indicate that it can train highly accurate classifiers even when the available memory budget drops below 1 Kbit. This promising result points to a possibility of implementing powerful learning algorithms even on the most resourceconstrained computational devices. ",COMPRESSED KERNEL PERCEPTRONS,,,,,core
12042846,2011-01-01T00:00:00,"In Global Navigation Satellites Systems (GNSS), multipaths (MP) are still one of the major error sources. The additional signal replica due to reflection will introduce a bias in conventional Delay Lock Loops (DLL) which will finally cause a strong positioning error. Several techniques, based on Maximum Likelihood estimation (ML), have been developed for multipaths mitigation/estimation such as the Narrow correlator spacing [1] or the Multipath Estimating Delay-Lock-Loop  (MEDLL) [2] algorithm. These techniques try to discriminate the MP from the Line Of Sight Signal (LOSS) on the time and frequency domains and thus, short delay multipaths (<0.1Chips) can not be completely mitigated. Antenna array perform a spatial sampling of the wave front what makes possible the discrimination of the sources on the space domain (azimuth and elevation). As the time-delay domain and space domain can be assumed independent, we can expect to mitigate/estimate very short delay MP by using an antenna array. However, we don't want to increase too much the size, the complexity and the cost of the receivers and thus, we focus our study on small arrays with a small number of antennas: typically a square 2x2 array. Consequently, conventional beamforming (space Fast Fourier Transform) is not directive enough to assure the mitigation of the multipaths, and then this first class of solutions was rejected. In order to improve the resolution, adaptive beamformers have also been tested. However, the LOSS and the MP signal are strongly correlated and thus, classical adaptive algorithms [3] are not able to discriminate the sources. These preliminary studies have shown that the mitigation/estimation of multipaths based on the space domain will exhibit limited performances in presence of close sources. Then, in order to propose robust algorithms, we decided to investigate a space-time-frequency estimation of the sources. Space Alternating Generalized Expectation maximisation (SAGE) algorithm [4], which is a low-complexity generalization of the Expectation Maximisation (EM) algorithm, has been considered. The basic concept of the SAGE algorithm is the hidden data space [4]. Instead of estimating the parameters of all impinging waves in parallel in one iteration step as done by the EM algorithm, the SAGE algorithm estimates the parameters of each signal sequentially. Moreover, SAGE algorithm breaks down the multi-dimensional optimization problem into several smaller problems. In [5], it can be seen that SAGE algorithm is efficient for any multipaths configurations (small relative delays, close DOAs) and space-time-frequency approach is clearly outperforming classical time-frequency approaches. Notwithstanding, SAGE algorithm is a post processing algorithm. Thus, it's necessary to memorise in the receiver the incoming signal in order to apply SAGE estimation. For example, if we want to process 10ms of signal with a 10MHz sampling rate, we need to store a matrix of m*105 with m the number of antennas. In such condition, we can understand than SAGE algorithm is hardly implemented in real time. The challenge is then to find a new type of algorithms that reach the efficiency of the SAGE algorithms, but with a reduced complexity in order to enable real time processing.

Furthermore, the implementation should be compatible with conventional GNSS tracking loops (DLL and PLL). To cope with these two constraints, we propose to apply the SAGE algorithm on the post-correlated signal. Indeed, the correlation step can be seen as a compression step and thus, the size of the studied signal is strongly reduced. In such a way, SAGE algorithm is able to provide estimates of the relative delay and Doppler of the received signals with respect to the local replicas. Thus, a post correlation implementation of SAGE can be seen as a discriminator for both the DLL and the PLL",A new tracking approach for multipath mitigation based on antenna array,https://core.ac.uk/download/12042846.pdf,,,,core
324082739,2012-11-21T00:00:00,"The Istituto Nazionale di Geofisica e Vulcanologia (INGV) is the Italian agency devoted to monitor in real time the seismicity on the Italian territory.  The seismicity in Italy is of course variable in time and space, being also very much dependant on local noise conditions. Specifically, monitoring seismicity in an alluvial basin like the Po one is a challenge, due to consistent site effects induced by soft alluvial deposits and bad coupling with the deep bedrock (Steidl et al., 1996). This problem was tackled by INGV first with the Cavola experiment (Bordoni et al., 2007), where a landslide was seismically characterized using a seismic array and also down-hole logging of P- and S-wave travel times at a borehole drilled within the array; later, with an ad hoc project in 2000-2001, with the first installation of a broad band seismic station nearby Ferrara in a borehole of 135 meters depth. Comparison of recordings with a surface seismic station indicated a noise reduction of 2 decades in power spectral density at frequencies larger than 1.0 Hz (Cocco et al., 2001). The instrumentation in Ferrara has been working for several months but after that the seismic station was discontinued due to lack of maintenance manpower.
The Centro di Ricerche Sismologiche (CRS, Seismological Research Center) of the Istituto Nazionale di Oceanografia e di Geofisica Sperimentale (OGS, Italian National Institute for Oceanography and Experimental Geophysics) in Udine (Italy) after the strong earthquake of magnitude M=6.4 occurred in 1976 in the Italian Friuli-Venezia Giulia region, started to operate the Northeastern Italy (NI) Seismic Network: it currently consists of 15 very sensitive broad band and 21 simpler short period seismic stations, all telemetered to and acquired in real time at the OGS-CRS data center in Udine (Fig. 1).
Real time data exchange agreements in place with other Italian, Slovenian, Austrian and Swiss seismological institutes lead to a total number of about 100 seismic stations acquired in real time, which makes the OGS the reference institute for seismic monitoring of Northeastern Italy. Since 2002 OGS-CRS is using the Antelope software suite on several workstations plus a SUN cluster as the main tool for collecting, analyzing, archiving and exchanging seismic data, initially in the framework of the EU Interreg IIIA project “Trans-national seismological networks in the South-Eastern Alps”. SeisComP is also used as a real time data exchange server tool (Bragato et al., 2011).
Among the various Italian institution with which OGS is cooperating for real time monitoring of local seismicity there is the Regione Veneto (Barnaba et al., 2012). The Southern part of the Veneto Region stands on the Po alluvial basin: earthquake localization and characterization is here again affected in this area by the presence of soft alluvial deposits. OGS ha already experience in running a local seismic network in difficult noise conditions making use of borehole installations (Priolo et al., 2012) in the case of the monitoring of a local storage site for the Italian national electricity company ENEL. Following the ML=5.9 earthquake that struck the Emilia region around Ferrara in Northern Italy on May 20, 2012 at 02:03:53 UTC, a cooperation of INGV, OGS, the Comune di Ferrara and the University of Ferrara lead to the reinstallation of the very broad band borehole seismic station in Ferrara. The aim of the OGS intervention was on one hand to extend its real time seismic monitoring capabilities toward South-East (Fig. 1), including Ferrara and its surroundings, and on the other hand to evaluate the seismic  response at the site.
As concerns the superficial geology of the area where the borehole seismic station  has been installed, the outcropping materials are represented by alluvial deposits of different environments, like channel and proximal levee, inter-fluvial, meander and swamps deposits. As  a consequence, the outcropping deposits are everywhere Holocene in age substantially loose or poorly compacted in the first meters-decameters and granulometrically could vary from clay to coarse sand.
Two preliminary reports prepared by the Italian Department of Civil Defense (Dipartimento Nazionale di Protezione Civile) in collaboration with other institutions describe the data  recorded by the national accelerometric network and complemented by additional data recorded by a number of temporary stations (Dolce et al., 2012a; Dolce et al., 2012b). These reports bear witness of strong ground motion values with an acceleration peak of about 0.9 g in the vertical component recorded during the ML=5.8 earthquake of May 29, 2012 by the Mirandola station, located at about 2 km from the epicentre. The analysis of the seismic noise recorded at some stations shows a quite pronounced peak of the horizontal-to-vertical spectral ratio (H/V) in the frequency range of 0.6 – 0.9 Hz common to all stations. Finally, strong evidence of liquefaction phenomena are reported at several sites (e.g.: S. Carlo, S. Agostino and Mirabello), most of which have been attributed to the occurrence of saturated sandy layer(s) at shallow depth deposited along an abandoned reach of the Reno River (Papathanassiou et al., 2012).
Details of the station configuration and installation will be outlined, with first results.PublishedPotenza (Italy)1.1. TTC - Monitoraggio sismico del territorio nazionaleope",Installation of a very broad band borehole seismic station in Ferrara (Emilia),,'MIT Press - Journals',,,core
235087894,2012-05-30T07:00:00,"First UND doctorate in computer science
Bismarck native Kirk Ogaard is using his know-how to mine flight data for the Army\u27s Aberdeen Test Center.
You hear them flying overhead every day—they\u27re the aircraft that University of North Dakota aviation majors use to learn their craft. UND\u27s aviation program makes the Grand Forks Airport one of the busiest in the country in terms of takeoffs and landings.
But for Kirk Ogaard, there\u27s a very different kind of business associated with those aircraft: he mines flight data gathered directly from devices aboard.
Ogaard, originally from Bismarck, recently earned UND\u27s first Ph.D. in computer science, opening the path for several others behind him who\u27re also enrolled in the program. Ogaard also got his bachelor\u27s and master\u27s degrees in computer science from UND, known for its prowess as a center of learning in computational science.
 For my Ph.D. dissertation, I wrote a program—a software package—to mine the data that we collected from airplanes used to train UND aviation students,  said Ogaard. His successful Ph.D. completion won Ogaard a spot in a coveted one-to five-year post-doctoral program with the U.S. Army Aberdeen Test Center in Maryland. That\u27s one of nine such centers that support the Developmental Test Command, the Army\u27s premier organization for developmental testing of weapons and equipment.
 At Aberdeen, I\u27ll be doing stuff similar to what I did for my Ph.D.—data mining and probably some visualization,  said Ogaard, who plans to go into full-time research once he\u27s done with his post-doc.
 I got help from Jim Higgins, a former captain with American Eagle Airline, who now teaches in UND\u27s aviation program,  Ogaard said.  He organized the system that collected all the data direct from the aircraft—such as global positioning system information—and offloaded it into a computer at the completion of each flight. 
The challenge, he says, is that once you collect and mine data, there\u27s more than straight analysis.
 To make the data analysis useful you need to be able to draw useful conclusions from it,  Ogaard said.  The real problem, then, is interpreting those results. Visualization can help you do that, whether you convert the answers into some sort of chart or other graphic—in other words, it helps you understand what you\u27ve found in the data. 
You can use off-the-shelf software to create the graphics or you can write your own visualization software. Ogaard wrote his own.
What\u27s UND doing with Ogaard\u27s aviation data mining results?
 Applicability is the key—the University can use it to look at the kinds of maneuvers that students perform, see which maneuvers are most frequent,  Ogaard said.  I think the most useful thing for the University is methodology I developed for analyzing and extracting value from the data. 
Advice for the next generation of Ph.D.\u27s?
 I would say most important thing, be persistent, keep working at, don\u27t get frustrated,  Ogaard said. He completed his PhD in three and a half years after completing the two year\u27s master\u27s program.  About UND PhD program in computer science
The Department of Computer Science offers graduate study leading to the Doctor of Philosophy in Scientific Computing (emphasizing the development of software, the science, and the technology required to support computational science and simulation based science and engineering). The department is a part of the John D. Odegard School of Aerospace Sciences, which provides unique opportunities for research by faculty and graduate students. There is especially strong interest within the department in the areas of artificial intelligence, compiler design, database, networks, operating systems, graphics, simulation, software engineering, and theoretical computer science.
Juan Pedraza
 Writer and Editor, University Relation",First UND doctorate in computer science,,UND Scholarly Commons,,,core
480906222,2011-11-26T08:00:00,"About ICAET
The International Conference on Advances in Engineering and Technology  provides a forum for R&D and academicians and open to researchers from all types of institutions and organizations, aimed at providing the platform for all to interact and share the domain knowledge with each other. Areas included (but not limited to) are electronics and communications engineering, electric energy, automation, control and instrumentation, computer and information technology, and the electrical engineering aspects of building services and aerospace engineering, MATLAB Applications etc. The wide scope encompasses analogue and digital circuit design, microwave circuits and systems, optoelectronic circuits, photovoltaics, semiconductor devices, sensor technology, transport in electronic materials, VLSI technology and device processing. Topic of Interest
The conference covers all aspects of theory and design of circuits, systems and devices for electronics, signal processing, and communication, including:   Signal and System theory, Digital signal processing Network theory and Circuit design Information theory, Communication theory and techniques, Modulation, Source and Channel coding Switching theory and techniques, Communication protocols Optical communications Microwave theory and techniques, Rradar, Sonar Aantennas, wave propagation Measurement and instrumentation; Circuit design, Simulation and CAD Signal and Image processing, Coding; Microwaves, Antennas and Radio propagation Optoelectronics; TV and Sound broadcasting; Telecommunication networks; Radio and Satellite communications; Radar, Sonar and navigation systems; Electromagnetic compatibility. Computer Architecture and Real time Systems Database and Data Mining Intelligent Information & Database Systems Algorithms and Bioinformatics Dependable, reliable and autonomic computing Distributed and parallel systems & algorithms Embedded system and software Game and software engineering Grid and scalable computing IT policy and business management IT Applications Artificial Intelligence and Soft Computing Computer Vision Mobile and ubiquitous computing Modeling and Simulation Multimedia systems and services Parallel and Distributed Systems Statistical computation and simulation Computational intelligence Computational complexity Theoretical computer science Computational biology Medical Informatics   Computer Science and Engineering º Information Technology º Electrical Engineering º Electrical and Electronics Engineering º Electronics and Communication Technology º Mechanical Engineering º Civil Engineering º Bio-informatics and Bio Technologyhttps://www.interscience.in/conf_proc_volumes/1009/thumbnail.jp",Proceedings of International Conference on  Advances in Engineering and Technology,https://core.ac.uk/download/480906222.pdf,Institute for Project Management Pvt. Ltd,,,core
195652143,2012-01-01T00:00:00,"This paper presents an innovative algorithm integrated with particle swarm optimization and artificial neural networks to develop short-term traffic flow predictors, which are intended to provide traffic flow forecasting information for traffic management in order to reduce traffic congestion and improve mobility of transportation. The proposed algorithm aims to address the issues of development of short-term traffic flow predictors which have not been addressed fully in the current literature namely that: a) strongly non-linear characteristics are unavoidable in traffic flow data; b) memory space for implementation of short-term traffic flow predictors is limited; c) specification of model structures for short-term traffic flow predictors which do not involve trial and error methods based on human expertise; d) adaptation to newly-captured, traffic flow data is required. The proposed algorithm was applied to forecast traffic flow conditions on a section of freeway in Western Australia, whose traffic flow information is newly-captured. These results clearly demonstrate the effectiveness of using the proposed algorithm for real-time traffic flow forecasting",Prediction of Short-term Traffic Variables using Intelligent Swarm-based Neural Networks,https://core.ac.uk/download/195652143.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TCST.2011.2180386,,core
37744849,2010,"<p>In most machine learning approaches, it is usually assumed that data are complete. When data are partially missing due to various reasons, for example, the failure of a subset of sensors, image corruption or inadequate medical measurements, many learning methods designed for complete data cannot be directly applied. In this dissertation we treat two kinds of problems with incomplete data using non-parametric Bayesian approaches: classification with incomplete features and analysis of low-rank matrices with missing entries.</p><p>Incomplete data in classification problems are handled by assuming input features to be generated from a mixture-of-experts model, with each individual expert (classifier) defined by a local Gaussian in feature space. With a linear classifier associated with each Gaussian component, nonlinear classification boundaries are achievable without the introduction of kernels. Within the proposed model, the number of components is theoretically ``infinite'' as defined by a Dirichlet process construction, with the actual number of mixture components (experts) needed inferred based upon the data under test. With a higher-level DP we further extend the classifier for analysis of multiple related tasks (multi-task learning), where model components may be shared across tasks. Available data could be augmented by this way of information transfer even when tasks are only similar in some local regions of feature space, which is particularly critical for cases with scarce incomplete training samples from each task. The proposed algorithms are implemented using efficient variational Bayesian inference and robust performance is demonstrated on synthetic data, benchmark data sets, and real data with natural missing values.</p><p>Another scenario of interest is to complete a data matrix with entries missing. The recovery of missing matrix entries is not possible without additional assumptions on the matrix under test, and here we employ the common assumption that the matrix is low-rank. Unlike methods with a preset fixed rank, we propose a non-parametric Bayesian alternative based on the singular value decomposition (SVD), where missing entries are handled naturally, and the number of underlying factors is imposed to be small and inferred in the light of observed entries. Although we assume missing at random, the proposed model is generalized to incorporate auxiliary information including missingness features. We also make a first attempt in the matrix-completion community to acquire new entries actively. By introducing a probit link function, we are able to handle counting matrices with the decomposed low-rank matrices latent. The basic model and its extensions are validated on</p><p>synthetic data, a movie-rating benchmark and a new data set presented for the first time.</p>Dissertatio",Non-parametric Bayesian Learning with Incomplete Data,,,,,core
6206268,2012-06-27T00:00:00,"We consider two active binary-classification problems with atypical
objectives. In the first, active search, our goal is to actively uncover as
many members of a given class as possible. In the second, active surveying, our
goal is to actively query points to ultimately predict the proportion of a
given class. Numerous real-world problems can be framed in these terms, and in
either case typical model-based concerns such as generalization error are only
of secondary importance.
  We approach these problems via Bayesian decision theory; after choosing
natural utility functions, we derive the optimal policies. We provide three
contributions. In addition to introducing the active surveying problem, we
extend previous work on active search in two ways. First, we prove a novel
theoretical result, that less-myopic approximations to the optimal policy can
outperform more-myopic approximations by any arbitrary degree. We then derive
bounds that for certain models allow us to reduce (in practice dramatically)
the exponential search space required by a naive implementation of the optimal
policy, enabling further lookahead while still ensuring that optimal decisions
are always made.Comment: Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012",Bayesian Optimal Active Search and Surveying,http://arxiv.org/abs/1206.6406,,,,core
59392322,2012-03,"We present a new approach to the problem of optimal control of solar sails for low-thrust trajectory optimization. The objective was to find the required control torque magnitudes in order to steer a solar sail in interplanetary space. A new steering strategy, controlling the solar sail with generic torques applied about the spacecraft body axes, is integrated into the existing low-thrust trajectory optimization software InTrance. This software combines artificial neural networks and evolutionary algorithms to find steering strategies close to the global optimum without an initial guess. Furthermore, we implement a three rotational degree-of-freedom rigid-body attitude dynamics model to represent the solar sail in space. Two interplanetary transfers to Mars and Neptune are chosen to represent typical future solar sail mission scenarios. The results found with the new steering strategy are compared to the existing reference trajectories without attitude dynamics. The resulting control torques required to accomplish the missions are investigated, as they pose the primary requirements to a real on-board attitude control system",Analysis of interplanetary solar sail trajectories with attitude dynamics,,Univelt Inc,,,core
103651836,2012,"While three deployed applications of game theory for securi-ty have recently been reported, we as a community of agents and AI researchers remain in the early stages of these de-ployments; there is a continuing need to understand the core principles for innovative security applications of game theo-ry. Towards that end, this paper presents PROTECT, a game-theoretic system deployed by the United States Coast Guard (USCG) in the Port of Boston for scheduling their patrols. USCG has termed the deployment of PROTECT in Boston a success; PROTECT is currently being tested in the Port of New York, with the potential for nationwide deployment. PROTECT is premised on an attacker-defender Stackelberg game model and offers five key innovations. First, this sys-tem is a departure from the assumption of perfect adversary rationality noted in previous work, relying instead on a quan-tal response (QR) model of the adversary’s behavior — to the best of our knowledge, this is the first real-world deployment of the QR model. Second, to improve PROTECT’s efficien-cy, we generate a compact representation of the defender’s strategy space, exploiting equivalence and dominance. Third, we show how to practically model a real maritime patrolling problem as a Stackelberg game. Fourth, our experimental re-sults illustrate that PROTECT’s QR model more robustly han-dles real-world uncertainties than a perfect rationality model. Finally, in evaluating PROTECT, this paper for the first time provides real-world data: (i) comparison of human-generated vs PROTECT security schedules, and (ii) results from an Ad-versarial Perspective Team’s (human mock attackers) analy-sis",PROTECT: A deployed game theoretic system to protect the ports of the United States,,,,,core
147977176,2011-12-16T12:31:33,"This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion",Consideration on Robotic Giant-swing Motion Generated by Reinforcement Learning,,"Ieee Service Center, 445 Hoes Lane, Po Box 1331, Piscataway, Nj 08855-1331 Usa",,,core
229270105,2009-01-01T08:00:00,"Over the last few decades design researchers have put forward theories and proposed methodologies that increase the chance that a design team will reliably arrive at the optimal solution to a given design problem. Studies, however, bear out that theories and methodologies alone will not guarantee an optimal or even good design solution. Instead, a breadth of knowledge across multiple engineering domains and the time and tools to thoroughly evaluate the design space are as important as any prescriptive design method. This work presents a set of underlying engineering technologies to define, archive and reuse product design knowledge to provide a breadth of domain knowledge for designers and to leverage artificial intelligence approaches to thoroughly, if not exhaustively, search the design space. Specifically, a database schema and entry application for a prototype design repository of product design knowledge is formulated and implemented. A real-time, knowledge basedriven, function-based conceptual design algorithm known as the morphological search is formulated to extract information from the design repository and support a thorough exploration of the design space for solutions. Currently, the Design Engineering Lab\u27s prototype Design Repository contains design knowledge for over 125 products and has over 300 user accounts representing 17 different countries. With the foundational repository elements in place, artificial intelligence methods are employed to generate a natural language to formal component naming terms thesaurus as part of a novel form-initiated concept generation approach. The approach, known as Form Follows Form, automatically generates a functional model based upon an initial component solution seed to a design problem. With a functional model in hand, established automated concept generation algorithms are employed to return more complete and varied solutions following a thorough search of the design space --Abstract, page iv",Information archival and reuse: drawing conclusions from the past,https://core.ac.uk/download/229270105.pdf,Scholars\u27 Mine,,,core
226590974,2011-01-01T00:00:00,"The notion of agent more and more appears in different contexts of computer science, often with different meanings. The main acceptation is the AI (Artificial Intelligence) and Distributed AI one, where agents are essentially exploited as a technique to develop special-purpose systems exhibiting some kind of intelligent behavior. In this paper, we introduce a further perspective, shifting the focus from AI to computer programming and programming languages. In particular, we consider agents and related concepts as general- purpose abstractions useful for programming software systems in general, conceptually extending object-oriented program- ming with features that \u2013 we argue \u2013 are effective to tackle some main challenges of modern software development. Accordingly, the main contribution of the work is first the definition of a conceptual space framing the basic features that characterize the agent-oriented approach as a programming paradigm, then its validation in practice by using a platform called JaCa, with real-word programming examples",Agent-Oriented Computing: Agents as a Paradigm for Computer Programming and Software Development,,place:s.l,,,core
17301215,2010-10-13T00:00:00,"This thesis and its accompanying project are concerned with the use of digital technology in the representation of material culture. The thesis aims to find ways of using such technology that are appropriate to our present needs and to its potential. The computer is a technology which we understand, interact with and relate to through metaphor. I propose that many of the metaphors through which we understand it invoke the idea of an enclosed space. The use of such a trope might seem suitable when using computers for representing museum collections, or material culture in general, since it invokes the enclosed space of the museum. I examine how this idea of enclosure is manifested in computer developments such as virtual reality and artificial intelligence.   I also look at how these developments are congruent with perspectival modes of visual representation privileged in the modern era. I argue that such metaphors and forms of representation, whether manifested in visual arts, the museum, or computer applications are problematic, bound up as they are with modern western ideas of mastery and transcendence, which are presently being subjected to critiques from various quarters.

Throughout the modern era there have been forms of representation which have contested the dominant visual mode of modernity. These include the art of the Baroque in the seventeenth and eighteenth centuries and, in this century, the work of the Surrealists. In contrast to the rational, orthogonal space of modernity, both these deal with complex and fragmented representations of spaces and time. Such developments have been discussed as forms of representation appropriate to contemporary concerns about knowledge They also have a corollary in computing developments, such as multimedia and hypermedia,

Yet, I argue, those working in multimedia have in the main failed to exploit the potential of such developments to enable new ways of representing knowledge. I propose looking to both the Baroque and Surrealism to find possible models and strategies for use in multimedia in the representation of material culture. In relation to this I describe practical work done in conjunction with this thesis which uses these models as the basis of a piece of multimedia software for the representation of material culture",The computer as an irrational cabinet.,https://core.ac.uk/download/17301215.pdf,,,,core
44254124,2010-01-01T00:00:00,"Electronics is under development in this country in an organized and institutional way since the beginning of 30-ties of the previous century. It grew up from electrical engineering of weak currents and its first name used popularly was communications. It was time when television was born and the radio was maturing. Electronics is a branch of research and technology which deals with generation and processing of electrical and electromagnetic signals. A subject of telecommunications is signal transmission for a distance. Electronics and telecommunications (ET) includes or is combined with other branches like: microelectronics, radioelectronics, optoelectronics, photonics, acoustoelectronics, magnetronics, bioelectronics, energoelectronics, material engineering, semiconductor physics, automation and robotics, mechatronics and microsystems, informatics, teleinformatics, software engineering and other. Devices and functional systems of ET such as computers, data warehouses, cell phones, TV sets, Internet, GPS are build of electronic components and circuits. ET is a branch which belongs to hi-tech area, where the products gather a large load of knowledge of value overcoming frequently the price of work and material. ET has recently turned to an active participant of the processes of generation, storing, processing, transportation, distribution and usage of knowledge in the society. ET started to create artificial intelligence, co-creates intellectual property, searches for knowledge in big data sets, aids medicine, extends virtual/augmented reality, builds Internet of persons and things, strengthens security, protects natural environment, facilitates our life, aids our decisions, activates individuals, equalizes chances, provides convenient personal communications and access to data, starts building a penetrating ubiquitous infrastructure, ceases to be only a branch of technology, grows into the social space, touches culture, sociology, psychology and art. Such an important role of ET is combined with the existence in the society of an adequate infrastructure which recreates the full development cycle of high technology embracing: people, institutions, finances and logistics, in this also science, higher education, education, continuous training, dissemination and outreach, professional social environment, legal basis, political support and lobbying, innovation structures, applications, industry and economy. The digest of chosen development tendencies in ET was made here from the academic perspective, in a wider scale and on this background the national one, trying to situate this branch in the society, determine its changing role to build a new technical infrastructure of a society based on knowledge, a role of builder of many practical gadgets facilitating life, a role of a big future integrator of today’s single bricks into certain more useful unity. This digest does not have a character of a systematic analysis of ET. It is a kind of an arbitrary utterance of the authors inside their field of competence. The aim of this paper is to take an active part in the discussion of the academic community in this country on the development strategy of ET, choice of priorities for cyclically rebuilding economy, in competitive environments. The review paper was initiated by the Committee of Electronics and Telecommunications of Polish Academy of Sciences and was published in Polish as introductory chapter of a dedicated expertise, printed in a book format. This version makes the included opinions available for a wider community",Electronics and telecommunications in Poland - issues and perspectives,,Proc. SPIE,,,core
51877228,2011-01-01T00:00:00,"Methods for intelligent mobile robots control which are based on principles of hierarchical control systems will be reviewed in this article. Hierarchical intelligent mobile robots are new direction for development of robotics, which have wide application perspectives. Despite increasing progress in technologies, the main problem of autonomous mobile robots development is that, they are ineffective in their control. In each of the hierarchical control levels (movement in space, problems solving and signal processing sets) will define by specific management of objectives, goals and rules. Communication and management between hierarchies are implemented by higher level of hierarchy using obtained information about the environment and lover level of hierarchy. Studies have shown that artificial neural networks, fuzzy logic are widely used for the development of the hierarchical systems. The main focus of the work is on communications in hierarchy levels, since the robot must be controlled in real time",Hierarchiniai autonominių mobiliųjų robotų valdymo metodai,,'Kaunas University of Technology (KTU)',10.5755/j01.eee.110.4.298,,core
357393669,2011-01-01T00:00:00,"ABSTRACT Data-intensive applications are becoming important in many science and engineering fields, because of the high rates in which data are being generated and the numerous opportunities offered by the sheer amount of these data. Large-scale datasets, however, are challenging to process using many of the current machine learning algorithms due to their high time and space complexities. In this paper, we propose a novel approximation algorithm that enables kernel-based machine learning algorithms to efficiently process very largescale datasets. While important in many applications, current kernel-based algorithms suffer from a scalability problem as they require computing a kernel matrix which takes O(N 2 ) in time and space to compute and store. The proposed algorithm yields substantial reduction in computation and memory overhead required to compute the kernel matrix, and it does not significantly impact the accuracy of the results. In addition, the level of approximation can be controlled to tradeoff some accuracy of the results with the required computing resources. The algorithm is designed such that it is independent of the subsequently used kernelbased machine learning algorithm, and thus can be used with many of them. To illustrate the effect of the approximation algorithm, we developed a variant of the spectral clustering algorithm on top of it. Furthermore, we present the design of a MapReduce-based implementation of the proposed algorithm. We have implemented this design and run it on our own Hadoop cluster as well as on the Amazon Elastic MapReduce service. Experimental results on synthetic and real datasets demonstrate that significant time and memory savings can be achieved using our algorithm","Distributed Approximate Spectral Clustering for Large-Scale Datasets. Master’s thesis,",,,,,core
236621381,2011-01-01T08:00:00,"Sensor exploitation (SE) is the crucial step in surveillance applications such as airport security and search and rescue operations. It allows localization and identification of movement in urban settings and can significantly boost knowledge gathering, interpretation and action. Data mining techniques offer the promise of precise and accurate knowledge acquisition techniques in high-dimensional data domains (and diminishing the “curse of dimensionality” prevalent in such datasets), coupled by algorithmic design in feature extraction, discriminative ranking, feature fusion and supervised learning (classification). Consequently, data mining techniques and algorithms can be used to refine and process captured data and to detect, recognize, classify, and track objects with predictable high degrees of specificity and sensitivity.
Automatic object detection and tracking algorithms face several obstacles, such as large and incomplete datasets, ill-defined regions of interest (ROIs), variable scalability, lack of compactness, angular regions, partial occlusions, environmental variables, and unknown potential object classes, which work against their ability to achieve accurate real-time results. Methods must produce fast and accurate results by streamlining image processing, data compression and reduction, feature extraction, classification, and tracking algorithms. Data mining techniques can sufficiently address these challenges by implementing efficient and accurate dimensionality reduction with feature extraction to refine incomplete (ill-partitioning) data-space and addressing challenges related to object classification, intra-class variability, and inter-class dependencies.
A series of methods have been developed to combat many of the challenges for the purpose of creating a sensor exploitation and tracking framework for real time image sensor inputs. The framework has been broken down into a series of sub-routines, which work in both series and parallel to accomplish tasks such as image pre-processing, data reduction, segmentation, object detection, tracking, and classification. These methods can be implemented either independently or together to form a synergistic solution to object detection and tracking.
The main contributions to the SE field include novel feature extraction methods for highly discriminative object detection, classification, and tracking. Also, a new supervised classification scheme is presented for detecting objects in urban environments. This scheme incorporates both novel features and non-maximal suppression to reduce false alarms, which can be abundant in cluttered environments such as cities. Lastly, a performance evaluation of Graphical Processing Unit (GPU) implementations of the subtask algorithms is presented, which provides insight into speed-up gains throughout the SE framework to improve design for real time applications.
The overall framework provides a comprehensive SE system, which can be tailored for integration into a layered sensing scheme to provide the war fighter with automated assistance and support. As more sensor technology and integration continues to advance, this SE framework can provide faster and more accurate decision support for both intelligence and civilian applications",Data mining based learning algorithms for semi-supervised object identification and tracking,https://core.ac.uk/download/236621381.pdf,Louisiana Tech Digital Commons,,,core
12042838,2010-01-01T00:00:00,"The potential of small antenna array (2x2 square) for multipaths (MP) mitigation in GNSS systems is considered in this paper. This study focuses on two different approaches. In the first one, we use antenna array algorithms in order to have a spatial discrimination of the incoming paths. Due to the low directivity of the array, conventional beamforming turns out to be inefficient. Thus, high resolution beamformers have been tested. In the second one, we compare this solution with an approach which uses the spatial sampling in combination with time and frequency dimensions. To discriminate the different incoming signals on the three domains, Space Alternating Generalized Expectation Maximisation (SAGE)algorithm, which is a low-complexity generalization of the Maximum Likelihood maximisation (ML) algorithm, has been considered. Monte Carlo simulations clearly show the superiority of SAGE. Nevertheless, SAGE implementation requires a high memory and computation capacities, and real time SAGE processing is still not possible. In order to reduce these requirements, we investigated a new implementation based on both the correlation properties of the code and the thermal noise. With the same level of performance, the new implementation of SAGE can strongly reduce the size of the signal and consequently, the memory and computation requirements. Thus, this is a promising method to take the advantages of SAGE with reasonable hardware requirements",Multipath mitigation methods based on antenna array,,,,,core
71914662,2011-01-27T00:00:00,"L’objectif de cette recherche est de développer un réseau de neurones impulsionnels analogiques afin d’améliorer la performance d’un pacemaker biventriculaire (aussi appelé le CRT-P) de nouvelle génération. L’implémentation sur silicium utilise l’approche réseau de neurones analogiques qui nécessite le développement d’une solution technique satisfaisant à une contrainte de très basse consommation énergétique. Nous proposons une approche de un réseau de neurones impulsionnels analogiques pour optimiser la prédiction des délais cardiaques avec l’algorithme d’apprentissage Hebb et l’algorithme d’apprentissage par renforcement dans des modes de fonctionnement différents. L’amélioration des prévisions permet au CRT-P de fournir des battements cardiaques optimaux en temps réel. Nous décrirons le comportement et les qualités de notre algorithme au travers de simulations mathématiques et comportementales. Des simulations complètes et cohérentes du système basées sur des modèles simples du coeur

(rythme cardiaque constant puis rythme cardiaque variable) avec des bruits uniformes aléatoires sont illustrées avec succès pour la validation de la faisabilité du système.

Nous proposons aussi une méthodologie renforcée de la conception analogique et mixte. Les simulations de tous niveaux (de hauts et bas niveaux)peuvent être faites rapidement afin de vérifier des performances du système dans chaque phase de conception et ainsi fournir une plage des spécifications acceptables facilitant la synthèse analogique et mixte suivante.The objective of this research is to develop an analog spiking neural network so as to improve the performance of a bi-ventricular pacemaker (also called the CRT-P) of new generation. The implementation on silicon using the analog neural network approach requires the development of a

satisfactory technical solution to meet the constraint of very low energy consumption. We propose an analog spiking neural network approach to optimize the cardiac delay prediction with the Hebbian learning algorithm and the reinforcement learning algorithm in different functional modes.

The delay improvement allows the CRT-P to provide optimal heartbeat in real time. We describe the behavior and the qualities of our algorithm through mathematical and behavioral simulations. The complete and coherent system simulations based on the simple heart models (constant heart

rate and variable heart rate) with random uniform noise are shown successfully to validate the system feasibility.

We also propose an enhanced methodology of the analog and mixed signal design. The simulations of all levels (high and low levels) can be carried out quickly in order to verify the system performance in each design phase and also carry out the acceptable specification space for facilitating the following analog and mixed signal synthesis",Study and design of an analog neural processor of very low power consumption : appliced to the navigation of a new generation pacemaker,,Université de Strasbourg,,,core
29405426,2011-11-01T00:00:00,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,https://core.ac.uk/download/29405426.pdf,'Springer Science and Business Media LLC',10.1007/s12369-011-0113-z,,core
2251936,2012-01-31T00:00:00,"We apply kernel-based methods to solve the difficult reinforcement learning
problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in
keepaway are the high-dimensionality of the state space (rendering conventional
discretization-based function approximation like tilecoding infeasible), the
stochasticity due to noise and multiple learning agents needing to cooperate
(meaning that the exact dynamics of the environment are unknown) and real-time
learning (meaning that an efficient online implementation is required). We
employ the general framework of approximate policy iteration with
least-squares-based policy evaluation. As underlying function approximator we
consider the family of regularization networks with subset of regressors
approximation. The core of our proposed solution is an efficient recursive
implementation with automatic supervised selection of relevant basis functions.
Simulation results indicate that the behavior learned through our approach
clearly outperforms the best results obtained earlier with tilecoding by Stone
et al. (2005)",Learning RoboCup-Keepaway with Kernels,http://arxiv.org/abs/1201.6626,,,,core
21175230,2010-03-31,"Abstract: The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, video streams are deemed to be a dominant traffic type, and require quality of service (QoS) support. However, in the current IEEE 802.15.3 standard for MAC (media access control) of high-rate WPANs, the implementation details of some key issues such as scheduling and QoS provisioning have not been addressed. In this paper, we first propose a Markov decision process (MDP) model for optimal scheduling for video flows in high-rate WPANs. Using this model, we also propose a scheduler that incorporates compact state space representation, function approximation, and reinforcement learning (RL). Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT, EDD+SRPT, and PAP scheduling algorithms in terms of a lower decoding failure rate",1 An RL-based Scheduling Algorithm for Video Traffic in High-rate Wireless Personal Area Networks,,,,,core
289919041,2010,"How close a computer model comes to recreating real-world phenomena often depends on the value of its internal parameters, but investigating the outcome of the model for every point in parameter space is in practice an impossible task. Here an artificial neural network is used as a numerical predictor on two different system biology computer models. A semi-implicit solver was also implemented for one of these models in order to speed up simulations in stiff regions of parameter space. The performance of the neural networks were measured using the area under the receiver operating characteristic curve (AUC), and neural networks were used as numerical predictors for three different four-dimensional parameter regions. In the ﬁrst region a training data set of 500 points were used and an auc of 1.0 was achieved. In the second region a training data set of 1000 points were used and an auc of 0.97 was obtained. In the last region training data sets of 100, 250, 1000 and 3000 points were used and the auc of the neural networks was 0.86, 0.95, 0.97 and 0.97 respectively",Demarcating good solutions in system biology computer models using artiﬁcial neural networks,,Lunds universitet/Beräkningsbiologi och biologisk fysik,,,core
203510207,2012-12-31,"AbstractThe long tail can be applied not only in the economical field, but also the culture field. Firstly, the paper details on the long tail phenomenon among data, information and knowledge. Then it points out that there exists significant long tail phenomenon in learning style, learning resources and learning space. According to the elements to boom the Long Tail market, the paper proposes a view on social network learning. It can be regarded as learning supported by social software, learning in real social environment, and learning with the integration of the three kinds of nets (neural networks, Internet, social networks). Knowledge is categorized into core knowledge and growth knowledge, the former depends on constructing and the latter depends on connecting. As the long tail does not replace the 80/20 rule, this paper holds the notion that connectivism also cannot be a substitute of constructivism, and they should complement each othe",Discussion on Social Network Learning from the Long Tail ,,Published by Elsevier B.V.,10.1016/j.ieri.2012.06.122,,core
41152694,2012-11-21,"The Istituto Nazionale di Geofisica e Vulcanologia (INGV) is the Italian agency devoted to monitor in real time the seismicity on the Italian territory.  The seismicity in Italy is of course variable in time and space, being also very much dependant on local noise conditions. Specifically, monitoring seismicity in an alluvial basin like the Po one is a challenge, due to consistent site effects induced by soft alluvial deposits and bad coupling with the deep bedrock (Steidl et al., 1996). This problem was tackled by INGV first with the Cavola experiment (Bordoni et al., 2007), where a landslide was seismically characterized using a seismic array and also down-hole logging of P- and S-wave travel times at a borehole drilled within the array; later, with an ad hoc project in 2000-2001, with the first installation of a broad band seismic station nearby Ferrara in a borehole of 135 meters depth. Comparison of recordings with a surface seismic station indicated a noise reduction of 2 decades in power spectral density at frequencies larger than 1.0 Hz (Cocco et al., 2001). The instrumentation in Ferrara has been working for several months but after that the seismic station was discontinued due to lack of maintenance manpower.
The Centro di Ricerche Sismologiche (CRS, Seismological Research Center) of the Istituto Nazionale di Oceanografia e di Geofisica Sperimentale (OGS, Italian National Institute for Oceanography and Experimental Geophysics) in Udine (Italy) after the strong earthquake of magnitude M=6.4 occurred in 1976 in the Italian Friuli-Venezia Giulia region, started to operate the Northeastern Italy (NI) Seismic Network: it currently consists of 15 very sensitive broad band and 21 simpler short period seismic stations, all telemetered to and acquired in real time at the OGS-CRS data center in Udine (Fig. 1).
Real time data exchange agreements in place with other Italian, Slovenian, Austrian and Swiss seismological institutes lead to a total number of about 100 seismic stations acquired in real time, which makes the OGS the reference institute for seismic monitoring of Northeastern Italy. Since 2002 OGS-CRS is using the Antelope software suite on several workstations plus a SUN cluster as the main tool for collecting, analyzing, archiving and exchanging seismic data, initially in the framework of the EU Interreg IIIA project “Trans-national seismological networks in the South-Eastern Alps”. SeisComP is also used as a real time data exchange server tool (Bragato et al., 2011).
Among the various Italian institution with which OGS is cooperating for real time monitoring of local seismicity there is the Regione Veneto (Barnaba et al., 2012). The Southern part of the Veneto Region stands on the Po alluvial basin: earthquake localization and characterization is here again affected in this area by the presence of soft alluvial deposits. OGS ha already experience in running a local seismic network in difficult noise conditions making use of borehole installations (Priolo et al., 2012) in the case of the monitoring of a local storage site for the Italian national electricity company ENEL. Following the ML=5.9 earthquake that struck the Emilia region around Ferrara in Northern Italy on May 20, 2012 at 02:03:53 UTC, a cooperation of INGV, OGS, the Comune di Ferrara and the University of Ferrara lead to the reinstallation of the very broad band borehole seismic station in Ferrara. The aim of the OGS intervention was on one hand to extend its real time seismic monitoring capabilities toward South-East (Fig. 1), including Ferrara and its surroundings, and on the other hand to evaluate the seismic  response at the site.
As concerns the superficial geology of the area where the borehole seismic station  has been installed, the outcropping materials are represented by alluvial deposits of different environments, like channel and proximal levee, inter-fluvial, meander and swamps deposits. As  a consequence, the outcropping deposits are everywhere Holocene in age substantially loose or poorly compacted in the first meters-decameters and granulometrically could vary from clay to coarse sand.
Two preliminary reports prepared by the Italian Department of Civil Defense (Dipartimento Nazionale di Protezione Civile) in collaboration with other institutions describe the data  recorded by the national accelerometric network and complemented by additional data recorded by a number of temporary stations (Dolce et al., 2012a; Dolce et al., 2012b). These reports bear witness of strong ground motion values with an acceleration peak of about 0.9 g in the vertical component recorded during the ML=5.8 earthquake of May 29, 2012 by the Mirandola station, located at about 2 km from the epicentre. The analysis of the seismic noise recorded at some stations shows a quite pronounced peak of the horizontal-to-vertical spectral ratio (H/V) in the frequency range of 0.6 – 0.9 Hz common to all stations. Finally, strong evidence of liquefaction phenomena are reported at several sites (e.g.: S. Carlo, S. Agostino and Mirabello), most of which have been attributed to the occurrence of saturated sandy layer(s) at shallow depth deposited along an abandoned reach of the Reno River (Papathanassiou et al., 2012).
Details of the station configuration and installation will be outlined, with first results",Installation of a very broad band borehole seismic station in Ferrara (Emilia),https://core.ac.uk/download/pdf/41152694.pdf,OGS,,,core
60418869,2010-03-29T00:00:00,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.This work has been partially supported by the Fundación Séneca de la Región de Murcia through the research projects 08801/PI/08 and 08788/PI/08, and by the Spanish Government through project TIN2008-06893-C03",A multi-FPGA distributed embedded system for the emulation of Multi-Layer CNNs in real time video applications,https://core.ac.uk/download/60418869.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/CNNA.2010.5430245,,core
79814016,2012-10-29T00:00:00,"International audienceThe Artificial Intelligence entities, capable to communicate with their vicinity, such as PDAs, Smartphones, sensors, robots, software, middleware, etc. are increasingly introduced in our environment. The conventional topography of our everyday space introduces Distributed Artificial Intelligence, Ambient Intelligence and Ubiquitous Computing concepts. For example, when traveling to a new city, information about sights in the city, hotels, or even bus timetable are needed. Thanks to ubiquitous computing, citizens as well as tourists can communicate with their environment and get all these information in real time right to their terminals. In this paper, we present a large package of ubiquitous services that might be requested in a ubiquitous space (in this case, the ubiquitous space is the Grand Stade Lille Métropole) and we have designed a multi-agents architecture which will support these services and optimize their quality as response time and information cost.Les entités de l'Intelligence Artificielle, communiquantes avec leur entourage, telles que les PDAs, les téléphones, les capteurs, les robots, les logiciels, les middlewares, etc. sont de plus en plus présentes dans notre environnement. La nouvelle topographie de nos espaces quotidiens introduit les notions de l'Intelligence Artificielle Distribuée, l'Intelligence Ambiante et l'Informatique Ubiquitaire. Notre objectif est d'avoir un accès en temps réel, via un support mobile, à différents types d'information issus d'un environnement ubiquitaire, tels que les lieux touristiques et les horaires de bus. Dans ce papier, nous proposons une architecture efficiente d'un système ubiquitaire ciblé basé sur une approche multi-agent. Nous nous focalisons dans notre étude sur le Grand Stade Lille Métropole et les services qui peuvent y être demandés. L'architecture proposée va devoir supporter les services et optimiser leur qualité par rapport au temps de réponse et au coût de l'information",Architecture à base d'agents pour optimiser les services d'aide à la mobilité : vers une conception d'un espace ubiquitaire,,HAL CCSD,,,core
360547026,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake- holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished1. Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons, Chichester, England (2002). ISBN 0 47149691X

2. Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

3. Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 10–15. ACM Press (2006)

4. Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

5. Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

6. Zhang, Z., Thangarajah, J., Padgham, L.: Model based testing for agent systems. In: J. Filipe, B. Shishkov, M. Helfert, L. Maciaszek (eds.) Software and Data Technologies, Communications in Computer and Information Science, vol. 22, pp. 399–413. Springer, Berlin/Heidelberg (2009)

7. Ekinci, E.E., Tiryaki, A.M., Çetin, Ö., Dikenelli: Goal-oriented agent testing revisited. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 173–186. Springer, Berlin/Heidelberg (2009)

8. Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 199–212. Springer, Berlin/Heidelberg (2009)

9. Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 187–198. Springer, Berlin/Heidelberg (2009)

10. Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

11. Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

12. Erol, K., Hendler, J.A., Nau, D.S.: HTN planning: Complexity and expressivity. In: Proceedings of the 12th National Conference on Artificial Intelligence (AAAI), pp. 1123–1128. AAAI Press (1994)

13. de Silva, L., Padgham, L.: A comparison of BDI based real-time reasoning and HTN based planning. In: G. Webb, X. Yu (eds.) AI 2004: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3339, pp. 1167–1173. Springer, Berlin/Heidelberg (2004)

14. Erol, K., Hendler, J., Nau, D.: Complexity results for HTN planning. Annals of Mathematics and Artificial Intelligence 18(1), 69–93 (1996)

15. Paolucci, M., Shehory, O., Sycara, K.P., Kalp, D., Pannu, A.: A planning component for RETSINA agents. In: N.R. Jennings, Y. Lespérance (eds.) Intelligent Agents VI, Agent Theories, Architectures, and Languages (ATAL), 6th International Workshop, ATAL ’99, Orlando, Florida, USA, July 15-17, 1999, Proceedings, Lecture Notes in Computer Science, vol. 1757, pp. 147–161. Springer, Berlin/Heidelberg (2000)

16. Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

17. Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

18. d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, Lecture Notes in Artificial Intelligence, vol. 1365, pp. 155–176. Springer, Berlin/Heidelberg (1998)

19. Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

20. Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

21. Lee, J., Huber, M.J., Kenny, P.G., Durfee, E.H.: UM-PRS: An implementation of the procedural reasoning system for multirobot applications. In: Proceedings of the Conference on Intelligent Robotics in Field, Factory, Service, and Space (CIRFFSS’94), pp. 842–849 (1994)

22. Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

23. Morley, D., Myers, K.: The SPARK agent framework. In: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 714–721. IEEE Computer Society, Washington, DC, USA (2004)

24. Pokahr, A., Braubach, L., Lamersdorf, W.: Jadex: A BDI reasoning engine. In: R.H. Bordini, M. Dastani, J. Dix, A. El Fallah Seghrouchni (eds.) Multi-Agent Programming: Languages, Platforms and Applications, pp. 149–174. Springer (2005)

25. Bratman, M.E., Israel, D.J., Pollack, M.E.: Plans and resource-bounded practical reasoning. Computational Intelligence 4, 349–355 (1988)

26. Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), Lecture Notes in Artificial Intelligence, vol. 1038, pp. 42–55. Springer, Berlin/Heidelberg (1996)

27. Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

28. Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

29. Dastani, M.: 2APL: a practical agent programming language. Autonomous Agents and Multi-Agent Systems 16(3), 214–248 (2008)

30. Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemelä (eds.) Logic Programming, Lecture Notes in Computer Science, vol. 4670, pp. 302–316. Springer, Berlin/Heidelberg (2007)

31. Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http: //www.math.upenn.edu/∼wilf/gfology2.pdf

32. Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

33. Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 37–44. IFAAMAS (2008)

34. Dorigo, M., Stützle, T.: Ant Colony Optimization. MIT Press (2004). ISBN 0-262-04219-3

35. van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

36. Thangarajah, J., Winikoff, M., Padgham, L., Fischer, K.: Avoiding resource conflicts in intelligent agents. In: F. van Harmelen (ed.) Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 18–22. IOS Press (2002)

37. Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: Proceedings of the Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

38. Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: Future of Software Engineering 2007, pp. 120–136. IEEE Computer Society, Los Alamitos, CA (2007)

39. Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

40. Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

41. Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007)

42. Burch, J., Clarke, E., McMillan, K., Dill, D., Hwang, J.: Symbolic model checking: 1020 states and beyond. Information and Computation 98(2), 142–170 (1992)

43. Fix, L., Grumberg, O., Heyman, A., Heyman, T., Schuster, A.: Verifying very large industrial circuits using 100 processes and beyond. In: D. Peled, Y.K. Tsay (eds.) Automated Technology for Verification and Analysis, Lecture Notes in Computer Science, vol. 3707, pp. 11–25. Springer, Berlin/Heidelberg (2005",On the testability of BDI agent systems,https://core.ac.uk/download/360547026.pdf,'University of Otago Library',,,core
288062662,2012-01-01T08:00:00,"Many supervised learning problems are considered difficult to solve either because of the redundant features or because of the structural complexity of the generative function. Redundant features increase the learning noise and therefore decrease the prediction performance. Additionally, a number of problems in various applications such as bioinformatics or image processing, whose data are sampled in a high dimensional space, suffer the curse of dimensionality, and there are not enough observations to obtain good estimates. Therefore, it is necessary to reduce such features under consideration. Another issue of supervised learning is caused by the complexity of an unknown generative model. To obtain a low variance predictor, linear or other simple functions are normally suggested, but they usually result in high bias. Hence, a possible solution is to partition the feature space into multiple non-overlapping regions such that each region is simple enough to be classified easily.  In this dissertation, we proposed several novel techniques for restricting supervised learning problems with respect to either feature selection or feature space partition. Among different feature selection methods, 1-norm regularization is advocated by many researchers because it incorporates feature selection as part of the learning process. We give special focus here on ranking problems because very little work has been done for ranking using L1 penalty.  We present here a 1-norm support vector machine method to simultaneously find a linear ranking function and to perform feature subset selection in ranking problems. Additionally, because ranking is formulated as a classification task when pair-wise data are considered, it increases the computational complexity from linear to quadratic in terms of sample size. We also propose a convex hull reduction method to reduce this impact. The method was tested on one artificial data set and two benchmark real data sets, concrete compressive strength set and Abalone data set. Theoretically, by tuning the trade-off parameter between the 1-norm penalty and the empirical error, any desired size of feature subset could be achieved, but computing the whole solution path in terms of the trade-off parameter is extremely difficult. Therefore, using 1-norm regularization alone may not end up with a feature subset of small size. We propose a recursive feature selection method based on 1-norm regularization which can handle the multi-class setting effectively and efficiently. The selection is performed iteratively. In each iteration, a linear multi-class classifier is trained using 1-norm regularization, which leads to sparse weight vectors, i.e., many feature weights are exactly zero. Those zero-weight features are eliminated in the next iteration. The selection process has a fast rate of convergence. We tested our method on an earthworm microarray data set and the empirical results demonstrate that the selected features (genes) have very competitive discriminative power. Feature space partition separates a complex learning problem into multiple non-overlapping simple sub-problems. It is normally implemented in a hierarchical fashion. Different from decision tree, a leaf node of this hierarchical structure does not represent a single decision, but represents a region (sub-problem) that is solvable with respect to linear functions or other simple functions. In our work, we incorporate domain knowledge in the feature space partition process.  We consider domain information encoded by discrete or categorical attributes. A discrete or categorical attribute provides a natural partition of the problem domain, and hence divides the original problem into several non-overlapping sub-problems. In this sense, the domain information is useful if the partition simplifies the learning task. However it is not trivial to select the  discrete or categorical attribute that maximally simplify the learning task. A naive approach exhaustively searches all the possible restructured problems. It is computationally prohibitive when the number of discrete or categorical attributes is large. We describe a metric to rank attributes according to their potential to reduce the uncertainty of a classification task. It is quantified as a conditional entropy achieved using a set of optimal classifiers, each of which is built for a sub-problem defined by the attribute under consideration. To avoid high computational cost, we approximate the solution by the expected minimum conditional entropy with respect to random projections. This approach was tested on three artificial data sets, three cheminformatics data sets, and two leukemia gene expression data sets. Empirical results demonstrate that our method is capable of selecting a proper discrete or categorical attribute to simplify the problem, i.e., the performance of the classifier built for the restructured problem always beats that of the original problem. Restricting supervised learning is always about building simple learning functions using a limited number of features. Top Selected Pair (TSP) method builds simple classifiers based on very few (for example, two) features with simple arithmetic calculation. However, traditional TSP method only deals with static data. In this dissertation, we propose classification methods for time series data that only depend on a few pairs of features. Based on the different comparison strategies, we developed the following approaches: TSP based on average, TSP based on trend, and TSP based on trend and absolute difference amount. In addition, inspired by the idea of using two features, we propose a time series classification method based on few feature pairs using dynamic time warping and nearest neighbor",Restricting Supervised Learning: Feature Selection and Feature Space Partition,https://core.ac.uk/download/288062662.pdf,eGrove,,,core
196689363,2011-11,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,,Springer,10.1007/s12369-011-0113-z,,core
81126890,2012-08-31,"AbstractA closed-form approximate maximum likelihood (AML) algorithm for estimating the position and velocity of a moving source is proposed by utilizing the time difference of arrival (TDOA) and frequency difference of arrival (FDOA) measurements of a signal received at a number of receivers. The maximum likelihood (ML) technique is a powerful tool to solve this problem. But a direct approach that uses the ML estimator to solve the localization problem is exhaustive search in the solution space, and it is very computationally expensive, and prohibits real-time processing. On the basis of ML function, a closed-form approximate solution to the ML equations can be obtained, which can allow real-time implementation as well as global convergence. Simulation results show that the proposed estimator achieves better performance than the two-step weighted least squares (WLS) approach, which makes it possible to attain the Cramér-Rao lower bound (CRLB) at a sufficiently high noise level before the threshold effect occurs",Approximate Maximum Likelihood Algorithm for Moving Source Localization Using TDOA and FDOA Measurements ,https://core.ac.uk/download/pdf/81126890.pdf,Chinese Journal of Aeronautics. Production and hosting by Elsevier B.V.,10.1016/S1000-9361(11)60423-8,,core
56688141,2011-01-01T08:00:00,"Trucking industry, the business of transporting products via trucks, is vital to the health of our economy for the sheer number of people it employs, the value of product it hauls, and the diversity of dispatching models it uses. One of these dispatching models is the spot dispatching. This form of dispatching went through a recent transformation as a result of the industry deregulation in the 80s and the emergence of the internet. The deregulation allowed easier establishment of new trucking companies and their access to the entire market; the internet allowed for spot freight to be posted on the on-line hosting sites where shippers, brokers and truckers can post their service.  This change, however, brings with it challenges and opportunities which is the focus of this dissertation.
In this dissertation, we give a broad introduction of the freight spot-market; we identify the challenges and the opportunities. Spot-market dispatching problem is formulated as a dynamic assignment problem, implemented as a Markov Decision Process (MDP), which has its objective as maximizing the operation profit at the end of the dispatching planning horizon. A freight spot-market loads generation platform is created to mimic the dynamics of trucks and loads in such markets. Platform allows generation of data representing different market settings. Approximate dynamic programming methods are proposed to solve the dispatching problem. To address the curse of MDP state-space dimensionality for real-world settings, Neural Networks were used to approximate the value function. We benchmark our methods with local and myopic policies typically used be dispatchers in the business. Load hosting sites offer information about available loads and trucks across the market. We also explore the use of such information into dispatching policies and study its effect on the overall performance",Trucking: novel spot-market dispatching models,,DigitalCommons@WayneState,,,core
230195551,2011-05-02T00:00:00,"Bir&ccedil;ok klasik g&ouml;r&uuml;nt&uuml; onarım tekniği bulanıklık işlevinin bilindiği varsayımı altında &ccedil;alışır. Ancak, ger&ccedil;ek hayat problemlerinde sadece g&ouml;zlem verisi elde edilebilmekte bozucu sistemler hakkında yeterli bilgi sağlanamamaktadır. Bu y&uuml;zden g&ouml;r&uuml;nt&uuml; onarımının ilk adımı bozucunun &ouml;ğrenilmesi (tanınması) işlemidir. Ge&ccedil;mişte, g&ouml;r&uuml;nt&uuml; ve bulanıklık parametrelerinin &ouml;ğrenilmesi Enb&uuml;y&uuml;k Olabilirlik (EO) problemi olarak ele alınmış ve Beklenti Enb&uuml;y&uuml;kleme (BE) yordamı ile &ccedil;&ouml;z&uuml;lm&uuml;şt&uuml;r. &Ouml;zellikle BE yordamının E adımında kapalı yapıda bir &ccedil;&ouml;z&uuml;m bulunması bu yordamı daha cazip bir hale getirmektedir. G&ouml;r&uuml;nt&uuml; ve bulanıklık parametrelerinin t&uuml;m g&ouml;r&uuml;nt&uuml; verisi kullanılarak &ouml;ğrenilmesi ge&ccedil;mişte &ccedil;alışılmış olmakla birlikte, parametrelerin yinelemeli BE&rsquo;ye dayalı &ouml;ğrenilmesi daha &ouml;nce &ccedil;alışılmamıştır. Yinelemeli teknikler dinamik işlem yetenekleri sayesinde t&uuml;m veri &uuml;zerinde işlem yapan y&ouml;ntemlere nispetle &ccedil;ok daha az bellek ihtiyacı duyarlar. Daha az bellek ihtiyacı ise &ouml;zellikle g&ouml;r&uuml;nt&uuml; işleme alanında &ccedil;ok &ouml;nemlidir. Bu &ccedil;alışmada yeni bir eşzamanlı yinelemeli parametre &ouml;ğrenme ve g&ouml;r&uuml;nt&uuml; onarım y&ouml;ntemi sunulmuştur. Dinamik Bayes&ccedil;i Ağ (DBA) yapısında yeni bir &ccedil;&ouml;z&uuml;m &ouml;nerilmiştir. Sunulan y&ouml;ntem EO parametre tanıma ve durum kestirimi i&ccedil;in en iyi Kalman yumuşatma ifadelerini i&ccedil;erir. Kalman yumuşatma ifadelerinin yoğun hesaplama gerektirmesi sebebi ile Kalman s&uuml;zge&ccedil; yaklaşıklığı kullanılmıştır. Aynı zamanda, onarılmış g&ouml;r&uuml;nt&uuml; eş zamanlı olarak bu s&uuml;zge&ccedil; &ccedil;ıkışından elde edilmektedir. G&ouml;r&uuml;nt&uuml; ve bulanıklık parametrelerinin BE &ouml;ğrenme problemi kapalı yapıda &ccedil;&ouml;z&uuml;mlenmesi başarılmıştır.&nbsp; Y&ouml;ntemin başarımı ger&ccedil;ek g&ouml;r&uuml;nt&uuml;ler &uuml;zerinde yapılan benzetim ve denemeler ile verilmiştir. &nbsp;Anahtar Kelimeler: Beklenti enb&uuml;y&uuml;kleme, bulanıklık ve g&ouml;r&uuml;nt&uuml; tanıma, yinelemeli işleme, kalman yumuşatma ve s&uuml;zge&ccedil;leme.The image restoration problem can be defined as the general problem of estimating the ideal image from its blurred and noisy version. Many classical image restoration techniques have been reported under the assumption that the blur operation is exactly known. In real life applications, the corruption mechanism of any system is not known because only observed data is available, so it is necessary to handle uncertain events and observations. The image restoration problem is in general ill-posed; a small perturbation on the given data produces large deviations in the solution. The direct inversion of the blur transfer function usually has a large magnitude at high frequencies, therefore excessive amplification of noise results at those frequencies. Clearly, this is not an acceptable solution for noisy images. To overcome the noise sensitivity problem of the inverse filter, some filters have been developed based on the least-squares structure. The Wiener filter is based on batch processing which is usually implemented in the frequency domain. The Kalman filter is based on recursive processing which is usually implemented in the spatial domain. Both solutions only work when blur, image and noise parameters are known. The first step for image restoration is the identification of degradation. Consequently, modeling uncertain relationships among many kinds of variables and learning (identification) such variables are important topics. The blur and image parameter identification problem was formerly formulated as a constrained Maximum Likelihood (ML) estimation procedure which was based on optimizing the probability density function (pdf) of the observed image with respect to the unknown parameters. But, the direct optimization of the likelihood function is not feasible, because of its highly nonlinear character. The Expectation Maximization (EM) algorithm is a very popular and widely used algorithm for the computation of ML estimates. There are two steps in EM algorithm, as E (Expectation) and M (Maximization). The EM algorithm finds the conditional expectation of the log-likelihood of complete data given the observed incomplete data. In the E-step, the conditional expectation of the ""hidden variables"" is calculated.  In the M-step, this expectation is maximized with respect to the parameters. The advantage of the EM method is such that it avoids operating directly on the nonlinear likelihood function. The EM algorithm becomes more attractive if its maximization step can be formulated analytically. Even though batch processing of the EM based blur identification and restoration problem needs large memory size, recursive techniques allow dynamic processing with modest storage requirements. Although the EM learning was applied to learning of unknown image and blur parameters based on batch image processing before, recursive EM learning of unknown image and blur parameters has not been studied as much as necessary. Many time series models, including the Hidden Markov Models (HMM) and Kalman Filter Models (KFM) used in filtering and control applications, can be viewed as examples of Dynamic Bayesian Network DBNs. Since, a Bayesian Network is a graphical way to represent a particular factorization of joint distribution; we propose that state space image model can be represented as a DBN. In this work, we introduce a new simultaneous recursive parameter learning and image restoration method based on the ML parameter identification and state estimation for images. We present a new formulation which is given in a Dynamic Bayesian Network (DBN) framework. We focus on the problem of learning the parameters of a Bayesian network. This technique incorporates optimal Kalman smoothing equations for ML parameter identification and state estimation. The use of Kalman filtering instead of Kalman smoothing is employed because of the computationally extensive processing of smoothing. In addition, a restored image is obtained simultaneously as the output of the Kalman filter. We manage to solve the EM learning problem for image and blur parameters in closed form. Although our proposed method processes huge data, because of the recursive structure it does not need large size storage. Performance evaluation of the method is given based on experiments carried out upon real images.  Keywords: Expectation-Maximization, Blur and Image Identification, Recursive Processing,  Kalman Smoothing and Filtering",Recursive learning of image parameters and restoration of images using EM based learning algorithm,https://core.ac.uk/download/230195551.pdf,İTÜDERGİSİ/d,,,core
9990415,2011-01-01T00:00:00,"Model-based sensor fault detection, isolation and accommodation (SFDIA) is a direction of development in particular with UAVs where sensor redundancy may not be an option due to weight, cost and space implications. SFDIA via neural networks (NNs) have been proposed over the years due to their nonlinear structures and online learning capabilities. The majority of papers tend to consider single sensor faults. While useful, this assumption can limit application to real systems where sensor faults can occur simultaneously or consecutively. In this paper we consider the latter scenario, where it is assumed that a 1 s time gap is present between consecutive faults. Furthermore few applications have considered fixed-wing UAVs where full autonomy is most needed. In this paper an EMRAN RBF NN is chosen for modelling purposes due to its ability to adapt well to nonlinear environments while maintaining high computational speeds. A nonlinear UAV model is used for demonstration, where decoupled longitudinal motion is considered. System and measurement noise is also included in the UAV model as wind gust disturbances on the angle of attack and sensor noise, respectively. The UAV is assumed to operate at an initial trimmed condition of speed, 32 m/s and altitude, 1000 m. After 30 separate SFDIA tests implemented on a 1.6 GHz Pentium processor, the NN-SFDIA scheme detected all but 2 faults and the NN processing time was 97% lower than the flight data sampling time",Survey and application of sensor fault detection and isolation schemes,,'Elsevier BV',10.1016/j.conengprac.2011.03.002,,core
2075018,2010-06-22T00:00:00,"We present a framework for simulating signal propagation in geometric
networks (i.e. networks that can be mapped to geometric graphs in some space)
and for developing algorithms that estimate (i.e. map) the state and functional
topology of complex dynamic geometric net- works. Within the framework we
define the key features typically present in such networks and of particular
relevance to biological cellular neural networks: Dynamics, signaling,
observation, and control. The framework is particularly well-suited for
estimating functional connectivity in cellular neural networks from
experimentally observable data, and has been implemented using graphics
processing unit (GPU) high performance computing. Computationally, the
framework can simulate cellular network signaling close to or faster than real
time. We further propose a standard test set of networks to measure performance
and compare different mapping algorithms.Comment: Revised following initial peer review. Current version 28 pages and 7
  figures. A slightly modified version has been accepted to Neural Computation
  and is now in pres","A framework for simulating and estimating the state and functional
  topology of complex dynamic geometric networks",http://arxiv.org/abs/0908.3934,,,,core
211464860,2010-01-01T00:00:00,"The thesis proposes pattern recognition techniques based on machine learning methods and automated data mining; the application domain is systems that evolve over time. It describes algorithms and resolution strategies that combine and exploit features from established methodologies developed in the framework of computational and statistical learning. The attributes of the proposed identification and recognition techniques are: 1) low time-space complexity, 2) incremental and fast learning, 3) real-time response, 4) application independence, 5) simplicity, 6) automated operation, 7) scalability, and 8) distributed computing. After the application of these techniques, the produced solutions are not proved to be the optimal ones; nevertheless they are sufficient and functional. The empirical verification is implemented in real-world problems of different complexity. These problems can be represented by time evolving systems and emerge from the fields of environmental parameters forecasting, voice/speech and image recognition, and video-based content extraction such as human pose recovery and human action recognition. It is known that time evolution occurs in a variety of problems encountered in physics, chemistry, biology, economy and more; thus, the proposed recognition schemes present a wide range of applications. Additionally, the main trait of incremental learning algorithms is the integration of new information into the knowledge base without the need for the model’s retraining. This attribute is applicable to domains where the data flow is continuous such as autonomous robots learning, information retrieval on the Web, human-machine interaction, medical monitoring devices, error diagnosis, security systems, predictive models for natural phenomena and more. The difficulty in defining the analytical form of functions that describe the aforementioned problems, the high dimensionality of data sets, the intervals of intense discontinuity and the presence of high noise make impossible the application of the differential calculus and classical linear or nonlinear analysis. This research work demonstrates experimentally that the adoption of techniques which combine the regression with continuous clustering and classification, allows the handling of non- well defined problems, especially in those cases where the conditions that lead to the convergence to the optimal solution cannot be verified in real world.Η παρούσα διατριβή προτείνει τεχνικές αναγνώρισης προτύπων/μορφωμάτων που βασίζονται σε μεθόδους μηχανικής μάθησης και αυτόματης εξόρυξης δεδομένων, με αντικείμενο εφαρμογής τα συστήματα που εξελίσσονται στο χρόνο. Περιγράφονται αλγόριθμοι και στρατηγικές επίλυσης που συνδυάζουν και αξιοποιούν χαρακτηριστικά από καθιερωμένες μεθοδολογίες που αναπτύχθηκαν στα πλαίσια της υπολογιστικής και στατιστικής μάθησης. Οι ιδιότητες με τις οποίες εφοδιάζονται οι προτεινόμενες τεχνικές αναγνώρισης είναι: 1) χαμηλή χωρο-χρονική πολυπλοκότητα, 2) σταδιακή και ταχεία μάθηση, 3) απόκριση σε πραγματικό χρόνο, 4) ανεξαρτησία από το συγκεκριμένο πεδίο εφαρμογής τους, 5) απλότητα στη χρήση, 6) αυτοματοποιημένη λειτουργία, 7) επεκτασιμότητα, και 8) δυνατότητα κατανεμημένης επεξεργασίας. Οι λύσεις που προκύπτουν μετά την εφαρμογή των εν λόγω τεχνικών δε βεβαιώνεται ότι είναι οι βέλτιστες, ωστόσο είναι επαρκείς και λειτουργικές. Η εμπειρική επαλήθευση υλοποιείται σε διαφορετικής πολυπλοκότητας προβλήματα του πραγματικού κόσμου, τα οποία δύναται ν' αναπαρασταθούν από χρονικώς εξελισσόμενα συστήματα. Τα προβλήματα που αντιμετωπίστηκαν προέρχονται από το χώρο της πρόβλεψης περιβαλλοντικών παραμέτρων, από τους χώρους αναγνώρισης φωνής/ομιλίας και εικόνας, καθώς και από το πεδίο αναγνώρισης περιεχομένου μέσω οπτικών εγγραφών (video), όπως είναι η ανάκτηση του ανθρώπινου σώματος και η αναγνώριση ανθρώπινης δραστηριότητας. Όπως είναι γνωστό, η χρονική εξέλιξη συναντάται σε μια πληθώρα προβλημάτων που εμφανίζονται στη φυσική, στη χημεία, στη βιολογία, στην οικονομία κα., με αποτέλεσμα οι διατάξεις αναγνώρισης που προτείνονται να παρουσιάζουν ένα ευρύ πεδίο εφαρμογών. Επιπρόσθετα, το κύριο χαρακτηριστικό των αλγορίθμων προοδευτικής μάθησης και αναγνώρισης είναι η ενσωμάτωση καινούργιας πληροφορίας στην υπάρχουσα γνωσιακή δεξαμενή επεξεργασμένων δεδομένων όποτε αυτή είναι διαθέσιμη, αποφεύγοντας έτσι το σκόπελο της επανεκπαίδευσης. Αυτή η ιδιότητα βρίσκει εφαρμογή σε προβλήματα όπου υπάρχει συνεχής ροή πληροφορίας, όπως στην εκμάθηση των αυτόνομων ρομπότ, στην ανάκτηση πληροφοριών στον Παγκόσμιο Ιστό, στην αλληλεπίδραση ανθρώπου-μηχανής, σε συσκευές ιατρικής παρακολούθησης, στη διάγνωση σφαλμάτων, σε συστήματα ασφαλείας, σε προγνωστικά μοντέλα φυσικών φαινομένων κα. Η δυσκολία ορισμού της αναλυτικής μορφής των συναρτήσεων που περιγράφουν τα εν λόγω προβλήματα, η υψηλή διαστατικότητα των συνόλων δεδομένων, τα διαστήματα έντονης ασυνέχειας, καθώς και η ύπαρξη υψηλού θορύβου καθιστούν ανέφικτη την εφαρμογή του διαφορικού λογισμού και της κλασικής γραμμικής ή μη-γραμμικής ανάλυσης. Στην παρούσα ερευνητική εργασία αποδεικνύεται πειραματικά ότι η υιοθέτηση τεχνικών που συνδυάζουν την παλινδρόμηση με τη συνεχή ομαδοποίηση και ταξινόμηση, επιτρέπει το χειρισμό μη-καλώς ορισμένων προβλημάτων, ειδικά εκείνων των περιπτώσεων όπου οι συνθήκες που διαμορφώνουν τη σύγκλιση στη βέλτιστη λύση δεν μπορούν να επαληθευτούν στο φυσικό κόσμο","Incremental machine learning methods in time-dependent problems: pattern, time-series and system recognition applications in real-time decision-making",,'National Documentation Centre (EKT)',10.12681/eadd/24658,,core
21130703,2010-01-31,"ABSTRACT-The focus of this research is to enable discovery of control knowledge in a complex, real-time environment. We present a metJwdology for automated discovery of rules in a dynamic engineering domain. TIu! addition of a discovery system to an expert system in a reactive environment offers an improvement in performance by enabling tlu! expert system to learn new knowledge and improve its existing knowledge base. In addition. discovery techniques can aid operators confronted by UIlfamiliar and complex situations. and assist autonomous machines to find problem-solving rules for unanticipated situations. TIu! goal of this system is unsupervised learning of a rule base by obse&quot;,ing how well the performance system controls the environmelll. This performance system explores and experiments with the environment when its present rules are inadequate. Through this experimentation. the learning component of the system predicts the proper setting of a set of control variables. To illuslTale the methodology. we describe an implementation called the HUBBLE Discovery System (HDS). built on a NASA software testbed. This system performs real-time operation. diagnosis. and repair of tile Electrical Power Subsystem (EPS) of a software simulalor of the HUBBLE Space Telescope satellite. The HDS successfully discovered control rules with both complete and incomplete domain theories. The performance of the HDS learning/problem solving system is compared to a control case of a blackboard system using hand-crafted rules",PQ2- 31&quot; Rule Acquisition For Dynamic Engineering Domains,,,,,core
21633426,2011-10-29,"Dynamic data visualization in real-time involves rapid simulations of computerized models. Although computing power has increased exponentially in the last few decades, detailed simulations using software introduce time-delay that defeat the notion of realtime data visualization of the generated results. Approximate techniques can pave way for efficient generation of continuous flow of data, thereby enabling real-time visualization. In this project, statistical (linear regression model) and neural (backpropagation) methodologies are used to learn and approximate post-processed CFD data to ensure rapid data generation for dynamic visualization. An indoor space is simulated using CFD software under various design conditions. Post-processed CFD data is used as training data to learn the AI system developed. The learnt system is simulated to generate rapid CFD data under various conditions to allow real-time visualization through AR technologies, in real-space. While, under normal conditions, CFD simulations cannot generate results that could be visualized in real-time, the integration of AI techniques enables efficient CFD data reduction for real-time visualization",CFD Data Reduction for Indoor Environments: Use of Statistical (Linear Regression) and Neural (Back-propagation) models,,,,,core
21135256,2010-02-18,"Abstract. We consider a novel “online semi-supervised learning ” setting where (mostly unlabeled) data arrives sequentially in large volume, and it is impractical to store it all before learning. We propose an online manifold regularization algorithm. It differs from standard online learning in that it learns even when the input point is unlabeled. Our algorithm is based on convex programming in kernel space with stochastic gradient descent, and inherits the theoretical guarantees of standard online algorithms. However, naïve implementation of our algorithm does not scale well. This paper focuses on efficient, practical approximations; we discuss two sparse approximations using buffering and online random projection trees. Experiments show our algorithm achieves risk and generalization accuracy comparable to standard batch manifold regularization, while each step runs quickly. Our online semi-supervised learning setting is an interesting direction for further theoretical development, paving the way for semi-supervised learning to work on real-world lifelong learning tasks. ",Online Manifold Regularization: A New Learning Setting and Empirical Study,,,,,core
160027784,2010-01-01T00:00:00,"Semi-Supervised Learning (SSL) is a machine learning research area aiming the development of techniques which are able to take advantage from both labeled and unlabeled samples. Additionally, most of the times where SSL techniques can be deployed, only a small portion of samples in the data set is labeled. To deal with such situations in a straightforward fashion, in this paper we introduce a semi-supervised learning approach based on neuronal synchrony in a network of coupled integrate-and-fire neurons. For that, we represent the input data set as a graph and model each of its nodes by an integrate-and-fire neuron. Thereafter, we propagate the class labels from the seed samples to unlabeled samples through the graph by means of the emerging synchronization dynamics. Experimentations on synthetic and real data show that the introduced technique achieves good classification results regardless the feature space distribution or geometrical shape.Fed Univ Sao Paulo Unifesp, Dept Sci & Technol DCT, Sao Jose Dos Campos, SP, BrazilFed Univ Sao Paulo Unifesp, Dept Sci & Technol DCT, Sao Jose Dos Campos, SP, BrazilWeb of Scienc",Label Propagation Through Neuronal Synchrony,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/IJCNN.2010.5596809,"[{'title': None, 'identifiers': ['1098-7576', 'issn:1098-7576']}]",core
13939734,2011-07-31T00:00:00,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic architectural environments to be implemented and tested in the last decade in virtual and physical prototypes. These prototypes are incorporating sensing-actuating mechanisms that enable interaction with their users and surroundings in real-time. While these prototypes obviously point towards a paradigm shift from inanimate towards animate architecture, they do not operate at building but at building component scale and do not address socio-economical or environmental aspects that affect architecture and society at large. This paper, on the one hand, critically discusses robotic prototypes built in the last decade at Delft University of Technology, on the other hand, it proposes a framework for future research envisioning robotic environments, as resizable, able to spatially expand or contract as well as move or be moved as needed. Such reconfigurable environments aim to validate the assumption that robotics incorporated in architecture improve efficiency of use due to multiple use of built space in condensed timeframes, while at the same time they advance technology for distributed autonomous robotic systems exhibiting collective behavior as well as test their application to sustainable architecture.ArchitectureArchitectur",Robotic environments,,"IAARC, International Association for Automation and Robotics in Construction",10.22260/isarc2011/0160,,core
360546180,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake-holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished[1] Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons (Chichester, England) (2002). ISBN 0 47149691X

[2] Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

[3] Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 10–15. ACM Press (2006)

[4] Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-Architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

[5] Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

[6] Zhang, Z., Thangarajah, J., Padgham, L.: Automated unit testing for agent systems. In: Second International Working Conference on Evaluation of Novel Approaches to Software Engineering (ENASE), pp. 10–18 (2007)

[7] Ekinci, E.E., Tiryaki, A.M., Çetin, Ö.: Goal-oriented agent testing revisited. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 85–96 (2008)

[8] Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 133–144 (2008)

[9] Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 165–176 (2008)

[10] Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

[11] Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

[12] Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

[13] Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

[14] d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, pp. 155–176. Springer-Verlag, LNAI 1365 (1998)

[15] Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

[16] Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

[17] Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

[18] Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), pp. 42–55. Springer Verlag, LNAI 1038 (1996)

[19] Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

[20] Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

[21] Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemel ¨ a (eds.) Proceedings of the 23rd International Conference on Logic Programming (ICLP), pp. 302–316. Springer, LNCS 4670 (2007)

[22] Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http://www.math.upenn.edu/∼wilf/gfology2.pdf

[23] Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

[24] Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), industry track., pp. 37–44. IFAAMAS (2008)

[25] Parunak, H.V.D.: “go to the ant”: Engineering principles from natural multi-agent systems. Annals of Operations Research 75, 69–101 (1997). (Special Issue on Artificial Intelligence and Management Science)

[26] van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

[27] Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: The Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

[28] Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: International Conference on Software Engineering: Future of Software Engineering, pp. 120–136 (2007)

[29] Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

[30] Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

[31] Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007",On the testability of BDI agent systems,https://core.ac.uk/download/360546180.pdf,'University of Otago Library',,,core
9660956,2012-12-01T00:00:00,"Autonomous Learning Systems is the result of over a decade of focused research and studies in this emerging area which spans a number of well-known and well-established disciplines that include machine learning, system identification, data mining, fuzzy logic, neural networks, neuro-fuzzy systems, control theory and pattern recognition. The evolution of these systems has been both industry-driven with an increasing demand from sectors such as defence and security, aerospace and advanced process industries, bio-medicine and intelligent transportation, as well as research-driven – there is a strong trend of innovation of all of the above well-established research disciplines that is linked to their on-line and real-time application; their adaptability and flexibility. Providing an introduction to the key technologies, detailed technical explanations of the methodology, and an illustration of the practical relevance of the approach with a wide range of applications, this book addresses the challenges of autonomous learning systems with a systematic approach that lays the foundations for a fast growing area of research that will underpin a range of technological applications vital to both industry and society. Key features: • Presents the subject systematically from explaining the fundamentals to illustrating the proposed approach with numerous applications. • Covers a wide range of applications in fields including unmanned vehicles/robotics, oil refineries, chemical industry, evolving user behaviour and activity recognition. • Reviews traditional fields including clustering, classification, control, fault detection and anomaly detection, filtering and estimation through the prism of evolving and autonomously learning mechanisms • Accompanied by a website hosting additional material, including the software toolbox and lecture notes Autonomous Learning Systems provides a ‘one-stop shop’ on the subject for academics, students, researchers and practicing engineers. It is also a valuable reference for Government agencies and software developers",Autonomous Learning Systems:From Data to Knowledge in Real Time,,John Willey and Sons,,,core
400276635,2010-02-07T00:00:00,"International audienceThe magnetic diagnostics subsystem of the LISA Technology Package (LTP) on board the LISA PathFinder (LPF) spacecraft includes a set of four tri-axial fluxgate magnetometers, intended to measure with high precision the magnetic field at their respective positions. However, their readouts do not provide a direct measurement of the magnetic field at the positions of the test masses, and hence an interpolation method must be designed and implemented to obtain the values of the magnetic field at these positions. However, such interpolation process faces serious difficulties. Indeed, the size of the interpolation region is excessive for a linear interpolation to be reliable while, on the other hand, the number of magnetometer channels does not provide sufficient data to go beyond the linear approximation. We describe an alternative method to address this issue, by means of neural network algorithms. The key point in this approach is the ability of neural networks to learn from suitable training data representing the behaviour of the magnetic field. Despite the relatively large distance between the test masses and the magnetometers, and the insufficient number of data channels, we find that our artificial neural network algorithm is able to reduce the estimation errors of the field and gradient down to levels below 10%, a quite satisfactory result. Learning efficiency can be best improved by making use of data obtained in on-ground measurements prior to mission launch in all relevant satellite locations and in real operation conditions. Reliable information on that appears to be essential for a meaningful assessment of magnetic noise in the LTP",Theory and modelling of the magnetic field measurement in LISA PathFinder,,'IOP Publishing',10.1088/0264-9381/27/3/035005,,core
19609981,2012-03-01T00:00:00,"We present a new approach to the problem of optimal control of solar sails for low-thrust trajectory optimization. The objective was to find the required control torque magnitudes in order to steer a solar sail in interplanetary space. A new steering strategy, controlling the solar sail with generic torques applied about the spacecraft body axes, is integrated into the existing low-thrust trajectory optimization software InTrance. This software combines artificial neural networks and evolutionary algorithms to find steering strategies close to the global optimum without an initial guess. Furthermore, we implement a three rotational degree-of-freedom rigid-body attitude dynamics model to represent the solar sail in space. Two interplanetary transfers to Mars and Neptune are chosen to represent typical future solar sail mission scenarios. The results found with the new steering strategy are compared to the existing reference trajectories without attitude dynamics. The resulting control torques required to accomplish the missions are investigated, as they pose the primary requirements to a real on-board attitude control system",Analysis of interplanetary solar sail trajectories with attitude dynamics,https://core.ac.uk/download/19609981.pdf,Univelt Inc,,,core
144060024,2011-04-19T00:00:00,"Desde o final da década de 1990 existe um interesse crescente na aplicação de técnicas de planejamento automático em IA para resolver problemas reais de engenharia. Além das características dos problemas acadêmicos, tais como a necessidade de raciocinar sobre as ações, problemas reais requerem elicitação, engenharia e gerenciamento detalhado do conhecimento do domínio. Para tais aplicações reais, um processo de design sistemático é necessário onde as ferramentas de Engenharia do Conhecimento e de Requisitons têm um papel fundamental. Esforços acadêmicos recentes na área da Engenharia do Conhecimento em planejamento automático vêm desenvolvido ferramentas e técnicas de apoio ao processo de design de modelos do conhecimento. Porém, dada a natural incompletude do conhecimento, experiência prática em aplicações reais, como por exemplo exploração do espaço, tem mostrado que, mesmo com um processo disciplinado de design, requisitos de pontos de vista diferente (por exemplo, especialistas, usuários e patrocinadores) ainda surgem após a análise, geração e execução de planos. A tese central deste texto é que uma fase de análise de pós-design para o desenvolvimento de aplicações de planejamento em IA resulta em modelos do conhecimento mais ricos e, conseqüentemente, aumenta a qualidade dos planos gerados e a performance dos planejadores automáticos. Neste texto, nós investigamos como os conhecimentos e requisitos ocultos podem ser adquiridos e reutilizados durante a fase de análise de plans (posterior ao design do modelo) e como estes conhecimentos afetam o desempenho do processo de planejamento automático. O texto descreve um framework de post-design chamado postDAM que combina (1) uma ferramenta de engenharia de conhecimento para a aquisição de requisitos e avaliação do plano, (2) um ambiente de prototipagem virtual para a análise e simulação de planos, (3) um sistema de banco de dados para armazenamento de avaliações de planos, e (4) um sistema de raciocínio ontológico para o re-uso e descoberta de conhecimento sobre o domínio. Com o framework postDAM demonstramos que a análise de pós-design auxilia a descoberta de requisitos ocultos e orienta o ciclo de refinamento do modelo. Este trabalho apresenta três estudos de caso com domínios conhecidos na literatura e oito planejadores do estado da arte. Nossos resultados demonstram que melhorias significativas na qualidade do plano e um aumento na velocidade dos planejadores de até três ordens de grandeza pode ser alcançada através de um processo disciplinado e cuidados de pós-design. Nós demonstramos também que rationales provenientes dos usuários capturados durante as avaliações de planos podem ser úteis e reutilizáveis em novas avaliações de plano e em novos projetos. Nós argumentamos que esse processo de pós-design é fundamental para a implantação da tecnologia de planejamento automático em aplicações do mundo real. Até onde sabemos, este é o primeiro trabalho que investiga a análise de pós-design em aplicações de planejamento automático da IA.Since the end of the 1990s there has been an increasing interest in the application of AI planning techniques to solve real-life problems. In addition to characteristics of academic problems, such as the need to reason about actions, real-life problems require detailed knowledge elicitation, engineering, and management. A systematic design process in which Knowledge and Requirements Engineering techniques and tools play a fundamental role is necessary in such applications. Research on Knowledge Engineering for planning and scheduling has created tools and techniques to support the design process of planning domain models. However, given the natural incompleteness of the knowledge, practical experience in real applications such as space exploration has shown that, even with a disciplined process of design, requirements from different viewpoints (e.g. stakeholders, experts, users) still emerge after plan generation, analysis and execution. The central thesis of this dissertation is that an post-design analysis phase in the development of AI planning applications leads to richer knowledge models and, consequently, to high-performance and high-quality plans. In this dissertation, we investigate how hidden knowledge and requirements can be acquired and re-used during a plan analysis phase that follows model design and how they affect planning performance. We describe a post-design framework called postDAM that combines (1) a knowledge engineering tool for requirements acquisition and plan evaluation, (2) a virtual prototyping environment for the analysis and simulation of plans, (3) a database system for storing plan evaluations, and (4) an ontological reasoning system for knowledge re-use and discovery. Our framework demonstrates that post-design analysis supports the discovery of missing requirements and guides the model refinement cycle. We present three case studies using benchmark domains and eight state-of-the-art planners. Our results demonstrate that significant improvements in plan quality and an increase in planning speed of up to three orders of magnitude can be achieved through a careful post-design process. We also demonstrate that rationales captured during plan evaluations from users can be useful and reusable in further plan evaluations and in new application designs. We argue that such a post-design process is critical for deployment of planning technology in real-world applications. To our knowledge, this is the first work that investigate post-design analysis for AI planning applications",Post-design analysis for AI planning applications.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.3.2011.tde-11042011-074633,,core
236535155,2011,"This folder contains two copies of a typewritten report: Kusko, Bruce H. ""The Development of Particle Induced X-Ray Emission for the Study and Analysis of Museum Objects."" Progress Report to J. Ligot, Director, LRMF. Only one copy has been digitized and is presented here.The items in this folder are part of the Thomas A. Cahill Papers--Crocker Historical and Archaeological Project, 1981-2009. They are from Series 1: Thomas A. Cahill Research Papers, 1981-1994. This series consists of various research papers and published articles based upon Dr. Cahill's research using Particle Induced X-ray Emission (PIXE) techniques in analyzing inks and papers.PROGRESS REPORT:
The Development of
Particle Induced X-ray Emission
for the
Study and Analysis of Museum Objects
Submitted by: Bruce H. Kusko
Fulbright Research Scholar
AGLAE
Laboratoire de Recherche
des Musees de France
to: M. Ligot
Directeur, LRMF
28 February, 1989
RESUME
Le Louvre est le premier musee au monde a posseder un
acceIerateur de particules pour l'etude et l'analyse des oeuvres
d'art et d'archeoLoqi.e. Le Laboratoire de Recherche des Musees de
France s'est donc equipe d'un outil t res puissant qui offrira aux
scientifiques de musee la palette des techniques d'analyse par
faisceaux d'ions acceleres qui comprend PIXE, PIGME, NRA, RBS et plus
tard datation C14.
Le rapport d'avancement consigne les travaux que j'ai accomplis
entre le 15 septembre et le 28 f'evr i.er 1989. Pendant cette peri ode
nous avons commence par monter un systeme PIXE d'analyse des oeuvres
d'art des collections de musee. PIXE est une technique multi-elementai
re, sensible, bon marche, rapide et ce qui est le plus
important dans le cas d'espece non destructive. Nous avons ainsi
obtenu des resultats preliminaires sur des echantillons de verre, des
etaIons qeoLoqi.ques , des pigments de peinture et des objets en or.
D'une certaine mani.ere, ces experiences ont ete plut6t un
apprentissage pour moi et l'equipe AGLAE. Nous nous sommes
familiarises avec l'accelerateur, la chambre d'analyse, le detecteur
de rayons X, l'eIectron.ique d'acquisition. Les premiers resul.t ats
sent; t res encourageants, il reste pourtant beaucoup de travail a
fournir pour que le systeme PIXE d'AGLAE soit operationnel en
routine.
Nous n'avons, de plus, qu'un temps limite de faisceau pour le
PIXE, puisque la priorite est donnee aux tests -verifier que
l'accelerateur est bien conforme aux specifications du constructeur.
L'accelerateur presente quelques problemes avec la source d'ions avec
comme consequence un courant de faisceau Leqeremerrt instable. De
plus, nous avons utilise une chambre provisoire. La nouvelle chambre
definitive a ete construite a Strasbourg et est en cours
d'installation. Il n'y a pas a l'heure actuelle un moyen sur pour
mesurer le courant de particules dans le cas d'echanti Ll.oris epai.s,
c'est pourquoi les resultats presentes ici ne s~nt que relatifs. Un
hacheur de faisceau (un composant pour mesurer le courant en
prelevant peri.odq.uiement; une partie du faisceau) est en cours de
construction a Jussieu-Universite Paris VII et sera monte au Louvre
en avril.
Nous avons porte nos efforts particulierement sur 'quatre
etalons de verre, qui represent erit les d.i f ferent s types de verre
etud.ies par les archeo Loques et les historiens d 'art. Ces etal.ons
sont bien connus et contiennent 27 elements chimiques entre le sodium
et le plomb dans des quarit i.tes variables comprises entre 100ppm et
35%. On a obtenus des resu l.tats avec une erreur relative comprise
entre 10 et 20% ,avec parfois des desaccords d'un facteur trois.
Les premiers echant i l.Lons que nous avons ree.l Lemerit analyses
sont des pigments de pe inture broyes provenant de la boutique ""A la
momie"", une maison de commerce en exercice pendant les 18 et 1gemes
siecles. Nous avons alors prouve que PIXE pouvait etre utilise pour
determiner les elements majeurs, mineurs et traces rent rant dans la
composition de ces pigments. De subtiles differences ont ete decelees
entre des pigments de meme denomination, et au contraire des pigments
avec des noms differents ont la meme composition chimique.
Le systeme de faisceau extrait a ete teste sur plusieurs
objets, y compris sur un tableau-faux primitif italien- , un cadre
dore du 15eme siecle et une petite statue en or antique. Pour
l'analyse de la plupart des objets de musee nous serons amene s a
utiliser ce systeme a l'air. 11 est donc crucial que ce systeme soit
parfaitement regle.
On a egalement commence a etudier l'atmosphere a l'interieur du
Louvre afin de determiner si des poussieres presentes dans les
galeries s~nt nocives pour les oeuvres d'art qui y sont conservees.
L'analyse des filtres aerosols pourra etre effectuee par AGLAE avec
seulement des modifications mineures sur le porte-cible.
Les resultats presentes ici ne s~nt pas entierement
satisfaisants. 11 est tout a fait rassurant que tous les equipements
et composants fonctionnent bien pour acquerir les spectres. Mais je
pense qu'il faut encore reduire les erreurs experimentales en
ameliorant le traitement des spectres(materiel et logiciel) pour que
le Louvre soit dote d'un systeme PIXE sur et precis.
EXECUTIVE SUMMARY
The Louvre Museum is the first museum in the world to have its own
particle accelerator for the study and analysis of works of art and
archaeology. The Laboratoire de Recherche des Musees de France has
thus taken a bold leap into the future with AGLAE, a powerful ""high-tech""
approach to the conservation of our cultural heritage.
This accelerator will enable the museum scientists to use a variety
of ion-beam techniques on works of art and archaeology, including
PIXE, PIGE, NRA, RBS and C-14 dating.
o
This progress report describes the work I accomplished between 15
September 1988 and 28 February 1989. During this time we have begun
to set up a PIXE system for the analysis of works of art in the
museum's collection. PIXE is multi-elemental, sensitive, inexpensive,
rapid, and most importantly, non-destructive. We have so far
acquired preliminary data on samples of glass, geological standards,
painting pigments, and gold artifacts. In a sense, these experiments
have been more of a learning experience for me and the AGLAE team. We
are becoming familiar with the accelerator, the target chamber, the
x-ray detector, the fast-pulse electronics, and the x-ray spectrum
reduction code. Although we have made a good start, much work still
needs to be done to make the AGLAE PIXE system ""state-of-the-art"".
We have had limited access to beam time for PIXE. Of course the
highest priority has been to ensure that the accelerator meets the
manufacturers specifications. The accelerator has had problems with
the ion source; as a result the beam current has not been very
stable. The target chamber we are using is a temporary one. A new one
has been built in Strasbourg and will be installed in March. There is
presently no reliable way of measuring the incident beam current for
thick samples, thus all the results presented here are relative. A
beam-chopper (a device for measuring the beam current) is being built
at Jussieu and will brought to the Louvre in April.
We have concentrated our efforts on four glass standards, which were
designed to duplicate the types of glass studied by archaeologists
and historians. These standards are well characterized and contain 27
elements between Na and Pb in quantities from 100 ppm to 35%. We have
been able to obtain results that are generally within 10 - 20% of the
given values, however, some occasionally there is a discrepancy of a
factor of three.
The firt ""real"" samples we have analyzed were pure pigments from the
boutique ""A la Momie"", a house of commerce in Paris during the 18th
and 19th centuries. We have seen how PIXE can be used to
quantitatively determine the major, minor, and trace elemental
composition of these paint pigments. Subtle differences were detected
between similar pigments, and pigments with different labels were
shown to have the same chemical composition.
The beam-in-air system has been tested on several samples, including
a false italian primitive, a gilded frame from the 15th century, and
a small gold statue from antiquity. For the analysis of most museum
objects we will be using this extracted beam system. It is thus vital
that we get it working well.
We are about start running air sampling equipment inside the Louvre
Museum in order to determine if any harmful pollutants are present in
the galleries that hold precious works of art. Analysis of the
aerosol filters can be done at AGLAE with only minor modifications of
the target holder.
I must say that I do not feel very confident about the numbers
presented here. I am glad we have all the equipment working to the
point that we can get results, but I have doubts that all the
hardware and software is working properly. We must now work to find
and reduce the experimental uncertainties, in order for the Louvre to
have a reliable and accurate PIXE system.
Progress Report, Fulbright Grant, 15 September 1988 - 28 February 1989
Bruce H. Kusko
AGLAE - LRMF
I. INTRODUCTION
During the last four years the Louvre Museum has been undergoing
a major renovation. Not only is the main entrance going to be a giant
glass pyramid, but three stories under the pyramid a team of
scientists will be bombarding precious works of art with a particle
accelerator. Analyse au Grand Louvre par Accelerateur Electrostatique
(AGLAE) is a part of the Laboratoire de Recherche des Musees de France
(LRMF), which has been given more space and equipment in a new
underground laboratory. It is only recently that methods of analysis
using high energy ion-beams (protons, alphas, 15N, etc) have been
applied to works of art and archaeology, and usually by physicists
working in nuclear laboratories in their spare time. The Louvre Museum
is thus the first museum to have an accelerator to be used exclusively
for the study and analysis of works of art and archaeology. The LRMF
has taken a bold leap into the future with this powerful ""high-tech""
approach to the conservation of our cultural heritage.
II. PIXE at AGLAE
My expertise is with particle induced x-ray emission (PIXE) and
I have been concerned with developing a PIXE system at AGLAE for the
study and analysis of works of art. We have had several opportunities
to do some PIXE analyses under vacuum, including pigment samples,
glass standards, geological standards, and gold samples. Although the
spectra were acquired under conditions that were not ideal, we were
able to perform tests of the target chamber, x-ray detector and
electronics, and the PIXE spectrum reduction code. All results
presented here are therefore considered preliminary and subject to
change.
a. Accelerator
The accelerator, a 2.0 Mev tandem pelletron, was purchased from
National Electrostatic Corporation in Middleton Wisconsin. It is
capable of accelerating protons from 0.3 to 4.0 Mev, alpha particles
from 0.3 to 6.0 Mev, and 15N ions to 8.0 Mev. In addition it can
accelerate deuterons and 3He ions. Guaranteed beam current for protons
is 5 pa through a 1 mm2 collimator. It was installed in the winter and
spring of 1988, and the first beam was realized in June 1988. Tests
and practice with the machine have been taking place ever since then,
and are expected to continue through the end of March.
B. Target area
The target chamber we are using is a temporary one. The target
ladder can hold up to six samples and is moved manually. The sample
surface is perpendicular to the axis of the incident beam. X-rays are
detected at a backward angle of 135 degrees. The detector is
collimated, and subtends a solid angle of 0.5 sr. A new target chamber
has been fabricated in Strasbourg and will be delivered to the Louvre
on 3 March. This new chamber is very versatile and will be used for
PIXE, PIGE, RBS, and NRA.
A beam-chopper for measuring the incident beam current has been
built at Jussieu. It is presently undergoing tests and will be brought
to the Louvre at the end of March. Presently the beam current is
determined by measuring the charge acquired by the target, target
holder, and target chamber, which are electrically isolated from the
rest of the accelerator. Since this method is not reliable, I have
designed and will be building a device to determine the beam current
by measuring the protons backscattered from a thin mylar foil placed
in front of the target. This thin foil monitoring technique has the
advantages of eliminating charge buildup on insulating samples
analyzed under vacuum, and it can be used to monitor the beam current
when analyzing samples in air with an extracted beam.
C. Electronics
The experimental setup for the x-ray electronics during PIXE
analyse is shown in figure 1. The detector is an EG&G Ortec 7900
2
Si(Li), with 30 mm2 area, 8.0 pm Be window, and a FWHM resolution of
147 ev at 5.895 kev. The high voltage bias is supplied by an Ortec 459
power supply. An Ortec 972 spectroscopy amplifier and an Ortec 444
biased amplifier are used together for pulse-processing and dead-time
corrections. A Seiko EGG 7800 multi-channel analyzer with a Seiko 1820
ADC interface is used to collect the spectra. An Enertec 7143 linear
ratemeter is used to monitor the x-ray count rate. The charge induced
by the beam current is measured by a Brookhaven Instruments
Corporation 1000a current integrator, and stored in an Ortec 996
counter and timer.
In order to obtain a resolution of 147 ev on a real sample it is
necessary to keep the amplifier close to the detector (less than 3 m)
and to acquire the data with the vacuum pumps off. Mechanical
vibration from the pumps adds almost 15 ev to the resolution. (We are
working to better isolate the pumps from the target chamber.)
D. PIXAN reduction code
Since my experience with computers has been with DEC and Apple
computers, I have had to become familiar with SUN (UNIX) and IBM (DOS)
computers. PIXE spectra are analyzed on a SUN MS 3/260 workstation.
The computer program PIXAN is used to reduce an x-ray spectrum
to its elemental composition, and it does this in two parts. The first
part of the program calculates the areas of the characteristic peaks.
The background is subtracted and then the peaks are fitted to a
modified gaussian form. The area of the characteristic peaks are
directly proportional to the number of x-rays coming from the
characteristic elements. The second part determines the theoretical x-ray
yield given the composition of the sample and the energy of the
proton beam. It is then necessary to combine the results of the two
parts to get the elemental composition in parts per million (ppm). The
program is designed to be used for both thin and thick targets.
I have had to spend most of my time geting PIXAN to run
correctly. It is an excellent program, but it evolved over a long
period of time at the Australian Atomic Energy Commission. Thus it is
3
suited to the types of samples they analyze in Australia and the
experimental conditions in their laboratory. The results are
critically dependent on the x-ray detector parameters (crystal
diameter, sensitive depth, silicon dead-layer, and the gold electrode
layer), any external detector filters, the x-ray electronics, and even
the target composition (which determines the self-absorption/self-enhancement
effects). The most important step, and perhaps the most
difficult one, is a proper subtraction of the background continuum.
With PIXAN the background can be modelled either by a polynomial (up
to 5th order), or by an iterative method that removes the
characteristic peaks and progressively reduces the spectrum to the
background continuum. We have found that small changes in the
parameters used to model the background leads to large changes in the
final results. We have found that the iterative method works better
than the polynomial method for our samples, and have achieved results
on glass standards that are generally within 20% of the given values
for the elements Na to Pb (see section 4).
The exact experimental uncertainties are hard to determine. The
precision when measuring an element well above the limit of detection
and isolated from any interfering peaks is better than 5%. Elements
found near the limit of detection have greater uncertainties. Elements
with low energy x-rays (less than 2 kev) and high energy x-rays
(greater than 30 kev) have larger uncertainties since the detector
efficiency is quite low and poorly known in these regions. The case of
interfering peaks must be considered individually. For example, the
uncertainty in the measurement of sulfur (Ka = 2.307 kev) depends on
the amount of lead (Ma = 2.346 kev) in the sample. The overall
accuracy of a PIXE measurement depends on the uncertainties in the
following factors: the number of x-rays in a characteristic peak; the
number of incident protons; the detector efficiency; and the
absorption/enhancement corrections. In general, uncertainties of
around 10% - 20% can be expected with PIXE.
Most of the results to date have been determined without seeing
graphically the background or least-squares fit to the spectrum, a
most unfortunate circumstance. Thomas Calligaro has recently written a
program that displays the x-ray spectrum, the background, and the fit
4
of the data, an indispensible step in proper spectrum analysis. Good
spectrum analysis, even with the best computer programs, still
requires experience. One must know what to look for in the background
subtraction, the gaussian fit, the calculation of the sum and escape
peaks, the relative peak intensities, the unfolding of overlapping
peaks, and so forth. One must be careful and not simply accept the
results given by the computer program.
The data files included with PIXAN lacked some vital information
to make it work correctly for the samples we have at the Louvre, and
needed to be supplemented. First of all they did not include any M-line
x-ray data, which are necessary for quantitative results on the
elements Na through Cl (1-3 kev). We have therefore added M-line data
for Pb, Hg, Pt, and Au into the PIXAN data files. (The elements whose
M-line data is important for us to know immediately.) Secondly, the
data file of relative peak heights is appropriate only for 2.5 Mev
protons. The relative peak heights are important to know since PIXAN
uses them to untangle the overlap of peaks. Although the Ka/KB ratio
is independent of energy, the LalLi ratio, (where i is anyone of the
many other L-line x-rays), and the Ma/Mi ratios vary unsystematically
as a function of incident proton energy. Therefore it will be
necessary for us to modify the data tables for energies lower and
higher than 2.5 Mev in order to obtain the most accurate results. (We
will use the tabulated theoretical x-ray cross sections of D.D. Cohen
and M. Harrington, Atomic Data and Nuclear Data Tables 33, 1985, 255-
343.) Thirdly, L- and M-shell sum (pile-up) peaks are not included in
the peak search and are therefore not removed from the x-ray spectra.
The justification for not including the L- and M-shell sum peaks was
to make it easier for PIXAN to determine accurate amounts of Ti
(Ka=4.508 kev) and Ba (La=4.467 kev) when both are present in the
sample. However, L- and M-line pileup peaks are a serious problem when
a sample contains major or minor amounts of platinum, gold, mercury,
lead, tin, or barium.
III. RESULTS
We have acquired 52 PIXE spectra to date: 12 on glass standards,
36 on paint pigments, 3 on ceramic standards, and 10 on gold samples.
5
A. Glass
We have concentrated our efforts on four colored glass samples.
These samples have been well characterized, and they contain 27
elements between Na and Pb in quantities of 100 ppm to 35%. The
elements between Na and K (X-rays between 1 and 3 Kev) are difficult
to determine yet important in archaeometric studies of glass,
pigments, ceramics, and geological samples. In addition these glass
samples are ideal PIXE targets, being completely homogeneous, flat,
and small enough to fit into a vacuum chamber. We felt it was
necessary to get good results on these standards before we started
analyzing unknown targets.
The glass samples were prepared in 1964 by R. H. Brill and A. A.
Erickson of the Corning Glass Works in Corning New York. The four
samples were designed to duplicate the types of glass studied by
archaeologists and art historians. Two samples (known as Brill A and
Brill B) are soda-lime-silica glasses, which are similar to ancient
Egyptian, Mesopotamian, Ionan, Byzantine, and Islamic glasses. One
sample (Brill C) is a glass with high-lead and high-barium levels,
which is similar to glasses from Eastern Asia. The fourth sample
(Brill D) is a potash-lime-silica glass which is similar to certain
medieval glasses and some glasses of the 17th to 19th centuries. Minor
and trace elements were also introduced at levels that are comparable
to those actually found in ancient glasses. The method of fabrication
of these samples is given in R. H. Brill, A Chemical-Analytical Round-robin
on Four Synthetic Glasses, Proceedings of the IX International
Congress on Glass, Versailles 27 September, 1971.
The glass samples are several millimeters long and are embedded
in amber. They were analyzed at 1.0 and 2.5 Mev, with the intention of
obtaining the light elements (Na to Cal at 1.0 Mev, when there is no
detector filter, and the heavy elements (K to Pb) at 2.5 Mev, while
using a detector filter which allows the use of higher beam currents
and better (lower) sensitivity. The spectrum for Brill C at 2.5 Mev is
shown in figure 2 and the best results to date for the glass samples
are given in table 1. The element K was chosen to be an ""internal
6
standard"", so all the results are relative and normalized to the K
concentration. A comparison between the experimental resu","Folder 39: Kusko, B.H. The Development of Particle Induced X-Ray Emission for the Study and Analysis of Museum Objects, 1989",,"Saint Louis University Libraries Special Collections, Archives & Manuscripts",,,core
23794219,2010,"Abstract — Semi-Supervised Learning (SSL) is a machine learning research area aiming at the development of techniques which are able to take advantage from both labeled and unlabeled samples. Additionally, most of the times where SSL techniques can be deployed, only a small portion of samples in the data set is labeled. To deal with such situations in a straightforward fashion, in this paper we introduce a semisupervised learning approach based on neuronal synchrony in a network of coupled integrate-and-fire neurons. For that, we represent the input data set as a graph and model each of its nodes by an integrate-and-fire neuron. Thereafter, we propagate the class labels from the seed samples to unlabeled samples through the graph by means of the emerging synchronization dynamics. Experiments on synthetic and real data show that the introduced technique achieves good classification results regardless the feature space distribution or geometrical shape. I",Label propagation through neuronal synchrony,,,10.1109/ijcnn.2010.5596809,,core
25859589,2011-08-01T00:00:00Z,"This paper demonstrates a new alternative way in estimating seismically thin-bed (below-tuning) thickness. Initial thickness is built by band-pass filtering the amplitude display of a zero-phase seismic. The filter removes the non minimum and or non maximum and left the maximum and or the minimum of seismic amplitude. The unresolved below-tuning thickness is then corrected by zero-INTENS-difference (z-i-d) attribute. INTENS is integrated energy spectra, an attribute which can be derived from spectral analysis. z-i-d attribute is zero difference of INTENS between the seismic and its synthetic. The method generates INTENS difference profile by subtracting seismic INTENS and its synthetic INTENS iteratively. The iteration is controlled by dipole space shifting from distance to closer or vice versa. The true thickness is derived by locating z-i-d which laid in INTENS different profile.  It has found that, for free noise true seismic and perfect-wavelet (a wavelet which only approximately similar with wavelet which constructing the true seismic) synthetic seismic, in INTENS different profile, the z-i-d location always corresponds to true dipole space or thickness. The method could resolve all thickness of a wedge-modeled seismic with three different dominant frequencies.  When the synthetic seismic is constructed with imperfect wavelet, slightly different analysis is needed to locate z-i-d attribute and the result is not as perfect as when perfect wavelet constructing synthetic seismic. A quiet similar result is got when the method is implemented for noisy wedge-modeled seismic. Bad thickness estimation is resulted for 20% noise seismic. The method algorithm is extended for similar dipole polarity model and multilayer model to bring the method to real seismic data nearer. The extension is done by estimating thickness of every layer of a stacked-wedge-modeled seismic. The algorithm then generalized for estimating layers thickness with several thickness combinations. The method was able to delineate shallow channel of Stratton Field by providing good pseudo-acoustic-impedance (pseudo AI) map",Combination of minimum-maximum (m-m) attribute and zero-INTENS-difference (m-i-d) attribute for estimating below-tuning layer thickness,,Institut Teknologi Bandung,,"[{'title': None, 'identifiers': ['issn:1978-3051', '1978-3051']}]",core
21761609,2012-04-02,"Abstract- Future science-driven landing missions, conceived to collect in-situ data on regions of planetary bodies that have the highest potential to yield important scientific discoveries, will require a higher degree of autonomy. The latter includes the ability of the spacecraft to autonomously select the landing site using real-time data acquired during the descent phase. This paper presents the development of an Evolutionary Fuzzy Cognitive Map (E-FCM) model that implements an artificial intelligence system capable of selecting a landing site with the highest potential for scientific discoveries constrained by the requirement of soft landing on a region with safe terrain. The proposed E-FCM evolves its internal states and interconnections as function of the external data collected during the descent, therefore improving the decision process as more accurate information is available. The E-FCM is constructed using knowledge accumulated by experts and it is tested on scenarios that simulate the decision-making process during the descent toward the Hyndla Regio on Venus. The E-FCM is shown to quickly reach conclusions that are consistent with what a planetary expert would decide if the scientist were presented, in real-time, with the same available information. The proposed methodology is fast and efficient and may be suitable for on-board spacecraft implementation and real-time decision-making during the course of any robotic exploration of the Solar System",Autonomous Real-Time Site Selection for Venus and Titan Landing using Evolutionary Fuzzy Cognitive Maps,,,,,core
88287197,2011-10-29T00:00:00,"Part 1: Decision Support Systems, Intelligent Systems and Artificial Intelligence ApplicationsInternational audienceData collected by measuring farmland environmental indicators are vital sources of agricultural information. Wireless sensor networks (WSNs) have been employed to acquire stable and real time farmland environment data. WSNs, recognized as one of the latest development trends, recently attracted widespread attention and application due to their relative economy, stability, and sophistication. In this study, based upon a large amount of WSNs-obtained data, Microsoft Visual Studio 2005 and ESRI ArcGIS Engine 9.3, amongst others, were utilized to develop a farmland test data processing system, resolve information storage and utilization problems and conduct system applications. The data processing software consisted of four modules: data receiving conversion, database maintenance management, data browsing analysis and the generation and application of data spatialization outputs. Specifically, the data receiving conversion module was mainly responsible for converting raw data acquired by WSNs into standard database outputs, including the automatic reception of measured value, error correction and alerting the observer to abnormal data. The Database maintenance management module’s primary functions were the generation and maintenance of metadata generated from stored data, authentic data enquiry, display and analysis. The generation and application of data spatialization products module was principally for the spatial expansion application of measured data, including space-time interpolation and conversion. The system underwent pilot scale testing and improvement at the same time as undertaking the processing of real data collected by WSNs deployed throughout the Hebi test zone. The observed results revealed that the system was able to complete real-time conversion and management of field measured data. In addition, it has several advantages, such as, excellent stability, perfect functionality and a convenient human - machine interface",Development and Application of a Farmland Test Data Processing System Designed for Wireless Sensor Network Applications,,'Springer Science and Business Media LLC',10.1007/978-3-642-27281-3_22,,core
293706985,2012-08-22T00:00:00,"In this thesis, advanced techniques for antenna array processing are addressed. The problem of autocalibration is considered and a novel method for a two-dimensional array is developed. Moreover, practicable methods for high-resolution direction-of-arrival (DOA) estimation and detection in automotive radar are proposed. A precise model of the array response is required to maintain the performance of DOA estimation. When the sensor environment is time-varying, this can only be achieved with autocalibration. The fundamental problem of autocalibration of an unknown phase response for uniform rectangular arrays is considered. For the case with a single source, a simple and robust least squares algorithm for joint two-dimensional DOA estimation and phase calibration is developed. An identification problem is determined and a suitable constraint is proposed. Simulation results show that the performance of the proposed estimator is close to the approximate CRB for both DOA estimation and phase calibration. The proposed algorithm for phase autocalibration is extended for the case with multiple sources. Simulation results demonstrate that the proposed algorithm enhances the resolution performance in the presence of phase errors. In automotive applications, modern driver assistance systems such as adaptive cruise control (ACC) or collision avoidance require an accurate description of the environment of a vehicle. For target localization in terms of range, relative velocity and DOA, a pulsed radar system with an array of receive antennas is considered. After pulse compression and Doppler processing, one obtains processing cells according to range and relative velocity, each represented by a single snapshot. In most cases, multiple targets can be distinguished by their range and/or relative velocity, so that each processing cell only contains a single target. However, there are situations, in which several targets are superposed in a processing cell. In the mentioned applications, this can occur in the presence of horizontal multipath with a close guardrail, which results in a ghost target. If the propagation paths cannot be resolved by conventional methods, this results in a false localization of the observed vehicle and high-resolution DOA estimation becomes necessary. The potential two-target model in the difficult case with a single snapshot is considered. An optimal generalized likelihood ratio test is applied, which involves the calculation of the computationally intensive maximum likelihood (ML) estimate of two targets. This approach provides good results with real data from experiments with a single and two corner reflectors. To achieve real-time capability, the computational cost has to be reduced substantially. Therefore, suitable criteria are presented to pre-select the processing cells, for which the ML estimator of two targets is necessary. When the targets are resolved in the spatial spectrum, the resulting DOA estimates are generally biased. For this case, a strategy for bias correction with low computational complexity is proposed. Results obtained from simulations and real data show that the performance of the developed algorithm is close to ML estimation, but at a significantly lower computational cost. When the spatial spectrum only shows a single significant peak, either a single target is present or two targets are unresolved. For this case, a computationally simple test is developed to decide whether the model with a single target is appropriate. Consequently, ML estimation of two targets is carried out only if the single-target model is rejected. This strategy is able to substantially save computations, when situations with more than one target per processing cell are unlikely. Finally, a practicable implementation for the ML estimator of two targets is developed, which is based on a simplified objective function and a delimited search range. The required projection operators are data-independent and can be pre-calculated off-line, which enables a trade-off between computational complexity and storage space. In simulations, the developed approach is shown to perform similarly to selected computationally efficient algorithms, but allows a straightforward and non-iterative implementation. The practical value of the proposed approach is further demonstrated using real data from a typical situation of an ACC application",Antenna Array Processing: Autocalibration and Fast High-Resolution Methods for Automotive Radar,,,,,core
203513227,2012-08-31,"AbstractA closed-form approximate maximum likelihood (AML) algorithm for estimating the position and velocity of a moving source is proposed by utilizing the time difference of arrival (TDOA) and frequency difference of arrival (FDOA) measurements of a signal received at a number of receivers. The maximum likelihood (ML) technique is a powerful tool to solve this problem. But a direct approach that uses the ML estimator to solve the localization problem is exhaustive search in the solution space, and it is very computationally expensive, and prohibits real-time processing. On the basis of ML function, a closed-form approximate solution to the ML equations can be obtained, which can allow real-time implementation as well as global convergence. Simulation results show that the proposed estimator achieves better performance than the two-step weighted least squares (WLS) approach, which makes it possible to attain the Cramér-Rao lower bound (CRLB) at a sufficiently high noise level before the threshold effect occurs",Approximate Maximum Likelihood Algorithm for Moving Source Localization Using TDOA and FDOA Measurements ,,Chinese Journal of Aeronautics. Production and hosting by Elsevier B.V.,10.1016/S1000-9361(11)60423-8,,core
22542211,2011,"In real-world applications, “what you saw ” during training is often not “what you get ” during deployment: the distribution and even the type and dimensionality of features can change from one dataset to the next. In this paper, we address the problem of visual domain adaptation for transferring object models from one dataset or visual domain to another. We introduce ARC-t, a flexible model for supervised learning of non-linear transformations between domains. Our method is based on a novel theoretical result demonstrating that such transformations can be learned in kernel space. Unlike existing work, our model is not restricted to symmetric transformations, nor to features of the same type and dimensionality, making it applicable to a significantly wider set of adaptation scenarios than previous methods. Furthermore, the method can be applied to categories that were not available during training. We demonstrate the ability of our method to adapt object recognition models under a variety of situations, such as differing imaging conditions, feature types and codebooks. 1",What you saw is not what you get: Domain adaptation using asymmetric kernel transforms,,,10.1109/cvpr.2011.5995702,,core
29440806,2012-01-01T00:00:00Z,"By introducing the conflicting effects of dynamic changes in blood flow, volume, and blood oxygenation, Balloon model provides a biomechanical compelling interpretation of the BOLD signal. 
In order to obtain optimal estimates for both the states and parameters involved in this model, a joint filtering (estimate) method has been widely used. However, it is flawed in several aspects (i) Correlation or interaction between the states and parameters is incorporated despite its nonexistence in biophysical reality. (ii) A joint representation for states and parameters necessarily means the large dimension of state space and will in turn lead to huge numerical cost in implementation. Given this knowledge, a dual filtering approach is proposed and demonstrated in this paper as a highly competent alternative, which can not only provide more reliable estimates, but also in a more efficient way. The two approaches in our discussion will be based on unscented Kalman filter, which has become the algorithm of choice in numerous nonlinear estimation and machine learning applications",Reliable and Efficient Approach of BOLD Signal with Dual Kalman Filtering,,Hindawi Limited,10.1155/2012/961967,"[{'title': None, 'identifiers': ['1748-6718', 'issn:1748-6718', '1748-670x', 'issn:1748-670X']}]",core
42337406,2012-01-01T00:00:00,"New embedded predictive control applications call for more efficient ways of solving quadratic programs (QPs) in order to meet demanding real-time, power and cost requirements. A single precision QP-on-a-chip controller is proposed, implemented in a field-programmable gate array (FPGA) with an iterative linear solver at its core. A novel offline scaling procedure is introduced to aid the convergence of the reduced precision solver. The feasibility of the proposed approach is demonstrated with a real-time hardware-in-the-loop (HIL) experimental setup where an ML605 FPGA board controls a nonlinear model of a Boeing 747 aircraft running on a desktop PC through an Ethernet link. Simulations show that the quality of the closed-loop control and accuracy of individual solutions is competitive with a conventional double precision controller solving linear systems using a Riccati recursion.This work was supported by the EPSRC (Grants EP/G031576/1, EP/G030308/1 and EP/I012036/1) and the EU FP7 Project EMBOCON, as well as industrial support from Xilinx, the Mathworks, and the European Space Agency.IFAC Conference on Nonlinear Model Predictive Control 2012 (NMPC'12), Noordwijkerhout, the Netherlands on August 23 - 27, 2012",Predictive control of a Boeing 747 aircraft using an FPGA,https://core.ac.uk/download/42337406.pdf,IFAC Proceedings Volumes (IFAC-PapersOnline),,,core
15944962,2010-01-01T00:00:00,"Beyeler M, Stefanini F, Proske H, Galizia G, Chicca E. Exploring Olfactory Sensory Networks: Simulations and Hardware Emulation. Presented at the Biomedical Circuits and Systems Conference (BIOCAS), Paphos.Olfactory stimuli are represented in a high-
dimensional space by neural networks of the olfactory system.
A great deal of research in olfaction has focused on this
representation within the first processing stage, the olfactory bulb
(vertebrates) or antennal lobe (insects) glomeruli. In particular
the mapping of chemical stimuli onto olfactory glomeruli and
the relation of this mapping to perceptual qualities have been
investigated. While a number of studies have illustrated the
importance of inhibitory networks within the olfactory bulb or
the antennal lobe for the shaping and processing of olfactory
information, it is not clear how exactly these inhibitory networks
are organized to provide filtering and contrast enhancement
capabilities. In this work the aim is to study the topology
of the proposed networks by using software simulations and
hardware implementation. While we can study the dependence
of the activity on each parameter of the theoretical models
with the simulations, it is important to understand whether the
models can be used in robotic applications for real-time odor
recognition. We present the results of a linear simulation, a
spiking simulation with I&F neurons and a real-time hardware
emulation using neuromorphic VLSI chips. We used an input
data set of neurophysiological recordings from olfactory receptive
neurons of insects, especially Drosophila",Exploring Olfactory Sensory Networks: Simulations and Hardware Emulation,https://core.ac.uk/download/15944962.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/BIOCAS.2010.5709623,,core
211472671,2013-01-01T00:00:00,"The advanced MRI techniques, such as Diffusion Weighted Imaging (DWI, DTI, DSCI), Diffusion Tensor Imaging (DTI), Dynamic Susceptibility Contrast Imaging (DSCI) and Phase-Contrast Imaging (PC), provide useful information that reflect cellular processes and complete conventional MRI.Despite their important diagnostic value in brain and breast tumor characterization, these techniques have not yet been fully integrated in the MR imaging protocols. In order for this to happen, it is important to understand the parameters that significantly affect the reliability of these techniques, and which should be properly selected in order to provide accurate results. Furthermore, the integration of advanced imaging techniques in the clinical routine requires the correlation of diffusion and perfusion metrics to certain pathologies, as well as the evaluation of their diagnostic value in common differential diagnostic problems. The results of the current PhD dissertation accent that the diffusion and perfusion indices provide valuable structural and functional information in a cellular level, highlighting the underlying pathophysiological mechanisms of tumors. However, the most important observation is that the combination and coevaluation of diffusion and perfusion parameters improves the diagnostic outcome.Nevertheless, the management, analysis and evaluation of these parameters combined with their numeric nature, is a time consuming process, hence in many cases these important diagnostic indices remain unexploited during the clinical routine. Therefore, the need for the automated evaluation and management of diffusion and perfusion data, in order to assist differential diagnosis, is continuously increasing. Towards this direction, the last years the application of advanced data analysis techniques is investigated, such as pattern recognition techniques and specifically machine learning algorithms. These methods allow the management of a large number of quantitative data that are correlated with complicated linear or non-linear relationships. However, the most important aspect of these techniques is that they provide an automated and rapid analysis and coevaluation of the overall clinical information, which enables the integration of diffusion and perfusion techniques in the MR imaging protocols in order to aid differential diagnosis. Hence, from the current PhD thesis, the complex diffusion and perfusion MR data from space occupying brain lesions, were inserted to an automated software -after their thorough processing, analysis and evaluation-, in order to become a part of the overall diagnostic information obtained, and consequently assist the differential diagnostic process in real clinical time. The related software is based on machine learning algorithms and pattern recognition techniques.Finally, a training tool has been developed and integrated into the software in order to contribute to the education and training of young scientists by providing information on a theoretical as well as on a clinical level.Οι εξελιγμένες τεχνικές απεικόνισης Πυρηνικού Μαγνητικού Συντονισμού, όπως είναι η απεικόνιση της Μοριακής Διάχυσης (Diffusion Weighted Imaging, DWI), η απεικόνιση του Τανυστού Διάχυσης (Diffusion Tensor Imaging, DTI), η δυναμική απεικόνιση της Αιμάτωσης (Dynamic- Susceptibility Contrast Imaging, DSCI) και η απεικόνιση Φάσης-Αντίθεσης (Phase-Contrast MRI) συμπληρώνουν την ανατομική απεικόνιση MRI με σκοπό να αυξήσουν την διαγνωστική ακρίβεια. Παρά την σημαντική τους διαγνωστική αξία στον χαρακτηρισμό εξεργασιών του εγκεφάλου και του μαστού, οι τεχνικές αυτές δεν έχουν ενσωματωθεί πλήρως στα απεικονιστικά πρωτόκολλα. Για να επιτευχθεί αυτό είναι αναγκαίο να κατανοηθούν οι παράμετροι που επιδρούν σημαντικά στην αξιοπιστία των εν λόγω τεχνικών, η σωστή ρύθμιση των οποίων οδηγεί στην εξαγωγή ασφαλών συμπερασμάτων. Επιπροσθέτως, η ενσωμάτωση των εξελιγμένων τεχνικών απεικόνισης στην κλινική ρουτίνα προϋποθέτει την συσχέτιση των ευρημάτων διάχυσης και αιμάτωσης με συγκεκριμένους τύπους παθολογίας, καθώς και την αξιολόγηση της διαγνωστικής τους αξίας για την διαφοροποίηση βλαβών με υψηλό διαφοροδιαγνωστικό ενδιαφέρον. Τα αποτελέσματα της παρούσας διδακτορικής διατριβής αναδεικνύουν ότι οι δείκτες διάχυσης και αιμάτωσης παρέχουν πολύτιμες δομικές και λειτουργικές πληροφορίες σε κυτταρικό επίπεδο, οι οποίες σε συνδυασμό με την ανατομική πληροφορία αναδεικνύουν την υποκείμενη παθοφυσιολογία. Ειδικότερα, κατέστη σαφές ότι ο συνδυασμός και η συναξιολόγηση των παραμέτρων παρέχει το βέλτιστο διαγνωστικό αποτέλεσμα.Εντούτοις, η διαχείριση, η ανάλυση και η αξιολόγηση των παραμέτρων σε συνδυασμό με την αριθμητική τους φύση αποτελεί χρονοβόρα διαδικασία, με αποτέλεσμα συχνά να παραμένουν αναξιοποίητες. Συνεπώς, αυξάνεται συνεχώς η ανάγκη αυτόματης διαχείρισης των δεδομένων διάχυσης και αιμάτωσης με σκοπό την έγκαιρη υποβοήθηση της διαφορικής διάγνωσης σε πραγματικό κλινικό χρόνο. Για τον λόγο αυτό τα τελευταία χρόνια διερευνάται η χρήση εξελιγμένων τεχνικών ανάλυσης δεδομένων, όπως είναι οι τεχνικές αναγνώρισης προτύπου (pattern recognition techniques) και συγκεκριμένα οι αλγόριθμοι μηχανικής εκμάθησης και ταξινόμησης (machine learning algorithms). Οι τεχνικές αυτές επιτρέπουν την διαχείριση και τον συνδυασμό πληθώρας αριθμητικών δεδομένων που σχετίζονται με πολύπλοκες γραμμικές ή μη γραμμικές σχέσεις. Το κυριότερο όμως πλεονέκτημα τους είναι η αυτόματη και ταχεία ανάλυση και συναξιολόγηση της ιατρικής πληροφορίας, γεγονός που επιτρέπει την εισαγωγή τέτοιων τεχνικών στην κλινική ρουτίνα με σκοπό την υποβοήθηση της διαφορικής διάγνωσης. Προς αυτή την κατεύθυνση, ολοκληρώνοντας την συλλογή, επεξεργασία, ανάλυση και αξιολόγηση των πολύπλοκων παραμέτρων διάχυσης και αιμάτωσης για την διαφοροποίηση εξεργασιών του εγκεφάλου, πραγματοποιήθηκε εισαγωγή των δεδομένων σε ένα αυτοματοποιημένο λογισμικό, με σκοπό να αποτελέσουν μέρος της συνολικής διαγνωστικής πληροφορίας για την υποβοήθηση της διαφορικής διάγνωσης σε πραγματικό κλινικό χρόνο, προς όφελος του ασθενούς. Το εν λόγω λογισμικό στηρίζεται σε αλγορίθμους μηχανικής εκμάθησης και μεθόδους αναγνώρισης προτύπων. Τέλος, αναπτύχθηκε και ενσωματώθηκε ένα εργαλείο εκμάθησης των εξελιγμένων τεχνικών διάχυσης και αιμάτωσης, με σκοπό να συμβάλλει στην επιμόρφωση και εκπαίδευση κυρίως νέων επιστημόνων, παρέχοντας πληροφορία τόσο σε θεωρητικό όσο και σε κλινικό επίπεδο","Combination and optimization of advanced diffusion and perfusion imaging techniques (DWI, DTI, DSCI) in high magnetic field of 3T",,'National Documentation Centre (EKT)',10.12681/eadd/29413,,core
153410494,2011-01-01T00:00:00,"Another two years have passed from the last HoloMAS conference held in Linz
in 2009. It is a pleasure to say that the R&D activities around holonic and
multi-agent systems for industrial applications have not faded during this period.
On the contrary, the number of scientific events aimed at the subject field
is growing steadily. Besides HoloMAS, which has been the pioneering event in
this field, there are multiple conferences such as the IEEE Conference on Industrial
Informatics (INDIN), the IEEE Conference on Emergent Technologies
and Factory Automation (ETFA) or the IFAC Symposium on Information Control
Problems in Manufacturing (INCOM) that aim their attention at advanced
industrial automation systems based on intelligent agents.
This year’s conference was the eighth in the sequence of HoloMAS events. The
first three (HoloMAS 2000 in Greenwich, HoloMAS 2001 in Munich and Holo-
MAS 2002 in Aix-en-Provence) were organized as workshops under the umbrella
of DEXA association. Starting with 2003, HoloMAS became an independent
conference organized biyearly on the odd years, still under the DEXA patronage
(HoloMAS 2003 in Prague, HoloMAS 2005 in Copenhagen, HoloMAS 2007 in Regensburg
and HoloMAS 2009 in Linz). On the even years the attention is focused
on specific events: the IEEE Workshop on Distributed Intelligent Systems (DIS
2006) with a special track covering the “obvious”HoloMAS topics was organized
in Prague in June 2006. Similarly, the IEEE Conference on Distributed Human–
Machine Systems (DHMS 2008), which has absorbed the HoloMAS field, was
held in Athens, Greece, in March 2008, and the IFAC Workshop on Intelligent
Manufacturing Systems (IMS 2010) in Lisbon, Portugal, in 2010. This approach
allows the HoloMAS community to be better integrated with both the information
society-oriented DEXA community as well as the IEEE Society aimed at
human–machine systems, cybernetics, and industrial informatics.
The research of holonic and agent-based systems receives constant support
from both the public sector and private institutions. There is an increased interest
from the IEEE System, Man, and Cybernetics (SMC) Society, namely,
from its Technical Committee on Distributed Intelligent Systems, which leverages
the experience gained in the former Holonic Manufacturing Systems consortium.
Another IEEE body - the Industrial Electronics Society - supports
the related R&D field through its Technical Committee on Industrial Agents
(http://tcia.ieee-ies.org/). Its mission is to provide a base for researchers and
application practitioners, to share their experiences with applications of holonic
and agent technologies in the industrial sector, especially in assembly and process
control, planning and scheduling, and supply chain managements. There
are number of impacted journals that provide space for articles dealing with industrial
agents such as the IEEE Transactions on SMC, Part C: Applications and Reviews, Journal of Engineering of Artificial Intelligence Applications (EAAI),
IEEE Transactions on Industrial Informatics, Computers in Industry or the
Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS). Let us
recall that the extended versions of selected best HoloMAS 2009 papers were
published in the special issue of the International Journal of Production
Research.
It is our pleasure to inform you that for HoloMAS 2011 there were 36 papers
submitted, from which the ProgramCommittee selected 25 papers to be included
in this volume, by authors from 13 countries all over the world. Among the key
trends that the accepted papers report on is the effort to shift the holonic/agentbased
control system from the personal computer (the prevalent hosting platform
in the past) closer to the “hardware.” The obvious reason is to increase even
more the distributiveness and thus the robustness and flexibility of the control
system. In this model the control application is designed as embedded software
running on a dedicated microcontroller. This brings new challenges related to
limited resources in terms of memory, processing power, battery life, etc. Related
topics discussed in this volume are the employment of simulation techniques for
modeling, designing and validating the control system prior to its deployment
on the real hardware. The boom in Web applications and smart mobile devices
like smart phones and tablets have recently drawn the attention of the industrial
sector as it brings new challenges and possibilities for building next–generation
user interfaces for SCADA and operator panels. Looking at the applications of
holonic and agent systems we collected quite an interesting portfolio this year,
including smart grids, supply chain and logistics, healthcare, mobile robots and
unmanned aerial vehicles.
There were two invited talks specifically targeted toward HoloMAS 2011:
• Peter Skobelev (Magenta Solutions): “Multi-agent Systems for Real-Time
Resource Allocation, Scheduling, Optimization and Controlling”
• Alois Zoitl (Vienna University of Technology): “A Control Architecture for
Self-Reconfigurable Production Systems”
Also, for the first time in the HoloMAS history, there was a special session
organized covering the topic of smart industrial systems.
The HoloMAS 2011 conference was a highly motivating environment, challenging
the future research and fostering integration in the subject field. It has
always served as a showcase of the holonic and agent-based manufacturing research
offering information on the state of the art to specialists in neighboring,
knowledge-processing research fields covered by the DEXA multi-conference
event. We are very grateful to the DEXA Association for providing us with this
excellent opportunity. We would like to express our many thanks to Gabriela
Wagner for all her organizational efforts which were of key importance for the
success of this event. We would like to thank the IEEE SMC Society, and especially the Technical
Committee on Distributed Intelligent Systems of this Society, for its technical
co-sponsorship",Holonic and multi-agent systems for manufacturing: proceedings of the 5th International Conference on Industrial Applications of Holonic and Multi-Agent Systems,,'Springer Science and Business Media LLC',10.1007/978-3-642-23181-0,,core
143830668,2011-08-01T07:00:00,"Online search advertising is currently the greatest source of revenue for many Internet giants such as Google™, Yahoo!™, and Bing™. The increased number of specialized websites and modern profiling techniques have all contributed to an explosion of the income of ad brokers from online advertising. The single biggest threat to this growth is however click fraud. Trained botnets and even individuals are hired by click-fraud specialists in order to maximize the revenue of certain users from the ads they publish on  their websites, or to launch an attack between competing businesses. Most academics and consultants who study online advertising estimate that 15% to 35% of ads in pay per click (PPC) online advertising systems are not authentic. In the first two quarters of 2010, US marketers alone spent $5.7 billion on PPC ads, where PPC ads are between 45 and 50 percent of all online ad spending. On average about $1.5 billion is wasted due to click-fraud. These fraudulent clicks are believed to be initiated by users in poor countries, or botnets, who are trained to click on specific ads. For example, according to a 2010 study from Information Warfare Monitor, the operators of Koobface, a program that installed malicious software to participate in click fraud, made over $2 million in just over a year. The process of making such illegitimate clicks to generate revenue is called click-fraud. Search engines claim they filter out most questionable clicks and either not charge for them or reimburse advertisers that have been wrongly billed. However this is a hard task, despite the claims that brokers\u27 efforts are satisfactory. In the simplest scenario, a publisher continuously clicks on the ads displayed on his own website in order to make revenue. In a more complicated scenario. a travel agent may hire a large, globally distributed, botnet to click on its competitor\u27s ads, hence depleting their daily budget. We analyzed those different types of click fraud methods and proposed new methodologies to detect and prevent them real time. While traditional commercial approaches detect only some specific types of click fraud, Collaborative Click Fraud Detection and Prevention (CCFDP) system, an architecture that we have implemented based on the proposed methodologies, can detect and prevents all major types of click fraud. The proposed solution analyzes the detailed user activities on both, the server side and client side collaboratively to better describe the intention of the click. Data fusion techniques are developed to combine evidences from several data mining models and to obtain a better estimation of the quality of the click traffic. Our ideas are experimented through the development of the Collaborative Click Fraud Detection and Prevention (CCFDP) system. Experimental results show that the CCFDP system is better than the existing commercial click fraud solution in three major aspects: 1) detecting more click fraud especially clicks generated by software; 2) providing prevention ability; 3) proposing the concept of click quality score for click quality estimation. In the CCFDP initial version, we analyzed the performances of the click fraud detection and prediction model by using a rule base algorithm, which is similar to most of the existing systems. We have assigned a quality score for each click instead of classifying the click as fraud or genuine, because it is hard to get solid evidence of click fraud just based on the data collected, and it is difficult to determine the real intention of users who make the clicks. Results from initial version revealed that the diversity of CF attack Results from initial version revealed that the diversity of CF attack types makes it hard for a single counter measure to prevent click fraud. Therefore, it is important to be able to combine multiple measures capable of effective protection from click fraud. Therefore, in the CCFDP improved version, we provide the traffic quality score as a combination of evidence from several data mining algorithms. We have tested the system with a data from an actual ad campaign in 2007 and 2008.  We have compared the results with Google Adwords reports for the same campaign. Results show that a higher percentage of click fraud present even with the most popular search engine. The multiple model based CCFDP always estimated less valid traffic compare to Google. Sometimes the difference is as high as 53%. Detection of duplicates, fast and efficient, is one of the most important requirement in any click fraud solution. Usually duplicate detection algorithms run in real time. In order to provide real time results, solution providers should utilize data structures that can be updated in real time. In addition, space requirement to hold data should be minimum. In this dissertation, we also addressed the problem of detecting duplicate clicks in pay-per-click streams. We proposed a simple data structure, Temporal Stateful Bloom Filter (TSBF), an extension to the regular Bloom Filter and Counting Bloom Filter. The bit vector in the Bloom Filter was replaced with a status vector. Duplicate detection results of TSBF method is compared with Buffering, FPBuffering, and CBF methods. False positive rate of TSBF is less than 1% and it does not have false negatives. Space requirement of TSBF is minimal among other solutions. Even though Buffering does not have either false positives or false negatives its space requirement increases exponentially with the size of the stream data size. When the false positive rate of the FPBuffering is set to 1% its false negative rate jumps to around 5%, which will not be tolerated by most of the streaming data applications. We also compared the TSBF results with CBF. TSBF uses only half the space or less than standard CBF with the same false positive probability. One of the biggest successes with CCFDP is the discovery of new mercantile click bot, the Smart ClickBot. We presented a Bayesian approach for detecting the Smart ClickBot type clicks. The system combines evidence extracted from web server sessions to determine the final class of each click. Some of these evidences can be used alone, while some can be used in combination with other features for the click bot detection. During training and testing we also addressed the class imbalance problem. Our best classifier shows recall of 94%. and precision of 89%, with F1 measure calculated as 92%. The high accuracy of our system proves the effectiveness of the proposed methodology. Since the Smart ClickBot is a sophisticated click bot that manipulate every possible parameters to go undetected, the techniques that we discussed here can lead to detection of other types of software bots too. Despite the enormous capabilities of modern machine learning and data mining techniques in modeling complicated problems, most of the available click fraud detection systems are rule-based. Click fraud solution providers keep the rules as a secret weapon and bargain with others to prove their superiority. We proposed validation framework to acquire another model of the clicks data that is not rule dependent, a model that learns the inherent statistical regularities of the data. Then the output of both models is compared. Due to the uniqueness of the CCFDP system architecture, it is better than current commercial solution and search engine/ISP solution. The system protects Pay-Per-Click advertisers from click fraud and improves their Return on Investment (ROI). The system can also provide an arbitration system for advertiser and PPC publisher whenever the click fraud argument arises. Advertisers can gain their confidence on PPC advertisement by having a channel to argue the traffic quality with big search engine publishers. The results of this system will booster the internet economy by eliminating the shortcoming of PPC business model. General consumer will gain their confidence on internet business model by reducing fraudulent activities which are numerous in current virtual internet world","Click fraud : how to spot it, how to stop it?",https://core.ac.uk/download/143830668.pdf,ThinkIR: The University of Louisville\u27s Institutional Repository,,,core
18540517,2013-05-01T00:00:00,"The current challenge in automatic brain tumor classification based on MRS is the improvement of the robustness of the classification models that explicitly account for the probable breach of the independent and identically distributed conditions in the MRS data points. To contribute to this purpose, a new algorithm for the extraction of discriminant MRS features of brain tumors based on a functional approach is presented. Functional data analysis based on region segmentation (RSFDA) is based on the functional data analysis formalism using nonuniformly distributed B splines according to spectral regions that are highly correlated. An exhaustive characterization of the method is presented in this work using controlled and real scenarios. The performance of RSFDA was compared with other widely used feature extraction methods. In all simulated conditions, RSFDA was proven to be stable with respect to the number of variables selected and with respect to the classification performance against noise and baseline artifacts. Furthermore, with real multicenter datasets classification, RSFDA and peak integration (PI) obtained better performance than the other feature extraction methods used for comparison. Other advantages of the method proposed are its usefulness in selecting the optimal number of features for classification and its simplified functional representation of the spectra, which contributes to highlight the discriminative regions of the MR spectrum for each classification task. © 2012 John Wiley & Sons, Ltd.The authors gratefully acknowledge former INTERPRET and eTUMOUR European project partners. Data providers: Professor B. Celda (Physical Chemistry, University of Valencia, Burjassot, Valencia, Spain); Dr F. A. Howe (St George's University of London, London, UK); Dr D. Monleon (Fundacion Investigacion HCUV-University of Valencia, Valencia, Spain); Professor A. Heerschap (Radboud University, Nijmegen, the Netherlands.); Dr. W. Gajewicz, Professor L. Stefanczyk and Dr J. Fortuniak (Uniwersytet Medycznyw Lodz, Lodz, Poland); Professor J. Griffiths (CR UK Cambridge Research Institute, Cambridge, UK); Professor A. C. Peet (Academic Department of Paediatrics and Child Health, University of Birmingham, Birmingham, UK); Professor W. Semmler (Department of Medical Physics in Radiology, German Cancer Research Center, Heidelberg, Germany); Dr. J. Calvar (Fundacion para la Lucha contra las Enfermedades Neurologicas de la Infancia, Buenos Aires, Argentina); Dr. J. Capellades (Hospital Universitari Germans Trias i Pujol, Badalona, Spain); and Dr. C. Majos (Hospital Universitari de Bellvitge, L'Hospitalet de Llobregat, Barcelona, Spain). Data curators: Professor C. Arus, Dr M. Julia-Sape, Dr A. P. Candiota, Dr I. Olier, Ms T. Delgado, Ms J. Martin, Ms M. Camison and Mr A. Perez [all from Grup d'Aplicacions Biomediques de la Ressonancia Magestica Nuclear (GABRMN), Universitat Aut noma de Barcelona (UAB) (GABRMN-UAB) and CIBER-BBN]; and Professor B. Celda, Dra. M. C. Martinez-Bisbal and Dra. B. Martinez-Granados (all from Physical Chemistry, University of Valencia, Burjassot, Valencia, Spain). The authors would also like to thank Dr F. A. Howe and F. Raschke for their suggestions and comments. This work was partially funded by the European Commission: eTUMOUR (contract no. FP6-2002-LIFESCIHEALTH 503094), the HEALTHAGENTS EC project (HEALTHAGENTS) (contract no. FP6-2005-IST 027213).Fuster García, E.; Tortajada Velert, S.; Vicente Robledo, J.; Robles Viejo, M.; García Gómez, JM. (2013). Extracting MRS discriminant functional features of brain tumors. NMR in Biomedicine. 26(5):578-592. doi:10.1002/nbm.2895S578592265Tate, A. R., Underwood, J., Acosta, D. M., Julià-Sapé, M., Majós, C., Moreno-Torres, À., … Arús, C. (2006). Development of a decision support system for diagnosis and grading of brain tumours usingin vivo magnetic resonance single voxel spectra. NMR in Biomedicine, 19(4), 411-434. doi:10.1002/nbm.1016Pérez-Ruiz, A., Julià-Sapé, M., Mercadal, G., Olier, I., Majós, C., & Arús, C. (2010). The INTERPRET Decision-Support System version 3.0 for evaluation of Magnetic Resonance Spectroscopy data from human brain tumours and other abnormal brain masses. BMC Bioinformatics, 11(1), 581. doi:10.1186/1471-2105-11-581Poullet, J.-B., Sima, D. M., Simonetti, A. W., De Neuter, B., Vanhamme, L., Lemmerling, P., & Van Huffel, S. (2007). An automated quantitation of short echo time MRS spectra in an open source software environment: AQSES. NMR in Biomedicine, 20(5), 493-504. doi:10.1002/nbm.1112Ratiney, H., Sdika, M., Coenradie, Y., Cavassila, S., Ormondt, D. van, & Graveron-Demilly, D. (2005). Time-domain semi-parametric estimation based on a metabolite basis set. NMR in Biomedicine, 18(1), 1-13. doi:10.1002/nbm.895Luts, J., Poullet, J.-B., Garcia-Gomez, J. M., Heerschap, A., Robles, M., Suykens, J. A. K., & Huffel, S. V. (2008). Effect of feature extraction for brain tumor classification based on short echo time1H MR spectra. Magnetic Resonance in Medicine, 60(2), 288-298. doi:10.1002/mrm.21626Menze, B. H., Lichy, M. P., Bachert, P., Kelm, B. M., Schlemmer, H.-P., & Hamprecht, F. A. (2006). Optimal classification of long echo timein vivo magnetic resonance spectra in the detection of recurrent brain tumors. NMR in Biomedicine, 19(5), 599-609. doi:10.1002/nbm.1041Provencher, S. W. (2001). Automatic quantitation of localizedin vivo1H spectra with LCModel. NMR in Biomedicine, 14(4), 260-264. doi:10.1002/nbm.698Wilson, M., Reynolds, G., Kauppinen, R. A., Arvanitis, T. N., & Peet, A. C. (2010). A constrained least-squares approach to the automated quantitation of in vivo1H magnetic resonance spectroscopy data. Magnetic Resonance in Medicine, 65(1), 1-12. doi:10.1002/mrm.22579Simonetti, A. W., Melssen, W. J., Edelenyi, F. S. de, van Asten, J. J. A., Heerschap, A., & Buydens, L. M. C. (2005). Combination of feature-reduced MR spectroscopic and MR imaging data for improved brain tumor classification. NMR in Biomedicine, 18(1), 34-43. doi:10.1002/nbm.919Raschke, F., Fuster-Garcia, E., Opstad, K. S., & Howe, F. A. (2011). Classification of single-voxel 1H spectra of brain tumours using LCModel. NMR in Biomedicine, 25(2), 322-331. doi:10.1002/nbm.1753Opstad, K. S., Ladroue, C., Bell, B. A., Griffiths, J. R., & Howe, F. A. (2007). Linear discriminant analysis of brain tumour1H MR spectra: a comparison of classification using whole spectra versus metabolite quantification. NMR in Biomedicine, 20(8), 763-770. doi:10.1002/nbm.1147Fuster-Garcia, E., Navarro, C., Vicente, J., Tortajada, S., García-Gómez, J. M., Sáez, C., … Robles, M. (2011). Compatibility between 3T 1H SV-MRS data and automatic brain tumour diagnosis support systems based on databases of 1.5T 1H SV-MRS spectra. Magnetic Resonance Materials in Physics, Biology and Medicine, 24(1), 35-42. doi:10.1007/s10334-010-0241-8García-Gómez, J. M., Luts, J., Julià-Sapé, M., Krooshof, P., Tortajada, S., Robledo, J. V., … Robles, M. (2008). Multiproject–multicenter evaluation of automatic brain tumor classification by magnetic resonance spectroscopy. Magnetic Resonance Materials in Physics, Biology and Medicine, 22(1), 5-18. doi:10.1007/s10334-008-0146-yTate, A. R., Majós, C., Moreno, A., Howe, F. A., Griffiths, J. R., & Arús, C. (2002). Automated classification of short echo time in in vivo1H brain tumor spectra: A multicenter study. Magnetic Resonance in Medicine, 49(1), 29-36. doi:10.1002/mrm.10315Julià-Sapé, M., Acosta, D., Mier, M., Arùs, C., & Watson, D. (2006). A Multi-Centre, Web-Accessible and Quality Control-Checked Database of in vivo MR Spectra of Brain Tumour Patients. Magnetic Resonance Materials in Physics, Biology and Medicine, 19(1), 22-33. doi:10.1007/s10334-005-0023-xLukas, L., Devos, A., Suykens, J. A. K., Vanhamme, L., Howe, F. A., Majós, C., … Van Huffel, S. (2004). Brain tumor classification based on long echo proton MRS signals. Artificial Intelligence in Medicine, 31(1), 73-89. doi:10.1016/j.artmed.2004.01.001Vanhamme, L., Sundin, T., Hecke, P. V., & Huffel, S. V. (2001). MR spectroscopy quantitation: a review of time-domain methods. NMR in Biomedicine, 14(4), 233-246. doi:10.1002/nbm.695Van der Graaf, M., Julià-Sapé, M., Howe, F. A., Ziegler, A., Majós, C., Moreno-Torres, A., … Heerschap, A. (2008). MRS quality assessment in a multicentre study on MRS-based classification of brain tumours. NMR in Biomedicine, 21(2), 148-158. doi:10.1002/nbm.1172Sáez C García-Gómez JM Vicente J Tortajada S Esparza M Navarro A Fuster-Garcia E Robles M Martí-Bonmatí L Arús C A generic decision support system featuring an assembled view of predictive models for magnetic resonance and clinical data 25th Annual Scientific Meeting ESMRMBKlose, U. (1990). In vivo proton spectroscopy in presence of eddy currents. Magnetic Resonance in Medicine, 14(1), 26-30. doi:10.1002/mrm.1910140104Van den Boogaart A Van Hecke P Van Huffel S Graveron-Demilly S Van Ormondt D de Beer R MRUI: a graphical user interface for accurate routine MRS data analysis 13th Annual Scientific Meeting ESMRMB 1996 319FISHER, R. A. (1936). THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS. Annals of Eugenics, 7(2), 179-188. doi:10.1111/j.1469-1809.1936.tb02137.xDe Boor, C. (1978). A Practical Guide to Splines. Applied Mathematical Sciences. doi:10.1007/978-1-4612-6333-3Ramsay, J. (2005). Functional Data Analysis. Encyclopedia of Statistics in Behavioral Science. doi:10.1002/0470013192.bsa239Craven, P., & Wahba, G. (1978). Smoothing noisy data with spline functions. Numerische Mathematik, 31(4), 377-403. doi:10.1007/bf01404567Pearson, K. (1901). LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11), 559-572. doi:10.1080/14786440109462720FISHER, R. A. (1936). THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS. Annals of Eugenics, 7(2), 179-188. doi:10.1111/j.1469-1809.1936.tb02137.xGarcía-Gómez, J. M., Tortajada, S., Vidal, C., Julià-Sapé, M., Luts, J., Moreno-Torres, À., … Robles, M. (2008). The effect of combining two echo times in automatic brain tumor classification by MRS. NMR in Biomedicine, 21(10), 1112-1125. doi:10.1002/nbm.1288Devos, A., Lukas, L., Suykens, J. A. K., Vanhamme, L., Tate, A. R., Howe, F. A., … Van Huffel, S. (2004). Classification of brain tumours using short echo time 1H MR spectra. Journal of Magnetic Resonance, 170(1), 164-175. doi:10.1016/j.jmr.2004.06.010Daubechies, I. (1992). Ten Lectures on Wavelets. doi:10.1137/1.9781611970104Hyvärinen, A., Karhunen, J., & Oja, E. (2001). Independent Component Analysis. Adaptive and Learning Systems for Signal Processing, Communications, and Control. doi:10.1002/047122131",Extracting MRS discriminant functional features of brain tumors,http://hdl.handle.net/10251/34457,'Wiley',10.1002/nbm.2895,,core
12206366,2010-06-25T00:00:00,"Artificial neural networks (ANN) offer tremendous promise in classifying electrocardiogram (ECG) for detection and diagnosis of cardiovascular diseases. In this thesis, we propose a reusable neuron architecture (RNA) to enable an efficient and cost-effective ANN-based ECG processing by multiplexing the same physical neurons for both feed-forward and back-propagation stages. RNA further conserves the area and resources of the chip and reduces power dissipation by coalescing different layers of the neural network into a single layer. Moreover, the microarchitecture of each RNA neuron has been optimized to maximize the degree of hardware reusability by fusing multiple two-input multipliers and a multi-input adder into one two-input multiplier and one two-input adder. With RNA, we demonstrated a hardware implementation of a three-layer 51-30-12 artificial neural network using only thirty physical RNA neurons.A quantitative design space exploration in area, power dissipation, and speed between the proposed RNA and three other implementations representative of different reusable hardware strategies is presented and discussed. An RNA ASIC was implemented using 45nm CMOS technology and verified on a Xilinx Virtex-5 FPGA board. Compared with an equivalent software implementation in C executed on a mainstream embedded microprocessor, the RNA ASIC improves both the training speed and the energy efficiency by three orders of magnitude, respectively. The real-time and functional correctness of RNA was verified using real ECG signals from the MIT-BIH arrhythmia database",RNA: REUSABLE NEURON ARCHITECTURE FOR ON-CHIP ELECTROCARDIOGRAM CLASSIFICATION AND MACHINE LEARNING,https://core.ac.uk/download/12206366.pdf,,,,core
293503102,2012-01-01T00:00:00,"The robust popularization of 3D videos noticed along the last decade, allied to the omnipresence of smart mobile devices handling multimedia-capable features, has led to intense development and research focusing on efficient 3D-video encoding techniques, display technologies, and 3D-video capable mobile devices. In this scenario, the Multiview Video Coding (MVC) standard is key enabler of the current 3D-video systems by leading to meaningful data reduction through advanced encoding techniques. However, real-time MVC encoding for high definition videos demands high processing performance and, consequently, high energy consumption. These requirements are attended neither by the performance budget nor by the energy envelope available in the state-of-the-art mobile devices. As a result, the realization of MVC targeting mobile systems has been posing serious challenges to industry and academia. The main goal of this thesis is to propose and demonstrate energy-efficient MVC solutions to enable high-definition 3D-video encoding on mobile battery-powered embedded systems. To expedite high performance under severe energy constraints, this thesis proposes jointly considering energy-efficient optimizations at algorithmic and architectural levels. On the one hand, extensive application knowledge and data analysis was employed to reduce and control the MVC complexity and energy consumption at algorithmic level. On the other hand, hardware architectures specifically designed targeting the proposed algorithms were implemented applying low-power design techniques, dynamic voltage scaling, and application-aware dynamic power management. The algorithmic contribution lies in the MVC energy reduction by shorten the computational complexity of the energy-hungriest encoder blocks, the Mode Decision and the Motion and Disparity Estimation.  The proposed energy-efficient algorithms take advantage of the video properties along with the strong correlation available within the 3D-Neighborhood (spatial, temporal and disparity) space in order to efficiently reduce energy consumption. Our Multi-Level Fast Mode Decision defines two complexity reduction operation modes able to provide, on average, 63% and 71% of complexity reduction, respectively. Additionally, the proposed Fast ME/DE algorithm reduces the complexity in about 83%, for the average case. Considering the run-time variations posed by changing coding parameters and video content, an Energy-Aware Complexity Adaptation algorithm is proposed to handle the energy versus coding efficiency tradeoff while providing graceful quality degradation under severe battery draining scenarios by employing asymmetric video coding. Finally, to cope with eventual video quality losses posed by the energy-efficient algorithms, we define a video quality management technique based on our Hierarchical Rate Control. The Hierarchical Rate Control implements a frame-level rate control based on a Model Predictive Controller able to increase in 0.8dB (Bjøntegaard) the overall video quality. The video quality is increased in 1.9dB (Bjøntegaard) with the integration of the basic unit-level rate control designed using Markov Decision Process and Reinforcement Learning. Even though the energy-efficient algorithms drive to meaningful energy reduction, hardware acceleration is mandatory to reach the energy-efficiency demanded by the MVC. Aware of this requirement, this thesis brings architectural solutions for the Motion and Disparity Estimation unit focusing on energy reduction while attending real-time throughput requirements. To achieve the desired results, as shown along this volume, there is a need to reduce the energy related to the ME/DE computation and related to the intense memory communication.  Therefore, the ME/DE architectures incorporate the Fast ME/DE algorithm in order to reduce the computational complexity while the memory hierarchy was carefully designed to find the optimal energy tradeoff between external memory accesses and on-chip video memory size. Statistical analysis where used to define the size and organization of the on-chip cache memory while avoiding increased memory misses and the consequent data retransmission. A prefetching technique based on search window prediction also supports the reduction of external memory access. Moreover, a memory power gating technique based on dynamic search window formation and an application aware power management were proposed to reduce the static energy consumption related to on-chip video memory. To implement these techniques a SRAM memory featuring multiple power states was used. The architectural contribution contained in this thesis extends the state-of-the-art by achieving real-time ME/DE processing for 4-views HD1080p running at 300MHz and consuming 57mW",Energy-efficient algorithms and architectures for multiview video coding,https://core.ac.uk/download/293503102.pdf,,,,core
357366105,2013-01-01T00:00:00,"Abstract. The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution. While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MI-PAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products. The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MI-PAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The LevenbergMarquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg-Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg-Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative selfadapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure Published by Copernicus Publications on behalf of the European Geosciences Union. P. Raspollini et al.: Ten years of MIPAS that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions. Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H 2 O, O 3 , HNO 3 , CH 4 , N 2 O, NO 2 , CFC-11, CFC-12, N 2 O 5 and ClONO 2 . The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements",Atmospheric Measurement Techniques Ten years of MIPAS measurements with ESA Level 2 processor V6 - Part 1: Retrieval algorithm and diagnostics of the products,,,,,core
132242008,2011-01-01T00:00:00,"This  paper  demonstrates  a  new  alternative  way  in  estimating seismically thin-bed (below-tuning) thickness. Initial thickness is built by bandpass filtering the amplitude display of a zero-phase seismic. The filter removes the  non  minimum  and  or  non  maximum  and  left  the  maximum  and  or  the minimum of seismic amplitude. The unresolved below-tuning thickness is then corrected  by  zero-INTENS-difference  (z-i-d)  attribute.  INTENS  is  integrated energy  spectra,  an  attribute  which  can  be  derived  from  spectral  analysis.  z-i-d attribute is zero difference of INTENS between the seismic and its synthetic. The method  generates  INTENS  difference  profile  by  subtracting  seismic  INTENS and its synthetic INTENS iteratively. The iteration is controlled by dipole space shifting from  distance to closer or  vice  versa.  The true thickness is derived  by locating z-i-d which laid in INTENS different profile. It has found that, for free noise  true  seismic  and  perfect-wavelet  (a  wavelet  which  only  approximately similar  with  wavelet  which  constructing  the  true  seismic)  synthetic  seismic,  in INTENS  different  profile,  the  z-i-d  location  always  corresponds  to  true  dipole space or thickness. The method could resolve all thickness of a wedge-modeled seismic with three different dominant frequencies. When the synthetic seismic is constructed with imperfect wavelet, slightly different analysis is needed to locate z-i-d  attribute  and  the  result  is  not  as  perfect  as  when  perfect  wavelet constructing synthetic seismic. A quiet similar result is got when the method is implemented  for  noisy  wedge-modeled  seismic.  Bad  thickness  estimation  is resulted  for  20%  noise  seismic.  The  method  algorithm  is  extended  for  similar dipole polarity model and multilayer model to bring the method to real seismic data  nearer.  The  extension  is  done  by  estimating  thickness  of  every  layer  of  a stacked-wedge-modeled  seismic. The algorithm then generalized for estimating layers  thickness  with  several  thickness  combinations.  The  method  was  able  to delineate shallow channel of Stratton Field by providing good pseudo-acousticimpedance (pseudo AI) map",Combination of Minimum-Maximum (M-m) Attribute and Zero-INTENS-Difference (Z-i-d) Attribute for Estimating Seismically Thin-Bed Thickness,https://media.neliti.com/media/publications/72383-EN-combination-of-minimum-maximum-m-m-attri.pdf,Bandung Institute of Technology,10.5614/itbj.eng.sci.2011.43.2.1,,core
214602749,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake- holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished1. Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons, Chichester, England (2002). ISBN 0 47149691X

2. Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

3. Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 10–15. ACM Press (2006)

4. Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

5. Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

6. Zhang, Z., Thangarajah, J., Padgham, L.: Model based testing for agent systems. In: J. Filipe, B. Shishkov, M. Helfert, L. Maciaszek (eds.) Software and Data Technologies, Communications in Computer and Information Science, vol. 22, pp. 399–413. Springer, Berlin/Heidelberg (2009)

7. Ekinci, E.E., Tiryaki, A.M., Çetin, Ö., Dikenelli: Goal-oriented agent testing revisited. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 173–186. Springer, Berlin/Heidelberg (2009)

8. Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 199–212. Springer, Berlin/Heidelberg (2009)

9. Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 187–198. Springer, Berlin/Heidelberg (2009)

10. Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

11. Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

12. Erol, K., Hendler, J.A., Nau, D.S.: HTN planning: Complexity and expressivity. In: Proceedings of the 12th National Conference on Artificial Intelligence (AAAI), pp. 1123–1128. AAAI Press (1994)

13. de Silva, L., Padgham, L.: A comparison of BDI based real-time reasoning and HTN based planning. In: G. Webb, X. Yu (eds.) AI 2004: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3339, pp. 1167–1173. Springer, Berlin/Heidelberg (2004)

14. Erol, K., Hendler, J., Nau, D.: Complexity results for HTN planning. Annals of Mathematics and Artificial Intelligence 18(1), 69–93 (1996)

15. Paolucci, M., Shehory, O., Sycara, K.P., Kalp, D., Pannu, A.: A planning component for RETSINA agents. In: N.R. Jennings, Y. Lespérance (eds.) Intelligent Agents VI, Agent Theories, Architectures, and Languages (ATAL), 6th International Workshop, ATAL ’99, Orlando, Florida, USA, July 15-17, 1999, Proceedings, Lecture Notes in Computer Science, vol. 1757, pp. 147–161. Springer, Berlin/Heidelberg (2000)

16. Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

17. Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

18. d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, Lecture Notes in Artificial Intelligence, vol. 1365, pp. 155–176. Springer, Berlin/Heidelberg (1998)

19. Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

20. Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

21. Lee, J., Huber, M.J., Kenny, P.G., Durfee, E.H.: UM-PRS: An implementation of the procedural reasoning system for multirobot applications. In: Proceedings of the Conference on Intelligent Robotics in Field, Factory, Service, and Space (CIRFFSS’94), pp. 842–849 (1994)

22. Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

23. Morley, D., Myers, K.: The SPARK agent framework. In: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 714–721. IEEE Computer Society, Washington, DC, USA (2004)

24. Pokahr, A., Braubach, L., Lamersdorf, W.: Jadex: A BDI reasoning engine. In: R.H. Bordini, M. Dastani, J. Dix, A. El Fallah Seghrouchni (eds.) Multi-Agent Programming: Languages, Platforms and Applications, pp. 149–174. Springer (2005)

25. Bratman, M.E., Israel, D.J., Pollack, M.E.: Plans and resource-bounded practical reasoning. Computational Intelligence 4, 349–355 (1988)

26. Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), Lecture Notes in Artificial Intelligence, vol. 1038, pp. 42–55. Springer, Berlin/Heidelberg (1996)

27. Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

28. Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

29. Dastani, M.: 2APL: a practical agent programming language. Autonomous Agents and Multi-Agent Systems 16(3), 214–248 (2008)

30. Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemelä (eds.) Logic Programming, Lecture Notes in Computer Science, vol. 4670, pp. 302–316. Springer, Berlin/Heidelberg (2007)

31. Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http: //www.math.upenn.edu/∼wilf/gfology2.pdf

32. Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

33. Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 37–44. IFAAMAS (2008)

34. Dorigo, M., Stützle, T.: Ant Colony Optimization. MIT Press (2004). ISBN 0-262-04219-3

35. van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

36. Thangarajah, J., Winikoff, M., Padgham, L., Fischer, K.: Avoiding resource conflicts in intelligent agents. In: F. van Harmelen (ed.) Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 18–22. IOS Press (2002)

37. Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: Proceedings of the Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

38. Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: Future of Software Engineering 2007, pp. 120–136. IEEE Computer Society, Los Alamitos, CA (2007)

39. Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

40. Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

41. Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007)

42. Burch, J., Clarke, E., McMillan, K., Dill, D., Hwang, J.: Symbolic model checking: 1020 states and beyond. Information and Computation 98(2), 142–170 (1992)

43. Fix, L., Grumberg, O., Heyman, A., Heyman, T., Schuster, A.: Verifying very large industrial circuits using 100 processes and beyond. In: D. Peled, Y.K. Tsay (eds.) Automated Technology for Verification and Analysis, Lecture Notes in Computer Science, vol. 3707, pp. 11–25. Springer, Berlin/Heidelberg (2005",On the testability of BDI agent systems,,'University of Otago Library',,,core
299980420,2013-01-01T00:00:00,"Co-Space is a co-existence of real world in a virtual environment where it reflects the physical world in terms of content, facilities and structures. Nanyang Technological University (NTU) has developed its very own Co-Space to empower users with the benefit to explore and understand NTU better in the comfort of their seats. This project aims to improve and further develop the existing NTU Co-Space by adding new scenes. By modeling and implementing the fast food stalls located in NTU a new scene named Makan Place is created to hold these 3D models. Interactive contents will then be added to make exploration more realistic and interesting.

This project is broken into 5 phases, Research and Analyze, Modeling, Cashier NPC Design, Knowledge Implementation and Integration. Initially, research and analysis was conducted to decide how the stalls are to be modeled. The stalls to be modeled are McDonald’s, SubWay, Canadian Pizza, and Old Chang Kee. In the Modeling phase, these stalls were modeled into 3D using Autodesk 3ds Max 2010. A Cashier Non-Player Character (NPC) was designed and created and it will be placed at each stall. These Casher NPCs will be representing each fast food stall and they are implanted with some knowledge. This is done using Artificial Intelligence Mark-Up Language (AIML). All these will be integrated into the existing Co-Space using Unity 3D. Interactive contents that were also developed include playing a video and pop-up menu.

The long term plan of Co-Spaces is to mimic the real world as closely as possible. Hence, there will always be room for improvements even with the completion of this project. Improvements that can be made are purchasing food using credits and animations that the user’s player is eating food can be made possible in the future developments of NTU Co-Space. NPCs of NTU Co-Space could also be added to wander round the Makan Place. This will create a scene that the canteen is a buzzing place to be. These recommendations will help make NTU Co-Space more informative for users and aid them in experiencing the vibrant life in NTU.Bachelor of Engineering (Computer Science",Creating makan places in co-space,,,,,core
21152389,2010-03-06,"Abstract — A major issue for reinforcement learning (RL) applied to robotics is the time required to learn a new skill. While RL has been used to learn mobile robot control in many simulated domains, applications involving learning on real robots are still relatively rare. In this paper, the Least-Squares Policy Iteration (LSPI) reinforcement learning algorithm and a new model-based algorithm Least-Squares Policy Iteration with Prioritized Sweeping (LSPI+), are implemented on a mobile robot to acquire new skills quickly and efficiently. LSPI+ combines the benefits of LSPI and prioritized sweeping, which uses all previous experience to focus the computational effort on the most “interesting ” or dynamic parts of the state space. The proposed algorithms are tested on a household vacuum cleaner robot for learning a docking task using vision as the only sensor modality. In experiments these algorithms are compared to other model-based and model-free RL algorithms. The results show that the number of trials required to learn the docking task is significantly reduced using LSPI compared to the other RL algorithms investigated, and that LSPI+ further improves on the performance of LSPI. I",Vision-Based Reinforcement Learning using Approximate Policy Iteration,,,,,core
58795057,2011-11,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,,Springer,10.1007/s12369-011-0113-z,,core
102246345,2010,"Abstract—Primates often perform coordinated eye and arm movements, contextually fixating and reaching towards nearby objects. This combination of looking and reaching to the same target is used by infants to establish an implicit visuomotor representation of the peripersonal space, useful for both oculo-motor and arm motor control. In this work, taking inspiration from such behavior and from primate visuomotor mechanisms, a shared sensorimotor map of the environment, built on a ra-dial basis function framework, is configured and trained by the coordinated control of eye and arm movements. Computational results confirm that the approach seems especially suitable for the problem at hand, and for its implementation on a real humanoid robot. By exploratory gazing and reaching actions, either free or goal-based, the artificial agent learns to perform direct and inverse transformations between stereo vision, oculomotor, and joint-space representations. The integrated sensorimotor map that allows to contextually represent the peripersonal space through different vision and motor parameters is never made explicit, but rather emerges thanks to the interaction of the agent with the environment. Index Terms — Eye–arm coordination, humanoid robots, ra-dial basis function networks, self-supervised learning, spatial awareness. I",Implicit sensorimotor mapping of the peripersonal space by gazing and reaching (submitted,,,,,core
226662482,2013-01-01T00:00:00,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.

While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.

The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg\u2013Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg\u2013Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg\u2013Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.

Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H2O, O3, HNO3, CH4, N2O, NO2, CFC-11, CFC-12, N2O5 and ClONO2. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements",Ten years of MIPAS measurements with ESA Level 2 processor V6 - Part 1: Retrieval algorithm and diagnostics of the products,,'Copernicus GmbH',10.5194/amt-6-2419-2013,,core
35168175,2012-03-15T00:00:00,"The ability of detecting people has become a crucial subtask, especially in robotic systems which aim an application in public or domestic environments. Robots already provide their services e.g. in real home improvement markets and guide people to a desired product. In such a scenario many robot internal tasks would benefit from the knowledge of knowing the number and positions of people in the vicinity. The navigation for example could treat them as dynamical moving objects and also predict their next motion directions in order to compute a much safer path. Or the robot could specifically approach customers and offer its services. This requires to detect a person or even a group of people in a reasonable range in front of the robot. Challenges of such a real-world task are e.g. changing lightning conditions, a dynamic environment and different people shapes. In this thesis a 3D people detection approach based on point cloud data provided by the Microsoft Kinect is implemented and integrated on mobile service robot. A Top-Down/Bottom-Up segmentation is applied to increase the systems flexibility and provided the capability to the detect people even if they are partially occluded. A feature set is proposed to detect people in various pose configurations and motions using a machine learning technique. The system can detect people up to a distance of 5 meters. The experimental evaluation compared different machine learning techniques and showed that standing people can be detected with a rate of 87.29% and sitting people with 74.94% using a Random Forest classifier. Certain objects caused several false detections. To elimante those a verification is proposed which further evaluates the persons shape in the 2D space. The detection component has been implemented as s sequential (frame rate of 10 Hz) and a parallel application (frame rate of 16 Hz). Finally, the component has been embedded into complete people search task which explorates the environment, find all people and approach each detected person",3D people detection in domestic environments,https://core.ac.uk/download/35168175.pdf,Hochschule Bonn-Rhein-Sieg,10.18418/978-3-96043-008-7,,core
4765372,2012-01-19T00:00:00,"Tropospheric aerosol information from NASA satellites in space has reached the milestone of ten years of continuous measurements. These higher resolution satellite aerosol records allow for a broader regional perspective than can be gained using only sparsely located ground based monitoring sites. Decadal satellite aerosol data have the potential to advance knowledge of the climatic impacts of aerosols through better understanding of solar dimming/brightening and radiative forcings on regional scales, as well as aid in air quality applications. The goal of this thesis is to develop and implement methodologies for using satellite remotely sensed data in conjunction with ground based observations and modeling for characterization of regional aerosol variations with applications to air quality and climate studies in the Southeastern U. S. This region is of special interest because of distinct aerosol types, less warming climate trends compared to the rest of U.S., and growing population.
To support this primary goal, a technique is developed that exploits the statistical relationship between PM2.5 (particulate matter that has an aerodynamic radius of 2.5 µm or less) and satellite AOD (Aerosol Optical Depth) from MODIS (Moderate resolution Imaging Spectroradiometer) where a probabilistic approach is used for air quality assessments in the metropolitan Atlanta area. The metropolitan Atlanta area experiences the poorest air quality during the warmer seasons. We found that satellite AODs capture a significant portion of PM2.5 concentration variability during the warmer months of the year with correlation values above 0.5 for a majority of co-located (in time and space) ground based PM2.5 monitors, which is significant at the 95% confidence interval. The developed probabilistic approach uses five years of satellite AOD, PM2.5 and their related AQI (Air Quality Index) to predict future AQI based solely on AOD retrievals through the use of AOD thresholds, e.g., 80% of Code Green AQI days have AOD below 0.3. This approach has broad applicability for concerned stakeholders in that it allows for quick dissemination of pertinent air quality data in near-real time around a satellite overpass.
Examination of the use of multiple satellite sensors to aid in investigating the impacts of biomass burning in the region is performed. The utility of data fusion is evaluated in understanding the effects of the large wildfire that burned in May 2007. 
This wildfire caused PM2.5 in the metropolitan Atlanta area to exceed healthy levels with some measurements surpassing 150 µg/m3 during the month. OMI (Ozone Monitoring Instrument) AI (Aerosol Index), which qualitatively measures absorbing aerosols, have high values of more than 1.5 during May 26 - 31, 2007. CALIPSO (Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observations) a space based lidar was used to determine the vertical structure of the atmosphere across the region during the active fire period. CALIPSO was able to identify wildfire aerosols both within the planetary boundary layer (likely affects local air quality) and aloft where aerosol transport occurs. This has important implications for climatic studies specifically aerosol radiative effects.
In-depth analysis of the satellite and ground based aerosol data records over the past decade (2000 - 2009) are performed from a climatic perspective. The long temporal scale allowed for better characterization of seasonality, interannual variability, and trends. Spatial analysis of ten years of AOD from both MODIS and MISR (Multi-angle Imaging Spectroradiometer) showed little variability of AOD during the winter with mean AOD below 0.1 for the entire region, while the summer had decidedly more variability with mean AOD around 0.33 for MODIS and 0.3 for MISR. Seasonal analysis of the PM2.5 revealed that summer means are twice as high as winter means for PM2.5. All of the datasets show interannual variability that suggests with time AOD and PM2.5 are decreasing, but seasonal variability obscured the detection of any appreciable trends in AOD; however, once the seasonal influence was removed through the creation of monthly anomalies there were decreasing trends in AOD, but only MODIS had a trend of -0.00434 (per month) that statistically significant at the 95% confidence level. 
Satellite and ground-based data are used to assess the radiative impacts of aerosols in the region. The regional TOA (Top Of the Atmosphere) direct radiative forcing is estimated by utilizing satellite AOD from MODIS and MISR both on Terra, along with satellite derived cloud fraction, surface albedo (both from MODIS), and single scattering albedo (SSA) from MISR data from 2000 - 2009. Estimated TOA forcing varied from between -6 to -3 W/m2 during the winter, and during the warmer months there is more variation with ΔF varying between -28 to -12.6 W/m2 for MODIS and -26 to -11 W/m2 for MISR. The results suggest that when AOD, cloud fraction and surface albedo are all consider they add an additional 6 W/m2 of TOA forcing compared to TOA forcing due to aerosol effects only. Varying SSA can create changes in TOA forcing of about 5 W/m2. With removal of the seasonal variability timeseries anomaly trend analysis revealed that estimated TOA forcing is decreasing (becoming less negative) with MODIS based estimates statistically significant at the 95% confidence level. 
Optical and radiative 1-D radiative transfer modeling is performed to assess the daily mean TOA forcing and forcing at the surface for representative urban and background aerosol mixtures for summer and winter. During the winter, modeled TOA forcing is -2.8 and -5 W/m2 for the WB and WU cases, and the modeled summer TOA forcings (SB = -13.3 W/m2) also generally agree with earlier estimates. While surface forcings varied from -3 to -210 W/m2. The radiative forcing efficiency at the TOA (amount of forcing per unit of AOD at 550 nm) varied from -9 to -72 W/m2 τ-1, and RFE at the surface varied from -50 to -410 W/m2 τ-1. It was found that the forcing efficiency for biomass burning aerosols are similar to the forcing efficiency of background aerosols during the summer that highlights the importance of possible increased biomass burning activity. Ultimately, the methodologies developed in this work can be implemented by the remote sensing community and have direct applicability for society as a whole.PhDCommittee Chair: Sokolik, Irina; Committee Member: Bergin, Michael; Committee Member: Curry, Judith; Committee Member: Doddridge, Bruce; Committee Member: Tatarskii, Viatchesla","Aerosol characterization in the  Southeastern U. S. using
satellite data for applications to air quality and climate",https://core.ac.uk/download/4765372.pdf,Georgia Institute of Technology,,,core
211485914,2012-01-01T00:00:00,"This Special Issue is an outgrowth of the HAIS'10, the 5th International Conference on Hybrid Artificial Intelligence Systems, which was held in San Sebastián, Spain, 23–25 June 2010. The HAIS conference series is devoted to the presentation of innovative techniques involving the hybridization of emerging and active topics in data mining and decision support systems, information fusion, evolutionary computation, visualization techniques, ensemble models, intelligent agent-based systems (complex systems), cognitive and reactive distributed AI systems, case base reasoning, nature-inspired smart hybrid systems, bio- and neuro-informatics and their wide range of applications. It is dedicated to promote novel and advanced hybrid techniques as well as interdisciplinary applications and practice. HAIS'10 received over 269 submissions worldwide. After careful peer-review, only 132 papers were accepted for presentation at the conference and for inclusion in the proceedings, published as Springer's Lecture Notes in Artificial Intelligence series. Authors of the most innovative papers within the scope of the NEUROCOMPUTING Journal were invited to submit their substantially extended and updated papers with additional original materials based on their most recent research findings. Each submitted paper was subsequently reviewed by 3–5 experts and leading researchers in the field. Finally eighteen papers passed the journal's rigorous review process and were included in this Special Issue. They present an exclusive sample of the conference and its recent topics. In the area of artificial vision and image processing, Segovia et al. present a comparison between two methods for analyzing PET data in order to develop more accurate CAD systems for the diagnosis of Alzheimer's disease. One of them is based on the Gaussian Mixture Model (GMM) and models the Regions Of Interest (ROIs) defined as differences between controls and AD subject. After GMM estimation using the EM algorithm, feature vectors are extracted for each image depending on the positions of the resulting Gaussians. The other method under study computes score vectors through a Partial Least Squares (PLS) algorithm based estimation and those vectors are used as features. Before extracting the score vectors, a binary mask based dimensional reduction of the input space is performed in order to remove low-intensity voxels. The validity of both methods is tested on the ADNI database by implementing several CAD systems with linear and nonlinear classifiers and comparing them with previous approaches such as VAF and PCA. The contribution of Chyzhyk et al. entitled “Hybrid Dendritic Computing with Kernel-LICA applied to Alzheimer's disease detection in MRI” presents the issue of enhancing the generalization classification power of the Dendritic Computing approach to classifier training. The paper contributes a hybrid of the Lattice Independent Component Analysis (LICA) and the Kernel transformation that provides an enhanced generalization on cross-validation experiments performed on a dataset of magnetic resonance imaging (MRI) features for Alzheimer's disease computer aided diagnosis design. The contribution by Cilla et al. presents a human action recognition system combining multiple camera observations. The proposal is centered on how to efficiently combine the observations from different viewpoints without the explicit usage of 3D models or camera calibration. To achieve this goal, a local action prediction is defined using the features extracted at each camera. These local predictions take the form of posterior probability distributions to capture the uncertainty on the classification. These distributions are combined to obtain a final posterior distribution on the performed action. Experiments show that the proposed scheme achieves successful results in the human recognition task. In bioinformatics and medical applications, the contribution by López et al. presents the theoretical and practical results of a novel data mining process that combines hybrid techniques of association analysis and classical sequentiation algorithms of genomics to generate grammatical structures of a specific language. The authors used an application of a compilers generator system that allows the development of a practical application within the area of grammarware, where the concepts of the language analysis are applied to other disciplines, such as bioinformatic. The tool allows the complexity of the obtained grammar to be measured automatically from textual data. A technique of incremental discovery of sequential patterns is presented to obtain simplified production rules, and compacted with bioinformatics criteria to make up a grammar. The work by Cuadra et al. entitled “Response Calibration in Neuroblastoma Cultures over Multielectrode Array” assesses the statistical relevance of neuroblastoma culture responses to excitation when they are grown in a Multielectrode Array (MEA) setup in order to provide an appropriate calibration of the systems response to excitation. The MAE are characterized by very low signal-to-noise ratio, which is improved by the proposed calibration, opening the possibility to employ neuroblastoma cultures for information processing in hybrid systems. In Evolutionary Computation, Maravall et al. present the application of evolutionary strategies to the self-emergence of a common lexicon in a population of agents. By modeling the vocabulary or lexicon of each agent as an association matrix or look-up table that maps the meanings (i.e. the objects encountered by the agents or the states of the environment itself) into symbols or signals it is checked whether it is possible for the population to converge in an autonomous, decentralized way to a common lexicon, so that the communication efficiency of the entire population is optimal. In the contribution by García-Gutiérrez et al. a novel hybrid classifier applied to remote sensing data fusion is presented. The classifier, called EVOR-STACK, is based on the use of ensemble techniques and evolutionary computation. The latter allows to calculate a set of weights for every feature depending on the candidate label and therefore, extending the classical evolutive feature weighting concept. The former improves the final results due to the introduction of contextual data previously weighted with the results of the label-dependent evolutionary algorithm. The results confirmed that EVOR-STACK outperforms SVM and R-STACK (its predecessor, presented in HAIS-2010) when they are applied to LIDAR and images data fusion. In terms of Neural-based models and applications, Fernández-Navarro et al. propose an alternative to the standard Gaussian Radial Basis Function for binary classification problems. The authors present q-Gaussian Radial Basis Function Neural Networks, where the basis functions include a supplementary degree of freedom in order to adapt the model to the distribution of the data. A Hybrid Algorithm (HA) is used to search for a suitable architecture and parameters for these models. In the contribution by Lei and Ghorbani, two new clustering algorithms are presented, the Improved Competitive Learning Network (ICLN) and the Supervised Improved Competitive Learning Network (SICLN), for the applications in the area of fraud detection and network intrusion detection. The ICLN is an unsupervised clustering algorithm applying new rules to the Standard Competitive Learning Neural Network (SCLN). In the ICLN, network neurons are trained to represent the center of the data by a new reward-punishment update rule. The authors claim that the new update rule overcomes the instability of the SCLN. The SICLN is a supervised clustering algorithm further developed from the ICLN by introducing supervised mechanism. According to the authors, in the SICLN, the new supervised update rule utilizes data labels to guide the training process in order to achieve a better clustering result, and can be applied to both labeled and unlabeled data. They claim that the SICLN algorithm is highly tolerant to missing or delayed labels. Furthermore, the SICLN is completely independent from the initial number of clusters because it is able to reconstruct itself according to the labels of the cluster members. The authors successfully demonstrate the SICLN's high performance through experimental analysis using both academic research data and practical real-world data for fraud detection and network intrusion detection. They also show that the SICLN outperforms traditional unsupervised clustering algorithms. Then Prieto et al. present a hybrid intelligent system to provide autonomous robots with the ability to classify the motion behavior patterns of a group of robots present in their surroundings. It is a first step in the development of a cognitive model that can detect and understand the events occurring in the environment, events that are not the robot's own actions. The hybrid system is called ANPAC (Automatic Neural-based Pattern Classifier). It uses a variable size ANN to perform pattern classification and an advisor module to adjust the preprocessing parameters and, consequently, the size of the ANN, depending on the learning results of the network. The components and operations of ANPAC are described in depth and illustrated using an example related to the recognition of behavior patterns in the flocks motion. The next contribution by Heras et al. presents an abstract argumentation framework for the support of agreement processes in agent societies. It takes into account some arguments, among them attacks arguments, and the social context of the agents that put forward arguments. The framework is implemented as a neural network that computes the set of acceptable arguments and can be tuned to give more or less importance to argument attacks. The proposal is illustrated with an example in a real domain of a water-rights transfer market. Two contributions based on ensembles techniques are included in this special issue. The contribution by Buza et al. proposes a model-selection framework for stacking regressors. Due to the differences in their underlying assumptions, principles and settings, various regression models (such as neural and Bayesian networks, support vector machines, regression trees, etc.) err differently on instances, i.e., compared to the true value of the target, some of the models predict lower values, while other models predict higher ones. Therefore, different models are potentially able to compensate for each other's errors. This idea of error compensation is explored and exploited for making stacking-based ensembles more accurate: the proposed framework focuses on the principled selection of a well-performing set of models, which may be substantially different from the individually best models due to the inter-correlations between them. Then Corchado and Baruque present a novel model for multi-dimensional data visualization in 2 dimensions. It can be considered an extension of a previously developed Visualization Induced Self-Organizing Map by the use of the ensemble meta-algorithm in an unsupervised context. The contribution shows, with a comparative study, how the calculation of slightly different ViSOM maps and its subsequent fusion into a final map can give as a result a more truthful visual 2D display of the analyzed data. This algorithm proves to be a useful tool for data visualization where the dataset under analysis is especially complex, as the enhanced representation compensates for the extra complexity of its calculations. Regarding issues of Classification methods & Information Processing, Wilk and Wozniak present a possibility of generalizing the two-class classification into multiclass classification by means of a fuzzy inference system. Fuzzy combiner harnesses the support values from classifiers to provide final response having no other restrictions on their structure. Authors compare the combination methods with ECOC and two variations of decision templates, based on Euclidean and symmetric distance. The quality of the proposed method was evaluated via computer experiment. Then, in the contribution by Chmielnicki and Stapor, the authors present a method of combining a support vector machine (SVM)—discriminative classifier with regularized discriminant analysis (RDA)—generative classifier. The hybrid SVM–RDA classifier is used in the protein fold prediction. It is a very challenging multiclass problem with high data dimensionality and a very small number of samples for each class. The authors show how to deal with these difficulties using advantages of generative and discriminative classifiers. In the contribution by Kazienko and Kajdanowicz entitled “Label-dependent Node Classification in the Network”, are presented an original extension and application of sampling algorithms to node classification in the networked data. In their new approach, they make use of new input variables, calculated based on structural network measures. Additionally, these measures, called label-dependent features, are computed separately for sub-networks of nodes with a given label-class. As a result, two new approaches of sampling algorithms—LDBootstrapping and LDGibbs have been developed for the purpose of collective classification. According to experimental studies carried out, the novel approaches provide more accurate results comparing to competitive ones in generalization for sparse networked datasets. In the contribution by Zafra et al., is presented a filter-based feature selection method for working in the multiple-instance learning scenario called ReliefF-MI. This method based on the principles of the well-known ReliefF algorithm is applied as a preprocessing step that is completely independent from the multi-instance classifier learning process and therefore is more efficient and generic than wrapper approaches proposed in this area. Different extensions are designed and implemented and their performance checked in multiple-instance learning. Experimental results on five benchmark real-world datasets and seventeen classification algorithms confirm the utility and efficiency of this method, both statistically and from the point of view of the execution time. Finally Villar et al. propose a novel approach to represent the uncertainty in the data in order to learn white box models. This representation includes; introducing fuzzy evaluation of the imprecise variables, a genetic programming approach and the learning algorithms to deal with the data. Two main multi-objective algorithms are used: Multi-objective Simulated Annealing and NSGA-II. The obtained results show this approach as a promising path for developing new controller design techniques. We would like to thank our peer-reviewers for their diligent work and efficient efforts. We are also grateful to the Editor-in-Chief, Prof. Tom Heskes, for his continued support for the HAIS conference and for this Special Issue on this prestigious journal",Special Issue: New trends and applications on hybrid artificial intelligence systems,,'Elsevier BV',,,core
214599845,2011-04-07T00:00:00,"Only an extended abstract was published in the proceedings. There is no full text.The process of classifying objects is a fundamental feature of most human pursuits, and the idea that people classify together those things that they find similar is both intuitive and popular across a wide range of disciplines. Therefore, similarity is important for people to make sense of the objects, structures and actions that exist in reality. Furthermore the ability to recognise a similar situation means that experience can be reused to solve problems, alleviating complex situations, save time and allow valuable resources to be used elsewhere.

Various philosophical and psychological theories of similarity have been implemented in information science. Specific information science terms associated with similarity include indexing, sub-setting, retrieval, matching, ranking, solution space, clustering, trees, categorising, equal and equivalence. Information science research in the field of similarity could be grouped under the headings of comparison, retrieval, evaluation and analysis functions. Various researchers from different information science disciplines are studying similarity. The results and ideas between some of these disciplines are interchangeable because of the overlapping interests. The different disciplines include computer vision, graphic design, pattern recognition, image analysis, databases, AI, remote sensing and GI systems.

Spatial similarity can be seen as a subset of similarity and all the entities being compared to each other have spatial components. Research areas that utilise spatial similarity are listed below in Table 1. It is acknowledged that some of the research overlaps, however it was decided to catergorise the general areas of spatial similarity research.PublishedNon Peer Reviewe",Spatial similarity,,,,,core
23928827,2010,"‡ These authors contributed equally to this work. Abstract — Olfactory stimuli are represented in a highdimensional space by neural networks of the olfactory system. A great deal of research in olfaction has focused on this representation within the first processing stage, the olfactory bulb (vertebrates) or antennal lobe (insects) glomeruli. In particular the mapping of chemical stimuli onto olfactory glomeruli and the relation of this mapping to perceptual qualities have been investigated. While a number of studies have illustrated the importance of inhibitory networks within the olfactory bulb or the antennal lobe for the shaping and processing of olfactory information, it is not clear how exactly these inhibitory networks are organized to provide filtering and contrast enhancement capabilities. In this work the aim is to study the topology of the proposed networks by using software simulations and hardware implementation. While we can study the dependence of the activity on each parameter of the theoretical models with the simulations, it is important to understand whether the models can be used in robotic applications for real-time odor recognition. We present the results of a linear simulation, a spiking simulation with I&amp;F neurons and a real-time hardware emulation using neuromorphic VLSI chips. We used an input data set of neurophysiological recordings from olfactory receptive neurons of insects, especially Drosophila. I",Exploring olfactory sensory networks: Simulations and hardware emulation,,IEEE,10.1109/biocas.2010.5709623,,core
33129848,2011-01-27,"L’objectif de cette recherche est de développer un réseau de neurones impulsionnels analogiques afin d’améliorer la performance d’un pacemaker biventriculaire (aussi appelé le CRT-P) de nouvelle génération. L’implémentation sur silicium utilise l’approche réseau de neurones analogiques qui nécessite le développement d’une solution technique satisfaisant à une contrainte de très basse consommation énergétique. Nous proposons une approche de un réseau de neurones impulsionnels analogiques pour optimiser la prédiction des délais cardiaques avec l’algorithme d’apprentissage Hebb et l’algorithme d’apprentissage par renforcement dans des modes de fonctionnement différents. L’amélioration des prévisions permet au CRT-P de fournir des battements cardiaques optimaux en temps réel. Nous décrirons le comportement et les qualités de notre algorithme au travers de simulations mathématiques et comportementales. Des simulations complètes et cohérentes du système basées sur des modèles simples du coeur
(rythme cardiaque constant puis rythme cardiaque variable) avec des bruits uniformes aléatoires sont illustrées avec succès pour la validation de la faisabilité du système.
Nous proposons aussi une méthodologie renforcée de la conception analogique et mixte. Les simulations de tous niveaux (de hauts et bas niveaux)peuvent être faites rapidement afin de vérifier des performances du système dans chaque phase de conception et ainsi fournir une plage des spécifications acceptables facilitant la synthèse analogique et mixte suivante.The objective of this research is to develop an analog spiking neural network so as to improve the performance of a bi-ventricular pacemaker (also called the CRT-P) of new generation. The implementation on silicon using the analog neural network approach requires the development of a
satisfactory technical solution to meet the constraint of very low energy consumption. We propose an analog spiking neural network approach to optimize the cardiac delay prediction with the Hebbian learning algorithm and the reinforcement learning algorithm in different functional modes.
The delay improvement allows the CRT-P to provide optimal heartbeat in real time. We describe the behavior and the qualities of our algorithm through mathematical and behavioral simulations. The complete and coherent system simulations based on the simple heart models (constant heart
rate and variable heart rate) with random uniform noise are shown successfully to validate the system feasibility.
We also propose an enhanced methodology of the analog and mixed signal design. The simulations of all levels (high and low levels) can be carried out quickly in order to verify the system performance in each design phase and also carry out the acceptable specification space for facilitating the following analog and mixed signal synthesis",Study and design of an analog neural processor of very low power consumption : appliced to the navigation of a new generation pacemaker,,,,,core
187877562,2010-10-01T00:00:00,"La valorizzazione del paesaggio comporta una riattivazione delle risorse e delle identit\ue0 locali in territori complessi (variet\ue0 di paesaggi urbani, aree verdi interne,
spazi periurbani, aree agricole di prossimit\ue0, riserve naturali e parchi regionali). Il ""valore"" del paesaggio non \ue8 dato soltanto dalla qualit\ue0 dell'ambiente naturale e del patrimonio culturale, ma anche dal suo ""valore"" simbolico e da valutazioni estetiche ed emozionali. Il risultato sperato \ue8 l'individuazione di strategie efficaci di ricomposizione territoriale di spazi urbani e rurali e la loro  valorizzazione culturale e turistica.
La qualit\ue0 e l'evoluzione dei rapporti di interscambio materiale e simbolico tra le aree urbane e metropolitane e il territorio dei parchi (del Ticino, di San Rossore e delle Madonie) \ue8 uno dei temi centrali della ricerca.
In modo complementare, si analizzeranno gli aspetti psico-sociali della relazione tra natura e benessere soggettivo, poich\ue9 l'ambiente naturale \ue8 fattore di benessere cognitivo ed emozionale. L'obiettivo \ue8 analizzare i legami tra qualit\ue0 dell'esperienza provata in ambienti naturali e tipologia e organizzazione dell'esperienza stessa, ipotizzando l'esistenza di mediatori attivi nella complessificazione del S\ue9.
Le considerazioni emerse dalle analisi condotte in tali ambiti confluiranno nella proposta di buone pratiche innovative per la fruizione dei territori considerati.
L'adeguamento delle politiche di gestione delle aree protette alle condizioni di valorizzazione si basa sull'applicazione di principi, strategie, dispositivi e tecnologie
finalizzati alla ecoefficienza e alla sostenibilit\ue0. A lungo il parco \ue8 stato il soggetto esclusivo delle politiche di conservazione; di recente l'interesse si \ue8 spostato ai margini, dove si concentrano nuove risorse e potenzialit\ue0.
L'ambito di ricerca, partendo dallo studio delle fasce periurbane, mira alla definizione delle modalit\ue0 di fruizione, gestione e sviluppo dei sistemi di organizzazione tecnico-funzionale integrati agli spazi del parco (gestione dell'accessibilit\ue0, integrazione della rete di percorsi, organizzazione degli spazi e attrezzature di servizio per l'accoglienza e gestione di flussi turistici; produzione e distribuzione di energia rinnovabile, e a basso impatto.
Obiettivo della ricerca \ue8 un sistema di indirizzi, strategie e tecnologie per la valorizzazione delle aree di margine di contesti complessi, adeguati ai principi del recupero sostenibile, del ripristino ambientale e della fruizione compatibile del patrimonio naturalistico e storico-culturale. Il processo operativo valuter\ue0 le
soluzioni possibili per i diversi sistemi, verificandone la conformit\ue0 rispetto ai requisiti specifici delle norme locali e del bilancio energetico finale.
Nel piano di azione strategico per la gestione sostenibile del territorio, con riferimento alle aree protette si propone un sistema di indicatori a supporto delle decisioni, finalizzato alla valutazione di progetti di intervento su assetti costruiti nelle zone ad alta valenza ambientale, in grado di orientare i decisori verso una
valorizzazione delle vocazioni locali, considerando gli impatti ambientali. Gli indicatori per la valutazione di sostenibilit\ue0 saranno declinati (approccio del Life Cycle) alle trasformazioni dell'ambiente costruito in indicatori di carattere ambientale e sociale relativi agli impatti sull'ambiente (consumo di risorse ed energie, produzione di emissioni e rifiuti, impatti relativi alla salute, al comfort e alla fruibilit\ue0 di spazi e servizi). La matrice degli indicatori aggregati costituir\ue0 uno strumento di comunicazione dei risultati in grado di favorire la convergenza di dati integrati e sinergici, e indirizzato a decisori e tecnici degli Enti locali, per l'applicazione di procedure di analisi/valutazione e la gestione in un quadro sinottico multidisciplinare.
L'unit\ue0 operativa analizzer\ue0 la realt\ue0 territoriale attraverso alcune azioni fondamentali: Analisi delle componenti territoriali; analisi SWOT dei sistemi economici locali; analisi della domanda turistica, dei principali trend di sviluppo. La SWOT Analysis, evidenzier\ue0 i settori di intervento relativi ai problemi e alle opportunit\ue0 reali e potenziali di crescita dell'offerta turistica; svilupper\ue0 una banca dati territoriale (GIS). Si implementeranno sistemi di scelte (public choice), con una molteplicit\ue0 di criteri e alternative, riguardanti le ricadute delle scelte operative.
La valutazione quali-quantitativa dei sistemi territoriali e delle interrelazioni fra essi esistenti consentir\ue0 di individuare scenari di marketing turistico.
L'ottimizzazione di un piano di marketing \ue8 deriva dagli strumenti innovativi della ricerca: le scelte matriciali permettono di modificare i criteri e le alternative di piano. Nel differenziare le strategie di marketing nei contesti urbani e rurali tali metodiche rappresentano un punto di forza.Improving the landscape implies the reactivation of local resources and identities in complex areas (urban areas, green areas within the urban space, peri-urban areas, agricultural areas, natural reserves and regional parks).
The ""value"" of the landscape is not merely due to the quality of natural environment and cultural heritage, but also to its symbolic ""value"" and aesthetic/emotional features
The research will focus on the quality and the evolution of functional relationships and material or symbolic exchange between urban/metropolitan areas and the parks (Ticino Park, San Rossore, Massaciuccoli Natural Reserve).
We will further analyse the same territories looking at the psychosocial aspects of the relation between subjective well-being and nature, because natural environment was often found as a key factor for emotional and cognitive well-being.
The general aim is therefore to analyse the links between natural positive experiences and their main psychological features, hypothesizing the existence of specific buffers able to create self-complexity. The necessary adjustment of management policies of nature reserves to the conditions for the defence, recovery and value increase of environment is characterised by the application of principles, strategies, devices and technologies meant for environmental effectiveness and sustainability.
For a long time, the park was the exclusive subject for territorial preservation policies; recently, the focus shifted to margins, where resources and potentialities are
concentrated.
Starting from the study of the peri-urban belts between built-up areas and naturalistic areas, the research is meant to study and define procedures for the fruition, management and development of technical and functional systems integrated with park areas (manegement of access, integration of the network of different routes,
ORGANISATION OF THE SYSTEM OF SPACES AND SUPPORT to tourist services, SYSTEMS FOR PRODUCING AND DISTRIBUTING ENERGY FROM RENEWABLE SOURCES)
The goal of the research is identifying a system of tendencies, strategies and technologies for increasing the value of marginal areas of complex sites, following the principles of sustainable recovery, environmental restoration and fruition compatible with the naturalistic, historical and cultural heritage. The operational process
will weigh multiple solutions which are possible for different systems, checking their conformity with respect to the specific requirements of local regulations and the ultimate energy balance.
As regards the strategic action plan for a sustainable land-use management, in reference to the protected sensitive areas will be developed a system of indicators, organized in a decisions tool.
This tool will be used to estimate design and management plans for built areas of high natural and environmental value, in order to guide decision makers towards the enhancement of the specific vocations of places, considering environmental impacts.
The indicators for the sustainability assessment are framed, according to the life cycle approach, into indicators of environmental and social impacts (energy
consumption, emissions and wastes, the impacts on health, comfort and usability of spaces and social services). The matrix of aggregated indicators is a communication tool used for furthering synergy and integration of data, and designed for decision makers and technicians of local authorities, to be applied on assessment and management procedures, in a highly multidisciplinary overview.
The working group will analyze the local context through the key actions listed below: 
Analysis of spatial components; Consideration of strengths and weaknesses of local economic systems; Analysis of the main developmental trends of the touristic demand.
A SWOT Analysis will give us the chance to highlight the areas on which to act in order to solve the problems already existing and to seize the real opportunities and potential for growth or diversification and segmentation of tourism. Later we will develop a territorial database that will require the collection and intake in the GIS
and will be implemented systems of choices (public choice model) that use a variety of criteria and  alternatives.
This methodology will be useful to evaluate territorial systems and their mutual connections, allowing to plan new turistical marketing strategies.
The fundamental point for the optimization of a marketing plan is, in fact, already contained in the innovative instruments adopted by the research. Indeed, the peculiarity of the chosen matrix allows to change real-time either the criteria for the selection or the alternative plans.
The methods which have been described above are a fundamental strength in differentiating marketing strategies in urban and rural contexts","Responsabile scientifico nazionale del Progetto PRIN 2009 ""Ricomposizione territoriale e valorizzazione sostenibile degli spazi urbani e rurali: turismo e vocazioni storiche, culturali,
architettoniche, ambientali a confronto""",,,,,core
41758544,2010-01-12T00:00:00,"The magnetic diagnostics subsystem of the LISA Technology Package (LTP)
on board the LISA PathFinder (LPF) spacecraft includes a set of four tri-axial
fluxgate magnetometers, intended to measure with high precision the magnetic
field at their respective positions. However, their readouts do not provide a
direct measurement of the magnetic field at the positions of the test masses, and
hence an interpolation method must be designed and implemented to obtain the
values of the magnetic field at these positions. However, such an interpolation
process faces serious difficulties. Indeed, the size of the interpolation region is
excessive for a linear interpolation to be reliable while, on the other hand, the
number of magnetometer channels do not provide sufficient data to go beyond
the linear approximation. We describe an alternative method to address this
issue, by means of neural network algorithms. The key point in this approach is
the ability of neural networks to learn from suitable training data representing
the behaviour of the magnetic field. Despite the relatively large distance
between the test masses and the magnetometers, and the insufficient number
of data channels, we find that our artificial neural network algorithm is able
to reduce the estimation errors of the field and gradient down to levels below
10%, a quite satisfactory result. Learning efficiency can be best improved by
making use of data obtained in on-ground measurements prior to mission launch
in all relevant satellite locations and in real operation conditions. Reliable
information on that appears to be essential for a meaningful assessment of
magnetic noise in the LTP.Peer ReviewedPostprint (published version",Theory and modelling of the magnetic field measurement in LISA PathFinder,,'IOP Publishing',10.1088/0264-9381/27/3/035005,"[{'title': 'Classical and Quantum Gravity', 'identifiers': ['0264-9381', 'issn:0264-9381']}]",core
30857249,2011-01-01T00:00:00,"Editors: Alexander Refsum Jensenius, Anders Tveit, Rolf Inge Godøy, Dan Overholt
Table of Contents

-Tellef Kvifte: Keynote Lecture 1: Musical Instrument User Interfaces: the Digital Background of the Analog Revolution - page 1
-David Rokeby: Keynote Lecture 2: Adventures in Phy-gital Space - page 2 
-Sergi Jordà: Keynote Lecture 3: Digital Lutherie and Multithreaded Musical Performance: Artistic, Scientific and Commercial Perspectives - page 3

Paper session A — Monday 30 May 11:00–12:30  
-Dan Overholt: The Overtone Fiddle: an Actuated Acoustic Instrument - page 4  
-Colby Leider, Matthew Montag, Stefan Sullivan and Scott Dickey: A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications - page 8 
-Greg Shear and Matthew Wright: The Electromagnetically Sustained Rhodes Piano - page 14
-Laurel Pardue, Christine Southworth, Andrew Boch, Matt Boch and Alex Rigopulos: Gamelan Elektrika: An Electronic Balinese Gamelan - page 18
-Jeong-Seob Lee and Woon Seung Yeo: Sonicstrument: A Musical Interface with Stereotypical Acoustic Transducers - page 24

Poster session B— Monday 30 May 13:30–14:30 
-Scott Smallwood: Solar Sound Arts: Creating Instruments and Devices Powered by Photovoltaic Technologies - page 28
-Niklas Klügel, Marc René Frieß and Georg Groh: An Approach to Collaborative Music Composition - page 32
-Nicolas Gold and Roger Dannenberg: A Reference Architecture and Score Representation for Popular Music Human-Computer Music Performance Systems - page 36
-Mark Bokowiec: V’OCT (Ritual): An Interactive Vocal Work for Bodycoder System and 8 Channel Spatialization - page 40
-Florent Berthaut, Haruhiro Katayose, Hironori Wakama, Naoyuki Totani and Yuichi Sato: First Person Shooters as Collaborative Multiprocess Instruments - page 44
-Tilo Hähnel and Axel Berndt: Studying Interdependencies in Music Performance: An Interactive Tool - page 48
-Sinan Bokesoy and Patrick Adler: 1city 1001vibrations: development of a interactive sound installation with robotic instrument performance - page 52
-Tim Murray-Browne, Di Mainstone, Nick Bryan-Kinns and Mark D. Plumbley:The medium is the message: Composing instruments and performing mappings - page 56
-Seunghun Kim, Luke Keunhyung Kim, Songhee Jeong and Woon Seung Yeo: Clothesline as a Metaphor for a Musical Interface - page 60
-Pietro Polotti and Maurizio Goina: EGGS in action - page 64
-Berit Janssen: A Reverberation Instrument Based on Perceptual Mapping - page 68
-Lauren Hayes: Vibrotactile Feedback-Assisted Performance - page 72
-Daichi Ando: Improving User-Interface of Interactive EC for Composition-Aid by means of Shopping Basket Procedure - page 76
-Ryan McGee, Yuan-Yi Fan and Reza Ali: BioRhythm: a Biologically-inspired Audio-Visual Installation - page 80
-Jon Pigott: Vibration, Volts and Sonic Art: A practice and theory of electromechanical sound - page 84
-George Sioros and Carlos Guedes: Automatic Rhythmic Performance in Max/MSP: the kin.rhythmicator - page 88
-Andre Goncalves: Towards a Voltage-Controlled Computer — Control and Interaction Beyond an Embedded System - page 92
-Tae Hun Kim, Satoru Fukayama, Takuya Nishimoto and Shigeki Sagayama: Polyhymnia: An automatic piano performance system with statistical modeling of polyphonic expression and musical symbol interpretation - page 96
-Juan Pablo Carrascal and Sergi Jorda: Multitouch Interface for Audio Mixing - page 100
-Nate Derbinsky and Georg Essl: Cognitive Architecture in Mobile Music Interactions - page 104
-Benjamin D. Smith and Guy E. Garnett: The Self-Supervising Machine - page 108
-Aaron Albin, Sertan Senturk, Akito Van Troyer, Brian Blosser, Oliver Jan and Gil Weinberg: Beatscape, a mixed virtual-physical environment for musical ensembles - page 112
-Marco Fabiani, Gaël Dubus and Roberto Bresin: MoodifierLive: Interactive and collaborative expressive music performance on mobile devices - page 116
-Benjamin Schroeder, Marc Ainger and Richard Parent: A Physically Based Sound Space for Procedural Agents - page 120
-Francisco Garcia, Leny Vinceslas, Esteban Maestre and Josep Tubau Acquisition and study of blowing pressure profiles in recorder playing - page 124
-Anders Friberg and Anna Källblad:Experiences from video-controlled sound installations - page 128
-Nicolas d’Alessandro, Roberto Calderon and Stefanie Müller: ROOM#81 —Agent-Based Instrument for Experiencing Architectural and Vocal Cues - page 132

Demo session C — Monday 30 May 13:30–14:30 
-Yasuo Kuhara and Daiki Kobayashi: Kinetic Particles Synthesizer Using Multi-Touch Screen Interface of Mobile Devices - page 136
-Christopher Carlson, Eli Marschner and Hunter Mccurry: The Sound Flinger: A Haptic Spatializer - page 138
-Ravi Kondapalli and Benzhen Sung: Daft Datum – an Interface for Producing Music Through Foot-Based Interaction - page 140
-Charles Martin and Chi-Hsia Lai: Strike on Stage: a percussion and media performance - page 142

Paper session D — Monday 30 May 14:30–15:30 
-Baptiste Caramiaux, Patrick Susini, Tommaso Bianco, Frédéric Bevilacqua, Olivier Houix, Norbert Schnell and Nicolas Misdariis: Gestural Embodiment of Environmental Sounds: an Experimental Study - page 144
-Sebastian Mealla, Aleksander Valjamae, Mathieu Bosi and Sergi Jorda: Listening to Your Brain: Implicit Interaction in Collaborative Music Performances - page 149
-Dan Newton and Mark Marshall: Examining How Musicians Create Augmented Musical Instruments - page 155
 
Paper session E — Monday 30 May 16:00–17:00 
-Zachary Seldess and Toshiro Yamada: Tahakum: A Multi-Purpose Audio Control Framework - page 161
-Dawen Liang, Guangyu Xia and Roger Dannenberg: A Framework for Coordination and Synchronization of Media - page 167
-Edgar Berdahl and Wendy Ju: Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform  - page 173
 
Paper session F — Tuesday 31 May 09:00–10:50 
-Nicholas J. Bryan and Ge Wang: Two Turntables and a Mobile Phone - page 179
-Nick Kruge and Ge Wang: MadPad: A Crowdsourcing System for Audiovisual Sampling - page 185
-Patrick O’Keefe and Georg Essl: The Visual in Mobile Music Performance - page 191
-Ge Wang, Jieun Oh and Tom Lieber: Designing for the iPad: Magic Fiddle - page 197
-Benjamin Knapp and Brennon Bortz: MobileMuse: Integral Music Control Goes Mobile - page 203
-Stephen Beck, Chris Branton, Sharath Maddineni, Brygg Ullmer and Shantenu Jha: Tangible Performance Management of Grid-based Laptop Orchestras - page 207
 
Poster session G— Tuesday 31 May 13:30–14:30
-Smilen Dimitrov and Stefania Serafin: Audio Arduino—an ALSA (Advanced Linux Sound Architecture) audio driver for FTDI-based Arduinos - page 211
-Seunghun Kim and Woon Seung Yeo: Musical control of a pipe based on acoustic resonance - page 217
-Anne-Marie Hansen, Hans Jørgen Andersen and Pirkko Raudaskoski: Play Fluency in Music Improvisation Games for Novices - page 220
-Izzi Ramkissoon: The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance - page 224
-Ajay Kapur, Michael Darling, James Murphy, Jordan Hochenbaum, Dimitri Diakopoulos and Trimpin: The KarmetiK NotomotoN: A New Breed of Musical Robot for Teaching and Performance - page 228
-Adrian Barenca Aliaga and Giuseppe Torre:  The Manipuller: Strings Manipulation and Multi-Dimensional Force Sensing - page 232
-Alain Crevoisier and Cécile Picard-Limpens: Mapping Objects with the Surface Editor - page 236
-Jordan Hochenbaum and Ajay Kapur: Adding Z-Depth and Pressure Expressivity to Tangible Tabletop Surfaces - page 240
-Andrew Milne, Anna Xambó, Robin Laney, David B. Sharp, Anthony Prechtl and Simon Holland: Hex Player—A Virtual Musical Controller - page 244
-Carl Haakon Waadeland: Rhythm Performance from a Spectral Point of View - page 248
-Josep M Comajuncosas, Enric Guaus, Alex Barrachina and John O’Connell: Nuvolet : 3D gesture-driven collaborative audio mosaicing - page 252
-Erwin Schoonderwaldt and Alexander Refsum Jensenius: Effective and expressive movements in a French-Canadian fiddler’s performance - page 256
-Daniel Bisig, Jan Schacher and Martin Neukom: Flowspace – A Hybrid Ecosystem - page 260
-Marc Sosnick and William Hsu: Implementing a Finite Difference-Based Real-time Sound Synthesizer using GPUs - page 264
-Axel Tidemann: An Artificial Intelligence Architecture for Musical Expressiveness that Learns by Imitation - page 268
-Luke Dahl, Jorge Herrera and Carr Wilkerson: TweetDreams: Making music with the audience and the world using real-time Twitter data - page 272
-Lawrence Fyfe, Adam Tindale and Sheelagh Carpendale: JunctionBox: A Toolkit for Creating Multi-touch Sound Control Interfaces - page 276
-Andrew Johnston: Beyond Evaluation: Linking Practice and Theory in New Musical Interface Design - page 280
-Phillip Popp and Matthew Wright: Intuitive Real-Time Control of Spectral Model Synthesis - page 284
-Pablo Molina, Martin Haro and Sergi Jordà: BeatJockey: A new tool for enhancing DJ skills - page 288
-Jan Schacher and Angela Stoecklin: Traces – Body, Motion and Sound - page 292
-Grace Leslie and Tim Mullen: MoodMixer: EEG-based Collaborative Sonification - page 296
-Ståle A. Skogstad, Kristian Nymoen, Yago de Quay and Alexander Refsum Jensenius: OSC Implementation and Evaluation of the Xsens MVN suit - page 300
-Lonce Wyse, Norikazu Mitani and Suranga Nanayakkara: The effect of visualizing audio targets in a musical listening and performance task - page 304
-Freed Adrian, John Maccallum and Andrew Schmeder: Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP - page 308
-Kristian Nymoen, Ståle A. Skogstad and Alexander Refsum Jensenius: SoundSaber —A Motion Capture Instrument - page 312
-Øyvind Brandtsegg, Sigurd Saue and Thom Johansen: A modulation matrix for complex parameter sets - page 316

Demo session H— Tuesday 31 May 13:30–14:30 
-Yu-Chung Tseng, Che-Wei Liu, Tzu-Heng Chi and Hui-Yu Wang: Sound Low Fun- page 320
-Edgar Berdahl and Chris Chafe: Autonomous New Media Artefacts (AutoNMA) - page 322
-Min-Joon Yoo, Jin-Wook Beak and In-Kwon Lee: Creating Musical Expression using Kinect - page 324
-Staas de Jong: Making grains tangible: microtouch for microsound - page 326
Baptiste Caramiaux, Frederic Bevilacqua and Norbert Schnell: Sound Selection by Gestures - page 329

Paper session I — Tuesday 31 May 14:30–15:30 
-Hernán KerlleÃevich, Manuel Eguia and Pablo Riera: An Open Source Interface based on Biological Neural Networks for Interactive Music Performance - page 331
-Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping - page 337
-Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: A Machine Learning Toolbox For Musician Computer Interaction - page 343
 
Paper session J — Tuesday 31 May 16:00–17:00 
-Elena Jessop, Peter Torpey and Benjamin Bloomberg: Music and Technology in Death and the Powers - page 349
-Victor Zappi, Dario Mazzanti, Andrea Brogni and Darwin Caldwell: Design and Evaluation of a Hybrid Reality Performance - page 355
-Jérémie Garcia, Theophanis Tsandilas, Carlos Agon and Wendy Mackay: InkSplorer : Exploring Musical Ideas on Paper and Computer - page 361

Paper session K — Wednesday 1 June 09:00–10:30 
-Pedro Lopes, Alfredo Ferreira and Joao Madeiras Pereira: Battle of the DJs: an HCI perspective of Traditional, Virtual, Hybrid and Multitouch DJing - page 367
-Adnan Marquez-Borbon, Michael Gurevich, A. Cavan Fyans and Paul Stapleton: Designing Digital Musical Interactions in Experimental Contexts - page 373
-Jonathan Reus: Crackle: A mobile multitouch topology for exploratory sound interaction - page 377
-Samuel Aaron, Alan F. Blackwell, Richard Hoadley and Tim Regan: A principled approach to developing new languages for live coding - page 381
-Jamie Bullock, Daniel Beattie and Jerome Turner: Integra Live: a new graphical user interface for live electronic music - page 387

Paper session L — Wednesday 1 June 11:00–12:30 
-Jung-Sim Roh, Yotam Mann, Adrian Freed and David Wessel: Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers - page 393
-Mark Marshall and Marcelo Wanderley: Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument - page 399
-Dimitri Diakopoulos and Ajay Kapur: HIDUINO: A firmware for building driverless USB-MIDI devices using the Arduino microcontroller - page 405
-Emmanuel Flety and Côme Maestracci: Latency improvement in sensor wireless transmission using IEEE 802.15.4 - page 409
-Jeff Snyder: The Snyderphonics Manta, a Novel USB Touch Controller - page 413
 
Poster session M — Wednesday 1 June 13:30–14:30
-William Hsu: On Movement, Structure and Abstraction in Generative Audiovisual Improvisation - page 417
-Claudia Robles Angel: Creating Interactive Multimedia Works with Bio-data - page 421
-Paula Ustarroz: TresnaNet: musical generation based on network protocols - page 425
-Matti Luhtala, Tiina Kymäläinen and Johan Plomp: Designing a Music Performance Space for Persons with Intellectual Learning Disabilities - page 429
-Tom Ahola, Teemu Ahmaniemi, Koray Tahiroglu, Fabio Belloni and Ville Ranki: Raja —A Multidisciplinary Artistic Performance - page 433
-Emmanuelle Gallin and Marc Sirguy: Eobody3: A ready-to-use pre-mapped & multi-protocol sensor interface- page 437
-Rasmus Bååth, Thomas Strandberg and Christian Balkenius: Eye Tapping: How to Beat Out an Accurate Rhythm using Eye Movements - page 441
-Eric Rosenbaum: MelodyMorph: A Reconfigurable Musical Instrument - page 445
-Karmen Franinovic: Flo)(ps: Between Habitual and Explorative Action-Sound Relationships - page 448
-Margaret Schedel, Rebecca Fiebrink and Phoenix Perry: Wekinating 000000Swan: Using Machine Learning to Create and Control Complex Artistic Systems - page 453
-Carles F. Julià, Daniel Gallardo and Sergi Jordà: MTCF: A framework for designing and coding musical tabletop applications directly in Pure Data - page 457
-David Pirrò and Gerhard Eckel: Physical modelling enabling enaction: an example - page 461
-Thomas Mitchell and Imogen Heap: SoundGrasp: A Gestural Interface for the Performance of Live Music - page 465
-Tim Mullen, Richard Warp and Adam Jansch: Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface - page 469
-Stefano Papetti, Marco Civolani and Federico Fontana: Rhythm’n’Shoes: a wearable foot tapping interface with audio-tactile feedback - page 473
-Cumhur Erkut, Antti Jylhä and Reha Di¸sçio˘glu: A structured design and evaluation model with application to rhythmic interaction displays - page 477
-Marco Marchini, Panos Papiotis, Alfonso Perez and Esteban Maestre: A Hair Ribbon Deflection Model for Low-Intrusiveness Measurement of Bow Force in Violin Performance - page 481
-Jonathan Forsyth, Aron Glennon and Juan Bello: Random Access Remixing on the iPad - page 487
-Erika Donald, Ben Duinker and Eliot Britton: Designing the EP trio: Instrument identities, control and performance practice in an electronic chamber music ensemble - page 491
-Cavan Fyans and Michael Gurevich: Perceptions of Skill in Performances with Acoustic and Electronic Instruments - page 495
-Hiroki Nishino: Cognitive Issues in Computer Music Programming - page 499
-Roland Lamb and Andrew Robertson: Seaboard: a new piano keyboard-related interface combining discrete and continuous control - page 503
-Gilbert Beyer and Max Meier: Music Interfaces for Novice Users: Composing Music on a Public Display with Hand Gestures - page 507
-Birgitta Cappelen and Anders-Petter Andersson: Expanding the role of the instrument - page 511
-Todor Todoroff: Wireless Digital/Analog Sensors for Music and Dance Performances - page 515
-Trond Engum: Real-time control and creative convolution— exchanging techniques between distinct genres - page 519
-Andreas Bergsland: The Six Fantasies Machine – an instrument modelling phrases from Paul Lansky’s Six Fantasies - page 523
 
Demo session N — Wednesday 1 June 13:30–14:30
-Jan Trützschler von Falkenstein: Gliss: An Intuitive Sequencer for the iPhone and iPad - page 527
-Jiffer Harriman, Locky Casey, Linden Melvin and Mike Repper: Quadrofeelia — A New Instrument for Sliding into Notes - page 529
-Johnty Wang, Nicolas D’Alessandro, Sidney Fels and Bob Pritchard: SQUEEZY: Extending a Multi-touch Screen with Force Sensing Objects for Controlling Articulatory Synthesis - page 531
-Souhwan Choe and Kyogu Lee: SWAF: Towards a Web Application Framework for Composition and Documentation of Soundscape - page 533
-Norbert Schnell, Frederic Bevilacqua, Nicolas Rasamimana, Julien Blois, Fabrice Guedy and Emmanuel Flety: Playing the ""MO"" —Gestural Control and Re-Embodiment of Recorded Sound and Music - page 535
-Bruno Zamborlin, Marco Liuni and Giorgio Partesana: (LAND)MOVES - page 537
-Bill Verplank and Francesco Georg: Can Haptics make New Music? —Fader and Plank Demos - page 53",Proceedings of the International Conference on New Interfaces for Musical Expression,https://core.ac.uk/download/30857249.pdf,,,,core
84835555,2013-06-27T00:00:00,"Tato práce se zabývá využitím ultrazvukových dálkových senzorů nainstalovaných na modelu myoelektrické protetické náhrady horní končetiny a návrhem metody vytvářející trojrozměrnou mapu okolí protetické náhrady. Na základě provedené rešerže týkající se robotického mapování byla pro modelování okolí protetické náhrady zvolena metoda založená na mřížce obsazenosti. Program implementovaný v prostředí Matlab rozděluje okolní prostor protetické náhrady do stejně velkých buněk, které uchovávají hodnotu pravděpodobnosti, že se na jejich místě nachází překážka. Senzorická měření jsou v reálném čase zpracovávána a podle získávaných dat jsou zmíněné pravděpodobnosti obsazenosti buněk průběžně aktualizovány podle Bayesovy věty. Ve vytvářené mapě lze pak identifikovat vzdálenosti nejbližších překážeka zaznamenávat, jak se jejich vzdálenost od paže mění v čase. Využití umělých neuronových sítí, pro jejichž aplikaci je použit Neural Network Toolbox v prostředí Matlab, umožní podle minulých hodnot vzdáleností získaných v předchozích časových krocích predikovat budoucí vzdálenost v následujícím časovém kroku. Toto by mohlo být základem antikolizního systému, který může řídící systém protetické náhrady upozornit na možnost srážky s objektem.This bachelor thesis is focused on utilization of ultrasonic range sensors which are installed on a model of myoelectric upper-limb prosthesis. The aim of the project is to design a method of creating three-dimensional map of proximate environment. The method of occupancy grid mapping was chosen to map the environment. Program implemented in Matlab divides the proximate environment of the prosthesis into cells of same size. Each cell holds the value of probability that the section of space allocated by the cell is occupied by any object. Sensory readings are utilized in real-time and used for updating occupancy probability of the cells using Bayes rule. Occupancy grid then enables identification of locations and distances of the closest objects to the prosthesis. It´s possible to determine, how these distances change in time. Utilization of artificial neural networks using Neural Network Toolbox in Matlab may predict future values of distances according to previous values measured in previous time-steps. The proposed system could be a part of anticollisional system, which may alert driving system of the prosthesis to prevent collisons with proximate objects",Design methods for evaluation 3D space around the upper limb prosthesis,,Czech Technical University in Prague. Computing and Information Centre.,,,core
11289518,2010-11-05T00:00:00,"Olfactory stimuli are represented in a highdimensional space by neural networks of the olfactory system. A great deal of research in olfaction has focused on this representation within the first processing stage, the olfactory bulb (vertebrates) or antennal lobe (insects) glomeruli. In particular the mapping of chemical stimuli onto olfactory glomeruli and the relation of this mapping to perceptual qualities have been investigated. While a number of studies have illustrated the importance of inhibitory networks within the olfactory bulb or the antennal lobe for the shaping and processing of olfactory information, it is not clear how exactly these inhibitory networks are organized to provide filtering and contrast enhancement capabilities. In this work the aim is to study the topology of the proposed networks by using software simulations and hardware implementation. While we can study the dependence of the activity on each parameter of the theoretical models with the simulations, it is important to understand whether the models can be used in robotic applications for real-time odor recognition. We present the results of a linear simulation, a spiking simulation with I&F neurons and a real-time hardware emulation using neuromorphic VLSI chips. We used an input data set of neurophysiological recordings from olfactory receptive neurons of insects, especially Drosophila",Exploring olfactory sensory networks: Simulations and hardware emulation,http://www.zora.uzh.ch/60744/1/05709623.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/BIOCAS.2010.5709623,,core
144064376,2012-03-09T00:00:00,"Robôs manipuladores espaciais serão aplicados, em um futuro próximo, em serviços de resgate e manutenção de naves e satélites em órbita. O estudo e o desenvolvimento de controladores para esse tipo de sistema é fundamental para que essas aplicações se tornem realidade. Nesta tese, uma plataforma experimental é construída para possibilitar a avaliação comportamental desse tipo de sistema. Baseada em um módulo de flutuação por colchões de ar, é composta por uma base livre, elos conectados por juntas e efetuadores. Duas possibilidades de flutuação foram definidas para tornar a estrutura mais versátil, a primeira utiliza uma câmara de ar na mesa de apoio e a segunda utiliza câmaras de ar na base e em cada junta do robô. Sua estrutura mecânica modular permite diversas configurações, com um ou dois braços compostos por elos rígidos ou flexíveis. Toda a eletrônica de comando e a alimentação dos componentes do robô são alocadas em sua base flutuante, baseando a comunicação do sistema com o computador remoto em um padrão de comunicação sem fio. O software de controle, desenvolvido em Matlab e residente no computador remoto, apresenta uma interface amigável e intuitiva, possibilitando a utilização tanto do UARM como do robô de base livre flutuante para testes simulados e experimentais de sistemas de controle. A principal característica dos manipuladores espaciais é o acoplamento dinâmico entre a base e o braço robótico. A fim de evitar as complicações envolvidas no mapeamento cinemático desses sistemas, o problema de acompanhamento de trajetória é formulado diretamente no espaço da tarefa. Assim as posições do efetuador do manipulador são diretamente controladas. O equacionamento dinâmico do manipulador de base livre flutuante é descrito a partir do conceito do Manipulador Dinamicamente Equivalente. Propõe-se uma solução de controle adaptativo robusto baseado no critério H Infinito para lidar com o problema de acompanhamento de trajetória sujeito a incertezas no modelo e distúrbios externos. A adaptabilidade das redes neurais é aliada à robustez definida por um controlador H Infinito  não linear, compondo diferentes técnicas desenvolvidas de acordo com o conhecimento e a disponibilidade do modelo do robô para o controlador. A análise de resultados de simulação e de experimentos realizados no UARM mostraram a aplicabilidade dos métodos, assim como sua capacidade de robustez. Gráficos ilustraram o procedimento do acompanhamento de trajetória realizado pelo efetuador do manipulador espacial identificando a ação das leis de controle propostas. Uma comparação numérica entre as estratégias foi estabelecida por índices de desempenho relacionados ao consumo de energia e ao erro de acompanhamentoSpace manipulators robots will be applied, in a near future, in rescue services and maintenance of spacecraft and satellites in orbit. The study and development of controllers for this type of system is crucial to ensure that those applications become reality. At this thesis, an experimental platform is built to enable behavioral assessment of this type of system. Based on a floating module by air bearings, it is composed by a free base, links connected by joints and end-effectors. Two possibilities of fluctuation were set to make the structure more versatile. The first uses an air chamber in the support desk and the second uses air chambers at the base and in each joint of the robot. Its modular mechanical structure allows a variety of configurations, with one or two arms which may be composed of flexible or rigid links. The entire command electronics and the power of the robots components are allocated in its floating base, basing the system communication with the remote computer in a wireless communication standard. The control software, developed in Matlab and residing on the remote computer, presents a friendly and intuitive interface, enabling the use of both the UARM and the free-floating base robot for simulated and experimental testing of control systems. The main characteristic of space manipulators is the dynamic coupling between the base and the robotic arm. In order to avoid the complications involved in kinematic mapping of these systems, the problem of trajectory tracking is formulated directly in task space. So the positions of the manipulator end-effector are directly controlled. The dynamic equation of the free-floating manipulator is described from the concept of Dynamically Equivalent Manipulator. A solution of adaptive robust control is proposed, based on H¥ criterion to deal with the problem of trajectory tracking subject to uncertainties in the model and external disturbances. The adaptability of neural networks is combined with robustness defined by a nonlinear H Infinite controller composing different techniques developed in accordance with the knowledge and the availability of the robots model to the controller. The analysis of results of simulation and experiments performed in UARM showed the applicability of the methods, as well as its capacity for robustness. Graphs have illustrated the trajectory tracking procedure conducted by the end-effector of the space manipulator identifying the action of control laws proposed. A numerical comparison between the strategies was provided by performance indices related to energy consumption and the tracking erro",Assembly and Nonlinear H Infinitye Control of Free-Floating Base Space Manipulators.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.18.2012.tde-14022012-101404,,core
144062887,2011-10-25T00:00:00,"Os métodos de Aprendizagem por Reforço (AR) se mostram adequados para problemas de tomadas de decisões em diversos domínios por sua estrutura flexível e adaptável. Apesar de promissores, os métodos AR frequentemente tem seu campo de atuação prático restrito a problemas com espaço de estados de pequeno ou médio porte devido em muito à forma com que realizam a estimativa da função de avaliação. Nesta tese, uma nova abordagem de AR, denominada de Agente Topológico de Aprendizagem por Reforço (ATAR), inspirada em aprendizagem latente, é proposta para acelerar a aprendizagem por reforço através de um mecanismo alternativo de seleção dos pares estado-ação para atualização da estimativa da função de avaliação. A aprendizagem latente refere-se à aprendizagem animal que ocorre na ausência de reforço e que não é aparente até que um sinal de reforço seja percebido pelo agente. Este aprendizado faz com que um agente aprenda parcialmente uma tarefa mesmo antes que este receba qualquer sinal de reforço. Mapas Cognitivos são usualmente empregados para codificar a informação do ambiente em que o agente está imerso. Desta forma, o ATAR usa um mapa topológico, baseado em Mapas Auto-Organizáveis, para realizar as funções do mapa cognitivo e permitir um mecanismo simples de propagação das atualizações. O ATAR foi testado, em simulação, para planejamento de navegação de um robô móvel em  ambientes inicialmente desconhecidos e não-estruturados. Comparações com outros seis algoritmos AR avaliaram comparativamente o desempenho do agente proposto na navegação. Os resultados obtidos são promissores e comparáveis com os algoritmos AR mais rápidos testados, alcançando em alguns ensaios desempenho superior aos dos demais algoritmos - principalmente nas simulações que consideram situações observadas em ambientes não-estruturados. Três características do ATAR original foram alteradas para tornar ainda mais viável sua aplicação prática: (i) mudanças no mapa topológico para reduzir o número de vértices, (ii) mudança  na heurística usada na seleção das ações do agente e (iii) variações na estratégia de exploração do ATAR. Do ponto (i), foi proposto e implementado um novo mapa topológico, o Mapa Topológico Incremental Classificador  MTIC, que a partir da classificação dos estados do ambiente gera os vértices de uma triangularização de Watson. O ponto (ii) criou um método aplicável a outros problemas de planejamento de trajetória em grafos denominado de Melhoria das trajetórias por detecção de ponto interior. O terceiro estudou estratégias direcionadas de exploração como uma opção para acelerar o aprendizado do ATAR.Reinforcement Learning (RL) methods have shown to be a good choice for decision-making problems due to their flexible and adaptive characteristics. Despite such promising features, RL methods often have their practical application restricted to small or medium size (at state, or state-action, space) problems mainly because of their standard strategies for value function estimation. In this thesis, a new RL approach, called \""Topological Reinforcement Learning Agent\"" - TRLA, is proposed  to accelerate learning through an alternative mechanism to update the state-action value function. TRLA is inspired in latent learning, which refers to animal learning that occurs in the absence of reinforcements and that is not visible until an environmental reinforcement is perceived. This concept considers that part of a task can be learned even before the agent receives any indication of how to perform such a task. Cognitive Maps are usually used to encode  information about the environment where the agent is immersed. Thus, the TRLA uses a topological map, based on  Self-Organized Maps, to implement cognitive map functions and permit a new simple mechanism to execute the propagation of state-action updates. The chosen problem to test TRLA is the simulation of a mobile robot navigation in some initially unknown and unstructured environments. Performance comparisons of the TRLA with six other RL algorithms were carried out to the execution of the navigation task. The obtained results are very promising and comparable with some of faster RL algorithms simulated. In some experiments, the TRLA\'s performance overcomes the others  especially in simulations with unstructured environments. Three characteristics of the original TRLA were modified to make it more suitable for real implementations: (i) changes in the topological map construction to reduce the vertices number, (ii) changes in the agents heuristic for action selection, and (iii) variations on the TRLAs strategy for exploration of the state-action space. In (i), a new procedure to construct topological maps was proposed and implemented, the Incremental Classifier Topological Map  ICTM, which generates the vertices for a Watsons triangulation from the classification of the input states. In (ii), it was proposed a method to optimize trajectory planning problems based on graphs, denoted \""trajectory improvement from inner point detection\"". The third point considers directed exploration strategies as an option for TRLA\'s learning acceleration",Topological reinforcement learning agent,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.18.2004.tde-21102011-081848,,core
268809713,2011-12-01T08:00:00,"This thesis addresses an expansion of the control programs for the Cyton Alpha 7D 1G arm. The original control system made use of configurable software which exploited the arm’s seven degrees of freedom and kinematic redundancy to control the arm based on desired behaviors that were configured off-line. The inclusions of the GraspIt! grasp planning simulator and toolkit enables the Cyton Alpha to be used in more proactive on-line grasping problems, as well as, presenting many additional tools for on-line learning applications. In short, GraspIt! expands what is possible with the Cyton Alpha to include many machine learning tools and opportunities for future research. Noteworthy features of GraspIt!:
• A 3D user interface allowing the user to see and interact virtual objects, obstacles, and robots, in addition to a 3D representation of the Cyton Alpha
• A collision detection and contact determination system within simulation • On-line grasp analysis routines
• Visualization methods for determining the weak points within a grasp, as well as, creating projections of grasp quality and ability to resist dynamic forces.
• Computation of numerical grasp quality metrics and visualization methods for proposed grasps
• Dynamics engine
• Support for lower-dimensional hand posture subspaces
• Interaction with sensors (Flock of Birds tracker) and hardware (Pioneer robot) within simulation
• GraspIt! can generate huge databases of labeled grasp data, which can be used for data-driven grasp-planning algorithms and has built in support for the Columbia Grasp Database.
By making use of the GraspIt! simulator, it is possible to test algorithms for grasp manipulation, grasp planning, or grasp synthesis more quickly and with greater repeatability than would be possible on the real robot. Contributions of this system include:
1. A joint based 3D rendering of the Cyton Alpha 7D 1G arm
2. Simulated bodies for several objects in the DI Lab
3. Support for multiple representations of joint data within three-dimensional space
• Euler Angles
• Quaternions
• Denavit-Hartenberg Parameters
4. Framework for future work in grasp-planning, grasp synthesis, cooperative grasping tasks, and transfer learning applications with the Cyton Alpha arm",cytonGrasp: Cyton Alpha Controller via GraspIt! Simulation,https://core.ac.uk/download/268809713.pdf,TRACE: Tennessee Research and Creative Exchange,,,core
380977013,2013-03-26T00:00:00,"This thesis deals with Statistical Relational Learning (SRL), a research area combining principles and ideas from three important subfields of Artificial Intelligence: machine learn- ing,  knowledge representation and reasoning on uncertainty.  Machine learning is the study of systems that improve their behavior over time with experience; the learning process typi- cally involves a search through various generalizations of the examples, in order to discover regularities or classification rules. A wide variety of machine learning techniques have been developed in the past fifty years, most of which used propositional logic as a (limited) represen- tation language. Recently, more expressive knowledge representations have been considered, to cope with a variable number of entities as well as the relationships that hold amongst them. These representations are mostly based on logic that, however, has limitations when reason- ing on uncertain domains. These limitations have been lifted allowing a multitude of different formalisms combining probabilistic reasoning with logics,  databases or logic programming, where probability theory provides a formal basis for reasoning on uncertainty.



In this thesis we consider in particular the proposals for integrating probability in Logic Programming, since the resulting probabilistic logic programming languages present very in- teresting computational properties.  In Probabilistic Logic Programming, the so-called ""dis- tribution  semantics"" has gained a wide popularity.  This semantics was introduced for the PRISM language (1995) but is shared by many other languages: Independent Choice Logic, Stochastic Logic Programs, CP-logic, ProbLog and Logic Programs with Annotated Disjunc- tions (LPADs).  A program in one of these languages defines a probability distribution over normal logic programs called worlds.  This distribution is then extended to queries and the probability of a query is obtained by marginalizing the joint distribution of the query and the programs. The languages following the distribution semantics differ in the way they define the distribution over logic programs.

The first part of this dissertation presents techniques for learning probabilistic logic pro- grams  under the distribution semantics.  Two problems are considered:  parameter learning and structure learning, that is, the problems of inferring values for the parameters or both the structure and the parameters of the program from data.  This work contributes an algorithm for parameter learning, EMBLEM, and two algorithms for structure learning (SLIPCASE and SLIPCOVER) of probabilistic logic programs (in particular LPADs). EMBLEM is based on the Expectation Maximization approach and computes the expectations directly on the Binary De- cision Diagrams that are built for inference. SLIPCASE performs a beam search in the space of LPADs while SLIPCOVER performs a beam search in the space of probabilistic clauses and a greedy search in the space of LPADs, improving SLIPCASE performance. All learning approaches have been evaluated in several relational real-world domains.



The second part of the thesis concerns the field of Probabilistic Description Logics, where we consider a logical framework suitable for the Semantic Web. Description Logics (DL) are a family of formalisms for representing knowledge. Research in the field of knowledge repre- sentation and reasoning is usually focused on methods for providing high-level descriptions of the world that can be effectively used to build intelligent applications.

Description Logics have been especially effective as the representation language for for- mal ontologies. Ontologies model a domain with the definition of concepts and their properties and relations.   Ontologies are the structural frameworks for organizing information and are used in artificial intelligence, the Semantic Web, systems engineering, software engineering, biomedical informatics, etc. They should also allow to ask questions about the concepts and in- stances described, through inference procedures. Recently, the issue of representing uncertain information in these domains has led to probabilistic extensions of DLs.



The contribution of this dissertation is twofold: (1) a new semantics for the Description Logic SHOIN(D) , based on the distribution semantics for probabilistic logic programs, which embeds probability; (2) a probabilistic reasoner for computing the probability of queries from uncertain knowledge bases following this semantics. The explanations of queries are encoded in Binary Decision Diagrams, with the same technique employed in the learning systems de- veloped for LPADs. This approach has been evaluated on a real-world probabilistic ontology",Integration of Logic and Probability in Terminological and Inductive Reasoning,,,10.5072//843,,core
18277918,2013-06-01T00:00:00,"This research takes an application-specific approach to investigate, extend and implement the state of the art in the fields of both visual information retrieval and machine learning, bridging the gap between theoretical models and real world applications. During an image-guided neurosurgery, path planning remains the foremost and hence the most important step to perform an operation and ensures the maximum resection of an intended target and minimum sacrifice of health tissues. In this investigation, the technique of content-based image retrieval (CBIR) coupled with machine learning algorithms are exploited in designing a computer aided path planning system (CAP) to assist junior doctors in planning surgical paths while sustaining the highest precision. Specifically, after evaluation of approaches of sparse coding and K-means in constructing a codebook, the model of sparse codes of 3D SIFT has been furthered and thereafter employed for retrieving, The novelty of this work lies in the fact that not only the existing algorithms for 2D images have been successfully extended into 3D space, leading to promising results, but also the application of CBIR, that is mainly in a research realm, to a clinical sector can be achieved by the integration with machine learning techniques. Comparison with the other four popular existing methods is also conducted, which demonstrates that with the implementation of sparse coding, all methods give better retrieval results than without while constituting the codebook, implying the significant contribution of machine learning techniques",3D CBIR with sparse coding for image-guided neurosurgery,https://core.ac.uk/download/18277918.pdf,'Elsevier BV',10.1016/j.sigpro.2012.10.020,,core
211463666,2010-01-01T00:00:00,"The present dissertation investigates the airflow pattern in naturally ventilated buildings using advanced computer simulation techniques such as field or Computational Fluid Dynamics (CFD) models. In particular, two-equation turbulence models, such as the Standard k-ε, the RNG k-ε, the “Realizable” k-ε, as well as the Reynolds Stress Model (RSM) are used to predict the flow field in and around the buildings considered. In the case of the Standard k-ε model the potential of improving the prediction accuracy is investigated by means of the following ways: (a) Application of a Two-layer Standard k-ε model, and (b) Modification according to the inlet flow variables profiles of the Atmospheric Boundary Layer (ABL). The aforementioned turbulence models and the proposed modifications are passive ventilation of a pilot building and of a real-scale building subject to actual passive ventilation (for which an experimental procedure is conducted). The CFD model is then used for the prediction of thermal comfort in naturally ventilated buildings by implementing the most appropriate thermal comfort indices (TCI), such as the Predicted Mean Vote (PMV), which, according to literature information, is modified for pure natural ventilation as follows: (a) Extension to account for the metabolic-rate reduction expected in warm environments and for the expectancy factor due to occupant’s habitat, and (b) Correction of the PMV for humidity effects, based on the Standard Effective Temperature (SET*). The present thermal comfort model is completed with the implementation of the Percentage Dissatisfied (PD) index to account for discomfort due to air draughts as well. Additionally, Indoor Air Quality (IAQ) indices are also implemented in the CFD model, i.e. the ventilation effectiveness related to the removal of common pollutants. Following the aforementioned coupled model (CFD-TC/IAQ) a database of input-output pairs is formed, which is used to train and validate radial basis functions (RBF) artificial neural networks (ANN) according to the fuzzy means method. The ANN distributions are then utilized to formulate a multiobjective optimization problem, which accounts for the available thermal comfort and indoor air quality indices, occupant activity level and special constraints provided by design guidelines. The problem is confronted using a gradient-based algorithm associated with a special handling of constraints and of the initialization space of the design variables (Initialization Grid Concept, IGC). Additionally, a technique to account for the non-dominated solutions of the objective functions according to the Pareto criteria is also introduced. Finally, for each case studied the optimal designs are presented by means of nomographs, which may serve as look-up tables for either decision-making strategies or automated systems to create “Intelligent Buildings”.Στην παρούσα διατριβή γίνεται διερεύνηση του πεδίου ροής σε φυσικά αεριζόμενα κτίρια με εξελιγμένες υπολογιστικές μεθόδους προσομοίωσης, όπως είναι τα μοντέλα πεδίου ή μοντέλα υπολογιστικής ρευστοδυναμικής (Computational Fluid Dynamics, CFD). Συγκεκριμένα, εφαρμόζονται μοντέλα τύρβης δύο εξισώσεων, όπως είναι το συμβατικό (Standard) k-ε, το επανακανονικοποιημένο (RNG) k-ε, το μοντέλο εφικτών λύσεων (Realizable) k-ε καθώς και το μοντέλο εξισώσεων τάσεων Reynolds (RSM). Στην περίπτωση του συμβατικού k-ε διερευνάται η δυνατότητα βελτίωσης της πρόβλεψης με δύο τρόπους: (α) Εφαρμογή ενός μοντέλου δύο στρωμάτων, και (β) Τροποποίηση με βάση τις κατανομές των μεταβλητών εισόδου (Μοντέλο k-ε βασισμένο στο Ατμοσφαιρικό Οριακό Στρώμα, ABL-based Standard k-ε model). Τα παραπάνω μοντέλα και οι προτεινόμενες τροποποιήσεις εφαρμόζονται σε πιλοτικό κτίριο (πείραμα αεροσήραγγας που βρίσκεται στη βιβλιογραφία) και σε κτίριο πραγματικής κλίμακας (Διεξαγωγή πειραματικής διαδικασίας). Επιχειρείται η σύζευξη του CFD μοντέλου με αντιπροσωπευτικά μοντέλα υπολογισμού δεικτών θερμικής άνεσης, όπως είναι ο μέσος αναμενόμενος θερμικός δείκτης (Predicted Mean Vote, PMV), ο οποίος τροποποιείται κατάλληλα για αμιγώς φυσικά αεριζόμενους χώρους με δύο τρόπους: (α) Επέκταση του PMV για αμιγώς παθητικά αεριζόμενα κτίρια, λαμβάνοντας υπόψη την ταπείνωση του ρυθμού μεταβολισμού του ενοίκου σε θερμές συνθήκες και τον συντελεστή προσδοκίας βάσει καταγωγής, και (β) Διόρθωση του PMV για την επίδραση της υγρασίας, με βάση την πρότυπη ενεργό θερμοκρασία (SET*). Το μοντέλο θερμικής άνεσης ολοκληρώνεται με την ενσωμάτωση του δείκτη δυσφορίας λόγω ελκυσμού (Percentage Dissatisfied, PD). Συμπληρωματικά, ενσωματώνονται και δείκτες ποιότητας εσωτερικού αέρα (Indoor Air Quality, IAQ) με βάση την εκτόπιση συνήθων ρύπων από την κατειλημμένη ζώνη. Με χρήση του παραπάνω συζευγμένου μοντέλου CFD-TCI/IAQ διαμορφώνεται μια βάση δεδομένων μεταβλητών εισόδου-εξόδου, η οποία χρησιμοποιείται για την εκπαίδευση και επαλήθευση ενός τεχνητού νευρωνικού δικτύου (Artificial Neural Network, ANN) ακτινικών συναρτήσεων βάσης (Radial Basis Function, RBF) με την μέθοδο των ασαφών μέσων (Fuzzy Means Method). Οι ΑΝΝ κατανομές χρησιμοποιούνται για την διαμόρφωση ενός προβλήματος πολυκριτηριακής βελτιστοποίησης με βάση τους διατιθέμενους δείκτες TC-IAQ, το αναμενόμενο επίπεδο δραστηριότητας και ειδικούς περιορισμούς που βρίσκονται στη βιβλιογραφία. Το πρόβλημα επιλύεται με μεθόδους καθόδου κλίσης που συνοδεύεται με κατάλληλο χειρισμό των περιορισμών και του πεδίου αρχικοποίησης των μεταβλητών σχεδιασμού. Συμπληρωματικά, εισάγεται μία τεχνική στον αλγόριθμο έτσι ώστε να προκύπτει το πεδίο των μη κυριαρχούμενων λύσεων σύμφωνα με τα κριτήρια Pareto. Τελικά, για κάθε περίπτωση προκύπτουν οι βέλτιστοι σχεδιασμοί υπό την μορφή νομογραφημάτων που μπορούν να χρησιμοποιηθούν ως πίνακες αναφοράς (look-up tables) στην λήψη αποφάσεων ή στην εφαρμογή συστημάτων αυτομάτου ελέγχου για την εξέλιξη των «Έξυπνων κτιρίων»",Mathematical modeling of thermal comfort and indoor air quality and advanced techniques for optimizing building design: applications on naturally ventilated buildings,,'National Documentation Centre (EKT)',10.12681/eadd/25592,,core
10535239,[2005],"The 2009 Mars Science Laboratory (MSL) will attempt the first precision landing on Mars using a modified version of the Apollo Earth entry guidance program. The guidance routine, Entry Terminal Point Controller (ETPC), commands the deployment of a supersonic parachute after converging the range to the landing target. For very dispersed cases, ETPC may not converge the range to the target and safely command parachute deployment within Mach number and dynamic pressure constraints. A full-lift up abort can save 85% of these failed trajectories while abandoning the precision landing objective. Though current MSL requirements do not call for an abort capability, an autonomous abort capability may be desired, for this mission or future Mars precision landers, to make the vehicle more robust. The application of artificial neural networks (NNs) as an abort determination technique was evaluated by personnel at the National Aeronautics and Space Administration (NASA) Johnson Space Center (JSC). In order to implement an abort, a failed trajectory needs to be recognized in real time. Abort determination is dependent upon several trajectory parameters whose relationships to vehicle survival are not well understood, and yet the lander must be trained to recognize unsafe situations. Artificial neural networks (NNs) provide a way to model these parameters and can provide MSL with the artificial intelligence necessary to independently declare an abort. Using the 2009 Mars Science Laboratory (MSL) mission as a case study, a non-adaptive NN was designed, trained and tested using Monte Carlo simulations of MSL descent and incorporated into ETPC. Neural network theory, the development history of the MSL NN, and initial testing with severe dust storm entry trajectory cases are discussed in Reference 1 and will not be repeated here. That analysis demonstrated that NNs are capable of recognizing failed descent trajectories and can significantly increase the survivability of MSL for very dispersed cases. NN testing was then broadened to evaluate fully dispersed entry trajectories. The NN correctly classified 99.7% of descent trajectories as abort or nonabort and reduced the probability of an unsafe parachute deployment by 83%. This second, broader testing phase is discussed in this paper",Entry Abort Determination Using Non-Adaptive Neural Networks for Mars Precision Landers,,,,,core
299992463,2007-01-01T00:00:00,"This research investigates a digital hardware oriented system that uses a
genetic algorithm (GA) for optimizing a pattern classifier based on the pulsed
neural network (PNN). The scheme avoids the usage of multipliers and dividers, which are the bottlenecks for digital hardware implementation of parallel computations like GA and neural networks. A new model for the
pulsed neural network has been developed in this research. In this model, the
information is coded in terms of firing times of pulses that are generated by
the neuron. The pulses transmit through the network and excite the dynamics of the neuron. Their synchronism is utilized to design the architecture of the neural network such that it acts as a RBF network. A new network-learning algorithm has also been developed for this PNN. The RBF neurons are generated based on the feature of the training data, and the synaptic delays can be adjusted to distribute these RBF neurons in the training data space. Utilizing the nature of RBF being inherent in the pulsed
neural network, the scheme yields very compact computational circuits for
massive parallel implementation on a chip that guarantees the speed of neural
computations. To optimize the network in real time, a hardware base GA has
been developed and implemented on a FPGA. The resultant on-chip GA-PNN system has been applied for terrain classification of a multi-spectral satellite image. Experimental results show that the performance of the proposed system is comparable to a back propagation (BP) neural network while its
training speed exceeds the BP network overwhelmingly. As another application demonstration, it is also extended to a nonlinear look-up table and applied to estimate the friction occurs in a precision linear stage",On chip pulse based neural network for signal processing,,,,,core
20713808,2008-04-03,"Abstract: Changes in the natural environment affect our quality of life. Thus, government, industry, and the public call for integrated environmental management systems capable of supplying all parties with validated, accurate and timely information. The ‘near real-time ’ constraint reveals two critical problems in delivering such tasks: the low quality or absence of data, and the changing conditions over a long period. These problems are common in environmental monitoring networks and although harmless for off-line studies, they may be serious for near real-time systems. In this work, we discuss the problem space of near real-time reporting Environmental Management Systems and present a methodology for applying agent technology this area. The proposed methodology applies powerful tools from the IT sector, such as software agents and machine learning, and identifies the potential use for solving real-world problems. An experimental agent-based prototype developed for monitoring and assessing air-quality in near real time is presented. A community of software agents is assigned to monitor and validate measurements coming from several sensors, to assess air-quality, and, finally, to deliver air quality indicators and alarms to appropriate recipients, when needed, over the web. The architecture of the developed system is presented and the deployment of a real-world test case is demonstrated. Keywords: Agent-based systems; Environmental monitoring systems; Decision support systems ",Applying agent technology in Environmental Management Systems under real-time constraints,,,,,core
24664991,2008-04-01,"Recently, several planners have been designed that can create conditionally branching plans to solve problems which involve uncertainty. These planners represent an important step in broadening the applicability of AI planning techniques, but they typically must search a larger space than non-branching planners, since they must produce valid plans for each branch considered. In the worst case this can produce an exponential increase in the complexity of planning. If conditional planners are to become usable in real-world domains, this complexity must be controlled by sharing planning effort among branches. Analogical plan reuse should play a fundamental role in this process. We have implemented a conditional probabilistic planner that uses analogical plan replay to derive the maximum benefit from previously solved branches of the plan. This approach provides valuable guidance for when and how to merge different branches of the plan and exploits the high similarity between the different branches in a conditional plan, which have the same goal and typically a very similar state. We present experimental data in which analogical plan replay significantly reduces the complexity of conditional planning. Analogical replay can be applied to a variety of conditional planners, complementing the plan sharing that they may perform naturally",Analogical Replay for Efficient Conditional Planning £,,,,,core
22698795,2007,"Intelligent systems based on machine learning techniques, such as classification, clustering, are gaining wide spread popularity in real world applications. This paper presents work on developing a software system for predicting crop yield, for example oil-palm yield, from climate and plantation data. At the core of our system is a method for unsupervised partitioning of data for finding spatio-temporal patterns in climate data using kernel methods which offer strength to deal with complex data. This work gets inspiration from the notion that a non-linear data transformation into some high dimensional feature space increases the possibility of linear separability of the patterns in the transformed space. Therefore, it simplifies exploration of the associated structure in the data. Kernel methods implicitly perform a non-linear mapping of the input data into a high dimensional feature space by replacing the inner products with an appropriate positive definite function. In this paper we present a robust weighted kernel k-means algorithm incorporating spatial constraints for clustering the data. The proposed algorithm can effectively handle noise, outliers and auto-correlation in the spatial data, for effective and efficient data analysis by exploring patterns and structures in the data, and thus can be used for predicting oil-palm yield by analyzing various factors affecting the yield",A Software Framework for Predicting Oil-Palm Yield from Climate Data,,,,,core
10546711,2008,"Expert system software programs, also known as knowledge-based systems, are computer programs that emulate the knowledge and analytical skills of one or more human experts, related to a specific subject. SHINE (Spacecraft Health Inference Engine) is one such program, a software inference engine (expert system) designed by NASA for the purpose of monitoring, analyzing, and diagnosing both real-time and non-real-time systems. It was developed to meet many of the Agency s demanding and rigorous artificial intelligence goals for current and future needs. NASA developed the sophisticated and reusable software based on the experience and requirements of its Jet Propulsion Laboratory s (JPL) Artificial Intelligence Research Group in developing expert systems for space flight operations specifically, the diagnosis of spacecraft health. It was designed to be efficient enough to operate in demanding real time and in limited hardware environments, and to be utilized by non-expert systems applications written in conventional programming languages. The technology is currently used in several ongoing NASA applications, including the Mars Exploration Rovers and the Spacecraft Health Automatic Reasoning Pilot (SHARP) program for the diagnosis of telecommunication anomalies during the Neptune Voyager Encounter. It is also finding applications outside of the Space Agency",Software Analyzes Complex Systems in Real Time,https://core.ac.uk/download/pdf/10546711.pdf,,,,core
33799208,2008-03-19T00:00:00,"Nowadays, more and more applications require systems that can interact with humans. Agents can be perceived as computing services that humans, or even other agents, can request in order to accomplish their tasks. Some services may be simple and others rather complex. A way to determine the best agents (services) to be implemented is to identify who the actors are in the object of study, which roles they play, and (if possible) what kind of knowledge they use. 
Socially Intelligent Agents (SIAs) are agent systems that are able to connect and interface with humans, i.e. robotic or computational systems that show aspects of human-style social intelligence. In addition to their relevance in application areas such as e-commerce and entertainment, building artefacts in software and hardware has been recognized as a powerful tool for establishing a science of social minds which is a constructive approach toward understanding social intelligence in humans and other animals.
Social intelligence in humans and other animals has a number of fascinating facets and implications for the design of SIAs. Human beings are biological agents that are embodied members of a social environment and are autobiographic agents who have a unique personality. They are situated in time and space and interpret new experiences based on reconstructions of previous experiences. Due to their physical embodiment, they have a unique perspective on the world and a unique history: an autobiography. Also, humans are able to express and recognize emotions, that are important in regulating individual survival and problem-solving as well as social interactions.
Like artificial intelligence research trend, SIA research trend can be pursued with different goals in mind. A deep AI approach seeks to simulate real social intelligence and processes. A shallow AI approach, which will be highlighted also within this thesis, aims to create artefacts that are not socially intelligent per se, but rather appear socially intelligent to a given user. The shallow approach does not seek to create social intelligence unless it is meaningful social intelligence vis-à-vis some user situation
In order to develop believable SIAs we do not have to know how beliefs-desires and intentions actually relate to each other in the real minds of the people. If one wants to create the impression of an artificial social agent driven by beliefs and desires, it is enough to draw on investigations on how people with different cultural background, develop and use theories of mind to understand the behaviours of others. Therefore, SIA technology needs to model the folk-theory reasoning rather than the real thing. To a shallow AI approach, a model of mind based on folk-psychology is as valid as one based on cognitive theory. 
Distance education is understood as online learning that is technology-based training which encompasses both computer-assisted and Web-based training. These systems, which appear to offer something for everyone at any time, in any place, do not always live up to the great promise they offer.
The usage of social intelligent agents in online learning environments can enable the design of “enhanced-learning environments” that allow for the development and the assessment of social competences as well as the common professional competences. 

Within this thesis it is shown how to corroborate affective theory with role theory with agent technology in a synchronous virtual environment in order to overcome several inconveniences of distance education systems. This research embraces also the shallow approach of SIA and aims to provide the first steps of a method for creating a believable life-like tutor agent which can partially replace human-teachers and assist the students in the process of learning. The starting point for this research came from the fact: anxious, angry or depressed students do not learn; people in these conditions do not absorb information efficiently, consequentially it is an illusion to think that learning environments that do not consider motivational and emotional factors are adequate",Corroborating Emotion Theory with Role Theory and Agent Technology: a Framework for Designing Emotional Agents as Motivational Tutoring Entities,https://core.ac.uk/download/33799208.pdf,,,,core
235570187,2003-04-01T00:00:00,"An archive of the Milo Canopener.The University of Lethbridge Library received permission from the Archives at Milo Library to digitize and display this content.Milo Can Opener
Box 12, Milo, AB T0L1L0
Canada Post Agmt. # 40607518
April, 2003
I 1
J^a^GO UO 0\)iAd Rates
Subscription Rates
Business Directory
$ 5.00
Milo $ 16.00
Quarter Page
6.00
(- pickup,delivery or mailed)
Half Page
8.00
Mailed (outside Milo) 24.00
Full Pages
15.00
Single Copies 2.00
Classifieds
2.00
Please note that those who paid for a Milo subscription by mail ($24) on our previous rates
have payed $8 too much. Your subscription
The following items are free of charge
renewal date will be extended by 6 months.
Notices Please sign them, no letters will be
Announcements printed if not signed.
( Wedding, Anniversary, Births, Showers, etc.) Requests to remain anonymous
Cards of Thanks will not be honoured.
News items Articles
Please send items to the following volunteer staff
Layout Editors - Barb Godkin - 599
-2213, 485-8389
Iris Gough - ..........................
599-2377
Production - Colleen Deitz
599-2306
Betty Armstrong
Zola Webber
Subscriptions -Georgina Ully -........................
..... 599 - 2424
Notices - Julie Nelson -.................................
.... 599-2175
Charlotte Nelson -..........................
599-2253
Cartoons & “Kids Say” - Marina Vannatta -...
..... 381 - 6389
Milo Can Opener
Box 12, Milo, Alberta, TOL 1L0
f Fax# 599 - 2457
? (fax shares line with phone so you will get the answering machine sometimes. You can also fax to Milo Municipal Library at 599-3850) Email: igodkin@telusplanet.net
Items may be left at Jamie’s Foods in the Can Opener box at back of store
or at Milo Municipal Library.
Please note the new fax number!
Please Note: The deadline for articles that need typing, etc. to be submitted is the Monday before the last Friday of each month. If your article is ready for press, we can accept it until Wednesday or Thursday.GOODS & SERVICES
Donald W. Kinney
Manager
Box 150
Milo, Alberta TOL 1LO
i
Tel: (403) 599-4101 Fax: (403) 599-2409
Scotiabank
BUSINESS HOURS:
Mon - Thurs 10:00 - 12:00 1:00 - 3:00 Friday 9:30 -12:00 1:00- 5:30
GRANT, KRYSTALOWICH & BENNETT
CERTIFIED GENERAL ACCOUNTANTS
FULL ACCOUNTING SERVICES AND CONSULTING
P.O. Box 239 Vulcan, Alberta TOL 2B0
Phone: 485-2996 485-2681
■
\^luVenture
Travel
Val Umscheid
travel consultant/tour guide
|J§g&
Box 88
Milo, AB mil T0L1L0 ^^1
e-mail: valuventure@telusplanet.net www.valuventure.com
tel: 1(403)599-2406 fax: 1(403)599-2247 toll free: 1(877)599-2499
*;■ •: ! Tift.
MILO
SEED CLEANING ASSOCIATION LTD.
599-2150
Cleaner Seed is Sown Cleaner Crops are Grown
Ed Posein - Manager
Box 7 Milo, AB T0L1L0
Doug Marks
PRESIDENT
Office: (403) 599-0003 Fax: (403) 599-3990 Mobile: (403) 362-1764
Marks
Oilfield Services Inc.
Trucking, Gravel
Oilfield Maintenance and Construction Pipelining
Pressure Washing and Steaming
(£sso)
VULCAN VILLAGE GAS BAR
P.O. BOX 425 VULCAN. ALTA. TOL 200 PHONE: 485-6000
FOOD TO CO
Garry &
Bernardine Nelson 485-2519
CORNER STORE & GARAGE
OIL - GAS - DIESEL - REPAIRS - WELDING A.M.A. TOWING
MERV & FRANCES GOLDTHORPE 485 - 6671We would like to thank our advertisers for their continued support.
Without them, we would not be able to print this newsletter for the enjoyment of the readers.
Dor, (Go Mo L3s®®imlb®
(403)485-6005 ■' P.O. Box 87, Vulcan, Alberta, Canada
FAIRBANKS DENTURE CLINIC
125 Centre Street, Vulcan, Alberta TOL 2B0
485-2368
Scott D. Fairbanks - Denturist
OFFICE HOURS:
Wednesday 9:00 am - 5 00 pm Friday 1.00 pm. - 5:00 p m
_________' ""__________ '_
Lori Vooys, CIM, FCSI Financial Planner
lori_vooys©scotiamcleoc).com
Suite 1800, Scotia Centre 700 - 2nd Street SW Calgary, AB T2P 2W1
Tel: (403) 298-7823 Fax: (403) 298-4054 Toll Free: 1-800-372-9274 Cell: (403) 815-6002
% ScotiaMcLeod
ScotiaMcLeod is a division of Scotia Capital Inc, a member of the Scotiabank Group.
V.->
Xr. B. X Drump
OPTOMETRIST
BOX 972
VULCAN, ALBERTA TGL280
telephones
485-2177
485-2886
Jamie’s H Foods
Carol and Jams Robertson Box 38 Milo, AB.TOLILO
Ph. 403-599-3922 Fax 599-3835
^LMARy KA>4
Donna Bennett Deiti
independent Beauty Consultant
P.O.. Box 37,
Milo, Alta. TOL 110 (403) 599-2140
A. P. C. S.
AARDVARK PEST CONTROL SERVICES®
JERRY GAUTREAU
P.C.T. Diploma, AIB Certified & ASI Certified
SUITE 213, 204 - 1440 52nd STREET N.E. CALGARY, ALBERTA T2A 4T8
Tel: (403) 273-MICE (6423) Fax: (403) 204-2125
^ ' -
Sc ©lean Sc pimple
INTERNATIONAL RECORDING ARTISTS
Cell: (403) 512-9066 Fax: (403) 599-2398
Keepln' the Country In music
LAH-MAR PROMO Ph: (403) 381-6389 Fax: (403) 381-6341& <5 £ Gsiecdiue ^beilc^t
for a&bMasitf*
599-2466
Milo, Alberta
Open - Monday, Wednesday, Friday, Saturday
RENO BEXTE
Weed Co ntrof (Centre
P & H GRAIN LTD. AGENT FOR ALTA HAIL INS.
TELEPHONE:
(403) 534-3461 ANYTIME FAX: (103) 534-2182
MOSSLE3GH. AB TOL IPO
■■ ■ / • ■■ .
MILO CAFE
CHINESE A'WESTERN TAKE OUT ORDERS
599-3832
Closed Monday
Monday-Sunday 8:30 am - 800 pm
Beer&. Wine with meals
_.......................
Next Canopener Deadline April 28""Organized for Savings Not fo r P rofit”
t
COOP
ARROWWOOD CO-OPERATIVE ASSOCIATION LIMITED
P.O. BOX 120 ARROWWOOD, Alberta TOL 0B0
(403) 534-3803 Store (403) 534-3804 Tire & Lube Center Fax (403) 534-3330
Your Suppliers of:
Petroleum Products - Fuel & Lubricants Tires - On and Off Road - New/Used/Repair Services Lube Center - Most vehicles - including 1 ton trucks Hardware /Lumber / Plumbing / Electrical Filters - Complete line of oil / air / fuel Belts / Bearings / Hydraulic Hoses Paint - interior/exterior - mixed to your color specifications Batteries - automotive/flashlight / watch / etc.
Automotive - lights /fuses /accessories Housewares / Sporting Goods Feed and Animal Health Supplies
Hours: Mon-Fri 8am-12 noon 1pm-5:30pm Saturday 8am-12 noon
DR. JAMES L. POTVIN
B.Sc (Hons), D.D.S.
General Dentistry
Patient Services
Nitrous gas for anxious patients Electronic freezing (no needle) Televisions & Walkmans Highest standard of sterilization for your protection Financing and payment plans . available 0A.C
Quality Dental Services
- Comprehensive preventative exams including periodontal and oral cancer screening
• Gentle hygiene care
• Toothcolored & silver fillings
• Cosmetic Bonding and Veneers to cover chips, cracks and stains
• Tooth whitening to brighten your smile
• Crown and Bridge
• Complete and Partial Dentures
Alt members of our Cavity Free Club are entered into a monMy draw for a S25 Gif Certificate Redeemable at Wolfe's Hardware Toy and Sporting Goods Department
Snake Valley Drop-In News
GENERAL MEETING Friday, April 4 at 2:30 p.m.
Everyone Welcome
Games
Bridge - every Tuesday at 1:30 p.m.
Crib - Monday, April 7 at 7:30 p.m. Monday, April 21 at 7:30 p.m.
NEW PATIENTS ARE ALWAYS WELCOME
J H, 3rd Avenue North, Vulcan
Health Nurse
The public health nurse will be at the drop-in Wednesday, April 9 from 1:00 to 2:00 p.m.VILLAGE OF MILO MINUTES
The regular meeting of the Village of Milo was held on Monday, February 17, 2003 at 7:00 p.m. at the Village Office.
Present were Mayor Vooys, Councillor Whaley, Councillor Phillips, Municipal Administrator Dorothy Way, John and Bea Kuzma, and Bob Phair.
The minutes of the meeting held on January 20, 2003 were read. Councillor Phillips moved the adoption of the minutes. Councillor Whaley seconded. CARRIED.
Mayor Vooys attended a FCSS meeting. The FCSS will be running a courtesy vehicle in this area. Councillor Whaley attended a VBDC meeting and they will be hosting an economic development training course. The VBDC will be joining up with Tourism. Mayor Vooys thanked the Council for the flowers.
Town man Victor Crowe reported that the tractor is leaking power steering fluid. The tractor is to be repaired before we start cutting the grass in the spring. It was suggested that a barrel be put up at the water treatment plant for the water to drain into after the trucks fill up. The water could also be drained into the sewer pipe on the south side of the water treatment plant. The culvert at the end of Center Street needs to be flushed out. Mayor Vooys will talk to Doug Marks and see if we can have an oilfield hydro truck flush out this culvert. The other end of this culvert will have to be cleaned out before this can be done.
Mr. Bob Phair attended the meeting to gather information on the water reservoirs so the company he works for KENECO can put together a proposal for this project. Mr. Bob Phair and a construction personnel will take a look at the reservoirs. The Municipal Administrator to look into funding from Alta. Transportation & Utilities as well as Infrastructure funding. A letter to be sent to Alpine Engineering Ltd. stating that we will not require his services.
A letter has been sent to JRD Contracting confirming that he has the contract for doing the sidewalk repairs. The Village has received the funding for this project.
Mr. & Mrs. John Kuzma attended the meeting in regards to the sale of the lots in BLOCK C PLAN 1403JK. The 2 trailers that are on this lot will be moved. There is no gas line on the property. The trailers that were there previously used propane.
MOVED by Councillor Phillips that the Village apply for the STEP grant this year. CARRIED.
We received 2 quotes one from Trinus Technologies Inc. and one from the Management Information Group on Municipal software. The Municipal Administrator to talk to Delanoy and Ziel and see if they have any information on what programs we should be using.
AXIA was here to take measurements of where the wire and the box will be installed for the Supemet. Mayor Vooys and Councillor Phillips want to be contacted when they return.
Letters were received from the Oldman River Intermunicipal Service Agency and the Vulcan County on the proposed subdivision for a petroleum card lock facility. Letter to be sent to the Vulcan County and the Oldman River Intermunicipal Service Agency stating that the Village of Milo has no objection to this proposed subdivision and petroleum card lock facility. MOVED by Councillor Phillips that the Village of Milo approve the petroleum card lock facility. Mayor Vooys seconded. CARRIED. Councillor Whaley is opposed to this proposal. Councillor Phillips to contact the Arrowwood Co-op and get a copy of the letter from Alta. Transportation and Utilities for Councillor Whaley.
MOVED by Councillor Whaley that the Village of Milo approve the proposed resolution for membership in the Oldman River Regional Services Commission. The proposed resolution is as follows: THAT the Village of Milo join the Oldman River Regional services Commission. CARRIED.
No one has come forward to organize the Communities in Bloom for the Village.
The next Council meeting of the Village of Milo was set for Monday, March 17, 2003 at 7:00p.m. at the Village Office.
The meeting adjourned at 8:55 p.m.DUSTING
M A house becomes a home when you can write “I love you” on the furniture!”
I can’t tell you how many countless hours that I have spent CLEANING! I used to spend at least 8 hours every weekend making sure things were just perfect—“in case someone came over.” Then I realized one day that no one came over; they were all out living life
and having fun!
Now when people visit, I find no need to explain the ‘condition’ of my home. They are more interested in hearing about the things I’ve been doing while I was away living life and having fun! If you haven’t figured this out yet, Please heed this advice!!!
LIFE IS SHORT! ENJOY IT! Dust if you must, but wouldn’t it be better to paint a picture or write a letter, bake a cake or plant a seed,—ponder the difference between
want and need?
Dust if you must, but there’s not much time, with rivers to swim and mountains to climb, music to hear and books to read, —friends to cherish and life to lead.
Dust if you must, but the world’s out there, with the sun in your eyes, the wind in your hair, the flutter of snow, a shower of rain,—this day will not come around again!
Dust if you must, but bear in mind, old age will come, and it’s not kind.
And when you go—and go you must—you, yourself, will make more dust!
4
“It’s not what you gather, but what you scatter, that tells what kind of life you have
lived!!”
• Aerator covers • Screens • Covers • Tarps • Tents
• Security pouches for radios
Bug sweeps
• Boat covers
• Patio swings
• Sprayer covers
• Tire covers
• Winter fronts
• Cargo nets
• Outfitter's tents
• Fertilizer & chemical spreader covers
N-l Upholstery & Tarp Manufacturing — 120 Main Street, Neville
&
Irene
Champion, flB P0 Box 206 TOL 0R0
Phone: 1-888-227-0170
Please phone before you corr>e if you're coming any distance to ensure we are here to ouoid disappointment.
• Portable generator covers
• Hopper covers
• Auger extension spouts
• Air conditioner covers
• Tonneau covers
• Windshield covers
• Bug screens
• Asphalt tarps 600°F rating
• Pack covers
• Truck tarps : flat & roll
• Recreational tents-
NAME
ADDRESS
Milo Minor Soccer Program 2003 Registration Form
______BIRTHD ATE: (d/m/y)__/__/__/ AGE(as at deadline)_
PHONE___________________________________SEX.
HEALTHCARE NUMBER
DOCTOR,
Please indicate any illness and physical limitations
Please indicate any illness or injury occurred over the last year,
PARENT/GUARDIAN(please print)__________________________
AGE CATEGORIES: (please check appropriate box)
(age categories may be combined if there are not enough registrations)
□
U-6 Mini-Soccer
4 to 5 years
□
U-8 Mini-Soccer
6 to 7 years
□
. U-10 Mini-Soccer
8to 9 years
□
U-12 Mini-Soccer
10 to 11 years
Important Information
All players must supply their own certified shin guards, soccer socks, and water bottles When: Practice to begin as weather permits(note will be sent home)
Parents will be responsible for ensuring their child has a ride for games.
Cost: $20/ participant (covers insurance and equipment costs)
Please make check payable to Milo Community Soccer Return form/payment by: April 10,2003
Shane Cranston Milo Community School
I the undersigned accept responsibility for the above registered player during activities associated with Milo Minor Soccer Program during the 2003 season in such that Milo Soccer Board and/or any of its agents cannot be held liable for any accidents which occur incidental thereto, and the player is accepted on this condition.
I would be interested in coaching.___________________________
Signature of Parent/Guardian:________________________________________Frank Mclnenly Auctions Ltd.
Vulcan, AB
. „ . - • , ..jss-**,.*,— .**»$*.*•; •' -
Serving The Agriculture Industry
Since 1967
(403)485-2440
Frank Mclnenly Stacey Mclnenly Les McIntyre
Foothills Ljvestock Auction
(403) 549-2120
Regular sales every Friday Special Calf Sales Bred Sales as announced
For up to date marketing call:
Frank Mclnenly (403) 485-2440 cell: (403) 485-8123
Marvin Fowler (403) 646 -2334 cell: (403) 625-6070
F M Trailer World
Located at Foothills Livestock Auction
Stavely. AB
SoulZern A/lerta's Exclusive JVorlerl Dealer
N0RBERT DEX TRAILTECH
Stock, Horse, Flatdecks SNew & Used
1-877-205-1999
Call StaceyGOALS:
* To provide affordable transportation where needed.
* To assist the Senior shut-in population and persons with special needs to be able to access health care, shopping and socialization.
* To make the transportation available every second week in each small community.
PARTNERS:
* Headwaters Health Authority
* Family and Community Support Services
* Vulcan Lions Club
* Friends of Little Bow
NOTE:
People who can not access the service without assistance would need to bring a companion along.
Pick up from residences will be made in each small town. Efforts will be made to accommodate stops in Vulcan
Headwafers Health Authority Regional Health Authority #3
Three Month Pilot Project
Start Date:
April 02, 2003
Pick up. Carmangay 9:00 am
Pickup Champion 9:30am
Pick up Milo 9:00 am
Arrive
Vulcan Community Health Centre
10:00 am
Downtown Vulcan - Mainstreet
10:30 am
Pick up VCHC 11:00 am
Arrive downtown 11:30 am
Pick up at designated point at 1:30 pm
Leave Vulcan for home 2:00 pm
April 09, 2003
Pick up Arrowwood 9:00 am
Pick up Mossleigh 9:30 am
Pick up Lomond 9:00 am
Amve
Vulcan Community Health Centre
10:00 am
Downtown Vulcan - Mainstreet
'l0:30am
Pick up VCHC 11:00 am
Arrive downtown T1:30 am
Pick up at designated point at 1:30 pm
Leave Vulcan for home 2:00 pm
Suggested Donation:
$6.00 per round trip
BUS SERVICE COMING TO THE COMMUNITIES OF THE VULCAN COUNTY
HANDI-GO BUS VULCAN COUNTY
ANSWERING A NEED IN OUR COMMUNITIES
Phone 485-2285 For information or bookingsif
caf
*'0**+' Are You A Poet and Don’t Know It?
4 Your talents could pay off
Enter the Milo Library Poetry Contest And you could win a “Chapters” gift certificate
3 Categories:
Grade 3 and Under Grades 4 to 6 Grades 7 and up
Your poem should have something to do with the history of Milo and/or the Milo Library Any type of poem is acceptable Submit to the Library by Thursday May 29th.
isrfbjfcrrH & comjpany
BARRISTERS and SOLJCnXJRS
SERVICING ALL YOUR LEGAL NEEDS
Dr. Robert J. (Bob) Langrldge will be In attendance at the Village Office in Milo the first Friday of each month from 1:00 p.m. to 3:00 p.m. Appointments may be made by calling 485-2070
Brian J. Murray and Robert J. (Bob) Langrldge servicing our Vulcan office 104 Centre Street Vulcan, Alberta Phone: (403) 485-2070
Areas of Law: Real Estate, Personal Injury, Divorce and Family Law, Wills and Estates. Dependent Adutts, Employment Law. Criminal Law, Business and Corporate Law. Mediation, Uhgatlon and Tax Law.
LETHBRIDGE OFFICE
#600, 220 - 4m Street South Phone: 403) 278-7781 Fax. (403) 320-8958 Toll Free: 1-800-552-8022
SOUTHERN ALBERTA'S REGIONAL LAW FIRMMilo Municipal Library
NEWS
a member of the Chinook Arch Regional Library System www.chinookarch.ab.ca
NASA arfti the “Association of Library Services to Children” have partnered to offer online and discovery-based activities focusing on space science and technology. http://www.ala.orq/alsc/spaceplace/clubspace.ht
ml
Search for current affairs radio and t.v.clips. http://archives.cbc.ca
Our next Library Board Meeting will be held on Thursday May 1st 2003.
THANK YOU
A HUGE THANK YOU goes out to David Healy for donating an incredible oak bookcase that he built. The “Friends of the Library” will be selling raffle tickets on it as a fund raiser. Check next months’ Can Opener for further details.
MARCH BESTSELLERS
“2nd Chance” by James Patterson “Everything’s Eventual” by Stephen King “Portrait in Death” by J.D. Robb “Body of Lies” by Iris Johansen “Midnight Voices” by John Saul “City of Bones” by Michael Connelly “Hunting Season” by Nevada Barr “The English Assassin” by Daniel Silva “Gone For Good” by Harlan Coben “Little Altars Everywhere” by Rebecca Wells
New Books
“The Academy Awards A Complete History of Oscar”
“Red Mafia” by Robert Friedman “Rush Hour Recipes” by Jean Pare “He Sees You When You’re Sleeping” by Mary Higgins Clark
“Take a Thief’ by Mercedes Lackey “Fortress Draconis” by Michael Stackpole “The Diamond Hunters” by Wilbur Smith “Shout at the Devil” by Wilbur Smith “Star by Star” by Troy Jenning
“Fit Not Fat at 40 Plus”
New Videos
“Cats and Dogs”
Junior Books
“Pokemon Jr. The Wobuffet Village” “Mathamazing” by Raymond Blum “Under The Stars” by Ben Baglio “Trouble With Girls” by Ted Stanton “Stinky” by Ted Stanton “Breakout at the Bug Lab” by Ruth Horowitz “Franklin’s Music Lessons” “Franklin and the Magic Show”
New CDs
“Scooby Doo Showdown in Ghost Town” “Kid Pix 3”
“Ocean Voyager”
“JumpStart Explorers”
“Improving Classroom Behavior” “Compton’s Encyclopedia 2000”
LIBRARY HOURS
Tuesdays 9:30 am -12:30 pm
.............1:30 pm -5:00 pm
Thursdays.........9:30 am - 12:30 pm
1:30 pm - 5:00 pm 6:30 pm - 8:00 pm Phone and Fax: 599-3850 email messages to libmil@chinookarch.ab.ca
FAMILY HAIRSTYLING
X 599-2491 X
MILO
HOURS TUES ■ FRI 9:00-5:00 SAT 10:00-2:00
WED. Mens walk in 9:00-12:00
NOTICE: Trends will be closed from March 11 to the 29th, and also on April 10th and 11th.
We apologize for any inconvenience.
Waxing, Eyelash & Eyebrow TintingSHARE the NEWS
MILO CORRESPONDENT for NEWS only
L. STUMPF 599-3748
NEW PUBLISHING DATE: THURSDAYS Deadline remains the same: Friday at 4:00 p.m.
CALL WANDA - 485-2036
COUNTY CALENDAR
Coming Events for non-profit groups Call Economic Development - 485-2992
CLASSIFIED AD RATES
$7.42 for 20 words + ,10p each additional word 2nd week half price (Minimum $4.45)
Phone. 485-2036 • Fax: 485-6938 Web site: www.vulcanadvocate.com
Customer Service
CHECK OUR WEBSITE!!
See the Classifieds, News and Photos on-line! www.vulcanadvocate.com
rnwr
the (rAP
and SHARE the NEWS
Bernice Finlay
main§vulHnadvocale.com
SUBSCRIPTION RATE
$25.00 per year (within county)For Those Who Grieve the Loss of a Loved One
It is hard to remain in the world and not feel a part of it. To watch others rush about like nothing has changed \$hen everything has.
Grief makes you feel alone.
Yet you do have a place in the world And others who care for you.
There is still beauty and meaning in life And you will find it again,
For now, it is enough to rest, to mourn,
And wrap yourself in your memories.
The world can wait.
A MOTHER’S WISH,
Oh, give me patience when the little hands Tug at me with their ceaseless demands.
Oh, give me gentle lips and smiling eyes And keep my mouth from hasty, sharp replies. Oh, let me not in weariness, confusion or noise Obscure life’s vision from its fleeting joys.
And when in years to come my house is still No bitter memories its rooms may fill.
Cl
-b_
a,
|h
5
P
p
\
1In
Jbu
.JL-
._U
?
T
X
v
X
\K
ID-
□
r
s
V
11
V
t]
£-
n 1 j
IX-
S-
o
p
UV
0
X
r
A_
nr
c
b-
p
a
P
In
31
JL
\ L
0
_y_
X-
b
m
a
^ r
Oh
hQ-
o
■P
P
<°
n
_s_
V 1
p
b_
_V_j
P
t
y
-* i
n
r
n
SI
Ul
a
JZj
b_
cv
q
^r
r
X
P
V |
V-
b
b
u
i.
_C_
i
Q
Z
c -
A
AJ
X.
\
b
lb
g
o_
_o__
c
\
T-
4
10
q
P
b.
a
b
$
-b
Q_
\
","Milo Canopener (April 1, 2003)",,Milo Community Volunteers,,,core
11339876,2003-09-01T00:00:00,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites.  The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality.  Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc.  Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world.  The Glasgow Directory is an established 3D web space,

with around 10,000 visitors per year.  The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography.  This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar

environments. These criteria take into account lessons learned by ‘observing’ and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface.  These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow : Welcoming Multiple Visitors to the Virtual City,,,,,core
20643503,2008-12-11T14:06:12,"The motivation of the thesis comes from the frustrations of young engineers confronted with real design problems. The inspiration of the thesis evolved from observations of bridge designers and analyses of bridge design competitions. Not only do designers adopt more than one strategy during design, they rarely perform a fixed sequence of tasks. Not only do designers consider more than one criterion during design, their priorities shift during the determination of parameters. The choice of task sequence, which is largely reflected in the style of the designer, is considered a major source of design variety. The thesis is founded on the premise that design is path dependent. Hence, freedom of sequence and support for an opportunistic strategy, are essential aspects to acquire, model and implement, in order to capture realistic design situations, to achieve design variety for the same initial conditions and to apply these aspects to interactive computational environments. In this thesis, an integrated approach was adopted throughout acquisition, modelling and implementation, in order to propose formalisms, a framework and a design tool that supports freedom of sequence.  The formalisms consist of four design characteristics (tasks, parameters, strategies and criteria), a decision making process (the labyrinth metaphor) and a control process (design navigation). The style of the designer is derived as the strategies and criteria adopted for making decisions about tasks and parameters. The labyrinth metaphor is considered the most realistic reflection of decision making paths observed with designers. Design navigation highlights a designer's skill for controlling the labyrinth, without getting lost. However, more acquisition is needed to strengthen the strategies proposed. The framework integrates these formalisms into four parts: a design space, a workspace, a decision path and design interactions. The notion of a workspace, as a continually changing subset of the design space, prepares the labyrinth metaphor for implementation into the interface. Design interactions model the interactions between the design space and the workspace, for navigating in the labyrinth of tasks and parameters. However, a more extensive design space is still required for improving the framework's functionalities, in particular for interacting with CAD tools. The design tool's interface is an implementation of the framework. The interface rests on two dynamic window environments called the task workspace and the parameter workspace. Each workspace has a labyrinth area and a navigation area, which separates decision making from control, explicitly. A prototype of the design tool, called FIBRES, was demonstrated to practitioners and students, whose feedback confirmed the importance of allowing the user freedom of sequence and control over interactions. However, a more robust prototype is needed to perform extensive testing with students.  A user-interface that accommodates unrestricted task sequencing has the potential to train novice designers in good bridge design by inducing an implicit mode of learning, and to provide experienced designers with computer workspaces compatible with their own decision making style. In the long term, such an approach may contribute to a better acceptance and penetration of artificial intelligence techniques into civil engineering education and practice",Preliminary bridge design navigation tool for novices,,,,,core
10563749,November 2004,"Topics include: Multifunction Imaging and Spectroscopic Instrument; Position-Finding Instrument Built Around a Magnetometer; Improved Measurement of Dispersion in an Optical Fiber; Probe for Sampling of Interstitial Fluid From Bone; Neuropsychological Testing of Astronauts; Method of Calibration for a Large Cathetometer System; Four-Channel PC/104 MIL-STD-1553 Circuit Board; Improved Method of Locating Defects in Wiring Insulation; Strobe Traffic Lights Warn of Approaching Emergency Vehicles; Improved Timing Scheme for Spaceborne Precipitation Radar; Concept for Multiple-Access Free-Space Laser Communications; Variable Shadow Screens for Imaging Optical Devices; Verifying Diagnostic Software; Initial Processing of Infrared Spectral Data; Activity-Centric Approach to Distributed Programming; Controlling Distributed Planning; New Material for Surface-Enhanced Raman Spectroscopy; Treated Carbon Nanofibers for Storing Energy in Aqueous KOH; Advanced Infant Car Seat Would Increase Highway Safety; Development of Biomorphic Flyers; Second-Generation Six-Limbed Experimental Robot; Miniature Linear Actuator for Small Spacecraft; Process for Making Single-Domain Magnetite Crystals; A New Process for Fabricating Random Silicon Nanotips; Resin-Transfer-Molding of a Tool Face; Improved Phase-Mask Fabrication of Fiber Bragg Gratings; Tool for Insertion of a Fiber-Optic Terminus in a Connector; Nanofluidic Size-Exclusion Chromatograph; Lightweight, Low-CTE Tubes Made From Biaxially Oriented LCPs; Using Redundancy To Reduce Errors in Magnetometer Readings; Compact Instrument for Measuring Profile of a Light Beam; Multilayer Dielectric Transmissive Optical Phase Modulator; Second-Generation Multi-Angle Imaging Spectroradiometer; Real-Time Adaptive Color Segmentation by Neural Networks; Research and Development in Optical Communications; Tests of Multibeam Scintillation Mitigation on Laser Uplinks; and Spaceborne Infrared Atmospheric Sounder","NASA Tech Briefs, November 2004",https://core.ac.uk/download/pdf/10563749.pdf,,,,core
53415069,2008-01-01T00:00:00,"For understanding a real-world environment on a conceptual level, any agent requires the capability for autonomous, open-ended learning. One of the main challenges in Artificial Intelligence is to bias the learning phase sufficiently in order to obviate complexity issues, while at the same time not restricting the agent to a certain environment or to a particular task. In this paper we describe a framework for autonomous design of experiments for a robotic agent, which enables the robot to improve and increase its conceptual knowledge about the environment through open-ended learning by experimentation. We specify our implementation of this framework and describe how its modules can recognize situations in which learning is useful or necessary, gather target-oriented data and provide it to machine learning algorithms, thus reducing the search space
for the learning target significantly. We describe the integration of these modules and the real world scenarios in which we tested them",Towards autonomous design of experiments for robots,,,,,core
21152401,2008,"Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement real-time, isolated digit speech recognition using a Liquid State Machine. The Liquid State Machine is a recurrent neural network of spiking neurons where only the output layer is trained. First we test two existing hardware architectures which we improve and extent, but that appear to be too fast and thus area consuming for this application. Next, we present a scalable, serialized architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. All architectures support leaky integrate-and-fire membranes with exponential synaptic models. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures only spanned part of it",Compact hardware liquid state machines on FPGA for real-time speech recognition,,,10.1016/j.neunet.2007.12.009,,core
24689661,2008-04-01,"This paper describes MEXAR2, a software tool that is currently used to synthesize the operational commands for data downlink from the on-board memory of the MARS EXPRESS spacecraft to the ground stations. The system generates from a two steps effort. A first study, performed before the launch of MARS EXPRESS, has produced a formalization of the memory dumping problem and developed the MEXAR prototype. This first result shows the feasibility of using an artificial intelligence constraint-based approach to solve the problem. A second effort has studied the real data flow during actual Mars operations, integrated more sophisticated solving algorithms and produced MEXAR2 which is successfully inserted in the overall mission planning process. The tool has been in daily use by the Mission Planning Team of MARS EXPRESS at the European Space Agency since February 2005. Goal of this paper is to present a complete overview of how the planning and scheduling problem has been addressed, a complete application customized and put into context in a challenging application environment. The story: in search of real data In the period November 2000- July 2002, authors research group has worked at the European Space Agency (ESA) study “Efficient Planning Algorithms for an Interplanetary Mission”. This study was aimed at demonstrating Artificial Intelligence techniques for Planning and Scheduling applied to a mission planning problem within the MARS EXPRESS space program. An open problem, jointly identified by the research group and the MARS EXPRESS Mission Planning experts, has been studied and formalized as the Mars Expres",MEXAR2 – An Operational Tool for Continuous Support to Mission Planning,,,,,core
20733722,2008-04-03,"Multi-Processor Systems-on-Chips (MPSoCs) are becoming increasingly complex, and mapping and scheduling of multi-task applications on computational units is key to meeting performance constraints and power budgets. Abstract models of system components and deployment of advanced algorithmic techniques for the optimization problem can provide for fast design space exploration and for optimal solutions. We exploit Constraint Programming (CP) from Artificial Intelligence and Integer Programming from Operations Research (OR) as a means to capture different aspects of the same problem (optimality and feasibility), and prove the effectiveness of this hybrid approach. Moreover, we exploit an accurate MPSoC virtual platform for capturing mismatches between problem formulation and real-life systems, and for assessing their impact on expected performance. We introduce the notion of execution constraints in the model of the problem, thus making the solution expressible and implementable in the real world. The model without execution constraints is a relaxation of the real problem and therefore provides a superoptimal solution. We compare the effectiveness of this latter solution with the one provided by the simulator, and try to refine our models as well as our optimization techniques accordingly",Measuring Efficiency and Executability of allocation and scheduling in Multi-Processor Systems-on-Chip,,,,,core
24487593,2007,"Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement realtime, isolated digit speech recognition using a Liquid State Machine (a recurrent neural network of spiking neurons where only the output layer is trained). First we test two existing hardware architectures, but they appear to be too fast and thus area consuming for this application. Then we present a scalable, serialised architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures only spanned part of it",Compact hardware for real-time speech recognition using a liquid state machine,,,,,core
24450113,2004,"Abstract—In a multiple-antenna system, an optimized design across the link and scheduling layers is crucial toward fully exploiting the temporal and spatial dimensions of the communication channel. In this paper, based on discrete optimization techniques, we derive a novel analytical framework for designing optimal space–time scheduling algorithms with respect to general convex utility functions. We focus on the reverse link (i.e., client to base station) and assume that the mobile terminal has a single transmit antenna while the base station has receive antennas. In order that our proposed framework is practicable and can be implemented with a reasonable cost in a real environment, we further assume that the physical layer involves only linear-processing complexity in separating signals from different users. As an illustration of the efficacy of our proposed analytical design framework, we apply the framework to two commonly used system utility functions, namely maximal throughput and proportional fair. We then devise an optimal scheduling algorithm based on our design framework. However, in view of the formidable time complexity of the optimal algorithm, we propose two fast practical scheduling techniques, namely the greedy algorithm and the genetic algorithm (GA). The greedy algorithm, which is similar to the one widely used in 3G1X and Qualcomm high-data-rate (HDR) systems (optimal when aI), exhibits significantly inferior performance when I as compared with the optimal approach. On the other hand, the GA is quite promising in terms of performance complexity tradeoff, especially for a system with a large number of users with even a moderately large.Asa case in point, for a system with 20 users and aR, the GA is more than 36 times faster than the optimal while the performance degradation is less than 10%, making it an attractive choice in the practical implementation for real-time link scheduling. Index Terms—Fairness, genetic algorithm (GA), multiple antenna, optimal algorithm, scheduling, single-input–multiple-output (SIMO), utility functions. I",Performance analysis of SIMO space-time scheduling with convex utility function: zero-forcing linear processing,,,10.1109/tvt.2004.823507,,core
10858184,2008-01-01T00:00:00,"Preface
The history of automated pattern recognition can be traced back to the advent of modern computing mid-way through the 20th century. Since that time, the popularity and growth of the pattern recognition field has been fueled by its scientific significance and its applicability to the real world. Pattern recognition is a very challenging and multidisciplinary research area attracting researchers and practitioners from many fields, including computer science, computational intelligence, statistics, engineering, and medical sciences, to mention just a few. Pattern recognition is a process described as retrieving a pattern from a database of known patterns. It has numerous real-world applications in areas such as security, medicine, information processing, and retrieval. Some pattern recognition applications in areas such as handwriting recognition, document retrieval, speech recognition, signature verification, and face recognition are the main focus of the current research activities in the pattern recognition and computational intelligence communities around the globe. Researchers and developers are facing many challenges to applying pattern recognition techniques in many real-world applications. This book consists of 17 peer-reviewed chapters that describe theoretical and applied research work in this challenging area. The state of the art in areas such as handwriting recognition, signature verification, speech recognition, human detection, gender classification, morphological structures for image classification, logic synthesis for image and signal processing, occlusion sequence mining, probabilistic neural networks for EMG patterns, multi-objective clustering ensembles, evolutionary ensembles, support vector machines for biomedical data, and unified support vector machines is presented in various chapters of this book.
The first two chapters focus on off-line cursive handwriting recognition. In Chapter I, Verma and Blumenstein review existing handwriting recognition techniques and present the current state of the art in cursive handwriting recognition. Standard handwriting recognition processes are presented, and each process is described in detail. Some novel segmentation strategies and a segmentation-based approach for automated recognition of unconstrained cursive handwriting are also presented.
In Chapter II, Uchida investigates the theoretical and practical importance of elastic matching for handwriting recognition. He argues that the use of elastic matching techniques instead of rigid matching
techniques improves the robustness of handwriting recognition systems. In addition, the optimized matching represents the deformation of handwritten characters and, thus, is useful for statistical analysis of the deformation. Elastic matching is formulated as an optimization problem of planar matching, or pixel-to-pixel correspondence, between two character images under a certain matching model such as affine and nonlinear.
The next two chapters focus on off-line signature verification. In Chapter III, Batista, Rivard, Sabourin,
and Granger present the current state of art in automatic signature verification. Automatic signature verification is a biometric method that can be applied in all situations where handwritten signatures are used, such as cashing a check, signing a credit card, and authenticating a document. They review existing
approaches in the literature and present a survey of the most important techniques used for feature extraction and verification in this field. They also present strategies used for problems such as limited amounts of data and show important challenges and some new research directions.
xiv
In Chapter IV, Madasu and Lovell present an off-line signature verification and forgery detection system based on fuzzy modeling. The various handwritten signature characteristics and features are first studied and encapsulated to devise a robust verification system. The verification of genuine signatures and detection of forgeries is achieved via angle features extracted using a grid method. The derived features are fuzzified by an exponential membership function, which is modified to include two structural
parameters. The structural parameters are devised to take into account the possible variations due to handwriting styles and to reflect other factors affecting the scripting of a signature. The proposed system has been tested on a large database of signatures comprising more than 1,200 signature images obtained from 40 volunteers.
Chapters V and VI focus on speech recognition. In Chapter V, Suárez-Guerra and Oropeza-Rodriguez present the state of the art in automatic speech recognition. Speech recognition is very challenging for researchers in many fields, including computer science, mathematical statistics, applied artificial intelligence,
and linguistics. The unit of essential information used to characterize the speech signal in the most widely used ASR systems is the phoneme. However, several researchers recently have questioned this representation and demonstrated the limitations of the phonemes, suggesting that ASR with better performance can be developed replacing the phoneme by triphones and syllables as the unit of essential information used to characterize the speech signal. This chapter presents an overview of the most successful
techniques used in ASR systems, together with some recently proposed ASR systems that intend to improve the characteristics of conventional ASR systems.
In Chapter VI, Leedham, Pervouchine, and Zhong investigate features of handwriting and speech and their effectiveness at determining whether the identity of a writer or speaker can be identified from handwriting or speech. For handwriting, some of the subjective and qualitative features used by document
examiners are investigated in a scientific and quantitative manner based on the analysis of three characters (d, y, and f) and the grapheme th. For speech, several frequently used features are compared for their strengths and weaknesses in distinguishing speakers. The results show that some features do have good discriminative power, while others are less effective. Acceptable performance can be obtained in many situations using these features. However, the effect of handwriting forgery/disguise or conscious speech imitation/alteration on these features is not investigated. New and more powerful features are needed in the future if high accuracy person identification can be achieved in the presence of disguise or forgery.
In Chapter VII, Yu, Pham, and Yan present a new pattern recognition method using morphological structure. First, smooth linearization is introduced based on various chain codes. Second, morphological structural points are described in terms of smooth followed contours and linearized lines, and then the patterns of morphological structural points and their properties are given. Morphological structural points are basic tools for pattern recognition-based morphological structure. Furthermore, how the morphological
structure can be used to recognize and classify images is presented. One application is document image processing and recognition, analysis, and recognition of broken handwritten digits. Another one is dynamic analysis and recognition of cell-cycle screening based on morphological structures.
In Chapter VIII, Shan, Bigdeli, Lovell, and Chen propose a variability compensation technique that synthesizes realistic frontal face images from nonfrontal views. It is based on modeling the face via active
appearance models and estimating the pose through a correlation model. The proposed technique is coupled with adaptive principal component analysis (APCA), which was previously shown to perform well in the presence of both lighting and expression variations. The proposed recognition techniques, although advanced, are not computationally intensive. So they are quite well suited to the embedded system
environment. Indeed, the authors have implemented an early prototype of a face recognition module on a mobile camera phone so the camera could be used to identify the person holding the phone.
xv
In Chapter IX, Guha, Mukerjee, and Venkatesh present complex multi-object interactions resulting in occlusion sequences that are a visual signature for the event. In this chapter, multi-object interactions are tracked using a set of qualitative occlusion primitives derived on the basis of the persistence hypothesis—
objects continue to exist even when hidden from view. Variable length temporal sequences of occlusion primitives are shown to be well correlated with many classes of semantically significant events. In surveillance applications, determining occlusion primitives is based on foreground blob tracking and requires no prior knowledge of the domain or camera calibration. New foreground blobs are identified as putative objects that may undergo occlusions, split into multiple objects, merged back again, and so forth. Significant activities are identified through temporal sequence mining, which bear a high correlation
with semantic categories. Thus, semantically significant event categories can be recognized without assuming camera calibration or any environmental/object/action model prior.
In Chapter X, Jia and Zhang review human detection techniques. Human detection is the first step for a number of applications such as smart video surveillance, driving assistance systems, and intelligent digital content management. It is a challenging problem due to the variance of illumination, color, scale, pose, and so forth. This chapter reviews various aspects of human detection in static images and focuses on learning-based methods that build classifiers using training samples. There are usually three modules for these methods: feature extraction, classifier design, and merging of overlapping detections. The chapter reviews most of the existing methods for each module and analyzes their respective pros and cons. The contribution includes two aspects: first, the performance of existing feature sets on human detection are compared; second, a fast human detection system based on the histogram of oriented gradients features and a cascaded Adaboost classifier is proposed. This chapter is useful for both algorithm researchers and system designers in the computer vision and pattern recognition communities.
In Chapter XI, Tivive and Bouzerdoum present a brain-inspired pattern recognition architecture. With the ever-increasing utilization of imagery in scientific, industrial, civilian, and military applications, visual pattern recognition has been thriving as a research field and has become an essential enabling technology for many applications. In this chapter, a brain-inspired pattern recognition architecture that easily can be adapted to solve various real-world visual pattern recognition tasks is presented. The architecture has the ability to extract visual features from images and classify them within the same network structure; in other words, it integrates the feature extraction stage with the classification stage, and both stages are optimized with respect to one another. The main processing unit for feature extraction is governed by a nonlinear biophysical mechanism known as shunting inhibition, which plays a significant role in visual information processing in the brain. The proposed architecture is applied to four real-world visual pattern
recognition problems; namely, handwritten digit recognition, texture segmentation, automatic face detection, and gender recognition. Experimental results demonstrate that the proposed architecture is very competitive with and sometimes outperforms existing state-of-the-art techniques for each application.
In Chapter XII, Rawski, Selvaraj, Falkowski, and Luba present the discussion on efficiency of various implementation methodologies of DSP algorithms targeting modern FPGA architectures. Nowadays,
programmable technology provides the possibility of implementing digital systems with the use of specialized embedded DSP blocks. In the first place, however, this technology gives the designer the possibility to increase the efficiency of designed systems by exploitation of parallelisms of implemented algorithms. Moreover, it is possible to apply special techniques such as distributed arithmetic (DA). Since in this approach general-purpose multipliers are replaced by combinational LUT blocks, it is possible to construct digital filters of very high performance. Additionally, application of the functional decomposition-
based method to LUT block optimization and mapping has been investigated. The chapter presents results of the comparison of various design approaches in these areas.
xvi
In Chapter XIII, Zhou and Wang present an approach to class-dependent feature selection and a novel support vector machine (SVM). The relative background and theory are presented for describing the proposed method, and real applications of the method on several biomedical datasets are demonstrated. The authors hope that this chapter can provide readers with a different view of the feature selection method and also the classifier so as to promote more promising methods and applications.
In Chapter XIV, Shilton and Palaniswami present a unified introduction to support vector machine (SVM) methods for binary classification, one-class classification, and regression. The SVM method for binary classification (binary SVC) is introduced first and then extended to encompass one-class classification (clustering). Next, using the regularized risk approach as a motivation, the SVM method for regression (SVR) is described. These methods are then combined to obtain a single, unified SVM formulation that encompasses binary classification, one-class classification, and regression (as well as some extensions of these), and the dual formulation of this unified model is derived. A mechanical analogy
for the binary and one-class SVCs is given to provide an intuitive explanation of the operation of these two formulations. Finally, the unified SVM is extended to implement general cost functions, and an application of SVM classifiers to the problem of spam e-mail detection is considered.
In Chapter XV, Faceli, Carvalho, and Souto investigate multi-objective clustering ensembles for clustering techniques. Clustering is an important tool for data exploration. Several clustering algorithms exist, and new algorithms are frequently proposed in the literature. These algorithms have been very successful
in a large number of real-world problems. However, there is no clustering algorithm, optimizing only a single criterion, able to reveal all types of structures (homogeneous or heterogeneous) present in a dataset. In order to deal with this problem, several multi-objective clustering and cluster ensemble methods have been proposed in the literature, including a multi-objective clustering ensemble algorithm. In this chapter, an overview of these methods, which, to a great extent, are based on the combination of various aspects from traditional clustering algorithms, is presented.
In Chapter XVI, Duell and Yao present negative correlation learning in evolutionary ensembles with suitable speciation techniques. Negative correlation learning (NCL) is a technique that attempts to create an ensemble of neural networks whose outputs are accurate but negatively correlated. The motivation for such a technique can be found in the bias-variance-covariance decomposition of an ensemble of the learner’s generalisation error. NCL is also increasingly used in conjunction with an evolutionary process, which gives rise to the possibility of adapting the structures of the networks at the same time as learning the weights. This chapter examines the motivation and characteristics of the NCL algorithm. Some recent work relating to the implementation of NCL in a single objective evolutionary framework for classification tasks is presented, and the authors examine the impact of two different speciation techniques: implicit fitness sharing and an island model population structure. The choice of such speciation techniques can have a detrimental effect on the ability of NCL to produce accurate and diverse ensembles and should, therefore, be chosen carefully. This chapter also provides an overview of other researchers’ work with NCL and gives some promising future research directions.
In Chapter XVII, Tsuji, Bu, and Fukuda present a recurrent probabilistic neural network for EMG pattern recognition. In the field of pattern recognition, probabilistic neural networks (PNNs) have been proven as an important classifier. For pattern recognition of EMG signals, the characteristics usually used are amplitude, frequency, and space. However, significant temporal characteristics exist in the transient
and nonstationary EMG signals, which cannot be considered by traditional PNNs. In this chapter, a recurrent PNN called recurrent log-linearized Gaussian mixture network (R-LLGMN) is introduced for EMG pattern recognition, with the emphasis on utilizing temporal characteristics. The structure of R-LLGMN is based on the algorithm of a hidden Markov model (HMM), which is a routinely used technique for modeling stochastic time series. Since R-LLGMN inherits advantages from both HMM and
xvii
neural computation, it is expected to have a higher representation ability and show better performance when dealing with time series such as EMG signals. Experimental results show that R-LLGMN can achieve high discriminant accuracy in EMG pattern recognition.
Brijesh Verma, Central Queensland University, Australia
Michael Blumenstein, Griffith University, Australia
Editor",Pattern Recognition Technologies and Applications,,,,,core
236257331,2005-01-01T08:00:00,"In this dissertation, we present vision based scene interpretation methods for monitoring of people and vehicles, in real-time, within a busy environment using a forest of co-operative electro-optical (EO) sensors. We have developed novel video understanding algorithms with learning capability, to detect and categorize people and vehicles, track them with in a camera and hand-off this information across multiple networked cameras for multi-camera tracking. The ability to learn prevents the need for extensive manual intervention, site models and camera calibration, and provides adaptability to changing environmental conditions. For object detection and categorization in the video stream, a two step detection procedure is used. First, regions of interest are determined using a novel hierarchical background subtraction algorithm that uses color and gradient information for interest region detection. Second, objects are located and classified from within these regions using a weakly supervised learning mechanism based on co-training that employs motion and appearance features. The main contribution of this approach is that it is an online procedure in which separate views (features) of the data are used for co-training, while the combined view (all features) is used to make classification decisions in a single boosted framework. The advantage of this approach is that it requires only a few initial training samples and can automatically adjust its parameters online to improve the detection and classification performance. Once objects are detected and classified they are tracked in individual cameras. Single camera tracking is performed using a voting based approach that utilizes color and shape cues to establish correspondence in individual cameras. The tracker has the capability to handle multiple occluded objects. Next, the objects are tracked across a forest of cameras with non-overlapping views. This is a hard problem because of two reasons. First, the observations of an object are often widely separated in time and space when viewed from non-overlapping cameras. Secondly, the appearance of an object in one camera view might be very different from its appearance in another camera view due to the differences in illumination, pose and camera properties. To deal with the first problem, the system learns the inter-camera relationships to constrain track correspondences. These relationships are learned in the form of multivariate probability density of space-time variables (object entry and exit locations, velocities, and inter-camera transition times) using Parzen windows. To handle the appearance change of an object as it moves from one camera to another, we show that all color transfer functions from a given camera to another camera lie in a low dimensional subspace. The tracking algorithm learns this subspace by using probabilistic principal component analysis and uses it for appearance matching. The proposed system learns the camera topology and subspace of inter-camera color transfer functions during a training phase. Once the training is complete, correspondences are assigned using the maximum a posteriori (MAP) estimation framework using both the location and appearance cues. Extensive experiments and deployment of this system in realistic scenarios has demonstrated the robustness of the proposed methods. The proposed system was able to detect and classify targets, and seamlessly tracked them across multiple cameras. It also generated a summary in terms of key frames and textual description of trajectories to a monitoring officer for final analysis and response decision. This level of interpretation was the goal of our research effort, and we believe that it is a significant step forward in the development of intelligent systems that can deal with the complexities of real world scenarios",Scene Monitoring With A Forest Of Cooperative Sensors,https://core.ac.uk/download/236257331.pdf,'Information Bulletin on Variable Stars (IBVS)',,,core
20863982,2008-12-03,"This report addresses the problem of making a humanoid robot learn a human partner’s preferences regarding personal space and adapt to these in real-time. An adaptive system using policy gradient reinforcement learning (PGRL) is proposed, implemented and evaluated in an experiment using human subjects. The experiment shows that this is a viable solution to the problem, but that there are some issues that remain to be resolved. Beteendeanpassning för en socialt interaktiv robo",Supervisor at Nada was Henrik Christensen,,,,,core
24619459,2008-04-01,"Abstract — A growing issue in the video game industry is the artificial intelligence that controls the non-player agents in their worlds; as visual and audio quality increases, the simple scripted behavior of the agents is made more obvious. This paper sets out to show how agent control systems can be evolved to provide truly dynamic agent behavior that adapts to the player. A demonstration of the power of this idea was built to model a simple space simulation game. The system allows the player to create and position targets for the game agents to swarm. rtNEAT (a real-time implementation of the NeuroEvolution of Augmenting Topologies method) is used to evolve the neural networks that control the individuals, and this paper details the results. In the future this technique could significantly advance the field of artificial intelligence in games and open up new genres for designers and players alike to explore",Real-Time NeuroEvolution for Game Agent Control,,,,,core
10549613,[2009],"During the 1980s, a community of artificial intelligence researchers became interested in formalizing problem solving methods as part of an effort called ""second generation expert systems"" (2nd GES). How do the motivations and results of this research relate to building tools for the workplace today? We provide an historical review of how the theory of expertise has developed, a progress report on a tool for designing and implementing model-based automation (Brahms), and a concrete example how we apply 2nd GES concepts today in an agent-based system for space flight operations (OCAMS). Brahms incorporates an ontology for modeling work practices, what people are doing in the course of a day, characterized as ""activities."" OCAMS was developed using a simulation-to-implementation methodology, in which a prototype tool was embedded in a simulation of future work practices. OCAMS uses model-based methods to interactively plan its actions and keep track of the work to be done. The problem solving methods of practice are interactive, employing reasoning for and through action in the real world. Analogously, it is as if a medical expert system were charged not just with interpreting culture results, but actually interacting with a patient. Our perspective shifts from building a ""problem solving"" (expert) system to building an actor in the world. The reusable components in work system designs include entire ""problem solvers"" (e.g., a planning subsystem), interoperability frameworks, and workflow agents that use and revise models dynamically in a network of people and tools. Consequently, the research focus shifts so ""problem solving methods"" include ways of knowing that models do not fit the world, and ways of interacting with other agents and people to gain or verify information and (ultimately) adapt rules and procedures to resolve problematic situations",Workflow Agents vs. Expert Systems: Problem Solving Methods in Work Systems Design,https://core.ac.uk/download/pdf/10549613.pdf,,,,core
101853796,2004,"Abstract: Changes in the natural environment affect our quality of life. Thus, government, industry, and the public call for integrated environmental management systems capable of supplying all parties with validated, accurate and timely information. The ‘near real-time ’ constraint reveals two critical problems in delivering such tasks: the low quality or absence of data, and the changing conditions over a long period. These problems are common in environmental monitoring networks and although harmless for off-line studies, they may be serious for near real-time systems. In this work, we discuss the problem space of near real-time reporting Environmental Management Systems and present a methodology for applying agent technology this area. The proposed methodology applies powerful tools from the IT sector, such as software agents and machine learning, and identifies the potential use for solving real-world problems. An experimental agent-based prototype developed for monitoring and assessing air-quality in near real time is presented. A community of software agents is assigned to monitor and validate measurements coming from several sensors, to assess air-quality, and, finally, to deliver air quality indicators and alarms to appropriate recipients, when needed, over the web. The architecture of the developed system is presented and the deployment of a real-world test case is demonstrated",Applying agent technology in environmental management systems under real-time constraints,,,,,core
432183940,2009-06-15T00:00:00,"International audienceTwo production models are candidates for e-science computing: grids enable hardware and software sharing; clouds propose dynamic resource provisioning (elastic computing). Organized sharing is a fundamental requirement for large scientic collaborations; responsiveness, the ability to provide good response time, is a fundamental requirement for seamless integration of the large scale computing resources into everyday use. This paper focuses on a model-free resource provisioning strategy supporting both scenarios. The provisioning problem is modeled as a continuous action-state space, multi-objective reinforcement learning problem, under realistic hypotheses; the high level goals of users, administrators, and shareholders are captured through simple utility functions. We propose an implementation of this reinforcement learning framework, including an approximation of the value function through an Echo State Network, and we validate it on a real dataset",Responsive Elastic Computing,,'Association for Computing Machinery (ACM)',10.1145/1555301.1555311,,core
22184805,2007-11-21,"A distributed monitoring and diagnosis system has been  developed and successfully applied to real-time monitoring  of interplanetary spacecraft at NASA&apos;s Jet Propulsion  Laboratory. This system uses a combination of  conventional processing and artificial intelligence. Four  knowledge-based diagnosis modules are embedded  within a monitoring system that detects on-board spacecraft  anomalies. Each of the four knowledge-based  systems is unique with respect to the others in its implementation  or use, resulting in an interesting set of  performance results that have been used as guidelines for  the design of our next generation real-time diagnostic  systems. Details of the distributed architecture, and the  general characteristics of the embedded diagnostic  are also provided.  1.0 INTRODUCTION  There are numerous definitions for real-time systems,  the most stringent of which involve guaranteeing correct system  response within a domain-dependent or  defined period of time. For applic..",Performance Results of Cooperating Expert Systems in a Distributed Real-time Monitoring System,,,,,core
21031672,2009-05-07,"Heterogeneous clusters and grid infrastructures are becoming increasingly popular. In these computing infrastructures, machines have different resources, including memory sizes, disk space, and installed software packages. These differences give rise to a problem of overprovisioning, that is, sub-optimal utilization of a cluster due to users requesting resource capacities greater than what their jobs actually need. Our analysis of a real workload file (LANL CM5) revealed differences of up to two orders of magnitude between requested memory capacity and actual memory usage. This paper presents an algorithm to estimate actual resource capacities used by batch jobs. Such an algorithm reduces the need for users to correctly predict the resources required by their jobs, while at the same time managing the scheduling system to obtain superior utilization of available hardware. The algorithm is based on the Reinforcement Learning paradigm; it learns its estimation policy on-line and dynamically modifies it according to the overall cluster load. The paper includes simulation results which indicate that our algorithm can yield an improvement of over 30 % in utilization (overall throughput) of heterogeneous clusters",A self-optimized job scheduler for heterogeneous server clusters,,,,,core
4909971,2008-05-01T00:00:00,"IASS-IACM 2008 2008 Session: Educational Software/Structural Monitoring -- 
""Finite element implementation for computer-aided instruction of structural mechanics"" by Jae Young LEE, Sung-Youll AHN (Chonbuk National University) -- 
""MASTAN2, educational analysis software for the 21st century"" by Ronald D. ZIEMIAN (Bucknell University), William McGUIRE (Cornell University) -- ""Spot monitoring and time-dependent analysis of high-rise building construction process"" by 
Shenwei ZHANG (Shandong University), Qilin ZHANG, Xin LOU (Tongji University) --
""A model-based framework for real-time structural monitoring in uncertain environments"" by 
Phaedon-Stelios KOUTSOURELAKIS (Cornell University) -- 
""Prediction of maximum deflection of double layer grid space structure using neural networks"" by 
Reza KAMYAB MOGHADAS (Iranian Academic Center for Education, Culture and Research), Kok Keong CHOONG, Sabarudin MOHD (Universiti Sains Malaysia)",Educational Software/Structural Monitoring,http://hdl.handle.net/1813/11537,Internet-First University Press,,,core
29455383,2007-12-01T00:00:00,"In this paper we present a real-time obstacle avoidance algorithm using a Bayesian neural network for a laser based wheelchair system. The raw laser data is modified to accommodate the wheelchair dimensions, allowing the free-space to be determined accurately in real-time. Data acquisition is performed to collect the patterns required for training the neural network. A Bayesian frame work is applied to determine the optimal neural network structure for the training data. This neural network is trained under the supervision of the Bayesian rule and the obstacle avoidance task is then implemented for the wheelchair system. Initial results suggest this approach provides an effective solution for autonomous tasks, suggesting Bayesian neural networks may be useful for wider assistive technology applications",Obstacle avoidance for power wheelchair using bayesian neural network.,http://hdl.handle.net/10453/2772,,10.1109/iembs.2007.4353406,"[{'title': None, 'identifiers': ['issn:1557-170X', '1557-170x']}]",core
56104766,2008-04-26T00:00:00,"Satisfiability (SAT) of propositional logic formulas is a canonical NP-complete problem; algorithms for its solution have been studied for over forty years. The representational power of propositional logic allows a host of research and real-world problems to be solved using SAT solvers: some prominent areas of application include mathematics, circuit design and verification, and AI planning. 

One technique receiving increasing recent attention is the use of quantified representations for SAT problems. Quantified representations are often more intuitive to use, can require exponentially less space, and in many cases allow speedup through their elimination of isomorphism. This dissertation explores the use of networks of joins operating upon quantified representations to compute key solver functions, including unit propagation and literal choice. The complexity of computing these functions using join networks becomes dependent upon the size of the truth assignment, or potential model explored at a given search space node. Because models are relatively small for many problems of interest, we show efficiency gains in these cases. JOINSAT, the implementation of these ideas, is competitive in performance with a number of state of the art satisfiability solvers",Using Join Networks to Compute Satisfiability,https://core.ac.uk/download/56104766.pdf,,,,core
55739124,2007-01-01T00:00:00,"Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement realtime, isolated digit speech recognition using a Liquid State Machine (a recurrent neural network of spiking neurons where only the output layer is trained). First we test two existing hardware architectures, but they appear to be too fast and thus area consuming for this application. Then we present a scalable, serialised architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures only spanned part of it",Compact hardware for real-time speech recognition using a liquid state machine,https://core.ac.uk/download/55739124.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ijcnn.2007.4371111,,core
210960336,2008-01-01T00:00:00,"In this thesis, computational models of adaptive motor control and visuomotor coordination are explored and developed. These models relate to hypotheses on how sensorimotor processing in biological organisms might be organized at an abstract level; furthermore, these models and their specific implementations offer solutions for technical problems in the domain of adaptive robotics. For this reason, both biological and technical aspects are addressed. On the one hand, this thesis focuses on the learning of so-called internal models (Miall et al., 1993; Kawato, 1999): ""forward models"", which predict the sensory consequences of the agent's own actions, and ""inverse models"", which act like motor controllers and generate motor commands. In this area, new strategies and algorithms for learning are suggested and tested on both simulated and real-world robot setups. This work contributes to the understanding of the ""building blocks"" of integrated sensorimotor processing. On the other hand, this thesis suggests complex models of sensorimotor coordination: In a study on the grasping to extrafoveal targets with a robot arm, it is explored how forward and inverse models may interact, and a second study addresses the question how visual perception of space might arise from the learning of sensorimotor relationships. The theoretical part of the thesis starts with a close view on sensorimotor processing. The cognitivist approach and the embodied approach to sensorimotor processing are contrasted with each other, providing evidence from psychological and neurophysiological studies in favor of the latter. It is outlined how the application of robots fits into the embodied approach as research method. Furthermore, internal models are defined in a formal way, and an overview of their role in models of perception and cognition is provided, with a special emphasis on anticipation and predictive forward models. Afterwards, a thorough overview of internal models in adaptive motor control (covering both kinematics and dynamics) and a novel learning strategy for kinematic control problems (""learning by averaging"") are presented. The experimental work comprises four different studies. First, a detailed comparison study of various motor learning strategies for kinematic problems is presented. The performance of ""feedback error learning"" (Kawato et al., 1987), ""distal supervised learning"" (Jordan and Rumelhart, 1992), and ""direct inverse modeling"" (e.g., Kuperstein, 1987) is directly compared on several learning tasks from the domain of eye and arm control (on simulated setups). Moreover, an improved version of direct inverse modeling on the basis of abstract recurrent networks and learning by averaging are included in the comparison. The second study is dedicated to the learning of a visual forward model for a robot camera head. This forward model predicts the visual consequences of camera movements for all pixels of the camera image. The presented learning algorithm is able to overcome the two main difficulties of visual prediction: first, the high dimensionality of the input and output space, and second, the need to detect which part of the visual output is non-predictable. To demonstrate the robustness of the presented learning algorithm, the work is not carried out on plain camera images, but on distorted ""retinal images"" with a decreasing resolution towards the corners. In the third experimental chapter, a model for grasping to extrafoveal (non-fixated) targets is presented. It is implemented on a robot setup, consisting of a camera head and a robot arm. This model is based on the premotor theory of attention (Rizzolatti et al., 1994) and adds one specific hypothesis: Attention shifts caused by saccade programming imply a prediction of the retinal foveal images after the saccade. For this purpose, the visual forward model from the preceding study is used. Based on this model, several grasping modes are compared; the obtained results are qualitatively congruent with the performance that can be expected from human subjects. The fourth study is based on the theory that visual perception of space and shape is based on an internal simulation process which relies on forward models (Moeller, 1999). This theory is tested by synthetic modeling in the task domain of block pushing with a robot arm",Adaptive Internal Models for Motor Control and Visual Prediction,,'Logos Verlag Berlin',,,core
23702691,2007-11-22,"The initialisation of a neural network implementation of Sammon&apos;s mapping, either randomly or based on the principal components (PCs) of the sample covariance matrix, is experimentally investigated. When PCs are employed, fewer experiments are needed and the network configuration can be set precisely without trial-and-error experimentation. Tested on five real-world databases, it is shown that very few PCs are required to achieve a shorter training period, lower mapping error and higher classification accuracy, compared with those based on random initialisation.  Keywords- classification, data projection, initialisation, Sammon&apos;s mapping, neural networks, principal component analysis (PCA)  1. Introduction  Sammon&apos;s nonlinear mapping [1] is a projection method for analysing multivariate data. The method attempts to preserve the inherent structure of the data when the patterns are projected from a higher-dimensional space to a lower-dimensional space by maintaining the distances between..",On the Initialisation of Sammon&apos;s Nonlinear Mapping,,,,,core
21015895,2009-04-07,"The ability to learn is a potentially compelling and important quality for interactive synthetic characters. To that end, we describe a practical approach to real-time learning for synthetic characters. Our implementation is grounded in the techniques of reinforcement learning and informed by insights from animal training. It simpli-Þes the learning task for characters by (a) enabling them to take advantage of predictable regularities in their world, (b) allowing them to make maximal use of any supervisory signals, and (c) making them easy to train by humans. We built an autonomous animated dog that can be trained with a technique used to train real dogs called “clicker training”. Capabilities demonstrated include being trained to recognize and use acoustic patterns as cues for actions, as well as to synthesize new actions from novel paths through its motion space. Akey contribution of this paper is to demonstrate that by addressing the three problems of state, action, and state-action space discovery at the same time, the solution for each becomes easier. Finally, we articulate heuristics and design principles that make learning practical for synthetic characters",Abstract Integrated Learning for Interactive Synthetic Characters,,,,,core
20958535,2008-12-30,"Intelligent systems based on machine learning techniques, such as classification, clustering, are gaining wide spread popularity in real world applications. This paper presents work on developing a software system for predicting crop yield, for example oil-palm yield, from climate and plantation data. At the core of our system is a method for unsupervised partitioning of data for finding spatio-temporal patterns in climate data using kernel methods which offer strength to deal with complex data. This work gets inspiration from the notion that a non-linear data transformation into some high dimensional feature space increases the possibility of linear separability of the patterns in the transformed space. Therefore, it simplifies exploration of the associated structure in the data. Kernel methods implicitly perform a non-linear mapping of the input data into a high dimensional feature space by replacing the inner products with an appropriate positive definite function. In this paper we present a robust weighted kernel k-means algorithm incorporating spatial constraints for clustering the data. The proposed algorithm can effectively handle noise, outliers and auto-correlation in the spatial data, for effective and efficient data analysis by exploring patterns and structures in the data, and thus can be used for predicting oil-palm yield by analyzing various factors affecting the yield",A Software Framework for Predicting Oil-Palm Yield from Climate Data,,,,,core
101989346,2004,"A wireless communication system using multiple antennas promises reliable trans-mission under Rayleigh 
at fading assumptions. Design criteria and practical schemes have been presented for both coherent and non-coherent communication channels. In this paper we generalize one dimensional phase shift keying (PSK) signals and introduce space time constellations from generalized phase shift keying (GPSK) signals based on the complex and real orthogonal designs. The resulting space time constellations real-locate the energy for each transmitting antenna and feature good diversity products, consequently their performances are better than some of the existing comparable codes. Moreover since the maximum likelihood (ML) decoding of our proposed codes can be decomposed to one dimensional PSK signal demodulation, the ML decoding of our codes can be implemented in a very ecient way. Index Terms{space-time coding, multiple antennas, orthogonal designs, diversity, phase shift keying ",Generalized PSK in space time coding,,,,,core
4967880,2009-01-01T08:00:00,"The objective of this work is to develop a framework, along with the tools required, for the development of process monitoring solutions. In most cases, it is impractical to develop precise models from first principles for monitoring purposes as it requires consideration of not only the complex physics involved in the process but also the interactions between different components constituting the process. In these cases, soft computational methods, which can make use of process data to capture its trends and dynamics, provide an attractive alternative for the quick development and deployment of process monitoring solutions. Firstly, signal based methods based on feature-level sensor fusion are considered for monitoring purposes. The problem of optimal sensor selection is formulated as the problem of selecting optimal groups of inputs during linear or non-linear model training from data. Novel penalty terms called as Group Selection Terms (GST) are derived based on hierarchical Bayesian modeling. A generalized algorithm based on the “Bound Optimization” approach is derived for simultaneously selecting the optimal sensors, sensor-features and model parameters from data. Three specific algorithms called Linear Embedded Sensor Selection (L-ESS), Nonlinear Embedded Sensor Selection (NL-ESS) and Sparse Multiple Kernel Learning (SMKL) are derived for training models which allow the user to tradeoff the number of points they can handle versus the degree of non-linearity allowed by the model. The ability of these algorithms to learn models which use fewer groups of features while achieving a high degree of prediction accuracy is tested using real data sets. Finally, NL-ESS is tested on experimental data obtained for the purpose of monitoring burn and chatter conditions during cylindrical plunge grinding. While the performance of the regularization based methods was found to be satisfactory in terms of prediction accuracy as well as the sparsity of the obtained solution, one of its main drawbacks is the fact that it requires the tuning of one or more tradeoff parameters. In an attempt to further automate the process of sensor and sensor-feature selection, the hierarchical Bayesian formulation is extended further and two algorithms, Variational Relevant Group Selector (VRGS) and the Relevant Group Selector (RGS), are derived to estimate the parameter probability distributions directly from the data. Experimental results using data from plunge grinding monitoring and diesel engine diagnostics verify the excellent performance of the algorithms without the need for any manual parameter tuning. As a complementary approach, some of the insights gained from classical model based fault detection and isolation (FDI) methods are exploited by making use of data based dynamic process models. In this work Recurrent Neural Networks (RNN) are used to capture the dynamics of any system that can be represented in the state space form. A novel constructive procedure for training RNNs from data is proposed. A systematic method for incorporating partial state measurements into the structure of a RNN is proposed where the measured states are augmented with hidden node activations to get a complete dynamic model of the system. A robust stochastic nonlinear discrete observer called the Adaptive Divided Difference Filter (ADDF) is developed for simultaneous state and parameter estimation in uncertain systems with the goal of combining it with the RNN system model for data based FDI. All the modules of this framework are validated using simulation examples","Data-based process monitoring, fault detection and diagnostics",,'Purdue University (bepress)',,,core
235571330,2004-01-14T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.Vol 2-02 PEOPLE CONNECTING WITH PEOPLE January 14, 2004
heasants Forever
Alarmed by a continuing decline
in pheasant numbers, a local group
has decided to take action.
A couple of years ago, they have
formed a southern Alberta chapter of
Pheasants Forever, a group that boasts
about 100,000 members in North
America, mostly in the United States.
Chapters have also been established in
Medicine Hat, Brooks and Calgary.
The aim of the group is to pre­serve
habitat for the pheasants in the
south, says one of eight local board
members, Brent Ehlert.
Magrath has always been one of
the best spots for pheasant hunting in
southern Alberta, he says. The bird
isn’t native to the area though, he
explains, and it requires healthy envi­ronment
for it to survive year round.
Unfortunately that habitat has
been disappearing at an alarming rate,
he says.
Fence lines, ditches, tree stands
and other kinds of cover have been
erased as farmers seek to take advan­tage
of as much land as possible for
growing crops.
The decline of the species was
initially sparked in the 60s when there
was an open season on hens.
“That was the beginning of the
end. You can’t kill the female of the
Brent Ehlert ofMagrath, showing the protected habitat at the Deerfield Colony north of town.
species and expect them to survive.”
So far the group’s efforts have
focused on keeping the habitat that is
currently available, rather than creating
new spots.
The group is receiving a lot of
support, says Ehlert and has had a lot
of success in various —
areas in the two years it
has been operating. It has received
support and has worked with other
groups like Ducks Unlimited, Alberta
Conservation, irrigation districts and
Hutterite Colonies.
The group’s major fundraiser is a
banquet held in Lethbridge every
September, where they have raised over
$60,000 each of the last two years. The
money raised goes toward buying trees
and
shrubs or trucking them, fencing off areas and last year the
group funded a summer employee with St. Mary’s Irrigation.
“We are making headway,’’ says Ehlert. “We rely on
the farmers and ranchers to help us out.” He finds many of
them have been very cooperative.
“If you have the habitat, everything else comes -
pheasants, song birds, deer...” he says.
He points out that last year’s hunting season from mid­October
to mid-November was one of the best in recent
years, although nobody knows why for sure.
“But we know that the pheasants can still be here if we
can maintain and keep the habitat we have.”
At the present time, the group has about six habitats
they are working with in southern Alberta.
Those wanting more information or wanting to join
Pheasants Forever can phone (403) 758-3712 to get more
details.
“We just want to have something for the young person
growing up so pheasant hunting doesn’t become a lost art,”
says Ehlert. ♦
Hagrath Rod And Sun Club
HORN MEASURING NIGHT
Friday, January 16
7;00 pm at the Club House
Only Clean Entries Will Be Accepted
No Late Entries.
STEELE SHERIDAN
RRfl-20-18, Lethbridge, Alberta Tl J 4P4
No Sunday Deliveries
Dtmestlc & Commercial Water Hauling
Cistern Cleaning
Road Snraylng
Bus: 328-2460
<ucum Published weekly on Wednesdays by Keviine Communications
Box 179, Magraih, AB TOK 1J0 Ph: 758-6911 • Fx: 758-3661
email magrathnews@telus.net
Ad deadline is Friday at 5pm and may be dropped off at the .Magrath
Pharmacy or at Keyline Communications' office
at 14 Centennial Place,
Duane & Carma Thomson's home.
S^WiShorts r Magrath Grade 7 Girls played Raymond on
Thursday, January 8. Raymond won 39 - 26. Sarah
Balderson was Magrath’s top scorer with 10 points. The two
top rebounders were Jordan Hansen and Sarah Balderson.
The Grade 8 Boys recorded one win before the
Christmas break, two wins after and remain with an
unspoiled record. They were all great games of basketball -
especially against Raymond where we pulled off the win in
the last minute of the game.
Magrath 60 vs Stirling 47
Conn Barnett 14. Russ Bennett 14
Magrath 39 vs St Francis 27
Nick Haynes 8. Russ Bennett 8
Magrath 46 vs Raymond 44
Nick Haynes 23. Russ Bennett 8
All the players on the team contribute in some way. It
may not always be with points but passing assists (which we
are great at!), rebounds and great team spirit. We invite you
to come watch us in action Thursday, January 22 at 3:30 in
the Auditorium! ♦
Heating &
Air Conditioning
• New Furnace Installation
• Repair all types of
Furnaces
• Commercial Heating /
Air Conditioning
• Change Hot Water Tanks
• Custom Sheet Metal and
Light Welding
Also: Custom Repairs of
Stock Trailers
Call Rick at
758-3353
30 years of Experience
Journeyman status
Located 3Km west of Magrath on
Sister Melinda Cole, daughter of Con & Gail Cole,
returned from the Idaho Pocatello Mission on January 7,
2004. She will be speaking in 6th Ward on January 25, at
1:00 pm in the Stake Centre.
Annual General Meeting of the
Magrath History and Museum Association
will be held January 22, 2004 at 7:00 p.m.
Everyone is welcome to attend.
We Really Need Your Support!
The monthly board meeting will be held
at 7:30 p.m. on the same evening.
Again you are welcome to attend.
The Museum needs your help -
NO PREVIOUS EXPERIENCE NECESSARY!
A reminder, anyone who would like to contribute
their time, donations, talents, historic photos or
memorabilia to the new Hall of Fame display
please contact: Donna Lybbert 758-3896 or
Lawrence Turner 381-2229
'TRamIc MOU.
We would like to express our gratitude ?o 1 thank­fulness
to the pixies who surprised us with multitudes
of gifts during the holiday season. Thank you for your
generosity and care.
Shawna and kids
PILLING
153 E IASI North
Magrath, AB
PILLING FURNITURE DIRECT INC.
High End Furniture Wholesaled Directly to You!
Lisa Haynes
758-3068
Cell: 317-3812
By Appointment Only
• Bedrooms
• Dinning Rooms
• Pool Tables
• Leather
• Matrasses
cbdrmed... rm sure/
Italian Charm Bracelets and Charms.
Buy 10 Charmes in a single purchase &
receive a Starter Bracelet for FREE or
Purchase $100 of product in a single purchase & receive
an Etched Crystal Figurine for FREE (Value of $40)
Call Us Today 394-6755 SKSS”
Water Tennis Anyone?
Saturday morning, January 10th, residents of Magrath
living in the south end of town gazed out on a newly formed
lake that covered the tennis courts and surrounding area.
About 8:00 am that morning, the town crew arrived on the
scene to see water flowing out of the doorway of the old
pump house that was a part of the original water system for
the town.
Sometime in the middle of the night, an old 4 inch steel
elbow pipe ruptured in the pumphouse’s basement. The 10
foot deep basement was soon filled and water began to flow
outside, covering the tennis court and surrounding area.
Eventually the two shut off valves were located and
turned off. At about 10:00 am the fire department began
pumping the basement and by about 3:30 pm there was still
three feet of water in the building.
It was about 9:30 pm when the fire department com­pleted
their job. ♦
New MHS Admin
Offices Now in Use
Left: Ila Eblert, secretary; Roger Baldty, principal; Darryl Christensen,
vice principal; and sitting, Cory Beres, Administrative Assistant.
The long anticipated day of the new high school offices
being completed has arrived. Moving took place this past
weekend as high school records, equipment and furniture
were moved to the newly renovated area in the hall north of
the main Tom Karren Gym entrance.
There is now office space for all the administrative
staff. With the extra room comes not only more work sta­tions
but more storage room and as Ila Ehlert, the high
school secretary says, “We can keep things more organized.”
“The advantage of having the two offices separated
(Elementary and Jr/Sr High School) is the work loads have
increased over the last few years to the point that it’s very
difficult to try and keep track of both schools,” Ehlert says.
We’ll be able to operate much more efficiently because we’ll
be focusing on our school.” The workload increase is partly
due to keeping closer track of attendance and more fundraising.
Hours of operation for the Magrath Jr/Sr High School
are the same as before, 8:00 am - 4:30 pm, but right now
they’re closing during lunch, 11:45 am -12:45 pm.
The elementary school’s hours are: 8:15 am - 3:45 pm,
also closing for the lunch hour, 11:45 am -12:45 pm. ❖
Brent Ehlert
Family Service Advisor
Free Estate Planning Guide
Prearranged Funerals
Away From Home Protection
Planning Group of Alberta
1003 - 4th Avenue South (Lower Level)
Lethbridge, Alberta T1J 0P7
Bus: 403-327-3195 • Res: 403-758-3712 • Cell: 403-308-4838
Representing: Christensen Salmon Funeral Home & Crematorium
and Salmon Funeral Home
9a^SSSia INGi&J
Holland Insurance (Magrath) LTD.
AGENTS FOR
The Alberta Government has passed
bill 53 AUTOMOBILE RATE FREEZE. This freeze
applies to all mandatory and optional insurance for
all classes of vehicles and is effective on or after
October 30,2003. This freeze is in effect for 18 months.
Any renewals that wrere issued with a premium
increase on or after Oct. 30th will be issued a credit.
Renewals issued after Feb. 2004 will have it’s premium
froze to last years rates.
PLEASE NOTE BILL 53
WILL INCREASE YOUR PREMIUMS
IF ONE OF THE FOLLOWING OCCURS.
1. An at fault accident.
2. Convicted of a criminal offense relating
to operation of an automobile.
3. Buy a new vehicle.
4. Change of address.
5. Add an additional driver to your policy.
The government is trying to come up with a new
insurance policy that all drivers will pay the same
base premium. Therefore no AGE, SEX, MARITAL
STATUS OR TERRITORY DISCRIMINATION.
So the 16 year old male living in Calgary will pay the
same base rate as a married 50 year old in Magrath
or rural area. You hear “them” saying government
insurance is cheaper in B.C. or Sask. And “they’re”
right if you are under age, single, have an impaired,
tickets or accidents. The average “Joe’s” premium is
still cheaper in Alberta. With Government insurance
the average “Joe” subsidizes the higher risks.
_____ » Driver’s Licenses, Plate Renewals, Driver’s Testing,
Pay F'bb> Birth Certificate, Marriage Licence, Death
rcwtocs Certificate, Annual Returns, Curporate Searches, Etc.
WE SELL TRAVEL INSURANCE, $1 .OO/day (restrictions oppiy)
Phone: Ted, Kathy or Jewelenc at
1-403-758-3391 Fax: 1-403-758-6607
ARP Preferred Brands are private label brands that
money with the same high quality content as more
expensive national brands. Why pay more?
only $3.08
save you
COMPARE
Pepto-Bismol 230 ml reg $5-33
vs. ARP Preferred Bismuth 237ml
COMPARE
Tylenol 325mg 100
Acetaminophen Caplets reg $9.91
vs. Novo-Gesic 325mg 100
Acetaminophen Caplets
only $2.09
COMPARE
NeoCitran Colds & Flu
10 pouches reg $8.91 vs
ARP Preferred Hot Lemon
Relief Cold & Flu 10 pouches
only $3o76
COMPARE
Advil 200mg 100 Ibuprofen Tablets
reg $13.02 or Motrin 200mg
100 Ibuprofen Tablets reg $12.64
vs. ARP Preferred 200mg 100
Ibuprofen Tablets
only $6.36
Magrath
80 South 1 st Street West
Mon-Fri 9:00am - 6:00pm
Ph: 758-3001 • Fax 758-3505
After Hours: 758-6222 • 382-0749
Seeing Things in a
Whole New Light
By Rick Humphreys
Jake Loose- owner of
Apex Thermal Imaging.
Do you worry that
your home may not be well
insulated? Do you suspect
that you may be losing heat
but don’t know where it is
leakingout? Or are you
concerned that you may
have wiring problems that
could eventually result in a
fire?
There is a new busi­ness
in town that can help
you find answers to these
questions. Apex Thermal
Imaging, owned and oper-ated
by Jacob (Jake) Loose, uses infrared photographic
technology to help you literally see the sources of your
problems.
Infrared energy is light that is not visible to the human
eye. It's the part of the electromagnetic spectrum that we
perceive as heat. Specialized equipment, such as night
vision goggles and infrared cameras are able to “see” this
type of light and
convert it into
images that we can
see.
Jake followed
his dream and pur­chased
a digital
Inframetrics cam­5.2'
C
- 2
L- Q
0.0’C
era to assist home
owners and busi­nesses
locate and
evaluate heat (or
heat loss) prob­lems.
He took an
intensive training
course supplied
by the camera
manufacturer in
order to learn the
proper way to
Heat Loss Area-Foundation on house is
loosing beat due to an undeveloped cold
room without insulation. More beat is being
lost here than from the kitchen windows
(marked SPO1+).
utilize the camera and interpret the resulting images.
In a typical home heat loss investigation, Jake will go
to the home at night and take multiple exterior and interior
infrared pictures. After downloading the images to a
computer, using specialized software, he creates a report
which includes images and data which provide the heat loss
information. Lastly, he delivers the report to the customer
and helps them understand where and how much heat is
being lost.
Heat loss is a comfort and economical problem, but
faulty electrical wiring is the cause of many fires. Infrared
images of walls and electrical panels will show any existing
heat spots, indicating that there may be a problem with
wiring in those locations.
Infrared imaging can assist in many business applica­tions.
By imaging flat roofs on large buildings, the pictures
can pinpoint specific locations of leaks. The company can
then fix localized problems, rather than replacing the entire
roof covering.
Images of mechanical equipment can show where
there is undesired heat build-up, indicating worn or failing
parts such as bearings. This helps the business do proper
preventative maintenance to keep expensive equipment up
and running.
Jake is excited about this technology and how it can
provide the first step for people to make their homes and
businesses more comfortable, economical and safe. For
further information, you can call Apex Thermal Imaging at
315-0014. ♦
Christensen Salmon
Funeral Home & Crematorium
Bus: (403) 329-1883
327 - 10th Street South
Lethbridge, AB TU 2M7
Since 1927
Marilyn & Reg Eyre
Independent Representatives
GID 1445516111
Box 35 - Welling, Alberta
Phone: (403) 752-3723
Cell: (403) 308-3644
Email: rmeyre@myexcel.ca
www.globalsuccess2000.com/rmeyre
Customer Service: 1-877-866-0865
Powered by
VarTec Telecom
Community Calendar - January 2004
The Community Calendar is yours to post any
upcoming events FREE OF CHARGE that
others might want to know about.
Please phone in your
information to die Magrath News at 758-6911.
Wed 14 Thur 15 Fri 16 Sat 17
5 pm - Seniors’
Supper
Grade 7 basketball
host Mtn. View
7:00 pm-Rod& Gun
Qub-Hom measuring
6 pm-JV/V boys bas­night
ketball host Taber
Sun 18 Mon 19 Tues 20 Wed 21 Thur 22 Fid 23 Sat 24
5pm-Seniors
Supper
Grade 9 basketball
host Raymond
Bingo @ Seniors’
Centre
12:15 pm-
Hementary School
Awards Assembly
Town Dog
Licences due
Grade 8 basketball
hostCardston
7 pm- Magrath
Museum Annual
General Meeting
Sun 25 Mon 26 Thes 27
Town Council Meeting
Wed 28 Thur 29 Fri 30
NO SCHOOL
5 pm - Senior’s Pot
Luck Supper
Sat31
1-4 pm-Magrath
Golf Chib Social for
Vem Wakefield
High Bench Placer Diggin's
Lance B. Harker
(story continued from last weeks issue)
That's one of the marvels of prospecting lunacy, getting
into places you have no business getting into. All the laws of
physics and probability just go right out the window. But not
the law of gravity... oh no, it has an iron, tenacious grip on
reality.
Well now, I had to get back down, because I certainly
couldn't go up. You can't climb a boulder clay cliff, no mat-ter
how high on prospecting, or any other heady stimulant,
you are. So, I took my first step down.
It actually wasn't so bad. I just leaned back into the hill
and put all my weight on that still-squishy boot heel.
Miraculously, it held, and I took another step. The bucket of
gravel felt like it must surely be mostly gold! Or, I was just an
idiot that had severely overloaded it, but no matter; this was
easier than I thought. I was now in amongst the smaller
boulders, the ones that had dogged me on the way up.
I took several more steps and then one of those lurking
tree branches snagged my boot. That bucket just kicked out
in front of me like it was on rocket-assisted autopilot. Well,
Newton sure was right about gravity—it grabbed me right then
and there. I don't know the mathematical formula for what
happened, or the principle of physics that took over, but it all
took place at about
the speed of light.
My brain franti­cally
went into disas­ter
salvation correc­tion
mode, and I
promptly, yet grace­fully
yanked myself
back as hard as I
could, bringing the bucket toward me. The problem was that
my feet had already headed down the mountain, and all I did
with my serene corrective maneuver was succeed in launch­ing
both feet further away from my point of most-precarious
equilibrium.
At a distance, say from the other side of the canyon, I'm
sure it looked like someone had shot something up on the
side of the mountain: some ugly beast, a raging bull moose,
or some other type of smelly obnoxious varmint (any of
which category I easily qualify for after three glorious weeks
in the bush), but nonetheless, it must have appeared that
some tortured, wracked savage form, thrashing out its death­throes,
was hurtling its way down the slope.
The real truth is that I was magnificently in control,
supremely in command. The fact that my rubber boots were
throwing off more smoke than a good smudge fire was only
Devonshire Realty Inc.
Jim Anderson agent
RESIDENTIAL - FARM ACREAGE -
COMMERCIAL IN MAGRATH AND AREA
2 Houses for Sale in Del Bonita
Comparative Market Analysis
(No Charge) - For people interested in getting an
evaluation of marketability of your property
Phone 758-6725 (leave message)
331-8882 (cellular)
Stitch
i
i
i
1
ii1
]
1
i
i
i
Ji
Stephanie Humphreys and Donna Thompson
50 North 1A Street East • phone: 758-3234
New Hours
Tuesday - Friday 10:00 am - 6:00 pm
Saturday 11:00 am - 3:00 pm
Evenings by Appointment.
and more.
(until January 24,2004)
Sign up for classes for all ages.
»□¡c)idtcJ|d|clielicl|cJ|cJiEJ|cJ|EJ|didicJÍa
tal------------ ”
aa
a
I?
tai
lâl
i°]
2^^? Libra Massage Therapy
Kim Par.czak - GMT^ RMT
* Relaxation • Therapeutic
Is? 1 VuVC*rO’>)755~521O
TOK I.JO Fo> appomtment
a
a
a
a
â5
a
Public Notice
1. Business Licenses, as required by By-Law
#897 arc being issued at the Town Office for
the 2004 business vear.
2. Dog Licenses are due to be renewed in
January. Cost - $20 not neutered or spayed;
$10 with proof of procedure being done.
3. 2004 Residential Water Charge -
$32 / month. Water prepayment plan with
the applicable one month discount is avail­able
to Town residential properties during
the month of JANUARY ONLY. The 2003
water account must be paid in full before
one can participate in the prepayment plan.
4. Tax prepayment Plan is available to property
owners witli taxes in a CURRENT STATUS.
This plan available only until January 31,
2004.
5. The Magrath Golf Club, in co-operation with
the Town of Magrath, provides one day each
season for a golf fund-raiser. Any non-profit
organization interested in hosting this event,
must make written application to the Town of
Magrath, Box 520, Magrath AB, TOK 1J0.
Final date to receive applications is Friday,
February 27, 2004.
PIONEER IRRIGATION AREA OF CANADA
my clever attempt to keep the bugs at bay as I tried to find my
brakes amongst the boulders. The feet that the three spare
gold pans in my backpack were absorbing more shock than
a crash-test-dummy doing macli V into a concrete wall was
only a minor annoyance, a brief test of my prospecting mettle.
At last, much more battered, but still breathing (though
hot and ragged those breaths were, I can tell you), I came to
a sudden stop. In fact, it just so happened that some far
friendlier branches, much more amiable than the evil one
that had originally tripped me up, had halted my avalanching,
yet quite ballet-like, plunge.
For those with a sense of the divine in nature, this was
the penultimate moment—the human at one with the moun­tain—
still alive. Yet, most remarkable of all, none of the dirt
had spilled from my bucket. Yes, that is the true heroism in
this high placer tale— not a stone lost from the bucket, not
one grain of sand!
Rearranging all my joints took considerably longer
than I thought it should have, but soon with all parts more
or less functioning, I was on my way again, with renewed
confidence in my abilities. This time it was only boulder clay,
quite laid out like a sinister lava flow, but boulder clay
nonetheless.
Remember, I had steps carved in it to safely guide my
footsteps back down again. At some point, you'd think the
brain would revolt and refuse to command the muscles in an
instance like this, where the whole body has just faced immi­nent
extinction at the hands of an ambitious idiot bent on
sampling some dirt, but no, the brain can always be overrid­den.
I know where the master switch to disarm it is. I've used
it many times, and sb’ll, I live to tell this tale. This is proof
that life is full of mysteries, not easily solved by rational
thought, or by overly predictable theories.
At any rate, about a dozen steps down, the clay remem­bered
one of its admirable qualities, the slipperier-than-
Teflon one, and off I went again. This time it was only a gentle,
yet thorough pummeling and bashing that lasted a mere
twenty feet, and I came to a feather-like stop on the gravel
below, the contents of the bucket still serenely undisturbed.
After I picked a pan full of golf ball-sized gravel out of
my teeth, replaced my left eyeball, and checked to see
whether the protrusion from my shoulders really was my
neck and that it was still attached to my head, so that ","Magrath Store News (January 14, 2004)",,J. A. Ririe,,,core
199600379,2007-01-01T00:00:00,"The World Wide Web is transitioning from being a mere collection of documents that contain useful information toward providing a collection of services that perform useful tasks. The emerging Web service technology has been envisioned as the next technological wave and is expected to play an important role in this recent transformation of the Web. By providing interoperable interface standards for application-to-application communication, Web services can be combined with component-based software development to promote application interaction and integration within and across enterprises. To make Web services for service-oriented computing operational, it is important that Web services repositories not only be well-structured but also provide efficient tools for an environment supporting reusable software components for both service providers and consumers. As the potential of Web services for service-oriented computing is becoming widely recognized, the demand for an integrated framework that facilitates service discovery and publishing is concomitantly growing.In our research, we propose a framework that facilitates Web service discovery and publishing by combining clustering techniques and leveraging the semantics of the XML-based service specification in WSDL files. We believe that this is one of the first attempts at applying unsupervised artificial neural network-based machine-learning techniques in the Web service domain. Our proposed approach has several appealing features: (1) It minimizes the requirements of prior knowledge from both service providers and consumers, (2) It avoids exploiting domain-dependent ontologies,(3) It is able to visualize the information space of Web services by providing a category map that depicts the semantic relationships among them,(4) It is able to semi-automatically generate Web service taxonomies that reflect both capability and geographic context, and(5) It allows service consumers to combine multiple search strategies in a flexible manner.We have developed a Web service discovery tool based on the proposed approach using an unsupervised artificial neural network and empirically evaluated the proposed approach and tool using real Web service descriptions drawn from operational Web services repositories. We believe that both service providers and consumers in a service-oriented computing environment can benefit from our Web service discovery approach","Facilitating Web Service Discovery and Publishing: A Theoretical Framework, A Prototype System, and Evaluation",,The University of Arizona.,,,core
1508061,2004-07-06T00:00:00,"Bargaining is becoming increasingly important due to developments within the field of electronic commerce, especially the development of autonomous software agents. Software agents are programs which, given instructions from a user, are capable of autonomously and intelligently realise a given task. By means of such agents, the bargaining process can be automated, allowing products and services together with related conditions, such as warranty and delivery time, to be flexible and tuned to the individual preferences of the people concerned. In this theses we concentrate on both fundamental aspects of bargaining as well as business-related applications of automated bargaining using software agents. The fundamental part investigates bargaining outcomes within a stylised world, and the factors that influence these outcomes. This can provide insights for the production of software agents, strategies, and setting up bargaining rules for practical situations. We study these aspects using computational simulations of bargaining agents. Hereby we consider adaptive systems, i.e., where agents learn to adjust their bargaining strategy given past experience. This learning behaviour is simulated using evolutionary algorithms. These algorithms originate from the field of artificial intelligence, and are inspired by the biological theory of evolution. Originally, evolutionary algorithms were designed for solving optimisation problems, but they are now increasingly being used within economics for modelling human learning behaviour. Besides computational simulations, we also consider mathematical solutions from game theory for relatively simple cases. Game theory is mainly concerned with the “rational man”, that is, with optimal outcomes within an stylised setting (or game) where people act rationally. We use the game-theoretic outcomes to validate the computational experiments. The advantage of computer simulations is that less strict assumptions are necessary, and that more complex interactions that are closer to real-world settings can be investigated. First of all, we study a bargaining setting where two players exchange offers and counter offers, the so-called alternating-offers game. This game is frequently used for modelling bargaining about for instance the price of a product or service. It is also important, however, to allow other product- and service-related aspects to be negotiated, such as quality, delivery time, and warranty. This enables compromises by conceding on less important issues and demanding a higher value for relatively important aspects. This way, bargaining is less competitive and the resulting outcome can be mutually beneficial. Therefore, we investigate using computational simulations an extended version of the alternating-offers game, where multiple aspects are negotiated concurrently. Moreover, we apply game theory to validate the results of the computational experiments. The simulation shows that learning agents are capable of quickly finding optimal compromises, also called Pareto-efficient outcomes. In addition, we study the effects of time pressure that arise if negotiations are broken off with a small probability, for example due to external eventualities. In absence of time pressure and a maximum number of negotiation rounds, outcomes are very unbalanced: the player that has the opportunity to make a final offer proposes a take-it-or-leave-it offer in the last round, which leaves the other player with a deal that is only slightly better than no deal at all. With relatively high time pressure, on the other hand, the first offer is most important and almost all agreements are reached in the first round. Another interesting result is that the simulation outcomes after a long period of learning in general coincide with the results from game theory, in spite of the fact that the learning agents are not “rational”. In reality, not only the final outcome is important, but also other factors play a role, such as the fairness of an offer. Using the simulation we study the influence of such fairness norms on the bargaining outcomes. The fairness norms result in much more balanced outcomes, even with no time pressure, and seem to be closer outcomes in the real world. Negotiations are rarely isolated, but can also be influenced by external factors such as additional bargaining opportunities. We therefore also consider bargaining within a market-like setting, where both buyers and sellers can bargain with several opponents before reaching an agreement. The negotiations are executed consecutively until an agreement is reached or no more opportunities are available. Each bargaining game is reduced to a single round, where player 1 makes an offer and player 2 can only respond by rejecting or accepting this offer. Using an evolutionary simulation we study several properties of this market game. It appears that the outcomes depend on the information that is available to the players. If players are informed about the bargaining opportunities of their opponents, the first player in turn has the advantage and always proposes a take-it-or-leave-it deal that leaves the other player with a relatively poor outcome. This outcome is consistent with a game-theoretic analysis which we also present in this thesis. If this information is not available, a theoretical analysis is very hard. The evolutionary simulation, however, shows that in this case the responder obtains a better deal. This occurs because the first player can no longer anticipate the response of the other player, and therefore bids lower to avoid a disagreement. In this thesis, we additionally consider other factors that influence the outcomes of the market game, such as negotiation over multiple issues simultaneously, search costs, and break off probabilities. Besides fundamental issues, this thesis presents a number of business-related applications of automated bargaining, as well as generic bargaining strategies for agents that can be employed in related areas. As a first application, we introduce a framework where negotiation is used for recommending shops to customers, for example on a web page of an electronic shopping mall. Through a market-driven auction a relevant selection of shops is determined in a distributed fashion. This is achieved by selling a limited number of banner spaces in an electronic auction. For each arriving customer on the web page, shops can automatically place bids for this “customer attention space” through their shop agents. These software agents bid based on a customer profile, containing personal data of the customer, such as age, interests, and/or keywords in a search query. The shop agents are adaptive and learn, given feedback from the customers, which profiles to target and how much to bid in the auction. The highest bidders are then selected and displayed to the customer. The feasibility of this distributed approach for matching shops to customers is demonstrated using an evolutionary simulation. Several customer models and auction mechanisms are studied, and we show that the market-based approach results in a proper selection of shops for the customers. Bargaining can be especially beneficial if not only the price, but other aspects are considered as well. This allows for example to customise products and services to the personal preferences of a user. We developed a system makes use of these properties for selling and personalising so-called information goods, such as news articles, software, and music. Using the alternating-offers protocol, a seller agent negotiates with several buyers simultaneously about a fixed price, a per-item price, and the quality of a bundle of information goods. The system is capable of taking into account important business-related conditions such as the fairness of the negotiation. The agents combine a search strategy and a concession strategy to generate offers in the negotiations. The concession strategy determines the amount the agent will concede each round, whereas the search strategy takes care of the personalisation of the offer. We introduce two search strategies in this thesis, and show through computer experiments that the use of these strategies by a buyer and seller agent, result in personalised outcomes, also when combined with various concession strategies. The search strategies presented here can be easily applied to other domains where personalisation is important. In addition, we also developed concession strategies for the seller agent that can be used in settings where a single seller agent bargains with several buyer agents simultaneously. Even if bargaining itself is bilateral (i.e., between two parties), a seller agent can actually benefit from the fact that several such negotiations occur concurrently. The developed strategies are focussed on domains where supply is flexible and can be adjusted to meet demand, like for information goods. We study fixed strategies, time-dependent strategies and introduce several auction-inspired strategies. Auctions are often used when one party negotiates with several opponents simultaneously. Although the latter strategies benefit from the advantages of auctions, the actual negotiation remains bilateral and consists of exchanging offers and counter offers. We developed an evolutionary simulation environment to evaluate the seller agent’s strategies. We especially consider the case where buyers are time-impatient and under pressure to reach agreements early. The simulations show that the auction-inspired strategies are able to obtain almost maximum profits from the negotiations, given sufficient time pressure of the buyers","Autonomous Agents in Bargaining Games: An Evolutionary Investigation of Fundamentals, Strategies, and Business Applications",,,,,core
155425695,2009,"In this paper, it new channel assignment strategy named compact dynamic channel assignment (CDCA) is proposed. The CDCA differs from other strategies by consistently keeping the system in the utmost optimal state, and thus the scheme allows to determine a call Succeeding or failing by local information instead of that of the whole network. It employs Hopfield neural networks for optimization which avoids the complicated assessment of channel compactness and guarantees optimum Solutions for every assignment. A scheme based on Hopfield neural network is considered before however, unlike others, in this algorithm an energy function is derived in such a way that for a neuron. file more a channel is currently being allocated in other cells, the more excitation the neuron will acquire, so as to guarantee each Cluster using channels as few as possible. Performance measures in terms of the blocking probability, convergence rate and convergence time are obtained to assess the viability of the proposed scheme. Results presented show that the approach significantly reduces stringent requirements of searching space and convergence time. The algorithm is simple and straightforward, hence the efficient algorithm makes the real-time implementation of channel assignment based oil neural network feasibility. Copyright (c) 2008 John Wiley &amp; Sons, Ltd.http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000262277600002&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=8e1609b174ce4e31116a60747a720701Engineering, Electrical &amp; ElectronicTelecommunicationsSCI(E)EI2ARTICLE125-352",A compact dynamic channel assignment scheme based on Hopfield networks   for cellular radio systems,,international journal of communication systems,10.1002/dac.954,"[{'title': None, 'identifiers': ['issn:1074-5351', '1074-5351']}]",core
378581175,2003-09-01T00:00:00,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites.  The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality.  Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc.  Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world.  The Glasgow Directory is an established 3D web space,

with around 10,000 visitors per year.  The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography.  This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar

environments. These criteria take into account lessons learned by ‘observing’ and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface.  These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow : Welcoming Multiple Visitors to the Virtual City,,,,,core
23796062,2005,"This paper introduces an integration of reinforcement learning and behavior-based control designed to produce real-time learning in situated agents. The model layers a distributed and asynchronous reinforcement learning algorithm over a learned topological map and standard behavioral substrate to create a reinforcement learning complex. The topological map creates a small and task-relevant state space that aims to make learning feasible, while the distributed and asynchronous nature of the model make it compatible with behavior-based design principles. We present the design, implementation and results of an experiment that requires a mobile robot to perform puck foraging in three artificial arenas using the new model, a random decision making model, and a layered standard reinforcement learning model. The results show that our model is able to learn rapidly on a real robot in a real environment, learning and adapting to change more quickly than both alternative models. We show that the robot is able to make the best choices it can given its drives and experiences using only local decisions and therefore displays planning behavior without the use of classical planning techniques. ","An architecture for BehaviorBased reinforcement learning,” Adaptive Behavior",,,,,core
24745888,2008-04-02,"Since the beginning of space exploration, the space community had the common belief that in the near future, the Automation &amp; Robotics will be an important element for such missions. Today, this has become a reality. The implementation of this technology will play an important role in future missions, as robotic systems and “Artificial Intelligence ” keeps developing further. The development of robotics arms as Canadarm and the ERA are citated as examples of contribution this kind of technology has made and how it will influence in the future. The main discussion in this paper is how this technology has been implemented in space exploration and how it will continuously keep contributing in such missions. A brief state of the art of how this technology has contributed to the space missions and a future approach for the coming years is proposed. ________________________________________________________________________________________________ 1",DEVELOPMENT OF AUTOMATION &amp; ROBOTICS IN SPACE EXPLORATION,,,,,core
20694246,2008-04-02,"ABSTRACT. Concept lattice is an effective tool and platform for data analysis and knowledge discovery such as classification or association rules mining. The lattice algorithm to build formal concepts and concept lattice plays an essential role in the application of concept lattice. In fact, more than ten algorithms for generating concept lattices were published. As real data sets for data mining are very large, concept lattice structure suffers from its complexity issues on such data. The efficiency and performance of concept lattices algorithms are very different from one to another.In order to increase the efficiency of concept lattice-based algorithms in data mining, it is necessary to make use of an efficient algorithm to build concept lattices.So we need to compare the existing lattice algorithms and develop more efficient algorithm. We implemented the four first algorithms in Java environment and compared these algorithms on about 30 datasets of the UCI repository that are well established to be used to compare ML algorithms. Preliminary results give preference to Ganter’s algorithm, and then to Bordat’s algorithm, nevertheless these algorithms still suffers when dealing with huge datasets. We analyzed the duality of the lattice-based algorithms. Furthermore, we propose a new efficient scalable lattice-based algorithm: ScalingNextClosure to decompose the search space of any huge data in some partitions, and then generate independently concepts in each partition. The experimental results show the efficiency of this algorithm. RÉSUMÉ",A Lattice Algorithm for Data Mining,,,,,core
54315909,2007-01-01T00:00:00,"La geometria descrittiva è la scienza che insegna a rappresentare, modellare e ricostruire nello spazio le forme a tre dimensioni che sono oggetto della invenzione in architettura, nella ingegneria e nel disegno industriale. Benché abbia ricevuto il suo nome da Gaspard Monge, intorno al 1795, questa scienza è tra le più antiche, tra quante fanno parte del patrimonio culturale dell'umanità, e comprende al suo interno importanti teorie e applicazioni quali la prospettiva, la teoria delle ombre e del chiaroscuro, il disegno dell'ordine architettonico, il taglio delle pietre e dei legnami, il disegno degli ingranaggi e molte altre ancora che qui non occorre ricordare. La geometria descrittiva è perciò, da sempre, uno strumento formativo essenziale nei curricula degli studenti architetti, ingegneri e designer. Nell'ultimo quarto dello scorso secolo, con il rapido sviluppo delle tecnologie informatiche (hardware e software), i problemi che avevano prima una soluzione esclusivamente grafica hanno trovato una soluzione digitale, vale a dire una soluzione di natura essenzialmente matematica che però si manifesta nei modi della geometria descrittiva classica e cioè attraverso immagini. Lo sviluppo di questi algoritmi ha anche arricchito il novero delle teorie di carattere geometrico descrittivo estendendo, ad esempio, il repertorio delle curve e delle superfici impiegate nella progettazione dalle coniche e dalle quadriche alle NURBS, il repertorio degli effetti della luce sui corpi che è possibile rappresentare con cura, dal semplice chiaroscuro della legge di Lambert, ai riflessi, ai punti brillanti, alle trasparenze degli attuali rendering, etc. A fronte di questa evoluzione, tuttavia, gli studi e, conseguentemente, l'insegnamento della geometria descrittiva, restano radicati alle forme antiche e questo radicamento provoca una pericolosa dicotomia tra l'insegnamento tradizionale, non più attuale ma ricco della sua storia, e l'insegnamento delle tecniche informatiche, attuale, ma ridotto a mera esecuzione di comandi programmati, avulsi da qualsiasi contesto teorico e perciò anche incontrollati. Urge dunque un rinnovamento della geometria descrittiva che, considerando il forte impatto dell'informatica in questo settore, può essere visto come una vera e propria rifondazione.The descriptive geometry is the science that teaches how to represent, model, and reconstruct the three-dimensional space forms which are the subject of the invention in architecture, in engineering and industrial design. Although it has received its name from Gaspard Monge, around 1795, this is one of the oldest science, including how many are part of the cultural heritage of mankind, and includes in its interior significant theories and applications such as Outlook, the theory of shadows and chiaroscuro, of the architectural design, the cutting of stones and wood, the design of the gears and many others here who do not need to remember. The descriptive geometry is therefore always a training tool essential in the curricula of students architects, engineers and designer.Nell 'last quarter of the last century, with the rapid development of information technology (hardware and software), the problems they had before a solution exclusively graphics have found a digital solution, namely a solution of essentially mathematics but manifests itself in the manner of descriptive geometry and classical ie through images. The development of these algorithms has also graced the ranks of the theories of geometric character descriptive extending, for example, the repertoire of curves and surfaces used in the design of the taper and the quadric NURBS, the repertoire of the effects of light on bodies that can be act with care, from simple chiaroscuro of Lambert law, the reflections, the shining points, the transparency of the current rendering, etc.A face of this development, however, studies and, therefore, the teaching of descriptive geometry, remain rooted the old forms and rooting this causes a dangerous dichotomy between the traditional teaching, no longer current but rich in its history and the teaching of information technology, the current, but reduced to mere execution of programmed commands, divorced from any context and theoretical therefore also incontrollati.Urge then a renewal of descriptive geometry, that given the strong impact of information technology in this area, can be seen as a real overhaul",Per una geometria descrittiva attuale,,Gangemi Editore,,,core
15731398,2005-01-01T00:00:00,"Tiedon louhinnalla pyritään paljastamaan tietokannasta tietomassaan sisältyviä säännönmukaisuuksia, joiden olemassaolosta ei vielä olla tietoisia. Kun tietokantaan sisältyvät tiedot ovat kovin moniulotteisia, yksittäisten tapausten sisältäessä lukuisia piirteitä, monen koneoppimisen menetelmän suorituskyky heikkenee ratkaisevasti. Tätä ilmiötä nimitetään ”moniulotteisuuden kiroukseksi”, koska se johtaa usein sekä koneellisen käsittelyn monimutkaisuuden että käsittelyn yhteydessä syntyvien luokitusvirheiden kasvuun. Toisaalta tietokantaan mahdollisesti sisältyvät epärelevantit tai vain epäsuorasti relevantit piirteet tarjoavat heikon esitysavaruuden tietokannan käsiterakenteen kuvaamiseen. Piirteiden muodostamisella pyritäänkin joko ulotteisuuden pienentämiseen tai esitysavaruuden parantamiseen, tai molempiin, ohjatun koneoppimisen tarpeita varten.Työ koostuu erillisistä artikkeleista ja niihin tukeutuvasta yhteenvedosta. Kukin artikkeli käsittelee yhtä tai kahta tutkimuskysymystä ja niihin liittyviä havaintoja, jotka Pechenizkiy lopuksi yhdistää ehdotukseksi sellaiseksi järjestelyksi, jonka avulla tiedonlouhintatekniikoiden ja niiden kombinaatioiden käyttökokemuksia kokoamalla voidaan systemaattisesti tukea sopivimman tiedonlouhintastrategian valintaa.Knowledge discovery or data mining is the process of finding previously unknown and potentially interesting patterns and relations in large databases. The so-called “curse of dimensionality” pertinent to many learning algorithms, denotes the drastic increase in computational complexity and classification error with data having a great number of dimensions. Beside this problem, some individual features, being irrelevant or indirectly relevant for the learning concepts, form poor problem representation space. The purpose of this study is to develop theoretical background and practical aspects of feature extraction (FE) as means of (1) dimensionality reduction, and (2) representation space improvement, for supervised learning (SL) in knowledge discovery systems. The focus is on applying conventional Principal Component Analysis (PCA) and two class-conditional approaches for two targets: (1) for a base level classifier construction, and (2) for dynamic integration of the base level classifiers. Theoretical bases are derived from classical studies in data mining, machine learning and pattern recognition. The software prototype for the experimental study is built within WEKA open-source machine-learning library in Java. The different aspects of the experimental study on a number of benchmark and real-world data sets include analyses of (1) importance of class information use in the FE process; (2) (dis-)advantages of using either extracted features or both original and extracted features for SL; (3) applying FE globally to the whole data and locally within natural clusters;  (4) the effect of sampling reduction on FE for SL; and (5) the problems of FE techniques selection for SL for a problem at consideration. The hypothesis and detailed results of the many-sided experimental research process are reported in the corresponding papers included in the thesis. The main contributions of the thesis can be divided into contribution (1) to current theoretical knowledge and (2) to development of practical suggestion on applying FE for SL",Feature extraction for supervised learning in knowledge discovery systems,,University of Jyväskylä,,,core
54676091,2005-01-01T00:00:00,"Multi-Processor Systems-on-Chips (MPSoCs) are be-

coming increasingly complex, and mapping and

scheduling of multi-task applications on computa-

tional units is key to meeting performance constraints

and power budgets. Abstract models of system com-

ponents and deployment of advanced algorithmic tech-

niques for the optimization problem can provide for

fast design space exploration and for optimal solu-

tions. We exploit Constraint Programming (CP) from

Artificial Intelligence and Integer Programming from

Operations Research (OR) as a means to capture dif-

ferent aspects of the same problem (optimality and

feasibility), and prove the effectiveness of this hybrid

approach. Moreover, we exploit an accurate MPSoC

virtual platform for capturing mismatches between

problem formulation and real-life systems, and for as-

sessing their impact on expected performance. We

introduce the notion of execution constraints in the

model of the problem, thus making the solution ex-

pressible and implementable in the real world. The

model without execution constraints is a relaxation

of the real problem and therefore provides a super-

optimal solution. We compare the effectiveness of this

latter solution with the one provided by the simulator,

and try to refine our models as well as our optimiza-

tion techniques accordingly",Measuring Efficiency and Executability of allocation and scheduling in Multi-Processor Systems-on-Chip,,,,,core
56365897,2005-05-31T00:00:00,"Realistic and intelligent agent movement remains one of the greatest challenges for games developers. Path-finding strategies are usually employed as a means of allowing an agent to navigate from one part of the game world to another. Typically the game world is stored in a pre-processed structure called a map which contains all of the relevant geometry. In order to cut down the search space for the path-finder, this map is broken down and simplified. The path-finder then uses this simplified representation to determine the best path from the starting point to the desired destination. These simplified representations correspond to graphs, and algorithms such as Dijkstra and A* [6] can then be employed to quickly find paths between the nodes in the graph. The graph used is based on a pre-processed static representation of the game world. However, the assumption that the geometry of the game remains static during the course of play is not necessarily valid anymore. This difficulty is then compounded by the fact that the agent typically has no real-time awareness of the environment around it. This situation results in a number of problems for path-finders, each of which we now outline. The increasing use of physics engines opens up the possibility of completely dynamic game geometry, where the players and agents can physically alter the structure of the game world as play progresses, by knocking over walls for example [2]. Dynamic obstacles can therefore be introduced that block previously accessible nodes on the graph. When this happens the agent will still believe it can walk along this path due to its reliance on the preprocessed static graph. Techniques have been developed that improve the agents’ reactive abilities when dynamic objects obstruct a path. These work well in some situations, but generally the agent will not react until it has collided with an obstacle, as it has no sense of awareness until a trigger is set when a collision occurs. Another problem is the rigid and unrealistic movement that occurs when the agent walks in a straight line between nodes. This is caused by the dilemma which arises in the trade off between speed (the less number of nodes to search the better) and realistic movement (the more nodes, the more realistic the movement). This has been improved in some games by applying spline curves for smoothing out paths along nodes. A further problem is implementing tactical path-finding. This involves not just finding the shortest route but also the route that offers the most cover, or avoids unnecessary encounters with undesirable game entities. One approach is to modify the cost heuristic of A* to take line of fire from other enemy agents into account [7]. This has the benefits of adding realism to the game and also presents a less predictable opponent for the human player. The drawback is that due to the added cost, the search space becomes much larger for A* to process. This approach also assumes that the threat remains static during the paths duration, which is seldom the case. Generally game developers add in special case code to deal with these problems but typically this is only applicable to that particular game [1]. This paper examines these problems and introduces the concept of applying learning techniques to solve them in a new novel way [4]. Our solution to this problem is to provide the agent with a means of navigating its own way around the world, rather than simply relying on routes provided by the game engine. In order to accomplish this the agent requires two important abilities. Firstly it needs to be able to examine its environment in some way in order to know what is in front of it and around it, thus giving it real-time awareness. Secondly it needs some way of processing this information to accomplish tasks such as steering around obstacles that have been placed in its path. The first ability is achieved by embedding sensors in the agent. This is a concept borrowed from robotics where ultrasound or infrared sensors are common. We adapt this idea for our agents by casting rays which test for intersections with the game geometry. In this way information can be provided to the agent pertaining to the proximity of objects within its field of vision. The second ability is being able to process this information in some way. Our solution to this problem is to furnish each agent with an Artificial Neural Network (ANN) [3] which takes the sensor information as input. The ANN is a learning algorithm that we have trained to exhibit the behaviour we want – namely that the agent has the ability to steer around objects. We describe how this provides robust steering behaviour that is tolerant of noisy data. Another advantage of this approach is that the processing required is minimal and hence multiple agents can be imbued with this behaviour without causing a major strain on the CPU. This is used in conjunction with a traditional path-finding algorithm. The algorithm works out a path for the agent but the sensors and the ANN are responsible for moving the agent along that path, and are capable of adapting the path to steer around obstacles or other dynamically introduced geometric changes. Our system is implemented using the Quake 2 [5] game engine and we have extensively tested these ideas against more traditional approaches to path-finding. The game engine gives us a test bed whereby a genetic algorithm [6] is used in real-time to evolve the weights of the neural network. Since the sensors are influencing the agents movement in real-time, as it walks from node to node, it tends to gradually veer away from obstacles thus resulting in less rigid movement. Giving the agent this real-time awareness also compliments the tactical elements of pathfinding as the agent can be alerted in real-time to imminent threats. Our results indicate that this approach is extremely useful in the dynamic environments that are becoming the norm in modern computer games. References [1] Cain, Timothy, ""Practical Optimizations for A*"", AI Game Programming Wisdom, Charles River Media, 2002 [2] Eberly,David,H, ""Game Physics"", Elsevier, Inc, 2004 [3] Fausett, Laurene, ""Fundamentals of Neural Networks Architectures, Algorithms, and Applications"", Prentice-Hall, Inc, 1994. [4] Graham, Ross,""Neural Networks for Real-time Pathfinding in Computer Games"", In proceedings of ITB journal, Issue 9 (2004) [5] www.idsoftware.com/games/quake/quake2/ [6] Russel, Stuart., Norvig, Peter., ""Artificial Intelligence A Modern Approach"", Prentice-Hall, Inc, 1995 [8] Van der Sterren, William,. ""Tactical Path-Finding with A*"", Game Programming Gems 3, Charles River Media, 200",Realistic Agent Movement in Dynamic Game Environments,https://core.ac.uk/download/56365897.doc,,,,core
10645726,2008-12-01T00:00:00,"textMachine learning techniques are now essential for a diverse set of applications in computer vision, natural language processing, software analysis, and many other domains. As more applications emerge and the amount of data continues to grow, there is a need for increasingly powerful and scalable techniques. Kernel methods, which generalize linear learning methods to non-linear ones, have become a cornerstone for much of the recent work in machine learning and have been used successfully for many core machine learning tasks such as clustering, classification, and regression. Despite the recent popularity in kernel methods, a number of issues must be tackled in order for them to succeed on large-scale data. First, kernel methods typically require memory that grows quadratically in the number of data objects, making it difficult to scale to large data sets. Second, kernel methods depend on an appropriate kernel function--an implicit mapping to a high-dimensional space--which is not clear how to choose as it is dependent on the data. Third, in the context of data clustering, kernel methods have not been demonstrated to be practical for real-world clustering problems. This thesis explores these questions, offers some novel solutions to them, and applies the results to a number of challenging applications in computer vision and other domains. We explore two broad fundamental problems in kernel methods. First, we introduce a scalable framework for learning kernel functions based on incorporating prior knowledge from the data. This frame-work scales to very large data sets of millions of objects, can be used for a variety of complex data, and outperforms several existing techniques. In the transductive setting, the method can be used to learn low-rank kernels, whose memory requirements are linear in the number of data points. We also explore extensions of this framework and applications to image search problems, such as object recognition, human body pose estimation, and 3-d reconstructions. As a second problem, we explore the use of kernel methods for clustering. We show a mathematical equivalence between several graph cut objective functions and the weighted kernel k-means objective. This equivalence leads to the first eigenvector-free algorithm for weighted graph cuts, which is thousands of times faster than existing state-of-the-art techniques while using significantly less memory. We benchmark this algorithm against existing methods, apply it to image segmentation, and explore extensions to semi-supervised clustering.Computer Science",Scalable kernel methods for machine learning,,,,,core
10516158,[2005],"Model-based reasoning is a powerful method for performing system monitoring and diagnosis. Building models for model-based reasoning is often a difficult and time consuming process. The Inductive Monitoring System (IMS) software was developed to provide a technique to automatically produce health monitoring knowledge bases for systems that are either difficult to model (simulate) with a computer or which require computer models that are too complex to use for real time monitoring. IMS processes nominal data sets collected either directly from the system or from simulations to build a knowledge base that can be used to detect anomalous behavior in the system. Machine learning and data mining techniques are used to characterize typical system behavior by extracting general classes of nominal data from archived data sets. In particular, a clustering algorithm forms groups of nominal values for sets of related parameters. This establishes constraints on those parameter values that should hold during nominal operation. During monitoring, IMS provides a statistically weighted measure of the deviation of current system behavior from the established normal baseline. If the deviation increases beyond the expected level, an anomaly is suspected, prompting further investigation by an operator or automated system. IMS has shown potential to be an effective, low cost technique to produce system monitoring capability for a variety of applications. We describe the training and system health monitoring techniques of IMS. We also present the application of IMS to a data set from the Space Shuttle Columbia STS-107 flight. IMS was able to detect an anomaly in the launch telemetry shortly after a foam impact damaged Columbia's thermal protection system",INDUCTIVE SYSTEM HEALTH MONITORING WITH STATISTICAL METRICS,https://core.ac.uk/download/pdf/10516158.pdf,,,,core
23422597,2009,"Abstract – Plane perching on a wire has been demonstrated with success in laboratory conditions by [Cory &amp; Tedrake, 2008]. The present paper focuses on designing a controller for a small wall landing glider. We use a simulator and a reinforcement learning algorithm in a coarsely discretized state-space to design an optimal control for the landing. Analysis of this policy allows us to deduce a simpler closed loop controller that is robust to variations in initial speed and that only relies on pitch and pitch rate measurement. A controller of that class is now being implemented on the real airplane",Problem Description,,,,,core
217047276,2009-08-01T07:00:00,"Phenomena occur both in space and time. Correspondingly, ability to model spatiotemporal behavior translates into ability to model phenomena as they occur in reality. Given the complexity inherent when integrating spatial and temporal dimensions, however, the establishment of computational methods for spatiotemporal analysis has proven relatively elusive. Nonetheless, one method, the spatiotemporal helix, has emerged from the field of video processing. Designed to efficiently summarize and query the deformation and movement of spatiotemporal events, the spatiotemporal helix has been demonstrated as capable of describing and differentiating the evolution of hurricanes from sequences of images. Being derived from image data, the representations of events for which the spatiotemporal helix was originally created appear in areal form (e.g., a hurricane covering several square miles is represented by groups of pixels). ii Many sources of spatiotemporal data, however, are not in areal form and instead appear as points. Examples of spatiotemporal point data include those from an epidemiologist recording the time and location of cases of disease and environmental observations collected by a geosensor at the point of its location. As points, these data cannot be directly incorporated into the spatiotemporal helix for analysis. However, with the analytic potential for clouds of point data limited, phenomena represented by point data are often described in terms of events. Defined as change units localized in space and time, the concept of events allows for analysis at multiple levels. For instance lower-level events refer to occurrences of interest described by single data streams at point locations (e.g., an individual case of a certain disease or a significant change in chemical concentration in the environment) while higher-level events describe occurrences of interest derived from aggregations of lower-level events and are frequently described in areal form (e.g., a disease cluster or a pollution cloud). Considering that these higher-level events appear in areal form, they could potentially be incorporated into the spatiotemporal helix. With deformation being an important element of spatiotemporal analysis, however, at the crux of a process for spatiotemporal analysis based on point data would be accurate translation of lower-level event points into representations of higher-level areal events. A limitation of current techniques for the derivation of higher-level events is that they imply bias a priori regarding the shape of higher-level events (e.g., elliptical, convex, linear) which could limit the description of the deformation of higher-level events over time. The objective of this research is to propose two newly developed kernel methods, support vector clustering (SVC) and support vector machines (SVMs), as means for iii translating lower-level event points into higher-level event areas that follow the distribution of lower-level points. SVC is suggested for the derivation of higher-level events arising in point process data while SVMs are explored for their potential with scalar field data (i.e., spatially continuous real-valued data). Developed in the field of machine learning to solve complex non-linear problems, both of these methods are capable of producing highly non-linear representations of higher-level events that may be more suitable than existing methods for spatiotemporal analysis of deformation. To introduce these methods, this thesis is organized so that a context for these methods is first established through a description of existing techniques. This discussion leads to a technical explanation of the mechanics of SVC and SVMs and to the implementation of each of the kernel methods on simulated datasets. Results from these simulations inform discussion regarding the application potential of SVC and SVMs",Support Vector Methods for Higher-Level Event Extraction in Point Data,https://core.ac.uk/download/217047276.pdf,DigitalCommons@UMaine,,,core
235572555,2003-08-20T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.Vol 1-22 PEOPLE CONNECTING WITH PEOPLE August 20, 2003
Why Buy Local?
by Ron Williams
There are economic forces at
work right now that threaten local
businesses in Magrath. When you buy
your groceries, your gas, or your home
improvement fixtures, how do you
decide where to buy? Most of us focus
on two questions: Which stores have
the lowest prices? And which stores are
most conveniently located? Many of us
then decide to save the bulk of our
purchases for when we visit one of the
super-discount, big-box chain stores
like Wal-Mart, Costco, or Home Depot.
These are well managed stores with
plenty of virtues but they tend to siphon
money out of towns like ours and put
traditional local retailers out of busi­ness.
Local retailers often can’t com­pete
with the big chain prices but they
can offer better service, convenient
locations, unique atmospheres, friend­liness,
and community investment.
There are at least three com­Top:
Some of the staffat the Magrath
Trading Co., Howard West (Manager), Joan
Bly, Heidi Grtininger, Kristi Dudley.
Left: Arlen Bennett, Pharmacist and owner
of the Magrath Pharmacy with one of his
staff, Roger Davies.
family members, who in turn use
their paycheques to buy at other
local stores. All these transactions
reinforce one another and pump up
the local economic multiplier, the
basic building block for community
prosperity.
SECOND, local ownership mini-pelling
economic rea­sons
to buy from
locally-owned
Magrath businesses:
FIRST, a locally
owned business is
likely to produce
income, jobs, and charitable donations
for a community over several genera­tions.
They employ our neighbors and
^Oc^^ire[ess
High-Speed Internet
is now available in Magrath and surrounding area!
320-2600 or visit our website at www.shockware.com.
Once we have 20 users the rate for all will go down to $39.95
Top: Albert andJasmine Lee
owners of ""The Store"",
Right: Jessica Schneyder, Ted
Holland owner of ""Hollands
Insurance”, Kathy Heninger.
mizes the chances of
calamity. Too often we see
local companies sell their
interests to outsiders only
to have the hometown
plant close its doors with­in
a year. Tragic conse­quences
always follow.
Taxpayers thrown out of work become tax drainers through
welfare and unemployment payments. When the tax base
shrinks, vital services like education, police, fire, and roads
must be cut. Property values plummet and, like so many
steel, auto and mining towns in the 1970s, 1980s, and
1990s, the community descends into an economic death
spiral. Local stores have no plans to pull up stakes and
move to Taiwan.
THIRD, local companies are interested in the quality
of life in the community. Today, many communities are held
hostage by their largest companies. Many communities have
large employers that are perennial polluters. They routinely
dump waste in local lakes and streams. They take the atti­tude:
regulate us and we’ll move to more lax cities that want
to have us.
Locally owned companies never practice this kind of
extortion. They take a long term approach to the communi­ty.
It’s their community and they gradually improve it with
investments and service over time. Look at professional
sports franchises. How often do we hear owners threatening
to leave town if their demands for new stadiums, tax conces­sions,
and other booty are not met. One exception is the
Green Bay Packers, a team that is owned primarily by the
citizens of Wisconsin. Local ownership effectively prevents
the Packers from ever threatening to leave town. There will
never be the San Diego Packers.
So the next time you need to purchase fight
bulbs, go to the local hardware store instead of a
national big box store. Next time you want to go
out for dinner, support the many locally owned
restaurants versus the national chains. Next time
you pass a lemonade stand, stop, buy a glass, and
support tomorrow’s entrepreneurs.
Now is the time to support your neighbors
and local businesses. Imagine how inconvenient
it would be if the Trading Company closed its
doors. What if you couldn’t buy gas in Magrath?
What if the closest drug store were in Lethbridge?
Let’s take pride in the quality and character
of our community. Put your money where your
house is. Buy Local! >
Devonshire Realty Inc.
Jim Anderson agent
RESIDENTIAL - FARM ACREAGE -
COMMERCIAL IN MAGRATH AND AREA
2 Houses for Sale in Del Bonita
House for Sale
Harker Ave. & 2nd West St.
<num
Published weekly on Wednesdays by Keyline Communications
Box 179, Magrath, AB TOK 1J0 Ph: 758-6911 • Fx: 758-3.661
email magrathnews@telus.net
Ad deadline is Friday at 5pm and may be dropped off at the
Magrath Pharmacy or at Keyline Communications’
office at 14 Centennial Place, Duane & Carma Thomson’s home.
For permission to reprint any
material found in this publication please
contact Keyline Communications.
2 story home - suite in upstairs with separate
gas and electrical for billing - $82,000
2 Br„ 1600 sq.f^iyl^Oith attached 1
REDUCED!! 3 Br. walkt^k^J^jQattached garage. $153,000
1st Ave North & 1st St. East
2 bedroom, single bungalow - $34,900
car garage $129,000
Comparative Market Analysis
(No Charge) - For people interested in getting an
evaluation of marketability of your property
Phone 758-6725 (leave message)
331-8882 (cellular)
The Family of
Kenneth S. Harker
&
Eva Marie Harker
Request the Pleasure
of Your Company at
Their 50th Wedding Anniversary Celebration
Saturday
August 23, 2003
7:00 - 8:30 pm
Magrath Seniors Center
Obituaries?
GERALD LEISHMAN passed away in Lethbridge at the
Lethbridge Regional Hospital on Friday, August 15, 2003 at
83 years of age.
Gerry was born December 27,1919 in Magrath,
Alberta, where he lived most of his life. He married
Margaret Dahl and had four children.
Funeral Services were held on Monday, August 18,
2003 at the CHURCH OF JESUS CHRIST OF LATTER-DAY
SAINTS, Garden Place Chapel, Magrath, AB, with Bishop
Shane Gurney officiating. Interment in the Magrath Cemetery.
Kyle Harker, son of Vince and Heather, will be
speaking tins Sunday, August 24, at 11:00 am in 4th Ward at
the Garden Place Chapel. He will be leaving for the
California Roseville Mission September 3rd.
Jared Olsen of Welling, son of Wes and Dian,
returned from the California Fresno Mission on August 20.
He will be speaking August 31 at 10:00 am in the
Welling Ward.
SAVINGSSAVINGS
White Glue - 225 ml, reg $2.69 $1.69
Index Dividers - reg $1.29 79*
Math or Geometry Sets - reg $2.99 $2.19
Pencils - 10 pk, reg $1.29 59*
Trilok Covers - 4 pk, reg $1.99 $1.29
Papermate ball pens -10 pk, reg $2.29 $ 1.80
Glue Sticks - reg $2.39 $1.49
Tape - Home brand, reg $ 1.99 $1.19
Crayons - 96 pk, reg $6.99 $5.77
nil Magrath Trading Company
Phone: 758-3065
“BLACK” ONCE
International Tae Kwon Do Federation offers
congratulations to Mr. Tim Moore on his successful
completion of 2nd Degree Black Belt testing requirements.
He is now a formally recognized
ITF Second Degree (Dan) Black Belt.
The objectives of Tae Kwon Do are to:
• cultivate character
• trim and slim the body
• display graceful techniques
• bring out one’s strengths
• cultivate the mind
Tae Kwon Do has spread World Wide since its inception 50 years
ago, even in the town of Magrath. A small but motivated group
of individuals now trains on a regular basis and has proven
very successful over the past 4 years.
Following summer break, classes will begin in
September, Tuesday and Thursday evenings.
If you have any questions please call 758-6835.
Help Defend Traditional Marriage in Canada!
Sign The National Marriage Petition - Write a Letter
We are entering a critical time for our nation. The
government is taking steps to legalize same sex marriage,
something that would undermine traditional marriage and
the natural family. Preserving these vital institutions is
critical to Canada’s future.
Canadian Citizens to Defend Marriage (CCDM), is a
nationwide, grass roots, citizen-based effort, headquartered
in Toronto and organized to defend traditional marriage in
Canada at this important time. CCDM invites you to join with
Canadians across Canada in showing our support for tradi­tional
marriage by signing the online National Marriage
Petition. Please go to www.marriagepetition.ca to sign the
petition and learn more.
Sponsored by Canadian Citizens To Defend Marriage
www.marriagenctition.ca
Canadians of ALL ages can sign
We need your help to spread the word.
For further information or to volunteer to help, please
call Lorraine Balderson at 758-6380.
Take a look at us now!
Now offering
Soft Serve Ice Cream
Assorted
Candy Bars 3/$l-40
Excel Gum 2/$1.40
Milk $385
With min, purchase of 15 litre of gas.
Blue
Goose
277N 1st Street West • 758-3322
Writing letters to your local government representa­tives
is more beneficial than you might realize. A possible
place to start is by writing to Premier Ralph Klein expressing
your views on the legal definition of marriage.
Address letters to:
The Honourable Ralph Klein MLA
Premier of Alberta
307 Legislature Building
10800 97th Ave.
Edmonton AB T5K 2B7
Using your own words you may wish to express the
following items.
/ support for the Premier’s stand on Marriage between
male and female
/ concurrence that the basic family unit consists of a
father and mother; that children born to that union is
a core value of our society.
/ opposition to same sex marriages, feeling that this
would erode the God-given and time tested family
unit as defined above.
/ support for the use of the not-withstanding clause
should it become necessary to defeat same-sex
marriages.
/ express appreciation for his courage and commitment
in defending our family unit in Alberta. ♦
If you ever find happiness by hunting for it, you will find it,
as the old woman did her lost spectacles, safe on her own
nose all the time. - Josh Billings
(Happiness) always looks small while you hold it in your
hands, but let it go, and you learn at once how big and
precious it is. - Maksim Gorky
Rockport Flour Mill Inc.
Phone: 758-3077
Fax: 758-3340
8miles south and 2
miles east of Magrath
--------- ---------
Coyote Pancake & Waffle Mix
Made from pure natural whole wheat flour.
Four flavors: regular, buttermilk, blueberry
and chocolate chip. Use for family reunions,
pancake breakfasts, etc.
Also available: Coyote Pure Natural Whole
Wheat Wheatlets - Germade Breakfast Cereal.
Available at all stores or buy direct
CM
O
IZ1
CU
DO
Jñ
CD
CU
DO
o
CU
E
o
CD
1-0 o
CU\
CD
“O
CP
-a
=3
O
O
CD
IX) u—
cu
CD
ÖO
S
CU
OO
""O
CD
en
e o
o
cu
CQÄK
CU 5
cu
tí
£
•Ö
<u
Q-E
o
o
""O
cu
Eoo
Q¿
DO-C
C
c
cu
E
c<uo <O
CQ
<U
O
en
ZJ
o
cd on
o
o
Raw Water Irrigation
Pumping Station
The Town’s irrigation pumping station is now operational.
Magrath has a rich heritage of irrigation and so it is
fitting that we have a state of the art system to water our
lawns and gardens.
In order to make the most efficient use of our grid of underground pipes,
we are changing our water use policy:
EFFECTIVE IMMEDIATELY:
Residents with EVEN HOUSE NUMBERS will water on EVEN DAYS.
Residents with ODD NUMBERED HOUSES will water on ODD DAYS.
This change in watering policy will allow all residents to appreciate even
. better water pressure and flow to their properties.
The Town requests that all residents discontinue the use of potable water
(that is, water that comes from a tap on your house) for watering lawns
and gardens. The potable water is treated for drinking and household use. This treatment is
expensive. We can reduce our overall Town taxes by not using potable/treated water outside.
Remember, the raw water irrigation should not be drunk. It is not treated. However, the raw
water is healthier for lawns than the potable water.
Connections to the Raw Water Irrigation System:
If you do not have a valve and spigot on your property that connects you to the Raw Water Irrigation
System, please call the Town Office at 758-3212. We will look at how close our pipes are to your property
and, if feasible, see that you are connected in a timely manner.
Watch for Leaks
We are pressurizing the irrigation system at about 40 pounds per square inch (p.s.i.). We may
increase this in time. But for now, we expect to spend some time monitoring the system and fixing leaks in
lines that are unaccustomed to this higher pressure. We ask that all residents be aware of the
irrigation lines on or around their property and inform the Town of all leaks,
accumulations of water, or Old Faithful-like geysers.
Town of Magrath
Telephone: 758-3212 Fax: 758-6333
E-mail: magrathl@telusplanet.net
Demolition Day
One of the last remaining historical icons of
Magrath met its fate in June of this year. The 103 year
old Ririe Barn on the east end of town was bulldozed to
the ground, burned, and buried. There are now few
buildings left in Magrath that are over 100 years old.
James and Elizabeth Ann Ririe were among the
first settlers of Magrath arriving in 1899- They began
building their beautiful home first, followed shortly by
the big livestock barn capable of housing dozens of cows
and horses. A bam which withstood the rigors of 100
years of Alberta weather took just one day to be com­pletely
demofished. The only evidence left of the barn is
a freshly leveled area of dirt. ♦ Value Priced Diapers
- For Boys or Girls .
Super absorbent contoured
shape is ultra thin and
comfortable.
24 Small-Medium 10-22 lbs
20 Large 22-271bs
$6.86 ea.
MY ROOTS
T-SHIRTS.
Logo reads:
“My ROOTS are in the Garden City
Magrath, Alberta, Canada”
Assorted colors and sizes YM to 2XL.
- Front Only
- Front and Back
- Golf Shirts
$12.95
$14.95
$19.95
Magrath
2000 ltd.
80 South 1 st Street West
Mon-Fri 9:00am - 6:00pm
Ph: 758-3001 • Fax 758-3505
After Hours: 758-6222 • 382-0749
New On-ramp to the Information Superhighway
by Rick Humphreys
High-speed internet has finally arrived in Magrath. Not
through your phone line or cablevision, but through radio
waves via a wireless network.
In late July, Shockware Wireless Inc. installed a wire­less
base unit on the top of the P&H elevator, making high­speed
internet a reality for Magrath businesses and resi­dents.
Shockware, owned by Dean Shock, has already
installed wireless internet for a number of small communi­ties
including Vauxhall, Barnwell, Seven Persons, Taber and
Stirling. They also service Lethbridge and Medicine Hat,
competing directly with Telus and Shaw Cable.
The basic range of the base station is about 15 miles.
Wireless networks are restricted to “line of sight” connec­tions.
In other words, the main base station and the dish
antenna at the residence must be in sight of each other. For
homes within a mile or so of the base station, the signal may
pass through trees, but that is not guaranteed. In order to
determine whether there is sufficient signal strength to a
prospective customer, a Shockware technician from
Lethbridge makes a personal visit and tests the signal before
setting up the connection.
Todd Johnson, the first customer signed up in Magrath,
is very happy with this new service. He says his download
speeds are at least 15-20 times faster than dial-up.
Something that took 10-15 minutes to download with a
modem is now taking him around 20 seconds.
The Town of Magrath has signed up for the service.
Ron Williams, the Town Administrator, said that there were
some initial problems with connectivity, but Shockware
remedied the problems for them. He said web pages come
up “almost instantaneously.”
The cost of this wireless internet service includes:
* $50 - Survey Travel Fee: to determine if a proper signal
can be established. Mr. Shock indicated that this fee
can be shared by customers by arranging for the
Lethbridge technician to come out and test multiple
homes in the same visit.
* $75 installation fee to mount the antenna on the roof,
install the wireless radio card in the computer, set up
the software and train the customer.
* $49-95 monthly fee rental of the equipment and con­nection
to the internet. This rate will drop to $39-95
for all users once 20 users are signed up, and to
$34.95 once 30 users are signed up.
This is the only high-speed internet option available for
Magrath at this time. Both Telus and Monarch Cable have
indicated that there is no schedule yet for installing their
high-speed internet services. ♦
r■II
III
I■I
>
EM
Start in September
Cost is $20 per Month.
Classes will be one day a week
Ages 5-11 and 12-17
An adult class considered if enough interest.
Call Leanne Sabey at 758-6846 to
register or for more information.
Limited Space Available
1 Tl
I
IIIIIII■
■
PAVING &
MAINTENANCE LTD.
""YOUR LOCAL SPECIALISTS""
Asphalt Paving & Repairs
Pot Holes & Pathways
Driveways
Parking Lots
• Seal Coating
• Sand Slurry Seal
• Hot & Cold Crack Sealing
• Line Painting
ROB @ Ph/Fax: 329-1381 Cell: 330-6956
s
CARPET AND
UPHOLSTERY CLEANING
* Walls * Windows * General Cleaning *
758 - 6414 Cell 308-6580 Magrath
Fop relief f pom all ®f youp Imposing and difficult yobs.
I am the person you need fop all of youp light construc­tion
and renovation needs. Fences and yapd clean-up ape
a snap when I do it fop you.
lODie Town Handymai
Fop mope Inf opmatlon about my skills and the reasonable
pates I ask. call Home Town Handyman at 738-b7b5
HOW TO STAY YOUNG
1. Throw out nonessential numbers. This includes age,
weight and height. Let the doctor worry about them. That
is why you pay him/her.
2. Keep only cheerful friends. The grouches pull you down.
3. Keep learning. Learn more about the computer, crafts, gar­dening,
whatever. Never let the brain idle. “An idle mind is
the devil’s workshop.” And the devil’s name is Alzheimer’s.
4. Enjoy the simple things.
5. Laugh often, long and loud. Laugh until you gasp for breath.
6. The tears happen. Endure, grieve, and move on. The only
person who is with us our entire life, is ourselves.
Be ALIVE while you are alive.
7. Surround yourself with what you love, whether it’s family,
pets, keepsakes, music, plants, hobbies, whatever. Your
home is your refuge.
8. Cherish your health: If it is good, preserve it. If it is
unstable, improve it. If it is beyond what you can improve,
get help.
9. Don’t take guilt trips. Take a trip to the mall, to the next
county, to a foreign country, but NOT to where the guilt is.
10. Tell the people you love that you love them, at every
opportunity.
AND ALWAYS REMEMBER: Life is not measured by the
number of breaths we take, but by the moments that take our
breath away. Have a Great Day! ♦
MAXWELL REALTY - LETHBRIDGE
Come See The New Look
FOR SALE — 153N - 2St East, Magrath
We have a conditional offer pending on this beautiful 5 bed­room,
3 bathroom family home. The kitchen / dining room
comes complete with wall-to-wall shelving. Absolutely beau­tiful
park-like yard. The near double lot is surrounded by a
newer cedar fence. The single attached garage is “topped
off” with a huge deck surrounded by maintenance free
aluminum railing. Nearly 2900 sq. ft. of immaculate family
home priced at $132,900. Call Carole Paquette for viewing
or more information.
Holland Insurance
(Magrath) LTD.
____________________AGENTS FOR___________________
INGitò MaiteH*
Vehicle vandalized and caller is not able to pay his
deductible. Can this be extended until he can afford
the full amount or can he negotiate a payment plan?
ANSWER: Deductibles are part of the loss and has to
be paid if caller wants his vehicle repaired.
jll___ Driver's Licenses, Plate Renewals, Driver's Testing, •/QlDdTCl Pay Fines- B>rth Certificate, Marriage License, Death
rc&strcs Certificate, Annual Returns, Corporate Searches, Etc
WE SELL TRAVEL INSURANCE, $1 .OO/doy (restrictions apply)
Phone: Ted, Kathy, Jewelene or Mike at
1-403-758-3391 Fax:1-403-758-6607
My Way Broadway My Way
My Way My Way
21
ns
Broadway
Thcri
Tickets Prices
Adult $15 Youth $13 Family $49
- Group rates available -
WESTWIND SCHOOL DIVISION 171
Westwind School Division #74
2003-2004 School Calendar
August September October
M
1
8
15
T
2
9
16
o-> • no • ->4 •
29 • 30 • 31 *
November
§ = PD Day (School Based)
= Easter Holidays
* = Christmas Holidays
oo = Diploma Exams
■ = 30 First day of Second Semester
§ = PD Day (School Based)
* = Family Day
• = Teacher's Convention
I 18 00
21 « 22 oo 23 x, 24 oo 25 oc
28 29 30
* = Remembrance Day
M T W T F M 1 T W T F M T W T F MTW T F
1 1 * 2 • 3 4 5 1 2 3 3 4 5 6 7
4 5 6 7 8 8 9 10 11 12 6 7 8 9 10 . 10 § Il • 12 13 14
11 12 13 14 15 15 16 17 18 19 13 * 14 15 16 17 17 18 19 20 21
18 19 20 21 22 22 23 24 25 26 § 20 21 22 23 24 24 25 26 27 28
25 26 27 28 29 29 30 27 28 29 30 31
* = Labour Day * = Thanksgiving Day § = PD Day (Divisional Based)
• = First Day of School Y ear
§ = PD Day (School Based)
December January
M T W T F
2 3 4 5 6
9 10 11 12 13
16 • 17 18 19 • 20 •
23 24 25 26 27 '
March
= Christmas Holidays
* = Victoria Day
M T W T F
3 4 5 6 7
10 11 12 13 14
17 18 19 20 21
24 ». 25 26 27 28
31
oo = Diploma Exams
• = Last Day of School
Achievement Examinations
Timed Number Facts Gr. 3 18-May
Language Arts Gr. 3 Part A 19-May
Mathematics Gr. 6 Part A 19-May
Language Arts Gr. 6 & 9 Part A 20-May
Language Arts Gr. 3 (Part B) 17-Jun
Science Gr. 6 17-Jun
Social Studies Gr. 6 18-Jun
Language Arts Gr. 6 Part B 21-Jun
Mathematics Gr. 3 22-Jun
Mathematics Gr. 6 (Part B) 22-Jun
Mathematics Gr. 9 22-Jun
Social Studies Gr. 9 23-Jun
Language Arts Gr. 9 (Part B) 24-Jun
Science Gr. 9 25-Jun
Diploma Examinations
Subject January' June
Eng. 30-1/30-2 Part A 14-Jan 8-Jun
Social Studies 30/33 Part A 16-Jan 10-Jun
Eng. 30-1/30-2 PartB 22-Jan 18-Jun
Mathematics 30 P/30 A 23-Jan 21-Jun
Biology 30 26-Jan 22-Jun
Social Studies 30/33 Part B 27-Jan 23-Jun
Chemistry 30 28-Jan 24-Jun
Physics 30 29-Jan 25-Jun
Instructional Days 188
P.D. Days (No Students) 6
Parent/Teacher Evenings 2
Total Days 196
* For further in","Magrath Store News (August 20, 2003)",,J. A. Ririe,,,core
236403325,,"Distanced learning poses challenges for all participants. Creation of a sense of learning community in online environments can reduce student isolation, attrition, and sense of disconnect with educational institution. It can also increase motivation, enrich the learning environment, and build social capital. Based on an analysis of the communities of practice literature, it was decided to create a sense of community in an Introduction to Business and Management course (BMGT 110) offered by UMUC Europe. It was also decided to select course structure, pedagogic strategy, and online interaction that allowed the instructor deliver a presentation of self grounded in authenticity. The briefly reviewing the relevant literatures, the paper examines strategies employed in the deliver of the course, and analyzes participant responses to questions about community and instructor authenticity. Findings suggest that instructor authenticity was at least a modifying element in the creation of what students recognized as a sense of community.Instructor Authenticity in Mediating a Sense of Online Community
by
David Starr-Glass
David Starr-Glass, born and educated in Scotland, has lived in California and Israel
for the last thirty-five years. He currently resides in Jerusalem. His undergraduate
degree, from the University of Glasgow, is in botany and he retains an active interest
in this field. In California, he obtained an undergraduate degree in accounting and
worked extensively in management and cost accounting in the retail and electronic
sectors in Silicon Valley before moving into business education. He is presently an
adjunct faculty member with UMUC, European Division, and a mentor with the
Center for International Programs of Empire State College (SUNY) in Prague.
David has earned an MBA from Notre Dame de Namur University, an M.Sc.
(organizational psychology) from the University of London, and an M.Ed. (online
pedagogy) from the University of Southern Queensland. His present areas of
academic interest include organizational behavior, international education, and the
dynamics of learning at a distance, which is the focus of this paper.
Contact information: dstarrglass@faculty.ed.umuc.edu
Page 1
Instructor Authenticity in Mediating a Sense of Online Community
Marco Polo describes a bridge, stone by stone. ‘But which is the stone that supports the bridge?’
Kublai Khan asks. ‘The bridge is not supported by one stone or another,’ Marco answers, ’but by
the line of the arch that they form.’ Kublai Khan remains silent, reflecting. Then he adds: ‘Why do
you speak to me of the stones? It is only the arch that matters to me.’ Polo answers: ‘Without
stones there is no arch.’
Calvino (1997, p. 82).
Abstract
Distanced learning poses challenges for all participants. Creation
of a sense of learning community in online environments can
reduce student isolation, attrition, and sense of disconnect with
educational institution. It can also increase motivation, enrich the
learning environment, and build social capital. Based on an
analysis of the communities of practice literature, it was decided to
create a sense of community in an Introduction to Business and
Management course (BMGT 110) offered by UMUC Europe. It
was also decided to select course structure, pedagogic strategy, and
online interaction that allowed the instructor deliver a presentation
of self grounded in authenticity. The briefly reviewing the relevant
literatures, the paper examines strategies employed in the deliver
of the course, and analyzes participant responses to questions about
community and instructor authenticity. Findings suggest that
instructor authenticity was at least a modifying element in the
creation of what students recognized as a sense of community.
Introduction
Distant learning may, in many senses, be regarded as an oxymoron. Distant learning
is not simply a displacement of learners; it is the displacement of the learning process –
distant learning is always distanced learning, and in that distancing there is always the
reduction of social engagement. The construction of knowledge is essentially a social
activity requiring language, sociocultural interaction, and ways of making comparative
assessments of knowledge gained. While there is obviously a psychological and
cogitative interiority, learning is embedded in a social matrix and it is through that matrix
that we communicate, share, and makes sense of what has been learned.
In its earliest forms, the term distant learning was applied to contexts when the
educational institution operated at a distance from enrolled students: distant referred to
geographic displacement. The implied emphasis on displacement between students and
institution has more recently shifted as the impact on the educational process in which
distant learners is better appreciated. Distant learning is increasingly understood in a
more problematically sense, as distanced learners engaged in an isolated process of
creating new knowledge. Distanced learners have to create and manage their own
Page 2
learning environments; distanced learners are displaced from other learners – in time and
in space – and such a displacement challenges notions of a community of learning, of a
social dimension in the learning process, and of the active co-creation of knowledge.
This article is based on a consideration of the theoretical challenge and personal
practice of attempts to create a social dimension, admittedly tenuous – a sense of
community – within an online course. The online course is Introduction to Business and
Management (BMGT 110), conducted on the WebTycho platform at the UMUC,
European Division and this article is written with members of the UMUC distanced
education collegiate community in mind: readers familiar with the current version of that
platform and familiar with online issues and practice.
If a sense of community within an online learning environment is deemed desirable,
then efforts must be made to create that sense among participants. The opportunity and
challenge fall initially on the instructor who has the chance to construct, deliver, manage,
and evaluate the online environment. The following review of one such attempt to create
and sustain community begins with a consideration of different aspects and appreciations
of community in learning, with particularly regard to the education of business majors
and online students. While there are many suggestions regarding the creation of social
dynamics in online settings, this paper considers the role of instructor authenticity in
initiating a sense of relatedness among participants. After a general discussion of
authenticity, there is a description of strategies and techniques used in an online
presentation of BMGT 110 to introduce and encourage authentic engagement. Student
feedback and comments on sense of community are presented and a review of student
feedback is used to underscore remaining challenges, opportunities, and future directions
in making the online learning environment one that acknowledges, encourages, and
develops a social dimension.
Social Learning and Community Benefit
Social context has often been seen as a uniquely significant factor in the production
of knowledge. In the varied, but complementary, works of Albert Bandura (Bandura,
1976), Jean Lave (Lave & Wenger, 1991), and Etienne Wenger (Wenger 1998a, 1998b),
social context is identified as one of the primary forces in shaping the way in which
learning occurs, in making learning experiences salient, and in reinforcing changed
behavior. While such theories isolate and privilege social components, it is clear that
there are many other factors that define and sustain learning environments. Certainly,
contemporary social theories of learning are more inclusive and sensitive to the non-social
factors that shape and moderate learning (Hughes, 2004) and are distinguish from
more exclusive theory of social learning that privileges social setting (Wenger, 2004, p.
4).
Social connectivity and an emerging sense of community are enhanced significantly
by participant interaction and by the length of engagement, which is characteristically
short for an academic course whether face-to-face and distanced. Before examining how
Page 3
social interaction and a sense of community can be enhanced, it is necessary to stress the
positive and useful outcomes generally attributed to learning within a social sense. These
include an enrichment of the learning environment; a reduction of student isolation
effects and consequences; a sense of legitimate peripheral participation for those who are
looking towards professional or career trajectories; and, the creation and refinement of
social and human capital.
Enriched learning environments
A social component of the learning process can provide learners with an enriched
environment for exploration of newly gained knowledge (Conrad & Donaldson, 2004),
experimentation with communication (Nagda, 2006), coping with personal doubt and
subject matter ambiguity, consideration of deep learning approaches (Chapman, Ramondt
& Smiley, 2005), elaboration of theoretical and personal narratives, and inclusion in a
broader sense-making discourse (Wiessner, Chapman, Berardinelli & Jones, 2006).
Learning environments are made potentially more interesting, more engaging, and more
accessible to participants. Such environments are also more open to diversity and
creativity when they contain a social element that allows participants to feel connected
with others (Conrad, 2002).
Reduction of isolation effects
Distanced learners are often isolated and solitary learners. For some, this is a
preferred way of learning: for others it is perceived as a difficulty. As adult learners,
online students have to demonstrate commitment to learning objectives and the ability to
mange time, effort, and motivation in achieving those learning goals. This can be difficult
and stressful for many. Perceived social inclusion can reduce the sense of isolation and
detachment often experienced by distanced and isolated learners, many of whom might
be first time college students, returning to college after a long absence, or seriously
conflicted about the utility of higher education and fear of failing (Lee & Chan, 2007).
Perceived communal inclusion may also provide students with mutual support; increased
self-esteem; serve to reevaluate educational goals; stimulate in-group comparisons;
reduce stress and anxiety; and increased levels of motivation (Aragon, 2003). At the
macro level, reduced isolation and a developing communal sense is most likely a
significant factor in decreasing course attrition and increasing overall student retention
rate (DiRamio & Wolverton, 2006).
Legitimate peripheral participation
Social contexts in academic settings may also emphasize possible bridges between
current study and anticipated future, especially where that future is considered to be
organizational membership or professional, or vocational, engagement (Wenger 1998a,
1998b). Distanced learners are distanced not only from the academic community of their
institute of higher learning: they are also distanced from those who are engaged in
communities of practice. This is particularly so with business students, who strive to
acquire competencies but need the reassurance that such competencies are a legitimate
Page 4
part of their future community of professional practice. Participants may come to
appreciate that they are being initiate into knowledge that will extend beyond the college
into future professional interest and practice. They may also come to appreciate that they
are connected through this pathway with their peers.
Social and human capital creation
Social context provides the matrix for the creation of social and human capital
(Portes, 1998). Social capital has proved a popular, albeit somewhat illusive, concept.
While its nature and function might differ contextually, Coleman (1990) notes two
characteristics common to all forms of social capital: “… they all consist of some aspect
of social structure, and they facilitate certain actions of individuals who are within the
structure (p. 303).”
Social capital refers to the mutual trust, norms of reciprocity, and access to human
capital that potentially come into play within social networks. These attributes are
recognized and valued by students and alumni as evidenced in their use of social and
professional networking, mutual association, and patterns of exchange that take place
within these networks. Social capital can be created among students and faculty when
they sense that there is a common commitment to teaching and learning (Cheng, 2004).
Encouraging social and human capital formation in undergraduate learning systems may
well predispose students to develop productive networks in subsequent professions and
careers. Distanced students are challenged to belong to such networks not because of a
lack of technical connectivity but by the absence of a sense that such networks can extend
beyond the purely recreational or superficially social.
Structural and Dynamics of Online Communities
Much of the understanding of communities in learning contexts has come from
individuals entering, learning, and being socialized into informal groupings, professions,
and vocations. Such “communities of practice” (Wenger 1998a, 1998b) exist when a
novice is seen as having legitimacy to enter into the practice of the group. Originally,
communities of practice almost inevitably described situations where novice and
practitioner were physically co-located (Seely Brown & Duguid, 1991); however, similar
social dynamics can also be seen where group members are physically, or temporally,
separated and never meet (Conkar, Noyes & Kimble, 1999). Such virtual communities of
practice develop in internationally distributed work teams, where permanently distanced
members engage cooperatively on research or innovation problems (Hildreth & Kimble,
2000; and Wiredu, 2007).
The creation of any sense of community is challenged by the degree of common
social interaction and the time that the group remains together. In learning situations
where a face-to-face delivery mode is employed, there are limits on the nature, degree of
authenticity, intensity, and longevity of an emergent sense of community. The result is
Page 5
seen as a “bounded community” (Wilson et al, 2004. p.2), “learning community within a
curriculum framework – bounded by the expectations inducing participation, but also by
the timeframe of a typical course.”
Communities of practice and bounded communities have three underpinning
dimensions, which must be addressed in any effort to create a community (Wenger,
1998a, p. 73): joint enterprise, shared repertoires, and mutual engagement.
Joint enterprise
Joint enterprise exists when students recognize a compelling purpose and
commonality in the tasks in which they engaged. They perceive interlocking roles and
scripts for their actions. They also see possibilities for social engagement rather than for
an association with a collectivity of individuals. There is an element of mutual
recognition and accountability. While individual participants might be working on
specific pieces, they appreciate that these separate parts parallel, or to some extent
overlap, the work of other participants.
Shared repertoire
Participants recognize common access to resources and ways of completing tasks.
In a dramaturgical sense, there is open access to roles, scripts, stories, acts, performances,
and measures of critical appreciation. Members have shared expectations as to tools,
procedures, and anticipated results. They have, in that sense, a commonly understood and
articulated language that allows them to communicate and appreciate the presence and
contribution of the other.
Mutual engagement
Participants sense a mutuality not simply in performing roles and producing
outcomes but in relating to other participants; in establishing and relying on trust; in
maintaining mutual confidence; and, in the recognition and maintenance of social
cohesiveness and belonging. Participants recognize that there is something that binds
them, however temporarily and however tenuously, apart from the centrality of the
designated task.
Based on these elements and the comprehensive analysis of online communities by
Stacey Ludwig-Hardman (2003), Wilson et al (2004) formulated a model that identified a
set of interlocking components that can serve both to create an online community and to
measure the extent to which participants recognize community. Their model identified
seven components: shared goals; safe and supportive conditions; community identity;
collaboration; respectful inclusion; progressive discourse towards knowledge building;
and mutual approbation. These components provide an instructive framework for
establishing and analyzing the extent of community perception within an online context.
Page 6
What is Instructor Authenticity?
In designing an online environment – and hopefully one in which a sense of
community can develop – the instructor is inevitably the initial formulator and
implementer of a pedagogic strategy. Sometimes that strategy is clearly developed and
coherently implemented: sometimes it is not. Sometimes the strategy seeks to create and
sustain community: sometimes it ignores or fails to adequately engage with such a
challenge or opportunity. In all cases, however, the dynamics that result within a learning
environment are a consequence of the instructor’s pedagogic strategy, its implementation,
subsequent in-process adjustments, and ongoing mediation.
In online environments, behaviors that suggest psychological immediacy and social
presence (Gunawardena & Zittle, 1997) have been associated with increased levels of
participant attitude, motivation, and satisfaction (Conway, Easton & Schmidt, 2005;
Richmond & McCroskey, 2000). Social presence and psychological immediacy signal to
online participants that they have the opportunity to engage with another person using the
provided instructional technology: the other is regarded as real and responsive. While
attention has often focused on the quantitative measurement of social presence, another
critical aspect is its quality. The suggestion in this paper is that participants do not rate
social presence in a binary fashion – present or absent – but that they attempt to uncover
the nature and quality of the others that constitute social presence, trying to develop a
sense of the psychological dimensions of the other.
It seems inevitable that in any attempt to create, or to recognize, social presence
there is also a process of trying to communicate and interpret presentations of self.
Through the creation and management of the WebTycho environment, through the
selection, the implementation of pedagogic strategy, and through attempts to engender
social presence online, it is the instructor who initially sets and determines an appropriate
presentation of self. Considering that most BMGT 110 cohorts comprise adult learners,
often with considerable experience and responsibility in their military duties, and taking
into account the instructor’s previous experience with creating an online presence, the
appropriate presentation of self in this present research seemed to be “authenticity”.
Patricia Cranton (2001) defines authenticity as, “the expression of the genuine self”.
With authenticity there is a presumed process of maturation, a sense of changed
perspective, and a developing confidence in self and in others. Generally, those who
teach in higher education are not trained in adult education and engage in teaching in
ways that they intuitively recall from their own educational experiences. While neither
intuitive nor obvious, reflective practice and a critical consideration of experience often
lead to faculty becoming more authentic in their lives and in their instruction. Displays of
authenticity are frequently attributed to more experienced faculty members who have
engaged in a positive and considered way with the educational process and the fate of
their students This career convergence towards authenticity is often revealed in the
sequential self-narratives and longitudinal studies of faculty members (Carusetta &
Cranton, 2005; Cranton, 2007; and Cranton & Carusetta, 2004)
Page 7
Patricia Cranton’s work, originally developed from a grounded theory analysis of a
range of teaching experiences in adult education and which reflects many themes in the
preexisting authenticity literature, views authenticity as having four interrelated
components: genuineness, authenticity, encouraging authenticity in others, and leading a
critical life.
Genuineness
Genuineness is seen in terms of self-awareness, where the learner recognizes
learning as a potentially transformative process allowing for the critically reappraise and
examine previously assimilated beliefs, values, and assumptions. This provides the
opportunity to become more open, flexible, considerate, and permeable (Mezirow, 2000).
Authenticity
Authenticity is viewed as a demonstrated consistency and congruency between
values and actions (Brookfield, 1997). The instructor is perceived as having genuinely
embraced the subject material, or as having subjected it to at least as critical review as
any other adult learner. There is a sense that the educator is actively looking for questions
and answers, and neither presumes to possess them nor wishes to communicate them in a
passive, unquestioned, or uncritical manner. This might also be recognized in terms of a
behavioral consistency that deliberately balances confidence and credibility with a
presentation of openness and critical sensitivity (Brookfield, 1997), or it might be
evidenced in an articulate",Instructor authenticity in mediating a sense of online community,,University of Maryland University College (UMUC),,,core
53959647,2005-01-01T00:00:00,"An important aspect in the issue of contaminated sites is their use after remediation. The choice of a certain use has got necessarily socio-economic implications that make it more or less suitable for that site. For this reason it is important to insert socio-economic analysis within the studies on contaminated site remediation and to implement approaches that calibrate extent and modality of the remediation on the basis of potential uses of the site after it. In particular it is important to provide decision-makers with tools that offer them the possibility to consider also this aspect, in order to support their decisions.The specific aim of the socio-economic module is providing the decision makers with a tool that makes possible to compare the different use destinations, outlining possible scenarios linked to alternative uses of the considered site, on the basis of socio-economic considerations (often founded on theories and methods of the spatial analysis) and of local characteristics. Comparing these scenarios it aims to give indications on which use is more suitable and why. The final objective is just to establish which is the \u201cbest\u201d use for that site. The term \u201cbest\u201d indicates that it is possible to rank the whole different use destinations. This is a typical multicriteria decision making problem and there is not a natural order in a multidimensional space, so it is necessary to find a device to do it. A method to rank them is to define a function from the attribute space in the real line and so obtain a total order induced by the real number one. The scientific literature is reach of several ways to approach this problem. Here we propose a method typical of Artificial Intelligence: a Fuzzy Expert Systems (FES). The final product is a software prototype for the remediation of contaminated sites: DESYRE (DEcision Support sYstem for the REqualification of contaminated sites, www.r3environmental.co.uk/dstdemo/), that represents a useful tool for decision-makers. Entering data, relative to the considered site, the software is able to elaborate them to give back one numerical index for each possible use destination (UD). These indexes represent synthetic and comparable expressions of the socio-economic implications deriving from each UD. In this way the decision-makers can compare the opportunities coming from the different uses and have a synthetic indicator, without losing the whole information. The software works in a very transparent way, so that it is possible to highlight which factors determine the high or low index value",A fuzzy expert approach for comparing alternative end uses for requalification of contaminated sites,,place:New York,,,core
147914829,2006-03-10T15:36:24,"Computerized human face processing (detection, recognition, synthesis) has known an intense research activity during the last few years. Applications involving human face recognition are very broad with an important commercial impacts. Human face processing is a difficult and challenging task: the space of different facial patterns is huge. The variability of human faces as well as their similarity and the influence of other features like beard, glasses, hair, illumination, background etc., make face recognition or face detection difficult to tackle. The main task during the internship was to study and implement a neural-network based face detection algorithm for general scenes, which has previously been developed within the IDIAP Computer Vision group. It also included the study and design of a multi-scale face detection method. A face database and a camera were available to make tests and perform some benchmarking. The main constaint was to have a real-time or almost real-time face detection system. This has beeen achieved. Evaluation of the face detection capability of the employed neural networks were demonstrated on a variety of still images. In addition, we introdudced an efficient preprocessing stage and a new post-processing strategy to eliminate false detections significantly. This allowed to deploy a single neural network for face detection running in a sequential manner on a standard workstation",Fast Multi-Scale Face Detection,https://core.ac.uk/download/147914829.pdf,IDIAP,,,core
24550707,2008-02-07,"Abstract—A fast responsiveness system incurs minimum latency and produces high throughput and quick response. Interacting with these systems using friendly natural language interfaces requires natural language (NL) capability and an NL processing component. Examples of fast responsive systems include mission-critical systems like aerospace applications and real-time text messaging applications. The NL component constitutes an important part of the interface. We propose NL interface models for fast responsiveness systems with a language disambiguation component. The language disambiguation component is based on using supervised Machine Learning (ML) techniques. For that, we designed and implemented a number of language disambiguation techniques for word prediction to be used with such systems that require fast responsiveness. In the experimental evaluation, proposed techniques demonstrated impressive performance in prediction accuracy. AFAST RESPONSIVENESS SYStem is a system that incurs minimum latency and produces high throughput and quick responses. Interacting with such systems requires smart interfaces with capabilities to maintain and support fast responsiveness. The smart interfaces of these systems require natural language (NL) capabilities and a natural language processing (NLP) component. Examples of fast responsiveness systems include Virtual Reality (VR) training programs, real-tim",Natural Language Interface Models for Fast Responsiveness Applications,,,,,core
219372139,2009-01-01T00:00:00,"The purpose of this paper is to lay the foundations of a new generation of closed loop
optimal control laws based on the plant state space model and implemented using artificial neural
networks. The basis is the long established open loop methods of Bellman and Pontryagin, which
compute optimal controls off line and apply them subsequently in real time. They are therefore open
loop methods and during the period leading up to the present century, they have been abandoned by
the mainstream control researchers due to a) the fundamental drawback of susceptibility to plant
modelling errors and external disturbances and b) the lack of success in deriving closed loop versions
in all but the simplest and often unrealistic cases. The recent energy crisis, however, has promoted the
authors to revisit
the classical optimal control methods with a view to deriving new practicable
closed loop optimal control laws that could save terawatts of electrical energy by replacement of
classical controllers throughout industry. First Bellman’s and Pontryagin’s methods are compared
regarding ease of computation. Then a new optimal state feedback controller is proposed based on the
training of artificial neural networks with the computed optimal controls",A comparison of fixed final time optimal control computational methods with a view to closed loop implementation using artificial neural networks,https://core.ac.uk/download/219372139.pdf,,,,core
1509418,2009-04-01T00:00:00,"1 Introduction Modelling biological systems, is impaired by the cost of experimentally obtaining the data required to build the models. The resources available are typically very limited compared to large experimental parameter spaces, so have to be used efficiently. Similarly, in engineering with biological systems such as is found in synthetic biology and molecular computation, models of the phenomena to be harnessed are required. These models can only be obtained experimentally. As a consequence, for engineering with biological systems to become more prevalent, tools and techniques are required to assist in the experimentation performed to develop these models. Here we focus on one such tool, a computational system capable of autonomously investigating an experimental parameter space to identify phenomena that exist within. We call this computational system an autonomous experimentation system. Autonomous experimentation systems try to capture the efficiency of experimentalists, who are able to successfully navigate a seemingly boundless space of potential experiments. Autonomous experimentation systems develop hypotheses, plan experiments and perform experiments, in a closed loop manner without human interaction. As a foundation for our approach to autonomous experimentation, we draw on ideas from the philosophy of science. 2 Experimentation Experimentation should work to disprove hypotheses [1]. A hypothesis gives a possible explanation for some observed phenomena. Hypothesis lead experimentation can benefit from considering many hypotheses simultaneously, so as to allow for different explanations for a particular phenomenon to be developed without prejudice [2, 1]. While human scientists are limited in the number of different working hypotheses that they can realistically contemplate and visualise at any one time, a computer has no such limitation and could compare many thousand different hypotheses simultaneously. The hypotheses that are developed through experimentation, need not be mechanistic in nature. The investigation of the relationship between cause and effect can be performed experimentally, without developing mechanistic hypotheses. It is known for instance that children display an ability to associate cause with effect from an early age through their early play [3]. The use of cause and effect experimentation in scientific, medical and engineering work, has allowed for developments in these areas without an understanding for why the cause and effect are related. For example, a cure for scurvy was produced long before anyone understood why the cure worked [4]. 3 From Experimentation to Autonomous Experimentation Computational systems capable of scientific discovery are interlinked with artificial intelligence systems. Computational scientific discovery puts into practice artificial intelligence methods and brings real world benefits with it. It is important to note that neither artificial intelligence or autonomous experimentation systems will be able to match the abilities of human creativity. The KEKADA system [5], was an early computational experimentation system that was able to develop hypotheses and plan experiments to investigate an experimental parameter space. The KEKADA system took the approach of performing a broad search of the experimental parameter space through experimentation until a surprising phenomena was found, at which point the system performs more focussed experiments on the surprising phenomena. The hypotheses developed were mechanistic in nature and the system was able to rediscover known phenomena, such as determining the mechanism for how urea is synthesised in the body, and determining the structure of common alcohol, which also showed the systems generality in its hypotheses [5]. The KEKADA systems limitations came to the fore when comparing it to scientists, where scientists have the advantage of being able to employ additional heuristics and so able to solve a significantly larger number of problems. The authors concluded that improvements to the KEKADA system could come through increasing the amount of domain knowledge available to the system. Domain knowledge to developing hypotheses in other computational experimentation systems. It was shown that reaction pathways could be obtained automatically from experimental evidence and domain knowledge [6]. The use of increased domain knowledge was influential in the DENDRAL project, which built an expert system for use in scientific reasoning, and in particular aiding structure elucidation in organic chemistry [7]. The combination of domain knowledge and experimental results through abductive reasoning, was shown to be able to rediscover the function of genes in known roles [8]. The Robot Scientist project largely automated the majority of the physical side of experimentation and was also able to show that the computational system and human scientists could comparatively well interpret experimental results, albeit in the limited domain space in which the Robot Scientist operated [8]. In the technical application of enzymes for molecular computing, such large amounts of domain knowledge do not exist. A different approach for autonomous experimentation therefore has to be taken than those that already exist. One autonomous experimentation system has moved away from the requirements of large amounts of domain knowledge and instead concentrated on searching the experimental parameter space for surprising phenomena, through a fully closed-loop system [9]. Such an approach does not lend itself well to the development of mechanistic hypotheses, which is why we look to investigate causality in our approach. By investigating causality, we are still able to develop hypotheses that are both useful to a scientist and are free from large amounts of domain information. 4 Autonomous Experimentation for Molecular Computing Enzymes can be thought of as a pattern recognizer working at the molecular level. An enzymes ability to process multiple inputs simultaneously, make it a candidate for use in molecular computing [10]. At present enzymes cannot be designed for purpose, therefore experimentation is necessary to develop these models. An example of a typical experiment that our system could perform, is monitoring the activity of an enzyme by spectroscopically measuring the ultra-violet (UV) absorbance of the enzyme. Models can then be made of the interplay between different substances that effect the enzymes activity and the quantities of those substances, with the resulting UV absorbance. Of particular interest are regions of the parameter space where a combination of factors cause a specific change in the enzymes behaviour. Such changes in behaviour are the properties that could be harnessed in molecular computation. Our system will determine these regions of interest for molecular computation. A microfluidic device is under development to allow for a fully autonomous, closed-loop experimentation system. This lab-on-a-chip technology uses only small amounts of chemistry for each experiment, which will make autonomous experimentation more practical in terms of cost. We follow a conceptual design similar to those of previous work [5, 11], as shown in figure 1, where hypotheses are generated, experiments are proposed so as to try and disprove hypotheses, then an automatically chosen experiment is performed, with the results being used to update the working set of hypotheses. Any practical experimentation system has to robustly model a set of data, capable of handling noise on the dependent and independent parameters. Our prototype implementation shows that smoothing splines offer these properties for modelling the type of data we expect. We are currently investigating the use of smoothing splines modified with a Bayesian framework, to develop models with lower dimensional data sets than those we expect to be using in later work. These lower dimensional data sets will allow for easier conceptualisation of the problems of autonomously developing hypotheses and methods for exploring the experimental parameter space",Towards Algorithms for Autonomous Experimentation,,,,,core
10547180,"September 20, 2004","This paper describes an overview of our ""Bioinspired Engineering of Exploration Systems for Mars"" ( ""BEES for Mars"") project. The BEES approach distills selected biologically inspired strategies utilizing motion cues/optic flow, bioinspired pattern recognition, biological visual and neural control systems, bioinspired sensing and communication techniques, and birds of prey inspired search and track algorithmic systems. Unique capabilities so enabled, provide potential solutions to future autonomous robotic space and planetary mission applications. With the first series of tests performed in September 2003, August 2004 and September 2004, we have demonstrated the BEES technologies at the El Mirage Dry Lakebed site in the Mojave Desert using Delta Wing experimental prototypes. We call these test flyers the ""BEES flyer"", since we are developing them as dedicated test platform for the newly developed bioinspired sensors, processors and algorithmic strategies. The Delta Wing offers a robust airframe that can sustain high G launches and offers ease of compact stowability and packaging along with scaling to small size and low ReynOld's number performance for a potential Mars deployment. Our approach to developing light weight, low power autonomous flight systems using concepts distilled from biology promises to enable new applications, of dual use to NASA and DoD needs. Small in size (0.5 -5 Kg) BEES Flyers are demonstrating capabilities for autonomous flight and sensor operability in Mars analog conditions. The BEES project team spans JPL, NASA Ames, Australian National University (ANU), Brigham Young University(BYU), DC Berkeiey, Analogic Computers Inc. and other institutions. The highlights from our recent flight demonstrations exhibiting new Mission enabling capabilities are described. Further, this paper describes two classes of potential new missions for Mars exploration: (1) the long range exploration missions, and (2) observation missions, for real time imaging of critical ephemeral phenomena, that can be enabled by use of BEES flyers. For example, such flyers can serve as a powerful black-box for critical descent and landing data and enablers for improved science missions complementing and supplementing the existing assets like landers and rovers by providing valuable exploration and quick extended low-altitude aerial coverage of the sites of interest by imaging them and distributing instruments to them. Imaging done by orbiters allows broad surface coverage at limited spatial resolution. Low altitude air-borne exploration of Mars offers a means for imaging large areas, perhaps up to several hundred kilometers, quickly and efficiently, providing a close-up birds-eye view of the planetary terrain and close-up approach to constrained difficult areas like canyons and craters. A novel approach to low-mass yet highly capable flyers is enabled by small aircraft equipped using sensors and processors and algorithms developed using BEES technology. This project is focused towards showing the direct impact of blending the best of artificial intelligence attributes and bioinspiration to create a leap beyond existing capability for our future Missions",Bioinspired Engineering of Exploration Systems (BEES) - its Impact on Future Missions,,,,,core
22088124,2007-11-21,"We introduce the RAN 2 SOM (Reconfigurable Architecure Neural Networks with Serially Operating Multipliers) architecture, a neural net architecture with a reconfigurable interconnection scheme based on bit stream arithmetic. RAN 2 SOM nets are implemented using field programmable gate array logic. By conducting the training phase in software and executing the actual application in hardware, conflicting demands can be met: training benefits from a fast editdebug cycle, and once the design has stabilized, a hardware implementation results in higher performance. While neural nets have been implemented in hardware in the past, larger digital nets have not been possible due to the real-estate requirements of single neurons. We present a bit-serial encoding scheme and computation model, which allows space-efficient computation of the sum of weighted inputs, thereby facilitating the implementation of complex neural networks. 1 Introduction Conventional computer hardware is not optimize..",RAN²SOM: A Reconfigurable Neural Network Architecture Based on Bit Stream Arithmetic,,,,,core
20755326,2005,"In this thesis, theory and applications of machine learning systems based on information theoretic criteria as performance measures are studied. A new clustering algorithm based on maximizing the Cauchy-Schwarz (CS) divergence measure between probability density functions (pdfs) is proposed. The CS divergence is estimated non-parametrically using the Parzen window technique for density estimation. The problem domain is transformed from discrete 0/1 cluster membership values to continuous membership values. A constrained gradient descent maximization algorithm is implemented. The gradients are stochastically approximated to reduce computational complexity, making the algorithm more practical. Parzen window annealing is incorporated into the algorithm to help avoid convergence to a local maximum. The clustering results obtained on synthetic and real data are encouraging. The Parzen window-based estimator for the CS divergence is shown to have a dual expression as a measure of the cosine of the angle between cluster mean vectors in a feature space determined by the eigenspectrum of a Mercer kernel matrix. A spectral clusterin",An Information Theoretic Approach to Machine Learning,,,,,core
22832417,2007-11-22,"While many issues in the area of virtual reality (VR) researchhave been addressed in recentyears, the constant leaps forward in technology continue to push the field forward. VR research no longer is focused only on computer graphics, but instead has become even more interdisciplinary,combining the fields of networking, distributed computing, and even artificial intelligence. In this article we discuss some of the issues associated with distributed, collaborative virtual reality,aswell as lessons learned during the developmentoftwo distributed virtual reality applications.  1 Introduction  The Futures Laboratory at Argonne National Laboratory has been exploring what is needed to support large-scale shared space virtual environments (VE) for wide-area collaborations. Our research has focused on the system architecture, software design, and features needed to implement suchenvironments. In this article we discuss twoprototype systems under development at Argonne.  Shared virtual spaces a..",Two Implementations of Shared Virtual Space Environments,,,,,core
83702939,2009-01-01T00:00:00,"This paper discusses a method for abnormal motion detection and its real-time implementation on a smart camera. Abnormal motion detection is a surveillance technique that only allows unfamiliar motion patterns to result in alarms. Our approach has two phases. First, normal motion is detected and the motion paths are trained, building up a model of normal behaviour. Feed-forward neural networks are here used for learning. Second, abnormal motion is detected by comparing the current observed motion to the stored model. A complete demonstration system is implemented to detect abnormal paths of persons moving in an indoor space. As platform we used a wireless smart camera system containing an SIMD (Single. Instruction Multiple-Data) processor for real-time detection of moving persons and an 8051 microcontroller for implementing the neural network. The 8051 also functions as camera host to broadcast abnormal events using Zig Bee to a main network system",Abnormal Motion Detection in a Real-Time Smart Camera System,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICDSC.2009.5289359,,core
268932471,2008-06-30T00:00:00,"In this work a direct method to measure the stability of metro system lines with respect to a previously constructed time schedule is presented. For this purpose we first model saturation effects using a real time discrete space state representation and then apply a Lyapunov-based stability analysis considering time delays of trains as disturbances. As a result we have been able to define a new set of indexes that relate time delays with the validity of the actual time schedule when falling inside a particular ‘stability area’. Results obtained in a simulated environment show that the new stability indexes are able to evaluate quantitatively and qualitatively the effects of saturation in metro lines as well as predict the need for rescheduling Keywords: metro system, stability, planning, genetic algorithm, artificial intelligence. 1 Introduction The dynamics of metro line systems have been deeply studied by several researchers [1–5,7–10]. Most of these dynamical models are based on the Sasama and Ohkawa [9] linear model. It is well known that such kind of linear models, usually yield simple formulation, implementation and simulation. As a result, some dynamic traffic linear controllers [1–3,7] and real time simulators [3,4] have been proposed.In this work a direct method to measure the stability of metro system lines with respect to a previously constructed time schedule is presented. For this purpose we first model saturation effects using a real time discrete space state representation and then apply a Lyapunov-based stability analysis considering time delays of trains as disturbances. As a result we have been able to define a new set of indexes that relate time delays with the validity of the actual time schedule when falling inside a particular ‘stability area’. Results obtained in a simulated environment show that the new stability indexes are able to evaluate quantitatively and qualitatively the effects of saturation in metro lines as well as predict the need for rescheduling Keywords: metro system, stability, planning, genetic algorithm, artificial intelligence. 1 Introduction The dynamics of metro line systems have been deeply studied by several researchers [1–5,7–10]. Most of these dynamical models are based on the Sasama and Ohkawa [9] linear model. It is well known that such kind of linear models, usually yield simple formulation, implementation and simulation. As a result, some dynamic traffic linear controllers [1–3,7] and real time simulators [3,4] have been proposed",Lyapunov Based Stability Analysis for Metro Lines,https://core.ac.uk/download/268932471.pdf,,,,core
54835945,2009-01-18T00:00:00,"International audienceThis paper deals with the decomposition of multivariate functions into sums and compositions of monovariate functions. The global purpose of this work is to find a suitable strategy to express complex multivariate functions using simpler functions that can be analyzed using well know techniques, instead of developing complex N-dimensional tools. More precisely, most of signal processing techniques are applied in 1D or 2D and cannot easily be extended to higher dimensions. We recall that such a decomposition exists in the Kolmogorov's superposition theorem. According to this theorem, any multivariate function can be decomposed into two types of univariate functions, that are called inner and external functions. Inner functions are associated to each dimension and linearly combined to construct a hash-function that associates every point of a multidimensional space to a value of the real interval [0,1]. Every inner function is the argument for one external function. The external functions associate real values in $[0,1]$ to the image by the multivariate function of the corresponding point of the multidimensional space. Sprecher, has proved that internal functions can be used to construct space filling curves, i.e. there exists a curve that sweeps the multidimensional space and uniquely matches corresponding values into $[0,1]$. Our goal is to obtain both a new decomposition algorithm for multivariate functions (at least bi-dimensional) and adaptive space filling curves. Two strategies can be applied. Either we construct fixed internal functions to obtain space filling curves, which allows us to construct an external function such that their sums and compositions exactly correspond to the multivariate function; or the internal function is constructed by the algorithm and is adapted to the multivariate function, providing different space filling curves for different multivariate functions. We present two of the most recent constructive algorithms of monovariate functions. The first method is due to Sprecher. We provide additional explanations to the existing algorithm and present several decomposition results for gray level images. We point out the main drawback of this method: all the function parameters are fixed, so the univariate functions cannot be modified; precisely, the inner function cannot be modified and so the space filling curve. The number of layers depends on the dimension of the decomposed function. The second algorithm, proposed by Igelnik, increases the parameters flexibility, but only approximates the monovariate functions: the number of layers is variable, a neural networks optimizes the monovariate functions and the weights associated to each layer to ensure convergence to the decomposed multivariate function. We have implemented both Sprecher's and Igelnik's algorithms and present the results of the decompositions of gray level images. There are artifacts in the reconstructed images, which leads us to apply the algorithm on wavelet decomposition images. We detail the reconstruction quality and the quantity of information contained in Igelnik's network",Kolmogorov Superposition Theorem and its application to wavelet image decompositions,,'SPIE-Intl Soc Optical Eng',10.1117/12.805916,,core
24606997,2008-02-07,"We address the problem of recognizing real flat objects from two-dimensional images. A new method is proposed which has mainly been designed to handle complex objects and performs under occlusion and similarity transformations. Matching operates hierarchically, guided by a Curvature Scale Space (CSS) segmentation scheme, and takes advantage of important object features first, that is, features which distinguish an object from other objects. The model database is implemented using a set of Artificial Neural Networks (ANNs) which pro vide the essential mechanism not only for establishing correct associations between groups of segments and models but also for enabling efficient searching and robust retrieval, especially when noisy or distorted objects are considered. 1",Recognition Using Curvature Scale Space and Artificial Neural Networks,,,,,core
24623753,2008-04-01,"Planning—in its classical sense—is the problem of finding a sequence of actions that achieves a predefined goal. As such, much of the research in AI planning has been focused on methodologies and issues related to the development of efficient planners. To date, several efficient planning systems have been developed (e.g., see [3] for a summary of planners that competed in the International Conference on Artificial Intelligent Planning and Scheduling). These developments can be attributed to the discovery of good domain-independent heuristics, the use of domain-specific knowledge, and the development of efficient data structures used in the implementation of the planning algorithms. Logic programming has played a significant role in this line of research, providing a declarative framework for the encoding of different forms of knowledge and its effective use during the planning process [5]. However, relatively limited effort has been placed on addressing several important aspects in real-world planning domains, such as plan quality and preferences about plans. In many real world situations, the space of feasible plans to achieve the goal is dense, but many of such plans, even if executable, may present undesirable behavior",Adding Preferences to Answer Set Planning,,,,,core
20727288,2008-04-03,"Model-based reasoning is a powerful method for performing system monitoring and diagnosis. Building models for model-based reasoning is often a difficult and time consuming process. The Inductive Monitoring System (IMS) software was developed to provide a technique to automatically produce health monitoring knowledge bases for systems that are either difficult to model (simulate) with a computer or which require computer models that are too complex to use for real time monitoring. IMS processes nominal data sets collected either directly from the system or from simulations to build a knowledge base that can be used to detect anomalous behavior in the system. Machine learning and data mining techniques are used to characterize typical system behavior by extracting general classes of nominal data from archived data sets. In particular, a clustering algorithm forms groups of nominal values for sets of related parameters. This establishes constraints on those parameter values that should hold during nominal operation. During monitoring, IMS provides a statistically weighted measure of the deviation of current system behavior from the established normal baseline. If the deviation increases beyond the expected level, an anomaly is suspected, prompting further investigation by an operator or automated system. IMS has shown potential to be an effective, low cost technique to produce system monitoring capability for a variety of applications. We describe the training and system health monitoring techniques of IMS. We also present the application of IMS to a data set from the Space Shuttle Columbia STS-107 flight. IMS was able to detect an anomaly in the launch telemetry shortly after a foam impact damaged Columbia’s thermal protection system",INDUCTIVE SYSTEM HEALTH MONITORING WITH STATISTICAL METRICS __________________________________,,,,,core
147933442,2007-12-13T13:42:30,"Embedded systems play an increasing role in our society. From consumer electronics to driving assistance in cars, from medical devices to space exploration, they are ubiquitous. To better assist human beings, embedded systems become more complex, and more intelligent or even autonomous behaviors are expected. To handle their interaction with the real-world, those systems rely on artificial intelligence methods but are also subject to real-time constraints imposed by their environment. Taking advantage of increasingly powerful embedded processors, most of the new intelligent functionalities are implemented in software. As a result, the complexity of embedded software increases dramatically and can no longer be managed by domain specialists that are neither computer scientists nor software engineers. Unfortunately, the computer science community has left aside embedded systems for many years, and most modern software engineering paradigms cannot be applied directly to develop embedded applications. In this work, we present a hybrid framework based on software components that embed value-adding contributions from domain specialists. The framework relies on a dual-kernel approach that executes both a real-time and a best-effort operating system on a single processor. It is centered on dual-portability of software components across hardware platforms and across execution modes. The hybrid framework empowers domain specialists and promotes reuse of valuable software assets. It also supports the generation of hybrid embedded applications that combine real-time and best-effort software components. Such applications can take advantage of both time determinism and high computational throughput to face uncertain and dynamic real-world environments in an intelligent manner. The application of the hybrid framework is illustrated through real-world case studies with autonomous mobile robots from ASL (Autonomous Systems Laboratory) and NASA (National Aeronautics and Space Administration). It is also adapted to support the development of integrated satellite payload controllers for ESA (European Space Agency)",A component-based software framework for hybrid real-time and best-effort embedded applications,,"Lausanne, EPFL",10.5075/epfl-thesis-4023,,core
26006,2009-12-01T00:00:00,"Increasingly, complex real-world problems (including distributed sensing, air-traffic control, disaster response, network routing, space exploration and unmanned aerial vehicles) are being tackled by teams of software agents, rather than the more traditional centralised systems. Whilst this approach has many benefits in terms of creating robust solutions, it creates a new challenge — how to flexibly coordinate the actions of the agent teams to solve the problem efficiently. In more detail, coordination is here viewed as the problem of managing the interactions of these autonomous entities so that they do not disrupt each other, can take proactive actions to help each other, and take multiple actions at the same time when this is required to solve the problem.In this context, communication underpins most solutions to the coordination problem. That is, if the agents communicate their state and intentions to each other then they can coordinate their actions. Unfortunately, however, in many real-world problems, communication is a scarce resource. Specifically, communication has limited bandwidth, is not always available and may be expensive to utilise. In such circumstances, typical coordination mechanisms break down because the agents can no longer accurately model the state of the other agents. Given this, in this thesis, we consider how to coordinate when communication is a restricted resource. Specifically, we argue for a rational approach to communication. Since communication has a cost then, similarly, we should be able to calculate a value of sending any given communication. Once we have these costs and values, we can use standard decision theoretic models to choose whether to send a communication, and in fact, generate a plan which utilises communications and other actions efficiently.In this research we explore ways to value communications in several contexts. Within the framework of decentralised Partially Observable Markov Decision Process (POMDP) we develop a simple information theoretic valuation function (based on Kullback–Leibler (KL) Divergence). This techniques allows agents to coordinate in large problems such as RoboCupRescue, where teams of ambulances must save as many civilians as possible after an earthquake. We found that, in this task, valuing communications before deciding whether to send them results in a level of performance which is higher than not communicating, and close to a model which utilises a free communication medium to communicate all the time. Furthermore, this model is robust to increasing communication restrictions, whereas simple communication policies are not.We then extend this framework to value communications based on a technique from the field of Machine Learning, namely Reward Shaping, which allows the decentralised POMDP to be transformed into individual agent POMDPs that can be solved more easily. This approach can use a heuristic transformation to allow the approach to work in large problems like RobocupRescue or the Multi–Agent Tiger problem, where it outperforms the current state of the art. Further to this, this approach can also use an exact reward shaping function in order to generate a bounded approximation of the intractable optimal decentralised solution in slightly smaller problems.Finally, we show how, if we restrict our attention to relatively static (i.e. the problem does not change without an agent doing something) problems than those which the reward shaping technique was designed for, we can generate an optimal solution to decentralised control based on communication valuations. In more detail, we extend the class of Bayesian coordination games to include explicit observation and communication actions. By so doing, the value of observation and exchange can be derived using the concept of opportunity costs. This is a natural way of measuring the relationship between communication and information gathering on an agent’s utility, and removes the need to introduce arbitrary penalties for communicating (which is what most existing approaches do). Furthermore, this approach allows us to show that the optimal communication policy is a Nash equilibrium, and to exploit the fact that there exist many efficient algorithms for finding such equilibria in a local fashion. Specifically, we provide a complete analysis of this model for two–state problems, and illustrate how the analysis can be carried out for larger domains making use of explicit information gathering strategies. Finally, we develop a procedure for finding the optimal communication and search policy as a function of the partial observability of the state and payoffs of the underlying game (which we demonstrate in the canonical Multi–Agent Tiger problem).In performing all of this work, we demonstrate how communication can be managed locally by accurately placing a value on the cost and benefit of using a restricted communication resource. This allows agents to coordinate efficiently in many interesting problem domains, where existing approaches perform badly. We contribute to the field of rational communication by providing several algorithms for utilising costly communication under different domain conditions. Our reward shaping approaches are highly scalable in problems with large state spaces and come with sound theoretical guarantees on the optimality of the solution they find",Rational communication for the coordination of multi-Agent systems,,,,,core
23519445,2008,"Abstract—Motivated by finding reduced complexity versions of the maximum-likelihood (ML) detector for highly distorted underwater channels, a multiuser detection (MUD) algorithm for joint data detection and channel estimation based on the cyclic coordinate descent method is proposed. Assuming that the data symbols are available, they are used to estimate the channel responses, which, in turn, are used to refine the symbol estimates. Adaptive estimation is performed using minimum mean square error as the overall optimization criterion. The receiver is implemented in a multichannel configuration, which provides the array processing gain necessary for many of the underwater acoustic channels. The complexity of the detection algorithm is linear in the number of receive elements and it does not depend on the modulation level of the transmitted signals. The algorithm has been tested using real data obtained over a 2-km shallow-water channel in a 20-kHz band, demonstrating good results. Index Terms—Adaptive algorithms, code-division multiple access, direct-sequence (DS) spread spectrum, multichannel combining, space-time processing, underwater acoustic communications. I",Efficient Channel-Estimation-Based Multiuser Detection for Underwater CDMA System,,,,,core
35461179,2005-01-01T00:00:00,"This paper describes a model of design, which takes a different perspective in that it emphasises mechanisms and constraints. The model accommodates design-related activities from the enterprise level through to activities of individuals (e.g. decision-making processes), and provides a holistic treatment of existing design methods. It is suggested that the output of a design stage is not so much the input to the next, as the provider of constraints. Consequently, the creative component of design is not concentrated in one task, with the others being technician tasks: instead each of the tasks augments the design in a creative way. A degree of commonality was observed across different domains and stages of design, which leads to the proposal of a generic design activity (GDA), that can be used in diverse design situations. Sub-activities within the GDA were identified as the generation of candidate solutions, solution assessment, solution selection, implementation, and retrieval of design intent. It is suggested that one of the limitations of many design tools, especially artificial intelligence, is the reliance on on complete problem and constraint specification. In real situations, designers have to determine constraints from incomplete and qualitative specifications, using subjective processes. Furthermore, they subsequently have to negotiate with others for the relaxation of constraints, as the design space may be over-constrained. This negotiation involves interaction with others, and adds the organisational behaviour factors to the design process. Decision-making during design needs to be able to accommodate multiple viewpoints, cope with uncertainty of analysis (incompleteness of knowledge), propagate uncertain variables, and accommodate varying degrees of information abstraction. Other areas of design that may benefit for additional research are identified",Design mechanisms and constraints,,'Springer Science and Business Media LLC',10.1007/s00163-005-0008-9,,core
62874453,2006-01-01T08:00:00,"Integrating human knowledge with modeling tools, an intelligent decision support system (DSS) is developed to assist decision makers during different phases of flood management. The DSS is developed as a virtual planning tool and can address both engineering and non-engineering issues related to flood management. Different models (hydrodynamic, forecasting, and economic) that are part of the DSS share data and communicate with each other by providing feedback. The DSS is able to assist in: selecting suitable flood damage reduction options (using an expert system approach); forecasting floods (using artificial neural networks approach); modeling the operation of flood control structures; and describing the impacts (area flooded and damage) of floods in time and space. The proposed DSS is implemented for the Red River Basin in Manitoba, Canada. The results from the test application of DSS for 1997 flood in the Red River Basin are very promising. The DSS is able to predict the peak flows with 2% error and reveals that with revised operating rules the contribution of Assiniboine River to the flooding of Winnipeg city can be significantly reduced. The decision support environment allows a number of “what-if” type questions to be asked and answered, thus, multiple decisions can be tried without having to deal with the real life consequences",An intelligent decision support system for management of floods,https://digitalscholarship.unlv.edu/fac_articles/108,Digital Scholarship@UNLV,10.1007/s11269-006-0326-3,,core
24739682,2008-04-02,"For some supervised learning tasks, researchers can control the data generation process. In such cases, it would be beneficial to have feedback during learning to guide future data collection. Our research is motivated by a real-world problem: discrimination of vapors with an “artificial nose”. The nose’s accuracy is vital, because it will be deployed to detect harmful gases in critical situations, such as an airport or a subway. We address how to improve accuracy if insufficient examples have been observed to accurately define the class’s decision boundaries. This problem differs from situations in which active learning is applicable. Active learning either requests labels for existing data or explicitly queries the feature space. In contrast, our task allows us to ask for additional examples from specific classes. In this paper we propose an adaptive heuristic to identify from which classes instances should be added during the learning process. We evaluate our methods on the artificial nose data and show significant improvement over random sampling. ",Abstract,,,,,core
291707018,2007-04,"As computer architectures become more complex, the task of writing efficient program to best utilize the underlying architecture's power increasingly becomes an extremely difficult and expensive process. Traditional approach of expert manual tuning of software performance becomes infeasible as both software and hardware complexity grow. To make things even worse, the relative cost of man labor compared with that of machine computation increases rapidly. One approach to attacking the problem is automatic library generation via empirical evaluation. The essential idea is to have a meta-program automatically generate other high performance program via empirical evaluation and intelligent search. The methodology has been successfully applied in several application domains, such as numerical computing, signal processing, sorting, etc.

This dissertation extends the automatic library generation methodology to emerging untraditional computer architectures and to a more complex application domain. Specifically, it consists of two parts of work: First, it studies and implements an automatic matrix multiply library generator for graphics hardware -- a specialized architecture with enormous computing power for graphics applications; Second, it uses machine learning techniques to automatically select the best algorithm for frequent pattern mining problems according to input characteristics.

In order to utilize the tremendous computing power of graphics hardware and to automatically adapt to the fast and frequent changes in its architecture and performance characteristics, this dissertation implements an automatic tuning system to generate high-performance matrix-multiplication implementation on graphics hardware. The automatic tuning system uses a parameterized code generator to generate multiple versions of matrix multiplication, whose performances are empirically evaluated by actual execution on the target platform. An ad-hoc search engine is employed to search over the implementation space for the version that yields the best performance. In contrast to similar systems on CPUs, which utilize cache blocking, register tiling, instruction scheduling tuning strategies, it identifies and exploits several tuning strategies that are unique for graphics hardware. These tuning strategies include optimizing for multiple-render-targets, SIMD instructions with data packing, overcoming limitations on instruction count and dynamic branch instruction. The generated implementations have comparable performance with expert manually tuned version in spite of the significant overhead incurred due to the use of the high-level BrookGPU language.

Frequent pattern mining is a fundamental problem in data mining and a large number of distinct algorithms have been proposed to solve it efficiently. However, no single algorithm outperforms all the others since their relative performance highly depends on the characteristics of the input data. In the dissertation, we present a machine learning based approach to select the best frequent pattern mining algorithm based on the input characteristics. Three of the fastest publicly available algorithms, FP\_Growth, LCM and Eclat, were extensively evaluated using synthetic data sets. The results of these evaluations were used to train a support-vector machine (SVM) prediction system, which is then used at runtime to predict the best mining algorithm for real-world data sets. Our experiments show that the runtime prediction overhead is negligible and that the trained SVM prediction system usually identifies the best algorithm. In case of misprediction, the selected algorithm is still competitive in performance",Automatic Software Performance Optimization on Modern Architectures,,,,,core
2575272,2004-01-14T00:00:00,"A wireless communication system using multiple antennas promises reliable
transmission under Rayleigh flat fading assumptions. Design criteria and
practical schemes have been presented for both coherent and non-coherent
communication channels. In this paper we generalize one dimensional phase shift
keying (PSK) signals and introduce space time constellations from generalized
phase shift keying (GPSK) signals based on the complex and real orthogonal
designs. The resulting space time constellations reallocate the energy for each
transmitting antenna and feature good diversity products, consequently their
performances are better than some of the existing comparable codes. Moreover
since the maximum likelihood (ML) decoding of our proposed codes can be
decomposed to one dimensional PSK signal demodulation, the ML decoding of our
codes can be implemented in a very efficient way.Comment: 22 pages, 3 figures, submitted to IEEE transactions on communicaton",Generalized PSK in Space Time Coding,http://arxiv.org/abs/math/0401157,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TCOMM.2005.847166,,core
11589081,2009-01-01T00:00:00,"Durch die steigende Anzahl verfügbarer Daten in unterschiedlichsten Anwendungsgebieten nimmt der Aufwand vieler Data-Mining Applikationen signifikant zu.  Speziell hochdimensionierte Daten (Daten die über viele verschiedene Attribute beschrieben werden) können ein großes Problem für viele Data-Mining Anwendungen darstellen. Neben höheren Laufzeiten können dadurch sowohl für überwachte (supervised), als auch nicht überwachte (unsupervised) Klassifikationsalgorithmen weitere Komplikationen  entstehen (z.B. ungenaue Klassifikationsgenauigkeit, schlechte Clustering-Eigenschaften, …). Dies führt zu einem Bedarf an effektiven und effizienten Methoden zur Dimensionsreduzierung. 

Feature Selection (die Auswahl eines Subsets von Originalattributen) und Dimensionality Reduction (Transformation von Originalattribute in (Linear)-Kombinationen der Originalattribute) sind zwei wichtige Methoden um die Dimension von Daten zu reduzieren. Obwohl sich in den letzten Jahren vielen Studien mit diesen Methoden beschäftigt haben, gibt es immer noch viele offene Fragestellungen in diesem Forschungsgebiet. Darüber hinaus ergeben sich in vielen Anwendungsbereichen durch die immer weiter steigende Anzahl an verfügbaren und verwendeten Attributen und Features laufend neue Probleme. Das Ziel dieser Dissertation ist es, verschiedene Fragenstellungen in diesem Bereich genau zu analysieren und Verbesserungsmöglichkeiten zu entwickeln.



Grundsätzlich, werden folgende Ansprüche an Methoden zur Feature Selection und Dimensionality Reduction gestellt: Die Methoden sollten effizient (bezüglich ihres Rechenaufwandes) sein und die resultierenden Feature-Sets sollten die Originaldaten möglichst kompakt repräsentieren können. Darüber hinaus ist es in vielen Anwendungsgebieten wichtig, die Interpretierbarkeit der Originaldaten beizubehalten. Letztendlich sollte der Prozess der Dimensionsreduzierung keinen negativen Effekt auf die Klassifikationsgenauigkeit haben - sondern idealerweise, diese noch verbessern. 



Offene Problemstellungen in diesem Bereich betreffen unter anderem den Zusammenhang zwischen Methoden zur Dimensionsreduzierung und der resultierenden Klassifikationsgenauigkeit, wobei sowohl eine möglichst kompakte Repräsentation der Daten, als auch eine hohe Klassifikationsgenauigkeit erzielt werden sollen. Wie bereits erwähnt, ergibt sich durch die große Anzahl an Daten auch ein erhöhter Rechenaufwand, weshalb schnelle und effektive Methoden zur Dimensionsreduzierung entwickelt werden müssen, bzw. existierende Methoden verbessert werden müssen. Darüber hinaus sollte natürlich auch der Rechenaufwand der verwendeten Klassifikationsmethoden möglichst gering sein. Des Weiteren ist die Interpretierbarkeit von Feature Sets zwar möglich, wenn Feature Selection Methoden für die Dimensionsreduzierung verwendet werden, im Fall von Dimensionality Reduction sind die resultierenden Feature Sets jedoch meist Linearkombinationen der Originalfeatures. Daher ist es schwierig zu überprüfen, wie viel Information einzelne Originalfeatures beitragen. 



Im Rahmen dieser Dissertation konnten wichtige Beiträge zu den oben genannten Problemstellungen präsentiert werden: Es wurden neue, effiziente Initialisierungsvarianten für die Dimensionality Reduction Methode Nonnegative Matrix Factorization (NMF) entwickelt, welche im Vergleich zu randomisierter Initialisierung und im Vergleich zu State-of-the-Art Initialisierungsmethoden zu einer schnelleren Reduktion des Approximationsfehlers führen. Diese Initialisierungsvarianten können darüber hinaus mit neu entwickelten und sehr effektiven Klassifikationsalgorithmen basierend auf NMF kombiniert werden. Um die Laufzeit von NMF weiter zu steigern wurden unterschiedliche Varianten von NMF Algorithmen auf Multi-Prozessor Systemen vorgestellt, welche sowohl Task- als auch Datenparallelismus unterstützen und zu einer erheblichen Reduktion der Laufzeit für NMF führen. Außerdem wurde eine effektive Verbesserung der Matlab Implementierung des ALS Algorithmus vorgestellt. Darüber hinaus wurde eine Technik aus dem Bereich des Information Retrieval -- Latent Semantic Indexing -- erfolgreich als Klassifikationsalgorithmus für Email Daten angewendet. Schließlich wurde eine ausführliche empirische Studie über den Zusammenhang verschiedener Feature Reduction Methoden (Feature Selection und Dimensionality Reduction) und der resultierenden Klassifikationsgenauigkeit unterschiedlicher Lernalgorithmen präsentiert. 



Der starke Einfluss unterschiedlicher Methoden zur Dimensionsreduzierung auf die resultierende Klassifikationsgenauigkeit unterstreicht dass noch weitere Untersuchungen notwendig sind um das komplexe Zusammenspiel von Dimensionsreduzierung und Klassifikation genau analysieren zu können.The sheer volume of data today and its expected growth over the next years are some of the key challenges in data mining and knowledge discovery applications. Besides the huge number of data samples that are collected and processed, the high dimensional nature of data arising in many applications causes the need to develop effective and efficient techniques that are able to deal with this massive amount of data. In addition to the significant increase in the demand of computational resources, those large datasets might also influence the quality of several data mining applications (especially if the number of features is very high compared to the number of samples). As the dimensionality of data increases, many types of data analysis and classification problems become significantly harder. This can lead to problems for both supervised and unsupervised learning. Dimensionality reduction and feature (subset) selection methods are two types of techniques for reducing the attribute space. While in feature selection a subset of the original attributes is extracted, dimensionality reduction in general produces linear combinations of the original attribute set. In both approaches, the goal is to select a low dimensional subset of the attribute space that covers most of the information of the original data. During the last years, feature selection and dimensionality reduction techniques have become a real prerequisite for data mining applications. 



There are several open questions in this research field, and due to the often increasing number of candidate features for various application areas (e.\,g., email filtering or drug classification/molecular modeling) new questions arise. In this thesis, we focus on some open research questions in this context, such as the relationship between feature reduction techniques and the resulting classification accuracy and the relationship between the variability captured in the linear combinations of dimensionality reduction techniques (e.\,g., PCA, SVD) and the accuracy of machine learning algorithms operating on them.  Another important goal is to better understand new techniques for dimensionality reduction, such as nonnegative matrix factorization (NMF), which can be applied for finding parts-based, linear representations of nonnegative data.  This ``sum-of-parts'' representation is especially useful if the interpretability of the original data should be retained. Moreover, performance aspects of feature reduction algorithms are investigated. As data grow, implementations of feature selection and dimensionality reduction techniques for high-performance parallel and distributed computing environments become more and more important. 



In this thesis, we focus on two types of open research questions: methodological advances without any specific application context, and application-driven advances for a specific application context. Summarizing, new methodological contributions are the following: The utilization of nonnegative matrix factorization in the context of classification methods is investigated.  In particular, it is of interest how the improved interpretability of NMF factors due to the non-negativity constraints (which is of central importance in various problem settings) can be exploited. Motivated by this problem context two new fast initialization techniques for NMF based on feature selection are introduced. It is shown how approximation accuracy can be increased and/or how computational effort can be reduced compared to standard randomized seeding of the NMF and to state-of-the-art initialization strategies suggested earlier. For example, for a given number of iterations and a required approximation error a speedup of 3.6 compared to standard initialization, and a speedup of 3.4 compared to state-of-the-art initialization strategies could be achieved. Beyond that, novel classification methods based on the NMF are proposed and investigated. We can show that they are not only competitive in terms of classification accuracy with state-of-the-art classifiers, but also provide important advantages in terms of computational effort (especially for low-rank approximations). Moreover, parallelization and distributed execution of NMF is investigated. Several algorithmic variants for efficiently computing NMF on multi-core systems are studied and compared to each other. In particular, several approaches for exploiting task and/or data-parallelism in NMF are studied. We show that for some scenarios new algorithmic variants clearly outperform existing implementations. Last, but not least, a computationally very efficient adaptation of the implementation of the ALS algorithm in Matlab 2009a is investigated. This variant reduces the runtime significantly (in some settings by a factor of 8) and also provides several possibilities to be executed concurrently.



In addition to purely methodological questions, we also address questions arising in the adaptation of feature selection and classification methods to two specific application problems: email classification and in silico screening for drug discovery. Different research challenges arise in the contexts of these different application areas, such as the dynamic nature of data for email classification problems, or the imbalance in the number of available samples of different classes for drug discovery problems. Application-driven advances of this thesis comprise the adaptation and application of latent semantic indexing (LSI) to the task of email filtering. Experimental results show that LSI achieves significantly better classification results than the widespread de-facto standard method for this special application context. In the context of drug discovery problems, several groups of well discriminating descriptors could be identified by utilizing the ``sum-of-parts`` representation of NMF. The number of important descriptors could be further increased when applying sparseness constraints on the NMF factors",Efficient feature reduction and classification methods,https://core.ac.uk/download/11589081.pdf,,,,core
55463,2009-01-01T00:00:00,"A major issue for reinforcement learning (RL) applied to robotics is the time required to learn a new skill. While RL has been used to learn mobile robot control in many simulated domains, applications involving learning on real

robots are still relatively rare. In this paper, the Least-Squares Policy Iteration (LSPI) reinforcement learning algorithm and a new model-based algorithm Least-Squares Policy Iteration with Prioritized Sweeping (LSPI+), are implemented on a mobile robot to acquire new skills quickly and efficiently. LSPI+ combines the benefits of LSPI and prioritized sweeping, which uses all previous experience to focus the computational effort on the most “interesting” or dynamic parts of the state space. 

The proposed algorithms are tested on a household vacuum

cleaner robot for learning a docking task using vision as the only sensor modality. In experiments these algorithms are compared to other model-based and model-free RL algorithms. The results show that the number of trials required to learn the docking task is significantly reduced using LSPI compared to the other RL algorithms investigated, and that LSPI+ further improves on the performance of LSPI","Vision-based reinforcement learning using approximate policy

iteration",https://core.ac.uk/download/55463.pdf,,,,core
29456730,2006-11-20T00:00:00,"This article presents a soft-computing-based data mining technique that addresses methodology aspects on extracting classification rules characterized by ellipsoidal regions in feature space. Self-organizing mapping and statistical techniques are employed to initialize the rules. A regularization model embedding some information on recognition rate and generalization ability is presented for refining the initial rules. Rule optimization is implemented for each individual rule using an evolutionary strategy. To generate rules for patterns with low probability of occurrence but considerable conceptual importance, a multilayer structure of rule generation and use is proposed. Simulation results are carried out by three benchmark data sets, and compared with other data mining tools and classifiers, such as decision trees, BRAINNE (Building Representations of Artificial Intelligence (AI) using Neural Networks), support vector machine, and neural networks. Our technique demonstrates its power and potential for real-world applications. © 2006, Taylor & Francis Group, LLC. All rights reserved",Extraction of classification rules characterized by ellipsoidal regions using soft-computing techniques,http://hdl.handle.net/10453/4120,'Informa UK Limited',10.1080/00207720600891489,"[{'title': 'International Journal of Systems Science', 'identifiers': ['issn:0020-7721', '0020-7721']}]",core
4820557,2007-04-01T00:00:00,"As computer architectures become more complex, the task of writing efficient program to best utilize the underlying architecture's power increasingly becomes an extremely difficult and expensive process. Traditional approach of expert manual tuning of software performance becomes infeasible as both software and hardware complexity grow. To make things even worse, the relative cost of man labor compared with that of machine computation increases rapidly. One approach to attacking the problem is automatic library generation via empirical evaluation. The essential idea is to have a meta-program automatically generate other high performance program via empirical evaluation and intelligent search. The methodology has been successfully applied in several application domains, such as numerical computing, signal processing, sorting, etc.

This dissertation extends the automatic library generation methodology to emerging untraditional computer architectures and to a more complex application domain. Specifically, it consists of two parts of work: First, it studies and implements an automatic matrix multiply library generator for graphics hardware -- a specialized architecture with enormous computing power for graphics applications; Second, it uses machine learning techniques to automatically select the best algorithm for frequent pattern mining problems according to input characteristics.

In order to utilize the tremendous computing power of graphics hardware and to automatically adapt to the fast and frequent changes in its architecture and performance characteristics, this dissertation implements an automatic tuning system to generate high-performance matrix-multiplication implementation on graphics hardware. The automatic tuning system uses a parameterized code generator to generate multiple versions of matrix multiplication, whose performances are empirically evaluated by actual execution on the target platform. An ad-hoc search engine is employed to search over the implementation space for the version that yields the best performance. In contrast to similar systems on CPUs, which utilize cache blocking, register tiling, instruction scheduling tuning strategies, it identifies and exploits several tuning strategies that are unique for graphics hardware. These tuning strategies include optimizing for multiple-render-targets, SIMD instructions with data packing, overcoming limitations on instruction count and dynamic branch instruction. The generated implementations have comparable performance with expert manually tuned version in spite of the significant overhead incurred due to the use of the high-level BrookGPU language.

Frequent pattern mining is a fundamental problem in data mining and a large number of distinct algorithms have been proposed to solve it efficiently. However, no single algorithm outperforms all the others since their relative performance highly depends on the characteristics of the input data. In the dissertation, we present a machine learning based approach to select the best frequent pattern mining algorithm based on the input characteristics. Three of the fastest publicly available algorithms, FP\_Growth, LCM and Eclat, were extensively evaluated using synthetic data sets. The results of these evaluations were used to train a support-vector machine (SVM) prediction system, which is then used at runtime to predict the best mining algorithm for real-world data sets. Our experiments show that the runtime prediction overhead is negligible and that the trained SVM prediction system usually identifies the best algorithm. In case of misprediction, the selected algorithm is still competitive in performance",Automatic Software Performance Optimization on Modern Architectures,https://core.ac.uk/download/4820557.pdf,,,,core
10563765,November 2004,"Artificial neural networks that would utilize the cascade error projection (CEP) algorithm have been proposed as means of autonomous, real-time, adaptive color segmentation of images that change with time. In the original intended application, such a neural network would be used to analyze digitized color video images of terrain on a remote planet as viewed from an uninhabited spacecraft approaching the planet. During descent toward the surface of the planet, information on the segmentation of the images into differently colored areas would be updated adaptively in real time to capture changes in contrast, brightness, and resolution, all in an effort to identify a safe and scientifically productive landing site and provide control feedback to steer the spacecraft toward that site. Potential terrestrial applications include monitoring images of crops to detect insect invasions and monitoring of buildings and other facilities to detect intruders. The CEP algorithm is reliable and is well suited to implementation in very-large-scale integrated (VLSI) circuitry. It was chosen over other neural-network learning algorithms because it is better suited to realtime learning: It provides a self-evolving neural-network structure, requires fewer iterations to converge and is more tolerant to low resolution (that is, fewer bits) in the quantization of neural-network synaptic weights. Consequently, a CEP neural network learns relatively quickly, and the circuitry needed to implement it is relatively simple. Like other neural networks, a CEP neural network includes an input layer, hidden units, and output units (see figure). As in other neural networks, a CEP network is presented with a succession of input training patterns, giving rise to a set of outputs that are compared with the desired outputs. Also as in other neural networks, the synaptic weights are updated iteratively in an effort to bring the outputs closer to target values. A distinctive feature of the CEP neural network and algorithm is that each update of synaptic weights takes place in conjunction with the addition of another hidden unit, which then remains in place as still other hidden units are added on subsequent iterations. For a given training pattern, the synaptic weight between (1) the inputs and the previously added hidden units and (2) the newly added hidden unit is updated by an amount proportional to the partial derivative of a quadratic error function with respect to the synaptic weight. The synaptic weight between the newly added hidden unit and each output unit is given by a more complex function that involves the errors between the outputs and their target values, the transfer functions (hyperbolic tangents) of the neural units, and the derivatives of the transfer functions",Real-Time Adaptive Color Segmentation by Neural Networks,https://core.ac.uk/download/pdf/10563765.pdf,,,,core
21740493,2009,"Two production models are candidates for e-science computing: grids enable hardware and software sharing; clouds propose dynamic resource provisioning (elastic computing). Organized sharing is a fundamental requirement for large scientific collaborations; responsiveness, the ability to provide good response time, is a fundamental requirement for seamless integration of the large scale computing resources into everyday use. This paper focuses on a model-free resource provisioning strategy supporting both scenarios. The provisioning problem is modeled as a continuous action-state space, multi-objective reinforcement learning problem, under realistic hypotheses; the high level goals of users, administrators, and shareholders are captured through simple utility functions. We propose an implementation of this reinforcement learning framework, including an approximation of the value function through an Echo State Network, and we validate it on a real dataset",Responsive Elastic Computing,,,,,core
24323934,2007-11-22,"While many issues in the area of virtual reality (VR) research have been addressed in recent years, the constant leaps forward in technology continue to push the field forward. VR research no longer is focused only on computer graphics, but instead has become even more interdisciplinary, combining the fields of networking, distributed computing, and even artificial intelligence. In this article we discuss some of the issues associated with distributed, collaborative virtual reality, as well as lessons learned during the development of two distributed virtual reality applications. 1 Introduction The Futures Laboratory at Argonne National Laboratory has been exploring what is needed to support large-scale shared space virtual environments (VE) for wide-area collaborations. Our research has focused on the system architecture, software design, and features needed to implement such environments. In this article we discuss two prototype systems under development at Argonne. Shared virtual sp..",Two Implementations of Shared Virtual Space Environments,,,,,core
24671626,2008-04-01,"This project is to implement a 2D face recognition algorithm proposed in [2], which models the density of intrapersonal and extrapersonal face space separately with a single Gaussian for each, and thus uses Bayesian theory to do classification. It includes both maximum a posteriori (MAP) and maximum likelihood (ML) decision. Besides, we will try two improvements: one is to use Gaussian Mixture Model for density modelling since there will be multiple modes in intrapersonal face space, and the other one is to use Gabor feature jets instead of pixel intensity in face representation. The traditional eigenfaces [1] method is also implemented as a base line for comparison. The experiments will be carried on the ORL database containing 40 subjects and each one has 10 images under different lighting, pose, facial expression, and facial details. All the recognition algorithms are evaluated by the Cumulative Rank Curve. I. LECTURE REVIEW 2D face recognition has been a popular and challenging research area since last decade. It arises general interests in computer vision, image analysis, psychology, etc. The problem can be formulated as that: given a set of face images with labelled identity (the gallery) and a set of unlabelled face images (the probe), identify the person in the probe images. Many works have been proposed for this problem till now. Below we briefly introduce the linear space methods, including eigenfaces and fisherfaces, and the method based on Bayesian theory on intrapersonal and extrapersonal classes. These methods are proved to be generally effective for many real applications [4]. A. Linear Methods In 1991, Turk and Pentland [1] proposed “eigenfaces ” method. They generate “eigenfaces” from the training face images by principal component analysis (PCA), and classify the ne",Biometrics Project: Bayesian Face Recognition,,,,,core
20727604,2008-04-03,"Abstract—Intelligent systems based on machine learning techniques, such as classification, clustering, are gaining wide spread popularity in real world applications. This paper presents work on developing a software system for predicting crop yield, for example oil-palm yield, from climate and plantation data. At the core of our system is a method for unsupervised partitioning of data for finding spatio-temporal patterns in climate data using kernel methods which offer strength to deal with complex data. This work gets inspiration from the notion that a non-linear data transformation into some high dimensional feature space increases the possibility of linear separability of the patterns in the transformed space. Therefore, it simplifies exploration of the associated structure in the data. Kernel methods implicitly perform a non-linear mapping of the input data into a high dimensional feature space by replacing the inner products with an appropriate positive definite function. In this paper we present a robust weighted kernel k-means algorithm incorporating spatial constraints for clustering the data. The proposed algorithm can effectively handle noise, outliers and auto-correlation in the spatial data, for effective and efficient data analysis by exploring patterns and structures in the data, and thus can be used for predicting oil-palm yield by analyzing various factors affecting the yield. Keywords—Pattern analysis, clustering, kernel methods, spatial data, crop yield I",A Framework for Predicting Oil-Palm Yield from Climate Data,,,,,core
21786625,2007-11-21,"Reinforcement learning algorithms without an internal world model often suer from overly long time to converge. Mostly the agent has to be successful a several hundred times before it could learn how to behave in even simple environments. In this case, a world model could be useful to reduce the number of real world trials by performing the action virtually in the world model. This may help to propagate the Reinforcement Q- or V- values much faster through the state (action) space and could be interpreted as a simple form of planning. In the following investigation we introduce a self organizing deterministic world model &quot;DIVA&quot; (&quot;Discretization Improvement by VAriancereduction&quot;) with an adaptive discretization, which can speed up learning by using common methods like Suttons Dyna-Q. Proposed in this article, the &quot;DIVA&quot;-model is implemented in a six legged walking robot, which learns how to walk in a minimum of time and with a minimum of real world moving trials.  ",DIVA: A Self Organizing Adaptive World Model for Reinforcement Learning,,,,,core
24415204,2003,"Supermarkets lose millions of pounds every year through lost trading and stock wastage caused by the  failure of refrigerated cabinets. Therefore, a huge commercial market exists for artificially intelligent systems which  are able to detect the early symptoms of faults. Previous work in this vein, using real-world data and now in the throes  of being deployed commercially, has employed evolved neural networks to predict volumes of temperature and other  alarms emerging from refrigeration system controllers, and also to predict likely refrigerant gas loss from such alarm  patterns. In this work we turn to the use of in-cabinet temperature data which has recently become available, and the  aim is to predict refrigeration system faults from the pattern of in-cabinet temperature over time. We argue that  artificial immune system inspired technologies are particularly appropriate for this task. The negative selection  algorithm is therefore investigated as a tool to detect anomalous patterns in temperature data. Using a simple AIS  system in preliminary experiments to assess feasibility, we compare the performance of a simple matching rule based  on Euclidean distance and one using traditional r contiguous bits matching. We also investigate a `differential&apos;  encoding scheme which reduces the size of pattern space while retaining what we feel to be the essential elements of  patterns in this application. We find that feasibility for AIS in this application is proven, with particularly good selfdetection  rates, and a fault-detection rate adequate as a springboard for further development. The best AIS  configuration of those examined here seems to be that which uses the novel differential encoding in conjunction with  the r-bits matching rule. The differential encoding sche..",An Investigation of the Negative Selection Algorithm for Fault Detection in Refrigeration Systems,,Springer,10.1007/978-3-540-45192-1_4,,core
224453777,2008-01-01T00:00:00,"Recently, advances in sensing and sensing methodologies have led to the deployment of multiple sensor arrays on structures for structural health monitoring (SHM) applications. Appropriate feature extraction, detection, and classification methods based on measurements obtained from these sensor networks are vital to the SHM paradigm. This dissertation focuses on a multi-input/multi-output approach to novel data processing procedures to produce detailed information about the integrity of a structure in near real-time. The studies employ nonlinear time series analysis techniques to extract three different types of features for damage diagnostics: namely, nonlinear prediction error, transfer entropy, and the generalized interdependence. These features form reliable measures of generalized correlations between multiple measurements to capture aspects of the dynamics related to the presence of damage. Several analyses are conducted on each of these features. Specifically, variations of nonlinear prediction error are introduced, analyzed, and validated, including the use of a stochastic excitation to augment generality, introduction of local state-space models for sensitivity enhancement, and the employment of comparisons between multiple measurements for localization capability. A modification and enhancement to transfer entropy is created and validated for improved sensitivity. In addition, a thorough analysis of the effects of variability to transfer entropy estimation is made. The generalized interdependence is introduced into the literature and validated as an effective measure of damage presence, extent, and location. These features are validated on a multi-degree-of-freedom dynamic oscillator and several different frame experiments. The evaluated features are then fed into four different classification schemes to obtain a concurrent set of outputs that categorize the integrity of the structure, e.g. the presence, extent, location, and type of damage, taking advantage of the capabilities of these features to extract damage-related information. First, a multivariate outlier and localization technique is established under an unsupervised learning assumption. Next, parallel and serial linear discriminant analysis-based classification algorithms are analyzed in a supervised learning paradigm. Finally, a back-propagation neural network is employed as an additional type of damage classifier. For both a simulated structural model and a tested frame experiment, these methods are shown to correctly classify the structure among a variety of different categories 85 to 99% of the time, depending on noise level",Time series analysis and feature extraction techniques for structural health monitoring applications,,"eScholarship, University of California",,,core
29139966,2006-01-01T00:00:00,"Today, combinatorial optimization is one of the youngest and most active areas of discrete mathematics. It is a branch of optimization in applied mathematics and computer science, related to operational research, algorithm theory and computational complexity theory. It sits at the intersection of several fields, including artificial intelligence, mathematics and software engineering. Its increasing interest arises for the fact that a large number of scientific and industrial problems can be formulated as abstract combinatorial optimization problems, through graphs and/or (integer) linear programs. Some of these problems have polynomial-time (“efficient”) algorithms, while most of them are NP-hard, i.e. it is not proved that they can be solved in polynomial-time. Mainly, it means that it is not possible to guarantee that an exact solution to the problem can be found and one has to settle for an approximate solution with known performance guarantees. Indeed, the goal of approximate methods is to find “quickly” (reasonable run-times), with “high” probability, provable “good” solutions (low error from the real optimal solution). In the last 20 years, a new kind of algorithm commonly called metaheuristics have emerged in this class, which basically try to combine heuristics in high level frameworks aimed at efficiently and effectively exploring the search space. This report briefly outlines the components, concepts, advantages and disadvantages of different metaheuristic approaches from a conceptual point of view, in order to analyze their similarities and differences. The two very significant forces of intensification and diversification, that mainly determine the behavior of a metaheuristic, will be pointed out. The report concludes by exploring the importance of hybridization and integration methods",Combinatorial optimization and metaheuristics,https://core.ac.uk/download/29139966.pdf,Brunel University,,,core
4751811,2004-03-01T00:00:00,"Presented at the AAAI Symposium on Accessible, Hands-on AI/Robotics Education, San Jose, CA, March 2004.The focus of Jet Propulsion Laboratory’s Office of Communication and Education is to engage the public through the implementation of innovative approaches such as informal and formal education methods, science outreach, and research development efforts [ 13. One of the direct outcomes of this emphasis is to create a bridge to provide students access to the research and development activities involved in space exploration, as well as to support and develop a pipeline program to encourage the next generation of engineers and scientists. Of special interest is to transfer robotics research knowledge derived from JPL efforts to the educational arena [2]. The robotics field represents the integration of multiple facets of engineering and science - from mechanical construction to intelligence programming to science data analysis. It is an ideal opportunity to showcase the relationship math and science have on tangible real-world applications. In addition, the robotics field incorporates non-traditional areas of study such as the social aspects of teamwork, system engineering, and hands-on experimentation methods. These represent only a small subset of the elements necessary for developing an integrated, intelligent robotic system for space exploration. To promote robotics education, there is a concrete process needed for transferring the knowledge derived from space robotics research to platforms capable of teaching individual artificial intelligence topics, providing step-by-step instructions on robotics techniques, or integrating hands-on learning into the curriculum. In this paper, we will discuss the AI Toolkit, a task carried out at NASA’s Jet Propulsion Laboratory (JPL) that focuses on bridging the gap between robotics research efforts and the classroom environment. The AI Toolkit enables training on artificial intelligence techniques through step-by-step instruction and hands-on exercises applied to the robotics arena",Bridging the Gap between Space Robotics Research and Robotics Education,,'American Institute of Aeronautics and Astronautics (AIAA)',,,core
10564967,October 2003,"Topics covered include: Cryogenic Temperature-Gradient Foam/Substrate Tensile Tester; Flight Test of an Intelligent Flight-Control System; Slat Heater Boxes for Thermal Vacuum Testing; System for Testing Thermal Insulation of Pipes; Electrical-Impedance-Based Ice-Thickness Gauges; Simulation System for Training in Laparoscopic Surgery; Flasher Powered by Photovoltaic Cells and Ultracapacitors; Improved Autoassociative Neural Networks; Toroidal-Core Microinductors Biased by Permanent Magnets; Using Correlated Photons to Suppress Background Noise; Atmospheric-Fade-Tolerant Tracking and Pointing in Wireless Optical Communication; Curved Focal-Plane Arrays Using Back-Illuminated High-Purity Photodetectors; Software for Displaying Data from Planetary Rovers; Software for Refining or Coarsening Computational Grids; Software for Diagnosis of Multiple Coordinated Spacecraft; Software Helps Retrieve Information Relevant to the User; Software for Simulating a Complex Robot; Software for Planning Scientific Activities on Mars; Software for Training in Pre-College Mathematics; Switching and Rectification in Carbon-Nanotube Junctions; Scandia-and-Yttria-Stabilized Zirconia for Thermal Barriers; Environmentally Safer, Less Toxic Fire-Extinguishing Agents; Multiaxial Temperature- and Time-Dependent Failure Model; Cloverleaf Vibratory Microgyroscope with Integrated Post; Single-Vector Calibration of Wind-Tunnel Force Balances; Microgyroscope with Vibrating Post as Rotation Transducer; Continuous Tuning and Calibration of Vibratory Gyroscopes; Compact, Pneumatically Actuated Filter Shuttle; Improved Bearingless Switched-Reluctance Motor; Fluorescent Quantum Dots for Biological Labeling; Growing Three-Dimensional Corneal Tissue in a Bioreactor; Scanning Tunneling Optical Resonance Microscopy; The Micro-Arcsecond Metrology Testbed; Detecting Moving Targets by Use of Soliton Resonances; and Finite-Element Methods for Real-Time Simulation of Surgery","NASA Tech Briefs, October 2003",https://core.ac.uk/download/pdf/10564967.pdf,,,,core
24414115,2005,"Motivation: Maximum likelihood methods have become very popular for constructing phylogenetic trees from sequence data. However, despite noticeable recent progress, with large and difficult data sets (e.g. multiple genes with conflicting signals) current ML programs still require huge computing times and can become trapped in bad local optima of the likelihood function. When this occurs, the resulting trees may still show some of the defects (e.g. long branch attraction) of starting trees obtained using fast distance or parsimony programs. Methods: Subtree Pruning and Regrafting (SPR) topological rearrangements are usually sufficient to intensively search the tree space. Here, we propose two new methods to make SPR moves more efficient. The first method uses a fast distance-based approach to detect the least promising candidate SPR moves, which are then simply discarded. The second method locally estimates the change in likelihood for any remaining potential SPRs, as opposed to globally evaluating the entire tree for each possible move. These two methods are implemented in a new algorithm with a sophisticated filtering strategy, which efficiently selects potential SPRs and concentrates most of the likelihood computation on the promising moves. Results: Experiments with real data sets comprising 35 to 250 taxa show that, while indeed greatly reducing the amount of computation, our approach provides likelihood values at least as good as those of the best known ML methods so far, and is very robust to poor starting trees. Furthermore, combining our new SPR algorithm with local moves such as PHYML’s nearest neighbor interchanges, the time needed to find good solutions can sometimes be reduced even more. Availability: Executables of our SPR program and the used data sets are available for download a",Improving the efficiency of SPR moves in phylogenetic tree search methods based on maximum likelihood,,,,,core
20693700,2008-04-02,"Abstract—A wireless communication system using multiple antennas promises reliable transmission under Rayleigh flat fading assumptions. Design criteria and practical schemes have been presented for both coherent and noncoherent communication channels. In this paper, we generalize one-dimensional (1-D) phase-shift keying (PSK) signals and introduce space–time constellations from generalized PSK (GPSK) signals based on the complex and real orthogonal designs. The resulting space–time constellations reallocate the energy for each transmitting antenna and feature good diversity products; consequently, their performances are better than some of the existing comparable codes. Moreover, since the maximum-likelihood (ML) decoding of our proposed codes can be decomposed to 1-D PSK signal demodulation, the ML decoding of our codes can be implemented in a very efficient way. Index Terms—Diversity, multiple antennas, orthogonal designs",Transactions Papers Generalized PSK in Space–Time Coding,,,,,core
24626153,2004,"Recently, multi-agent games have received attention because of their high complexity and close association with real life problems. We are studying multi-agent games by building an automated player for the non-cooperative version of the game “Diplomacy”, a multi-player, simultaneous move game with imperfect information. “Diplomacy ” is a good test bed for algorithms that have to coordinate multiple agents with nontrivial interactions so as to produce an effective policy in a hostile stochastic environment. We model our problem as a multi-agent Markov decision process where the optimum policy attempts to maximize the reward of controlled provinces. The reward function is learned through self-play by using temporal difference learning. The most important aspect of the learning procedure is the proper selection of features for the distinction of provinces. To effectively explore the search space of possible moves we break up the problem into a series of province-focused matrix games. We then compute the expected probability of success of each strategy for the corresponding province. Finally, we formulate a constrained stochastic policy optimization problem that yields optimal policies among the class of realizable ones given the resource limitations, the learned reward function and the probabilities of success. We have implemented our algorithm on the DAIDE platform and tested the algorithm versus available agents. Although our agent does not yet successfully compete with all the possible AI opponents, the results show the positive prospectives of our approach. We also provide a short discussion on the directions that future work on this problem could follow. ","Learning, Resource Allocation and the Art of War",,,,,core
10543849,"October 12, 2006","Electroactive polymers (EAP) are human made actuators that are the closest to mimic biological muscles. Technology was advanced to the level that biologically inspired robots are taking increasing roles in the world around us and making science fiction ideas a closer engineering reality. Artificial technologies (AI, AM, and others) are increasingly becoming practical tools for making biologically inspired devices and instruments with enormous potential for space applications. Polymer materials are used to produce figures that resemble human and animals. These materials are widely employed by the movie industry for making acting figures and by the orthopedic industry to construct cyborg components. There are still many challenges ahead that are critical to making such possibilities practical. The annual armwrestling contest is providing an exciting measure of how well advances in EAP are implemented to address the field challenges. There is a need to document natures inventions in an engineering form to possibly inspire new capabilities",Biomimetics as a Model for Inspiring Human Innovation,,,,,core
10538168,"August 20, 2007","It can take weeks or months to incorporate a new aerodynamic model into a vehicle simulation and validate the performance of the model. The Dynamic Aerospace Vehicle Exchange Markup Language (DAVE-ML) has been proposed as a means to reduce the time required to accomplish this task by defining a standard format for typical components of a flight dynamic model. The purpose of this paper is to describe an object-oriented C++ implementation of a class that interfaces a vehicle subsystem model specified in DAVE-ML and a vehicle simulation. Using the DaveMLTranslator class, aerodynamic or other subsystem models can be automatically imported and verified at run-time, significantly reducing the elapsed time between receipt of a DAVE-ML model and its integration into a simulation environment. The translator performs variable initializations, data table lookups, and mathematical calculations for the aerodynamic build-up, and executes any embedded static check-cases for verification. The implementation is efficient, enabling real-time execution. Simple interface code for the model inputs and outputs is the only requirement to integrate the DaveMLTranslator as a vehicle aerodynamic model. The translator makes use of existing table-lookup utilities from the Langley Standard Real-Time Simulation in C++ (LaSRS++). The design and operation of the translator class is described and comparisons with existing, conventional, C++ aerodynamic models of the same vehicle are given",The DaveMLTranslator: An Interface for DAVE-ML Aerodynamic Models,https://core.ac.uk/download/pdf/10538168.pdf,,,,core
380927184,2003-09-01T00:00:00,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites.  The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality.  Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc.  Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world.  The Glasgow Directory is an established 3D web space,

with around 10,000 visitors per year.  The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography.  This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar

environments. These criteria take into account lessons learned by ‘observing’ and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface.  These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow : Welcoming Multiple Visitors to the Virtual City,,,,,core
23063096,2004,"Representations of CCR algebras in spaces of entire functions are classified on the basis of isomorphisms between the Heisenberg CCR algebra AH and * algebras of holomorphic operators. To each representation of such algebras, satisfying a regularity and a reality condition, one can associate isomorphisms and inner products so that they become Krein * representations of AH, with the gauge transformations implemented by a continuous U(1) group of Krein space isometries. Conversely, any holomorphic Krein representation of AH, having the gauge transformations implemented as before and no null subrepresentation, are shown to be contained in a direct sum of the above representations. The analysis is extended to CCR algebras with [ai, a ∗ j]  = δi,j ηi, ηi = ±1, i = 1,...M, the infinite dimensional case included, under a spectral condition for the implementers of the gauge transformations",Representations of CCR algebras in Krein spaces of entire functions,,,,,core
11779721,2006-05-01T00:00:00,"Intelligent systems based on machine learning techniques, such as classification, clustering, are gaining wide spread popularity in real world applications. This paper presents work on developing a software system for predicting crop yield, for example oil-palm yield, from climate and plantation data. At the core of our system is a method for unsupervised partitioning of data for finding spatio-temporal patterns in climate data using kernel methods which offer strength to deal with complex data. This work gets inspiration from the notion that a non-linear data transformation into some high dimensional feature space increases the possibility of linear separability of the patterns in the transformed space. Therefore, it simplifies exploration of the associated structure in the data. Kernel methods implicitly perform a non-linear mapping of the input data into a high dimensional feature space by replacing the inner products with an appropriate positive definite function. In this paper we present a robust weighted kernel k-means algorithm incorporating spatial constraints for clustering the data. The proposed algorithm can effectively handle noise, outliers and auto-correlation in the spatial data, for effective and efficient data analysis by exploring patterns and structures in the data, and thus can be used for predicting oil-palm yield by analyzing various factors affecting the yield",A framework for predicting oil-palm yield from climate data,https://core.ac.uk/download/11779721.pdf,,,,core
12953039,1998-01-01,"One of the main issues in the research on time series is its prediction. 
Artificial neural networks are suitable tools for this purpose. Most traditional models are global models,assuming stationary. This ignores the fact that most real—world time series are non—stationary. An important subclass of non—stationary is piecewise stationary, where the series switches between different stationary regimes. Using an artificial neural network for the prediction in each regime, solves the problem of non—stationarity. To predict the transitions between the regimes, an additional artificial neural network can be used,assuming that these transitions are unknown. The gated experts network combines these properties. Key elements are: non—linearity, predicting regime transitions and local predictors for each regime.
The Expectation—Maximization learning algorithm is used to update the free parameters of the network. Our goal is to gain insight in the application of the gated experts network to
real—world time series prediction. This is achieved by studying the gated experts network, implementing it in InterAct© and conducting several experiments.
The gated experts network is a reasonable good choice for real—world time series prediction.
It uses the experts as local predictors and the gate plausibly allocates the experts to local regions of the input space. The gate splits the input space, but not always as one might expect.
This input space splitting depends on the initialization of the weights of the gate and experts.
The choice of the free parameters depends on the kind of experiment conducted. This network is a useful tool for analyzing the underlying dynamics of a time series.
The gated experts network can be modified by adding different density functions to individual experts, applying dynamic growth or pruning of the number of experts and hidden units of the experts. The implementation of this network can be extended to include separate tapped delay lines for the experts and the gate, to capture the periodicities in the time series.",The Gated experts network and time series prediction,,,,,core
42822513,"May 1, 1990","A great deal of attention has recently been given to Artificial Intelligence research in the area of computer aided diagnostics. Due to the dynamic and complex nature of space shuttle red-line parameters, a research effort is under way to develop a real time diagnostic tool that will employ historical and engineering rulebases as well as a sensor validity checking. The capability of AI software development tools (KEE and G2) will be explored by applying object oriented programming techniques in accomplishing the diagnostic evaluation",Object oriented fault diagnosis system for space shuttle main engine redlines,https://core.ac.uk/download/pdf/42822513.pdf,,,,core
42810151,"Oct 1, 1992","The Autonomous Power System (APS) brassboard is a 20 kHz power distribution system which has been developed at NASA Lewis Research Center, Cleveland, Ohio. The brassboard exists to provide a realistic hardware platform capable of testing artificially intelligent (AI) software. The brassboard's power circuit topology is based upon a Power Distribution Control Unit (PDCU), which is a subset of an advanced development 20 kHz electrical power system (EPS) testbed, originally designed for Space Station Freedom (SSF). The APS program is designed to demonstrate the application of intelligent software as a fault detection, isolation, and recovery methodology for space power systems. This report discusses both the hardware and software elements used to construct the present configuration of the brassboard. The brassboard power components are described. These include the solid-state switches (herein referred to as switchgear), transformers, sources, and loads. Closely linked to this power portion of the brassboard is the first level of embedded control. Hardware used to implement this control and its associated software is discussed. An Ada software program, developed by Lewis Research Center's Space Station Freedom Directorate for their 20 kHz testbed, is used to control the brassboard's switchgear, as well as monitor key brassboard parameters through sensors located within these switches. The Ada code is downloaded from a PC/AT, and is resident within the 8086 microprocessor-based embedded controllers. The PC/AT is also used for smart terminal emulation, capable of controlling the switchgear as well as displaying data from them. Intelligent control is provided through use of a T1 Explorer and the Autonomous Power Expert (APEX) LISP software. Real-time load scheduling is implemented through use of a 'C' program-based scheduling engine. The methods of communication between these computers and the brassboard are explored. In order to evaluate the features of both the brassboard hardware and intelligent controlling software, fault circuits have been developed and integrated as part of the brassboard. A description of these fault circuits and their function is included. The brassboard has become an extremely useful test facility, promoting artificial intelligence (AI) applications for power distribution systems. However, there are elements of the brassboard which could be enhanced, thus improving system performance. Modifications and enhancements to improve the brassboard's operation are discussed",Autonomous power system brassboard,https://core.ac.uk/download/pdf/42810151.pdf,,,,core
42788389,"Mar 1, 1993","Real-time artificial intelligence is gaining increasing attention for applications in which conventional software methods are unable to meet technology needs. One such application area is the monitoring and analysis of complex systems. MARVEL, a distributed monitoring and analysis tool with multiple expert systems, was developed and successfully applied to the automation of interplanetary spacecraft operations at NASA's Jet Propulsion Laboratory. MARVEL implementation and verification approaches, the MARVEL architecture, and the specific benefits that were realized by using MARVEL in operations are described",Combining real-time monitoring and knowledge-based analysis in MARVEL,https://core.ac.uk/download/pdf/42788389.pdf,,,,core
24701046,1994,"Interaction in Virtual Reality environments is still a challenging task. Static hand posture recognition is currently the most common and widely used method for interaction using glove input devices. In order to improve the naturalness of interaction, and thereby decrease the user–interface learning time, there is a need to be able to recognize dynamic gestures. Dynamic Gesture Recognition (DGR) is difficult for various reasons. The large variations in the speed of execution of various phases of a gesture is one such reason. Another is the quality and positions of the physical properties describing a gesture themselves. These problems are then exaggerated by the differences which arise when various people attempt the same gesture, as well as when the same person attempts repeated executions of the same gesture. Other factors effecting the difficulty of DGR are the emotional state of the person doing the gesture and the accuracy of the input device used. And finally, a large amount of data has to be processed in real time because of large variances in the length of time to execute a gesture. In this paper we describe our approach to overcoming the difficulties of DGR using neural networks. Backpropagation neural networks have already proven themselves to be appropriate and efficient for posture recognition. However, the extensive amount of data involved in DGR requires a different approach. Because of features such as topology preservation and automatic-learning, Kohonen Feature Maps are particularly suitable for the reduction of the high dimensional data space which is the result of a dynamic gesture, and are thus implemented for this task. 1",Dynamic gesture recognition using neural networks; a fundament for advanced interaction construction,,,,,core
23741230,1999,"Abstract- In this contribution we present neural network training algorithms, which are based on the differential evolution (DE) strategies introduced by Storn and Price [Journal of Global Optimization 11, 341-359, 19971. These strategies are applied to train neural networks with small integer weights. Such neural networks are better suited for hardware implementation than the real weight ones. Furthermore, we constrain the weights and biases in the range [-2k + 1, 2k- 11, for k = 3,4,5. Thus, they can be represented by just k bits. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are easier to be implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, as we have found here, the network training procedure can be more effective and efficient when large weights are allowed. Thus, for a given application a trade off between effectiveness and memory consumption has to be considered. Our intention is to present results of evolution algorithms on this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable. ",Neural network training with constrained integer weights,,,,,core
10473436,Oct. 1996,"The NASA OPAD spectrometer system relies heavily on extensive software which repetitively extracts spectral information from the engine plume and reports the amounts of metals which are present in the plume. The development of this software is at a sufficiently advanced stage where it can be used in actual engine tests to provide valuable data on engine operation and health. This activity will continue and, in addition, the OPAD system is planned to be used in flight aboard space vehicles. The two implementations, test-stand and in-flight, may have some differing requirements. For example, the data stored during a test-stand experiment are much more extensive than in the in-flight case. In both cases though, the majority of the requirements are similar. New data from the spectrograph is generated at a rate of once every 0.5 sec or faster. All processing must be completed within this period of time to maintain real-time performance. Every 0.5 sec, the OPAD system must report the amounts of specific metals within the engine plume, given the spectral data. At present, the software in the OPAD system performs this function by solving the inverse problem. It uses powerful physics-based computational models (the SPECTRA code), which receive amounts of metals as inputs to produce the spectral data that would have been observed, had the same metal amounts been present in the engine plume. During the experiment, for every spectrum that is observed, an initial approximation is performed using neural networks to establish an initial metal composition which approximates as accurately as possible the real one. Then, using optimization techniques, the SPECTRA code is repetitively used to produce a fit to the data, by adjusting the metal input amounts until the produced spectrum matches the observed one to within a given level of tolerance. This iterative solution to the original problem of determining the metal composition in the plume requires a relatively long period of time to execute the software in a modern single-processor workstation, and therefore real-time operation is currently not possible. A different number of iterations may be required to perform spectral data fitting per spectral sample. Yet, the OPAD system must be designed to maintain real-time performance in all cases. Although faster single-processor workstations are available for execution of the fitting and SPECTRA software, this option is unattractive due to the excessive cost associated with very fast workstations and also due to the fact that such hardware is not easily expandable to accommodate future versions of the software which may require more processing power. Initial research has already demonstrated that the OPAD software can take advantage of a parallel computer architecture to achieve the necessary speedup. Current work has improved the software by converting it into a form which is easily parallelizable. Timing experiments have been performed to establish the computational complexity and execution speed of major components of the software. This work provides the foundation of future work which will create a fully parallel version of the software executing in a shared-memory multiprocessor system",Development of a Computer Architecture to Support the Optical Plume Anomaly Detection (OPAD) System,https://core.ac.uk/download/pdf/10473436.pdf,,,,core
334969733,1993-07-26T00:00:00,"Supervised concept learning based on Horn clauses is one of the most active areas of machine learning research. Two popular systems in this area are FOIL (First Order Inductive Learner) and FOCL (First Order Combined Learner). This paper points out sorne conceptual traps and pitfalls in which these two systems fall when they cope with some tasks taken both from the machine learning literature and from real world domains. An interpretation of the obtained results is provided. It is based on a comparison between the search space that these two systems should explore and the search space that they actually explore and on considerations about the representation of the examples as tuples in a relational database. Theoretically-founded solutions to the detected problems are suggested. Moreover, a more manageable practical solution is proposed and its strengths and weaknesses in comparison with the theoretically-founded ones are evaluated. Such a solution has been satisfactorily implemented in a new version of FOCL",Traps and pitfalls when learning logical theories : a case study with FOIL and FOCL,,"eScholarship, University of California",,,core
71171036,1997-04-01,"In recent years, multilayer feedforward neural networks (NN) have been shown to be very effective tools in many different applications. A natural and essential step in continuing the diffusion of these tools in day by day use is their hardware implementation which is by far the most cost effective solution for large scale use. When the hardware implementation is contemplated, the issue of the size of the NN becomes crucial because the size is directly proportional with the cost of the implementation. In this light, any theoretical results which establish bounds on the size of a NN for a given problem is extremely important. In the same context, a particularly interesting case is that of the neural networks using limited integer weights. These networks are particularly suitable for hardware implementation because they need less space for storing the weights and the fixed point, limited precision arithmetic has much cheaper implementations in comparison with its floating point counterpart. This paper presents an entropy based analysis which completes, unifies and correlates results partially presented in [Beiu, 1996, 1997a] and [Draghici, 1997]. Tight bounds for real and integer weight neural networks are calculated",Entropy based comparison of neural networks for classification,,Los Alamos National Laboratory,,,core
357555467,1996-01-01T00:00:00,"Introduction In the early 1960s, M. E.  Salukvadze&apos;s complete solution consists of a constant-gain feedback of the full state vector (the well-known LQR solution), summed with a Duhamel integral term. This solution has a very complicated algebraic form that would seem useful only for very simple and low-order cases, based on our attempts at implementation  In the stochastic-control realm, the disturbance attenuation problem has commonly been treated by a method called disturbance-accommodation. With this approach a colored input disturbance (process noise) is modeled as the result of passing zero-mean white Gaussian noise through a linear filter. The additional states associated with the filter model are used to augment the state-space model of the original system. The end result is a homogeneous LQG problem, for which the solution is well-known: the optimal controller consists of a matrix of constant feedback gains, determined by solving an algebraic Riccati equation (ARE). It has been shown (L. A.  Although the deterministic disturbance-rejection problem has also received some attention since Salukvadze&apos;s work, a literature search indicates that these investigations have not provided either a simphfication, or an implementation, of Salukvadze&apos;s unwieldy solution form. In general, published approaches have followed one of two basic paths. Some investigators, following C. D. Johnson&apos;s seminal work (1968, 1970, 1971)  have assumed an a priori knowledge of the form of the input disturbance vector, in terms of an appropriate linear ordinary differential equation that it has been required to satisfy. The disturbance model has then been used to augment the state-space model of the undisturbed system. (Indeed, C. D. Johnson&apos;s work apparently also provides the background for the disturbance-accommodation technique noted above regarding stochastic disturbances, although his treatment was from a deterministic perspective.) This approach has appeared in a number of recent books. (See, for example, B. Friedland, 1986; and F. L. Lewis,  1992.)  Other investigators, such as B. D. O. Anderson and J. B.  Moore (1971), have treated a &quot;modified regulator problem,&quot; in which &quot;derivative constraints&quot; have been added by incorporating derivative control terms, in quadratic form, into the performance index. This second approach produces an integral feedback control which allows the controller to reject slowly varying disturbances. Either basic approach involves solving a homogeneous LQR problem, with its associated algebraic Riccati equation (ARE). In contrast, the fully &quot;clairvoyant LQR problem&quot; addressed by Salukvadze (albeit without that appellation) has an optimal control solution with both a feedback-and a Duhamel integral term. The solution is exact, the state vector remains unaugmented, and the disturbance suffers no requirement to satisfy any particular differential equation. In 1975, M. Tomizuka reported using dynamic programming theory to determine an optimal control for what he called the &quot;continuous finite preview problem.&quot; This is a tracking problem in which command signals (commanded plant outputs) are known deterministically for a finite time duration into the future, and probabiUstically thereafter. Tomizuka&apos;s controller included the standard LQR feedback term, plus two integral terms to incorporate the preview information. He found both that the integral terms could be expressed in compact, real-valued matrix form, and that the preview information needed to be available only for an &apos; &apos;effective preview length&apos;&apos; of about three times the longest time constant of the closed-loop system. Although the problems are distinct, Tomizuka&apos;s continuous finite preview problem bears a close enough relationship to Salukvadze&apos;s nonhomogeneous LQR problem to suggest that the latter might have a simplified solution-form as well. The similarities between the homogeneous LQR and -LQG problems proffer the same suggestion, in light of Davis&apos; aforementioned work. Tomizuka&apos;s idea of an &apos; &apos;effective preview length&apos;&apos; might also be useful in determining circumstances under which Salukvadze&apos;s optimal control could be realizable to a useful level of approximation. That is, for &quot;sufficiently slow&quot; disturbances and a &quot;sufficiently fast&quot; closed-loop system, the future disturbances might be known well enough, based solely on present measurements, to make Salukvadze&apos;s control solution essentially realizable. Transactions of the ASME Copyright © 1996 by ASME expressed in a practical and usable matrix form. In addition to filling a longstanding gap in LQR theory, this solution considerably improves the system performance index (cost) over that of LQR-feedback alone, by incorporating the forcing function or disturbance information into the control. An example demonstrates the improvement achievable with the enhanced LQR controller. Problem Statement Consider a system with linear dynamical equations of motion expressed in state-space form as where x = x(0 is the state vector; u = u_{t) is the control vector; f_ = j^(t) is a known or measurable bounded disturbance vector; and £ is a disturbance selection matrix. The term E£_ distinguishes this system from that of the standard LQR problem. Assume that {A, B] is stabihzable and that {C, A} is detectable. The performance index to be minimized is Wi, W2, and W3 are real-valued weighting matrices: Wi is positive semidefinite (PSD), and W^ is positive definite (PD). The superscript &quot;7&quot;&apos; indicates the transpose of the matrix. This performance index has the &quot;cost rate functional&quot; form (Sage  and White, 1977: 272, 276) to accommodate nondwindling disturbances without giving an unbounded cost. For dwindling disturbances is an appropriate form. The problem solution is the same in both cases. Let an admissible control M (/) be a bounded vector function that depends only on the past accumulative observation data 2^(0 (since future states are not available for control use) and on the future disturbance _F(f) (to allow for the case when the disturbances can be assumed to be fully known). Expressed mathematically, and where f_(T) is a bounded vector disturbance function. u(t) has the form where a is a vector operator that is linear in terms of its arguments. The objective is to find an admissible control function u_*{t) which minimizes the cost J (the asterisk indicates optimality in this sense) with respect to the set of admissible control functions u(t) subject to the dynamic constraint {la,b). For a timeinvariant system, A,B,C,D,E,WuW2, and ^3 are all constant matrices. Problem Solution The problem is formulated as a variational problem of Lagrange (Elbert, 1984). After the appropriate differential equations are produced using the calculus of variations, they are solved by a state transition matrix approach. First, augment the performance index J with the state equations using Lagrange multipliers X(0-The resulting augmented performance index is where H is the Hamiltonian: The boundedness of j; for f &lt; S? is ensured by the form of the cost functional. For generality, the state vector x is partitioned as follows: The states Xi are subject to terminal constraint equations &lt;/&apos;i(9&apos;)xi(5) = &apos;±, Equation (6^) can be expressed equivalently by  The remaining states Xp. are subject to a terminal weighting, expressed by the weighting term Equation (6/) can be expressed equivalently by where J* is now augmented with the constraint Eqs. (6c) using additional Lagrange multipliers i^(t), and with the terminal weighting term of Eq. (6g). The resulting fully augmented performance index is To determine the optimal control, the first variation 67 is set equal to zero:  6x and 6u are independent variations of x and uj, 6x3 is the (arbitraryT variation of Sxatt = S7. Since the variations Ax and Su_ are independent, Eqs. (la) and  (9e) Based on the form of the transversality condition (9*5?), \(t) is assumed to have the form \(t) = -[P(t)x(t) + R(t)i^(t)] The terminal condition (9e) is satisfied by requiring that R(?7) = il/&apos;^Cn). (Ub) Differentiation of Eq.  Substitution for x from Eq. (9a) and for \ from Eq. (10) into Eq. (12) yields An alternate expression for \(t) can be obtained by substituting into Eq. (9b) the expression for \(t) given in Eq. (10): Equations  Equation  356 / Vol. 118, JUNE 1996 and We can let s = Rv, such that Eq.  The terminal conditions of Eqs. (lla) and (11^) still hold. Substitution for X. from Eq. (10) into Eq. (9 c), and replacement of Ri^ by s yields where P and £ can be found from the following: and £., :=£(g&apos;) (20e) Note that £( ST) is not actually known. However, it is the product of/f(i7) and J^(9&apos;); the former is known [Eq. (Mb)], and the latter is finite (since t/ is a Lagrange multiplier). Consequently, 5(S7) is finite, and it turns out that it can be absorbed into the solution (below) so that its value need not be expressly known. As S/-* 00, the augmented performance index 7 [Eq.  Equations (22a, b) can be solved directly by using state transition matrices (see below). Define &quot;^(t, T) := e-^&apos;^&apos;-^\ Then the solution for s is Since A is the closed-loop system dynamic matrix (which is guaranteed by LQR theory to have eigenvalues with all negative real parts), and since s, is finite, Eq. (24) reduces to Finally, substitution into Eq. (19) yields the optimal control: Transactions of the ASME This optimal solution exists under the same circumstances that the positive definite solution P exists to the ARE, namely, when {A, B] is stabilizable and {C, A] is detectable. Equation  Realizability of the Integral Term Under most circumstances future disturbance information will not be available other than in a probabilistic sense. This will preclude realizability of the optimal preview control, except for a few simple cases in which the disturbance information will permit the integral term to be realized exactly. One case is that of constant deterministic disturbances, viz., f (t) = F. In this case, the optimal preview control u_tiF(t) is  (28rf) IfF(t) is measurable, this preview term will be realizable physically, since it depends only on present values of f_(t). Under most practical circumstances, however, such simplified expressions for u_*/f will not be possible. Consequently, it is appropriate to consider whether or not there is any approximation to the integral term that is &quot;close enough&quot; under some circumstances. And if so, what is such an approximation and when is it useful? Integral Term Approximation Repeated integration by parts of the integral term of Eq. (27) yields where the post-superscript &quot;(r)&quot; denotes the rth time derivative. If the first k derivatives of /(;) are available, this approximation to the optimal control u_*{t) can be designated by the presuperscript in parentheses, as indicated below.  and Note that. for . * constant disturbance f{t) = F, &quot;&apos;&gt;M?,f(f) = *&quot;Mf/f(f) = M*(0 as expected, per Eq. (28c). Consider again the optimal preview control term, from Eq. (27), A, given by Eq. (28d), is a dynamic matrix for the closedloop system. Since the feedback control is a stabilizing control, all of the eigenvalues of A have negative real parts. Additionally, /(f) is bounded, by assumption. These two facts imply that the integral exists (i.e., is finite). In practical terms, this means that the future makes less and less of a contribution to the optimal preview control as it becomes more and more distant from the present. Assume that at any time t the disturbance f(t) is known into the future for a finite time period, or &quot;preview length&quot; \. (Let X be a constant, for simplicity.) One would expect that for X &quot;large enough&quot; the preview control integral could be approximated to a sufficient degree of accuracy by the following: Following Tomizuka (1975: 364, 365), the maximum practically effective preview length may be considered to be about three times the longest time constant of the closed loop system. This means that if the disturbance vector f_(t) has low enough frequency content relative to the bandwidth of the closed-loop system, then a good approximation of the optimal preview control term uf,F will be possible with present preview knowledge only of/(f) or, less restrictively, of/(f) and /&apos;&quot;(f). Under these circumstances the optimal preview control can be considered implementable, for practical purposes. Example: Vibration Isolation Let a mass m be subject to a time-dependent disturbance force/(f), and to an actuator force ai(t), as shown in  Case 1. For the purpose of comparison, consider the case of a constant disturbance/(O = F. The following additional parameter assignments are made for simplicity:  a \ a (37) In terms of the closed-loop-system eigenvalues \i and \2. and using (for convenience) the variable ^i as defined below, the optimal and suboptimal controls of interest are as follows: For this case the optimal control (&apos;* and its approximation &apos;-&quot;H* are identical. This is because all future information about/(?) is contained in its present measurements. Note also that the inclusion of preview information into the control reduces the quadratic cost by a factor of two. For the case of feedback control only, with zero initial conditions, the mass has position With the addition of the preview control term, the position is described by Case 2. Consider next the case of an exponentially decaying disturbance The respective optimal and suboptimal controls are as follows: where /j, is defined as before (Eq. 39c), and The closed-loop system eigenvalues X. i and X. 2 are the same as for the preceding case. Again, for simplicity, assign parameter values as in Eqs. (40a-e), and let C = 1. Then the three controls under consideration become   Pi = a(wia + wjfiyi) + mw,yi With zero initial conditions, the respective system responses are as follows: 4(2 -71) 72(1 +7i)&apos;+ 1 cos (j) (55c) 1) with i*(t): 27? x(t) = 7?+ 27,+2 3) with ifm(t): Comparisons of the three controls, in terms of their associated costs, are presented in  With Tl = 10, the costs are as follows: J* = 2.499995, &lt;&quot;&gt;/* = 2.500000, and Jfm = 5.259745 Clearly the control &apos;&quot;&apos;«* approximates i* much more closely The cost functional values corresponding to the respective controls can be written as follows: than i*,B in this case. Note also that *°&apos;7* approximately doubles 7* in value. In the limit with increasing r i the doubling becomes exact. This relationship was noted earlier, in the treatment of constant disturbances. [See Eqs. (42).] For disturbances with small time constants (TI &lt; 0.01), the plot of &apos;&quot;V* diverges widely from that of /*. This is because the truncated preview control term no longer approximates the optimal preview control to any degree of accuracy, for these rapidly decaying disturbances. On the other hand, the plots of jf/ii and J* practically coincide for these rapidly decaying disturbances. This too is not suiprising, inasmuch as the homogeneous LQR solution (ifm) is optimal for arbitrary initial conditions. These initial conditions can be conceived of as a system response to impulsive disturbances at time f = 0. Conclusion The infinite-horizon, time-invariant LQR problem originates from a homogeneous set of state-space equations. The solution consists of constant-gain feedback of the full state vector, and minimizes a quadratic performance index. If the state equations are made nonhomogeneous, however, by the addition of a vector of deterministic forcing terms, the standard LQR solution is no longer optimal. In this paper, variational calculus and state-transition-matrix methods have been used to produce a simplified, optimal matrix solution to this nonhomogeneous LQR problem. The solution adds a matrix preview (Duhamel integral) term to the familiar constant-gain feedback term. A practical approximation to this optimal control solution has also been developed. The approximation consists of a constant preview-gain matrix, and permits the effective use of real-time disturbance information in the controller. An example shows the improvement in controller performance with the use of this preview-gain matrix, for exponentially decaying disturbances with a range of time constants. Cost versus time-constant curves permit comparison of homogeneous LQR, nonhomogeneous LQR, and constant preview-gain solutions. The constant preview-gain solution provides significant improvements in quadratic cost over that afforded by LQR feedback alone. For disturbances that are slow in comparison to the closed-loop-system time constants, the constant preview-gain controller and the optimal controller have nearly identical costs. Acknowledgments Th",A practical solution to the deterministic nonhomogeneous lqr problem,,,,,core
23264805,2000,"This paper discusses our ongoing experimental work to train classier software for automated detection of rare events of interest in space instruments. As a concrete real-world example, this paper focuses on the task of detecting possible thermal snap events, using accelerometer data describing the movements of the boom structure during the 1997 IPEX-2 experiment. In particular, this paper explores the use of recent promising techniques for automated classication based on support vector machines.  Keywords: Interferometry, Thermal Snap, IPEX-2, Event Detection, Support Vector Machines, Data Mining, Machine Learning, Pattern Recognition, Automated Classication  1. INTRODUCTION  Detecting predened types of events in the time-series sensor data of operational space instruments is important for a variety of tasks, including health/condition monitoring, plan execution monitoring, and general engineering analysis. Traditionally, such detection is based on human-intensive methods, such as ..",Automated Event Detection in Space Instruments: A Case Study Using IPEX-2 Data and Support Vector Machines,,,,,core
23154532,1997,"Recently, several planners have been designed that can  create conditionally branching plans to solve problems  whichinvolve uncertainty. These planners represent  an important step in broadening the applicabilityof  AI planning techniques, but they typically must search  a larger space than non-branching planners, since they  must produce valid plans for each branch considered.  In the worst case this can produce an exponential increase  in the complexity of planning. If conditional  planners are to become usable in real-world domains,  this complexitymust be controlled by sharing planning  e#ort among branches. Analogical plan reuse  should play a fundamental role in this process. We  have implemented a conditional probabilistic planner  that uses analogical plan replay to derive the maximum  bene#t from previously solved branches of the  plan. This approach provides valuable guidance for  when and how to merge di#erent branches of the plan  and exploits the high similaritybetween th..",Analogical Replay for Efficient Conditional Planning,,,,,core
24372916,1994,". The implementation of larger digital neural networks has not been possible due to the real-estate requirements of single neurons. We present an expandable digital architecture which allows fast and spaceefficient computation of the sum of weighted inputs, providing an efficient implementation base for large neural networks. The actual digital circuitry is simple and highly regular, thus allowing very efficient space usage of fine grained FPGAs. We take advantage of the re-programmability of the devices to automatically generate new custom hardware for each topology of the neural network. 1 Introduction As conventional computer hardware is not optimized for simulating neural networks, several hardware implementations for neural networks have been suggested ([MS88], [MOPU93], [vDJST93]). One of the major constraints on hardware implementations of neural nets is the amount of circuitry required to perform the multiplication of each input by its corresponding weight and their subsequent..",A Fast FPGA Implementation of a General Purpose Neuron,,,10.1007/3-540-58419-6_88,,core
24466105,2002,"AbstractÐIn k-means clustering, we are given a set of n data points in d-dimensional space R d and an integer k and the problem is to determine a set of k points in R d, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd&apos;s algorithm. In this paper, we present a simple and efficient implementation of Lloyd&apos;s k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm&apos;s running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation. Index TermsÐPattern recognition, machine learning, data mining, k-means clustering, nearest-neighbor searching, k-d tree, computational geometry, knowledge discovery. ",An efficient k-means clustering algorithm: Analysis and implementation,,,,,core
23930998,1996,"NASA has recently announced the New Millennium Program (NMP) to develop \faster, better, cheaper&quot; spacecraft in order to establish a \virtual presence &quot; in space. A crucial element inachieving this vision is onboard spacecraft autonomy, requiring us to automate functions which havetraditionally been achieved on ground by humans. These include planning activities, sequencing spacecraft actions, tracking spacecraft state, ensuring correct functioning, recovering in cases of failure and recon guring hardware. In response to these challenging requirements, we analyzed the spacecraft domain to determine its unique properties and developed an architecture which provided the required functionality. This architecture integrates traditional real-time monitoring and control with constraint-based planning and scheduling, robust multi-threaded execution, and model-based diagnosis and recon guration. In a ve month e ort we successfully demonstrated this implemented architecture in the context of an autonomous insertion of a simulated spacecraft into orbit around Saturn, trading o science and engineering goals, and achieving the mission goals in the face of any single point of hardware failure. This scenario turned out to be among the most complex handled by each of the component technologies. As a result of this success, the integrated architecture has been selected to control the rst NMP ight, Deep Space One, in 1998. It will be the rst AI system to autonomously control an actual spacecraft",A Remote Agent Prototype for Spacecraft Autonomy,,,,,core
216777371,,"Summarization: In this paper we review the applicability of representative inductive machine learning
approaches in multicriteria decision making. We limit our review to four systems. We use
SICLA and KBG as representative conceptual clustering systems and ID3 and CN2 as
representative learning from examples systems. We demonstrate our results by way of two
real world decision making exemplars. The first exemplar concerns the evaluation of retail
outlets [15]. The second exemplar concerns venture capital assessment [16]. We discuss the
conditions under which inductive learning methodologies can be effectively implemented to
support decision making.
Inductive machine learning was pioneered by Michalski [9]. It aims at the derivation
of knowledge from a set of observations, or facts. In cases where facts are known to belong
to a certain class we speak of concept acquisition or learning from examples. In such
an instance we target our inquiry towards the derivation of concept identification rules.
Rules may be either discriminant or characteristic. When concept classes underlying fact
membership are not known we speak of learning from observations, or conceptual clustering.
Accordingly, we look forward toward the partitioning of facts into a meaningful and disjoint
set of clusters. A cluster represents a “coming together in space and time so that the density
of whatever is clustered contrasts with the density around” [6, p.33]. Generalization and
specialization are essential processes when making inductive inferences. The basic premise
characterizing any inductive inference is falsity preservation. The derivation of a hypothesis
H from facts E is falsity preserving in the sense that “if some facts falsify E, then they
must also falsify H” [9, p.89].
Although inductive machine learning is a rather new field there are several and
successful ‘fielded’ applications [7, 8]. Carter and Catlett [2] propose a methodology for
credit card assessment using inductive learning techniques. Also, Shaw and Gentry [14]
present an approach for company risk assessment that is based on inductive learning. Both
applications are exploratory; they, however, stress the potential of inductive learning in
decision making support. We maintain that learning is a trait of decision making: “quite
often the decision maker is interested in finding out what his weights are or what they
should be under different decision circumstances. In this sense, the weights of importance
could be considered as desirable outputs rather than independent inputs of an analysis.
Weights must be revealed or learned through a careful interactive process”, [17, p.22] -
emphasis is ours.
In this paper we discuss the methodological issues underlying the application of
inductive learning techniques in business decision making. We limit our endeavor to four
representative and well-known inductive learning systems, ID3 [12, 11, 13], CN2 [5, 4], KBG
[1] and SICLA [3]. These systems are part of the Machine Learning Toolbox [7, 18]. We
explore inductive system suitability by way of three decision making exemplars. We draw
our exemplars from retail outlet evaluation and venture capital assessment. We target our
inquiry toward the evaluation of pros and cons, concerning the application of the selected
inductive learning systems, in real world business decision making. Specifically, our research
focuses around the following lines:
1. Grouping of alternatives into disjoint cluster groups. We use a Lexicographic Evaluation
Functional, LEF, criterion to optimize clustering [10].
2. Identification of the most significant criteria for either alternative discrimination or
alternative characterization. Suppose that we have two alternative courses of action,
a1 and a2. We are interested in differences between a1 and a2, or in what a1 and a2
are all about. Furthermore, we present a methodology for inducing criteria weights.
3. Identification of relevant and accurate discrimination and recognition rules. We associate
this line with the previous one.
4. Identification of the most representative alternative for each decision class. We steer
our venture in the direction of deriving a conceptual indexing scheme for alternative
courses of action.
5. Identification of bias and error resulting from contextual factors. We define context
to represent the decision making environment.
Furthermore, we explore the implications of our research in decision making. We
place emphasis upon the expert critiquing and case based reasoning paradigms.Παρουσιάστηκε στο: 3rd International Workshop on Artificial Intelligence in Economics and Managemen",Inductive learning support for decision making,,,,,core
100118663,1998,"The recent explosion of on-line information in Digital Liraries and on the World Wide Web has given rise to a number of query-based search engines and manually constructed topical hierarchies. However, these tools are quickly becoming inadequate as query results grow incomprehensibily large and manual classification in topic hierarchies creates an immense bottleneck. We address these problems with a system for topical in-formation space navigation that combines query-based and taxonomic systems. We employ machine learning techniques to create dynamic document categorizations based on the full-text of articles that are retrieved by users&apos; queries. Our system, named SONIA (Service for Organizing Networked Information Autonomously), has been implemented as part of the Stanford Digital Libraries Testbed. It employs a combination of technologies that take the results of queries to networked information sources and, in real-time, automatically retrieve, parse and organize these documents into coherent categories for presentation to the user. Moreover, the system can then save such document organizations in user profiles which can then be used to help classify future query results by the same user. SONIA uses a multi-tier approach to extracting relevant terms from documents as well as statistical clustering methods to determine potential topics within a document collection. It also makes use of Bayesian classification techniques to classify new documents within an existing categorization scheme. In this way, it allows navigate the results of a query at a more topical level than having to examine each document text separately",SONIA: A Service for Organizing Networked Information Autonomously,,,,,core
1497023,1994-01-01T00:00:00,"Advances in Intelligent Control is a collection of essays arranged in two parts. Part one contains recent contributions of artificial neural networks to modelling and control. Part two concerns itself primarily with aspects of fuzzy logic in intelligent control, guidance and estimation although some of the contributions either make direct equivalence relationships to neural networks or use hybrid methods where a neural network is used to develop the fuzzy rule base. Written by an internationally respected team of experts, contents include: Neural networks for modelling and control. Learning control with interpolative memories. Neural network model-based predictive control. Hierarchical fuzzy control. Indirect adaptive fuzzy logic control. Adaptive expert systems. Bibliography. C.J. Harris is Lucas Professor of Aerospace Systems Engineering at the University of Southampton, Southampton, UK. Index: 1. Editor's Introduction - C.J. Harris Part I - Neural Networks in Intelligent Control 2. Neural Networks for Modelling and Control: A Review - M. Brown and C.J. Harris 3. Neural Net Computing and Intelligent Control of Systems - Y.H. Pao et al. 4. Neural Networks for Nonlinear Dynamic System Modelling and Identification - S. Chen and S.A. Billings 5. Learning Control with Interpolative Memories - H. Tolle et al. 6. ASMOD: An algorithm for Adaptive Spline Modelling of Observation Data - T. Kavli 7. Adaptive Control of Nonlinear Systems - F.C. Chen and H.K. Khalil 8. Neural Network Model Based Predictive Control - J. Saint-Donat et al. Part II - Fuzzy Logic Control 9. Aspects of Fuzzy Control and Estimation: A Review - C.G. Moore and C.J. Harris 10. Hierarchical Fuzzy Control - G. Raju 11. Unified Real Time Approximate Reasoning - D. Linkens and J. Nie 12. Indirect Adaptive Fuzzy Logic Control - C.G. Moore and C.J. Harris 13. Adaptive Expert Systems - C. Batur and V. Kasparian 14. Neural Network Based Approximate Reasoning: Principles and Implementation - J. Nie and D. Linkens 15. Self-Organizing Control using Fuzzy Neural Networks - T. Yamaguchi et al. Bibliography Further information can be obtained from: Taylor &amp; Francis Ltd Rankine Road Basingstoke Hampshire RG24 8PR UK or Taylor and Francis Inc. 1900 Frost Road Suite 101 Bristol PA 19007-1598 USA Content",Advances in Intelligent Control,,Taylor and Francis,,,core
15351700,2001,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites. The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality. Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc. Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world. The Glasgow Directory is an established 3D web space, with around 10,000 visitors per year. The model represents approximayely 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography. This paper will describe the background to this VR space, and suggest a set of design criteria for successfully deploying multi-user software within this and similar environments. These criteria will take into account lessons learned by'observing'and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface. These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users.",VRGLASGOW.CO.UK implementation of internet multi-user functionality to Glasgow\u27s virtual city,,,,,core
23742866,2000,"Abstmct- Evolutionary neural network training algorithms are presented. These algorithms are applied to train neural networks with weight values confined to a narrow band of integers. We constrain the weights and biases in the range [-2&quot;-l + 1, 2k-1- 11, for k = 3,4,5, thus they can be represented by just k bits. Such neural networks are better suited for hardware implementation than the real weight ones. Mathematical operations that are easy to implement in software might often be very burdensome in the hardware and therefore more costly. Hardware-friendly algorithms are essential to ensure the functionality and cost effectiveness of the hardware implementation. To this end, in addition to the integer weights, the trained neural networks use threshold activation functions only, so hardware implementation is even easier. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are easier to be implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, as we have found here, the network training procedure can be more effective and efficient when larger weights are allowed. Thus, for a given application a trade off between effectiveness and memory consumption has to be considered. Our intention is to present results of evolutionary algorithms on this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable. ",Training neural networks with threshold activation functions and constrained integer weights,,,10.1109/ijcnn.2000.861451,,core
327070322,2002-01-01T00:00:00,"To date there have been few implementation of Holland’s Learning Classifier System (LCS) on real robots. The paper introduces a Temporal Classifier System (TCS), an LCS derived from Wilson’s ZCS. Traditional LCS have the ability to generalise over the state action-space of a reinforcement learning problem using evolutionary techniques. In TCS this generalisation ability can also be used to determine the state divisions in the state space considered by the LCS. TCS also implements components from Semi-Mark-Decision Process (SMDP) theory to weight the influence of time on the reward functions of the LCS. A simple light-seeking task on a real robot platform using TCS is presented which demonstrates desirable adaptive characteristics for the use of LCS on real robots",TCS learning classifier system controller on a real robot,,'Springer Science and Business Media LLC',10.1007/3-540-45712-7_57,,core
48534097,2001-01-01T00:00:00,"Ying-Qian Zhang.Thesis (M.Phil.)--Chinese University of Hong Kong, 2001.Includes bibliographical references (leaves 115-124).Abstracts in English and Chinese.Abstract ---  p.iAcknowledgement ---  p.iiiChapter 1 --- Introduction ---  p.1Chapter 1.1 --- Background ---  p.1Chapter 1.2 --- Objective ---  p.2Chapter 1.3 --- Contributions ---  p.3Chapter 1.4 --- Thesis Overview ---  p.4Chapter 2 --- Literature Review ---  p.6Chapter 2.1 --- Takens' Theorem ---  p.6Chapter 2.2 --- Linear Models for Prediction ---  p.7Chapter 2.2.1 --- Autoregressive Model ---  p.7Chapter 2.2.2 --- Moving Average Model ---  p.8Chapter 2.2.3 --- Autoregressive-moving Average Model ---  p.9Chapter 2.2.4 --- Fitting a Linear Model to a Given Time Series ---  p.9Chapter 2.2.5 --- State-space Reconstruction ---  p.10Chapter 2.3 --- Neural Network Models for Time Series Processing ---  p.11Chapter 2.3.1 --- Feed-forward Neural Networks ---  p.11Chapter 2.3.2 --- Recurrent Neural Networks ---  p.14Chapter 2.3.3 --- Training Algorithms for Recurrent Networks ---  p.18Chapter 2.4 --- Combining Neural Networks and other approximation techniques ---  p.22Chapter 3 --- ForeNet: Model and Representation ---  p.24Chapter 3.1 --- Fourier Recursive Prediction Equation   ---  p.24Chapter 3.1.1 --- Fourier Analysis of Time Series  ---  p.25Chapter 3.1.2 --- Recursive Form  ---  p.25Chapter 3.2 --- Fourier Recurrent Neural Network Model (ForeNet)  ---  p.27Chapter 3.2.1 --- Neural Networks Representation  ---  p.28Chapter 3.2.2 --- Architecture of ForeNet   ---  p.29Chapter 4 --- ForeNet: Implementation ---  p.32Chapter 4.1 --- Improvement on ForeNet  ---  p.33Chapter 4.1.1 --- Number of Hidden Neurons  ---  p.33Chapter 4.1.2 --- Real-valued Outputs  ---  p.34Chapter 4.2 --- Parameters Initialization  ---  p.37Chapter 4.3 --- Application of ForeNet: the Process of Time Series Prediction ---  p.38Chapter 4.4 --- Some Implications   ---  p.39Chapter 5 --- ForeNet: Initialization ---  p.40Chapter 5.1 --- Unfolded Form of ForeNet  ---  p.40Chapter 5.2 --- Coefficients Analysis  ---  p.43Chapter 5.2.1 --- ""Analysis of the Coefficients Set, vn  "" ---  p.43Chapter 5.2.2 --- ""Analysis of the Coefficients Set, μn(d) "" ---  p.44Chapter 5.3 --- Experiments of ForeNet Initialization  ---  p.47Chapter 5.3.1 --- Objective and Experiment Setting  ---  p.47Chapter 5.3.2 --- Prediction of Sunspot Series  ---  p.49Chapter 5.3.3 --- Prediction of Mackey-Glass Series  ---  p.53Chapter 5.3.4 --- Prediction of Laser Data  ---  p.56Chapter 5.3.5 --- Three More Series  ---  p.59Chapter 5.4 --- Some Implications on the Proposed Initialization Method   ---  p.63Chapter 6 --- ForeNet: Learning Algorithms ---  p.67Chapter 6.1 --- Complex Real Time Recurrent Learning (CRTRL)  ---  p.68Chapter 6.2 --- Batch-mode Learning  ---  p.70Chapter 6.3 --- Time Complexity  ---  p.71Chapter 6.4 --- Property Analysis and Experimental Results  ---  p.72Chapter 6.4.1 --- Efficient initialization:compared with random initialization ---  p.74Chapter 6.4.2 --- Complex-valued network:compared with real-valued net- work   ---  p.78Chapter 6.4.3 --- Simple architecture:compared with ring-structure RNN . ---  p.79Chapter 6.4.4 --- Linear model: compared with nonlinear ForeNet  ---  p.80Chapter 6.4.5 --- Small number of hidden units  ---  p.88Chapter 6.5 --- Comparison with Some Other Models  ---  p.89Chapter 6.5.1 --- Comparison with AR model  ---  p.91Chapter 6.5.2 --- Comparison with TDNN Networks and FIR Networks . ---  p.93Chapter 6.5.3 --- Comparison to a few more results  ---  p.94Chapter 6.6 --- Summarization  ---  p.95Chapter 7 --- Learning and Prediction: On-Line Training ---  p.98Chapter 7.1 --- On-Line Learning Algorithm  ---  p.98Chapter 7.1.1 --- Advantages and Disadvantages   ---  p.98Chapter 7.1.2 --- Training Process   ---  p.99Chapter 7.2 --- Experiments  ---  p.101Chapter 7.3 --- Predicting Stock Time Series  ---  p.105Chapter 8 --- Discussions and Conclusions ---  p.109Chapter 8.1 --- Limitations of ForeNet  ---  p.109Chapter 8.2 --- Advantages of ForeNet  ---  p.111Chapter 8.3 --- Future Works  ---  p.112Bibliography ---  p.11",ForeNet: fourier recurrent neural networks for time series prediction.,https://core.ac.uk/download/48534097.pdf,,,,core
197952730,1992-01-01T00:00:00,"In this thesis the application of artificial intelligence to monitoring and diagnosis of robotic spacecraft is discussed. Several software prototype systems were developed to serve as testbeds for the research and to evaluate the effectiveness of the approach against real problems and current techniques used in NASA\u27s planetary exploration program.  Software prototypes were used to investigate the verification of robot plan execution. New artificial intelligence algorithms for monitoring and diagnosis of robot systems were designed, programmed, and tested. These included plan analysis for monitoring, sensor planning, generation of expected sensor values, and diagnosis of execution failures caused by hardware, environmental or plan anomalies. Testing was performed on a laboratory telerobotic hardware testbed for satellite servicing and on a mobile planetary rover robot operating in natural terrain.  Artificial intelligence algorithms, software prototypes, and more advanced, operationally capable systems for monitoring ground support systems and actual spacecraft in flight were designed, programmed, and tested. A ground support system that served as one test domain was the mirror cooling circuit of the 25-foot Space Simulator at the Jet Propulsion Laboratory (JPL) in Pasadena, California. A prototype monitoring system for this device based on a theory of ""predictive monitoring"" was developed and tested. Mission operations for the Voyager II spacecraft served as another test domain for an intelligent spacecraft health-monitoring and diagnosis system. This system was successfully tested in support of telecommunications operations during Voyager II\u27s encounter with the planet Neptune in 1989. This was the one of the first artificial intelligence systems to be used in planetary spacecraft operations at NASA/JPL. Subsequently, this system was adapted and tested in support of operations of the Magellan spacecraft telecommunications subsystem and the Galileo spacecraft power and pyro subsystem.  Some of the specific artificial intelligence algorithms that were developed for monitoring and diagnosis included the use of heuristic and causal model-based reasoning techniques for predictive generation of sensor values, sensor selection planning, dynamic alarm limit checking, hierarchical procedure specialists for fault diagnosis, and integration of Al with conventional systems in full-scale monitoring and diagnosis applications.  In support of this overall program of research, novel software engineering tools for artificial intelligence research and application development were also developed and will be discussed in the thesis.  The application of artificial intelligence techniques to the monitoring and diagnosis of robotic space systems was shown to be very effective with specific benefits in the areas of systems autonomy, spacecraft safety, ground operations productivity and automation. As a result of this work in part, artificial intelligence is now considered by senior mission designers to be an enabling technology for on-board automation of planetary rovers and for automation in mission operations at the Jet Propulsion Laboratory",Artificial Intelligence for Monitoring and Diagnosis of Robotic Spacecraft,,,,,core
20938224,2002,"This paper presents our attempt to automatically define feedforward neural networks using genetic programming. Neural networks have been recognized as powerful approximation and classification tools. On the other hand, the genetic programming has been used effectively for the production of intelligent systems, such as the neural networks. In order to reduce the search space and guide the search process we employ grammar restrictions to the genetic programming population individuals. To implement these restrictions, we selected to apply a context-free grammar, such as a BNF grammar. The proposed grammar extends developments of cellular encoding, inherits present advances and manages to express arbitrarily large and connected neural networks. Our implementation uses parameter passing by reference in order to emulate the parallel processing of neural networks into the genetic programming tree individuals. The system is tested in two real-world domains denoting its potential future use",A Scheme for the Evolution of Feedforward Neural Networks using BNF-Grammar Driven Genetic Programming,,,,,core
24533986,1992,"Navigation is a fundamental human activity and an integral part of everyday life. Humans use their knowledge about previous experiences with geographic space to find their way. Considerable research in the areas of cognitive science, psychology, and artificial intelligence has been carried out to examine the means by which humans navigate. Humans need to abstract from reality and form concepts of space in order to reason about space or carry out tasks e.g., navigating. Concepts are used to organize space and to structure the perception of reality. They depend strongly on the knowledge of the human and on the task at hand. Humans may use multiple levels of abstraction, representing different parts of geographic space, to carry out a single navigation task. Although descriptions of spatial concepts already exist, they are defined in natural language and, therefore, lack the formality to be implemented. Conceptual modeling is a design technique that models the concepts of an application (e.g., highway navigation) on a highly abstracted level. Conceptual models provide the possibility to communicate spatial concepts in a formal and unambiguous way. For large and complex applications in Geographic Information Systems, conceptual modeling is a promisin",Conceptual modeling of highway navigation,,,,,core
24329260,1996,". This paper describes a novel qualitative navigation method for indoor environments, implemented on top of a commercial power wheelchair. Our approach is based on qualitative representations of variations in sensor behavior between adjacent regions in space. We use these representations to localize and guide planning and reaction. Off-line, the system accepts as input a topological diagram of the environment and generates a map based on a simple qualitative model of sensor behavior. During execution, the robot controller integrates this map into a reaction module. We have tested this architecture both in simulation and in a real wheelchair. Our experimental results show that the proposed method can safely navigate a real robot in indoor environments. 1 INTRODUCTION  Qualitative Reasoning seeks to develop qualitative models of physical systems [9]. Furthermore, recent AI research aims to develop computational theories of autonomous agents&apos; involvement in their environments [1]. This pa..",Qualitative Autonomous Navigation for Wheelchair Robots,,,,,core
201255634,2000-10-01T00:00:00,"The term 'temporal' in spatial analysis has a number of potential meanings, each of which requires an alternative approach for the provision of analytic support. Much of the present work in spatio-temporal information is concerned with transaction versioning. Object based representations often demand a high level of initial understanding of object relationships. Many GIS users are seeking to understand the object relationships over time, past, present and future; their research focus is how real-world features interact in time and space. Despite this requirement, little present work will support this requirement to understand the drivers of change, rather than simply to report what changed. A number of workers have / are attempting to formalise a theory of spatio-temporal reasoning (eg Hermosilla, 1994, Qian, et al, 1997, Claramunt et al, 1997), in the most part working from a theoretical abstraction. Worboys, 1998, uses a problem oriented approach, as does Halls and Miller (1995, 1996). Representation of change over time by means of spline curves offers possibilities for this type of work, neural networks are explored as an implementation solution. We show that the AURA neural network architecture offers particular hope and that the proposals of Yeh &#38; de Cambray and Halls &#38; Miller need to be recast in terms of the AURA architecture",A new approach to the spatial analysis of temporal change using nodes and neural nets,,'OpenEdition',10.4000/cybergeo.911,"[{'title': 'Cybergeo', 'identifiers': ['issn:1278-3366', '1278-3366']}]",core
102671191,1998,"Abstract. Off-road autonomous navigation is one of the most difficult automation challenges from the point of view of constraints on mobility, speed of motion, lack of environmental structure, density of hazards, and typical lack of prior information. This paper describes an autonomous navigation software system for outdoor vehicles which includes perception, mapping, obstacle detection and avoidance, and goal seeking. It has been used on sev-eral vehicle testbeds including autonomous HMMWV’s and planetary rover prototypes. To date, it has achieved speeds of 15 km/hr and excursions of 15 km. We introduce algorithms for optimal processing and computational stabilization of range imagery for terrain map-ping purposes. We formulate the problem of trajectory generation as one of predictive control searching trajectories expressed in command space. We also formulate the problem of goal arbitration in local autonomous mobility as an optimal control problem. We emphasize the modeling of vehicles in state space form. The resulting high fidelity models stabilize coordinated control of a high speed vehicle for both obstacle avoidance and goal seeking purposes. An intermediate predictive control layer is introduced between the typical high-level strategic or artificial intelli-gence layer and the typical low-level servo control layer. This layer incorporates some deliberation, and some envi-ronmental mapping as do deliberative AI planners, yet it also emphasizes the real-time aspects of the problem as do minimalist reactive architectures",Terrain Autonomous Mobility - Part 2: An Active Vision,,,,,core
481647398,1998-03-01T08:00:00,"There is a growing consensus among computer science faculty that it is quite difficult to teach the introductory course on Artificial Intelligence well [4, 6]. In part this is because AI lacks a unified methodology, overlaps with many other disciplines, and involves a wide range of skills from very applied to quite formal. In the funded project described here we have addressed these problems by: Offering a unifying theme that draws together the disparate topics of AI; Focusing the course syllabus on the role AI plays in the core computer science curriculum; and Motivating the students to learn by using concrete, hands-on laboratory exercises. Our approach is to conceive of topics in AI as robotics tasks. In the laboratory, students build their own robots and program them to accomplish the tasks. By constructing a physical entity in conjunction with the code to control it, students have a unique opportunity to directly tackle many central issues of computer science including the interaction between hardware and software, space complexity in terms of the memory limitations of the robot\u27s controller, and time complexity in terms of the speed of the robot\u27s action decisions. More importantly, the robot theme provides a strong incentive towards learning because students want to see their inventions succeed. This robot-centered approach is an extension of the agent-centered approach adopted by Russell and Norvig in their recent text book [11]. Taking the agent perspective, the problem of AI is seen as describing and building agents that receive perceptions as input and then output appropriate actions based on them. As a result the study of AI centers around how best to implement this mapping from perceptions to actions. The robot perspective takes this approach one step further; rather than studying software agents in a simulated environment, we embed physical agents in the real world. This adds a dimension of complexity as well as excitement to the AI course. The complexity has to do with additional demands of learning robot building techniques but can be overcome by the introduction of kits that are easy to assemble. Additionally, they are lightweight, inexpensive to maintain, programmable through the standard interfaces provided on most computers, and yet, offer sufficient extensibility to create and experiment with a wide range of agent behaviors. At the same time, using robots also leads the students to an important conclusion about scalability: the real world is very different from a simulated world, which has been a long standing criticism of many well-known AI techniques. We proposed a plan to develop identical robot building laboratories at both Bryn Mawr and Swarthmore Colleges that would allow us to integrate the construction of robots into our introductory AI courses. Furthermore, we hoped that these laboratories would encourage our undergraduate students to pursue honors theses and research projects dealing with the building of physical agents",A Robot Laboratory For Teaching Artificial Intelligence,,'Transformative Works and Cultures',,,core
323904252,2002-01-01T00:00:00,"To date there has only been one implementation of Holland's Learning Classifier System (LCS) on real robots. In this paper the use of Wilson's ZCS system is described for an obstacle avoidance task. Although the task is simple it does present some advances and change of emphasis over the previous LCS robotic implementation. The controller model is ""event"" based. Instead of the robot being assigned fixed length actions, continuous actions are taken. These actions are taken until an ""event"" occurs. An event can be thought of as a change of state. This division of the world into states is usually part of the problem description, and to do this automatically is currently one of the challenges facing machine learning. The paper then introduces TCS, a form of ZCS that attempts to address this issue. LCS have the ability to generalise over the state action-space. In TCS this generalisation ability can also be used to determine the extent of this space. TCS also implements components from SMDP reinforcement learning theory to weight the influence of time on the reward functions of the LCS. A simple light-seeking task on the robot platform using TCS is presented which demonstrates desirable adaptive characteristics for the use of LCS on real robots",ZCS and TCS learning classifier system controllers on real robots,,,,,core
57031841,2002,"Reinforcement learning algorithms without an internal world model often suffer from overly long time to converge. Mostly the agent has to be successful a several hundred times before it could learn how to behave in even simple environments. In this case, a world model could be useful to reduce the number of real world trials by performing the action virtually in the world model. This may help to propagate the Reinforcement Q- or V- values much faster through the state (action) space and could be interpreted as a simple form of planning. In the following investigation we introduce a self organizing deterministic world model “DIVA” (“Discretization Improvement by Variance reduction”) with an adaptive discretization, which can speed up learning by using common methods like Suttons Dyna-Q. Proposed in this article, the “DIVA”-model is implemented in a six legged walking robot, which learns how to walk in a minimum of time and with a minimum of real world moving trials",DIVA: A self organizing adaptive world model for reinforcement learning,,,,,core
23916409,1998,"This report is a Master&apos;s Thesis on a new data structure called difference decision diagrams and consists of four parts: A presentation of a first-order logic for difference constraint expressions. Difference constraint expressions are Boolean combinations of difference constraints, x-y&lt;c and x-y&lt;=c, over integer or real-valued variables. We prove that determining satisfiability of quantifier-free difference constraint expressions is NP-complete. For quantified difference constraint expressions the problem is PSPACE-hard. A specification of difference decision diagrams (DDDs). DDDs are a data structure to represent difference constraint expressions, and combine symbolic representation of states known from BDDs with timing constraint information. We use a built-in feasibility check to make DDDs semi-canonical. Semi-canonical DDDs make testing of functional properties (such as satisfiability and validity) constant time operations. This part furthermore contains a thorough description of the core algorithms to construct DDDs. A number of applications of difference decision diagrams. We show how to model timing constraints of combinational circuits, and we present a DDD-based algorithm to compute the reachable state space of a timed Petri net. Furthermore, we show how to verify non-Boolean properties of an imperative programming language. An implementation of difference decision diagrams. This part contains a C library with a Moscow ML interface, and a number of example programs",Difference Decision Diagrams,,,,,core
4751372,2001-06-01T00:00:00,"Presented at the 6th International Symposium on Artificial Intelligence and Robotics and Automation in Space: i-SAIRAS 2001, Canadian Space Agency, St-Hubert, Quebec, Canada, June 18-22, 2001.This paper presents a new strategy for autonomous navigation of eld mobile robots on hazardous natural terrain using a fuzzy logic approach and a novel mea- sure of terrain traversability. The navigation strategy is comprised of three simple, independent behaviors: seek-goal, traverse-terrain, and avoid-obstacle. The recommendations from these three behaviors are com- bined through appropriate weighting factors to gen- erate the nal steering and speed commands that are executed by the robot. The weighting factors are pro- duced by fuzzy logic rules that take into account the current status of the robot. This navigation strategy requires no a priori information about the environ- ment, and uses the on-board traversability analysis to enable the robot to select relatively easy-to-traverse paths autonomously. Field test results obtained from implementation of the proposed algorithms on the commercial Pioneer AT rover are presented. These results demonstrate the real-time capabilities of the terrain assessment and fuzzy logic navigation algorithms",Terrain-Based Navigation of Planetary Rovers:  A Fuzzy Logic Approach,https://core.ac.uk/download/4751372.pdf,Canadian Space Agency,,,core
235573821,2001-02-21T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH NEWS ! i
Published Weekly Since 1932 by
The Magrath Trading Company
35t February 21,2001
Zeniths win Luther Tournament
The Zeniths traveled to Regina over the weekend and won the
prestigious *49 annual Luther Invitational Basketball Tournament,
beating Regina Winston Knoll Wolverines 87-84 in the championship
game. Jimmy Balderson led the points for the Zeniths in the final game
with 34 points and Allen Tollestrup added 21.
Magrath has only won the Luther Tournament twice in its
history. The other was in 1963.
Jimmy was named tournament most valuable player with a total
of 107 points in three games. This is the second highest point total ever
recorded over three games with the highest being 112 made by
Raymonds, Richard Bohne in 1989.
Magrath advanced to the final by opening with a 78-60 win over
the host Luther Lions. The dub then posted a 72-61 triumph over
Shddon-Williams Spartans of Regina in the semifinal game.
The Zeniths have had a great season this year with a lot of
exciting games. There is another exciting game coming up this Friday,
March 23 in Cardston. Also, you wont want to miss the playoffs to be
held at the Enmax Centre in Lethbridge on March 7-10. Come out and
support the Zeniths.
Points of Interest
* Sports
* Case Lot Sale
* Grocery Specials
* Home Hardware
Sales
Make your dream come true
have your home built by
JENSEN ENTERPRISES
New construction:
Additions:
Patio Decks:
403-758-3669
Ken Jensen, Owner
r i
Inside this issue:
Community Interests 1-3
Sports 4
Classified Ads 6-7
Community Calendar 8
Grocery Specials 9-10
Meat & Produce 11
Hardware Specials 12
k
Finewoodwork:
Hardwood furniture:
Renovations:
Box 93
Magrath, AB TOK 1J0
f LADIES '
| The Croft Niqht that was formerly held |
•at the Lutheran Church will now be held at.
I Maria Passey's the third Wednesday of each |
jmonth. ‘
■ »
| Any info, you need phone 758-3240 -Betty j
■ or758-3569-Darlene .
V. ""EVERYONE WELCOME!"" /
Mag rath United Church
Pancake Supper
Tuesday, February 27th
5:00 - 7:00 p.m.
Adults & Children $4.00 each
6 & under eat free
or $15.00 per family
Come enjoy the food & fellowship!
Proceeds go to the Magrath United Church Sunday School.
Cardston Lutheran School
A Community Forum meeting to discuss the
feasibility of a Lutheran School in Cardston will
be held Thursday, February 22 - 7:30 p.m. -
Cardston Faith Lutheran Church basement.
Welcoming enrollment
from all cultures and denominations.
The South Country Jamboree Society
will be holding their regular jam at the
702 Wing on
Sunday, February 25th at 2:00 p.m.
Meeting at 12:30 p.m. Everyone welcome.
Phone 327-8477 or 330-4098.
to- Gfaputa,
The parents of Chynna Johnston wish to say
how proud they are of her being named
Student of the Month for January at Robert
Warren Junior High School in Calgary. Chynna
was awarded this honour after being runner-up
in November. The school has 164 students in
Grade 7 and the judging criteria includes the
areas of supporting school spirit, positive peer
relationships, leadership, attitude, helpfulness,
and academic achievement. Chynna is the
only student in Grade 7 asked to work in the
school’s store and she has a regular shift in the
office where she answers the phones and fulfils
other receptionist duties. She ran on the cross
country team last fall and plays flute in the
junior band. She is also an original member of
the Homework Club. Well done Chynna! Keep
doing your best and remember we both love
you very much!
Mifemtm
patterai mi
ttelJHBfitf
Lance Miller
Agent
Bub. 3284411
Fax. 3204427
• You are invited to a fun evening...
! Late Night Shopping
in Del Bonita!
: Tupperware, Candle lights,
I Pampered Chefs, Mary Kaye,
= Ufana (health), Jewellry, underthings,
I New Balance clothes.
: Thursday, February 22nd
from 4 p.m. til closing
l at Kelly Newton’s (across from the
L.D.S. Church in Dei Bonita).
i Hosted by Kelly & Syd
♦
I■■
I
■■
I■■
I□■
I■
■
I■
I
We specialize in:
• Video slide shows (weddings,
reunions, sports highlights,
the Web (Output format:
VHS tapes CD)
• Hi-Res Photo scanning/Repair s
Retouching/Printing (colors B/W),
OCR (text)
• Banners, Posters, Ads, etc.
• Private tutoring
• Computer troubleshooting
• Desktop Publishing
Phone Bonny/Brenda
758-3844 (evenings)/
leave a message (day)
Cards of Thanks
Norman Christensen’s family would like to express thanks to - families, friends & neighbors for
-
prayers, calls, cards, food, flowers and donations we received during this difficult time. They were greatly appreciated.
To Dr. Gentlemen. Tollestrup, Dr. Ken Dahl and the staff at the Magrath Hospital we thankyou for the kind loving careyou gave Norm while he was there.
A special thanks to Joe Keeler for his tribute
-
to our family for honoring a Great Guy by doing such a great Job with their parts in the Service.
To the honorary pallbearers and to the many people who took time to attend the Service - We so appreciateyou.
A gracious thankyou to Max and staff of Christensen Funeral Home for the caring, professional manner they took care of all our needs.
Melva, Joan, Barrie, Kim, Joy and Families
SENIOR CENTRE NEWS
Suppers will be held Februay 28th Pot Luck - Februay 23~ All at 5.00 p.m. Senior Centre Your Hostesses for Februay are Bernice Coleman, May Tanner & Evie Hillmer.
*******
Contact Jean or Jack Butlin for rental of the Seniors Centre. Fee $75.00 Phone 758-3030
*******
BINGO*BINGO
Bingo is Thursday, March Ist at the Seniors Centre
Doors open at 6:30 p.m. Bingo at 7:00 p.m. Public Welcome!!
GEORGE BURGER
We would like to express our sincere gratitude to our friends for the many cards, food, visits, flowers, prayers and memorial donations we received during this difficult time.
Also special thanks to:
-
the Doctors (especially Dr. Regehr), nurses, caregivers and staff at the Magrath Hospital & Clinic and the Magrath Ambulance Crew for their excellent care.
-
Pastor Brian Amison, Organist Joy Johnson, Soloist Cheyl Walburger for their part in the service.
-
All those who attended George’s Memorial Service and the U.C.W. Ladies and friends who helped out and supplied the luncheon.
Your thoughtfulness and kindness will always be remembered.
Sincerely,
Lila & Family
SENIOR NEWS
We will be going to Stage West in Calgay on Wednesday, March 21st.
Matinee for luncheon & stage play “OVER THE RIVER & THROUGH THE WOODS"" Members - $45.00 Non-members - $55.00
ALSO
“THE BLUE CASTLE”
is a play adapted from L.M. Montgomey’s book. March 22-31
Thursday, March 29th has been reserved for the Magrath Seniors.
Cost: $25/person includes dinner & theatre. Bus cost will be extra.
Call now to make your reservations. Phone: Levaun Thompson - 758-6672 Hazel Rasmussen - 758-3545 Norma Owens - 758-3560SPORTS I
SOCCER ~l
SOCCER REGISTRATION .
Ages 4-U13 play in Magrath
$35 per player, $105 familyof3 or more
U15 & U19 boys play in Lethbridge
$70 per player
(teams playing in Lethbridge must be submitted by April 1)
Coaches and referee registration
Place: Magrath Pool Foyer
Dates: Thurs. March 15 7:00pm - 9:00pm Sat. March 17 9:00am- 12:00pm
Thurs. March 22 7:00pm-9:00pm _____|
; CURLING
Magrath Ladies Open Bonspiel
February 22 to February 28, 2001
■ All teams eligible for the Investors Group, Ken Leavitt & Mike Holt “Draw to the Button” Challenge Possible Prize: $5,000.00
Entry Deadline: February 20,2001 Phone entries to:
Sandy Meldrum: 758-3697 :
Lindy Oliver: 758-6521
Rod & Gun Club Annual Banquet & Awards Night March 3,2001
United Church Tickets $10.00 each - Juniors $5.00 each Tickets at Trading Co. Hardware EVERYONE WELCOME!
SKATING
The Skating Rink is closing, with the last day being February 25*.
BASKETBALL
Pandas and Cubs
Last Tuesday the Pandas upended Raymond Comettes 54-42 with Joan Leishman scoring 14 points and Cassy Foggin 10.
The Cubs also came out on top with a score of 47-29 with Lauren Balderson and Bonnie Wilde scoring 12 and 11 respectively.
On Friday the girls went to Winston Churchill where the Pandas suffered a defeat of 64-57 against the Griffins. Danielle Wilde made 18 points for the Pandas with Kelsey Helgeson and Joan Leishman adding 10 points each.
The Cubs, on the other hand, were able to come out victorious with a score of68-27 as Lauren Balderson hit for 16 points. Debbie Balderson made 13 points and Bonnie Wilde 12.
Saturday afternoon the Pandas and Cubs hosted Cardston Cougars. The Pandas put up a good fight but were not able to defeat the Cougars as the Cougars won 78-67. Kelsey Helgeson made 24 points while Joan Leishman and Cassy Foggin added 11 and 10 points respectively.
The Cubs earned a 70-56 victory over Cardston with Debbie Balderson scoring 26 points. Elaine Woolf added 12 points and Lauren Balderson 10.
The girls next game is Saturday, March 3 in Medicine Hat.
Please submit your sports scores and schedules by dropping off at the Trading company office, faxing to 758-6888, or emailing to tidmarsh@telusplanet.net._________Adult Education Computer Courses
January-March 2001
g Instructors: Bonny West/Brenda Beck
I MARCH
' Cardston: Microsoft Word-Intermediate
This 3 week will cover creating and editing tables, using Word’s drawing tools, adding headers, footnotes,
and endnotes, importing files, modifying templates, mailing labels, customizing bullets and special characters,
adding web links, and additional editing features.
Dates: Tuesdays, March 6-20 (3 weeks) Time: 7:00-9:30 p.m.
Location: Cardston High School Tuition: $35
Magrath: Make Your Own Memory Video
. Using slides, photos, scanners, video dips, background music, voice annotations, transitions, and easy to use
Software, it is now possible to create “living” video scrapbooks for home movies, weddings, family reunions, baby
books, gifts, sports highlights, etc. Participants will create their own personal memory video output to VHS tape.
(These movies may also be stored on CD, on the Web, o r sent via email.)
Participants need to bring their own photographs & video clips and choice of badcground music (preferably
CD). Additional photos and music and blank tapes will be provided.
Dates: Wednesdays, March 7-28 (4 weeks) Time: 7:00-10:00 p.m.
Location: Magrath Elementary School Tuition: $65
To register call Kathy Richards @ 653-4991
IDA MAY COX PILLING
passed away in Cardston of February ,*19 2001 at
the age of 94 years. She was predeceased by her
husband Owen Leonard Pilling in 1990 and two
infant children: Freeman in 1935, and Kay LaRene
in 1938. She is survived by six children - Maxine
(Ronald) Rodgers of Cardston, Gaelynd (Norma)
Pilling, and Merna (Reed) Coleman of Magrath,
Dale (Karen) Pilling, and Curtiss (Janice) Pilling of
Leavitt, and Norma (Al) Hatch of Valleyview, AB; 30
grandchildren; and 59 great grandchildren. Also
surviving is her sister - Birdie Lybbert of Spokane,
WA; brother - Myron (Virgie) Cox of Buhl, ID; sisters-in-
law - Essie Cox of Blairmore, AB and Dulcie Cox
of Brooks, AB.
The Funeral Service will be held at The
Church of Jesus Christ of Latter-Day Saints, Temple
Street Chapel, Cardston on Thursday, February
22nd at T.00 p.m. With Bishop Michael Nelson
officiating. Friends may meet the family at the
Church from 11:30 a.m. to 12:40 p.m. prior to the
service. Interment in the Leavitt Cemetery.
WWW
FEBRUARY Is
HEART & STROKE
MONTH
To Magrath & District Residents:
The canvas by the Magrath Hospital
Auxiliary is now under way with a total
of 48 volunteers involved.
We ask that when called on,
you would please welcome them.
Every dollar will help.
Please consider a donation.
Thanks,
Magrath Hospital Auxiliary
RE^tlX real estate
John Latham
Associate Broker
- magrath 758-6060
TOP CLASS ACREAGE
1444 SQ.FT. BUNGALOW - $89,900
Don’t wait around, this one won’t wait. -
Call John immediately, mls#210241
This 25 year
bungalow is
definitely priced
right. Owners
are moving to
another city.
Large Luxury Bungalow (2195 sq.ft.) on 5 acres
nestled in mature trees. 5 acres of irrigation water
rights. On town water. This is a must to see. For
details call John at 758-6060. mls #210402
$79,900
4 Bedroom Bungalow with Front Drive Garage
Beautiful
character home
fully renovated
from top to
bottom. Only
asking $99,900.
For more details
call John at 758­6060.
MLS #202461
Excellent starter home. Priced to sell. Large flat lot in
nice location. Call John at 758-^60^^1-521^77^^^^
BUY & SELL
■'i'
* Want to Lower your Natural Gas or Electric Bills?
Invest in a “Magic Cooking Box”! $35.°° each.
Just bring water to a boil for a few minutes. Turn
your stove off & place your pot in your “Magic
Cooking Box“. Your food will finish cooking in
your Box. Phone Pat Harrison - 758-3714.
* FOR SALE - 9 cu. ft. Chest Freezer - $85- Over
Head Garage Door Opener - $45. New motorized
Pea Podder - $100. Interior Mahogany doors plus
hardware, very good-$10 each. Garden Potting &
Trays. Ph. 758-6339.
* FOR SALE - Ice House - 4 man. Used twice.
$250. 758-6360, space #17 Trailer Park.
BUSINESS
* TUPPERWARE
Have defectives? Need replacements?
Call Heather MacKay at 758-3662.
All orders receive discounts, no pressure, last
order. Call before March 7*.
* FOR SALE - Home raised, grain fed beef. Ready
now for slaughter. Split one with a friend.
Thomson Livestock Ltd.
Merrill Thomson - 758-3209.
* REMEMBER TO CALL TAI
Heating, Air conditioning, Refrigeration, and
Appliance Service, Gas & Electrical Service and
Trenching. 752-3866.
* If your child needs help in Math or Reading the
Kumon Method might help. For more
information call Rusty or Martine Rollingson
at 758-3648.
CLASSIFIEDS
* Jeanie’s Hair Fashion
136 S. 1 St. W. - four doors south of the Trading Company. 758-3379. Open Tues. - Fri. Professional Hair Care at pleasing prices.
*
CANADIAN SECURITY SYSTEMS
We sell, install & service alarms, safes, camera systems, deadbolts & key locks. Call Ross Moore at 758-3945. Free estimate.
*
For all your cleaning needs from hospital clean to a touch up, carpet to ceiling & everything in between. No job too big or too small.
Call Waynes Carpet & Upholstery Cleaning 758-6414.
*
Will Tutor, grade 1-12 Math or Chemistry.
Call Martine Rollingson at 758-3648.
USED VEHICLES
ÇJ 1-^5 A-;'* A-;1 JA-: eT-P J L—
* 2 bedroom apartment. For information call 758-3876.
k FOR SALE - 1999 Ford XLT loaded short box 4x4. 30,000 km. Offers. Call Codey - 758-3395 days or 758-3767 evenings.
REALESTATE
FOR RENT
*
Storage space & Commercial space for rent. For information call 758-3876.
*
2 Bedroom House - including washer/dryer, stove & refrigerator. 122 Civic Ave. W. Abstainers. Phone 758-3082.
*
2 bedroom, furnished apartment. No pets. Abstainers. Call Ty Alston - 758-3322
*
Small 2 bedroom home. Private. Treed lot under the hill. Inexpensive. 758-3700. Available now!!
*
4 bedroom, 2 story home. Close to school and LDS stake center. 1 Vi bath, living & family room, utility room. Fridge & stove included. Abstainers. No pets. 758-3700. Leave message.
FOR SALE
*
Character Home in Magrath. 1300 sq. ft. Fully renovated. Loads of oak throughout main floor. 9 ft. ceilings. Large country kitchen with oak cabinets. Finished basement. 25’x24’ heated garage with finished loft. Landscaped lot with mature trees. A must to see. Call collect for Viewing and more details - (403) 227-2611.
*
3 bedroom, 3 bathrooms. 1250 sq. Ft. on main floor. Treed lot - Fully developed basement. Call 758-6991.
*
5 bedroom, 3000 sq. Ft. Home on 1 acre.
2 bath, 2 family rms (one with wood burning stove), living rm, dining rm, oak in kitchen.
A steal at $132,000. Phone 758-6789.
*
House for sale - all brand new inside when finished renovations. For complete details See “www.retailcontrols.com/house” Asking $49,000 when complete.
*
New 1230 sq. ft. home. 3 bedrooms, 2 bath, garage, fridge, stove. No GST.
Phone 758-6835 or 758-3446fMGMTHNEWS
In Our Community... February2001
Sunday Monday Tuesday Vvfednesday Thursday
-- —------- -r
Friday ! Saturday
18
19 H
FAMILY DAY
HOLIDAY
20 21 22 23
Seniors Ftot Luck
5:00
BASKETBALL
Boys at
Cardston
24
NO SCHOOL FOR STUDENTS THIS WEEK!
25 26 27
I
§8
Seniors Supper
5:00
1
BINGO
2
BASKETBALL
Boys host Kate
Andrews
3
BASKETBALL
Boys at
Raymond
LIBRARY NEWS
We are still offering introductory computer skills, covering word processing, internet and
email. There is no charge and you do not have to be a library member. For further information
phone us @ 758-6498. . . .
Besides pre-school books, the library has special pre-school reading and activity kits that can
be checked out.
We have a used Heintzman piano for sale.
Please note that the deadline for submissions to the paper is MONDAY at 6:00 p,m. Entries submitted
after the deadline will be published the following week. Phone 758-6377, Fax 758-6888 or drop off your
submissions at the Magrath Trading Company Office.
Jtagrath trading Company
GBOCCBB SPUIAZS
“3rom Our Janfly to tymin...”
9
Dairy Delights and frozen Jawrites!
Western Family Margarine, parchment - 454 g
Case of 12
$8.98
Good Humor Ice Cream - select varieties
4 litre pail
$3.98
Dairyland Sour Cream
750 mL
$2.88
Cheemo Perogy, frozen - select varieties •
2 kg
$5.68
Pillsbury Pizza Pops, frozen - select varieties
1 -2 kg
$8.98
Five Alive Chilled Citrus Beverage - select variety s 1.89 litre
$2.48
Dairyland Multi-pack Yogurt - select varieties
8x 125g
$2.98
Dairyland Milk 2 GO - select varieties
500 mL
$1.08
Tampico Chilled Punch - select varieties
4 litre
$2.88
Western Family Vegetables, frozen - select varieties
1 kg '
3 for $5.88
Niagara Orange Juice - frozen, 341 rm
Case of 12
$10.88
Kellogg’S Eggo Waffles - select varieties
48 pack
$8.98
Groceries
McCain Punch - select varieties - 200 mL
Case of 24
$5.98
Green Giant Vegetables, select varieties- 341-398 rm
Case of 12
$7.98
western Family Flour - an purpose
10kg
$3.98
Ocean’s Chunk Light Tuna, in water -170 g;
pack of 10
$6.98
Kraft Dinner - 225 g
Case of 12
$7.98
western Family Coffee - select varieties
300 g
6 for $12.98
Alpha or Pacific Evaporated Milk - 385 mL
Case of 12
$ 1 1.98
Western Farpily Apple Blends -1 litre
Case of 12
$8.98
Kellogg’s Mini Wheats, jumbo
13OOg
$7.98
Del Monte Peach Fruit Cups
16 pack
$10.98
Western Family Coffee - select varieties
300 g
6 for $ 12.98
Dasani Water
6 x 500 mL
$5.48
Heinz Tomato Juice, 284 mL
Case of 12
$8.98I kJ
Jlore Grocery Specials...
Betty Crocker Frosting - select varieties
340-450 g
2 for $4.00
Betty Crocker Cake Mix - select varieties
510g
3 for $3.99
Dare Simple Pleasures Cookies - select varieties
300-350 g
$2.28
Hunt’s Pudding Snacks - select varieties
12 pack
$3.98
Kraft Handi Snacks - select varieties
696 g
$10.98
Hostess Super Size Potato Chips - select varieties
410-475 g
$2.98
Old Dutch Potato Chips -select varieties
300 g
2 for $4.00
Old Dutch Variety Pack Potato Chips - select vari< ties3O x 32 g
$9.98
Hunt’s Tomatoes - select varieties, 398 mL
Pack of 6
$4.98
Hunt’s Tomato Sauce - 398 mL
Case of 12
$8.98
Hunt’s Tomato Paste - select varieties, 156 mL
Case of 12
$7.98
Hunt’s Thick & Rich Pasta Sauce - 68o mL
Case of 12
$ 19.98
Campbell’s Vegetable or Chicken Noodle soup
Case of 12
$8.98
Heinz Beans - select varieties, also kidney beans
Case of 12
$8.98
Heinz Pastas - select varieties, 398 mL
Case of 12
$9.98
Value Priced Noodle Cups - select varieties, 64 g
case of 12
$7.98
Catelli Pasta - select varieties
4-kg
$6.98
Western Family Mushrooms - pieces & stems, 284 n l Case of 12
$7.98
Dempster’s Sliced Bread - sesame white or 100% whc e 680 g
$1.98
Household Items...
Scotties Facial Tissue, iso’s-muitt-pack
Pack of 6
$5.88
Viva Paper Towels, white - 2 roll pack
Case of 12
$ 1 1.98
Purex Bathroom Tissue - regular 24 roll
Case of 2
$ 14.98
ABC Laundry Detergent - select varieties, 34 37 uses
Case of 4
$23.88
Western Family Winshield Washer Fluid - 4 litre
Case of 4
$7.88
Electrasol for Dishwashers
4 kg pail
$8.98
Pears Shampoo or Conditioner - select varieties
300 mL
$1.8811
JHore Household Items...
Western Family Dog Food - select varieties, 624 g
Case of 12
$ 1 1.98
Mainstay Dog Food
18kg
$ 1 1.98
Friskies cat Food - select varieties, I56g
Case of 24
$10.98
Mainstay Cat Food
8kg
$8.98
Value Priced Cat Litter
10kg
$3.98
JlABKtt JBSSB PBODUCC!
Sunkist Navel Oranges - California grown
8 lb bag
$3.98
Texas Sweet Red Grapefruit - Texas grown
5 lb bag
$3.48
Russet Potatoes, Canadian grown
20 lb bag
$3:98
Medium Carrots, u.s. grown, #i grade
5 lb bag
$2.48
Medium Onions. U.S. grown, #1 grade
5 lb bag
$2.48
BMCCHCB’S BUBS 07 ZBS WttK!
Lean Ground Beef, value pack 4-e ibs
$4.37/kg
$1.98/lb
Pork Loin Chops, rib or tenderloin ends - value pack 3-4
bs $5.47/kg
$2.48/lb
Boneless Chicken Breasts, skinless - iqf
1 kg
$8.78
Western Family Sliced Meats - select varieties
175 g
$1.28
Burn’s Wieners
450 g
$2.28
Burn’s Cooked Ham
375 g
$3.28
Burn’s Sliced Bacon
500 g
$3.48
Burn’S Breakfast Grill - select varieties
375 g
$3.48
Burn’s Bologna
500 g
$2.98
Western Family Beef Burgers, frozen
4.4 kg box
$19.98""Home of the Handyman""_________
Unisex Digital Wrist Watch
Only $3.49
Deluxe Pocket Watch with Fob
$29.99
Cattlemen!
Please order special number
ear tags now.
50 cc Roux
Automatic Syringe
for the Cattlemen
Reg. $54.99
Special $39.99
Crosley Gas 30 inch - white
Gas Range ..
Sealed burners - 2 high speed burners
Regular $999.00
Save $50 - $949.00
Old fashioned Big Ben Radio
Black in color with alarm
Reg. $49.99
Save $10 - Special $39.99
Easy chair Magazine & remote
control Holder........
Regular $19.99
Sale Price $13.99
Ford Motorcraft
Anti-freeze
Reg. $8.95/gallon
Sale Price $7.95
\Ne have two
Garden Seed
racks come in.
We have several
Microwaves
to choose from.
We have several boxes of
Valentines left.
Now V2 Price!
We have some
Toboggans & GT racers
left
Orville Redenbacher's / ■ —
blip Microwave Popcorn 1
v—.J Low Fat or Buttery J
Regular $449 SncSHre
Special $3","Magrath Store News (February 21, 2001)",,J. A. Ririe,,,core
324086752,2000-01-01T00:00:00,"We conclude that precursory quiescence episodes
are a real feature of the Central Apennines seismicity,
but it is difficult to define a simple hypothesis
which applies to the generality of cases and can be
tested before implementation in a system of earthquake
countermeasures. The quality of the results are
affected by the circumstance that three of the four
main shocks in the study area, occurred in the early
years of observation, when the completeness at low
magnitude levels was not total. We are confident that
in future, quiescence episodes or other kinds of seismicity
patterns will be more clearly recognized as it
was the case for the Colfiorito 1997 earthquakesThe variations of seismicity rate in Central Apennines prior to the sequence started in September, 1997 (at 00:33 UTC, ML5.6) has been analysed by statistical methods, with the purpose of pointing up eventual periods of quiescence.
The analysis was carried out on the instrumental catalogue of the Istituto Nazionale di Geofisica (ING),
covering the period from January 1975 to March 1998. In a preliminary phase, the catalogue was declustered using the Reasenberg algorithm. After that, eventual magnitude shifts due to variations in the modalities of observation have been individuated and corrected. The subsequent analysis, carried out making use of the Zmap software package, has put in evidence that the sequence of September 1997 was preceded by a 2.5 year period characterised by absence of events of magnitude larger than 3.2, in an area approximately 20 x 40 km wide, including the epicentre of the main shock. The statistical methodology shows that only 1/103 of the space-time volumes analysed in this study, exhibited quiescence of the same level. The study of seismicity rate change correlated to previous main shocks in a larger area of Central Apennines shows that none of them were preceded by a seismic quiescence, specially close to the epicentre of the main shock, and lasting until the time of occurrence of the main shock as in the 1997 case. Actually, we found other patterns of precursory quiescence with different time or space distribution.
We conclude that precursory quiescence is a real feature of Central Apennines seismicity, but it is difficult to define a simple hypothesis, which applies to the generality of cases and can be tested before implementation in a system of earthquake risk mitigationPublished435-4496T. Studi di pericolosità sismica e da maremotoJCR Journa",Statistical assessment of seismicity patterns in Italy: Are they precursors of subsequent events?,,,,,core
42783398,"Oct 1, 1994","The Illinois Computing Laboratory of Aerospace Systems and Software (ICLASS) was established to: (1) pursue research in the areas of aerospace computing systems, software and applications of critical importance to NASA, and (2) to develop and maintain close contacts between researchers at ICLASS and at various NASA centers to stimulate interaction and cooperation, and facilitate technology transfer. Current ICLASS activities are in the areas of parallel architectures and algorithms, reliable and fault tolerant computing, real time systems, distributed systems, software engineering and artificial intelligence",Current research activities at the NASA-sponsored Illinois Computing Laboratory of Aerospace Systems and Software,https://core.ac.uk/download/pdf/42783398.pdf,,,,core
20851133,1997,"this article, we describe and develop methodologies for mod- eling and transferring human control strategy (HCS). This research has potential application in a variety of areas such as the Intelligent Vehicle Highway System (IVHS), human-machine interfacing, real-time training, space telerobotics, and agile manufacturing. We specifically address the following issues: (1) how to efficiently model human control strategy through learning cascade neural networks, (2) how to select state inputs in order to generate reliable models, (3) how to validate the computed models through an independent, Hidden Markov Model-based procedure, and (4) how to effectively transfer human control strategy. We have implemented this approach experimentally in the real-time control of a human driving simulator, and are working to transfer these methodologies for the control of an autonomous vehicle and a mobile robot. In providing a framework for abstracting computational models of human skill, we expect to facilitate analysis of human control, the development of humanlike intelligent machines, improved human-robot coordination, and the transfer of skill from one  human to anothe","Human Control Strategy: Abstraction, Verification, and Replication",,,10.1109/37.621469,,core
71181670,1998-02-01,"This paper presents several upper and lower bounds for the number-of-bits required for solving a classification problem, as well as ways in which these bounds can be used to efficiently build neural network chips. The focus will be on complexity aspects pertaining to neural networks: (1) size complexity and depth (size) tradeoffs, and (2) precision of weights and thresholds as well as limited interconnectivity. They show difficult problems-exponential growth in either space (precision and size) and/or time (learning and depth)-when using neural networks for solving general classes of problems (particular cases may enjoy better performances). The bounds for the number-of-bits required for solving a classification problem represent the first step of a general class of constructive algorithms, by showing how the quantization of the input space could be done in O (m{sup 2}n) steps. Here m is the number of examples, while n is the number of dimensions. The second step of the algorithm finds its roots in the implementation of a class of Boolean functions using threshold gates. It is substantiated by mathematical proofs for the size O (mn/{Delta}), and the depth O [log(mn)/log{Delta}] of the resulting network (here {Delta} is the maximum fan in). Using the fan in as a parameter, a full class of solutions can be designed. The third step of the algorithm represents a reduction of the size and an increase of its generalization capabilities. Extensions by using analogue COMPARISONs, allows for real inputs, and increase the generalization capabilities at the expense of longer training times. Finally, several solutions which can lower the size of the resulting neural network are detailed. The interesting aspect is that they are obtained for limited, or even constant, fan-ins. In support of these claims many simulations have been performed and are called upon",How to build VLSI-efficient neural chips,,Los Alamos National Laboratory,,,core
4963439,2000-01-01T08:00:00,"In this thesis, the soft-output detection and decoding algorithms with joint channel estimation are developed for aperiodic random direct-sequence spread-spectrum (DS-SS) systems. Soft-decision algorithms provide more information than those with hard decisions, giving better bit-error-rate (BER) performance in the following decoding stage. The receiver considered is a single-user receiver without knowledge of any other users, and the channel is assumed to vary within tens of symbol time. For such time-varying channels, channel estimation is necessary for generating soft decisions. Especially, for a single-user, aperiodic random DS-SS receiver, the channel estimation relies additionally on interference suppression because the channel state information and crosscorrelations from interfering users are unknown random variables which change from symbol to symbol. To provide sufficient dimensions for interference suppression, multiple signal samples are taken either in time or space domain. With a state model formulation of the channel and the received signal, a joint channel estimation and MAP detection algorithm with real-time model parameter identification is proposed. First, based on the MAP detection requirement, a maximum likelihood (ML) estimator of unknown model parameters is derived. Under certain assumptions on the interference, the estimated parameters are proved to converge asymptotically to the true parameters with probability one. However, the complexity of the ML parameter estimation is prohibitively high for practical implementation. Instead of the ML estimator, the well known extended Kalman filter (EKF) is applied as part of the solution. The linear constraint in the antenna array and the non-observable model noise variance are estimated by simple and effective approximations. In the special case of time-domain oversampling, further complexity reduction is achieved, and negligible performance degradation in the following decoding stage is found through computer simulation. In addition, a unified implementation of fixed-delay, symbol-by-symbol MAP detection/decoding algorithms is proposed. With proposed modifications to lower the numerical requirement, the Optimum Soft-output Algorithm (OSA) and its suboptimal approximations can be integrated into a common algorithmic structure. Furthermore, by applying the principle of the Soft-Output Viterbi Algorithm (SOVA), the number of add-compare operations in the Sub-optimal Soft-output Algorithm (SSA) can be further reduced with little performance degradation",Soft -output detection and decoding algorithms with joint channel estimation for direct -sequence spread -spectrum systems,,'Purdue University (bepress)',,,core
235573160,2002-07-03T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.1
I
MAGRATH NEWS 1 •>
Published Weekly Since 1932 by
The Magrath Trading Company
50c o ... July 3,2002
r J
la .t-.k.
TOWN OF MAGRATH
“the garden crrr
DISASTER SERVICES
COORDINATOR
The Magrath and District Emergency Services is seeking an
extraordinary volunteer to serve as Disaster Services Coordinator for
Magrath and area. ... t:
This is a volunteer position to assist in the delivery of disaster services for
the Town of Magrath and the eastern half of Cardston County during
emergencies. This individual will be responsible for the following:
1. The development and implementation of a disaster plan in
consultation with the municipalities, local agencies and disaster
preparedness group such as Red Cross..
2. The reassessment of the disaster plan on an annual basis.
3. Coordinate training for disaster response personnel.
4. During emergency incidents, through a consultative process with
other emergency services, municipalities and agencies, ensure that
disaster needs are provided.
The initial development process would require a considerable time
commitment, however, once the disaster plan is in place the time
involved would be significantly less. If you require further information,
contact Rod Bly at 758-3212. Interested parties may submit their intent
in writing to the Town of Magrath, Box 520, Magrath, AB TOK 1J0, .
Attention: Rod Bly
*
r
Points of Interest
♦ Employment
* Grocery Specials
Home Hardware
Inside this Issue:
Community 1-3
Sports 4
Classified Ads
Calendar
Library News
Grocery Specials
6
8
8
9-11
^Hardware Specials 12y
!
DONT FORGET
Magrath Parade - Saturday July 27th
If you would like to participate in the parade, we would love to see you.
Please remember the date and get your decorations ready.
li
2
¿Samudfornii, ^ctbiâ'ul
have chosen to be married
Saturday, July 20,2002
at the Coaldale LDS Church
2216 - 21A Avenue
Together with their parents
they invite you to join the celebration
Wedding Ceremony 1:30 in the afternoon
Calling Reception 2:30 - 4:30
Parents of the Groom
Carol J. Patey
the late Clark Dainard '
Parents of the Bride Parents of the Bride
Tammy J. Noble William A Knopf
Leo Trenlær Beverly J. Knopf
h , I
Community No Cost Clothing
Exchange
Magrath Arena
July 30 - Evening only
Seniors & Disabled 5-6 p.m.
Open to public 6-9 p.m.
July 31
Open 9 a.m. - 4 p.m.
Donations of CLEAN clothing can be
dropped off at Wendy Coleman’s garage,
157 S. 1st St. W. before July 19th.
u,. • J •
^DESIGN
We specialize in:
-Scanning: slides, photos, negatives, text..
-Photo repair, retouching, restoration &
photo collages
-Memory videos (weddings, reunions, sports...
All of the above can be preserved
on VHS, CD and
(phone for details)
We also do private tutoring, troubleshooting,
desktop publishing & more!
Phone Bonny/Brenda: 758-3844 eve-/voicemail (day)
Duane & Carma Thomson are
pleased to announce the birth
of their daughter,
Keltie
She was born June 20,2002 and
weighed 9 lbs. 4 oz.
She is a baby sister to a very
happy Nathan, Cara and
Benjamin.
î -
MINI-GUARD
The Spirit of Alberta Barid i$ sponsoring a
one Week camp for all girls grades K-6,
July 22-26 (Magrath Celebration Week).
The camp will be held at the school from
10:00 to 11:30 each morning with
instruction according to age level in
dance, flag, and precision drill. The camp
will feature a special performance for
parents on the last day, and all students
may march with the Spirit of Alberta Band
in the Magrath Days parade on Saturday.
The cost of the camp is $39.00 which
includes a camp T-shirt. Registration
forms have been sent home with all girls
in elementary school (except
Kindergarten girls, which were missed).
Extra registration forms are available at
the Trading Co. Office or till. Also
welcome are next fall’s kindergarten girls
as well as friends and cousins here for
the celebration.
For additional information, contact
Joanne Dever (758-3631), Nikki Bogdan
(758-3217), or Jerry Chatwin (758-3765).
ATTENTION!!
QJtADE_ /2 $TUDENT± NOTE
Students entering Nursing
Magrath Hospital Auxiliaiy present each year
a “Margaret Long"" Scholarship
to the student with the highest marks entering
Nursing.
To apply, send a letter and copy of final marks
before September I, 2002 to:
Magrath Hospital Auxiliaiy
Box 639
Magrath, AB TOK I JO
Now Booking Appointments!!
'Wakeup With Make-up!
Permanent Cosmetics
SENIOR’S NEWS
Suppers will resume Sept. 4th
Cornbust date will be announced in August
*******
Contact Hazel Rasmussen for rental of the
Senior’s Centre. Fee $75.00 Phone 758-3545
*******
Garage Sale September 20th.
All donations gladly accepted.
Phone Grace Navratil at 758-3291
or Bob Clifton at 758-6733
HAVE A WONDERFUL SUMMER!
Denturist
* Complete denture service
* Soft Liners & stabilizing wings
for problems, sore lowers
George
Torre Alba
JULY 15 - 6:00 p.m.
Diamond Willow Terrace
In Your Town
MOBILE
Tina. D. Reidcpct
Phone: (403) 758-3930
758-3936
Call for your Free Consultation
MUSEUM NEWS
Il Darryl Passey has kindly loaned us his
(taxidermy collection which is a display of
local animals. These items are currently on
(display and can be viewed during the
! extended
|{ Summer Hours:
1 Monday 11:00 a.m. - 8:00 p.m.
I Tuesday - Friday 9:00 a.m. - 5:00 p.m.
(The Museum will be open from 2-4 p.m. on
July 27th (Celebration day) for family and
II friends to enjoy the displays and remember
1 the good ol’ days.
Make your dream come true
Have your home built by
JENSEN ENTERPRISES
New construction:
Additions:
Patio Decks:
403-758-3669
Ken Jensen, Owner
Fine woodwork:
Hardwood furniture:
Renovations:
Box 93
Magrath, AB TOK 1 JO
MR. DANIEL J. WIPF
beloved husband of Mrs. Sarah Wipf of Magrath,
passed away at the Magrath Hospital on Monday,
July 1, 2002 at the age of 68 years. .
He is survived by his wife Sarah Wipf of Magrath;
two sons, Tim (Judy) Wipf of Magrath, Joe Wipf of
Lethbridge; three daughters, Judy, Sarah, and Annie
Wipf all of Magrath; and 5 grandchildren.
A Funeral Service will be held at Deerfield Colony
Church on Wednesday, July 3rd, 2002 at 12:00
p.m.
Interment to follow at Deerfield Colony Cemetery.
4. 'BASKETBALL -
I---------- -----u.
’F J ; to
—----- —-----—SO »Fr.T1 BAL■ L u < •
MAGRATH
FAMILY/COMMUN1TY -
‘ softball TOURNEY w ❖ * . * 4 ♦
July 25-27,2002
All families/community members wanting
play in the Celebration' Softball Toiifnament
need to get their names in to either the town
office (758-3212) or Bill Alston (758^895).
Any community or district member^j^; are""'
not playing on a family team, may put their,
names on aijisf ht the Town Hall. No hand-p.
icked teams, please.
• -in? r-vi:<r •& ^ MqfnoO ■»
.'IHIdI*
..-»a • , .'
*
I
rI
... CLOGGING flaggy
■.... Krxikvd New cXoggiwg St*
* .....Opgkuuvg iw
* September 2002 *
GANAIMAN
-- intermediate arvdy^va*Aced/
Classes znoiubbr
- 45 mii/vwte Lessors 1 day a weetetl 7 c’1
- 4^5.00 -for 1 session. (12 lessors)
i- starting middle of «September td'*""'1'11 ”
December ai/vd will commen.ee iiA<__.. __
Jan.uary oi/vce again, .’¿M
- Competition. in, ApriL @ i^exburg, • rj*v/ *
- Filled with Fun,,jazzy steps autd-lots
of LearlA-luvg ■ -*nn
- Instructors: wt.uk <i
Paige Hudson,,jillei/ve Hudson,,
Cheriess H-LtdsoiA. § Noelle Wolsey s f ¡-v,-
P^AS^^LL:
Paige,jilleiA-e or cberiess @ y-5^.-3±^
on Noelle @ 7^2-3992
----------------- "" i' ■ 1 •—jfc.-. .
i Ji
i
r* I DANNY BALDERSON
BASKETBALL CAMP
.JULY 29 - AUGUST 2, 2002
Tom Karren Gym Magrath, Alberta
for BOYS and GIRLS Grades 3-9
Head Instructor: DANNY BALDERSON
Guest Instructors: DIANE SMITH
JIMMY BALDERSON
.. z Camp Includes:
Free Camp T-Shirt
5 Hours of Skill Instruction/Day (9-12, 1-3)
Skills, One-on-One and Team Competitions
Daily Awards and Prizes
Camp Tuition: $105.00 ($90'.00 for each additional
> ?fimiTy-member)
Forms can be found at the Trading Co. Office,
Magrath High School Offioe, or by contactin Wes
or Debbie Baldersoti (758-6380). 1
Because of Limited Space - Register Early!!
I
I
J
ÌI
I
REMEMBER TO CALL
TAI
Heating, Air Conditioning,
Refrigeration and Appliance Service
Gas and Electrical Service
and Trenching
752-3866
TAI HANCOCK - MGR.
BRIDGE SEFTIC SERVICE u -
?!
A DIVISION OF CHINOOK WATER WAGON ~'
'! r'-j, ""TiO""lJ
: •» vl'mel vv?w
RR 8-20-18 ■ ’
LETHBRIDGE, AB •’ .
T1J4P4
STEELE SHERIDAN - PROPRIETOR
PHONE (403) 328-2460
CELL (403) 330-8066
FAX (403) 327-3337
5
REMEMBER WHEN ....
Close your eyes... A nd go back in time Before semi automatics and crack... Before SEGA dr Super Nintendo... Way back...I'm talking about Hide and seek at dusk. Red light, green light.
The corner Store.
Hopscotch, butterscotch, doubledutch, jacks, kickball, dodgeball.
Mother May I... Red Rover arid Roly Poly. ...... Hula Hoops.
Running through the sprinkler.
An ice cream coneion a warm summer night... Chocolate or vanilla or strawberry or maybe butter pecan.
• Wait...
Watching Saturday Morning cartoons... Short commercials.
Fat Albert, Road Runner, The Three Stooges, and Bugs.
Or back f urther...
i When around the comer seemed far away, And going downtown seemed like going somewhere.
Cops and Robbers, Cowboys and Indians, Zorro. Climbing trees, building igloos out of snow banks
. Running till you were out of breath. Laughing so hard that your stomach hurt. ..
Jumping on the bed. , ""a Pillow fights.
Spinning around, getting dizzy, and falling
down...
Being tired from playing...Remember that?
The worst embarrassment was being picked last for a team.
War was a card game.
Water balloonsweretheultimate weapon; < Baseball cards in the spokes transformed any bike
into a motorcycle. :
I'm not finished just yet...
When you'd reach into a muddy gutter for a
! penny.
When you got your windshield cleaned, oil checked,
and gas pumped without asking, for free, every ; time...
and, you didn't pay for air.
if When nearly everyone's mom was at home . when the kids got there.
When it took five minutes for the TV to warm up.
if you even had one.
It was magic when dad would ""remove* his thumb.
When it was considered a great privilege to be taken out to dinner at a real restaurant with your parents.
When girls neither dated nor kissed until late high school, if then. When they threatened to keep kids back a grade if they failed...and did!
When being sent to the principal's office was nothing compared to the fate that awaited a misbehaving student at home. Basically, we were in fear for our lives but it wasn't because of drive-by shootings, drugs, gangs, etc.
Our parents and grandparents were a much bigger threat!
Didn't that feel good? Just to go back and say,
""Yeah, I remember that!"" Remember when... Decisions were made by going ""eeny-meeny- miney-mo.""
Mistakes were corrected by simply exclaiming, ""Do over!""
""Race issue"" meant arguing about who ran the fastest.
. The worst thing you could catch from the opposite sex was cooties.
It was unbelievable that dodgeball wasn't an . Olympic event.
Having a weapon in school meant being caught with a slingshot.
Scrapes and bruises were kissed and made better.
Taking drugs meant orange-flavored chewable aspirin.
Abilities were discovered because of a ""double­dog-dare.""
If you can remember most or all of these, then you have LIVED!!!6
CLASSIFIED ADS
FREE MARKET
❖
FOR SALE - Dark Blue Toddler Stroller. Good condition/clean ~ $25.00. Lightweight Pull Golf Cart 2
*******
yrs. old ~ $15.00. Never used Bike Rack for vehicle
❖
ALL FORMER GUARD GIRLS or their parents. Please check your closets for any sailor outfits or any other guard costumes you may have. Please contact Jerry Chatwin or Joanne Dever if you find any.
❖
(front or back). Holds 2 bikes ~ $20.00. Never used Heavy Hay Tarp. 25’ x 14’ ~ $90.00 o.b.o. RonYoshihara 758-3556.
HORSES FOR SALE-
❖
FOUND - Border Collie Dog, south of Magrath. Male with brown markings. 758-6632
1 - 5 yr. old Quarter Gelding
1 - older Appaloosa Gelding, great kids horse.
CaU Craig at 758-3188.
GARAGE SALES
❖
WANTED - Good used 3-wheel Bike. Call George Fyfe at 394-3663 ■ < .. . .
*******,; ,,.v.
MOVING SALE 0
Everything Must Go! !; Microbe,.:T;y.’s. furniture, bedroom states, videos* ..
AUTOMOTIVE (
.. ‘ .''A'.
******* ._ -
284 South 2nd St West
: Saturday, July'6
❖
FOR S ALE - 1988 Plymouth Sundance - 5 speed
9 am. until everything’s gone
standard. CD player, great gas mileage, good condition,
.c
excellent car for student or small family. $1500.00 Call
❖
GARAGE SALE
Dustin at 317-1682 or 758-3188.
S. J^St.tW. (Wendy Coleman’s) .. . < < ■.
Saturday, July 6th
❖
FOR SALE - 1987 Chev % ton Customized Van
9 am. - 2 p.m.
(pacemaker package). Loaded, no rust, good condition.
Pinky' Coleman - Avbn Sale, too! '1
runs great. $4000.00 Call Craig at 758-3188.
❖
YARDS^LE:;j .-2, ,,-,.(3,,- . -
Saturday, July 6
9 am.-1p.m. 'ISdmsfn:.-: •
BUSINESS
****-***••■•’■
159 W. la Ave. NHll(,.-. •>,- i.: :
(M. Schad) '
❖
Looking for WEEKLY HOUSE CLEANER to clean my house near Spring Coulee. Mature person preferred. Transportation costs will be covered. 653-3461.
•. i ...
BUY & SELL
❖
CUSTOM CUTTING & BAILING
*******
Call Cody Ririe 758-6785
❖
FOR SALE - Moores Pure Virgin Wool Black Suit.
• .. , ,■
Regular 42 ~ $100.00. Moores Pure Virgin Wool Dark
JUST RIGHT ROTOTILLING by David & Mark
Blue Jacket Regular 42 ~ $45.00. Jessica black, green,
HcUKCi. 758-3009.
gold Dress. Size 14 ~ $30.00. J.R. Petite black sleeveless c/w % length royal bluejacket ~ dress ~ size
❖
BENNETT LAWN MOWING - Jonathan, Lauren &
12 ~ $45.00. 758-6024.
Russell - Riding mower and push mower. 758-6222.
❖
FOR SALE - Compac Laptop Presario 1694-AMD 450
❖
MIKE HARKER is back and ready to rototill your
MHZ - 64 MB - 6 GB - 4xDVD - 14 with warranty until Dec. 17,2002. Programs. Never used as computer
garden. Call me whenever you’re ready at 758-6664.
- like new - $2000.00.
Water cont - holding tank for R.V.’s - $50.00 758-6024
❖
DO YOU HAVE AN ELDERLY OR ILL LOVED ONE? Medical Pendants can save lives. They call for help when you’re away. Call Canadian Security
❖
FOR SALE - Cell Phone. Nokia 5160 c/w 12V charger, leather holder, hands free ear buds, window mount
Systems at 758-3945.
antenna, 3 watt booster, home charger. $400.00
❖
For all your cleaning needs from hospital clean to a
758-6024
touch-up, carpet to ceiling & everything in between. No job too big or too small.
Call Wayne’s Carpet & Upholstery Cleaning 758-6414.CLASSIFIED ADS
♦
REALESTATE *******
FOR RENT
2 bedroom house for rent in Magrath. 2 houses east of
the Credit Union. Available July 1* 758-6725
❖ House for rent. 2 bedroom home in Welling. No
smoking unit for abstainers. No pets. Fridge, stove,
washer & dryer in eluded in rent. Call 752*3848.
FOR SALE
NEWER MOBILE HOME FOR SALE - 3 bedroom, 2
bathroom, new carpet, new lino, fridge, stove, dishwasher.
On lot in Coalhurst — can be moved. $36,000. 758-6277.
❖ HOME FOR SALE -Completely Renovated 4 bedroom
home. Good location close to school. Asking $72,000.
758-3731 or 758-6826.
ili
: :
J J
c? HomeLife
Higher Standards
M.L.S.
Jim Anderson
agent
Residential ~ Farm
Acreage ~ Commercial
in Magrath and Area
2 Houses for Sale in Del Bonita
2 teacherages for sale in Del Bonita
Comparative Market Analysis
(No Charge) - For people interested in getting an
evaluation of marketability of your property.
Phone: 758-6725 (leave message)
331 -8882 (cellular)
• t •
•<
Z
.15
Therapy
can help you
call KIM at
Z58-3210
to book your
appointment today
ALSO SPECIALIZING IN
MATERNITY MASSAGE
COBH) By MOST HEALTH CMC HUMS
i s- •)
AMAZING UiZLEii FACTS
Almonds are not really nuts, but a member of
the peach family.
Eskimos use refrigerators to keep food
FROM freezing.
The name Wendy was made up for the book
“Peter Pan”. . ,.A
... •.u Ti .. i .
The reason firehouses have circular stairways
is from the days when the engines were pulled
by horses. They kept the horses from walking
up the stairs.
An estimated 35 percent of people who use
personal ads for dating are already married!
The Bible has been translated in to Klingon.
Butterflies taste with their feet!
According to surveys, on average people fear
spiders more than they fear death.
«i
I■I4
JMfiRAÎHATWS
Im Our Community... July 2002
Sunday
Monday
Tuesday
Wednesday
Thursday
Friday
Saturday
30
/
Mobile Denturist 6:00 Library closed
2
3
4
5
Mo
<
Yar V
6
ving Sale 28^ >. 2nd St. W.
dSale 15<
1.Is* Ave. N.
»
7
8
Co-ed Slo-pitch 6:30
9
*
¡0
II
12
!3
LIBRARY NEWS
The Summer Reading Program starts Thursday, July 4, 2002 from 2 p.m. to 3 p.m. for children from Grade 1 to Grade 12. The theme is “Sailing the 7 Seas”. This program promises to have fim activities, games, stories and treats. The summer reading program will run every Thursday for the month of July. For more information do not hesitate to call the library. 758-6498 ™ ;
Movies that are loaned from the Magrath Library now have a two day renewal period instead of one day renewal. We hope that this will be more convenient for .everyone.
We are selling Chinook Arch 10 Year Anniversary Quilt Book Bags. They are made with strong canvas and have the picture of the 10 year anniversary quilt. We are selling them for $12.00.
. . • • ‘.'..it’d ""b..
We will be closed Saturday, June 29 and Monday, July 1st for the long weekend.
Adult Fiction
To Trust a Stranger by Karen Robards 1
An Execution of Honor by Thomas L. Muldoon Looking Back by Belva Plain v’
The Edge of Town by Dorothy Garlock , •
The Summerhouse by Jude Deveraux
Seals Sub Rescue: Operation Endurance by S.M. Gunn
Juvenile Fiction
The Bear Dance by Chris Riddell ''
A Letter to Grandma by Paul Rogers
Disney’s The Hunchback of Notre Dame
J ' Non-Fiction
■ Redefining Beauty by Victoria Jackson ,: .
Cooking for Jack by Tommy Baratta ’
- ■: ; Other Creations by Christopher Manes s ’?
. t Pheasants and Their Breeding and Management by K.C.R. Howman
The Diabetes Holiday Cookbook by Carolyn Leontos
Videos ;
Dudley Do-Right
-
My Life
Please note that the deadline for submissions to the pajjer is MONDAY at 6:00 p.m. Entries submitted after the deadline will be published the following week. Phone 758-6377, email to tidmarsh@telusplanet.net, fax 758­6888 or drop off your submissions at the Magrath Trading Company Office.•if
9
- f MagTath Trading Company ■ -
3 5>. J,
jQ’
GROCERY SPECIALS
“From Our Family To Yours, v”
Daily Delights and Frozen Favorites!’
'•hi viy.iv ir. ■ •'
Kraft Single Slices or Cheez Whiz-select varieties
1 kg
$6.98
Kraft Philadelphia Cream Cheese-select varieties
j? 250gii.
$2.98
Western Family Margarine - select varieties
>- ,,454g. .
.98
Armstrong Aged Cheddar - select varieties .
750 g
$8.98
Armstrong Cheddar Cheese or Mozzarella - select varieties
750 g
$7.98
Dairyland Yogurt - regular or fat free - select varieties
750 g
$2.68
Dairyland Cottage Cheese -select varieties
750 g
$3.68
i DairyiaRd Sour Cream ?*.
750 ml
$2.88 A i
BreyersiAII Natural or Blends ice Cream - select varieties
2 litre
$5.98 - ;
i Tropicana or Tropics Juice, Cocktails or Blends-select varieties
... . 1.89 litre
2 for $7.00V; 1
Minute Maid Orange Juice, frozen-select varieties
355 ml
3 for $3.00* .
Minute Maid Punch or Nestea Iced Tea. frozen - select varieties
355 ml
5 for $4.00 O 1
Five Alive or Fruitopia Punch or Blends-select varieties
1.89 litre
2 for $5.00 J *
McCain Rising Crust or International Pizza, frozen - select var.
464-835 g
$6.98
McCain Diep n Delicious or Triple Chill Cakes, frozen
510-530 g
'•Jp- $3:98
Groceries...
Kraft Pourable Dressing-select varieties
..,475 ml
:■ 2 for $5.00 '
Western Family Apple Blend .. :,1G7
1 -litre- ':j •'
5 for $4.00 j.
Kraft Jam - strawberry or raspberry
500 ml
J . $3.98 . rih *
Kraft Peanut Butter-select varieties
1 kg-
$4.98
V8 Splash or Vegetable Cocktail - select varieties
1.89 litre .....
$3.68
Minute Maid, Bibo or Five-Alive-select varieties
10 pack
$3.48
Ocean Spray Cranberry Cocktail-select varieties
1.89 litre
$3.98
Nabob Tradition Coffee - select varieties
300 g
2 for $5.98
. . - ------------------------------------
10
More Grocery Specials...
Quaker Granola Bars or Dipps - select varieties
187-225 g
$2.28
General Mills Cereal-select varieties
385-525 g
2 for $7.00
Aunt Jemima Pancake Syrup - select varieties
750 ml
$2.98
Aunt Jemima Pancake Mix or Snackery-select varieties
9O5g- 1 kg
$2.68
DelMonte Fruit Cups-select varieties
4 pack
$2.98
Chef Boyardee Pasta Dinners-select varieties
212-425 g
3 for $3.00
Kraft Mayonnaise or Miracle Whip - select varieties
750 ml - 1 litre
$3.98
Dempster’s Original Bread - white or 100% whole wheat
675 g
$1.68
Dempster’s Hot Dog or Hamburger Buns
I2’s
$1.68
Lipton Sidekicks, Noodles or Rice & sauce - select varieties
120-142 g
3 for $3.99
Hunt’s Thick & Rich Pasta Sauce-select varieties
680 ml
3 for $4.98
Kraft Dinner
225 g
.78
Campbell’s Chunky Soup-select varieties
540 ml
$1.88
Puritan Flaked Ham, Turkey or Chicken
184 g
3 for $3.00
Swift Canned Ham
680 g
$4.98
Cadbuiy or Neilson Bars - select varieties
Singles
3 for $1.89
Cadbury or Neilson Bars-select varieties
Family Size
3 for $3.99
Tetley Iced Tea-select varieties
3 x 250 ml
2 for $3.00
Goodhost or Nestea Iced Tea Mix-select varieties
640 g - I kg
$4.48
DoritOS or FritOS Nachos - select varieties
300-370 g
2 for $5.00
Christie Snack Crackers - select varieties
185-300 g
2 for $5.00
Quaker Rice Cakes or Crispy Minis-select varieties
100-199 g 1
2 for $4.00
Pringles Potato Chips-select varieties
145 -170 g
$1.78
Rave Potato Chips - select varieties
170 g
2 for $3.00
Dare Cookies - select varieties
325-350 g
2 for $5.00
Kraft Handi Snacks - select varieties
87 g
3 for $4.98
Western Family Freeze Pops - kid s size
110 pack
$4.48Ill'I-
11
Market Fresh Produce!
Fresh Nectarines - u.s. grown
$l.94Ag
,88/lb
Fresh Broccoli - u.s. grown
$l.94Ag
.884b
Fresh Strawberries - California grown, #1 grade
1 lb. Package
$2.48 each
Premium Celery Stalks - Cal","Magrath Store News (July 3, 2002)",,J. A. Ririe,,,core
42808011,"Jan 1, 1993","Neural networks are a very useful tool for analyzing and modeling complex real world systems. Applying neural network simulations to real world problems generally involves large amounts of data and massive amounts of computation. To efficiently handle the computational requirements of large problems, we have implemented at Los Alamos a highly efficient neural network compiler for serial computers, vector computers, vector parallel computers, and fine grain SIMD computers such as the CM-2 connection machine. This paper describes the mapping used by the compiler to implement feed-forward backpropagation neural networks for a SIMD (Single Instruction Multiple Data) architecture parallel computer. Thinking Machines Corporation has benchmarked our code at 1.3 billion interconnects per second (approximately 3 gigaflops) on a 64,000 processor CM-2 connection machine (Singer 1990). This mapping is applicable to other SIMD computers and can be implemented on MIMD computers such as the CM-5 connection machine. Our mapping has virtually no communications overhead with the exception of the communications required for a global summation across the processors (which has a sub-linear runtime growth on the order of O(log(number of processors)). We can efficiently model very large neural networks which have many neurons and interconnects and our mapping can extend to arbitrarily large networks (within memory limitations) by merging the memory space of separate processors with fast adjacent processor interprocessor communications. This paper will consider the simulation of only feed forward neural network although this method is extendable to recurrent networks",Efficiently modeling neural networks on massively parallel computers,https://core.ac.uk/download/pdf/42808011.pdf,,,,core
214170588,1992-03-22T08:00:00,"We use findings in machine learning, developmental psychology, and neurophysiology to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually-driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a gripper, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up such important issues as observation sparsity in high-dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well-established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for projections of high-dimensional data sets that capture task invariants.
We also pursue the following problem: how can we use human expertise and insight into grasping to train a system to select both appropriate hand preshapes and approaches for a wide variety of objects, and then have it verify and refine its skills through trial and error. To accomplish this learning we propose a new class of Density Adaptive reinforcement learning algorithms. These algorithms use statistical tests to identify possibly  interesting  regions of the attribute space in which the dynamics of the task change. They automatically concentrate the building of high resolution descriptions of the reinforcement in those areas, and build low resolution representations in regions that are either not populated in the given task or are highly uniform in outcome.
Additionally, the use of any learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate mistakes during learning and not damage itself. We address this by the use of an instrumented, compliant robot wrist that controls impact forces",A Robotic System for Learning Visually-Driven Grasp Planning (Dissertation Proposal),https://core.ac.uk/download/214170588.pdf,ScholarlyCommons,,,core
15351702,2001,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites. The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality. Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc. Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world. The Glasgow Directory is an established 3D web space, with around 10,000 visitors per year. The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography. This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar environments. These criteria take into account lessons learned by “observingi and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface. These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow - Welcoming multiple visitors to the Virtual City,,,,,core
24298749,1997,"Recently, several planners have been designed that can create conditionally branching plans to solve problems which involve uncertainty. These planners represent an important step in broadening the applicability of AI planning techniques, but they typically must search a larger space than non-branching planners, since they must produce valid plans for each branch considered. In the worst case this can produce an exponential increase in the complexity of planning. If conditional planners are to become usable in real-world domains, this complexity must be controlled by sharing planning effort among branches. Analogical plan reuse should play a fundamental role in this process. We have implemented a conditional probabilistic planner that uses analogical plan replay to derive the maximum benefit from previously solved branches of the plan. This approach provides valuable guidance for when and how to merge different branches of the plan and exploits the high similarity between the different..",Analogical Replay for Efficient Conditional Planning,,AAAI Press,,,core
55197373,2001-01-01T00:00:00,"We present ELF, a learning fuzzy classi®er system (LFCS), and its application to the

®eld of Learning Autonomous Agents. In particular, we will show how this kind of

Reinforcement Learning systems can be successfully applied to learn both behaviors and

their coordination for Autonomous Agents. We will discuss the importance of knowl-

edge representation approach based on fuzzy sets to reduce the search space without

losing the required precision. Moreover, we will show how we have applied ELF to

learn the distributed coordination among agents which can exchange information with

each other. The experimental validation has been done on software agents interacting in

a real-time task",Learning Fuzzy Classifier Systems for Multi-Agent Coordination,,'Elsevier BV',10.1016/S0020-0255(01)00149-9,,core
235573972,2001-02-28T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH NEWS
Published Weekly Since 1932 by
The Magrath Trading Company
?------------------------------------- <
Points of Interest
35* February 28, 2001
T^e Rd^monò playhouse Society
Proudly Presents
The Blue Castle
A dramatic play based on tfe book by L.M. Montgomery
Stage Adaptation by Kjiye Merrill
Performance Dates:
March 22 - 31st
8:00 p.m. Raymond Broadway Theatre
(Sunday & Wednesday Dark)
Directed by Kaye Merrill
One of L.M. Montgomery’s most endearing classics.
Starring:
Beth Nielson as Valancy Melanie Rudd as Mrs. Stirling
Mike Holt as Barney Snaith Robin Steed as Cousin Stickles
Stuart Fowler as Uncle Benjamin
Tickets available at Raymond Broadway Theatre
Beginning March 5,2001 Weekdays 1-5 p.m.
or by nailing 752-2336 Ticket Price $10.00
Dinner Theatre Tickets call 752-4036 or 752-3572
Monday 26th Family Night - Tickets $8.00
Tuesday 27th Youth Night - Tickets $8.00
* Sports
* Rod & Gun Banquet
* Grocery Specials
* Home Hardware
Sales
- -
—
Inside this issue:
Community Interests 1-3
Sports 4
Classified Ads 6-7
Community Calendar 8
Grocery Specials 9-10
Meat & Produce 11
Hardware Specials 12
i____________________
?
■Ï Make your dream come true
have your home built by
JENSEN ENTERPRISES
New construction:
Additions:
Patio Decks:
403-758-3669
Ken Jensen, Owner
Fine woodwork:
Hardwood furniture:
Renovations:
Box 93
Magrath, AB TOK 1 JO
The March meeting of
St. Joseph’s C.W.L.
will be held on Tuesday, March 13th
at 7:00 p.m. at the home of Elizabeth Stanek.
Reminder that memberships are due.
Jtagrnth United Church
is having a fund raiser.
They are selling
Avon
Skin-So-Soft Candles in a Can
3 y4"" x 3 72""
Cost: $10.00 each - $5 of
which goes to the church.
ATTENTION!!
Members, Parents, Friends
of the
Magrath Concert Bands
1972 to 1975
Kennedy Jenson, has compiled a number
of Band selections on to
a 40 minute CD
which will be available for sale as soon
as the covering Jacket is made.
We are asking you to search your photos for
pictures of the band or of individual members -
humorous or sedate!!
These she plans to use in a collage for the
Jacket.
If you have any pictures please contact:
Ruth Jenson 653-3461
Mary Yvonne Hohm 758-6608
VINNIE S KITCHEN
j Preparing all Kinds of Food For Special Events
Full Service Catering or We Deliver For Yon to Serve
Over 25 Varieties of Our Own Soup
No a restaurant - a large commercial kitchen with
Alberta Health Food Establishment Permit
For great comfort food» Call 758-3292
To order you candle call:
Betty Twitchen at 758-3240
or
Pinky Coleman at 758-6899
MISSIONARY CORNER :
ai
Wayne Kinder, son of James & Debera, has been;
called to the Idaho, Boise Mission. His farewell j
will be held on March 18th at 1:00 p.m. in the!
Garden Place Chapel. He reports to the M.T.C. on •
March 28th. i
WHY WE LOVE CHILDREN
A small boy is sent to bed by his father.
Five minutes later:
""Da-ad...""
""What?'' h
""I'm thirsty. Can you bring me a drink of water?“
""No. You had your chance. Lights out.“
Five minutes later: ""Da-aaaad... I'm THIRSTY.
Can I have a drink of water??“
""I told you NO! If you ask again, I'll have to
spank you!!“ ?
Five minutes later... ""Dciaaa-aaaad..:
""WHAT??!!""
""When you come in to spank me, can you bring
me a drink of water?""
3
Cards of Thanks
Our families would like to express our
appreciation to all those who helped with the
wedding in anyway. People were so generous with
help and also with gifts. We are grateful for such a
wonderful community. Thankyou so much.
Brad & K.D. Wolsey
Rod & Sharon Gibb
Scott & Falene Wolsey
Thankyou to all who gave us condolences
for the loss of our daughter, sister and best friend,
Gail Drab.
Muriel Sabey
Sharon Owens
SENIOR CENTRE NEWS
Supper will be held tonight, February 28th
at 5:00 p.m. Senior Centre
Your Hostesses for Februaiy are Bernice
Coleman, Mary Tanner & Evie Hillmer.
*******
Contact Jean or Jack Butlin for rental of the
Seniors Centre. Fee $75.00
Phone 758-3030
*******
BINGO*BINGO*BINGO
Bingo is tomorrow, Thursday, March 1st
at the Seniors Centre
Doors open at 6:30 p.m.
Bingo at 7:00 p.m.
Public Welcome!!
Magrath Co-op Annual Meeting
Wednesday, March 21
2:00 p.m.
at the Magrath Co-op
All members welcome.
Lance Miller
Agent
Bub. 320-6411
Fax. 320-6427
SENIOR NEWS
We will be going to Stage West in Calgaiy on
Wednesday, March 2l5t.
Matinee for luncheon & stage play
OVER THE RIVER & THROUGH THE WOODS”
Members - $45.00 Non-members - $55.00
ALSO
Raymond Playhouse Society presents
“THE BLUE CASTLE""
is a play adapted from L.M. Montgomeiy’s book.
March 22-31
Thursday, March 29th has been reserved for the
Magrath Seniors.
Cost: $25/person includes dinner & theatre.
Bus cost will be extra.
Call now to make your reservations.
Phone: Levaun Thompson - 758-6672
Hazel Rasmussen - 758-3545
Norma Owens - 758-3560
SPORTS
BASKETBALL
CURLING
the Zenith scoring with 31 points. Brooks Blacki and Allan Tollestrup added 22 and 19 respectively.
Zeniths have 14-0 Record
Last Friday night, February 23, the Zeniths and Eagles travelled to Cardston for what turned out to be a very exciting game. The Zeniths beat the Cardston Cougars with a score of93-88 to make this years league record 14-0.
Jimmy Balderson came off his sick bed to lead
The Eagles didn’t fair as well as Cardston came out on top with a score of68-64. Dylan Alston scored 22 points and Kyle Clifton added 14.
j SOCCElT ...
SOCCER REGISTRATION
Ages 4-U13 play in Magrath $35 perplayer, $105 family of3 ormore
U16 & U18 boys play in Lethbridge $70 per player
(teams playing in Lethbridge must be submitted by April 1)
(players born in 1982 not eligible for U18 team)
Coaches and referee registration
Place: Magrath Pool Foyer
Dates: Th urs. March 15 7:00pm - 9:00pm
Sat. March 17 9:00am-12:00pm
Thurs. March 22 7:00pm - 9:00pm
Rod & Gun Cluh Annual Banquet & Awards Night
Saturday, March 3, 2001 6:30 p.m.
Magrath United Church Lots of Door Prizes - Excellent Food Guest Speaker
Adults $10.00 each - Under 16 $5.00 each Tickets at Trading Co. Hardware EVERYONE WELCOME!
Magrath Curling Club News
The Magrath Curling Club held their Annual Ladies Bonspiel, February 22 to February 25 with ten teams participating. Results of the bonspiel were:
Carly Quigley and her Lethbridge team won over Sandy Meldrums Magrath team to capture the first event sponsored by Kenneth C. Long Seeds Ltd.
The Sherri Kowalchuk rink of Lethbridge won over Janet Walter s Lethbridge/Cardston rink to win the second event sponsored by Alberta Treasury Branch of Magrath. The Jean Malmberg rink from Magrath placed third with the Sally Bond team of Coaldale placing fourth.
Saturday evening, Ken Leavitt, Mike Holt and Investor’s Group sponsored a “Draw to the Button” challenge. Each rink selected a team mate to enter the shoot-out. No one claimed the $5000.°° possible prize. Kristen Simpson, representing the Janet Walter rink drew her rock within 15 inches of the button and Ken Leavitt presented each member of her team with a vest.
The Magrath Curling Club would like to Thank everyone who made donations to make our bonspiel a success! ' „ .
Magrath Mixed Bonspiel
March 13 - 18,2001
Banquet and Dance: Saturday, March 17, 2001
A, .
To Enter, Contact:
Maurice Bevers at 758-6726
or
Ron Balderson at 758-6521
Please submit your sports scores and schedules by dropping off at the Trading Company office, faxing to 758-6888, or emailing to tidmarsh@telusplanet.net.MARCH
Adult Education Computer Courses
January-March 2001
Instructors: Bonny West/Brenda Beck
A
Cardston: Microsoft Word-Intermediate
This 3 week will cover creating and editing tables, using Word’s drawing tools, adding headers, footnotes,
and endnotes, importing files, modifying templates, mailing labels, customizing bullets and special characters,
adding web links, and additional editing features.
Dates: Tuesdays, March 6-20 (3 weeks)
Location: Cardston High School
Time: 7:00-9:30 p.m.
Tuition: $35
Magrath: MakeYour Own Memory Video
Using slides, photos, scanners, video clips, background music, voice annotations, transitions, and easy to use
software, it is now possible to create “living” video scrapbooks for home movies, weddings, family reunions, baby
books, gifts, sports highlights, etc. Participants will create their own personal memory video output to VHS tape.
(These movies may also be stored on CD, on the Web, or sent via email.)
Participants need to bring their own photographs & video clips and choice of background music (preferably
CD). Additional photos and music and blank tapes will be provided.
Dates: Wednesdays, March 7-28 (4 weeks) lime: 7:00-10:00 p.m.
Location: Magrath Elementary School Tuition: $65
To register call Kathy Richards @ 653-4991
We specialize in:
• Video slide shows (weddings,
reunions, sports highlights,
the Web (Output format:
VHS tape & CD)
• Hi-Res Photo scanning/Repair &
Retouching/Printing (color & B/W),
OCR (text)
• Banners, Posters, Ads, etc.
• Private tutoring
• Computer troubleshooting
• Desktop Publishing
Phone Bonny/Brenda
758-3844 (evenings)/
leave a message (day)
WWW
FEBRUARY
HEART & STROKE
MONTH
is quickly drawing to a close. We still
have 20 Volunteer Canvassers out there
and ask that they get their districts
finished and turn their books in as soon
as possible.
If you have been missed & would like to
give, call 758-3030 a pick up could be
arranged.
A sincere thanks to all that have given.
A hearty note of thanks to all our 48
volunteers. We appreciate you all. You
are what makes our canvass a success
each year.
Magrath Hospital Awdliaiy
6
Zola Woolf
Passed away in Magrath on Tuesday,
February 27*\ 2001 at the age of 68 years,
beloved sister of Verna Leishman Peterson
ofCardston.
Friends may meet the family at
Salmon Funeral Home, Cardston on
Thursday, March 1st from 7:00 to 8:00 p.m.
or at the Church from 10:00 to 10:45 a.m.
prior to the service. The Funeral Service will
be held at the Church of Jesus Christ of
Latter-day Saints, Magrath Stake Center
on Friday, March 2nd at 11:00 a.m. with
Bishop Burns Alston officiating. Interment in
the Hill Spring Cemetery at 2:30 p.m.
<- ' 4.,
*2Vcti
V,
NON taxable
S/LEni
1 TCM'A4* '
^coST
_______CLASSIFIEDS____
FREE MARKET
* MISSING - 6 month old gray Llama.
Went missing from my corral sometime on
Saturday, March 24. If you have any information
Please call 758-6051.
BUY & SELL
* Bumper Bale Handler for sale.
In good condition. Phone 758-3226.
BUSINESS
* TUPPERWARE
Have defectives? Need replacements?
Call Heather MacKay at 758-3662.
All orders receive discounts, no pressure, last
order. Call before March 7*.
* If your child needs help in Math or Reading the
Kumon Method might help. For more
information call Rusty or Martine Rollingson
at 758-3648.
CLASSIFIEDS
*
Jeanie s Hair Fashion
136 S. 1 St. W. - four doors south of the Trading Company. 758-3379. Open Tues. - Fri. Professional Hair Care at pleasing prices.
*
CANADIAN SECURITY SYSTEMS
We sell, install & service alarms, safes, camera systems, deadbolts & key locks. Call Ross Moore at 758-3945. Free estimate.
*
For all your cleaning needs from hospital clean to a touch up, carpet to ceiling & everything in between. No job too big or too small.
Call Wayne’s Carpet & Upholstery Cleaning 758-6414.
*
Will Tutor, grade 1-12 Math or Chemistry. Call Martine Rollingson at 758-3648.
USED VEHICLES
* FOR SALE - 1999 Ford XLT loaded short box 4x4. 30,000 km. Offers. Call Codey - 758-3395 days or 758-3767 evenings.
REAL ESTATE
*******
FOR RENT
*
Storage space & Commercial space for rent. For information call 758-3876.
*
Rent or Buy! Newly decorated 2 bedroom home, large garage on beautiful double lot. Magrath. Phone 758-3789.
*
2 bedroom apartment. For information call 758-3876.
*
2 bedroom, furnished apartment. No pets. Abstainers. Call Ty Alston - 758-3322
*
Small 2 bedroom home. Private. Treed lot under the hill. Inexpensive. 758-3700. Available now!!
*
4 bedroom, 2 story home. Close to school and LDS stake center. 1 16 bath, living & family room, utility room. Fridge & stove included. Abstainers. No pets. 758-3700. Leave message.
FOR SALE
*
A 1340 sq. foot home near schools for sale.
3 Bedrooms, 2 bathrooms, main floor family room with fireplace, kitchen has new cupboards. New carpet and ceramic tile upstairs throughout, new furnace, two care garage attached and shop in backyard. Selling price $118,000.
Phone 758-3992 - Wolseys. Address is #50 - 1A Street East.
*
Character Home in Magrath. 1300 sq. ft. Fully renovated. Loads of oak throughout main floor.
9 14 ft. ceilings. Large country kitchen with oak cabinets. Finished basement. 25’x24’ heated garage with finished loft. Landscaped lot with mature trees. A must to see. Call collect for Viewing and more details - (403) 227-2611.
*
3 bedroom, 3 bathrooms. 1250 sq. Ft. on main floor. Treed lot - Fully developed basement. Call 758-6991.
*
5 bedroom, 3000 sq. Ft. Home on 1 acre.
2 bath, 2 family rms (one with wood burning stove), living rm, dining rm, oak in kitchen.
A steal at $132,000. Phone 758-6789.
*
House for sale - all brand new inside when
. finished renovations. For complete details See “www.retailcontrols.com/house” Asking $49,000 when complete.
*
New 1230 sq. ft. home. 3 bedrooms, 2 bath, garage, fridge, stove. No GST.
Phone 758-6835 or 758-34468 MaGROTHNEWS
In Our Community ... March2001
SUMÒay Monday Tuesday Wednesday T^ursttay Friday Saturday
25 ~ 26 27 28
Seniors Supper
5:00
I
BINGO
2
BASKETBALL
Boys host Kate
Andrews
3
BASKETBALL
Boys at
Raymond
. -
4 5 6 7
Seniors Supper
5:00
8 9 I0
Southern Alberta E
Lethbridge Er
lasketball Playoffs
imax Centre
758-6060
1444 sq. ft. Bungalow. This 25
year bungalow is definitely priced
right. Owners are moving to
another city. Don’t wait around,
this one won’t wait, mls
Large Luxury Bungalow (2195 sq.ft.) on 5
acres nestled in mature trees. 5 acres of
irrigation water rights. On town water. This
is a must to see. mls
4 bedroom bungalow with front
drive garage. Excellent starter
home. Priced to sell. Large flat lot in
nice location, mls
OLDIEBUTAGOODI Call
John
Latham
aiioalalz inokvi
EXCLUSIVE
Beautiful character home. Fully
renovated from top to bottom.
^Only asking $99I,.900. MLS
Phone 758-6060
Fax 758-3008
Price $39,900. Close to schools.
Woodburning fireplace. 2
Bedrooms.
Please note that the deadline for submissions to the paper is MONDAY at 6:00 p.m. Entries submitted
after the deadline will be published the following week Phone 758-6377, Fax 758-6888 or drop off your
submissions at the Magradi Trading Company Office.
Jlagrath trading Company 9
GBOCSRU $P£CIA£S
“3rom Our Jamily lo Hours...”
Dairy Delights »nd frozen favorites!
Parkay Margarine - select varieties, quarters or soft tub
1.36 kg
$2.98
Western Family Single Slices - select varieties
500 g
$2.98
Dairyland Cottage Cheese - select varieties
500 mL
$2.18
Dairyland Sour Cream - select varieties
500 mL
2 for $2.00
Western Family Cheddar Cheese - select varieties
750 g
$6.98
Western Family Aged Cheddar
750 g
$7.98
Minute Maid Chilled Orange Juice - select varieties
2.84 litre
$4.48
Minute Maid Lemonade Punch, Limeade or Nestea iced 1
;a 355 mL
.88
Sunny Delight Chilled Punch - select varieties
3.78 litre
$3.98
McCain Superfries, frozen - select varieties
1.5 kg
$3.48
Stouffer's Lean Cuisine Dinners, frozen - select
196-340 g
$2.98
Swanson Dinners, frozen - select varieties
280-376 g
$2.98
Breyer’s Classic ice Cream - select varieties
2 litre
$4.48
McCain Deep *N Delicious or Triple Chill Cake
510-530 g
$3.48
Groceries
Western Family Chicken or Beef Broth
284 mL
3 for $ 1.98
Sunburst Noodle Cups - select varieties
^... 64 g< - ,,
4 for $3.00
Western Family Chicken Noodle Soup, dry soup
JK?4 patrk
^$1.48
Western Family Sliced Bread, white or 60% whole v
heat-.» 567 g
.88
Western Family Egg Noodles - select varieties
375 g
.98
PregO Pasta Sauce - select varieties
750 g
$2.48
Western Family Flakes of Ham
184 g
.98
MJB Coffee - select varieties
250-300 g
2 for $5.00
Sun-Rype Pure Apple Juice
1 litre
4 for $5.00
Western Family Peaches - select varieties
398 mL
3 for $3.99jnore grocery specials...
Kellogg’s Rice Krispies or Raisin Bran
700-775 g
$3.98
Post Cereals - select varieties
375-620 g
2 for $5.98
Kraft Peanut Butter - select varieties
500 g
$2.98
Western Family Liquid Honey - unpasteurized
500 mL
$2.48
Robin Hood Quick Oats
3kg
$3.98
Western Family Jam - select varieties
375 mL
$2.28
Kellogg’S Pop TUrtS - select varieties
300 g
2 for $3.98
Western Family Granola Bars - select varieties
187-240 g
$1.88
Western Family Ketchup - squeeze
1 litre
$1.98
Western Family Pickles - select varieties
1 litre
$1.88
Western Family Mayonnaise or whipped salad dres<
ing 950 mL
$2.88
Western Family Pure Canola Oil
1 litre
$1.98
Western Family Pink Salmon
213g
$1.48
McCain Punches or Blends - select varieties
1 litre
.98
Minute Maid Juice, Punch or Five Alive Citrus
10x200mL
$3.28
Ruffles Potato Chips - select varieties
270 g
3 for $4.98
Old Dutch Crunchys or Corn Chips - select varied
!S 400-430 g
2 for $5.00
Old Dutch Arriba Tortilla Chips - select varieties
190 g
3 for $3.00
Old Dutch Restaurante Tortilla Chips or Salsa
312-4O(Jg,
4.7^ ml
2 for $5.00
Western Family Jelly Powders - select varieties
9-85 g
2 for .88
Western Family Pudding Snacks - select varieties
4 pack
4 for $6.00
Western Family Cookies - select varieties
400 g
$2.28
Pepsi, 7-up and Pepsi Products - select varieties
2 litre
2 for $3.00
Household Items...
Western Family Reclosable Sandwich Bags
50 pack
$ 1.48
Sifto Aqua Magic Crystal Plus Water Conditior
¡er 20 kg
$3.88
Value Priced Fabric Softener
3.6 litre
$1.8811
More Household Items...
Pedigree Mealtime Dog Food - select varieties
8 kg
$13.98
AlpO Dog Food - select varieties
624-630 g
2 for $3.00
Alpo Balanced Diet Dog Food - select varieties
7.2 kg
$11.98
Western Family Cat Food - select varieties
156 g
2 for .88
value Priced cat Food - complete, dry
8kg
$8.88
Alley Cat Cat Food
1 kg box
$1.98
Kitty Kit Clay Cat Litter
10 kg
$5.98
MARKtt 3R£SH PBOBUC£!
Green Seedless Grapes - Chilean grown, # i grade
$3.04/kg
$ 1.38/lb
Fresh Broccoli - U.S. grown
$2.16/kg
.98/lb
Mini Carrots - U.S. grown, # 1 grade
2 lb bag
$2.98
Premium Bananas - product of Ecuador
$1.28/kg
.58/lb
Kiwifruit - California grown
3 for .88
BUtUttR'S BURS 03 £H£ W££K!
Whole Frying Chicken - 2 pack
$3.48/kg
$1.58/lb
Chicken Legs, back attached - value pack 3-4 lbs
$ 1.94/kg
.88/lb
Chicken Breasts, back attached - value pack 3-4 lbs
$5.47/kg
$2.48/lb
Schneider’s Bologna - regular or beef
500 g
$3.28
Schneider’s Bacon - select varieties
500 g
$4.48
Fleetwood Rings - select varieties
300 g
$2.78
Fleetwood Bavarian Smokies
1 kg
$6.98
Schneider’s Wieners - select varieties
450 g
$2,98
Western Family Sliced Meats - select varieties
175 g
$1.18""Home of the Handyman”
Ryobi Power Tool Sale!
7 '/4"" Circular Saw -13 amp
Special Buy $139.99
13/4 HP Router
Special Buy $119.99
8 inch Drill Press
Special Buy $99.99
7.2 Volt Cordless Drill
w/ 2 batteries
Special BuY S69?9______
4 pack - Home Hardware
Light Bulbs - 40, 60, 100 W
Regular $1.09/pack
Special 67t/pack
Check our price on our Home Pack
Electrical Devices.
Prices are comparable to
Wholesale Prices!_______
Home Manual Food Chopper
Regular $15.99
Special $11.97
6 Outlet Electrical Tap
Regular $3.19
Special $2.25
Duracell Batteries are on
Special!
AM - Reg. $4.49 - Special $3.47
C - Reg. $4.49 - Special $3.97
9V - Reg. $4.49 - Special $3.97
M - Reg. $2.99 - Special $2.57
D - Reg. $4.49 - Special $3.97
Unival
10/30 Engine Oil - 4 litre
Regular Low Price $6.49
Sale Price $5.32
40 Watt Appliance Bulb
Regular $1.49
Special $1.00 each
Home Plumber Iron Remover
567 G - Regular $6.49
Special $5.00 each
Fire Sentry Smoke Detector
9V Battery included
Regular $5.99
Special $4.97
We have three
Seed Displays out!
Home b-OKtwaie
Century S 244E Wood Burning Stove
High Efficiency - Heats up to 1000 sq. Ft.
Only $469.9","Magrath Store News (February 28, 2001)",,J. A. Ririe,,,core
24358785,1997,". This paper presents a hardware-friendly approach for adapting the structure of a reinforcement, learning-based neurocontroller. An unsupervised clustering algorithm is used to partition the state space of a system and to adapt the size of its reinforcement module. In the wellknown inverted pendulum problem, the system has proven to be much faster than previous neurocontroller approaches. We are currently working on an implementation of the system using field-programmable logic devices. 1 Introduction  A major problem in nonlinear control is the tuning and adaptation of the controller. For this purpose a model of the process is usually developed. Then, following an approximation of the inverse relation between the desired outputs and the control actions, the controller is adjusted. However, for many real-world problems there is no available quantitative data regarding input-output relations, rendering analytical modeling very difficult [6]; furthermore, errors in the model can lead to..",Structure-Adaptable Neurocontrollers: A Hardware-Friendly Approach,,Springer Verlag,10.1007/bfb0032585,,core
42783246,"Nov 1, 1994","Since the launch of NASA's Extreme Ultraviolet Explorer (EUVE) satellite in 1992, there has only been a handful of occurrences that have warranted manual intervention in the EUVE Science Operations Center (ESOC). So, in an effort to reduce costs, the current environment is being redesigned to utilize a combination of off-the-shelf packages and recently developed artificial intelligence (AI) software to automate the monitoring of the science payload and ground systems. The successful implementation of systemic automation would allow the ESOC to evolve from a seven day/week, three shift operation, to a seven day/week one shift operation. First, it was necessary to identify all areas considered mission critical. These were defined as follows: (1) The telemetry stream must be monitored autonomously and anomalies identified. (2) Duty personnel must be automatically paged and informed of the occurrence of an anomaly. (3) The 'basic' state of the ground system must be assessed. (4) Monitors should check that the systems and processes needed to continue in a 'healthy' operational mode are working at all times. (5) Network loads should be monitored to ensure that they stay within established limits. (6) Connectivity to Goddard Space Flight Center (GSFC) systems should be monitored as well, not just for connectivity of the network itself but also for the ability to transfer files. (7) All necessary peripheral devices should be monitored. This would include the disks, routers, tape drives, printers, tape carousel, and power supplies. (8) System daemons such as the archival daemon, the Sybase server, the payload monitoring software, and any other necessary processes should be monitored to ensure that they are operational. (9) The monitoring system needs to be redundant so that the failure of a single machine will not paralyze the monitors. (10) Notification should be done by means of looking though a table of the pager numbers for current 'on call' personnel. The software should be capable of dialing out to notify, sending email, and producing error logs. (11) The system should have knowledge of when real-time passes and tape recorder dumps will occur and should know that these passes and data transmissions are successful. Once the design criteria were established, the design team split into two groups: one that addressed the tracking, commanding, and health and safety of the science payload and another group that addressed the ground systems and communications aspects of the overall system",Designing an autonomous environment for mission critical operation of the EUVE satellite,https://core.ac.uk/download/pdf/42783246.pdf,,,,core
42769102,"Oct. 18, 1998","We have outlined our work in the last half of the funding period. We have shown how a demo package for RESSAP using MPI can be done. However, we also mentioned the difficulties with the UNIX platform. We have reiterated some of the suggestions made during the presentation of the progress of the at Fourth Annual HBCU Conference. Although we have discussed, in some detail, how TURBDES/PUMPDES software can be run in parallel using MPI, at present, we are unable to experiment any further with either MPI or PVM. Due to X windows not being implemented, we are also not able to experiment further with XPVM, which it will be recalled, has a nice GUI interface. There are also some concerns, on our part, about MPI being an appropriate tool. The best thing about MPr is that it is public domain. Although and plenty of documentation exists for the intricacies of using MPI, little information is available on its actual implementations. Other than very typical, somewhat contrived examples, such as Jacobi algorithm for solving Laplace's equation, there are few examples which can readily be applied to real situations, such as in our case. In effect, the review of literature on both MPI and PVM, and there is a lot, indicate something similar to the enormous effort which was spent on LISP and LISP-like languages as tools for artificial intelligence research. During the development of a book on programming languages [12], when we searched the literature for very simple examples like taking averages, reading and writing records, multiplying matrices, etc., we could hardly find a any! Yet, so much was said and done on that topic in academic circles. It appears that we faced the same problem with MPI, where despite significant documentation, we could not find even a simple example which supports course-grain parallelism involving only a few processes. From the foregoing, it appears that a new direction may be required for more productive research during the extension period (10/19/98 - 10/18/99). At the least, the research would need to be done on Windows 95/Windows NT based platforms. Moreover, with the acquisition of Lahey Fortran package for PC platform, and the existing Borland C + + 5. 0, we can do work on C + + wrapper issues. We have carefully studied the blueprint for Space Transportation Propulsion Integrated Design Environment for the next 25 years [13] and found the inclusion of HBCUs in that effort encouraging. Especially in the long period for which a map is provided, there is no doubt that HBCUs will grow and become better equipped to do meaningful research. In the shorter period, as was suggested in our presentation at the HBCU conference, some key decisions regarding the aging Fortran based software for rocket propellants will need to be made. One important issue is whether or not object oriented languages such as C + + or Java should be used for distributed computing. Whether or not ""distributed computing"" is necessary for the existing software is yet another, larger, question to be tackled with",Parallelization of Rocket Engine Simulator Software (PRESS),https://core.ac.uk/download/pdf/42769102.pdf,,,,core
480869613,2000-01-01T08:00:00,"This thesis studies the applicability of the Self-Organizing Maps (SOM) in generating the necessary joint space coordinates for robotics path planning.A survey of literature about robotics path planning and neural networks, more specifically, Self-Organizing Maps was conducted. The problem of how to approximate a Cartesian path to be followed by the end-effector of a robot was investigated. Possible methodologies for path approximation based on the SOM and linear interpolation was formulated. Thereafter, the SOM training procedures and the approximation methods were implemented. SOM of various sizes were trained to determine the appropriate values for the training parameters and to ensure that proper map organization is accomplished.The approximation methods were tested via simulation. The simulation was conducted considering the dimensions of an actual SCARA robot. Approximation of primitive paths, specifically, straight-line paths and circular arcs were carried out. The approximation results were then analyzed based on the Euclidean distance between the theoretical and approximated points as the basis.In the study, a SOM based path planner for a SCARA robot was designed and implemented to use three approximation methods, namely approximation using nearest node, rectangular patches and triangular patches.
Because these approximation methods require proper SOM organization, the proper training parameters, a and y, were determined. The parameter a represents the percentage the map values are brought closer to the training data and y represents the neighborhood parameter, which correlates to the training radius. The unsupervised learning scheme was used because its performance was comparable to that of the supervised learning scheme. Thus, the values of the joint coordinates, 01 and 02 are the only information which had to be stored in the SOM nodes.Simulation was done for approximation of straight lines and circular arcs, the primitive paths of a robot to determine if the approach was applicable. The Euclidean distance between the ideal sampling point and the approximated point was used as a criterion of the accuracy of the approximation method.The simulation results revealed that the approximation using nearest node is not a feasible method because of the large discrepancy between the theoretical and approximated path. Approximation using rectangular and triangular patches prove to be applicable for use with SOM of reasonable size. In both cases, the errors were deemed acceptable for most real life industrial applications. It was also noted that the equations for the triangular patch approach are linear while those for the rectangular approach are quadratic. Thus, the triangular patch approach is simpler, faster, and more suitable for real-time applications.In conclusion, the SOM is applicable for use in the 2D path planning of a SCARA robot, although further research is needed to refine the methodology",Self-organizing maps for path planning of a SCARA robot,,Animo Repository,,,core
100067320,1996,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a tar-get by detecting and avoiding occlusions. We call the internal representation that describes the learned be-havior “stereo sketch. ” First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with MDL principle in terms of image coordinates and disparity information obtained from the fast stereo matcher based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion dispari-ty estimation process, we construct a state space for a reinforcement learning method to obtain target reach-ing behavior. As a result, the robot can avoid obstacles without explicitly describing them. We give the com-puter simulation results and real robot implementation to show the validity of our method. ",Stereo sketch: Stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance,,,,,core
24653700,1991,"Next generation real-time systems will require greater flexibility and predictability than is commonly found in today&apos;s systems. These future systems include the space sta-tion, integrated vision/robotics/AI systems, collections of humans/robots coordinating to achieve common objectives (usually in hazardous environments such as undersea explo-ration or chemical plants), and various command and control applications. The Spring kernel is a research oriented kernel designed to form the basis of a flexible, hard real-time operating system for such applications. Our approach challenges several basic assump-tions upon which most current real-time operating systems are built and subsequently advocates a new paradigm based on the notion of predictability and a method for on-line dynamic guarantees of deadlines. The Spring kernel is being implemented on a network of (68020 based) multiprocessors called SpringNet. ",The Spring Kernel: A New Paradigm for Real-Time Systems,,,10.1109/52.88945,,core
24291065,1998,"The recent explosion of on-line information in Digital Libraries and on the World Wide Web has given rise to a number of query-based search engines and manually constructed topical hierarchies. However, these tools are quickly becoming inadequate as query results grow incomprehensibly large and manual classification in topic hierarchies creates an immense bottleneck. We address these problems with a system for topical information space navigation that combines the query-based and taxonomic systems. We employ machine learning techniques to create dynamic document categorizations based on the full-text of articles that are retrieved in response to users&apos; queries. Our system, named SONIA (Service for Organizing Networked Information Autonomously), has been implemented as part of the Stanford Digital Libraries Testbed. It employs a combination of technologies that takes the results of queries to networked information sources and, in real-time, automatically retrieve, parse and organize the..",SONIA: A Service for Organizing Networked Information Autonomously,,,,,core
42787396,"Jan 1, 1994","THOR is a knowledge-based system which incorporates techniques from signal processing, pattern recognition, and artificial intelligence (AI) in order to determine the boundary of small thunderstorms which develop and dissipate over the area encompassed by KSC and the Cape Canaveral Air Force Station. THOR interprets electric field mill data (derived from a network of electric field mills) by using heuristics and algorithms about thunderstorms that have been obtained from several domain specialists. THOR generates two forms of output: contour plots which visually describe the electric field activity over the network and a verbal interpretation of the activity. THOR uses signal processing and pattern recognition to detect signatures associated with noise or thunderstorm behavior in a near real time fashion from over 31 electrical field mills. THOR's AI component generates hypotheses identifying areas which are under a threat from storm activity, such as lightning. THOR runs on a VAX/VMS at the Kennedy Space Center. Its software is a coupling of C and FORTRAN programs, several signal processing packages, and an expert system development shell",Thunderstorm Hypothesis Reasoner,https://core.ac.uk/download/pdf/42787396.pdf,,,,core
235574739,1998-04-22T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.I
MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
35 cents
CANADIAN
CANCER
SOCIETY
SOCIÉTÉ
CANADIENNE
DU CANCER
ALBERTA/NWT
LETHBRIDGE & DISTRICT UNIT
Wednesday April 22, 1998
Profile of A Volunteer
What is a Volunteer? A volunteer is a special
human being who helps others get a job done-any
job-without being paid. Volunteers are
found among men & women, the young & old,
the large & small, the educated & not-so-educated.
They come from all races, ethnic
groups, social & economic classes. While they
are not identifiable by appearance, ""by their
smiles you shall know them"". They are also
known by their willingness to jump to the task
at hand. Nobody rolls up their sleeves faster,
uses more elbow grease, & gets the job done
better. Volunteers are good at lots of things,
especially one that the rest of us don't get
around to doing. They can file, stuff, count,
sort, collect, distribute, bargain, organize,
supervise, manage. Volunteers have the tenacity
of mountain goats, the gentleness of lambs, the
strength of oxen, the speed of impalas, the
dependability of work horses & the eagerness
of new puppies. If we are comparing them to
animals, we must also note they are a breed
apart. They can raise funds, lower costs, spread
enthusiasm, reduce work loads, ease pressure &
warm hearts. They are pros at soothing,
cajoling, explaining, describing, convincing &
adapting. They often arrive just when the stress
is greatest, the moment bleakest, the work
hardest, the light in the tunnel the farthest
away. When the work is finished, try to say
""Thank You"" to a volunteer & he or she is apt
to say ""The Pleasure was all mine”.
******
Volunteer Week - April 19 - 25th
The Magrath Hospital Auxiliary Executive
would like to pay tribute to all volunteers
within the town & district and to our own
members for the hours they give each year
as volunteers.
Our heartfelt thanks to all
******
April is Canadian Cancer Society Month
Together We Can Beat Cancer.
The Canadian Cancer Society is a national
community-based organization of volunteers,
whose mission is the eradication of cancer &
the enhancement of the quality of life of people
living with cancer.
The Canadian Cancer Society is the only
cancer organization in Canada that works on
all aspect of the disease.
Toll free Cancer info line 1-888-939-3333
Canvassers will be calling on you,
please be generous.
To volunteer call Mary Sallenbach @ 758-6839
or coordinator Hazel Dudley @ 758-3213
******
Reform Party Nominations
Policy Meeting
Rick Casson M.P.
will be in attendance.
Thursday
April 23, 1998
7:00 - 8:00 p.m.
Trading Company Hall ,
For more information call 758-6392.
******
Library News
Adult Non-Fiction Saga of a Wayward Sailor by Tristan Jones, In the Name of Love by Ann Rule, Eco Friendly House Plants by B.C. Wolverton, Better Homes and Gardens New Garden Book, Exploring America's Backcountry by National Geographic Society, Grand Canyon The Story Behind the Scenery, The St. Lawrence Valley by Natural Science of Canada.
Juvenile Non-Fiction
What's Inside? The Story of an Egg That Hatched, the Young Rider by Lucinda Green, Draw and Write Your Own Picture Book by Emily Hearn.
Juvenile Fiction
For the Love of Benji, Return From Witch Mountain by Alexander Key, Escape to Witch Mountain by Alexander Key, The Hawk That Dare Not Hunt by Day by Scott O'Dell(Winner of the Hans Christian Andersen Award) Elizabeth is Mine by Kate William, Karen's Big Move by Ann Martin, The Mystery of the Lake Monster by Gertrude Chandler, Jessica's No Angel by Jamie Suzanne, The Skates of Uncle Richard by Carol Fenner.
Children's Books
See You Later, Alligator by Norma Charles, Too Many Rabbits by Peggy Parish, Never Tease a Weasel by Jean Soule.
Videos
The Stranger(directed by Orson Welles) The Terrible Tales of Mr. Bean, The Butter Cream Gang, Humpty Dumpty and Friends.
We would like to thank Joan boy for the many hours she devoted to the Library's pre­school story hour. Joan did a great job and we will miss her alot. We are very happy to have Shanna Clifton taking Joan's place, Wednesday mornings 10:00 a.m. at the library.
******
Friends of the Library will be making meat pies this Thursday, April 23rd. Orders can be placed at the Library, Thank-You.
******
They say swimming is great exercise, but I don't believe that.
Why not? Have you ever seen s skinny whale?
The Magrath Jr/Sr. High Parent Council will be selling
Candied Apples
& Popcorn balls at the School April 23rd at noon *Please support this fundraiser.
******
The Magrath Elementary School is now accepting KINDERGARTEN REGISTRATIONS for the 1998/99 school year.
Your child must be 5 by September 1/98 in order to be eligible.
Please come to the school and fill out a registration form. A birth certificate is needed.
MES Administration.
******
Museum News the Museum board now have a quilt display set up and would enjoy having you come and see the quilt art of years gone by. Thanks to those who have loaned the museum the various items to make this and previous displays a success. Our museum is open Monday evenings from 5-9 p.m. and Fridays from 1-5 p.m.
******
FOOTHILLS
APPLIANCE,-^ SERVICEMgg,
Owner Operator: Art Heninger formerly of Magrath , now living in Cardston
We repair all major appliances. We have good rebuilt appliances in stock, sold with guarantee Will come to Magrath weekly or whenever needed *No mileage charge* 653-1919Tyler Zobell has been called to serve in the
Utah, Provo Mission. He will enter the MTC
on May 13th. His farewell will be held on
Sunday, April 26th, 1 p.m. at the Garden Place
Chapel. All are invited to attend.
******
Rhonie & Wally Czech
are the proud parents
of a 9 lb baby boy
""Nathaniel Wallace""
Born April 14th, 1998.
Proud grandparents are
Joe & Lois Czech from Magrath
Marcell & Wendy Godeneir of
Medicine Hat.
******
Bearly Country
lace, ribbon & craft
FINAL dispersal
wide laces (flats) .50 yd. or less
everything else 1/2 price or less
*Huge Garage Sale everything from: lots of
clothes to older chrome suite:
Fri. & Sat. May 1st. & 2th
10: 00 a.m to 4:00 p.m.
at Stringam's
5 1/2 mi. West of Magrath
on Cardston High way
NO early birds please!
******
Keep moulded gelatin desserts from drooping
in summer weather by adding an extra package
of gelation to the mix. By doubling the gelatin,
your mould will stay firm & weatherproof.
GOOD'S AND SERVICES AUCTION
Sponsored by the Magrath First Ward
Garden Place Chapel(new white church)
Saturday May 2, 1998
To raise funds for the Ward's Youth Summer
Camps. Breakfast will start at 9:00 a.m.
Auction will start at 10:00 a.m.
This will be a goods & services auction with all
proceeds going to help fund Boys & Girls
Youth Camps.
Donations or info on goods & services can be
made by contacting up to the time of the
auction.
Darcy Sorpold 758-3835
John Balderson 758-6392
BradSabey 758-3779
Come out for a great breakfast and support
our youth by donating and/or by buying some
of the much needed items.
Nutrition Basics
by Ted Bouvier
One of the major factors to whether you will loose
or gain weight is your metabolic rate. This determines
how your body will use up the nutrients you take in.
One way to increase your metabolism is to
increase your activity level. When you increase your
activity level, you need to increase your nutritional intake
appropriately. To sustain your new activities, you can
increase your intake by having 5-6 small balanced meals
each day.
By having a well balanced meal plan and a
moderate activity level every day, you will be able to start
increasing your metabolism. This in turn will make your
body more efficient in burning more calories.
ANGE-EMILE LABBE
• Dry wall ‘Boarding • Taping
• Texture Wall & Ceiling
* Small Renovations
• Commercial & Residential
BOX 803
MAGRATH, AB TOK 1 JO
PHONE
(403) 758-6876
J
Physical Fitness Consulting
Specializing In
Individualized
Fitness & nutrition
Programs
FREE Body Fat Tests in May!
For More Information Call Ted Bouvier
75S-3236
b ou viere ® tel u epl a n et. n et
MAGRATH
MAC GREGOR HIGHLANDERS
Please mail or bring me a picture of yourself
(name on back) (if no picture then leave your
name if you danced with the performers in the
20 years of Ceilidhs held each spring in
Magrath Alberta from 1972 when this dance
group was first formed, to 1995 when I retired.
People to call regarding this is myself
Alice Stevenson (Sparky) Phone 758-3190 - or
mail to Box 36 magrath AB. TOK UO.
or phone: Anne Pilling - 758-3088
Melissa Hoy - 758-3191
Lois Harris - 758-3686.
We need this for the History Book of Magrath
Thanks for your help on this
Alice Stevenson.
******
Magrath Co-ed Softball
Grade 4-6
Fee $10.00
($15.00 after April 24th)
Practises begin week after Easter Break.
League play begins
Thursdays May 7 - June 18th
Please hand in registration form
to Dari Cook 758-3639
NOTE
Late registrations may be put on a waiting list
Magrath Minor Hockey would like to have
anyone who is interested in playing roller
hockey one night a week to give Rod Ririe a
call at 758-6785. If there is enough interest you
will be contacted.
******
Remember to come and support your library
and museum meat pie project this Thursday
the 23rd.
The museum will also be hosting a ""Down
Home Social"" on June 5th with a roast beef
barbecue dinner and excellent entertainment.
******
*When rolling dough, instead of flouring the
pastry cloth or board, use a light dusting of
cornstarch. It actually works better than flour,
leaves no starchy aftertaste & makes cleanup
easier.
Thanks for vour help!!
The Elementary School Council would like to
show their appreciation to all the volunteers
who donated their time and talents to make our
programs, activates and fund raisers such a
Success. We would also like to offer a special
thanks to the parent room reps for all the extra
things that you do to help out!
The magrath Elementary School Council
******
On thursday, April 9th, the elementary
students and many staff participated in our
final playground fundraiser. It was great to see
all the kids out to support the Walk-a-thon. We
were thrilled to have almost all the students
walk the entire hour.
Thank you to all the family and friends for
your pledges.
Thank you also to the town for your help with
traffic control
The Magrath Elementary, School Council
LIVING TRIBUTE
Since the beginning of time, trees have symbolized life.
It’s a tradition proudly honoured in our Living Tribute
selection of monuments, now at specially discounted
prices. With your purchase, a tree wdl be planted in our
northern forests. It’s a beautiful tribute to your loved
one, while creating a green space to be enjoyed and
v... ôimicu uy au.
For more information contact:
HEATHER THOMSON
758-6386
FABRIC SALE
Saturday April 25th at 9:00 a.m. come check out the great deals at Marty’s Draperies. 231 1 st Ave. South
Magrath.
(Straight East of the Post Office).
Fabric on sale for:
$1.00/ yard
$2.00/ yard
$3.00/ yard
Also on sale: already made curtains and valances. Great Selection!CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758-6377
Less than 30 words—$1.50 Small ad (2.5""X 3.5"")-$6.50
1/4 page---------------------$8.50
1/3 page---------------------$9.50
1/2 page-------------------$15.00
Full Page-—Copy Ready-— $30.00Full Page—We do— $45.00
Flyer insertion (your paper)
Free for the taking-iris rhizomes-already dug, ready to plant Holt’s 758-3201.
******
For sale: boy's 12"" bike, good cond, tent trailer for parts. Call 758-6632.4-29.
******
Gun for sale: 270 caliber Remington Gamemaster model 760 with 4 x 32
J.C. Higgens Scope. Phone 758-6141. 4-22.
******
For sale: Harley Daviedson golf cart with canopy, very good cond. Call Reed
@ 758-3230. 4-22.
For sale: woodmade toybox/deacon bench. Call 758-6521. 4-22.
******
For Sale:soccer shoes-3 1/2 new, size 2 hardly worn. $15 each, soccer shin pads-Puma 10"", 8"" pair $5 ea. Girl's Fila bike from Costco-2 yrs old. $50. Wamboldt's 758-3994
******
""Discovery Toys"" are you interested in the very best developmental toys & software? Call Katherine
Culley @ 758-3433.4-22
******
For sale: 2 single shot 22 rifles $70 ea. 758-3729.
For sale from Magrath hospital-several manually operated beds, inc. mattress only $100/bed. Call 758-3371 & ask for Dan Reeder in maintenance. 4-22.
******
For sale: 160 gal upright water storage tank with bottom ouflet/valve. $100. 758-3153. 4-22.
******
For sale: Inglis gas dryer, recent overhauled new motor, kenmore elec dryer. Burning barrels for sale. Call 758-3968.4-29.
******
Important Notice Starting May 11/1998 I will be cutting hair at home.
Mondays Only: from 9:00 - 4:00.
Thank You Frank's Barber Shop
5-6. ***rf**r .
For sale: 3 bikes, 2-10 sp; 1 lady's bike between $5-$10 ea. 1 complete deep water well pump system 758-6839.
******
Spring Garden Plowing
& Rototilling Call Mike/Kyle Harker at 758-6664.
******
POWER RAKING 2 yrs. experience with professional power rake. Call Tom Kerran 758-6233. 5-13.
******
Moving Sale Sat. April 25 9 a.m. - 12 noon The Murray's 185 East - 1 Ave. North desk, couch set, games, 2 microwave stands, puzzles etc.
******
Multi-Neighbour Garage Sale Blackmer's, Hansen's & Cook's are having a garage sale on Sat. Apr. 25. 9 am-12 noon at the Blackmer's, some of the items will be: bathtub, t.v., china cabinet, lamps, vases, dresser, babycar seat, flowers & ribbon for your deocrating needs etc.
******
Renovation/Garage Sale Perry's & Johnson's. Sat. May 2nd 8:00 a.m. #5 Magrath Place­Perry's Residence, bunkbeds, dresser, rowing machine, boys & girts clothes, couches, chairs, Nintendo, doors & door frames. Much Much More.
******
******I’m Moving Garage Sale no furniture
Friday April 24th
10 - 12 noon & 2 - 5 p.m. 10% off Regal 530 South- 3 St. West 758-3474.
AAAAAA
Bearly Country lace, ribbon & craft FINAL dispersal wide laces(flats) .50/yd or less, everything else is 1/2 price or less *Huge garage sale - everyting from lots of clothes to older chrome suite.
Fri & Sat. May 1 & 2 10-4 p.m. at Stringams’s 5 1/2 mi. West of Magrath on Cardston Highway *NO early birds please!!
A A A A A A A
BINGO - BINGO Next Bingo is Thurs. Apr 30 at the Seniors' Centre. Doors open @ 6:30 Bingo @ 7 p.m. All Welcome!!!
AAAAAA
JEANNIE'S HAIR FASHION 136S - 1 St West 4 doors South of Trading Co. 758-3379 Open Tues thru Friday Professional Haircare at pleasing prices AAAAAA
For rent: 2 bed apt. furnished or unfurnished. Cah Tyler @ 758-3590 or 758-3595.
******
RICK'S PORTABLE WELDING Owner Rick Beres We weld everthing from chairs to tractors Please call me for all your Welding needs ""B"" pressure qualified. 758-6427
For Rent: Commercial & storage space (interior or exterior) available in Magrath- for more info 758-3490.
******
For rent: Clean 3 bedroom house, 2 baths, great location available immediately 758-378
******
For rent: 2 bedroom house east of Bank of Montreal avail immediately.758-6725. ******
For Rent:Evergreen Manor Senior's apartment in Magrath, 2 bedrooms, laundry room appliances inc. $525/mos. Call @ 758-6380
For rent: 4 bedrm, 4 bath, partially furnished house, Irg double car garage, nice yard. NS, ND, NP $400.mo+ utilities. $400 d.d. some cond apply. 758-3153.
******
For rent: 2 bedroom house in Welling. Non-smoking for abstainers-adults only fridge & stove inc. 752-3848.
******
Attention
New Bottle Depot Hours Monday - 2 - 5 tuesday - Closed Wedmesdau - 2 - 5 Thursday - Closed Fro day - 10 am - 5 pm Saturday - 1- am - 5 pm Sorry for the incovenince but due to other committments we need to change. Thanks Tom Alston.
******
House for rent: 3 bedroom nice yard garden & garage 139 - 2 St. West. 758-6569
******
This very nice remodelled 1600 sq. ft. home has new wiring, plumbing, furnace, jacuzzi bath, new European cupboards with oak trim neutral colors throughout. The main floor has 3 bedrooms, 1 1/2 baths, laundry, family room, livingroom & kitchen. Only asking $67.000, available April 1st. Located 22W Harker Ave. Phone 758-3992 or 382-5205 to view.
******
For sale double wide mobile home to be moved. 1440 sq. ft.
3 bedroom, family room, living room, large dining & kitchen area, laundry room, 2 baths (2 sinks, garden tub in master bedrooin) garage & deck attached. Call 758-3343.
******MEAT SPECIALS APRIL 20-25
Outside Round Roast $2.28/lb $5.03/kg
Standing Rib Steak $4.48/lb $9.88/kg
Western Family ^Sliced Cooked Meats-bologna, garlic
bologna, mac & cheese, mock chicken
175g .98
Western Family Sliced Cooked Ham 175g $1.98
Fletcher's Wieners 450g $2.48
Monday Ma-
Scficoi
O Î i
■p'^CSCrA^ ’
WOLSET0S
WORKS • *
Renouation & Handyman Seruice<£
■ ' '_________ •_________ —. ■ '•••'•>??
Mr. ""Meant-to"" has a comrade And his name is
""Didn't-do"",
Have you ever chanced to meet them?
Have they ever called on You?
These fellows live together
In a house called ""Never-win""
And I'm told that it is haunted
By the ghost of ""Might-have-been.
Don't complain about the way the ball bounces
if you're the one who dropped it.
www.ssdirect.com ssdirect@telusplanet.net
1J0
Roger Davies
General Manager voice 758-3577
fax 758-9174
General & Sub
Contracting
Curt’s Construction
P.O. Box 535
Magrath, Alberta
TOK 1J0 CURTIS HATCH
Ph: (403) 758-3759
MORE GROCERY SPECIALS APRIL 20 - 25
""Hi Folks""
The United Church's
3rd Annual
Garage & Bake Sale is coming soon!
SATURDAY MAY 23rd.
9:00 a.m. - 2:00 p.m.
So as you spring clean and have stuff yon don't
know what to do with and want to donate to a
good cause, phone us for pick up or delivery
Garage: Betty Twitchen @ 758-3240 or
Darlene Holzworth @ 758-3569
Baking: Joan Frenzel @ 758-3273.
Pringles Potato Chips 170-190g $2.18
Western Family Potato Chips 32g 3 for .98
Best Foods Mayonnise 500 ml $2.98
Western Family Mustard 500 ml $1.48
Kraft BBQ Sauce 455 ml $2.28
Mainstay Dog Food 8 kg $5.98
9 Lives Cat Food 156g 2 for .88
Kleenex Ultra Soft Bathroom Tissue 24 pk $7.98
Viva Jumbo Paper Towels 6 pk $5.98
Glad Garbage Bags 10 pk $2.48
Sunny Delight Drinks 1.89 liter $1.98
Kraft Chip Dip 227g $2.18
McCain Superfries 1 kg $1.98
Western Family Ice Cream 4 liter $4.98
Western Family Orange Juice 355 ml .88
Minute Maid Limeade 355 ml .88
Minute Maid Lemonade 355 ml .88
Minute Maid Punch 355 ml .88
In Your Town™
MOBILE DENTURE CLINIC
GEORGE TORRE-ALBA
DENTUR1ST
MAY 4
PRODUCE SPECIALS APRIL 20 - 25 -
Bananas .48/lb $1.06/kg
Bartlett Pears .98/Ib $2.16/kg
Cantaloupe .78/lb $1.72/kg
Celery Heats $1.48
Fresh Peeled Mini Carrots 2 lb $2.78
| Cauliflower
$1.18/lb $2.60/kg
SENIORS NEWS
We will have the Wednesday suppers for
April the 29th at 5:00 p.m. at the Seniors'
Center.
The Pot Luck supper will be Friday April
24th. Come join us for good food and good
CRAFT SALE IN MAGRATH
The Senior Center in Magrath will be selling
tables for our June 5th Craft Sale - $10/table.
We have room for 20 tables. 1st come 1st
company.
♦Memberships, & cleaning fees are now due
at $10.00 each for 1998.
♦Bookings for the Senior's Centre now should
be made through Jack or Jean Butlin 758-3030,
rental fee is $75.00.
SENIORS TRAVEL NEWS
Would you like to go to dinner & a show?
The seniors are going to dinner at the Seniors
Centre in Lethbridge & then a Variety Show at
the Yates Centre on April 28/98. It will be
$15.00 for the bus, dinner & show. The bus
will arrive at 4 p.m. and leave at 4:15. Supper
is at 5 o'clock and the entertainment will be at
7 p.m. Come & enjoy the entertainment.
Contact Margaret Leishman 758-3241 or
Wallace & Mary Baker 758-3207.
WANTED TO BUY; a good used Bosch for the
Magrath Seniors' Centre. Contact Lois Czech
at 758-6308. ******
_ Domestic & Commercial
Water Hauling
3000 Gal & 4000 Gal capacity
Pumping Services
Available
Cistern Cleaning
Bus. 328-2460
steele sheridan Home 758-3698
♦No Deliveries Sundays and Holidays
served. There will be no smoking for this sale,
there will be scones, hot dogs & fresh rolls for
sale along with a Bake Table. Please contact
Lois Czech 758-6308 if interested.
******
The Raymond and District Agricultural Society
will be holding their annual
FLEA, CRAFT, TRADE, & BAKE SALE
Sat. April 25th, 9 a.m. - 3 p.m..
Tables are available for $15.00 each.
Phone Myrna Segboer at 752-3801
or fax at same number.
Success
""To laugh often & love intelligent persons &
the affection of children; to earn the
approbation of honest critics & endure the
betrayal of false friends, to appreciate beauty;
to find the best in others; to give of one's self;
to leave the world a bit better, whether by a
healthy child, a garden patch or a redeemed
social condition; to have played & laughed with
enthusiasm & sung with exultation; to know
even one life has breathed easier because you
have lived - this is to have succeeded""
Gem of the Day(Credit the Prairie Rambler)
No amount of careful planning will ever replace
dumb luck.
HOME HARDWARE
""HOME OF THE HANDY MAN""
4BEAI1I-K)M£ LATEX ACRYLIC SEMI-GLOSS REGULAR $30.99 ON SALE FOR $22.97.
‘LATEX CEILING FLAT PAINT. REGULAR $25.99, NOW ON SALE FOR $16.97.
‘BEAUTI-TONE LATEX KITCHEN & BATHROOM ENAMEL. REGULARLY $31.99, NOW ON SALE FOR $24.97.
‘SCRAPER SETS 2"" AND 5"" PACK - $3.33.
‘BEAUTI TONE SPRAY PAINT 350G SIZE ON SALE FOR #3.37.
‘EXTRA STRONG GARBAGE BAGS. CLEAN & ORANGE ARE ON AT 1/2 PRICE. REGULAR $1.49, NOW ON FOR .74.
‘25 LITRES OF TOP SOIL ON FOR $2.47.
‘GATOR GRIP 3/8 UNIVERSAL SOCKET. REGULAR $39.99, NOW ON SALE FOR $33.97.
*OUR BULK SEEDS ARE NOW OUT, SO WHY NOT DROP BY AND CHECK OUT OUR VARIED SELECTION.
‘OUR STAFF ARE VERY KNOWLEDGABLE AND WILLING TO HELP IN ANYWAY. THEY KNOW THEIR SEEDS AND CAN GIVE YOU ADVICE ABOUT WHAT DOES WELL AND WHAT DOESN'T IN THIS PART OF ALBERTA. SO COME ON IN AND HAVE A TALK WITH THEM.GROCERY SPECIALS APRIL 20 - 25
Dairyland Aerosol Real Whip
225g
$2.98
Orchard Hill California Citrus Drink
1 ","Magrath Store News (April 22, 1998)",,J. A. Ririe,,,core
235573287,2000-10-11T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH NEWS! a
Published Weekly Since 1932 by
The Magrath Trading Company
irà
(cI
Come to the
I AH VAKIETT SHEW
Friday, October 13th 7:30 p.m.
. School Auditorium Ly Admission $2/person, $10/family
Featuring Local Talent
Tickets sold at the door
Sponsored by the
•MAGRATH CULTURAL ARTS SOCIETY
Points of Interest
* Town of Magrath
Public Meeting
* Seniors News
*
* Home Hardware
Sales
Magrath Elementary
Family Portrait Fundraiser
October 23 & 24
""Randy Neufeld Photographies"" will be bringing their studio to our
school to take family portrait sittings. The purpose of this event is to
help raise money for classroom cubbies in the school. For the small fee
of $15, all of which is kept by the school,you will receive the sitting plus
one 8x10 color portrait of your choice in a natural finish. You can book
your sitting by calling Randy anytime at 320-2121. Portraits will be
ready by Christmas. All are welcome so call and book your appointment
today.
For more information call Lorelei Fisher at 758-6184
Time Is Running Out!
Book Your Appointment Today!
We have 7 students from Magrath
playing football with Raymond. Last
Friday they beat Taber 21-16 with all 3
touchdowns coming from Magrath.
Jacob Swainson with 2, and Jimmy
Balderson with 1. Other players are:
Brooks Blackmer, Cody Lauscher, Jason
Still, Doug Mehew, & Jamie Zobell.
The School will be having a
BOOKFAIR
Thursday, Friday, Monday, & Tuesday
October 12,13,16, &17
In the School Library
>____________________________________________________-
' ■
Inside this issue:
Community Interests 2-5
Classified Ads 6-7
Community Calendar 8
Grocery Specials 9-10
Meat & Produce 11
Hardware Specials 12
——____
The MAGRATH CHIEFS are pleased to
bring back
“SUPER BILL BENTLEY”
On Nov. 3 @ 7:00 p.m. in the Tom Karren
Gym
Tickets are available from Del Bonita
Store, Home Hardware, Blue Goose 2000
or Hairstyles By Marion.
Tickets also available by calling 758-6567
Price is $5 - advance. $6 at the door.
If you are interested in getting nuts for
Christmas again, this is the time to do it
Call Peggy @ 758-6567 or Shainne @
758-3311.
The Orders Must be in by Oct. 24.
Deliveiy will be the week of Dec. 3-IOth.
and'’Joggin
dj-’iz. asiddrtahincj t&nbi
Óotii. O/lkdding
^Ptaf-F.: fDdoEr&i lejhii, 2000
fPianz: {¿Sei ßoniha. ¿dkdd
<D[i£ n liotut ihviti ah q.:OO jure.
Id^innzt iztasd ah 6:00 ^.Jn.
dPw^wjn and Usance. ho foidoar
¿No ^Pie-ienhi ate te^aeited
TOWN OF MAGRATH
PUBLIC MEETING
:•
WHERE: TOWN COUNCIL CHAMBERS
55 South 1st Street West
WHEN: Wednesday, October 18, 2000
TIME: 7:00 p.m.
Residents are invited to meet with members of Town
Council to share concerns, ideas and plans for our
community. Please make room on your calendar and
plan to attend as the future of these meetings depends on
your participation.
-r
Mayor and Council
Corns and join us for the 4th Annua' TOWN OF MAGRATH
CHRISTMAS IN THE COUNTRY.
We have Siained Glass.Handmatte
Quilts,Foikart and 30 much more.
Come join U3 for a cup of eider and
browse through our treasures. ■
YING’S RESTAURANT
Western & Chinese Cuisine
NOWOPEN!
Eat In & Take Out
Open: Mon. - Sat. from 8:30a.m. - 9:00 p.m.
Lunch Buffet (Mon. & Fri. only) 11:45-1:30,
$5.25
Supper Buffet (Sat. only) 5:30-8:00, $7.75
Lunch Combo Special (Tues., Wed., Thurs.)
$4.99
7% GST will be added to prices
Phone 758-3198
I
Ravon open hguse^
Wednesday Oct 1Ô, 2000 II
1 <“&
__
1
1i
'<■?•
! Come See What Avon *1as For
II
I
». !1
gSS i
fSSK 1
.11 New' Christmas Products *1
j ONDISPLAY !
'See Avon Fashlohs & StocMng Stuffer Ideasl*
l LOTS ©F DOOF- PMZESI •
I
:
I
i:
I
I!I
!i
l
!I
!II
!I
(No One Leaves Empty Handed)
Draw for Grand Priza
Place: Allyson Christenseh's
(115 NIA St. East)
Time: 1:00-0:00 PM.
Hosted by: Allyson Christensen
PinFy Coleman
Please feel Free to bring a Friend I!
The Raymond & District Agricultural
Society will be holding their 14th Annual
Christmas Craft & Bake Sale on Dec. 2/00
from 9am to 3 pm in the Agricultural
building and the ""Barn"".
Tables are $15.00. For booking phone
Myrna at 752-3801.
Thomas Scott Nordquist,
Son of Neil & Renee Nordquist
has been called to serve in the Bordeaux,
France Mission
He enters the MTC on November 29.
Elder Casey Norman Christensen
has been called to serve in the Colorado - Denver North Mission.
Be is having an Open Bouse October 14 from 7 - 9 pjn. at his home.
Bis Farewell will be held October 15,2000 at 10:00 atn. Spring Coulee tach.
Be welcomes everyone to come out and join in.
3
SENIOR CENTRE NEWS
Our weekly supper will be held to-nite,
Oct. 11, at 5 p.m.
Next Wed., Oct. 18 will be a strictly Pot
Luck.
Also, it’s Flu Shot Nite -
shots 6:00-8:30 p.m.
Come enjoy all the food.
Our regular monthly Pot Luck will be
Oct. 28 at 5:00 p.m.
All at the Centre.
Contact lean or Jack Butlin for rental of
the Seniors Centre.
Fee $75.00
Phone 758-3030
B1NGO*BINGO*B1NGO
Next Bingo is tomorrow night - Thursday,
October 12,
at the Senior’s Centre.
Doors open at 6:30 p.m.
Bingo at 7:00 p.m.
Everyone Welcome!!
/ Daughter's of the Utah Pioneers \
I are having a
Garage Sale on
Friday, October 13.
This will be held at
Hazel Dudley's from
1:00 - 7:00 p.m.
I
I
8&
a
i
iiI
Donated articles can be taken to
Hazel's or Lois Bourne's 9
J
4
Raymond Community Choir
Presents
""It's A Grand Night For Singing""
Oct. 20 at 7:00 p.m.
Raymond Community Theater
Tickets $5.00
Available from Choir Members
or Ph. 752-3661
Special Guest Artists
""Chords 'N' Stuff Barber Shoppers
Shelley Holt - pianist
Hometown Harmony
Ladies Barbershop Chorus
Maqrath Jr./Sr. Hiqh School
Parent Council
arc selling
Avon Collectable Ornaments
There are 3 ornaments/pkq. for
$10
This is a fund-raiser for Student
Awards.
Call Ilona at 758-6579
• ■ BBBB • ■ M^B ■ ■ «B • • BMB ■ ■ • • BBS ■ ■ BBBB ■ ■ B ■ iM■ ■ M BBBB I
The Skating Rink
is getting ready to open on
October 16
with Public Skating on the
16j 19, & 21
from
Free this week only!
6:30 - 7:30
Invites interested individuals to forward a Proposal for the following:
MAGRATH ELEMENTARY/MAGRATH JR./SR. HIGH SCHOOLS
SNOW REMOVAL CONTRACT
* Proposal for the 2000-2001 School Year and the 2001-2002 School Year
This would include the removal of snowfrom the sidewalks andparking lot
of the schools at least 20 minutes prior to the start of school each morning.
* Interested individuals may contact Mr. John Halvorson, Director of Operations and
Maintenance at 653-4751 for more information if required.
* Proposal must be in at Westwind School Division No. 74 Central Office located at
445 Main Street, Cardston by the close of business (4:30 p.m.) Wednesday, October 25,2000.
Please forward proposal to:
“Magrath Snow Removal”
Westwind School Division No. 74
Attention: Mr. Drew Chipman
P.O. Box 10,445 Main Street
Cardston, Alberta TOK 0K0
5
IN THE MUSEUM...
For the next two months we are featuring a Ceramics
""Sunburst"" display. The company vho made this glassware
was originally owned by Ralph Thrall of the McIntyre Ranch.
They were made in Lethbridge between 1960-1975 and are
now a collectors item.
We would like to thank all those who have contributed
artifacts and money to the museum. As a board we are
dedicated to preserving our heritage. Thanks also to those
who have loaned items for our dispbys.
Community Adult Learning
Computer Classes
Maximizing the Internet-Part 1
You’ve got the Internet or you’re thinking about
getting it. How do you make the most of it? This three-week
course will introduce you to Internet Web browsing-basic
search techniques, using bookmarks, etc. and E-mail
(POP3 and free web-based).
Dates: Wed., Oct. 11-25 (Cardston)
Time: 7:00-9:30p.m.
Location: CHS
Tuition: $35
Instructors: Bonny West/Brenda Beck
Microsoft Word Basics
This three-week course will introduce you to the
basics of word processing using Microsoft Word.
Participants will learn how to create, edit, and format
documents. File management, tabs, text alignment,
adding graphics, spell checking/thesaurus,
bullet/numbering, auto-formatting, creating special text
effects, etc.
Dates: Tues., Nov. 7-21 (Magrath)
Wed., Nov. 8-22 (Cardston)
Time: 7:00-9:30 p.m.
Location: MHS and CHS
Tuition: $35
Instructors: Bonny West/Brenda Beck
Microsoft Powerpoint Basics
During this three-week course participants will
learn how to create an effective electronic slide show
presentation. Creating/modifying slide layouts, back­grounds,
color schemes, sorting/outlinging, adding clip art,
movies, sounds, and web linkswill be covered.
Dates: Tues., Dec. 5-19 (Magrath)
Wed., Dec. 6-20 (Cardston)
Time: 7:00-9:30 p.m.
Location: MHSandCHS
Tuition: $35
Instructors: Bonny West/Brenda Beck
MAGRATH CURLING CLUB
FALL EVENTS
Annual Meeting: Oct. 19, 2000 at 7:00 p.m.
Free Curling Clinic: Oct. 23, 2000 at 7:00 p.m.
This is an opportunity for new Curlers to try
the sport or the seasoned curler to brush up a bit.
Just show up at the rink!
Open Funspiel: October 24 - 29, 2000
Team or individual entries accepted.
Individuals will be placed on a team.
League Curling:
Starts the first week of November.
Please join us and enter a team today.
Anyone wanting to curl and have not found a
team, contact us and we will try to get you on a
team.
To enter the Funspiel or League, please
contact:
Maurice Bevers: 758-6726
Or
Ron Balderson: 758-6521
SEE YOU AT THE RINK!!
Make Your Own Desktop Movie
Using scanners, digital cameras and easy-to-use
video editing computer software-it is now easy to make
your own desktop movie. Participants will learn how to
create a personal memory video using slides, photos and
video clips. They will then add their choice of background
music, transitions, and voice annotation. A great idea for
Christmas gift-giving, weddings, sporting highlights, video
baby book, family reunions, etc. ftuticipants will take
home a completed digital video “movie” output to VHS
tape. (These movies may also be stored on CD, on the
Web, or sent via email.)
Participants need to bring their own photographs,
video clips and choice of background music (preferably on
CD). There will also be additional photos and music
available for use.
Dates: Thurs., Nov. 9-30 (Magrath)
Time: 7:00-10:00 p.m.
Location: Magrath Elementary School Lab
Tuition: $60 for course and materials
Instructors: Bonny West/Brenda Beck
CLASS
FREE MARKET
* NEED help with autistic girl on Sundays.
Phone Trish at 758-6834.
* Took an old Mac 2C Computer to the
clothing exchange then found more
accessories to it. If you’re interested phone
758-3712.
* WANTED - Green (or red) tomatoes if you
have some to give away. Phone 758-3545.
* TO GIVE AWAY - Olivetti Electric
Typewriter plus ribbon and eraser tape.
Call Doreen Alston at 758-3281.
* WANTED - patterns to buy or borrow for
hand puppets. Call Lois at 758-6308.
BUY & SELL
* For sale - Small square alfalfa bales, second
cut. Call Marilyn Eyre at 758-3723.
* For sale - Exercise Machine (never been used).
$65 obo. Call 758-6362.
* For sale - Ladies Irons with Bag. Excellent
condition. W/HC 3-p. $50 obo. Call
758-6362.
* Wanted - Garage/Workshop. Seeking secure,
insulated double garage/workshop for
storage & auto restoration at reasonable rent.
Contact David at 758-3568.
* For sale - Stroller. $40 obo. Phone
758-6224.
FIEDS
* For sale - Queen size mattress with 2 foams.
$10. Call 758-6224.
* Wanted - Good, used Metronome to buy.
Phone 758-3675.
* For sale - An exercise bike! Brand New
Expressions Giant Steppers st-1.
Phone 758-3545.
BUSINESS
* Wanted - Part-time Restaurant Helper to serve
in the front and help in the kitchen.
Daytime hours. $6.00/hr.
Apply in person with resume between
4:00 - 5:00 p.m. at
YING’S RESTAURANT 758-3198
* If your child needs help in Math or Reading
I the Kumon Method might help. For more
information call Rusty or Martine Rollingson
at 758-3648.
* DAHL’S HANDYMAN SERVICES - Yard
maintenance, hauling trash, rototilling and
lawn mowing. 758-6264.
* Will Tutor, grade 1-12 Math or Chemistry.
Call Martine Rollingson at 758-3648.
* Jeanie’s Hair Fashion
136 S. 1 St. W. - four doors south of the
Trading Company. 758-3379.
Open Tues. - Fri.
Professional Hair Care at pleasing prices.
* NEED YOUR FEED BARLEY MOVED?
Will also haul hay or straw. Also can haul
your quota for you. Sabey Trucking -
758-3119 or 308-1944.
CLASSIFIEDS '
*
CANADIAN SECURITY S7STEMS We sell, install & service alarns, safes, camera systems, deadbolts & key locks. Call Ross Moore at 758-39^5. Free estimate.
*
For all your cleaning needs torn hospital clean to a touch up, carpet to ceding & everything in between. No job too Hg or too small. Call Wayne s Carpet & Upholstery Cleaning - 758-6414.
USED VEHICLES
W
jTp-
*
84 Chev 2500 van. $800. 758-6569.
*
84 Chrysler New Yorker. $5C0 firm. 758-3712.
*
88 Mercury Topaz. 4 dr, auto. $1700. 758-6527.
*
1988 GMC Sierra Vi ton truck for sale. 5.7 litre. $3000. Phone 758-3409,
*
1985 Olds Cudass for sale. 2 Door. Runs very well. $1700. Ph.758-3409.
REAL ESTATE
FOR RENT
*
Prime Office Space for rent. This space is now occupied by Marion Mostoway, Hairstyles By Marion. It is located next to the Hons Hall and is now available. Ideal for a beauty salon. Rent $400/month which includes utilities. Contact Dorothv at the Magrath Trading Company - 758-3033.
2 bedroom apartment, available Oct. 1st. Phone 758-3781.
*
2 bedroom, furnished, no pets. Abstainers. Call Ty Alston - 758-3322
*
Small 2 bedroom home. Private. Treed lot under the hill. Inexpensive. 758-3700. Available now!!
*
Large 3 bedroom basement suite. Fridge & stove. Laundry hook-ups. Newer carpet & lino. 758-6494
*
One bedroom upstairs suite for rent. Many recent renovations.
$275 per month plus utilities. Damage Deposit $275 required. Phone 758-3409.
FOR SALE
*
5 bedroom, 3000 sq. Ft. Home on 1 acre.
2
bath, 2 family rms (one with wood burning stove), living rm, dining rm, oak in kitchen. A steal at $132,000. Phone 758-6569.
*
House for sale E Magrath. 3 bdrms + loft. Currendy half renovated. Needs handyman. Rent to Own. No deposit for renovator. Immediate possession. $31,900.
(604) 824-6787.
*
New 1230 sq. ft. Home
3
bedroom, 2 bath, garage, fridge, stove. No GST. Phone 758-6835 or 758-3446.
The South Country Jamboree Society^ will be bolding their regular jam and Hallowe en Dance on Sunday, Oct. 29 at the 702 Wing.
Meeting at 12:30 p.m. Jam at 2:00 p.m. Pot Luck supper at 6:00 p.m.
Costume judging and dance to follow. Phone 327-8477
J2ome and enjoy the music and dancing^/8
MAGRATH WS
la Pur Comcnmîty... Oc tober ¿000
Monday
Tuesday
Wednesday
Thursday <
Friday
Satunlaj?
8
9
THANKSGIVING
10
MID stopping water delivery
VOLLEYBALL Sr. Girts host St. Mary's
11
Elementary
School Pictures
Js&iors Bingo 7:00 p.m.
i
Jr/Sr High School Pictures
Elementary Parent/Teacher
IB
Town Talent Night
14
Beth & George Foggin Celebration
Casey Christensen
Open House
15
Casey Christensen Farewell
16
13
VOLLEYBALL Sr. Giris host RHS
18
VOLLEYBALL Sr. Boys host LCI
Curling Club Annual Mtg.
20
21
LIBRARY NEWS
During October, tbe library will be displaying OUR FLORA AND FAUNA, a multimedia art exhibit, which explores the plant and animal life that inhabits Northern Alberta. This exhibition presents a selection of work produced by members of the Alberta Society of Artists.
If you are interested in learning about the internet, email or other computer skills, please contact the library to schedule a session. There is no charge and you do not have to be a library member.
For the months of October and November, the library needs a pre-school story hour leader. Please contact Cyd Cunningham for further information.
have January 2001 calendars for sale, which contain 8 new original sketches of Magrath's original homes and buildings by Gary Heaton. They can be purchased at the Library for $12-00.
If you require a recital, concert, or meeting room, the Merkley Complex, in the downstairs portion of the library, is available for rent.
For further information about any of the above items, please call the library at 758-6498.
Leia Dickson, wife of Lee Dickson passed away at Cranbrook, BC
A memorial will be held Saturday, Oct. 14, 2000 from 2:00 - 4:00 Ramada Hotel
Suite 2375
Lee Dickson will be residing in Diamond Willow Terrace
If you have an advertisement or announcement to submit to the paper, please note that the deadline for submissions is TUESDAY at 12:00 NOON. Entries submitted after the deadline will be published the following week. Phone 758-6377, Fax 758-6888 or drop off your submissions at the Magrath Trading Company Office.9
Jtagrath trading Company
GROttUR SP£CIA£S
“3nm OurSumily io yours...”
Kelloggs Special K or Rice Krispies
375-700 g
$3.98
POSt Cereals - select varieties
400-620 g
2 for $5.00
Western Family Coffee - select varieties
300 g
2 for $4.48
Del Monte Fruit - select varieties
398 mL
3 for $4.98
Pacific or Alpha Evaporated Milk - select varieties
385 mL
$1.18
Aunt Jemima Pancake Mix or Syrup
1 kg, 750 mL
2 for $4.98
Robin Hood Quick Oats
3kg
$3.98
Dole Pineapple Blends - select varieties
1 litre
$1.58
Dempster Delisioso Bread - select varieties
600 g
$1.48
E.D. Smith TTipie Fruit Jam - select varieties
500 mL
$2.98
Betty Crocker Helpers - hamburger, tuna, & chicken
180-240 g
2 for $4.00
Gold Seal Chunk Light or Flaked Light Tuna
170 g
.88
Kraft Dinner
225 g
2 for $ 1.48
Kraft Pasta Dinner - select varieties
130-200 g
.98
Puritan Ham or Turkey
184 g
$1.78
Catelli Lasagne - select varieties
375-500 g
$1.98
Western Family Beef or Chicken Broth
284 mL
3 for $ 1.98
Dempster’s 7“ Tortilia Wraps - select varieties
10 pack
$1.58
Western Family Drinks - select varieties
3x250 mL
.78
General Mills Bugles - select varieties
140-150 g
3 for $4.98
Nestles or Rowntree Chocolate Bars - select vari
sties
3 for $ 1.89
Old Dutch Potato Chips - select varieties
430 g
$2.88
| Jell-O Instant Pudding Mix - select varieties
30-1 13 g
.8810
JIOR£ 6R0C£Ry SP££IA£S...
Christie Cookies - select varieties
350 g
$2.98
Christie Ritz Crackers - select varieties
250 g
$2.28
Robin Hood Flour - select varieties
10kg
$6.48
Western Classics Orange Juice, chilled
1.89 litre
$2.88
Becel Margarine - select varieties
907 g
$3.98
Black Diamond Cheddar Cheese - select varieties
600 g
$5.98
Dairyland Yogurt - select varieties
175 g
3 for $1.98
Dairyland Cottage Cheese - fat free
500 g
$2.18
Western Family Large Eggs - Canada Grade a
One Dozen
$1.78
Value Priced Hashbrowns - frozen
1 kg
.98
Western Family PerOgies frozen, select varieties
2kg
$4.48
McCain’S Superfries frozen, select varieties
1.5 kg
$2.98
Dairyland Chocolate Milk
1 litre
$1.38
International Coffee Whitener-select varieties
473 mL
$1.78
Household Items...
Ultra or Original ABC Detergent - select varieties
2.8-3.3 kg $5.98
Value Priced Jumbo Paper Towels - white
6 roll pack $5.88
Purex Bathroom Tissue
$8.48
Value Priced Complete Dog Food
8 kg $5.48
Value Priced Complete Cat Food
8 kg $7.98
Western'Family Liquid for Dishes
950 mL
$1.88
Western Family Cat Food - select varieties
156 g
2 for .88
Western Family Cat Litter
8kg
$3.88
Windex Glass Cleaner - select varieties
765-950 mL
$3.48
Javex Bleach - select varieties
3.6 litre
$2.4811
JM BK£i 3RESH PRODUCE
Large Delta Valencia Seedless Oranges
.58/lb
Red Grapefruit
2 for .98
Fresh White Mushrooms.
$ 1,98/lb
Fresh Express Caesar Salad Kit - regular or nte
10 oz pkg
$3.48 each
Roma Tomatoes
.98/lb
B1ttCH£R’S BUMS 07 IB£ W££K!
Pork Loin Chops Rib or Tenderloin End
$5.46/kg
$2.48/Ib
Olympic Sliced Bacon-select varieties
500 g
$3.98
Olympic Garlic Coil
750 g
$4.48
Oympic Mayfair Cooked Ham
375 g
$2.68/lb
Olympic Bologna - select varieties
500 g
$2.48
Pork Loin Center Cut Chops bone in
7.67/kg
$3.48/lb
Western Family Party Sticks - select varieties
500 g
$2.68
Olympic Sliced Meats - select varieties
175 g
4 for $5.00
THE MAGRATH CULTURAL ARTS SOCIETY NEEDS YOU!
Our annual general meeting will be
Wed., Nov. 8th, 8:30 at the United Church.
We will elect a 7 member administrative board and take volunteers sign-ups to help with our various projects throughout the year. We need your input at this meeting. Our projects for the 2000/2001 season are:
FALL VARIETY SHOW- Oct. 13th
TOWN CHRISTMAS CAROL FESTIVAL - Dec. VALENTINE DINNER THEATRE - Feb. COMMUNITY THEATRE PRODUCTION - Fall 2001
We would like to see CONTINUING EDUCATION CLASSES get going again in Magrath and any suggestions you may have at our meeting. Any Questions Please call:
Laurel Bennett 758-6222
Falene Wolsey 758-3992
Anne Bowen 758-6345
Michelle Alders-Moors 758-6353""Home of the Handyman""
We still have
Fall Bulbs on
Special!
Halloween Merchandise
Is out!
Trick or Treat Pail
Reg. $2.59
Special $1.57
Duracell 9 Volt Batteries
2 pack
Reg. $7.99
Special $6.33
Carbon Monoxide Alarm
First Response
Reg. $27.99
Special $21.99
Don't wait for winter!
Buy this 1500 watt Heater/Fan
Duracraft - Reg. $25.99
Sale Price $18.97
Protect Your Trees with the
Tree Protection Kit.
Burlap Tree-Pee protects from wind, freezing
rain, heavy snowfall & wildlife. :
3 sizes - Starting at $4.97
We have lots of
Garbage Bags
for Fall Clean-up, orange or clear
10 pack Home Garden Bags
Special $1.66/pack
Baker's Secret
12“ non-stick Pizza Pan f
Reg. $6.29 '
Sale Price $4.00
Quest Sterifoam Rose Huts
Reg. $3.79
Sale Price $2.77
Unival Anti-Freeze
Still on special!
Reg, Low Price $8.47
We have other
Anti-Freeze
-¿IgJ Magrath Lion’s Club («¿j
llll have recently put in place a
Home Recyclable Eye Glass Box UZZJ
h£xsartare in Magrath Trading Company. hoKtwcne
Hearing Aids can also be recycled.
They are sent to the underprivileged countries of the world","Magrath Store News (October 11, 2000)",,J. A. Ririe,,,core
4960343,1994-01-01T08:00:00,"Color classification is an important method in grading agricultural and biological materials. The objective of this thesis was to develop color classification methods for biological products, with application to real-time grading and seed corn husk deduction. A nomenclature of classification systems was developed to formalize a review of color classification methods. The description and functional classifier types were introduced. To overcome the limitations of available classifiers, when applied to real-time hardware, two original classifiers were developed using a binary representation of class assignments in the color space. The binary classifier of type one (BC1) used pairwise discriminant functions, whereas the binary classifier of type two (BC2) used a more complex logic. The binary representations can be implemented with look-up tables or template matching neural networks. Three software packages and a number of tools, implementing the color classifiers and error evaluation methods, were developed. SPR implemented statistical pattern recognition classifiers, nSPR implemented neural network based classifiers, and Purclass implemented binary classifiers. Four methods were developed to evaluate classifier accuracy: (1) the global error measurements with resubstitution error, leave-one-out error, and hold-out error, (2) the confusion matrix analysis for individual classes, (3) the dimensionality analysis computing the resubstitution errors for all possible combinations of color bands and classifiers, and (4) a set of graphical representations of color classification problems. The software allowed further analysis of neural network classifier\u27 behavior. It was found, for the problem studied, that the learning coefficient of the binary linear classifier of type one (BLC1) did not influence the convergence of the linear algorithms. The number of iterations necessary to reach the best resubstitution error was random. The BC1 and BC2 algorithms were successfully implemented on real-time image processing hardware. Classification rate was 6 images per second with the BLC2 classifier, for a three class problem, and 512 by 220 pixel color images. The developed real-time color classification system can accurately classify seed corn images and the color vision system improves the method of deduction, over the current manual method",Advanced classification system for biological products,,'Purdue University (bepress)',,,core
27102624,2000-10-01T00:00:00Z,"The term 'temporal' in spatial analysis has a number of potential meanings, each of which requires an alternative approach for the provision of analytic support. Much present work in spatio-temporal information is concerned with transaction versioning. Object based representations often demand a high level of initial understanding of object relationships. Many GIS users are seeking to understand the object relationships over time, past, present and future ; their research focus is how real-world features interact in time and space. Despite this requirement, little present work will support this requirement to understand the drivers of change, rather than simply to report what changed. A number of workers have / are attempting to formalise a theory of spatio-temporal reasoning (eg Hermosilla, 1994, Qian, et al, 1997, Claramunt et al, 1997), in the most part working from a theoretical abstraction. Worboys, 1998, uses a problem oriented approach, as does Halls and Miller (1995, 1996). Representation of change over time by means of spline curves offers possibilities for this type of work, Neural Networks are explored as an implementation solution. We show that the AURA neural network architecture offers particular hope and that the proposals of Yeh & de Cambray and Halls & Miller need to be recast in terms of the AURA architecture",A new approach to the spatial analysis of temporal change using todes and neural nets.,,Unité Mixte de Recherche  8504 Géographie-cités,10.4000/cybergeo.911,"[{'title': None, 'identifiers': ['issn:1278-3366', '1278-3366']}]",core
226251478,1999,"We present two different algorithms implemented through neural networks on a multiprocessor device. The parallel single-chip TI TMS32C80 Multimedia Video Processor (MVP). The goal of this experimentation is to test, on real problems, the performance of this powerful unit made up by one Master Risc Processor and by four Slave Digital Signal Processors (DSPs) and to evaluate its suitability to neural network applications. The first problem implemented is a typical classification algorithm in which the network recognises which points belong to different regions inside a 2D space. The second problem is more computationally heavy and consists of a network able to recognise `handwritten' digits. The parallel version of the first algorithm, was also tested on a commercially available supercomputer",A Parallel Processor for Neural Networks,,Computer Society Press,,,core
42772710,Mar. 1995,"Four important generic issues are identified and addressed in some depth in this thesis as part of the development of an adaptive neural network based control system for an experimental free flying space robot prototype. The first issue concerns the importance of true system level design of the control system. A new hybrid strategy is developed here, in depth, for the beneficial integration of neural networks into the total control system. A second important issue in neural network control concerns incorporating a priori knowledge into the neural network. In many applications, it is possible to get a reasonably accurate controller using conventional means. If this prior information is used purposefully to provide a starting point for the optimizing capabilities of the neural network, it can provide much faster initial learning. In a step towards addressing this issue, a new generic Fully Connected Architecture (FCA) is developed for use with backpropagation. A third issue is that neural networks are commonly trained using a gradient based optimization method such as backpropagation; but many real world systems have Discrete Valued Functions (DVFs) that do not permit gradient based optimization. One example is the on-off thrusters that are common on spacecraft. A new technique is developed here that now extends backpropagation learning for use with DVFs. The fourth issue is that the speed of adaptation is often a limiting factor in the implementation of a neural network control system. This issue has been strongly resolved in the research by drawing on the above new contributions",Experiments in Neural-Network Control of a Free-Flying Space Robot,https://core.ac.uk/download/pdf/42772710.pdf,,,,core
230461017,2002-12-01T08:00:00,"The contribution of this research effort was to show that a reliable RPV could be built, tested, and successfully used for flight testing and parameter estimation purposes, in an academic setting. This was a fundamental step towards the creation of an automated Unmanned Aerial Vehicle (UAV). This research project was divided into four phases. Phase one involved the construction, development, and initial flight of a Remotely Piloted Vehicle (RPV), the West Virginia University (WVU) Boeing 777 (B777) aircraft. This phase included the creation of an onboard instrumentation system to provide aircraft flight data. The objective of the second phase was to estimate the longitudinal and lateral-directional stability and control derivatives from actual flight data for the B777 model. This involved performing and recording flight test maneuvers used for analysis of the longitudinal and lateral-directional estimates. Flight maneuvers included control surface doublets produced by the elevator, aileron, and rudder controls. A parameter estimation program known as pEst, developed at NASA Dryden Flight Research Center (DFRC), was used to compute the off-line estimates of parameters from collected flight data. This estimation software uses the Maximum Likelihood (ML) method with a Newton-Raphson (NR) minimization algorithm. The mathematical model used a traditional static and dynamic derivative buildup. Phase three focused on comparing a linear model obtained from the phase two ML estimates, with linear models obtained from a (i) Batch Least Squares Technique (BLS) and (ii) a technique from the Matlab system identification toolbox. Historically, aircraft parameter estimation has been performed off-line using recorded flight data from specifically designed maneuvers. In recent years, several on-line parameter identification techniques have been evaluated for real-time on-line applications. Along this research line, a novel contribution of this work was to compare the off-line estimation results with results obtained using a recently introduced frequency based on-line estimation method. Specifically, phase four focused on comparing the ML results with a frequency domain based on-line estimation technique. The RPV vehicle and payload was designed and constructed with the combined efforts of WVU researchers, graduate and undergraduate students of the Mechanical and Aerospace Engineering Department, and a private sub-contractor, Craig Aviation",Flight testing of a remotely piloted vehicle for aircraft parameter estimation purposes,https://core.ac.uk/download/230461017.pdf,The Research Repository @ WVU,,,core
24607558,1998,"The recent explosion of on-line information in Digital Libraries and on the World Wide Web has given rise to a number of query-based search engines and manually constructed topical hierarchies. However, these tools are quickly becoming inadequate as query results grow incomprehensibly large and manual classification in topic hierarchies creates an immense bottleneck. We address these problems with a system for topical information space navigation that combines the query-based and taxonomic systems. We employ machine learning techniques to create dynamic document categorizations based on the full-text of articles that are retrieved in response to users ’ queries. Our system, named SONIA (Service for Organizing Networked Information Autonomously), has been implemented as part of the Stanford Digital Libraries Testbed. It employs a combination of technologies that takes the results of queries to networked information sources and, in real-time, automatically retrieve, parse and organize these documents into coherent categories for presentation to the user. Moreover, the system can then save such document organizations in user profiles which can then be used to help classify future query results by the same user. SONIA uses a multi-tier approach to extracting relevant terms from documents as well as statistical clustering methods to determine potential topics within a document collection. It also makes use of Bayesian classification techniques to classify new documents within an existing categorization scheme. In this way, it allows users to navigate the results of a query at a more topical level rather than having to examine each document text separately",Sonia: A service for organizing networked information autonomously,,ACM Press,,,core
24541625,1998,This paper describes the New Millennium Remote Agent �NMRA � architecture for autonomous spacecraft control systems. This architecture integrates tradi� tional real�time monitoring and control with constraint� based planning and scheduling � robust multi�threaded execution � and model�based diagnosis and recon�g� uration. We implemented a prototype autonomous spacecraft agent within the architecture and demon� strated the prototype in the context of a challenging autonomous mission scenario on a simulated spacecraft. As a result of this success � the integrated architecture has been selected to control Deep Space One �DS�1�� the �rst �ight of NASA�s New Millennium Program �NMP� � which will launch in 1998. It will be the �rst AI system to autonomously control an actual spacecraft. ,An autonomous spacecraft agent prototype,,ACM Press,10.1007/978-1-4615-5735-7_4,,core
24370637,1997,"A new reinforcement learning architecture for nonlinear control is proposed. A direct feedback controller, or the actor, is trained by a value-gradient based controller, or the tutor. This architecture enables both efficient use of the value function and simple computation for real-time implementation. Good performance was verified in multi-dimensional nonlinear control tasks using Gaussian softmax networks. 1 INTRODUCTION  In the study of temporal difference (TD) learning in continuous time and space (Doya, 1996b), an optimal nonlinear feedback control law was derived using the gradient of the value function and the local linear model of the system dynamics. It was demonstrated in the simulation of a pendulum swing-up task that the value-gradient based control scheme requires much less learning trials than the conventional &quot;actor-critic&quot; control scheme (Barto et al., 1983). In the actor-critic scheme, the actor, a direct feedback controller, improves its control policy stochastically ..",Efficient Nonlinear Control with Actor-Tutor Architecture,,MIT Press,,,core
15355954,2000,"Virtual Reality (VR), and Artificial Intelligence (AI) technology have become increasingly more common in all disciplines of modern life. These new technologies range from simple software assistants to sophisticated modelling of human behaviour. In this research project, we are creating an AI agent environment that helps architects to identify user preferences through a Virtual Reality Interface. At the current stage of development, the research project has resulted in a VR application - MuseV2 that allows users to instantly modify an architectural design. The distinctive feature of this application is that a space is considered as a base for all user modifications and as a connection between all design elements. In this paper we provide some technical information about MuseV2. Presentation of a design through VR allows AI agents to observe user-induced modifications and to gather preference information. In addition to allowing for an individualized design, this information generalized across a sample of users should provide the basis for developing basic designs for particular market segments and predict the market potential of those designs. The system that we envision should not become an automated design tool, but an adviser and viewer for users, who have limited knowledge or no knowledge at all about CAD systems, and architectural design. This tool should help investors to assess preferences for new community housing in order to meet the needs of future inhabitants",Measuring user satisfaction for design variations through virtual reality,,,,,core
23919526,1991,"We provide a conceptual framework for understanding  similarities and differences among various  schemes of compositional representation, emphasizing  problems that arise in modelling aspects  of human language. We propose six abstract dimensions  that suggest a space of possible compositional  schemes. Temporality turns out to play  a key role in defining several of these dimensions.  From studying how schemes fall into this space,  it is apparent that there is no single crucial difference  between AI and connectionist approaches  to representation. Large regions of the space of  compositional schemes remain unexplored, such as  the entire class of active, dynamic models that do  composition in time. These models offer the possibility  of parsing real-time input into useful segments,  and thus potentially into linguistic units  like words and phrases.  Introduction  What is the relationship between the kinds of symbolic representations deployed in &quot;classical&quot; cognitive models and repr..",Representing Aspects of Language,,Cognitive Science Society,,,core
10494802,[2002],"Information Technologies being developed by NASA to assist astronaut-physician in responding to medical emergencies during long space flights are being employed for the improvement of women's health in the form of ""smart surgical probe"". This technology, initially developed for neurosurgery applications, not only has enormous potential for the diagnosis and treatment of breast cancer, but broad applicability to a wide range of medical challenges. For the breast cancer application, the smart surgical probe is being designed to ""see"" a suspicious lump, determine by its features if it is cancerous, and ultimately predict how the disease may progress. A revolutionary early breast cancer detection tool based on this technology has been developed by a commercial company and is being tested in human clinical trials at the University of California at Davis, School of Medicine. The smart surgical probe technology makes use of adaptive intelligent software (hybrid neural networks/fuzzy logic algorithms) with the most advanced physiologic sensors to provide real-time in vivo tissue characterization for the detection, diagnosis and treatment of tumors, including determination of tumor microenvironment and evaluation of tumor margins. The software solutions and tools from these medical applications will lead to the development of better real-time minimally-invasive smart surgical probes for emergency medical care and treatment of astronauts on long space flights",NASA Smart Surgical Probe Project,,,,,core
42808056,"JAN 1, 1991","Significant advances have occurred during the last decade in intelligent systems technologies (a.k.a. knowledge-based systems, KBS) including research, feasibility demonstrations, and technology implementations in operational environments. Evaluation and simulation data obtained to date in real-time operational environments suggest that cost-effective utilization of intelligent systems technologies can be realized for Automated Rendezvous and Capture applications. The successful implementation of these technologies involve a complex system infrastructure integrating the requirements of transportation, vehicle checkout and health management, and communication systems without compromise to systems reliability and performance. The resources that must be invoked to accomplish these tasks include remote ground operations and control, built-in system fault management and control, and intelligent robotics. To ensure long-term evolution and integration of new validated technologies over the lifetime of the vehicle, system interfaces must also be addressed and integrated into the overall system interface requirements. An approach for defining and evaluating the system infrastructures including the testbed currently being used to support the on-going evaluations for the evolutionary Space Station Freedom Data Management System is presented and discussed. Intelligent system technologies discussed include artificial intelligence (real-time replanning and scheduling), high performance computational elements (parallel processors, photonic processors, and neural networks), real-time fault management and control, and system software development tools for rapid prototyping capabilities",Intelligent systems technology infrastructure for integrated systems,https://core.ac.uk/download/pdf/42808056.pdf,,,,core
235571705,1995-03-01T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
30 cents
WEDNESDAY, March 1,1995
SCHOOL NEWS: This last Wednesday in
basketball, the Eagles & Zeniths were at
Winston Churchill, the Eagles lost 74-86. Eric
Nordquist was the top scorer with 23 points &
Janies Smith came second with 14 points. The
Zeniths also lost by a very close score of 87-91.
Cassio Maffia led in points with 36. On
Thursday they hosted Raymond. The Eagles lost
58-87. Wes Gurney was the top scorer with 15
points and Chad Harris scored 10. The Zeniths
also lost 74-109. Scott Clifton led in points with
16 and Cassio Maffia followed with 15. Then on
Saturday they hosted LCI. The Eagles won 84­70
top scorers were Joseph Johnson 21, Eric
Nordquist 19 & Chad Harris 15. The Zeniths
were not as lucky though and lost 86-91. Cassio
Maffia led in points with 44.
Also in basketball the Pandas were at the Red
Deer tournament. The first game their opponent
were the Camille Cougars. The Pandas won 98­63.
top scorers were Kris Anderson 24, Patti
Balderson 24, Krysten Tanner 15 and Karli Dahl
11. Their second game was against Ardrossan,
which the Pandas won 69-42. Top scorers were
Patti Balderson 20, Kris Anderson 15 & Karli
Dahl 12. In their final game trough they lost to
WR Meyers by 15 points.
This week come out and support the Eagles and
Zeniths. On Friday they are at Catholic Central
and on Saturday they host Kainai. Games are at
6:00 & 7:30pm.
The Pandas & Cubs play on Wednesday at
home against Cardston. On Friday at home
against Winston Churchill. Game times are 6:00
& 7:30pm. Then on Saturday they host LCI.
Game times are 1:00 & 2:30pm.
This week the teacher voted with the best legs
was Mr. Greg Hansen.
Biology 30 student of the week was Mike
Mehew, Chemistry 30 student of the week was
Tiffany Reigling.
******
Can you teach a craft; play the piano; know
words to old time songs; can you read a story or
poetry; can you play games or are you just
willing to help? If so, Magrath General Hospital
needs you!! Please volunteer to help us with
activities to enrich the lives of those who are
permanent residents of the hospital. As little as 1
hour per month can be immensely rewarding.
Please call Karen Fillmore 758-3397.
****
ATTENTION: All Kindergarten Parents, School
fees for Kindergarten not paid by Feb 14/95 will
now be $75.00 for the rest of the year. They may
be paid at the school office.
Pres. Noreen Papp
. *******
If someone offers you the world on a silver
platter... take the platter.
*********
UCW Meeting
March 6th, 7:30pm.
Tray favors, all ladies welcome.
*******
MAGRATHT.O.P.S, (Take off pounds
sensiblvlCLUB
The Magrath Tops club meets every Tuesday at
6:30pm: weigh-in 6:30 - 6:45 and the meeting is
at 6:45
For further information, please call Joann
Takacs at 758-3796.*A*A ** **
BINGO - BINGO - BINGO - BINGO - BINGO
All members of the community are welcome to
play Bingo at the Senior's Centre every second
Thursday. Next Bingo March 2nd /95. Come on
out one & all, win cash! Doors open 6:30pm
. start at 7:00pm. *******
Magrath Curling Club Ladies Bonspiel
February 28 - March 5,1995
$100 per rink entry with banquet
Call for registration: Cathy Hohm 758-6228
Lindy Oliver 758-6521
Curling Club 758-3455
Please join us for lots of fun and prizes! We'll be
having an auction sale to raise money for the
club on Saturday, March 4/95 at 9:00 at the club
as well. ********
Deanne & Sharee want to thank all those who
have come and supported us and joined us at
Aerobics for the past year. For those who are
unaware of our NO COST Aerobic classes, they
run.....
Mon. - 9:00 - 10:00 p.m. @ School Gym
Wed. - 10:00 -11:00 a.m. @LDS Stake Bldg.
Thurs. - 9:00 - 10:00 p.m. @ School Gym
Thanks Again .... See You There!!!! , ******
Jennifer Atwood has been called to serve in the
Tempe, Arizona Mission. A farewell service will
be held on March 12/95 at the Magrath Stake
Centre at 1:00pm. ********
Elder Robin West has been called to serve in the
France Paris Mission. His farewell will be held
on 12 March at 9 a.m. in Magrath third/fourth
Ward Building. An Open House will be held
that evening from 6-8 p.m. at Robin's home.
******
MAGRATH CURLING CLUB ANNUAL
MIXED BONSPIEL
March 14 - 19,1995
$120.00 per rink entry with banquet and dance
( $15.00/person/$25.00/couple for banquet &
dance only)
Banquet & Dance held at Raymond Agri-Plex
Call to register Cathy Holm 758-6228, Lindy
Oliver 758-6521, Lynne Schultz 63-4114 or 65>pTRST
2819, Al Jasperson 653-4114 or 653-4831 MONOX
All league presentations will be presented at the
banquet & dance on Mar. 18/95
Everyone is welcome!
*******
*WOULI
RENTED
PLEASE
THAT H
>of I
f REMEMBER TO-..7......CALL T?»|ax
ip FOR: All your Home and .Appliance Repairs, and FIXTURI
j Maintenance Servicing. $14.97
’¡¡FOR: Service and Repairs to all makes and models of
¡iapplicances. Including: Maytag, Whirlpool, G.
jiKitchenaide and Sears Kenmore, Etc.
■IFOR: Service and Repairs also to Furnaces, H
¡¡Tanks, Water Softeners, Air Conditioners, Hum
insecurity Lighting & Alarm Systems, Etc.
iijREMEMBER: Call Tai Hancock
PHONE: 752-3866, Raymond
FULLY LICENSED TECHNICIAN
SYLVANIA
-v
J
ft
Box 1942 I1J4K5
Lethbridge, Alberta
Res/Fax: (403) 758-6339
TRl-LIGH
BULK PAI
Soft white 3 way I
50-TOO-150 watt.
3655^03
►a-
DALE GREAVES
24 HO(!R SERVICE (329-0965) *« GUN,
CABINE
CHINOOK HEALTH UNIT NEWS:
The Chinook Health Unit will be offering post­natal
classes to provide information on child
care and parenting. Topics discussed will
include physical and emotional development,
infant nutrition, childhood illness and safety as
well as others.
The classes are designed to meet the needs of
the first year parenting.
Parents with children 0-1 year of age are
invited to register. Your babies are welcome to
attend with you. The classes will be held at the
Chinook Heath Unit, Magrath office from 10-12
on Thursdays, March 23 and March 30/95. TO
register please contact the Chinook Heath Unit
at 758-3331 any Monday, Tuesday or
Wednesday.
A*** *** *
MEETING- Spring Coulee - General Special
Agenda - Hall Board Meeting, thurs. Mar. 9th
7 p.m. at the hall. This meeting will be to
finalize the future of the Hall. In order for the
hall to remain open it is IMPERATIVE at this
meeting that someone come forth with a
proposal on how to make the hall viable. IF no
proposal is brought forth & accepted by the
majority present. Closure of the hall will
proceed.
* * * * * *
POSITION AVAILABLE
Diamond Willow Terrace Lodge requires a part
time cook ( 8 shifts/month, more during holiday
time) Applicant should have experience in food
preparation, enjoy work with seniors. Send
resume to Charlotte Anderson, Diamond
Willow Terrace Lodge, Magrath, Alta, by
March 15,1995.
******
LATIONS!! Brenna Di Harris
did the town proud at the recent
Canada Winter Games, taking
the gold medal in the -52kg
Brenna works out of the
Club. Great job Brenna!!
DRY GOODS
SALE
■ ■-
 v •
■
xx
ALL REMAINING COTTON &
PRINT FABRICS
Y* . ' • •
S • . *
• • • .. ' "" v : ■
We have Mountain Mist Quilt
Bats in stock!!
A very disappointing crowd attended the
’’Bingo” at the school small gym cafeteria on
Monday night.
The Westwind School Division has subscribed
to the Lions Quest Program to be implemented
in all our schools at a cost of $80,000 for 3
years for materials and training of the teachers.
Our small Lions Club will be donating $1000
to this project and money raised from 3 bingos
will go to this Quest Program.
THIS IS NOT A GAMBLING PROJECT BUT
A FUN NIGHT FOR THE WHOLE FAMILY
Please support this worthy cause
The Quest Program is world wide and is
already in most of the schools in Alberta with
very positive results -- ""Skills for Growing"";
""Skills for Adolescence"" — programs that
should be taught in the Home, Churches and
Schools.
""IT TAKES A COMMUNITY TO RAISE A
CHILD""
If unable to attend, donations can be made to
the School or to any Lions Member.
TO MAGRATH & DISTRICT RESIDENTS:
The Heart Fund
Canvas: To the
residents that have
already given, our
grateful thanks.
There are 7
canvassers still out,
so please give when called on - Every $Dollar$
helps. Jean Butlin, Town Captain. Magrath
Hospital Auxilliary.
For your carpet & upholstery steam
cleaning, wall & window washing needs
call Wayne Bourelle 758-6414
GARDEN CITY
REALTY
SERVING THE AREA FOR
ALL YOUR REAL ESTATE
★New listing
Beautiful, newly renovated,small bungalow on
large lot. Attached garage,many extras.
April possession available. Priced to sell
$34,000.00
Call John Latham or Joanne Walker
★LAND
Lots, Acreages, Farm Land Call John Latham
★TWO YEARS YOUNG
Beautiful Bi-level Fully Developed With
Attached Double Garage On Large Lot
Priced Well Under Construction Cost. Call
John Latham or Joanne Walker
★NEW LISTING
Older 2 bedroom home in excellent location
close to shopping, post office & banks.
Wheelchair accessible, garage, workshop,
rootcellar and greenhouse on large lot. Lower
priced. For more information call John
Latham or Joanne Walker.
CALL 758-6060
;<■
<■
' »
CARRIAGE HOUSE
THEATRE
-Presents-
Feb. 24th - Mar. 3rd
""LION KING""
showing at 7 p.m.
■ ■ ■ •- '• i ■ .■ ■ ’ . . • ■ .. ■
""DUMB & DUMBER""
■ <
showing at 9pm
Mar. 3rd - Mar. 10th
""MAN OF THE HOUSE""
showing at 7 p.m. & 9 p.m.
Erica Van der Linden was recently honored at
a surprise shower at the home of Sue Bly.
Neighbors enjoyed a pleasant evening hosted by
Tammy Thomson, Donna Thompson and
Stephanie Goodsell. Getting to know Erica was
a very enjoyable experience, and we wish her
happiness as she becomes a part of our
community. Erica became the bride of Monte
Thomson on Friday, February 24.
ft * ft * * *
Magrath School- February Elementary
Assembly - The February Assembly was held in
the auditorium at 8:45 a.m. on February 16,
1995. Thank you to parents who were able to
attend this assembly. We celebrated our
Students-of-the-Month Awards. These awards
are earned by students for academic successes
as well as peacemaker/citizenship successes.
Each student was given a certificate and a
Student-of-the Month Pencil which was paid for
by our Parent Committee.. It is our school goal
to honer every elementary student for some
achievement throughout the year. Also
awarded were Highest % of Attendance for
grades 1 to 3 and grades 4 to 6. Winners this
month were Mrs. Toly’s grade 1 class and Mr.
Harker's grade 4 class. They get to keep the
Attendance Q-Bears in their room until next
month. (Continued on page 8 )
GOALS FOR POSITIVE GROWTH
To foster in responsibility, decision making,
communication, self-confidence and goal
setting. Skills For Growing is designed to
meet these key program goals:
‘Involving parents, school staff and the
community in supporting the healthy
development and success of all children.
‘Providing opportunities for children to
practice good citizenship through service
to others.
‘Encouraging a healthy, drug-free approach
to life.
‘Providing support for teachers and
administrators through effective materials
high-quality training and follow-up services.
‘Celebrating diversity and encouraging
respect for others.
magrath
LIONS
""Having Been involved with Skills for
Aaoiescencs our G:ub was very anxious
to sponsor the pilot program for Skills
for Growing, it was great participating
on the School Climate Committee,
seeing the school and community
working together
REACHING ALL AREAS OF i
CHILD’S WORLD
Skills for Growing' is made up of five
components, each playing a vital role In
creating a healthy environment for child
and teaching them essential life skills. T
components are:
•Classroom Curriculum
•Positive School Climate
• Parents as Partners
• Community Involvement
•Staff Training and Follow-up Support
SCHOOL
FAMILY
COMMUNITY...
... WORKING TOGETHER Lion Harry Anderson
Edmonton Alberts Canada
Donations are accepted at the school
or given to any LION member.
at 7:30pm
Monday, March 6, 1995
Monday, March 13, 1995
Three (3) BINGO’S will be held at:
the Magrath School Small Gym
(Cafeteria)
¿il unMFV RâKrnwüi nn mu/Aon Tur ntircT pnnrcRAM
CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758-6377
Less than 30 words—$1.07
Small ad (2.5""X3.5"")-$5.35
1/4 page-------------!-------$7.49
1/3 page---------------'■-----$8.56
1/2 page-------------------$10.70
Full Page—Copy Ready—$25.00
Full Page-—We do---------$37.45
Flyer insertion (your paper)
Speech Lessons!! Now starting for spring. Limited space. To register or ask questions call Melissa Gibb 758-6879. M 1-2
*******
If you need a good hard worker and a great babysitter, call Colleen 758-6834 Ml-1
* ah A A * * ft
KIDS ONLY CLOTHES OPEN HOUSE-MAR.4 9 am-2pm @ Lions Hall
*A * * * **
TO GIVE AWAY: 7 foot rubber plant call 758-3409 *******
FOUND: Ladies bike, white, call Burns Alston 758-6894
******
FOUND: Green men's bike; call Jim Webster & identify 758-3424
******
FOUND: Men's 5 speed ""Nashiki"" bike. Owner please claim at Town Office.
******
WANTED: Volume #1- Irrigation Builders - Magrath History Book. Need 2 copies. Call 758-3284 or 328-6347 and leave a message. M-8
******
WANTED: Pre-1960 tatting, crochet or knitted lace patterns. Iwant to preserve them and start a library for them Ellen Zobell 752-4050
*******
WANTED: Men’s skates size 12 or 13 ph 752-4133.
* * * * * * *
MERCHANDISE :
FOR SALE: 1. CCM Skitrack 200 with monitor asking $85
2.
270 rifle with scope in very good condition asking $400 Call 758-6527
*******
FOR SALE:A gold trimmed wood burning & coal fireplace insert, approx 23 l/3x 25, used very little. Also a grate, screen & glass doors for a fireplace call 758-3545
*******
FOR SALE: Women's Mt.
Bike with fenders $65 also 1979 Scorpion 440 Snowmobile $300 758-3752
*******
FQR SALE : Moffat electric stove, gold, very good condition. $150 obo call 752­4075
*******
FOR SALE: 8x12 garden shed, $500 call Boyd Woodruff at the school
******
FOR SALE: chesterfield, chair & love seat $150.00 call 758-3726 F22-1
*******
FOR SALE: used carpet rolls $50 phone 758-3072
*******
stove,good condition-$300' Kenmore electric dryer, excellent condition $150.
Call 758-3362
******
FOR SALE: AST 486 DX-2
66 systems, 4 megs ram, 420 H.D., SVGA monitor, Tower or desk top, $1725 Grizz's Den computer systems. Call Brad 380-2362. F22-9
******
FOR SALE: like new, rebuilt, rawhide ropping sadlle - $750 758-3802 F22-2
*******
FOR SALE: Hoyt Raider Bow, like new, fully loaded, 30 in draw, set for 50-70 lbs. $180 Call 758-3802 F22-2
*******
FOR SALE: One Pro-shot NBA basketball backboard complete with metal hoop. New still in the original box $65 obo call 758-3733
*******
FOR SALE: One steel Neufledt rear bumper off a 1982 chev 1/2 ton truck, no rust, step-up type bumper $55 obo Phone 758-3733
*******
FOR SALE: Woman's winter boots, black, size 8, ankle height. Worn 3 times. Will sell for $20 phone 758­3409 F22-2
******FOR SALE: Beautiful
Bamboo Bedroom Furniture,
headboard (single), matching
mirror and night table.
Excellent condition, must see,
$150obo call 758-3072 F15-2
SERVICES:
Will laminate paper goods in
my home. Reasonable rates.
27” wide machine. Posters,
charts, games, decorations,
learning resources, visual
aides, recipes, art, any paper
you want preserved, ph
Sharon 758-6423
ftftftftftftft
THE HANDY MAN
Call Robert 758-6212
1. All Building Maintenance
2. Repair Work
3. Windows and Doors
4. Lawns - large & small
5. Snow Removal
You Want It? Ask About It!
758-6212 24hours
ft ft ft ft ft ft ft ft
Custom Tree Service - tree
removal, trimming, removal
of dead branches or storm,
damaged trees, hauling of
junk piles and debris etc,
controlled burns. CDB
Enterprises. Call Cam
Bruce 758-3729
*******
JEANIE’S HAIR FASHION
Professional Haircare
at pleasing prices
Open hours 9am - 5pm
Mon thru Sat
For appointment call
758-3379
ftftftftftftft
JUST FOR YOU
HAIRSTYLES
Will be open Monday -
Friday 8:00 - 5:00 and
Saturday 8:00 - 12:00.
Call 758-6350.
ftftftftftft • .
FOR ALL YOUR
LASER PRINTER NEEDS/
/ type setting for books I
labels / flyers / resumes /
Give me a call - Bonny 758­6309
ft ft ft ft ft ft ft
VEHICLES:
AGRICULTURE
fOR SALE: 3 registered Red
Angus bull. (Thomson)
contacct Dan Atwood 758­6528
ft ft ft ft ft ft ft
REAL ESTATE
YOU WANTED THEM
CHEAPER...SO HERE
THEY ARE!
*3+1 bedrooms, 2 baths, 1195
sq ft, garage, only $77,000
*2+2 bedrooms, 2 baths, 1120
sq ft, almost new only
$89,900
Call Jack Elliott - 752-3449,
758-3551
Coldwell Banker Hancock
Land
ftftftftftftft
UNDER CONSTRUCTION
310W 4th Ave S Magrath
Beautiful custom built 1450
sq ft home, main floor
laundry, huge master
bedroom with ensuite &
jacuzzi, attached double
garage, lot 160x165. Truly a
gorgeous home. Call today
Exclusive Chinook 21 Realty
Merrill Larsen 758-3300 or
380-2100
«ftftftftftftft
28 Harker Ave, Magrath
Lovely 1 1/2 storey,
maintenance free home, 1469
sq ft 3 bed, 2 baths. Close tft;
school and church. Asking
$75,000. Merrill Larsen
Century 21 Chinook Realty
758-3300 or 380-2100
ftftftftftftft
HOUSE FOR RENT: phone
758-3767 after 6pm or 758­3395
days F15
ftftftftftftft
FOR SALE; Partially
serviced lot, 2 blocks west of
the Trading Co. asking
$5500.00 758-6826
ftftftftftftft
FOR RENT: Older 4
schools - Call 758-3700 F22
bedroom home close to
OPEN HOUSE
Saturday, March 4
10am - 4pm
141 - South - 2A St. West
Magrath
Everyone Welcome
Jack Elliott
758-3551
Coldwell Banker Hancock
Land
FOR SALE: Clean & open 2
yr old, 2 story, 1430 sq
ft/floor, 5 bedrooms, 3 full
baths, crawl space under
house for dry storage. All on
a large 110x150 ft lot.
$119,500 653-2696 F22-4
ftftftftftftft«
FOR SALE: Over 1300sq ft,
3 bedroom bungalow, with
detached garage off alley. ■
Many upgrades include
wiring; water & sewer line to
street, forced air furnace,
ceramic tile & lino. MLS.
Call Sharon Bacon 382-7692
Royal Lepage 327-2111 Ml-4
MORE GROCERY SPECIALS
Feb 27 - Mar 4
Hunts Thick & Rich Pasta Sauce
725 ml
$1.78 each
Pedigree Dog Food
380g
.88 each
Whiskas Cat Food
170g
2 for .98
Kleenex Ultra Soft Bathroom Tissue
8pk
$2.78 each
Sunlight Laundry Detergent
12 litre
S7.98 each
Kraft Cheese Whiz
500g
$3.98 each
Kraft Cheese Slices
500g
$3.98 each
McCain Pizza Pockets
400g
$2.98 each
Old South Orange Juice
355ml
$1.18 each
continued from page 4
Clean Room Awards were presented to Mrs. Andrew’s grade 3 class & Mr. Harker's grade 4 class. Kelly Hatch gave the students in those classes a certificate and some delicious cookies. Carol Dahl told a Valentine story to the students. Grade 2 students, led by music teacher Elizabeth Olsen, treated us to 2 songs.
Thank you to those parents who supported the Elementary Family Dance. Students had a chance to feature one of the dances that they have learned in their Phys. Ed. classes. Parents and students had a lot of fun participating in the very well attended dance. Thanks to the parents of students from grades4-6 who donated goodies for sale. Also a big thanks to Jack & Joanne Fox for getting the pop & manning the concession booth. We raised over a $100. The proceeds from this will help us bring Tink & Judy to perform for the students. We are in the process of booking them for some time in March. The assembly which began with O Canada ended with our Magrath School Song. The next scheduled Elementary Assembly is to be on March 15 at 8:45 a.m. Everyone is welcome to attend. STUDENTS OF THE MONTH Grade 1: (Mrs. Toly) Robert Alston, Cole Barnett, Jenny Jensen, Michael Nordquist,
Christine Spencer; (Mrs. Hatch) Mitch Blackmer, Katelyn Harker, Mathias Prince, Kendra Tidmarsh.
Grade 2: (Mrs. Court) Brady Bogdan, Layne Gill, John Leishman, Ann Gibbons, Stephen Taylor; (Mrs. Davies) Brittney Baines, Erin Wilde, Tanner Hinman, Jacie Chipman, Katie Hatch, Riley Moore.
Grade 3: (Mrs. Andrews) Ben Koskewich, Brady Mehew, Krystal Woodruff, Joel Wolsey, Jenna Ripley; (Mrs Dahl/Coppieters) Leanne Godionton, Annalee Low, Brenton Taylor, Joshua Hill, T.J. Quinton.
Grade 4: (Mr. Harker) Russel Johnson, Bonnie Wilde, Valena-Rose Wyght, James Perry, Viki Chatwin; (Miss Nielson) Beth Folkes, Haley Cook, Ivy Godsalve, Natasha Williams, Jill Mistaken Chief.
Grade 5: (Mrs. Karren) Marcie Johnson, Jenna Still, Tyler Knowlton, Brandon Davidsen; (Mrs. Dorner) Connor Low, Kelsey Helgeson, Chelsea Anderson, Amy Bevers; (Mrs. Coleman/Beck) Michelle Smith, Cody Ririe, Danielle McHugh, Kezia Alston, Curtis Litchfield, Kathy Bourne. Grade 6: (Mr. Alston) Joan Leishman, Joshua Bourelle, Kyle Schneyder, Brittanee Leavitt; (Mr. Baker) Jani Meldrum, Kyle Fillmore, Chris Chatwin, Brooklyn Gurney.MEAT SPECIALS Feb 27 - Mar 4
Standing Rib Roast $3.98/lb $8.77/kg
Sirloin Tip Steak $2.98/lb $6.57/kg
Pork Loin Chops(rib & tenderloin) $1.98/lb $4.37/kg
Chicken Legs(back attached) .78/lb $1.72/kg
Chicken Breast(backattached) $2.28/lb $5.03/kg
Burns Bacon what $2.28 each
Burns Beef Wieners what $2.19 each
EXTRA!!! EXTRA!!!
READ ALL ABOUT IT U
IT’S WHAT YOU’VE BEEN WAITING FOR !!
A SPECIAL OPEN HOUSE - and YOU’RE INVITED!!!
SAT.- MARCH 4TH
FROM 10.00 A.M. - 5 P.M. (mark it down)
I said MARK IT DOWN!! - Because you won’t want to miss a Golden Opportunity to
see this beautiful home at a SUPER LOW PRICE ! It has a large living room, large
kitchen, 3 bedrooms and 2 baths. With thermopane windows and the soft elegance of
wall to wall carpets, your comfort is assured in any season. But wait, THERE'S
MORE - a family room, storage room, cold room and laundry room. WOW!! Not to
mention complete vinyl siding, bsmt entrance, double car garage, double side driveway
and a host of other EXTRAS too numerous to list... so join me Sat. and I'll personally
show you all the goodies this exceptional home has to offer.
Mark it down -
Don't forget -
and don't miss it!!
SAT. - MAR 4, 10-5PM
4 Harker Ave West
Meet your host -
Keith (Happy) Hunter
of Sutton Group, Leth.
or call him at","Magrath Store News (March 1, 1995)",,J. A. Ririe,,,core
10483845,February 1997,"NASA is planning to send numerous unmanned planetary missions to explore the space. This requires autonomous robotic vehicles which can navigate in an unstructured, unknown, and uncertain environment. Landmark based navigation is a new area of research which differs from the traditional goal-oriented navigation, where a mobile robot starts from an initial point and reaches a destination in accordance with a pre-planned path. The landmark based navigation has the advantage of allowing the robot to find its way without communication with the mission control station and without exact knowledge of its coordinates. Current algorithms based on landmark navigation however pose several constraints. First, they require large memories to store the images. Second, the task of comparing the images using traditional methods is computationally intensive and consequently real-time implementation is difficult. The method proposed here consists of three stages, First stage utilizes a heuristic-based algorithm to identify significant objects. The second stage utilizes a neural network (NN) to efficiently classify images of the identified objects. The third stage combines distance information with the classification results of neural networks for efficient and intelligent navigation",Neural Network Based Sensory Fusion for Landmark Detection,,,,,core
22945452,1994,"Abstract. This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal. We discuss several issues in applying the reinforcement learning method to a real robot with vision sensor by which the robot can obtain information about the changes in an environment. First, we construct a state space in terms of size, position, and orientation of a ball and a goal in an image, and an action space is designed in terms of the action commands to be sent to the left and right motors of a mobile robot. This causes a “state-action deviation ” problem in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To deal with this issue, an action set is constructed in a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of Learning from Easy Missions (or LEM) is implemented. LEM reduces the learning time from exponential to almost linear order in the size of the state space. The results of computer simulations and real robot experiments are given",purposive behavior acquisition on a real robot by vision-based reinforcement learning,,,,,core
20849386,2002,"We have taught several distributed software engineering project courses with students and real clients [4]. During these projects, students in Pittsburgh and Munich, Germany collaborated on the development of a single system. Our experiences showed that software development is communication intensive and requires the collaboration of many stakeholders. Communication is challenging in distributed contexts: participants do not all know each other and work at different times and locations; the number of participants and their organization change during the project; participants belong to different communities. Hence, to deal with the global market place, it is critical to provide students with distributed collaboration skills. To improve the teaching of collaboration in software engineering, we propose iBistro [2], an augmented, distributed, and ubiquitous communication space. iBistro aims to overcome problems resulting from miscommunications and information loss in informal or casual meetings. iBistro enables distributed groups to collaborate and cooperate in software projects and therefore provides an environment for learning in such diverse aspects as project management, programming skills, and social skills. With the addition of techniques from artificial intelligence, such as student modeling, and intelligent support mechanisms, such as computer supported group formation, distributed tutoring becomes feasible. 1. Teaching Informal Communication  The development of engineering products and software is becoming increasingly distributed. Participants located at different geographical sites have to collaborate to specify, design, realize, and test products, usually across several time zones and often without meeting each other in person. There are many reasons for the dis..",iBistro: A Learning Environment for Knowledge Construction in Distributed Software Engineering Courses,,,,,core
100267664,2002,"AbstractÐIn k-means clustering, we are given a set of n data points in d-dimensional space Rd and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd&apos;s algorithm. In this paper, we present a simple and efficient implementation of Lloyd&apos;s k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm&apos;s running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation. Index TermsÐPattern recognition, machine learning, data mining, k-means clustering, nearest-neighbor searching, k-d tree, computational geometry, knowledge discovery. æ ",An efficient k-means clustering algorithm: Analysis and implementation,,,10.1109/tpami.2002.1017616,,core
224593067,1996-01-01T00:00:00,"A universal freeway incident detection framework is a task that remains unfulfilled despite the promising approaches that have been recently explored. The need for an operationally successful incident detection and management system as a vital component of any advanced traffic management system, is well established and recognized. Only recently however, researchers and practitioners have begun to increasingly realize that for an incident detection framework to be universally operational and successful, it needs to fulfill all components of a set of recognized needs. It is the objective of this research to define those universality requirements and produce an incident detection framework that possesses the potential to fulfill them.A new potentially universal freeway incident detection framework has been proposed, developed and evaluated. The research effort was started by defining a comprehensive set of requirements that any universal incident detection algorithm or framework should fulfill. Among these requirements, an incident detection algorithm needs to be operationally accurate, automatically transferable, and capable of automatically adapting to changes in the freeway environment. This set of universality requirements was used as a template against which all algorithms within the scope of this study have been evaluated. Three major incident and loop detector databases were heavily utilized, two of which are unprecedented real databases collected from two major freeway sites in California and Minnesota, namely the Alameda County's I-880 freeway database and the Minneapolis' I-35W database. The universality of the most well known existing incident detection algorithms was tested using the above databases. Serious lack of the universality, particularly transferability, was detected in all existing algorithms. Prior to the development of the new universal framework, limits on acceptable performance were elicited from TMC surveys conducted as part of this effort. Preliminary investigation of two promising advanced neural networks, namely the LOGICON and the PNN, was conducted. The PNN was more appealing due to its universality potential. The PNN was modified using a principal components transformation layer that resulted in performance enhancements. This together with its potential universality, led to the choice of the modified PNN for in-depth development. The in-depth development stage was divided into three phases. The first was the extraction of a new and improved input feature set that produced more distinct classes in the input feature space. The new features enhanced the transferability of the PNN and made the framework more compliant with the universality requirements. The second phase was the on-site real time retraining of the PNN after transferability, a phase that produced near optimal classification results and detection performance. The third phase was the development of a post processor output interpreter that linked the isolated 30 second outputs of the PNN and produced a sequentially updated probabilistic measure of existence of an incident in the field. The overall PNN-based framework was found to be fully compliant with the entire set of universality requirements. Finally, a new approach for training a multi-smoothing-parameter version of the PNN was investigated. The approach utilized genetic algorithms for optimizing the selection of the smoothing parameters. Obtained results indicated an improvement in performance over the single smoothing parameter PNN but at the expense of longer training time.The superiority and universality of a particular advanced neural network model, namely the PNN, was concluded in this research, as compared to the Logicon and the MLF neural networks, as well as existing conventional freeway incident detection algorithms. Adding the principal components transformation layer to the PNN was found to enhance its performance. Although the genetically optimized version of the PNN showed better transferability, both versions showed equally good performance after retraining. The PNN was concluded to be more practical for TMC implementation due to its instantaneous training capabilities",A Neuro-Genetic-Based Universally Transferable Freeway Incident Detection Framework,,"eScholarship, University of California",,,core
481035022,2001-06-01T00:00:00,"International audienceFor the last thirty years, progress in the field of physics, known as ""Chaos theory""--or more precisely: non-linear dynamical systems theory--has increased our understanding of complex systems dynamics. This framework's formalism is general enough to be applied in other domains, such as biology or psychology, where complex systems are the rule rather than the exception. Our goal is to show here that this framework can become a valuable tool in scientific fields such as neuroscience and psychiatry where objects possess natural time dependency (i.e. dynamical properties) and non-linear characteristics. The application of non-linear dynamics concepts on these topics is more precise than a loose metaphor and can throw a new light on mental functioning and dysfunctioning. A class of neural networks (recurrent neural networks) constitutes an example of the implementation of the dynamical system concept and provides models of cognitive processes (15). The state of activity of the network is represented in its state space and the time evolution of this state is a trajectory in this space. After a period of time those networks settle on an equilibrium (a kind of attractor). The strength of connections between neurons define the number and relations between those attractors. The attractors of the network are usually interpreted as ""mental representations"". When an initial condition is imposed to the network, the evolution towards an attractor is considered as a model of information processing (27). This information processing is not defined in a symbolic manner but is a result of the interaction between distributed elements. Several properties of dynamical models can be used to define a way where the symbolic properties emerge from physical and dynamical properties (28) and thus they can be candidates for the definition of the emergence of mental properties on the basis of neuronal dynamics (42). Nevertheless, mental properties can also be considered as the result of an underlying dynamics without explicit mention of the neuronal one (47). In that case, dynamical tools can be used to elucidate the Freudian psychodynamics (34, 35). Recurrent neuronal networks have been used to propose interpretation of several mental dysfunctions (12). For example in the case of schizophrenia, it has been proposed that troubles in the cortical pruning during development (13) may cause a decrease in neural network storage ability and lead to the creation of spurious attractors. Those attractors do not correspond to stored memories and attract a large amount of initial conditions: they were thus associated to reality distorsion observed in schizophrenia (14). Nevertheless, the behavior of these models are too simple to be directly compared with real physiological data. In fact, equilibrium attractors are hardly met in biological dynamics. More complex behaviors (such as oscillations or chaos) should thus to be taken into account. The study of chaotic behavior have lead to the development of numerical methods devoted to the analysis of complex time series (17). These methods may be used to characterise the dynamical processes at the time-scales of both the cerebral dynamics and the clinical symptoms variations. The application of these methods to physiological signals have shown that complex behaviors are related to healthy states whereas simple dynamics are related to pathology (8). These studies have thus confirmed the notion of ""dynamical disease"" (20, 21) which denotes pathological conditions characterised by changes in physiological rhythms. Depression has been studied within this framework (25, 32) in order to define possible changes in brain electrical rhythms related to this trouble and its evolution. It has been shown that controls' brain dynamics is more complex than depressive one and that the recovery of a complex brain activity depends on the number of previous episodes. In the case of the symptoms time evolution, several studies have demonstrated that non-linear dynamical process may be involved in the recurrence of symptoms in troubles such as manic-depressive illness (9) or schizophrenia (51). These observations can contribute to more parcimonious interpretation of the time course of these illnesses than usual theories. In the search of a relationship between brain dynamics and mental troubles, it has been shown in three depressed patients an important correlation between the characteristics of brain dynamics and the intensity of depressive mood (49). This preliminary observation is in accordance with the emergence hypothesis according which changes in neuronal dynamics should be related to changes in mental processes. We reviewed here some theoretical and experimental results related to the use of ""physical"" dynamical theory in the field of psychopathology. It has been argued that these applications go beyond metaphor and that they are empirically founded. Nevertheless, these studies only constitute first steps on the way of a cautious development and definition of a ""dynamical paradigm"" in psychopathology. The introduction of concepts from dynamics such as complexity and dynamical changes (i.e. bifurcations) permits a new perspective on function and dysfunction of the mind/brain and the time evolution of symptoms. Moreover, it offers a ground for the hypothesis of the emergence of mental properties on the basis of neuronal dynamics (42). Since this theory can help to throw light on classical problems in psychopathology, we consider that a precise examination of both its theoretical and empirical consequences is requested to define its validity on this topic.Depuis une trentaine d’années, certains développements de la physique ont amélioré notre compréhension de la dynamique des systèmes complexes. Connues sous le nom de « Théorie du chaos » – ou plus sobrement : théorie des systèmes dynamiques non-linéaires –, ces avancées possèdent une généralité qui permet d’envisager leur utilisationdans des domaines autres que ceux dans lesquels elles ont vu le jour. Notre objectif est de montrer qu’elles peuvent devenir des outils importants dans des disciplines telles queles neurosciences et la psychiatrie où les phénomènes étudiés possèdent naturellement une inscription temporelle i.e. une dynamique. L’introduction des concepts de la dynamiquenon-linéaire dans ces disciplines dépasse la simple métaphore et peut fournir à terme un nouvel éclairage du fonctionnement mental et de ses dérèglements. À partir d’exemplestirés de la littérature, nous montrons comment ces nouveaux concepts permettent un renouvellement des façons d’envisager des problèmes classiques comme l’évolutiontemporelle des symptômes ou la relation entre le cérébral et le mental","Dynamical paradigm in psychopathology : « Chaos theory », from physics to psychiatry",,Elsevier Masson,,,core
24333695,1994,"The central idea is to represent the real-valued signals passing between neurons as delta encoded binary sequences. A real value v in the range [-1, 1] is presented with delta encoded sequence. Adding this sequence appropriately to itself or to a zero-sequence, the multiplication by any real value is obtained. The addition of sequences is realised using simple logic circuitry: a single one-bit full adder. In this paper an expandable digital architecture that provides an efficient implementation base for large neural networks, is presented. The architecture uses the circuit for arithmetic operations on delta encoded signals to carry out the large number of required parallel synaptic calculations. All real valued quantities are encoded on delta bit streams. The actual digital circuitry is simple and highly regular, thus allowing very efficient space usage of fine grained FPGAs.  RELATED WORK  The pulse-stream encoding scheme for representing values is used in an analog implementation by ..",Neural Networks Using Bit Stream Arithmetic: a Space Efficient Implementation,,,,,core
203672587,1997-01-31,"AbstractUncertainty, inherent in most real-world domains, can cause failure of apparently sound classical plans. On the other hand, reasoning with representations that explicitly reflect uncertainty can engender significant, even prohibitive, additional computational costs. This paper contributes a novel approach to planning in uncertain domains. The approach is an extension of classical planning. Machine learning is employed to adjust planner bias in response to execution failures. Thus, the classical planner is conditioned towards producing plans that tend to work when executed in the world.The planner's representations are simple and crisp; uncertainty is represented and reasoned about only during learning. The user-supplied domain theory is left intact. The operator definitions and the planner's projection ability remain as the domain expert intended them. Some structuring of the planner's bias space is required. But with suitable structuring the approach scales well. The learning converges using no more than a polynomial number of examples. The system then probabilistically guarantees that either the plans produced will achieve their goal when executed or that adequate planning is not possible with the domain theory provided. An implemented robotic system is described",Permissive planning: extending classical planning to uncertain task domains ,,Published by Elsevier B.V.,10.1016/S0004-3702(96)00031-8,,core
42781573,"May 11, 1994","The research mission is the development of computer assisted diagnostic (CAD) methods for improved diagnosis of medical images including digital x-ray sensors and tomographic imaging modalities. The CAD algorithms include advanced methods for adaptive nonlinear filters for image noise suppression, hybrid wavelet methods for feature segmentation and enhancement, and high convergence neural networks for feature detection and VLSI implementation of neural networks for real time analysis. Other missions include (1) implementation of CAD methods on hospital based picture archiving computer systems (PACS) and information networks for central and remote diagnosis and (2) collaboration with defense and medical industry, NASA, and federal laboratories in the area of dual use technology conversion from defense or aerospace to medicine",Neural networks: Application to medical imaging,https://core.ac.uk/download/pdf/42781573.pdf,,,,core
23469009,1999,"Abstract — The paper presents a formulation and an implementation of a system for voice output extraction (VOX) in real-time and near-real-time realistic real-world applications. A key component includes voice-signal separation and recovery from a mixture in practical environments. The signal separation and extraction component includes several algorithmic modules with a variety of sophistication levels, which include dynamic processing neural networks in tandem with (dynamic) adaptive methods. These adaptive methods make use of optimization theory subject to the dynamic network constraints to enable practical algorithms. The underlying technology platforms used in the compiled VOX software can significantly facilitate the embedding of speech recognition into many environments. Two demonstrations are described: one is PC-based and is near-realtime, the second is digital signal processing based and is real time. Sample results are described to quantify the performance of the overall systems. Index Terms — Adaptive networks, audio signal processing, DSP, gradient descent, independent component analysis, neural networks, nonlinear networks and systems, optimization, speech processing, state–space models, statistical independence criteria. I",Voice extraction by on-line signal separation and recovery,,,10.1109/82.775387,,core
34058045,1994,"In the context of pattern classification, the success of a classification scheme often depends on the geometrical properties of the pattern classes under consideration. As radial basis functions (RBF) neural networks have largely been applied in pattern classification problems, in this paper we present a brief overview of different trends in radial basis functions neural networks and their applications. The meanings of the weights and the processing units for a RBF network applied for pattern classification are given. A new learning algorithm for a RBF neural network is proposed in this paper. This algorithm gives a solution for classifying configurations of patterns in a feature space providing the minimum number of hidden units for the network implementation. The learning is based on the backpropagation algorithm. The performance of the proposed algorithm is assessed on different artificial and real applications. The algorithm is successfully applied for estimating a distribution, as well as for separating signals in a multiple access communication system and for recognizing static speech. © 1994 Academic Press. All rights reserved",Minimal topology for a radial basis functions neural network for pattern classification,,,10.1006/dspr.1994.1016,"[{'title': None, 'identifiers': ['issn:1051-2004', '1051-2004']}]",core
23689678,2000,"This research investigates the analysis and efficiency of neural networks, using a technique for network link pruning. The technique is tested with inefficient architectures for the XOR problem and then for a network from a real world, complex, image recognition task. By removing each link and examining effect upon error level, a fuzzy set is developed with membership indicating link saliency. As well as efficiency, the technique is useful to investigate solution architecture. It is hypothesised that similar insights may be gained for any problem solved by similar architecture This paper begins with the background, research and possible applications. Experimental design, implementation, methodology and results are given. The conclusion considers implications and suggests further research. Results indicate that this technique can significantly improve efficiency of a neural network for a real application. Both memory requirements and execution speeds improve by nearly 30 times. Further development is hoped to deliver improvements to efficiency and depth of investigation. KEYWORDS: Image processing; Neural networks; Pruning; Skeletonising; Face Recognition 1. BACKGROUND RESEARCH There are few known practical design steps for the architure of a neural net modeling a complex problem space. Huang and Huang, (1991) consider theoretical methods to assess bounds on the number of hidden neurons. However the solution is itself too theoretical for practical application. Others suggest the use of principal components to design the required number of hidden neurons but this is only a heuristic. Another approach is to use a fully connected net,with what is thought to be a sufficiently large size. If a net over generalises then it may be increased in size, otherwise, if it is not ge..",Investigating neural network efficiency and structure by weight investigation,,,,,core
23263190,1997,"In this paper, we consider the Multi-skill Crew Rostering Problem. We propose a constraint  programming approach based on constraint set variables. We prove that this approach  yields an exponential factor improvement in the worst-case search space over the conventional  constraint programming approach. To handle preferences, we follow a &quot;relax and enrich&quot;  scheme of Schmidt. Using our approach, we implemented a prototype using ILOG SOLVER.  This prototype is used to solve a real-world problem of rostering technical crews for TV  drama production.  1 Introduction  The Crew Rostering Problem (CRP) is a much-researched problem in Operations Research and Artificial Intelligence. This class of problem arises in the management of manpower resources for organizations which provide round-the-clock services, such as the scheduling of nurses, maintenance crew and so on. CRP is a computationally intractible problem, even when the problem is severely restricted in scope.  The OR approach is to fo..",Efficient Multi-Skill Crew Rostering via Constrained Sets,,Press,,,core
42807998,"Jan 1, 1993","Traditional control theory is well-developed mainly for linear control situations. In non-linear cases there is no general method of generating a good control, so we have to rely on the ability of the experts (operators) to control them. If we want to automate their control, we must acquire their knowledge and translate it into a precise control strategy. The experts' knowledge is usually represented in non-numeric terms, namely, in terms of uncertain statements of the type 'if the obstacle is straight ahead, the distance to it is small, and the velocity of the car is medium, press the brakes hard'. Fuzzy control is a methodology that translates such statements into precise formulas for control. The necessary first step of this strategy consists of assigning membership functions to all the terms that the expert uses in his rules (in our sample phrase these words are 'small', 'medium', and 'hard'). The appropriate choice of a membership function can drastically improve the quality of a fuzzy control. In the simplest cases, we can take the functions whose domains have equally spaced endpoints. Because of that, many software packages for fuzzy control are based on this choice of membership functions. This choice is not very efficient in more complicated cases. Therefore, methods have been developed that use neural networks or generic algorithms to 'tune' membership functions. But this tuning takes lots of time (for example, several thousands iterations are typical for neural networks). In some cases there are evident physical reasons why equally space domains do not work: e.g., if the control variable u is always positive (i.e., if we control temperature in a reactor), then negative values (that are generated by equal spacing) simply make no sense. In this case it sounds reasonable to choose another scale u' = f(u) to represent u, so that equal spacing will work fine for u'. In the present paper we formulate the problem of finding the best rescaling function, solve this problem, and show (on a real-life example) that after an optimal rescaling, the un-tuned fuzzy control can be as good as the best state-of-art traditional non-linear controls",Nonlinear rescaling of control values simplifies fuzzy control,https://core.ac.uk/download/pdf/42807998.pdf,,,,core
52032744,2001-12-20T00:00:00,"My research field is in the area of engineering for Human Computer Interaction. My contributions to the field therefore include ergonomic as well as software design (software architecture). My primary motivation is to develop ways for making novel interaction techniques more usable. I first developed a design space for multimodality. The considerations involved in this design space are not only based on technological capabilities but also on the psychological consequences of the design features on future users. With the results of this work in hand (modality definition and design space), I then examined two research avenues. In the first one, I focused on output multimodal user interfaces applied to visualization of large information spaces. In the second one, I concentrated on new interaction modalities based on the augmented reality paradigm. For each of these two avenues, new ergonomic properties, design rules and patterns, software architectural solutions and new interaction techniques such as the clickable reality and the augmented stroll have been developed.Mes travaux sont résolument ancrés dans l'ingénierie de l'Interaction Homme-Machine. Ce faisant, les deux facettes, conception ergonomique et conception logicielle, sont apparentes dans mes contributions. Mon étude s'articule autour des termes de modalité d'interaction et de multimodalité. Je me suis appuyée sur des résultats généraux incluant la définition d'une modalité et d'un espace de composition de modalités et je les ai infléchis à deux axes de recherche, l'exploration visuelle d'espaces informationnels et la réalité augmentée. Pour chacun de ces axes, des propriétés ergonomiques, des règles et patrons de conception, des solutions architecturales et logicielles, ainsi que des techniques d'interaction comme la réalité cliquable et la ballade augmentée ont été conçus",Modalité d'interaction et multimodalité,,HAL CCSD,,,core
22720997,1996,"The EM--ML algorithm belongs to a family of algorithms that compute PET  (positron emission tomography) reconstructions by iteratively solving a large linear  system of equations. We describe a preprocessing scheme for focusing the attention,  and thus the computational resources, on a subset of the equations and unknowns  in order to reduce both the time and space requirements of such algorithms. The  approach is completely data-driven and uses no prior anatomic knowledge. Experimental  results are given for a CM--5 parallel computer implementation of the EM-ML  algorithm using a simulated phantom as well as real data obtained from an ECAT 921  PET scanner.  Keywords: positron emission tomography, image reconstruction,  expectation-maximization, parallel computing.  1  2 I. Introduction  Positron emission tomography (PET) is a method for non-invasively studying the physiology of the human body. A patient is injected with a radio-isotope which collects in the part of the body to be stu..",A Focus-of-Attention EM-ML Algorithm for PET Reconstruction,,,,,core
24363905,1996,"We present a mathematical implementation of a quantum mechanical artificial neural network, in the quasi-continuum regime, using the nonlinearity inherent in the real-time propagation of a quantum system coupled to its environment. Our model is that of a quantum dot molecule coupled to the substrate lattice through optical phonons, and subject to a timevarying external field. Using discretized Feynman path integrals, we find that the real time evolution of the system can be put into a form which resembles the equations for the virtual neuron activation levels of an artificial neural network. The timeline discretization points serve as virtual neurons. We then train the network using a simple gradient descent algorithm, and find it is possible in some regions of the phase space to perform any desired classical logic gate. Because the network is quantum mechanical we can also train purely quantum gates such as a phase shift.  I. INTRODUCTION  Many artificial neural networks are simulatio..",A Quantum Dot Neural Network,,,,,core
57101619,1994,"Interaction in Virtual Reality environments is still a challenging task. Static hand posture recognition is currently the most common and widely used method for interaction using glove input devices. In order to improve the naturalness of interaction, and thereby decrease the user-interface learning time, there is a need to be able to recognize dynamic gestures. Dynamic Gesture Recognition (DGR) is difficult for various reasons. The large variations in the speed of execution of various phases of a gesture is one such reason. Another is the quality and position of the physical properties describing a gesture themselves. These problems are the exaggerated by the differences which arise when various people attempt the same gesture, as well as when the same person attempts repeated executions of the same gesture. Other factors effecting the difficulty of DRG are the emotional state of the person doing the gesture and the accuracy of the input device used. And finally, a large amount of dat a has to be processed in real time because of large variances in the length of time to execute a gesture. In this paper we describe our appoach to overcoming the difficulties of DGR using neural networks. Backpropagation neural networks have already proven themselves to be appropriate and efficient for posture recognition. However, the extensive amount of data involved in DGR requires a different approach. Because of features such as topology presentation and automatic-learning, Kohonen Feature Maps are particularly suitable for the reduction of the high dimension data space which is the result of a dynamic gesture, and are thus implemented for this task",Dynamic Gesture Recogition Using Neural Networks. A Fundament for Advanced Interaction Construction,,,,,core
416915018,2001-06-01T00:00:00,"International audienceFor the last thirty years, progress in the field of physics, known as ""Chaos theory""--or more precisely: non-linear dynamical systems theory--has increased our understanding of complex systems dynamics. This framework's formalism is general enough to be applied in other domains, such as biology or psychology, where complex systems are the rule rather than the exception. Our goal is to show here that this framework can become a valuable tool in scientific fields such as neuroscience and psychiatry where objects possess natural time dependency (i.e. dynamical properties) and non-linear characteristics. The application of non-linear dynamics concepts on these topics is more precise than a loose metaphor and can throw a new light on mental functioning and dysfunctioning. A class of neural networks (recurrent neural networks) constitutes an example of the implementation of the dynamical system concept and provides models of cognitive processes (15). The state of activity of the network is represented in its state space and the time evolution of this state is a trajectory in this space. After a period of time those networks settle on an equilibrium (a kind of attractor). The strength of connections between neurons define the number and relations between those attractors. The attractors of the network are usually interpreted as ""mental representations"". When an initial condition is imposed to the network, the evolution towards an attractor is considered as a model of information processing (27). This information processing is not defined in a symbolic manner but is a result of the interaction between distributed elements. Several properties of dynamical models can be used to define a way where the symbolic properties emerge from physical and dynamical properties (28) and thus they can be candidates for the definition of the emergence of mental properties on the basis of neuronal dynamics (42). Nevertheless, mental properties can also be considered as the result of an underlying dynamics without explicit mention of the neuronal one (47). In that case, dynamical tools can be used to elucidate the Freudian psychodynamics (34, 35). Recurrent neuronal networks have been used to propose interpretation of several mental dysfunctions (12). For example in the case of schizophrenia, it has been proposed that troubles in the cortical pruning during development (13) may cause a decrease in neural network storage ability and lead to the creation of spurious attractors. Those attractors do not correspond to stored memories and attract a large amount of initial conditions: they were thus associated to reality distorsion observed in schizophrenia (14). Nevertheless, the behavior of these models are too simple to be directly compared with real physiological data. In fact, equilibrium attractors are hardly met in biological dynamics. More complex behaviors (such as oscillations or chaos) should thus to be taken into account. The study of chaotic behavior have lead to the development of numerical methods devoted to the analysis of complex time series (17). These methods may be used to characterise the dynamical processes at the time-scales of both the cerebral dynamics and the clinical symptoms variations. The application of these methods to physiological signals have shown that complex behaviors are related to healthy states whereas simple dynamics are related to pathology (8). These studies have thus confirmed the notion of ""dynamical disease"" (20, 21) which denotes pathological conditions characterised by changes in physiological rhythms. Depression has been studied within this framework (25, 32) in order to define possible changes in brain electrical rhythms related to this trouble and its evolution. It has been shown that controls' brain dynamics is more complex than depressive one and that the recovery of a complex brain activity depends on the number of previous episodes. In the case of the symptoms time evolution, several studies have demonstrated that non-linear dynamical process may be involved in the recurrence of symptoms in troubles such as manic-depressive illness (9) or schizophrenia (51). These observations can contribute to more parcimonious interpretation of the time course of these illnesses than usual theories. In the search of a relationship between brain dynamics and mental troubles, it has been shown in three depressed patients an important correlation between the characteristics of brain dynamics and the intensity of depressive mood (49). This preliminary observation is in accordance with the emergence hypothesis according which changes in neuronal dynamics should be related to changes in mental processes. We reviewed here some theoretical and experimental results related to the use of ""physical"" dynamical theory in the field of psychopathology. It has been argued that these applications go beyond metaphor and that they are empirically founded. Nevertheless, these studies only constitute first steps on the way of a cautious development and definition of a ""dynamical paradigm"" in psychopathology. The introduction of concepts from dynamics such as complexity and dynamical changes (i.e. bifurcations) permits a new perspective on function and dysfunction of the mind/brain and the time evolution of symptoms. Moreover, it offers a ground for the hypothesis of the emergence of mental properties on the basis of neuronal dynamics (42). Since this theory can help to throw light on classical problems in psychopathology, we consider that a precise examination of both its theoretical and empirical consequences is requested to define its validity on this topic.Depuis une trentaine d’années, certains développements de la physique ont amélioré notre compréhension de la dynamique des systèmes complexes. Connues sous le nom de « Théorie du chaos » – ou plus sobrement : théorie des systèmes dynamiques non-linéaires –, ces avancées possèdent une généralité qui permet d’envisager leur utilisationdans des domaines autres que ceux dans lesquels elles ont vu le jour. Notre objectif est de montrer qu’elles peuvent devenir des outils importants dans des disciplines telles queles neurosciences et la psychiatrie où les phénomènes étudiés possèdent naturellement une inscription temporelle i.e. une dynamique. L’introduction des concepts de la dynamiquenon-linéaire dans ces disciplines dépasse la simple métaphore et peut fournir à terme un nouvel éclairage du fonctionnement mental et de ses dérèglements. À partir d’exemplestirés de la littérature, nous montrons comment ces nouveaux concepts permettent un renouvellement des façons d’envisager des problèmes classiques comme l’évolutiontemporelle des symptômes ou la relation entre le cérébral et le mental","Dynamical paradigm in psychopathology : « Chaos theory », from physics to psychiatry",,Elsevier Masson,,,core
76360605,1992-03-22T08:00:00,"We use findings in machine learning, developmental psychology, and neurophysiology to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually-driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a gripper, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up such important issues as observation sparsity in high-dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well-established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for projections of high-dimensional data sets that capture task invariants.
We also pursue the following problem: how can we use human expertise and insight into grasping to train a system to select both appropriate hand preshapes and approaches for a wide variety of objects, and then have it verify and refine its skills through trial and error. To accomplish this learning we propose a new class of Density Adaptive reinforcement learning algorithms. These algorithms use statistical tests to identify possibly  interesting  regions of the attribute space in which the dynamics of the task change. They automatically concentrate the building of high resolution descriptions of the reinforcement in those areas, and build low resolution representations in regions that are either not populated in the given task or are highly uniform in outcome.
Additionally, the use of any learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate mistakes during learning and not damage itself. We address this by the use of an instrumented, compliant robot wrist that controls impact forces",A Robotic System for Learning Visually-Driven Grasp Planning (Dissertation Proposal),https://core.ac.uk/download/76360605.pdf,ScholarlyCommons,,,core
22088867,1994,"We show how field-programable gate arrays can be used to efficiently implement neural nets. By implementing the training phase in software and the actual application in hardware, conflicting demands can be met: training benefits from a fast edit-debug cycle, and once the design has stabilized, a hardware implementation results in higher performance. While neural nets have been implemented in hardware in the past, larger digital nets have not been possible due to the real-estate requirements of single neurons. We present a bit-serial encoding scheme and computation model, which allows space-efficient computation of the sum of weighted inputs, thereby facilitating the implementation of complex neural networks. 1 Introduction Conventional computer hardware is not optimized for simulating neural networks. Therefore, several hardware implementations for neural nets have been suggested ([MS88], [Sal90], [CB92], [vDJST93]). While the functions of neural networks are comparatively s..",Space Efficient Neural Net Implementation,,CONCLUSION,,,core
42782006,"Oct 1, 1994","There are numerous definitions for real-time systems, the most stringent of which involve guaranteeing correct system response within a domain-dependent or situationally defined period of time. For applications such as diagnosis, in which the time required to produce a solution can be non-deterministic, this requirement poses a unique set of challenges in dynamic modification of solution strategy that conforms with maximum possible latencies. However, another definition of real time is relevant in the case of monitoring systems where failure to supply a response in the proper (and often infinitesimal) amount of time allowed does not make the solution less useful (or, in the extreme example of a monitoring system responsible for detecting and deflecting enemy missiles, completely irrelevant). This more casual definition involves responding to data at the same rate at which it is produced, and is more appropriate for monitoring applications with softer real-time constraints, such as interplanetary exploration, which results in massive quantities of data transmitted at the speed of light for a number of hours before it even reaches the monitoring system. The latter definition of real time has been applied to the MARVEL system for automated monitoring and diagnosis of spacecraft telemetry. An early version of this system has been in continuous operational use since it was first deployed in 1989 for the Voyager encounter with Neptune. This system remained under incremental development until 1991 and has been under routine maintenance in operations since then, while continuing to serve as an artificial intelligence (AI) testbed in the laboratory. The system architecture has been designed to facilitate concurrent and cooperative processing by multiple diagnostic expert systems in a hierarchical organization. The diagnostic modules adhere to concepts of data-driven reasoning, constrained but complete nonoverlapping domains, metaknowledge of global consequences of anomalous data, hierarchical reporting of problems that extend beyond a single domain, and shared responsibility for problems that overlap domains. The system enables efficient diagnosis of complex system failures in real-time environments with high data volumes and moderate failure rates, as indicated by extensive performance measurements",Performance results of cooperating expert systems in a distributed real-time monitoring system,https://core.ac.uk/download/pdf/42782006.pdf,,,,core
82536972,1997-01-31,"AbstractUncertainty, inherent in most real-world domains, can cause failure of apparently sound classical plans. On the other hand, reasoning with representations that explicitly reflect uncertainty can engender significant, even prohibitive, additional computational costs. This paper contributes a novel approach to planning in uncertain domains. The approach is an extension of classical planning. Machine learning is employed to adjust planner bias in response to execution failures. Thus, the classical planner is conditioned towards producing plans that tend to work when executed in the world.The planner's representations are simple and crisp; uncertainty is represented and reasoned about only during learning. The user-supplied domain theory is left intact. The operator definitions and the planner's projection ability remain as the domain expert intended them. Some structuring of the planner's bias space is required. But with suitable structuring the approach scales well. The learning converges using no more than a polynomial number of examples. The system then probabilistically guarantees that either the plans produced will achieve their goal when executed or that adequate planning is not possible with the domain theory provided. An implemented robotic system is described",Permissive planning: extending classical planning to uncertain task domains ,https://core.ac.uk/download/pdf/82536972.pdf,Published by Elsevier B.V.,10.1016/S0004-3702(96)00031-8,,core
101779918,1992,"Real-time AI is gaining increasing attention for applications in which conventional software methods are unable to meet technology needs. One such application area is the monitoring and analysis of complex systems. MARVEL (multimission automation for real-time verification of spacecraft engineering link), a distributed monitoring and analysis tool with multiple expert systems, was developed and successfully applied to the automation of interplanetary spacecraft operations at the Jet Propulsion Laboratory (JPL) of the National Aeronautics and Space Administration (NASA). In this chapter, we describe MARVEL implemen-tation and validation approaches, the MARVEL architecture, and the specific benefits that were realized by using MARVEL in operations. MARVEL is an automated system for telemetry monitoring and analy-sis. It has been actively used for mission operations since 1989. It was first deployed for the Voyager spacecraft’s encounter with Neptune and has remained under incremental development since this time, wit",MARVEL: A Distributed Real-time Monitoring and Analysis Application,,MIT Press,,,core
235574532,2000-01-12T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
35 cents
Wednesday January 12, 2000
A very significant event took place in
Geneva Switzerland this past November.
The World Congress of Families 11 was
held with representatives from around
the world, including 15 people from
Southern Alberta.
The purposes of this Congress were to:
1. rally organizations and leaders to
protect and fortify the natural family;
2. develop guidelines for the
formulation and implementation of
family-centered policies and laws;
3. raise worldwide awareness of the
objectives and results of the Congress;
4. create ongoing structures for mutual
cooperation and support
The Honorable Grant Hill, MP, was
one of the leaders who attended the
Congress. He has graciously accepted an
invitation to speak to our Magrath
Community about his experience in
Geneva.
This meeting will be held
Feb. 2 at 7:00 p.m.
in the Magrath School
(Band Room)
All are invited to attend.
BAND CALENDARS
""Spirit of Alberta Birthday Calendars""
There are a few of you
who haven't picked your's up yet!!
You've already paid for your calendar,
so why not pick it up!!
Doing your groceries!
Shopping at Home Hardware!!
Come to the Trading Company Office
to get yours now!!
Magrath Museum
Annual General Meeting
will be held
January 27, 2000
at 7 p.m.
in the lunch room
of the library museum building.
Anyone
who wishes to join
the Museum Association
is welcome!
No experience necessary!
We need You!
Call Donna Lybbert at 758-3896
or
Andrea Burrows at 758-3843.
&&A
New Books
Adult Non-Fiction
The Kid's Guide to Service Projects by
Barbara A. Lewis; Skin: An Owner's Manual
by Robert Buckman, M.D.;
Making Change Irresistible by Ken Hultman
Adult Fiction
The Last Dragon Lord by Joanne Berlin;
So Speaks the Heart by Johanna Lindsey;
Beyond Recall by Robert Goddard.
Juvenile
Creepy Riddles by Katy Hall, Peace Tales:
World Folktales to Talk About by Margaret
MacDonald, Now You See It - Now You Don't:
Optical Elusions by Seymour Simon; Karen's
Mistake by Ann Martin.
Children's
Cocoa Ice by Diana Appelbaum, Tanya and the
Magic Wardrobe by Patricia Lee Gauch; Here
In Space by David Milgrim.
******
The Magrath Museum wishes to thank the
community for their donations of artifacts,
photos and financial assistance during the
Centennial Yeai.
Keep your donations coming, our museum is
growing and improving each year.
Donors who wish to contribute S20.00 or more
can be issued a tax deductible receipt.
******
BINGO - BINGO -BINGO
Next Bingo is Thursday nite
January 20, 2000
at the Seniors' Centre.
Doors open 6:30 p.m.
Bingo @ 7 p.m.
Everyone Welcome.
******
General & Sub
Contracting
For all your Concrete
& Stucco needs
Garden City Gymnastic
classes and registration
begin January 18th
4:30 p.m.
in the small gym.
To begin with
age groups and hours
remain the same as last term.
******
MAGRATH HOSPITAL
The Magrath Hospital Committee held a
meeting December 15th to discuss the status of
the Magrath Hospital and what actions should
be taken in order to keep it functioning. At
this time the committee feels that the best
thing that we as residents of the hospital area
can do is to write letters stating the need for
medical and emergency care in our area. The
committee is scheduled to make a presentation
to the CHR Board on January 27, 2000.
Letters should be sent prior to that meeting to
the following:
The Honorable Halvar Jonson, MLA
Minister of Health
228, Legislature Bldg
10800 - 97 Avenue
Edmonton, AB T5K 2B6
Ron Hierath, MLA
503, Legislature Bldg
10800 - 97 Avenue
Edmonton, AB T5K 2B6
Mr. Frank Eden
Chairman of the Board
Chinook Health Region
960, 19th Street South
Lethbridge, AB T1J 1W5
Curt’s Construction
P.O. Box 535
Magrath, Alberta
TOK 1J0
CURTIS HATCH
Home: (403) 758-3759
Cell: (403) 308-4585
Before laundering, pretreat stains on clothing
with a mixture of three parts baking soda to
two parts white vinegar.
Mr. Garth Coleman passed away on Sunday,
January 2, 2000 at the age of 86 years.
Besides his wife Avilda, he is survived by his
children: Ross (Irene) of Preston, Idaho; Peggy
Ann (Blair) Meldrum of Christiansburg,
Virginia, Paul (Gwen) Coleman of Magrath
and step-children: Alan (Marie) Harker; Diena
(Randall) Meeks, Elaine (Grant) Fisher, Brent
(Sheila) Harker and Kevin Harker. He is also
survived by brother: Reed (Myrna) of
Magrath, sisters-in-law: Desmonia (Warren)
Harris of Magrath & Joanne Coleman of
Chilliwack, BC.
He was predeceased by his first wife, Adele;
daughter Brenda, sister Margaret Clifton and
brother Meade.
Funeral services were held on Thursday,
January 6, 2000 with interment in the Magrath
Cemetery.
Mrs. Catherine ""Katie"" Fisher passed away
Sunday January 2, 2000 at the age of 86 years.
Katie is survived by one daughter: Rose (Don)
Dunkel of Michigan; two sisters: Lorna Mosher
of Burnaby B. C., Elsa Kaytor, of Vancouver,
B.C. & one brother Steve Butte of Hixon, B.C.
Katie was predeceased by a brother Joe
Mirkovich and best friend and companion of
35 years Mike Parczen.
A Funeral Mass was held Friday, January 8,
2000, with interment in the Magrath Cemetery.
Mrs. Zelma Mary Strong, passed away on
Sunday, January 2, 2000 at the age of 81 years.
She is survived by two sons and one daughter­in-
law: Randall Strong of Lethbridge, Kendall
(Gail ""Perky"") of Calgary.
She was predeceased by her husband, her
parents: George & Leona Loxton and 2
brothers: George and Delby.
Funeral Services were held on Thursday,
January 6, 2000. Interment in the Magrath
Cemetery.
He who teaches his child to be thrifty &
economical has already bequeathed him a
fortune.
As the family of Garth Coleman we would
like to take this opportunity to thank all those
who have helped ease the sorrow at his passing.
We appreciate the many acts of kindness that
were performed on our behalf. A special thank
you to all the staff of the Diamond Willow
Terrace for taking such good care of our Dad
& helping make it his home. We would also
like to thank the medical staff of the Magrath
Hospital & ambulance for their response to his
medical needs.
Sincerely, The Garth Coleman Family.
Jessica (Reeder) and Jay Salmon
would like to invite you
to their wedding reception,
Saturday evening January 15th, 2000
at the Garden Place Chapel
(White Church)
All welcome
On behalf of myself and my family, I want to
thank the anonymous giver of their generous
gift. The timing was perfect.
With much appreciation. ""D""
Mrs. Fern Cook, long time resident of
Magrath, is now residing in Cardston at the
home of her daughter, Donna & Art Heninger.
Guests are welcome to drop in at 265 - 3rd
Avenue West. Telephone calls will be gladly
received at 653-1512.
Happy New Year to all of the wonderful people
in Magrath from Fern.
Thank you so much
Sincerely, Fern's Family.
ANGE-EMILE LABBE
•Drywall 'Boarding • Taping
• Texture Wall & Ceiling
• Small Renovations
• Commercial & Residential
BOX 803
MAGRATH, AB TOK 1 JO-PHONE
(403) 758-6876
Magrath Pee Wee Chiefs Hockey Club News Release Chiefs split weekend games
Friday night had the Pee Wee Chiefs home to the Lethbridge Warriors. A large Lethbridge fan contingent went away disappointed with a 7-2 win for the Chiefs. The win was helped by a three goal performance from L. Hofer. S. Johnson scored twice and added an assist, while S.Ragan and A. Wamboldt finished the scoring with singles. R. Yoshihara moved the puck around well, contributing 3 assists and J. Rudd managed 1 assist before leaving the game in the first period.
Saturday night the Chiefs met the Raymond Ice in Raymond. The wheels fell off early when the chiefs managed to give the Ice a 2 goal lead, putting the puck into our own net twice. The final score of Raymond 10 Magrath 6 leaves little to the imagination about how the rest of the game went. The only bright spot was a rowdy fan celebration for S. Nishikawa's hat trick. S. Johnson came up with two goals while L. Hofer dished up one. J. Rudd assisted 3 times while A. Wamboldt and S. Ragan also added assists.
The Chiefs play in Picture Butte Friday and are back home on Saturday night, Jan. 15 hosting Raymond at 7:45 p.m.
We would like to thank all the people in Magrath & area who supported the Magrath & District Minor Hockey Pasta Fundraiser. It was a great success! (We sold 374 boxes, a Lethbridge team recently sold 200.)
Special thank-you to Leonard Bourne for helping out and to Allen Clarke from ""Greg's Transport"" for giving Oodles of Noodles a reduction in the shipping costs. Magrath & District Minor Hockey Assoc. Sharon Ragan (Pasta Coordinator)
If You Ask Me.....
""It's hard to detect good luck - it looks so much like something you've earned."" -FredA. Clark
Friday in the Southern Alberta Girls Basketball Magrath 50 - Kainai 41.
Kelsey Helgeson 12 pts, Marcie Johnson 10 pts. J.V action Magrath 47 - Kainai 33.
Ginger Holladay 13 pts. Saturday in Boys Basketball Magrath 70 Medicine Hat 51. Jimmy Balderson 25 pts, Jared Kilkenny 14 pts. Darren Bevers 12 pts. Dylan Alston 11 pts.
TOWN OF MAGRATH PUBLIC NOTICE
1.
Business Licenses, as required by By-Law #897, are being issued at the Town Office for the 2000 business year.
2.
Dog Licenses are due to be renewed in January. Cost = $10 not neutered or spayed; $5 with proof of procedure being done.
3.
2000 Residential Water Charge = $27/mo. Water prepayment plan with the applicable one month discount is available to Town residential properties during the month of JANUARY ONLY. The 1999 water account must be paid in full before one can participate in the prepayment plan.
4.
Tax Prepayment Plan is available to property owners with taxes in a CURRENT STATUS. This plan is available only until January 31, 2000,
★“Young man,"" said the angry father from the top of the stairs, “didn't I hear the clock strike three when you brought my daughter home?” '
“Indeed you did, sir,” re­plied the boyfriend. “It was going to strike eleven, but I grabbed the pendulum and held it so it would not disturb you.”
As the father walked back to his bedroom, he was heard to mutter, “Now, why didn’t I think of that one in my courting days?”Magrath Rod & Gun
Measuring night will be held
January 21 from 6:00 p.m. - 9:00 p.m.
at the Club House.
All entries must be clean and tagged
with owners name.
New memberships now available from
John in Trading Company Hardware.
A A A A A A
How safe a driver are you? Try answering the
following questions .
1) What four things should you do if the brakes
in your car fail suddenly while you are driving?
2) What are the different techniques used when
parking a car uphill on a street with a curb or
on one without? .
3) How many drinks can a driver consume in
one hour before he starts taking risks?
Here are the answers:
1) Pump the brake pedal up and down; apply
parking brake while holding the release lever in
""off"" position. Shift to lowest gear. Steer car
off road and rub wheels along curb or bushes.
Turn off the ignition.
2) Front wheels should be turned left and
resting against the cur. Turn wheels right when
there is no cur.
3) The average driver will start to take risks
after just one or two drinks.
-How many make a dozen?
Twelve.
And how many make a million?
Very few!
PASSEY ELECTRIC
New Home Wiring
Renovations
Farm Wiring
Feedlois
7/ p
(4031752-4005
1999 Tree of Hope
Magrath Hospital Auxilians wish to
thank all the kind people, clubs,
businesses, and organizations of Magrath
and areas for their generous monetary
donations to our Tree of Hope Fund
Raising Campaign held in the months of
November and December 1999 and
finishing in January 2000. Because of
your great help, we have raised over
$5,000.00 to be able to purchase 2
electric beds for out residents at our
Extended Care Facility, for their
everyday comfort & enjoyment.
A very special Thank you goes out to the
48 volunteers, who so willingly donated
their time in this the very busy season,
who manned the table where the monies
were collected, without your assistance,
we would not have succeeded. Also,
many thanks to the Magrath Trading
Co. Hardware Dept, for letting us use
the space and many many thanks to
John Bourne for his great help as
always. And thank you, Sharon Owens,
for making the wonderful posters. Last
but not least, thank you Grace Toomer,
for your dedication to our Auxiliary, all
the hours you spent on telephoning,
making sure that everything went well,
and it did.
Again, A Very Big Thank You To All.
Magrath Hospital Auxiliary. 1-19.
AAA**A
-Wife: ""All men are fools!""
Husband: ""Of course we are, dear, we're made that way so
that women don't have to be old maids.""
Little Brad fell and cut his knee; When the
wound was cleaned & bandaged, his mother
gave him a pill to soothe him. After he had
swallowed it Brad asked, ""How will the pill
know which leg to go down?""
CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758^63.77
Less than 30 words------------$3.00
Small ad (2.5""X 3.5"")-------$8.00
Subscription------------------------$15.00/year
1/4 page---------------------------$10.00
1/3 page---------------------------$13.50
1/2 page---------------------------$20.00
Full Page—Copy Ready---------$40.00
Full Page—We do-------------------$55.00
Flyer insertion (your paper)....$40.00
To give away: german shepherd pups, 3 males, 1 female. Call 758-6567. 1-18
******
Found:trailer license plate, monday on 2 St. East. Call 758-6564 to claim. 1-12.
******
For sale: Piano, older upright $500.00. Call Regehr's 758-3614. 1-12.
******
For s«le: 6 mos old N64 with pair real N64 controllers(not no name brand) red, yellow, grey, blue $165, excellent cond. Call Susan Loose at 758-6328. 1-12
******
For sale: 1 piano, & 1 queen size waterbed. 758-6220. 1-18.
******
For sale: Travel trailer tandem 21', 1971, renovations, sleeps 4, $2,400.00 tele: Lowell Kendrick 758-3620 Magrath to see. 1-19.
******
HIGHLAND DANCE LESSONS Now available - boys & girls ages 5 & up. Beginner & advanced levels.
Phone Kandi Russell @ 758-3900. 1-18.
******
Mother of teenager:
“Her idea of a clean room is one where you can find the phone by the fifth ring. '’
For sale: Tractor, Mitsubishi Bull Diesel 23 hp, 4 wheel drive, 9 sp trans, diff lock/rear wheels, 3 pt. hitch, front end loader, 5' Bush Hog rotary tiller, 6' blade for leveling or snow plowing, 4'8'' Haban 3 blade finish mower, 21"" stroke log splitter, tractor has 2,685 hrs. Price $13,000.00—includes the above implements. Tele: Lowell Kendrick 758-3620. Magrath to see. 1-19
******
Need your feed barley moved?
Will also haul hay or straw. Also can haul your quota for you. Contact Sabey Trucking @ 758-3119. or 308-1944 ******
Jayco Contracting Do you need a handy man for all your building needs? I do everything from concrete slabs to circular stairs. Call Tim Sabey @758-3327 or 308-1209 .1-26.
******
♦Prevent rust by keeping your car clean - inside, outside & underneath. Set a lawn sprinkler under the car & turn the water on full blast. This washes off the salt & other chemicals that collect on the car's underside during the winter.
For all your cleaning needs, from hospital clean to a touch up, carpet to ceiling & everything in between. No job to big or to small. Call Wayne's Carpet & Upholstery Cleaning @ 758-6414. 12-22 ******
JEANNIE'S HAIR FASHION 136S - 1 St. West 4 doors South of Trading Co. 758-3379 Open Tues thru Friday Professional Haircare at pleasing prices ******
RICK'S PORTABLE WELDING Owner Rick Beres We weld everything from chairs to tractors Please call me for all your welding needs ""B"" pressure qualified. 758-6427
******
There’s one good thing about snow—it makes your lawn look as nice as your neighbor’s.
—Clyde MooreCANADIAN SECURITY SYSTEMS We sell, install, and service alarms, camera systems, safes.
*Also dead bolts and key locks Call Ross Moore 758-3945 for a free estimate.
******
Remember to Call Tai heating, air conditioning, refrigeration and appliance service.
Gas & Electrical Service and Trenching 752-3866. Tai Hancock mgr.
******
For rent: various spaces, in Magrath, ideal for small business. Call 758-3876.
******
For rent: main floor house, 3 bedrooms. Avail immediately 2000.
135 East-1 Ave. N. Magrath. $500 + 1/2 utilities, $500 d.d., abstainers, no pets.
Call Harold Murray 758-3325.
******
For rent: small 2 bedroom apart, avail, immediately. Call 758-3781.
******
For rent: one bedroom self­contained upstairs apart. Many renovations. $275/mo. plus utilities. Damage deposit of $275.00 required.
Phone 758-3409. 1-18.
******
Karen Filmore residence is for sale by owner 3 bedroom bungalow attached double garage, family room has wood floors & fireplace.
New lino, carpet
& paint upstairs.
Games room, 2 bedrms & large cold storage down.
$103,000.00 lowered to $98,500.00 contact: Kathy Dahl @ 758-3039 or Irene MacDonald @ 1(403) 257-4477. l.r.
******
House for sale 1550 sq. ft., 1/2 yr. old bung. Completely finished up & down, total of 6 bedrooms, 3 full baths, Irg f.r., main floor laundry, built in vacuum & appliances. Located on corner lot, 1 block East of school @ 180E Harkder Ave.
400 sq ft.shop is separate from home. Call Scott or Falene @ 758-3992.
Fast food for birds
Get some 100% cotton thread, a needles, popped corn & whole cranberries. Cut a 1 m(39 3/8"") length of thread. Tie a large know in one end; thread the other end through the needles. String the corn & cranberries onto thread & knot remaining end. Then hang the finished garland on a tree.
House For Sale by Owner 1456 sq. ft., open greatroom concept, sky lights, fireplace, main floor laundry, master bedroom has patio doors to deck & large ensuite with jacuzzi tub, walk out basement, appliances & draperies inc.
On 1 acre+ in Magrath, ideal for horse. 758-6805. 1-19.
******
*Fast food for the birds Pinecones, peanut butter, wild bird seed, waxed paper and some string or yarn are all you need. Lay two pieces of waxed paper on work surface. In the centre of one, spoon out a small amount of peanut butter; on the other, sprinkle bird seed. Roll each pinecone in peanut butter until it's coated, then in seeds. Spoon extra seeds into spaces. Tie one yarn end around tip of cone. Tie other yam end around a tree branch just outside your window.
Hot drinks can leave you cold. It's true they usually boost your body temperature a bit. but if you drink too many, hot beverages dilate blood vessels in the skin, which actually causes a slight heat loss.MEAT SPECIALS JANUARY 10-15
Fresh Whole Frying Chicken $3.38/kg $1.58/lb
Hot or Mild Italian Sausage $4.37/kg $1.98/lb
Marinated, Boneless Skinless Chicken Breast $8.77/kg $3.98/lb
Fletcher’s Sliced Bacon 500g $3.48
Fletcher's Wieners 375-450g $2.28
Fletcher's Bologna Chunks 750g $3.68
Fletcher's Smokies 450g $3.28
Western Family Thin Sliced Meats 70g .88
ATTENTION:
A meeting to hear about
ALBERTA FIRST
a new party to bring about
a political change
in the next provincial election.
If you are tired of the present
""give away""
come out and
hear about some alternatives.
Thursday,
January 13, at 7:00 p.m.
in the Trading Company Hall (Lions)
•J» *1* *1*
How come I can find two of everything
except socks?”
lîulilüj H
758-3992 ®
“Actually, I can take a peekaboo
Silence is a powerful weapon.
— Sir Wiltiasn Osler
MORE GROCERY SPECIALS JANUARY 10-15
ssdirect@telusplanet.net ww"".ssdirect.com
Kraft Cracker Barrel Cheddar Cheese 454g $5.48
McCain Deep Pan Pizza 500-530g $3.28
Tampico Punch (citrus or mango) 4 litre $2.48
Dairyland Sour Cream 250 ml .98
Dairyland Multipack Yogurt 8 x 125g $2.68
McCain’s Punch or Iced Tea 355 ml .88
Cadbury Chocolate Bars 3 for $1.78
Betty Crocker Fruit Snacks 110-175g $2.28
Magic Moment Pudding Snacks 4 pack $2.28
Kraft Handi Snacks 87g x 3 pk $1.48
Nature Valley Granola Bars 226-230g $2.48
McCormicks Wagon Wheels 350-400g $2.98
McCormicks Viva Puffs 300g $1.98
The Del Bonita 4-H Club held a meeting on
December 6th at the Del Bonita School, later
followed by a game of volleyball. On December
19th we carolled to all the seniors of the Del
Bonita Community, then we met at the school
and had a nice lunch, followed by our
Christmas party and a game of basketball.
The Club had each member bring $5.00 worth
of food for the food bank.
Our Annual Bottle and Battery Drive was
held on January 8th.
Our January 10th meeting was held at the Del
Bonita School at 7:00 p.m. It was decided that
our next meeting will be held on February 7th,
2000 at the Del Bonita School. We will hold a
Judging at Dave Newton's on February 18th,
right after school. We spent some time learning
the correct procedures on judging and then
adjourned the meeting.
This report was submitted by Kaycee Newton,
Del Bonita 4-H Club Reporter.
Computer Consulting, Sales, and Service
Box 389, 132 - Street SW, Magrath, AB TOK 1JO
Roger Davias voice 758-3577
General Manager fax 758-91*74
Magrath Golf Club Notice of Annual Meeting
- Meeting to be held Monday February 7, 2000
at 7:00 p.m. at the Magrath High School
- Nominations for executives to be forwarded to
Box 432, Magrath, AB. TOK IJO,
prior to February 1, 2000.
For more information contact: Ron Johnson @
(403) 758-3341.
PRODUCE SPECIALS JANUARY 10 -15
Texas Sweet Red Grapefruit
5 lbs.
$3.48
Fresh Limes or Lemons
2 for .88
Blood Oranges
$3.70/kg
$1.68/lb
Minneola Tangelos
$3.70/kg
$1.68/lb
Fresh Express Coleslaw or Garden Salad
lib
$1.78
SENIORS NEWS
Seniors Wednesday supper will be Wednesday January 12th, we will have Chinese food; January 19th, soup & sandwich or something else. These are at 5 p.m. at the Seniors Centre. Friday Pot Luck will be Friday January 28th at 5 p.m.
Following the Pot Luck supper the Seniors Annual Meeting will be held. Selection of officers will be made, please come and help the Seniors'
Bookings for the Senior's Centre, Call: Jack or Jean Butl","Magrath Store News (January 12, 2000)",,J. A. Ririe,,,core
24295028,1996,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learned behavior  \stereo sketch.&quot; First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with MDL principle in terms of image coordinates and disparity information obtained from the fast stereo matcher based on the coarse-to- ne control method. Then, in terms of the segmented regions including the target area and their occlusion status identied during the stereo and motion disparity estimation process, we construct a state space for a reinforcement learning method to obtain target reaching behavior. As a result, the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method. 1 Introduction Realization o# a#tono#o## a#ent# t#at o##anize t#ei# o#..",Stereo Sketch: Stereo Vision-Based Target Reaching Behavior Acquisition with Occlusion Detection and Avoidance,,IEEE,,,core
22616091,1994,"The implementation of larger digital neural networks has not  been possible due to the real-estate requirements of single neurons. We  present an expandable digital architecture which allows fast and spaceefficient  computation of the sum of weighted inputs, providing an efficient  implementation base for large neural networks. The actual digital circuitry  is simple and highly regular, thus allowing very efficient space usage  of fine grained FPGAs. We take advantage of the re-programmability  of the devices to automatically generate new custom hardware for each  topology of the neural network",A Fast FPGA Implementation of a General Purpose Neuron,,,,,core
22610706,2002,"In this paper we examine the underlying similarities and differences between two major computational formalisms in developing intelligent robots; namely, artificial potential fields, which are often implemented for real-time robot planning and control, and artificial neural networks, which are usually considered as one of the biologically-inspired powerful learning techniques. Such comparisons will offer us new insights into how the two can complement each other in learning and control during robot-environment interaction. (This paper is based on an article by Liu and Khatib for the forthcoming The Handbook of Brain Theory and Neural Networks, Michael Arbib, Editor, MIT Press, 2000.)  1 INTRODUCTION  The problem of robot motion planning was traditionally treated as an optimization problem, in which the configuration of a robot is represented in a parameter space, and a solution to this problem is computed by searching the parameter space in an attempt to satisfy a predefined cost funct..",Practical Connection between Potential Fields and Neural Networks,,The MIT Press,,,core
101221017,1994,"A pilot program, CME, is described for generating a physical genetic map from hybridization fingerprinting data. CME is implemented in the parallel constraint logic programming language ElipSys. The features of constraint logic programming are used to enable the integration of pre-existing mapping information (partial probe orders from cytogenetic maps and local physical maps) into the global map generation process, while parallelism enables the search space to be traversed more efficiently. CME was tested using data from chromosome 2 of Schizosaccharomyces pombe and was found able to generate maps as well as (and sometimes better than) a more traditional method. This paper illustrates the practical benefits of using a symbolic logic programming language and shows that the features of constraint handling and parallel execution bring the development of practical systems based on AI programming technologies nearer to being a reality",Genetic Map Construction with Constraints,,AAAI Press,,,core
46863863,2001-12-20T00:00:00,"My research field is in the area of engineering for Human Computer Interaction. My contributions to the field therefore include ergonomic as well as software design (software architecture). My primary motivation is to develop ways for making novel interaction techniques more usable. I first developed a design space for multimodality. The considerations involved in this design space are not only based on technological capabilities but also on the psychological consequences of the design features on future users. With the results of this work in hand (modality definition and design space), I then examined two research avenues. In the first one, I focused on output multimodal user interfaces applied to visualization of large information spaces. In the second one, I concentrated on new interaction modalities based on the augmented reality paradigm. For each of these two avenues, new ergonomic properties, design rules and patterns, software architectural solutions and new interaction techniques such as the clickable reality and the augmented stroll have been developed.Mes travaux sont résolument ancrés dans l'ingénierie de l'Interaction Homme-Machine. Ce faisant, les deux facettes, conception ergonomique et conception logicielle, sont apparentes dans mes contributions. Mon étude s'articule autour des termes de modalité d'interaction et de multimodalité. Je me suis appuyée sur des résultats généraux incluant la définition d'une modalité et d'un espace de composition de modalités et je les ai infléchis à deux axes de recherche, l'exploration visuelle d'espaces informationnels et la réalité augmentée. Pour chacun de ces axes, des propriétés ergonomiques, des règles et patrons de conception, des solutions architecturales et logicielles, ainsi que des techniques d'interaction comme la réalité cliquable et la ballade augmentée ont été conçus",Modalité d'interaction et multimodalité,,HAL CCSD,,,core
432153495,1999-01-01T00:00:00,"International audienceIn 2001, the DEMETER micro-satellite will belaunched to perform Detection of Electro-Magnetic EmissionsTransmitted from Earthquake Regions.  Its main scientific ob-jective is related to the investigation of the ionospheric perturba-tions due to the seismic and volcanic activity. A system allowingan onboard identification and characterization of spatially andtemporally coherent structures associated with the measurementof one or several electromagneticwavefield components is used.It is based on neural networks. The choice and trainin gof theneural network are done on the ground from availablewaveforms.The parameters of the neural network system are then transmit-ted to the satellite. This reconfiguration process can be repeatedwhenever necessary durin gthe space mission. Details about thefunctionin gand codin goptimization for DSP implementation ispresented. The first function of this system which will be per-formed on the satellite DEMETER is the real-time identifica-tion and characterization of whistler phenomena. An applicationto the analysis of such phenomena observed in data from theAUREOL-3 satellite is exposed",Neural network system for the analysis of transient phenomena onboard the DEMETER micro-satellite,,HAL CCSD,,,core
42820251,"Aug 15, 1990","The development and application of the Spacecraft Health Automated Reasoning Prototype (SHARP) for the operations of the telecommunications systems and link analysis functions in Voyager mission operations are presented. An overview is provided of the design and functional description of the SHARP system as it was applied to Voyager. Some of the current problems and motivations for automation in real-time mission operations are discussed, as are the specific solutions that SHARP provides. The application of SHARP to Voyager telecommunications had the goal of being a proof-of-capability demonstration of artificial intelligence as applied to the problem of real-time monitoring functions in planetary mission operations. AS part of achieving this central goal, the SHARP application effort was also required to address the issue of the design of an appropriate software system architecture for a ground-based, highly automated spacecraft monitoring system for mission operations, including methods for: (1) embedding a knowledge-based expert system for fault detection, isolation, and recovery within this architecture; (2) acquiring, managing, and fusing the multiple sources of information used by operations personnel; and (3) providing information-rich displays to human operators who need to exercise the capabilities of the automated system. In this regard, SHARP has provided an excellent example of how advanced artificial intelligence techniques can be smoothly integrated with a variety of conventionally programmed software modules, as well as guidance and solutions for many questions about automation in mission operations",A report on SHARP (Spacecraft Health Automated Reasoning Prototype) and the Voyager Neptune encounter,https://core.ac.uk/download/pdf/42820251.pdf,,,,core
341985689,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds  Sarra Alqahtani, Rose Gamble  Adv. Sci. Technol. Eng. Syst. J. 2(3), 449-459 (2017); View Description Image Segmentation Using Fuzzy Inference System on YCbCr Color Model  Alvaro Anzueto-Rios, Jose Antonio Moreno-Cadenas, Felipe Gómez-Castañeda, Sergio Garduza-Gonzalez  Adv. Sci. Technol. Eng. Syst. J. 2(3), 460-468 (2017); View Description Segmented and Detailed Visualization of Anatomical Structures based on Augmented Reality for Health Education and Knowledge Discovery  Isabel Cristina Siqueira da Silva, Gerson Klein, Denise Munchen Brandão  Adv. Sci. Technol. Eng. Syst. J. 2(3), 469-478 (2017); View Description Intrusion detection in cloud computing based attack patterns and risk assessment  Ben Charhi Youssef, Mannane Nada, Bendriss Elmehdi, Regragui Boubker  Adv. Sci. Technol. Eng. Syst. J. 2(3), 479-484 (2017); View Description Optimal Sizing and Control Strategy of renewable hybrid systems PV-Diesel Generator-Battery: application to the case of Djanet city of Algeria  Adel Yahiaoui, Khelifa Benmansour, Mohamed Tadjine  Adv. Sci. Technol. Eng. Syst. J. 2(3), 485-491 (2017); View Description RFID Antenna Near-field Characterization Using a New 3D Magnetic Field Probe  Kassem Jomaa, Fabien Ndagijimana, Hussam Ayad, Majida Fadlallah, Jalal Jomaah  Adv. Sci. Technol. Eng. Syst. J. 2(3), 492-497 (2017); View Description Design, Fabrication and Testing of a Dual-Range XY Micro-Motion Stage Driven by Voice Coil Actuators  Xavier Herpe, Matthew Dunnigan, Xianwen Kong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 498-504 (2017); View Description Self-Organizing Map based Feature Learning in Bio-Signal Processing  Marwa Farouk Ibrahim Ibrahim, Adel Ali Al-Jumaily  Adv. Sci. Technol. Eng. Syst. J. 2(3), 505-512 (2017); View Description A delay-dependent distributed SMC for stabilization of a networked robotic system exposed to external disturbances",,'ASTES Journal',10.25046/aj020366,,core
106097620,1989,"Next generation real-time systems will require greater flexibility and predictability than is commonly found in today&apos;s ystems. These future systems include the space sta-tion, integrated vision/robotics/AI systems, collections of humans/robots coordinating to achieve common objectives (usually in hazardous environments such as undersea explo-ration or chemical plants), and various command and control applications. The Spring kernel is a research oriented kernel designed to form the basis of a flexible, hard real-time operating system for such applications. Our approach challenges everal basic assump-tions upon which most current real-time operating systems are built and subsequently advocates a new paradigm based on the notion of predictability and a method for on-line dynamic guarantees ofdeadlines. The Spring kernel is being implemented on a network of (68020 based) multiprocessors called SpringNet. 1 In t roduct ion Real-t ime computing is that type of computing where the correctness of the system depends not only oll the logical result of the computation, but also on the time at which the results are produced. Real-t ime computing systems play a vital role in our society and the spectru",A New Paradigm for Real-Time Operating Systems*,,,,,core
357198335,1990-01-01T00:00:00,"The use of AI techniques in developing design programs provides formalisms for representing p r o b l e m -s o l v i n g processes. In this article, the use of AI techniques in modeling design processes is elaborated with the presentation of three models that formalize the representation of design knowledge within a design program. For the purpose of establishing the relevance of the models presented here, a structured approach to the overall process of design is adopted in which the design process comprises three phases: formulation, synthesis, and evaluation. Design formulation involves identifying the requirements and specifications of the design problem. The result of design formulation is sometimes referred to as the design brief or program or, more simply, the definition of the design problem. Design synthesis includes Studies in design methodology provide various structured approaches to the design process. Many books provide definitions and elaborations of the design process: In the structural engineering field, such books include  There is a need to The focus in this article is on the synthesis phase. During design synthesis, the form of the design solution is identified. This phase of the design process is not well supported by computer-based tools unless the design problem can be formulated in mathematical terms; for example, optimization techniques are used during synthesis when the design problem can be formulated as an objective function and constraints. The recent use of knowledgebased systems for the synthesis of design descriptions has shown promise and forms the basis for the models described here. Experienced designers resort to trial and error less frequently than novice designers when they synthesize designs, suggesting that the use of knowledge-based systems to represent experithe identification of one or more design solutions consistent with the requirements defined during formulation and any additional requirements identified during synthesis. Design evaluation involves interpreting a partially or completely specified design description for conformance with expected performances. This phase of the design process often includes engineering analysis. Although the phases might not be addressed in the order prescribed for the entire design process and are often carried out recursively, there is an inherent order in which designers approach a design problem. Typically, a designer starts with a definition of the design problem, identifies one or more potential design descriptions, and then evaluates the design. The variation occurs in the revisions of the requirements or descriptions and the iterations of the various phases. The identification of different phases in the design process is a beginning for the formalization and understanding of design. What is missing from such methods is how each phase is completed. The use of a structured approach is acceptable for identifying various  ence might aid in synthesizing designs. The major issue then is the explicit representation of design experience in a knowledge base. During design synthesis, a designer considers a design space that contains the knowledge that is used to develop the design solution. A human designer does not need to explicitly identify his(her) design space; it is implicitly developed and expanded as s/he gains experience. A design program, however, does contain an explicit representation of the relevant design space. The nature of the knowledge in the design space must be explicit when considering a knowledge-based approach to design. Simon (1969) presents design as a problemsolving process and, even more specifically, as a search process. The implication of search as a model for design processes is that design knowledge can be expressed as goals and operators. As a general approach to modeling design, search provides a formalism for expressing design knowledge; however, it does not directly address some of the intricacies and idiosyncrasies of design problems. Considering design as problem solving is a beginning to understanding and modeling design, but design problem solving has some additional characteristics that can be exploited by more explicit models. It is interesting to consider the three phases of design as a search process within a design space: Design formulation involves identifying the goal(s) of the design problem. Design synthesis involves the search for one or more design solutions through the selection and application of operators. Design evaluation involves assessing whether the goal(s) have been satisfied. Variations in both the goals and the statespace descriptions as the design process proceeds and the difficulty in predetermining the relevant operators are some of the issues that are not readily addressed in using search for solving design problems. The goals of the problem can change during the problem-solving process, which can indicate a different design space to be searched. One reason design has been difficult to implement as a search process is this change in problem definition during the problem-solving process. One way of dealing with this difficulty is to identify models of the design synthesis process, assuming that formulation has occurred. Another way is to allow synthesis to proceed even with a change of goals. The implication here is that using search as a model for the design process is too general; more specific models that use search in various ways are needed to bridge the gap between a model of design and the eventual representation of design knowledge and experience. The use of AI techniques combined with research in design methodology provides an opportunity to exploit the results of both to produce an understanding of design problem solving. The early efforts in using AI techniques in design resulted in expert systems capable of designing specific artifacts using rule-based approaches. In many cases, the experience in developing rule-based design systems led to the generalization of design problem solving in which the knowledge base was no longer made up of unstructured rules. The justifications for this transition from an unstructured rule base to a design-oriented knowledge base included ease of knowledge acquisition and an increased understanding of design problem solving. Three Models of Design Synthesis In generalizing design problem solving, three distinct models of design synthesis can be identified: decomposition, case-based reasoning, and transformation (figure 1). These models are distinct in their associated formalisms for design knowledge. The models are not necessarily cognitive models; although they might match various approaches humans take when producing design solutions, the correspondence has not 52 AI MAGAZINE been adequately tested. The distinction among the models lies in the representation of design knowledge rather than in their appropriateness for a specific design domain or phase of design. The purpose of identifying more than one design model is in identifying appropriate formalisms for representing design knowledge. Decomposition (figure 2) is perhaps the most ubiquitous model of design. It follows directly from the development of design methodology and has been shown to be useful in the development of knowledgebased design systems. The idea of dividing large complex problems into smaller, less complex problems is well accepted and practiced. It is possible to consider all models of design as some form of decomposition. When we consider a knowledge-based approach to representing design knowledge, the decomposition model has provided a clear position about the type of design knowledge needed. Specific knowledge-based systems for design by decomposition have been developed that identify specific languages for describing design knowledge. Examples of such languages include DSPL  Case-based reasoning is a model of design that directly uses design experience in the form of episodes rather than compiles and generalizes it. The model (figure 3) uses analogic reasoning to select and transform specific solutions to previous design problems to be appropriate as solutions for a new design problem. This model is attractive because the knowledge acquisition for developing generalized representations of design knowledge in a particular domain can be difficult and time consuming. The issues associated with using this model for design include the identification of the necessary information about a design episode to reason about its applicability in a different context, the meaning of a similar design, and the transformation of the solution from the original context to the new context. Although human designers appear to be good at using this type of analogy, it is difficult to automate it. Transformation is a holistic approach to design, similar to case-based reasoning, but uses generalizations rather than specific episodes, like decomposition. In the transformation model (figure 4), the design knowledge is expressed as a set of transformational rules in which the left-hand side (LHS) of the rule is replaced by the right-hand side (RHS) of the rule. The most common application of the transformation model is manifested as grammars. The issues associated with using this model are the representation of the design description, the control in selecting an eligible transformational rule, and the termination of the application of rules. In the following subsections, the definition and use of these models are elaborated. Currently, the models are ill defined, and many issues need to be resolved before they can become domain-independent formalisms defined in sufficient detail to be computer environments for knowledge-based design. In addition, examples are presented of systems that have been implemented that use one of the models. The purpose of the following subsections is to illustrate that such models of design are not domain dependent-the examples are drawn from different domainsand that the models provide an understanding of the nature of the design knowledge that needs to be acquired to implement knowledge-based systems for design. Decomposition Decomposition by definition means that something is decomposed; it also implies a recomposition. The early expert systems for design synthesis implicitly used the decomposition model. Hi-Rise (Maher and Fenves 1984; What is decomposed? It is easy to say that the design problem is decomposed into subproblems, but what is the meaning of decomposition in design? There are at least two meanings to the decomposition of a design problem into subproblems: (1) to decompose a domain of design knowledge, say, structural design, into the various physical components that are used to construct design solutions, say, walls, slabs, and so on, or (2) to decompose into the various functions that must be provided for by a design solution, for example, resisting various types of load and providing open space, until a component can be identified that will provide a specified function. The first approach to decomposition is an object-centered approach in which the design knowledge is organized around the physical systems and components that a particular domain is concerned with. The second approach considers a functional decomposition to a sufficient level of detail in which a function to form mapping can occur. The issue of function versus form as a basis for decomposition is not resolved yet. The point here is that design knowledge comprises various types of knowledge, for example, function and form, and recognizing these types of knowledge facilitates the formalization of the design process and, subsequently, the development of a solution. One approach to dealing with the decomposition of the function versus form dilemma is to ignore the distinction between function and form and identify a uniform representation for either function or form. In a search process approach to design, this uniform representation is typically called a goal. Another approach is to dictate a specific representation for functional decomposition and a specific representation for form decomposition. How is the problem decomposed? The assumption behind decomposition is that each subproblem can be solved independent of the other subproblems. However, this assumption is an idealization rather than a reality for any problem definition. The rule of thumb in decomposition is to decompose into nearly independent subproblems or loosely coupled subproblems. Usually, this decomposition is easier said than done. However, because many have been successful in solving complex problems using decomposi- Articles 54 AI MAGAZINE tion, there must be something to it. A wellunderstood problem is easier to decompose than one that has not yet been explored. Is the decomposition fixed? A fixed decomposition implies that it is not altered for a specific problem and is used without modification for all problems presented. At a general level of abstraction, a fixed decomposition might be possible. For example, it is useful to decompose design into formulation, synthesis, and evaluation at a general level. Such a decomposition is not as useful for a more detailed view of a specific design problem. As soon as a more detailed approach to design is required, a single decomposition is inadequate. For example, the decomposition of the design of a structural system might be (1) design the lateral load resisting system, (2) design the gravity load resisting system, and (3) design the foundation. This decomposition makes sense when the structure is a building with more than five stories. Variations can occur when (1) the structure is a single-family residence, where the lateral load resisting system is insignificant and where the order of the decomposition might change but not the subproblems; (2) the structure is underground, where the lateral load resisting system cannot be considered separate from the gravity load resisting system, and the nature of the subproblems must be reconsidered; and (3) the structure is an offshore oil platform, where the foundation and the various structural systems for lateral and gravity loads are integrated. Once it has been determined that the decomposition should not be fixed, a representation that accommodates variations must be identified. How does recomposition occur? The decomposition of a design problem into subproblems implicitly assumes that recomposition will occur. The fact that recomposition is implicit, rather than explicit, indicates that it is not an obvious process. Recomposition can occur implicitly; that is, by stating the solutions to the subproblems, the entire problem is solved. Recomposition can, however, introduce complications. Considering the subproblems as independent problems is only an idealization of reality. Putting the subproblems together must take into account the interactions. One way of representing the interactions is as constraints; the issue of recomposition then becomes one of constraint satisfaction. The various computer environments that have been developed for design by decomposition answer each of these questions through the identification of a structured language or syntax for describing a knowledge base. As an example, Edesyn provides two major representation structures for the representation of design knowledge: the system and the constraint. The representation of various systems allows the design knowledge base to explicitly represent the decomposition knowledge discretely, although the decomposition of any one system can vary as design proceeds. Edesyn provides a uniform representation for form and function, leaving the distinction to the knowledge base developer. The representation of various constraints allows the recomposition knowledge to be explicitly stated. What we gain from the implementation of the decomposition model as domainindependent computer environments for design is a better understanding of decomposition and its associated problems but, more importantly, the realization that models of design are not domain specific. Case-Based Reasoning Case-based reasoning in design involves the generation of a design solution using the solution or solution process from a previous design problem as a basis. This model of design synthesis requires episodes of design cases rather than generalizations about a design domain. Examples of a case-based reasoning approach to design include Struple  As a process model, case-based reasoning involves several operations. One set of operations is (Darpa 1989) (1) recall relevant cases from case memory, (2) select the most promising case, (3) construct a solution for the new problem, (4) test the solution, (5) evaluate the results, and (6) update case memory by storing the new case. In design synthesis, the operations of most interest are centered on case retrieval, selection, and modification. With the assumption that case memory is large (that is, many design cases are stored), retrieval becomes a search problem in a large space. The selection of a case among potential cases requires recognition of the relevance of each case and how close the case is to providing a solution to the design problem. The modification of the case for the new design problem raises the issues of what should be changed and what should stay the same. It can be assumed that the changes are based on the results of a partial match, where the features of the case that did not match should be changed, but such local changes might not be sufficient. There are guidelines for the development of a case-based reasoning system, regardless of whether the problem is design, planning, or diagnosis, as in the following: Case memory organization: The extremes in representing cases in memory are to represent each case in its entiret",Process Models for Design Synthesis,,,,,core
42829881,"Oct 1, 1988","Over the years, demand on space systems has increased tremendously and this trend will continue for the near future. Enhanced capabilities of space systems, however, can only be met with increased complexity and sophistication of onboard and ground systems. Artificial Intelligence and expert system techniques have great potential in space applications. Expert systems could facilitate autonomous decision making, improve in-orbit fault diagnosis and repair, enhance performance and reduce reliance on ground support. However, real-time expert systems, unlike conventional off-line consultative systems, have to satisfy certain special stringent requirements before they could be used for onboard space applications. Challenging and interesting new environments are faced while developing expert system space applications. This paper discusses the special characteristics, requirements and typical life cycle issues for onboard expert systems. Further, it also describes considerations in design, development, and implementation which are particularly important to real-time expert systems for space applications",Considerations in development of expert systems for real-time space applications,https://core.ac.uk/download/pdf/42829881.pdf,,,,core
491346693,,"With the recent advances of Deep Neural Networks (DNNs) in real-world
applications, such as Automated Driving Systems (ADS) for self-driving
cars, ensuring the reliability and safety of such DNN- enabled Systems
emerges as a fundamental topic in software testing. One of the
essential testing phases of such DNN-enabled systems is online
testing, where the system under test is embedded into a specific and
often simulated application environment (e.g., a driving environment)
and tested in a closed-loop mode in interaction with the environment.
However, despite the importance of online testing for detecting safety
violations, automatically generating new and diverse test data that
lead to safety violations present the following challenges: (1) there
can be many safety requirements to be considered at the same time, (2)
running a high-fidelity simulator is often very
computationally-intensive, and (3) the space of all possible test data
that may trigger safety violations is too large to be exhaustively
explored. In this paper, we address the challenges by proposing a
novel approach, called SAMOTA (Surrogate-Assisted Many-Objective
Testing Approach), extending existing many-objective search algorithms
for test suite generation to efficiently utilize surrogate models that
mimic the simulator, but are much less expensive to run. Empirical
evaluation results on Pylot, an advanced ADS composed of multiple
DNNs, using CARLA, a high-fidelity driving simulator, show that SAMOTA
is significantly more effective and efficient at detecting unknown
safety requirement violations than state-of-the-art many-objective
test suite generation algorithms and random search. In other words,
SAMOTA appears to be a key enabler technology for online testing in
practice",Efficient Online Testing for DNN-Enabled Systems using Surrogate-Assisted and Many-Objective Optimization,https://core.ac.uk/download/491346693.pdf,,,,core
323896730,,"Safety is an important requirement for human-robot interaction. Compliance control can often help to ensure safety in human-robot interaction (HRI). The aim of this work is to develop a compliance control strategy for safe HRI. Compliance can be achieved through passive means (mechanical structure or passive actuation) or through active compliance methods, employing force/torque feedback. This thesis deals with the compliance control of Bristol-Elumotion Robot Torso (BERT) II robot arm which is inherently rigid  and heavy. As the dynamic model of the arm is difficult to obtain and prone to inaccuracies, parametric uncertainties and un-modelled  nonlinearities, a model-free adaptive compliance controller is employed. The control scheme is using a mass-spring-damper system as reference model to produce compliant behaviour. The adaptive control scheme may cause actuator saturations, which could lead to instabilities and eventually windup. Hence, an anti-windup compensator is employed to address actuator saturation issues.  The control scheme is a Cartesian one (tracking x, y and z coordinates) and employing four joints (namely, shoulder flexion, shoulder abduction, humeral rotation and elbow flexion joints) of the BERT II arm. Although, this needs three degrees of freedom (DOF), the fourth redundant DOF is employed to generate human-like motion, minimising a gravitational function. The adaptive compliance control scheme works efficiently for the application and produces good tracking and compliance results.It is often the case that adaptive control schemes are not necessarily (control) optimal, which may create difficulties in the controller design. Furthermore, it is difficult to incorporate constraints or any other desired behaviour. Therefore,  bio-inspired reinforcement learning (RL) schemes are explored. A recently formulated RL based optimal adaptive controller scheme is employed and modified for real time testing on our robot arm. The RL based scheme is implemented for non-constrained and constrained cases in the joint space. Particularly, the results produced with the constrained case are encouraging, where the controller learns to deal with the constraints in the form of joint limits.  An RL based Cartesian model reference  compliance controller is also tested for two links of the BERT II arm. Generally, the results with this scheme are very good. However, there are limitations on the representation of the RL cost functions and the control scheme using neural networks (NNs).To a large extent these limitations have been overcome through a novel practical approach of representing the cost function and the control via a simple neural network. Nevertheless, available computational power permitted only two link experimental implementation.Integration of these new control approaches into practical HRI system is important. A final achievement is an initial HRI experiment for passing of objects between human and robot employing the model reference adaptive compliance control scheme mentioned in the beginning. This experimental scenario is implemented using also separate hand controller and speech interface",Adaptive and reinforcement learning control methods for active compliance control of a humanoid robot arm,,,,,core
343121984,,"Condition monitoring (CM) deliveries significant benefits to the industry by minimising breakdown losses and enhancing the safety and high-performance operation of machinery. However, the use of data acquisition systems with multiple sensors and high sampling rates leads to massive data and causes considerably high cost for purchasing and deploying hardware for data transmission, storage and processing. Hence, data compression is crucial and important to reduce the data size and speed up the calculation for the development of intelligent machine CM systems. Although data compression has received high attention in many fields, few researchers have focused on their research in the field of machine CM. Therefore, this PhD research concentrates on investigating novel and high-performance data compression algorithms according to the characteristics of one-dimensional (1D) and two-dimensional (2D) signals to solve the bottleneck of the massive data transmission, and hence improve the performance of remote and real-time machine CM systems.





The research is carried out according to a compound experimental and analytic route based on a wireless senor network. To demonstrate the effectiveness of data compression based techniques for CM, the prototype of an intelligent wireless sensing system is developed using cost-effective micro-electromechanical systems (MEMS) accelerometers and the Bluetooth low energy (BLE) communication module. Moreover, various waveform parameters with low cost computing in time and frequency domains are investigated and identified that RMS is the most effective parameter to give good indication for the leakage in a piping system, showing that data compression via statistics is effective and thus indicates that the performance of data compression for CM highly depends on applications.





Subsequently, high-performance but high-complexity methods are proposed base on dimension reduction, sparse representation, feature extraction and advanced compressive sensing (CS) for fault diagnosis of rotating machinery with 1D or 2D signals, which have the potentials to be implemented on MEMS modules in a wireless sensor network (WSN) in future. Firstly, a compression scheme based on dimension reduction is proposed to extract the periodic characteristics of the 1D vibration signal. Recurrence plot (RP) of vibration phase space trajectory and its quantification indicators, as well as principal component analysis (PCA), are combined to realize feature extraction, compression and fault classification for a tapered roller bearing system.





Secondly, a two-step compression method is performed on 1D vibration signals based on frequency shift, adaptive sparse representation and CS is explored to overcome the problem of the large quantity of data storage for ball bearing fault diagnosis. Simultaneously, this compression method has the capability to reconstruct envelope signals with noise elimination.Then, for 2D thermal images captured from a two-stage reciprocating compressor, the dense scale-invariant feature transform (SIFT) features indicating edge information are extracted and represented as a sparse matrix by sparse coding. The compressed features are used for the classification of six different types of faults with the support vector machine (SVM).





Finally, the advanced CS technique is exploited on pre-processing the 2D thermal images of gearboxes to realise intelligent fault classification with high accuracy of more than 99.81% by a typical deep learning algorithm, namely convolutional neural network (CNN). The CNN calculation speed is dramatically accelerated with compressed images. All these proposed approaches are evaluated by simulations and experiments, which verifies that they can reliably detect the fault types or classify different fault types with very high accuracy. Besides, the proposed data compression based intelligent CM approaches provide theoretical bases for maintenance-free CM systems because data compression can save the transmission bandwidth and power consumption for remote and real-time machine CM systems",Investigation of Data Compression Methods for Intelligent Machine Condition Monitoring,,,,,core
443941067,,"In this thesis, we investigate Deep Learning models as an artistic medium for new modes of performative, creative expression. We call these Deep Visual Instruments: realtime interactive generative systems that exploit and leverage the capabilities of state-of-the-art Deep Neural Networks (DNN), while allowing Meaningful Human Control, in a Realtime Continuous manner. We characterise Meaningful Human Control in terms of intent, predictability, and accountability; and Realtime Continuous Control with regards to its capacity for performative interaction with immediate feedback, enhancing goal-less exploration. The capabilities of DNNs that we are looking to exploit and leverage in this manner, are their ability to learn hierarchical representations modelling highly complex, real-world data such as images. Thinking of DNNs as tools that extract useful information from massive amounts of Big Data, we investigate ways in which we can navigate and explore what useful information a DNN has learnt, and how we can meaningfully use such a model in the production of artistic and creative works, in a performative, expressive manner. We present five studies that approach this from different but complementary angles. These include: a collaborative, generative sketching application using MCTS and discriminative CNNs; a system to gesturally conduct the realtime generation of text in different styles using an ensemble of LSTM RNNs; a performative tool that allows for the manipulation of hyperparameters in realtime while a Convolutional VAE trains on a live camera feed; a live video feed processing software that allows for digital puppetry and augmented drawing; and a method that allows for long-form story telling within a generative model's latent space with meaningful control over the narrative. We frame our research with the realtime, performative expression provided by musical instruments as a metaphor, in which we think of these systems as not used by a user, but played by a performer","Deep Visual Instruments: Realtime Continuous, Meaningful Human Control over Deep Neural Networks for Creative Expression",https://core.ac.uk/download/443941067.pdf,"Goldsmiths, University of London",10.25602/GOLD.00030191,,core
42833627,"Nov 1, 1987","The Automated Subsystem Control for Life Support System (ASCLSS) program has successfully developed and demonstrated a generic approach to the automation and control of Space Station subsystems. The hierarchical and distributed real time controls system places the required controls authority at every level of the automation system architecture. As a demonstration of the automation technique, the ASCLSS system automated the Air Revitalization Group (ARG) of the Space Station regenerative Environmental Control and Life Support System (ECLSS) using real-time, high fidelity simulators of the ARG processess. This automation system represents an early flight prototype and an important test bed for evaluating Space Station controls technology including future application of ADA software in real-time control and the development and demonstration of embedded artificial intelligence and expert systems (AI/ES) in distributed automation and controls systems",Prototype space station automation system delivered and demonstrated at NASA,https://core.ac.uk/download/pdf/42833627.pdf,,,,core
76971,,"Performing realistic deformation simulations in real time is a challenging problem in computer graphics. Among numerous proposed methods including Finite Element

Modeling and ChainMail, we have implemented a mass spring system because of its acceptable accuracy and speed. Mass spring systems have, however, some drawbacks such as, the determination of simulation coefficients with their iterative nature. Given the correct parameters, mass spring systems can accurately simulate tissue deformations but choosing parameters that capture nonlinear deformation behavior is extremely difficult. Since most of the applications require a large number of elements

i. e. points and springs in the modeling process it is extremely difficult to reach realtime performance with an iterative method. We have developed a new parameter

identification method based on neural networks. The structure of the mass spring system is modified and neural networks are integrated into this structure. The input

space consists of changes in spring lengths and velocities while a ""teacher"" signal is chosen as the total spring force, which is expressed in terms of positional changes and

applied external forces. Neural networks are trained to learn nonlinear tissue characteristics represented by spring stiffness and damping in the mass spring algorithm. The learning algorithm is further enhanced by an adaptive learning rate, developed particularly for mass spring systems. In order to avoid the iterative approach in deformation simulations we have developed a new deformation algorithm. This algorithm defines the relationships between points and springs and specifies a set of rules on spring movements and deformations. These rules result in a deformation surface, which is called the search space. The

deformation algorithm then finds the deformed points and springs in the search space with the help of the defined rules. The algorithm also sets rules on each element i. e.

triangle or tetrahedron so that they do not pass through each other. The new algorithm is considerably faster than the original mass spring systems algorithm and provides an

opportunity for various deformation applications.

We have used mass spring systems and the developed method in the simulation of craniofacial surgery. For this purpose, a patient-specific head model was generated

from MRI medical data by applying medical image processing tools such as, filtering, the segmentation and polygonal representation of such model is obtained using a

surface generation algorithm. Prism volume elements are generated between the skin and bone surfaces so that different tissue layers are included to the head model. Both

methods produce plausible results verified by surgeons",Efficient techniques for soft tissue modeling and simulation,https://core.ac.uk/download/76971.pdf,,,,core
217406971,1990-01-01T08:00:00,"This dissertation addresses a fundamental problem in computational AI--developing a class of massively parallel, neural algorithms for learning robustly, and in real-time, complex nonlinear transformations from representative exemplars. Provision of such a capability is at the core of many real-life problems in robotics, signal processing and control. The concepts of terminal attractors in dynamical systems theory and adjoint operators in nonlinear sensitivity theory are exploited to provide a firm mathematical foundation for learning such mappings with dynamical neural networks, while achieving a dramatic reduction in the overall computational costs. Further, we derive an efficient methodology for handling a multiplicity of application-specific constraints during run-time, that precludes additional retraining or disturbing the synaptic structure of the  learned  network. The scalability of proposed theoretical models to large-scale embodiments in neural hardware is analyzed. Neurodynamical parameters, e.g., decay constants, response gains, etc., are systematically analyzed to understand their implications on network scalability, convergence, throughput and fault tolerance, during both concurrent simulations and implementation in concurrently asynchronous VLSI, optical and opto-electronic hardware. Dynamical diagnostics, e.g., Lyapunov exponents, are used to formally characterize the widely observed dynamical instability in neural networks as  emergent computational chaos . Using contracting operators and nonconstructive theorems from fixed point theory, we rigorously derive necessary and sufficient conditions for eliminating all oscillatory and chaotic behavior in additive-type networks. Extensive benchmarking experiments are conducted with arbitrarily large neural networks (over 100 million interconnects) to verify the methodological robustness of our network  conditioning  formalisms. Finally, we provide insight for exploiting our proposed repertoire of neural learning formalisms in addressing a fundamental problem in robotics--manipulation controller design for robots operating in unpredictable environments. Using some recent results in task analysis and dynamic modeling we develop the  Perceptual Manipulation Architecture . The architecture, conceptualized within a perceptual framework, is shown to be well beyond the state-of-the-art model-directed robotics. For a stronger physical interpretation of its implications, our discussions are embedded in context of a novel systems\u27 concept for automated space operations",Computational Neural Learning Formalisms for Perceptual Manipulation: Singularity Interaction Dynamics Model.,https://core.ac.uk/download/217406971.pdf,LSU Digital Commons,,,core
42829043,"Nov 1, 1988","An intelligent interface is often characterized by the ability to adapt evaluation criteria as the environment and user goals change. Some factors that impact these adaptations are redefinition of task goals and, hence, user requirements; time criticality; and system status. To implement adaptations affected by these factors, a new set of capabilities must be incorporated into the human-computer interface design. These capabilities include: (1) dynamic update and removal of control states based on user inputs, (2) generation and removal of logical dependencies as change occurs, (3) uniform and smooth interfacing to numerous processes, databases, and expert systems, and (4) unobtrusive on-line assistance to users of concepts were applied and incorporated into a human-computer interface using artificial intelligence techniques to create a prototype expert system, Your Orbit Determination Assistant (YODA). YODA is a smart interface that supports, in real teime, orbit analysts who must determine the location of a satellite during the station acquisition phase of a mission. Also described is the integration of four knowledge sources required to support the orbit determination assistant: orbital mechanics, spacecraft specifications, characteristics of the mission support software, and orbit analyst experience. This initial effort is continuing with expansion of YODA's capabilities, including evaluation of results of the orbit determination task",An intelligent interface for satellite operations: Your Orbit Determination Assistant (YODA),https://core.ac.uk/download/pdf/42829043.pdf,,,,core
490670982,,"The subject of autonomy within unmanned aerial vehicles (UAVs) has proven to be a remarkable research field - mostly due to 

the development of AI techniques within embedded advanced bespoke microcontrollers - during the last several decades. For drones, as safety-critical systems, there is an increasing need for onboard detect & avoid (DAA) technology to see, sense or detect conflicting traffic or other hazards due to their high mobility and the complexity of deployed unstructured environments, and subsequently take the appropriate action to avoid collisions depending upon various levels of autonomy. The safe integration of UAV traffic management (UTM) systems with air traffic management (ATM) systems, using intelligent autonomous approaches, is an emerging requirement where the number of diverse UAV applications is increasing on a large scale in dense air traffic areas for completing swarms of multiple complex missions flexibly and simultaneously. Significant progress over the past few years has been made in detecting UAVs present in airspace, identifying them, and determining their existing flight path. With electronic conspicuity (EC) information made available by commercially available low-cost systems such as PilotAware, this study makes greater use of this information by developing an advanced collision management (CM) methodology capable of determining and executing a variety of evasive collision avoidance (CA) manoeuvres using a reactive geometric conflict detection and resolution (CDR) technique. The merits of the proposed methodology have been demonstrated through extensive simulations and real-world field tests. The results show that the proposed methodology can be employed successfully in avoiding collisions while limiting the deviation from the original trajectory in dynamic aerospace without requiring sophisticated sensors and prior training",Towards collision-free trajectory for autonomous and pilot-controlled unmanned aerial vehicles,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
229267628,1990-01-01T08:00:00,"Flexible manufacturing system (FMS) scheduling is a complex problem in nature that leads to a high level of uncertainty due to limited feasible solutions in an extensive search space. Heuristics involving dispatching rules have been widely utilized to obtain good solutions. This strategy has been recently enhanced by FMS scheduling researchers using knowledge-based expert systems as means of resolving scheduling problems. Unfortunately, the knowledge-based expert systems (KBESs) developed are limited in real-time performance due to cracks in their encoded knowledge or lack of adequate plans to address the changing environment.
A framework is developed displaying the capabilities of automatic learning and self-improvement, providing the necessary adaptive scheme to respond to the dynamic nature of flexible manufacturing systems. This proposed framework uses a hybrid architecture that integrates artificial neural networks and knowledge-based expert systems to generate solutions for the real time scheduling of flexible manufacturing systems. In this framework, the artificial neural networks perform pattern recognition and, due to their inherent characteristics, support the implementation of automated knowledge acquisition and refinement strategies through a feedback mechanism. They enable the system to recognize patterns in the tasks to be solved in order to select the best scheduling rule according to different criteria. The knowledge-based expert systems, on the other hand, drive the inference strategy and interpret the constraints and restrictions imposed by the upper levels of the control hierarchy of the flexible manufacturing system. The level of self-organization thus achieved provides a system architecture with a higher probability of success than traditional approaches --Abstract, page iii",A hybrid artificial neural networks and knowledge-based expert systems approach to flexible manufacturing system scheduling,,Scholars\u27 Mine,,,core
236530006,,";-II-yo7A.., ~kl'~_'
0""1'0,-rztZh
_'tad. @~17I:k m;;.,~
1/,-"""",:(:,lM- "".;.7t. ...."". Na,t;.r,
j
1
4
i
tJA--~:P y.oU:
In interpretin9 verbal utterance.Awe can be called on to
interpret oral
?v--M~
are""different.
speech or to interpret text. The two activities
but not entirelv different. One paradigmatic form
of interpretation in oral performance is that of reciprocal
discourse or conversation between two (or more) persons in which
an utterance of one interlocutor gives rise to another utterance
by the other interlocutor. that to another by the first. and so
on. This person-to-person dial09ue M. M. Bakhtin rightly
maintains lies at the ultimate base of all utterance. written as
well as oral. scientific as well as casually conversational or
formally 1ite ra rv (PJi\lQ9ic::lfl)i\9iOi\tJo.o--esdeietor's
Introduction xx). In such dialogue the speakers are alwavs / ?ecessarily engaged in nermeneutics. in interpretin9 one another. /!,\;,-.-0 p- it, ;;.-N L/~ 1---,>'/~ jy:""""'h..< s!-&//'//,:--
;,/""-e-r----ee-t-h speak"""" and h8are~ meaning is being negotiated in the
_. k s' ~ t~W~ _/J_""~/'"".""--;~/;:-'~'··:>o,.. ... ~./""( .,?t.r/~ r.....r;,-> #';'"" ~~~u;?;~P!/~-2~ss;;:~/.../.-.-.~~-€' //~: /I/~\ r: ;A1W~:'"" ..?.P ~it-, // 1tj""J~4~
In fact. in oral utterance the negotiation begins even
before the oral utterance itself. The first speaker needs to
c2rtL
anticipate some coniectural feedback from ~ interlocutor before
ijr..-n- :1?
he or she can devise something to ~~y o~lthink. Only if we are
.)/,-...-?,/,ff_
to some degree in the mind of tbe e~~~ can we formulate our own
thought. for what I say (and articulately think) depends on my
conjectures. before I beain to speak, about your state of mind
4. The Interpersonalism of Hermeneutics, Oral and Other
and about the possible range of your responses. Without
coniectural feedback from at least an imagined interlocutor,
there is nothing for me to say (or think. even to myself).
Speaking of a given matter to a child, I will say something quite
2
different from what I say in speaking about the same matter to an
adult. The audience whose state of mind I feel as enabling me to
shape my own words and thoughts may be vague and only
subconsciously attended to, but it will be there. When I am
writing a text or even when I am mulling over something silently
to myself, I imagine myself or someone else, more or less
distinctly, as an interlocutor.
Your actual response to what I say mayor may not fit my
earlier conjecture about your state of mind on the subject. In
either event, your response enables me to clarify my own thought.
Your actual response makes it possible for me to find out for
myself and to make clear in my counter-response what my fuller
meaning was or is or can be. Your response, whether it is a
question or some other type of utterance, forces me to apply
hermeneutics or interpretation to my own thinking. Even if yOU
say simply, ""Yes,"" I have to interpret this ""yes"" in terms of the
whole situation, calculating whether the ""yes"" refers to what
was trying to say or whether it came so easily that I know I was
not clear. (Normally we do this type of interpretation
automatically and swiftly, not reflectively, and are thus mostly
unaware of it.) But I need an interlocutor, real or imagined, in
order to carryon thought: my own thought constantly calls for
I
J
4. The Interpersonalism of Hermeneutics, Oral and Other 3
interpretation, even to myself, or especially to myself, as well
as to mv heare""r_ I///:,.,;n-I .-Ir f;:~ Z:;-J.!/'~:(A/"".A... f', y -- !/""'V..,ro't. -,.(.J/'/"" """".J~
/7t.,.. ~:,;..o_./;;'1""""'AI '- ..... I~)""~ - ~~/ '""
Thus rec i p roca'I oral discourse commonly interprets itsel f
bilaterally (or multilaterally if more than two are engaged in
it) as it proceeds. It negotiates meaning out of meaning.
j
/
Dialogue is thus itself a form of hermeneutic, inde~the
f ;k~JJ;n...-~0~~·/'~
ultimate model of hermeneutic. Dialogue~is hermeneutic in
hermeneutic's natural habitat. The person I am speaking to often /
;:;-;"" ""-' J
makes clearer to me what I myself mean, by questioning me, ~f'"""""",~, :';""""/',
calling attention to what was not clear (often I was half aware
of exactly this unclarity in speaking, but I left my utterance
unclear, perhaps advertently, perhaps inadvertently). Oral
conversation advertises the intersubjectivity Of all human
thought and its tie-in with the intersubjectivity of expression.
The etymologies of the terms ""hermeneutic"" and
""interpretation"" advertise their grounding in the world of
interpersonal exchange or intersubjectivity. Let us look at
""interpretation"" first. The thi rd syllable (::RJ:§ls) ofio.t§lLPL§lS,
the Latin word lying back of the term ""interpretation"" and its
cognates, comes from an Indo-European root, P.§lL.:m:e,aning to
traffic in, to sell. and. more remotely, to hand over. to
distribute. This root of the PL§l.:i:n. ""interpretation"" belongs,
with many other verbal roots, to a more generalized Indo-European
root group. also pg.t:.::: .., which forms the base of many prepositions
/1
and many preverbs with the fundamental meaning of ""forward,
""through,"" a meaning which becomes widely extended to senses such
!
/
4. The Interpersonalism of Hermeneutics, Oral and Other 4
as ""in front of,"" ""before,"" ""early,"" ""around,"" etc. To this
root, PJ;)L, which in its ""through"" meaning bespeaks betweenness,
the Latin form adds the preposi tion .to.];.§wXh,ich itsel f also
means ""between"" (Indo-European root §n, in) and thus compounds
the notion of exchange underlying intersubjectivity: between-between.
The Latin term !n.:!&.LRX§Bt.hus refers ini tially to an
agent who enters between two parties concerning something already
at stake between the two parties, a between-the-between. From
this tangle, 1n];.s_CRX§Bcomes to mean interpreter pretty much in
the English sense of the word, that is, an explainer, It will be
noted how far all this is from any sense of language and/or
thought as logocentric, that is, as a set of signs simply cued
one-to-one to each other and purportedly to external reality !outside consciousness. Rather, the whole situation is radically
.i.:O.t&,Cpersonal,w/<./
To sum up, in the world of interpretation or hermeneutics
we are in a climate of interpersonal negotiation, in which
meaning is sustained by the often complex interaction of two (or
more) persons~ith one another in discou~, The issue, the
meaning, lies somehow ""between"" the two discussants, and the
interpreter inserts himself or herself into this betweenness to
deal with it so as to bring out what may need being brought out,
for utterance always both reveals and conceals, as Heidegger has
ij /' -. / 0 // .y,. ~ made us insistently aware..vwd'ad ""lVn ""~- -Y/J/~ ,r ~ );?,
Moreover, an interpreter can be not only in between persons
interacting with one another in conversation, but also between
4. The Interpersonalism of Hermeneutics. Oral and Other 5
persons and phenomena not yet verbalized by them. as when an
interpreter interprets something previously outside the verbal
continuum itself. such as a band of red clouds at sunset or a~ ~~~me.~UJSIn such cases. the interpreter herself or himself
introduces the subject into the interpersonal world.
The Greek term bgrm~.n~U? interpreter. which. with its
cognates. provides the Greek-based English ""hermeneutics."" more
or less the equivalent of the Latin-based English term
""interpretation."" is of obscure origin. Strangely enough. it may
or may not be directly related etymologically to the name of the
Greek god Hermes. a name seemingly derived from the Greek word
.h.~.rm.a?.heap of stones such as was commonly used to mark
boundaries. and which was thus itself a kind of in-betweenness.
Whatever the exact etymological connections here. Hermes was
conceived of as a supreme negotiator. the god of commerce.
eloquence. invention. travel. and theft--all
I~k-
It~appears evident that.
involving some sort
of in-betweenness. whatever the
etymological facts. the termsh.~rT1]~n,~\da?nd tJ.~UlJ.w~o.u?ld
doubtless be associated in the minds of Greek speakers. Thus
""hermeneutics"" is. even if not etymologically. at least in
meaning very much of a piece with ""interpretation."" Both
underscore interchange. although. as earlier noted.
""hermeneutics"" today tends to refer to a more studied. self- !conSCiOUS.""scientific"" operation than does ""interpretation.""
, ..~ . ii .,'
f0/1J'/Y'_W'i~r J?iI.-Oc.-.AVY4: ~!NV'..-<' -r ~/C""/ftY""~ :/..&,
, ~Despite our tendency to think of interpretation as applying
4. The Interpersonalism of Hermeneutics, Oral and Other 6
paradigmatically to texts, the world of interpretation
historically (that is, here, etymologically--for real etymology
is history) is in a deep, basic sense primarily an oral world,
for oral communication is immediately and irremediably
interactive and interpretive. In oral expression, real people
must here and now really interact with real people, as all must
interact with the environment, human and nonhuman, in which they
are embedded. interpreting to one another and to themselves as
they verbalize.
It is easy to believe that texts are not at all part of such
open, ongoing interpretive negotiation. One of the most
widespread and fundamental errors of the past few generations of
literary critics (such as the Russian Formalists or the American
and British New Critics) has been the assumption, quite often not
clearly articulated, that to put an utterance in writing is to
remove it from this state of oral discourse and thus to ""fix"" it,
to specify and totalize its meaning once and for all. Such
assumptions have not been uncommon in philosophy and science and
they lie back of the often mindless attempts to establish exactly
what the totally explicit meaning of the United States 1 Constitution o~oth~r legal doc~mentA was when they were o:..#.::?~..i-, //'4'# '.-//..r/4·~//'.
written. 71\Jlne6an.of course discover much about the original
meaning of a text and the intent of its author or authors, but
not with total explicitness, for the original intent and text
could not be totally explicit, even to the authors themselves.
As noted earlier, total verbal explicitness is impossible, in
J
/
4. The Interpersonalism of Hermeneutics, Oral and Other 7
inscribed texts as much as in spoken words.
As a member of President Lyndon B. Johnson's 1966-67 Task
Force on Education, which was engaged in, among other things,
suggesting exact texts out of which to devise laws, I can testify
personally to some of the problems here. After the Task Force
had settled on a text for a proposed law, we would regularly have
to call in expert educational consultants to help us discover
more fully what our wording would actually mean in the real world. _ /
1'/£ r~&';x:< c""u-m:/;/~ u{,y-c.-~-L rA J ~~ r: V""""""
of education to which we were addre.2sing ourselves.!\ The law had ~'(
to deal with and had to be given its meaning in terms of specific
imaginable situations. No one could envision all the situations.
But the expert consultants could envision more than we could and
thus could enable us to understand better what we were saying
and/or writing.
As earlier suggested, because texts are totally fixed
visually, because they have limited borders, they encourage us to
think that the meanings they express are similarly bordered--all
act and no residual potential. ""A poem should not mean / But
be,"" Archibald MacLeish wrote in his widely acclaimed ""Ars
Poetica,"" presumably having in mind a poem set down in writing or
print, as his own poems were. But words cannot just ""be."" Words
IN/it£,'
are~events, happenings, moving with the flow of time, not things,
as texts make them appear.
iii
A text certainly does separate an utterance from its author,
who, once he or she has written down the text, may as well be
4. The Interpersonalism of Hermeneutics, Oral and Other 8
dead. In this sense, writing creates anonymous discourse, as has
often been pointed out. But removing an utterance from its
author is not removing it from discourse. No utterance can exist
outside discourse, outside a transactional setting. Putting an
utterance into script can only interrupt discourse, string it out
indefinitely in time and space. But not ""fix"" it, in the sense
of endowing it with a totally determinate meaning apart from all
other discourse. A closed system of any sort is always
impossible, and texts are no exceptions to this rule. In fact,
they are conspicuous nonexceptions since the meaning of every
word in them has to be determined from outside the text itself
and ultimately from nonverbal rather than verbal context.
Putting an utterance into script, then, can only interrupt
discourse, postpone its continuation, protract it indefinitely in
time and space. But not ""fix"" it, not give it a totally
determinate meaning all from within, apart from all other
discourse. An utterance does not become clearer or more
independent of context simply by being written down. It becomes
simply retrievable--quite often with the original context largely
lost.
When is a text most truly a text, an expression of words?
When does an inscribed text ""say"" something? Insofar as a text
is static. fixed. ""out there,"" reified, it is not an utterance
but a visual (and/or tactile) design, not a reproduction of
sound. but a visual code for the code of spoken words, a code of
a code. This is of course by no means to say that script is
4. The Interpersonalism of Hermeneutics. Oral and Other 9
simply a sUbstitute for writing. In one way or another, codes
modify what they encode. Writing raises consciousness (Ong,
Text. howeyer. functions fully as a text (and thus in
actuality raises consciousness) only when it reenters auditory
discourse (or in the case of congenitally deaf, persons. the
equivalent of auditory di scou rse.l , Text can be made to reenter
I discourse. to function as utterance only by something nontextual)';.-j' ~1r
a code in a living person's mind for converting the visual into
the auditory. the code that we learn in order to read. When a
person possessing the appropriate reading code moves through the
visual structure of an inscription and converts it into a
temporal sequence of sound, aloud or in the imagination. directly
or indirectly--that is, when a reader reads a text--only then
does the text become what it was put down to be, that is, an
utterance and only then does the suspended discourse continue,
and with it verbalized meaning.
There is some evidence (Stubbs 12) that reading can be done
without subvocal speech, that is, without muscular and nervous
activity around the larynx, but even were this absolutely
certain, lack of subvocal speech does not mean lack of imagined
sound. Reading silently even without subvocal speech, a person
reading a Korean text is imaginatively in a different sound world
from that of a person reading an English text. Since they have
to be read to function as texts. all texts have human meaning ! only insofar as they are converted into the extratextual. the
f ""Jr''''')
aUditory\ at least in the imagination.
4. The Interpersonalism of Hermeneutics, Oral and Other
iv
All text is pretext--in pretty well all the senses that can
10
be assigned to this statement. This is why any text can be so
readily deconstructed: despite appearances, it never was standing
and never could stand on its own feet. (Neither, for that
matter, can spoken words: for their function, they depend
ultimately on the nonverbal, including silence.) Writing is a
technology that restructures thought, which at its origins had
been orally based. Writing has restructured thought marvelously
and productively but also stressfully--the stressfulness must not
be forgotten. (See Ong, ""Writing Is a Technology"" and QJ:?!.l.iJ;;y.)
Since text separates utterance from its author, a
distinctive feature of textual utterance as against oral
utterance is that its author cannot absolutely predict or often
even discover who all will continue the discourse he or she has
engaged in by inscribing the text. To this extent the writer
cannot discover by any means all that he or she has said in his
or her written text, because it is in the interchange with
interlocutors, as indicated above, that the fuller meaning of
what is said in an utterance appears even to the utterer. Anyone
might pick up and read a text once it has been set down and in
doing so would have to interpret it, that is, fit it in one way
or another into the ongoing thought and conversation of the
milieu in which the reading is being done--which means develop
its concealed implications, as speakers in a dialogue do with one
another's utterances.
4. The Interpersonalism of Hermeneutics, Oral and Other 11
This is what hermeneutics does in explaining a text from
antiquity: it makes the text intelligible by introducing it into
living dialogue that must be carried on in present-day terms, not
infrequently with great difficulty. When the reader reads the
text, to understand it he or she has not only to recreate the
lifeworld in which the text actually came into existence, insofar
as this world can be recreated (and it never can be fully A/ recreated), but also to introduce that world and wi th it the text .f0',,,\-l""
in one way or another into the reader's own world--Gadamer's
""conversation with the text"" (Ir:.\,JJJL._9.IJ.9...i'!g.:t09.,;t 331).
That is to say, when the reader reads a text, the
interrupted discourse is picked up--one can hardly say simply
resumed, for the interlocutor who has had his or her say in the
text is generally no longer at hand after the point at which his
or her text ends. He or she cannot respond to the reader's
response, which may actually be a quer.v . (1his i~*,wt;,atP~at9 in. - --;t t .•~ V', ""~~""""'A- ""'-LN""0.1}'/'y./:·.-'<'~
the PJ1.9.E;l9.C\,J_s.objects to about writin91\ although ma.gf course puts
Rsb·vD
his objection into writing--see Ong, QH""-U.t.y 80.) With special
skills and great effort the reader may be able to reconstruct
conjectured responses of the absent writer which will fit the
text somehow into the milieu in which the reading is being done.
No matter what, this fit must somehow be made. for to understand
the text the reader must relate it somehow to what he or she
knows in the present living milieu. In doing so, of course, the
reader may well have to enlarge the present milieu's purview--
one way in which studying the past enriches the present. But if
4. The Interpersonalism of Hermeneutics.
C)<.J~t\
the reader cannot in any way/\relate what was said to the ongoing
Oral and Other 12
dialogue setting in which the reader really lives. the reading is
utterly meaningless.
All this is to say that when the reader reads a text, the
interrupted discourse resumes, often--or even most often--between
persons who have never known one another, perhaps resumes with
great effort, implemented by laborious, self-conscious, and one-sided
interpretive work (fitting past into present-day living
consciousness), without which the inscription may say very
little. for the living interlocutor (the reader) must align the
dialogue alone, that is. in the absence of the other
interlocutor. the writer. who may have been dead for centuries.
Hermeneutics isn't always easy. And the discommoding time-span
need not be centuries. One can even have problems of this sort
in interpreting a note made to oneself a month ago--or even
yesterday. (Where was 1. in what state of mind. when I wrote .tIl.a..t
down?)
Human thought is thus all linked to discourse. Thought and
discourse are always contoured bv time. There is no time-free
knowledge floating about somewhere outside specific, individual,
time-bounded human beings. Again, this does not mean that
thought cannot be ""objective,"" but that objectivity can be known
only ~ubj~c!i-vel~:. that is, ?-nly by individual historical
h1 r.rCj#-/#-<- ~M ~ tCIt?/~
personsl\-there is no objective human knowledge at large somewhere
outside individual existing human consciousnesses.
Once again, this existence only inside human consciousnesses
4. The Interpersonalism of Hermeneutics, Oral and Other 13
does not make the truth of human knowledge unverifiable. Quite
the contrary, it makes the truth of human knowledge eminently
verifiable, but historically and complexly and open-endedly so.
To verify the truth, one has to take into consideration the
knowers, too--speaker and hearer or writer and reader--and the
density of the history in which the knowers are embedded. To
v~rify human thought, one has to verify as well other matters in
the total environment of the originator and of the receiver of
the message. In other words, as earlier noted, human thought is
marked not by ""relativism"" (indeterminacy, lack of fixity) but by
relatedness (multiple and active engagement with other thought
and with actuality in a variety of ways, always including the
nonverbal as well as the verbal, the unsaid as well as the said-
-see Ty 1e r , §.<!..td.and \'!J1sRe!'tl<.<!.RJ.§J,
All human thought and expression is complex, If it ties
into the oral world, to be interpreted it must be related to all
the density of the existential world in which the oral expression
is embedded, If it ties into writing, it must be verified and
interpreted by being related, so far as possible, to the world of
the original text and to the other texts (and oral utterances)
with which the original is interwoven explicitly or implicitly.
It must be related as well as to readings that the text can be
given over time as readers come to the text with more and more
knowledge that will enable them, for example, to make more and
more explicit what the writer, not only consciously, but also
subconsciously or unconsciously and unwittingly, but really, was
4. The Interpersonalism of Hermeneuti",Item 0002,,Saint Louis University Libraries Special Collections,,,core
457494925,,"Blade Runner 2049 (Villeneuve, 2017) uses the manner with which near-future technology recreates or feigns consciousness in order to present a wider discourse around notions of identity, memory, and the formulation of the human self and subjectivity. The franchise, including predecessor Blade Runner (Ridley Scott, 1982) and three short film prequels – Blade Runner: Black Out 2022 (Wantabe, 2017), 2036: Nexus Dawn (Luke Scott, 2017), and 2048: Nowhere to Run (Luke Scott, 2017) – explores the impact of technological change on society, and the ethics and philosophical concerns pertaining to the use of such technologies.



The world of Blade Runner is orientated around three main themes; (1) the development, use, and exploitation of technology, (2) the ethics related to the deployment of this technology by members of the public and corporations, and (3) an exploration of the nature of what constitutes consciousness specifically related to artificial intelligence and bioengineered technology. In Blade Runner 2049, these themes manifest themselves through two key characters. K is a ‘replicant’ slave whose memories are entirely artificial though who experiences life in the world through a bioengineered body and Joi, K’s holographic companion initially restricted to K’s home through the limits of its projection system, and born of an artificial intelligence that evolves over time.



Mireille Hildebrandt observes, though both automation and autonomic machine learning processes rely on algorithms, the distinction is that automation is static, while autonomic machine learning is ‘adaptive, dynamic and more or less transformative” (2016: 57). As such both K and Joi can be determined as autonomic – or self-governing systems – where their respective bioengineered or machine learning systems allows them the space to develop – and in some readings of the film, fully form - their own human-like emotions. Each satisfies Martin Heidegger’s notion of an “an openness-of-being” in that both entities are present and able to directly relate to the world around them (1977). 



Andrew Schopp notes that proponents of transhumanism such as Raymond Kurzweil (1999, 2012) commonly represent new technologies as a catalyst for such changes as a positive outcome or ideal (2019). Each iteration of Bladerunner aligns with other transmedia representations of AI - challenges this notion, presenting the ethical and socio-political challenges of technological change through the prism of current experience throwing light on how and where such change presents a challenge to the human experience (2019: 66).



This paper considers how the film presents representations of machine learning or AI as a biocapitalist discourse that presents the philosophical and ethical impacts of real-world applications of technology",Mere Data Makes A Man: Artificial Intelligences In Blade Runner 2049,https://core.ac.uk/download/457494925.pdf,'Edinburgh University Press',,,core
42829923,"Oct 1, 1988","ADEPT is an expert system that integrates knowledge from three different suppliers to offer an advanced fault-detection system, and is designed for two modes of operation: real-time fault isolation and simulated modeling. Real time fault isolation of components is accomplished on a power system breadboard through the Fault Isolation Expert System (FIES II) interface with a rule system developed in-house. Faults are quickly detected and displayed and the rules and chain of reasoning optionally provided on a Laser printer. This system consists of a simulated Space Station power module using direct-current power supplies for Solar arrays on three power busses. For tests of the system's ability to locate faults inserted via switches, loads are configured by an INTEL microcomputer and the Symbolics artificial intelligence development system. As these loads are resistive in nature, Ohm's Law is used as the basis for rules by which faults are located. The three-bus system can correct faults automatically where there is a surplus of power available on any of the three busses. Techniques developed and used can be applied readily to other control systems requiring rapid intelligent decisions. Simulated modelling, used for theoretical studies, is implemented using a modified version of Kennedy Space Center's KATE (Knowledge-Based Automatic Test Equipment), FIES II windowing, and an ADEPT knowledge base. A load scheduler and a fault recovery system are currently under development to support both modes of operation",Automatic Detection of Electric Power Troubles (ADEPT),https://core.ac.uk/download/pdf/42829923.pdf,,,,core
8363147,,"Inferring the gene content of ancestral genomes is a fundamental challenge in molecular evolution. Due to the statistical nature of this problem, ancestral genomes inferred by the maximum likelihood (ML) or the maximum-parsimony (MP) methods are prone to considerable error rates. In general, these errors are difficult to abolish by using longer genomic sequences or by analyzing more taxa. This study describes a new approach for improving ancestral genome reconstruction, the ancestral coevolver (ACE), which utilizes coevolutionary information to improve the accuracy of such reconstructions over previous approaches. The principal idea is to reduce the potentially large solution space by choosing a single optimal (or near optimal) solution that is in accord with the coevolutionary relationships between protein families. Simulation experiments, both on artificial and real biological data, show that ACE yields a marked decrease in error rate compared with ML or MP. Applied to a large data set (95 organisms, 4873 protein families, and 10,000 coevolutionary relationships), some of the ancestral genomes reconstructed by ACE were remarkably different in their gene content from those reconstructed by ML or MP alone (more than 10% in some nodes). These reconstructions, while having almost similar likelihood/parsimony scores as those obtained with ML/MP, had markedly higher concordance with the coevolutionary information. Specifically, when ACE was implemented to improve the results of ML, it added a large number of proteins to those encoded by LUCA (last universal common ancestor), most of them ribosomal proteins and components of the F0F1-type ATP synthase/ATPases, complexes that are vital in most living organisms. Our analysis suggests that LUCA appears to have been bacterial-like and had a genome size similar to the genome sizes of many extant organisms",Reconstructing ancestral gene content by coevolution,,Cold Spring Harbor Laboratory Press,,,core
491164275,,"Testing has been widely recognised as di\u81cult for AI applications. Œis paper proposes a set
of testing strategies for testing machine learning applications in the framework of the datamorphism
testing methodology. In these strategies, testing aims at exploring the data space
of a classi€cation or clustering application to discover the boundaries between classes that the
machine learning application de€nes. Œis enables the tester to understand precisely the behaviour
and function of the so‰ware under test. In the paper, three variants of exploratory
strategies are presented with the algorithms implemented in the automated datamorphic testing
tool Morphy. Œe correctness of these algorithms are formally proved. Œeir capability and
cost of discovering borders between classes are evaluated via a set of controlled experiments
with manually designed subjects and a set of case studies with real machine learning models",Discovering boundary values of feature-based machine learning classifiers through exploratory datamorphic testing,,,,,core
373847869,,"The information about ships’ fuel consumption is critical for condition monitoring, navigation planning, energy management and intelligent decision-making. Detailed analysis, modelling and optimisation of fuel consumption can provide great support for maritime management and operation and are of significance to water transportation. In this study, the real-time status monitoring data and hydrological data of inland ships are collected by multiple sensors, and a multi-source data processing method and a calculation method for real-time fuel consumption are proposed. Considering the influence of navigational status and environmental factors, including water depth, water speed, wind speed and wind angle, the Long Short-Term Memory (LSTM) neural network is then tailored and implemented to build models for prediction of real-time fuel consumption rate. The validation experiment shows the developed model performs better than some regression models and conventional Recurrent Neural Networks (RNNs). Finally, based on the fuel consumption rate model and the speed over ground model constructed by LSTM, the Reduced Space Searching Algorithm (RSSA) is successfully used to optimise the fuel consumption and the total cost of a whole voyage",Prediction and optimisation of fuel consumption for inland ships considering real-time status and environmental factors,,'Elsevier BV',10.1016/j.oceaneng.2020.108530,,core
435142828,,"The Cosmologies project aims to situate the listener inside a virtual grand piano by enabling computer processes to learn from the spatial presence of the live instrument and performer. We propose novel techniques that leverage mea- surements of natural acoustic phenomena to inform spatial sound composition and synthesis. Measured radiation pat- terns of acoustic instruments are applied interactively in response to a live input to synthesize spatial forms in real time. We implement this with software tools for the first time connecting audio descriptor analysis and corpus-based syn- thesis to spatialization using Higher-Order Ambisonics and machine learning. The resulting musical work, Cosmologies for piano and 3D electronics, explodes the space inside the grand piano out to the space of the concert hall, allowing the listener to experience its secret inner life",Instrumental Radiation Patterns as Models for Corpus-Based Spatial Sound Synthesis: Cosmologies for Piano and 3D Electronics,https://core.ac.uk/download/435142828.pdf,,,,core
385853823,,"2020PDFTech ReportShahabi, CyrusTran, LuanMun, MinChiang, Yao YiUniversity of Southern CaliforniaPacific Southwest Region 9 UTC, University of Southern CaliforniaCalifornia. Dept. of Transportation. Division of Research, Innovation, and System InformationUnited States. Department of Transportation. University Transportation Centers (UTC) ProgramCalifornia. Dept. of Transportation. Division of Research and InnovationLos Angeles (California)United StatesBus transitForecastingMachine learningNeural networksPassenger transportationPublic transitTraffic analysis zonesTraffic estimationTraffic flowTraffic flow rateTraffic modelsTransit trafficTransportationPublic Transportation Information Sharing and Analysis CenterPSR-18-10USDOT Grant 69A3551747109 Caltrans Grant 65A0674, Task Order 001In this project, we developed a deep learning approach for traffic flow forecasting and bus arrival time estimation in Los Angeles. First, we developed a novel Graph Convolutional Recurrent Neural Network (GCRNN) to model and forecast traffic flows at different spatial and temporal resolutions. Our GCRNN model considers not only the location of traffic sensors but also their relationships (i.e., topological dependency) in space, which was critical to achieving the best performance for all forecasting horizons compared to the existing methods. Next, we implemented a Geo-Convolution Long Short-Term Memory (Geo-Conv LSTM) framework to model bus Estimated Time of Arrival (ETA) by incorporating the traffic flow predictions of our GCRNN. Using the real-world traffic sensor datasets archived in our data warehouse, we showed that our proposed bus ETA model is more accurate than the existing method, Gradient Boosted Decision Tree (GBDT), by 27% in estimating bus travel time. Lastly, we deployed both models as web applications so that users can access traffic prediction data and check bus arrival times to a destination location from a starting point.104",Deep-Learning Traffic Flow Prediction for Forecasting Performance Measurement of Public Transportation Systems,,,,,core
42829070,"Nov 1, 1988","Automatic Detection of Electric Power Troubles (A DEPT) is an expert system that integrates knowledge from three different suppliers to offer an advanced fault-detection system. It is designed for two modes of operation: real time fault isolation and simulated modeling. Real time fault isolation of components is accomplished on a power system breadboard through the Fault Isolation Expert System (FIES II) interface with a rule system developed in-house. Faults are quickly detected and displayed and the rules and chain of reasoning optionally provided on a laser printer. This system consists of a simulated space station power module using direct-current power supplies for solar arrays on three power buses. For tests of the system's ablilty to locate faults inserted via switches, loads are configured by an INTEL microcomputer and the Symbolics artificial intelligence development system. As these loads are resistive in nature, Ohm's Law is used as the basis for rules by which faults are located. The three-bus system can correct faults automatically where there is a surplus of power available on any of the three buses. Techniques developed and used can be applied readily to other control systems requiring rapid intelligent decisions. Simulated modeling, used for theoretical studies, is implemented using a modified version of Kennedy Space Center's KATE (Knowledge-Based Automatic Test Equipment), FIES II windowing, and an ADEPT knowledge base",Automatic Detection of Electric Power Troubles (ADEPT),https://core.ac.uk/download/pdf/42829070.pdf,,,,core
341985688,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds",,'ASTES Journal',10.25046/aj020358,,core
42822532,"May 1, 1990","The Maintenance and Diagnostic System (MDS) that is being developed at Honeywell to enhance the Fault Detection Isolation and Recovery system (FDIR) for the Attitude Determination and Control System on Space Station Freedom is described. The MDS demonstrates ways that AI-based techniques can be used to improve the maintainability and safety of the Station by helping to resolve fault anomalies that cannot be fully determined by built-in-test, by providing predictive maintenance capabilities, and by providing expert maintenance assistance. The MDS will address the problems associated with reasoning about dynamic, continuous information versus only about static data, the concerns of porting software based on AI techniques to embedded targets, and the difficulties associated with real-time response. An initial prototype was built of the MDS. The prototype executes on Sun and IBM PS/2 hardware and is implemented in the Common Lisp; further work will evaluate its functionality and develop mechanisms to port the code to Ada",Attitude Determination and Control System (ADCS) and Maintenance and Diagnostic System (MDS): A maintenance and diagnostic system for Space Station Freedom,https://core.ac.uk/download/pdf/42822532.pdf,,,,core
222824231,,"The global imbalance between the healthcare provider and patient ratio, an increasingly elderly population and resource-limited settings have triggered the demand for point-of-care (POC) platforms, prompting the growth of personalised healthcare and homecare solutions. This thesis presents an investigation into an AI-enabled image-based system to perform automatic colourimetric tests in real-time. The case study of wet-chemical-based enzyme-linked immunosorbent assay (ELISA) and dry-chemical-based lateral flow assay (LFA) were utilised to design and develop an intelligent framework for chromaticity analysis with minimal user intervention or additional hardware attachments.   

The proposed system was designed by exploring state-of-the-art solutions for each component of an image-based colourimetric test, trial and error, and domain knowledge. At first, a reaction phase and time-dependent approach was proposed to track the dynamic changes in a colourimetric reaction by calculating the Euclidean distances. Subsequently, the final static stage of the reactions were considered and the images were pre-processed and segmented before applying vigorous noise removal techniques. The 10-fold cross-validated classifiers were trained with the optimum number of features using supervised machine learning. A completely separate testing dataset was utilised while testing the model.  Additionally, a pre-trained model of deep learning was deployed to determine the type of colourimetric test, which can be integrated into the system where feasible. 

Based on our study, the reaction phase and time-dependent scheme was found to be more suitable for wet-chemical-based assays, particularly for low concentration samples. In addition to classification, the approach can assist in optimising the reaction time.  However, due to the requirement of significant memory space by the video frames, the final system consisted of an alternative approach - considering only the reaction phase and time-independent scheme. On an ideal condition, the later approach provided more than 98% accuracy for colourimetric decision. Furthermore, the exploration of a pre-trained deep learning model revealed its strength in the test-type detection, instead providing the colourimetric classification. Therefore, deep learning was deployed to initiate the system based on the assay type (i.e. ELISA or LFA), which provided 100% accuracy.  

The system we demonstrated complies with the ASSURED criteria. As compared to the existing systems, the proposed intelligent and robust system with real-time processing capabilities has experienced a more extensive course of validation to enumerate the reliability of the system. Unlike most of the works in the literature, the proposed system provided the colourimetric prediction without any opto-mechanical attachment.  Such an easy-to-use and computationally efficient system can be integrated into a server or deployed on a mobile platform to create better harmony between biochemical and computational complexity and eliminate the subjectivity of interpretation",An Intelligent Image-based Colourimetric Test Framework for Diagnosis,https://core.ac.uk/download/222824231.pdf,,,,core
42770953,"Dec. 18, 1985","A high level of automation is of paramount importance in most space operations. It is critical for unmanned missions and greatly increases the effectiveness of manned missions. However, although many functions can be automated by using advanced engineering techniques, others require complex reasoning, sensing, and manipulatory capabilities that go beyond this technology. Automation of fault diagnosis and malfunction handling is a case in point. The military have long been interested in this problem, and have developed automatic test equipment to aid in the maintenance of complex military hardware. These systems are all based on conventional software and engineering techniques. However, the effectiveness of such test equipment is severely limited. The equipment is inflexible and unresponsive to the skill level of the technicians using it. The diagnostic procedures cannot be matched to the exigencies of the current situation nor can they cope with reconfiguration or modification of the items under test. The diagnosis cannot be guided by useful advice from technicians and, when a fault cannot be isolated, no explanation is given as to the cause of failure. Because these systems perform a prescribed sequence of tests, they cannot utilize knowledge of a particular situation to focus attention on more likely trouble spots. Consequently, real-time performance is highly unsatisfactory. Furthermore, the cost of developing test software is substantial and time to maturation is excessive. Significant advances in artificial intelligence (AI) have recently led to the development of powerful and flexible reasoning systems, known as expert or knowledge-based systems. We have devised a powerful and theoretically sound scheme for representing and reasoning about procedural knowledge",Development of an Expert System for Representing Procedural Knowledge,https://core.ac.uk/download/pdf/42770953.pdf,,,,core
291403759,,"Autonomic optical transmission and networking requires machine learning (ML) models to be trained with large datasets. However, the availability of enough real data to produce accurate ML models is rarely ensured since new optical equipment and techniques are continuously being deployed in the network. One option is to generate data from simulations and lab experiments, but such data could not cover the whole features space and would translate into inaccuracies in the ML models. In this paper, we propose an ML-based algorithm life cycle to facilitate ML deployment in real operator networks. The dataset for ML training can be initially populated based on the results from simulations and lab experiments. Once ML models are generated, ML retraining can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits of the proposed learning cycle for general use cases. In addition, two specific use cases are proposed and demonstrated that implement different learning strategies: (i) a two-phase strategy performing out-of-field training using data from simulations and lab experiments with generic equipment, followed by an in-field adaptation to support heterogeneous equipment (the accuracy of this strategy is shown for a use case of failure detection and identification), and (ii) in-field retraining, where ML models are retrained after detecting model inaccuracies. Different approaches are analyzed and evaluated for a use case of autonomic transmission, where results show the significant benefits of collective learning.Peer Reviewe",Learning life cycle to speed up autonomic optical transmission and networking adoption,,Institute of Electrical and Electronics Engineers (IEEE),,,core
6221756,,"In this paper, a Travel Demand Management strategy known as the Downtown Space Reservation System (DSRS) is introduced. The purpose of this system is to facilitate the mitigation of traffic congestion in a cordon-based downtown area by requiring people who want to drive into this area to make reservations in advance. An integer programming formulation is provided to obtain the optimal mix of vehicles and trips that are characterized by a series of factors such as vehicle occupancy, departure time, and trip length with an objective of maximizing total system throughput and revenue. Based upon the optimal solution, an ""intelligent"" module is built using artificial neural networks that enables the transportation authority to make decisions in real time on whether to accept an incoming request. An example is provided that demonstrates that the solution of the ""intelligent"" module resembles the optimal solution with an acceptable error rate. Finally, implementation issues of the DSRS are addressed.Transportation Travel demand management (TDM) Trip reservation Integer programming Neural networks",A travel demand management strategy: The downtown space reservation system,,,,,core
341985691,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds  Sarra Alqahtani, Rose Gamble  Adv. Sci. Technol. Eng. Syst. J. 2(3), 449-459 (2017); View Description Image Segmentation Using Fuzzy Inference System on YCbCr Color Model  Alvaro Anzueto-Rios, Jose Antonio Moreno-Cadenas, Felipe Gómez-Castañeda, Sergio Garduza-Gonzalez  Adv. Sci. Technol. Eng. Syst. J. 2(3), 460-468 (2017); View Description Segmented and Detailed Visualization of Anatomical Structures based on Augmented Reality for Health Education and Knowledge Discovery  Isabel Cristina Siqueira da Silva, Gerson Klein, Denise Munchen Brandão  Adv. Sci. Technol. Eng. Syst. J. 2(3), 469-478 (2017); View Description Intrusion detection in cloud computing based attack patterns and risk assessment  Ben Charhi Youssef, Mannane Nada, Bendriss Elmehdi, Regragui Boubker  Adv. Sci. Technol. Eng. Syst. J. 2(3), 479-484 (2017); View Description Optimal Sizing and Control Strategy of renewable hybrid systems PV-Diesel Generator-Battery: application to the case of Djanet city of Algeria  Adel Yahiaoui, Khelifa Benmansour, Mohamed Tadjine  Adv. Sci. Technol. Eng. Syst. J. 2(3), 485-491 (2017); View Description RFID Antenna Near-field Characterization Using a New 3D Magnetic Field Probe  Kassem Jomaa, Fabien Ndagijimana, Hussam Ayad, Majida Fadlallah, Jalal Jomaah  Adv. Sci. Technol. Eng. Syst. J. 2(3), 492-497 (2017); View Description Design, Fabrication and Testing of a Dual-Range XY Micro-Motion Stage Driven by Voice Coil Actuators  Xavier Herpe, Matthew Dunnigan, Xianwen Kong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 498-504 (2017); View Description Self-Organizing Map based Feature Learning in Bio-Signal Processing  Marwa Farouk Ibrahim Ibrahim, Adel Ali Al-Jumaily  Adv. Sci. Technol. Eng. Syst. J. 2(3), 505-512 (2017); View Description A delay-dependent distributed SMC for stabilization of a networked robotic system exposed to external disturbances  Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 513-519 (2017); View Description Modelization of cognition, activity and motivation as indicators for Interactive Learning Environment  Asmaa Darouich, Faddoul Khoukhi, Khadija Douzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 520-531 (2017); View Description Homemade array of surface coils implementation for small animal magnetic resonance imaging  Fernando Yepes-Calderon, Olivier Beuf  Adv. Sci. Technol. Eng. Syst. J. 2(3), 532-539 (2017); View Description An Encryption Key for Secure Authentication: The Dynamic Solution  Zubayr Khalid, Pritam Paul, Khabbab Zakaria, Himadri Nath Saha  Adv. Sci. Technol. Eng. Syst. J. 2(3), 540-544 (2017); View Description Multi-Domain Virtual Network Embedding with Coordinated Link Mapping  Shuopeng Li, Mohand Yazid Saidi, Ken Chen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 545-552 (2017); View Description Semantic-less Breach Detection of Polymorphic Malware in Federated Cloud",,'ASTES Journal',10.25046/aj020371,,core
478048273,,"US Transportation Collection2021PDFTech ReportWells, JenniferLovelace, BarrittCollins Engineers, Inc.Minnesota. Dept. of Transportation. Office of Policy Analysis, Research & InnovationMinnesota. Dept. of Transportation. Office of Policy Analysis, Research & InnovationMinnesotaUnited StatesBridgesDronesInspectionUnmanned aircraft systemsData miningComputer modelsArtificial intelligenceMN 2021-13(c)1031084Final ReportDrones for bridge inspection research has been completed by MnDOT in multiple phases since 2015. As of summer, 2017, Phase III of this research began using the SenseFly Albris and the Flyability Elios, a collision-tolerant drone more suited to confined spaces such as box girders, culverts, or areas that are difficult to access. Due to the success of this research, MnDOT Metro District purchased the Elios drone to supplement bridge inspection access where space is confined and optimal lane closures are prohibited, which has been an on-going issue in the District due to traffic volumes. This project implements drone inspection for the metro bridge inventory and other similar representative structures by creating an inspection plan that identifies bridges best suited for drone use, what parameters govern drone use in bridge inspection, and how unmanned aircraft systems (UAS) can be integrated into standard inspection operations. The project explores relevant technology, including reality modeling software, drone hardware, artificial intelligence, and autonomous flights. This project also delivers the UAS Safety and Operation Manual specific to the Metro District.114",Unmanned Aircraft Systems (UAS) \u2013 Metro District Bridge Inspection Implementation,,,,,core
90501451,,"In brain imaging, solving learning problems in multi-subjects settings is difficult because of the differences that exist across individuals. Here we introduce a novel classification framework based on group-invariant graphical representations, allowing to overcome the inter-subject variability present in functional magnetic resonance imaging (fMRI) data and to perform multivariate pattern analysis across subjects. Our contribution is twofold: first, we propose an unsupervised representation learning scheme that encodes all relevant characteristics of distributed fMRI patterns into attributed graphs; second, we introduce a custom-designed graph kernel that exploits all these characteristics and makes it possible to perform supervised learning (here, classification) directly in graph space. The well-foundedness of our technique and the robustness of the performance to the parameter setting are demonstrated through inter-subject classification experiments conducted on both artificial data and a real fMRI experiment aimed at characterizing local cortical representations. Our results show that our framework produces accurate inter-subject predictions and that it outperforms a wide range of state-of-the-art vector- and parcel-based classification methods. Moreover, the genericity of our method makes it is easily adaptable to a wide range of potential applications. The dataset used in this study and an implementation of our framework are available at http://dx.doi.org/10.6084/m9.figshare.1086317",Graph-based inter-subject pattern analysis of FMRI data.,,Public Library of Science (PLoS),10.6084/m9.figshare.1086317.,"[{'title': None, 'identifiers': ['issn:1932-6203', '1932-6203']}]",core
291383821,,"The magnetic diagnostics subsystem of the LISA Technology Package (LTP)
on board the LISA PathFinder (LPF) spacecraft includes a set of four tri-axial
fluxgate magnetometers, intended to measure with high precision the magnetic
field at their respective positions. However, their readouts do not provide a
direct measurement of the magnetic field at the positions of the test masses, and
hence an interpolation method must be designed and implemented to obtain the
values of the magnetic field at these positions. However, such an interpolation
process faces serious difficulties. Indeed, the size of the interpolation region is
excessive for a linear interpolation to be reliable while, on the other hand, the
number of magnetometer channels do not provide sufficient data to go beyond
the linear approximation. We describe an alternative method to address this
issue, by means of neural network algorithms. The key point in this approach is
the ability of neural networks to learn from suitable training data representing
the behaviour of the magnetic field. Despite the relatively large distance
between the test masses and the magnetometers, and the insufficient number
of data channels, we find that our artificial neural network algorithm is able
to reduce the estimation errors of the field and gradient down to levels below
10%, a quite satisfactory result. Learning efficiency can be best improved by
making use of data obtained in on-ground measurements prior to mission launch
in all relevant satellite locations and in real operation conditions. Reliable
information on that appears to be essential for a meaningful assessment of
magnetic noise in the LTP.Peer Reviewe",Theory and modelling of the magnetic field measurement in LISA PathFinder,,,,,core
291390850,,"We present a reachability graph-based search optimization tool for scheduling.Motivated by the lack of tool support for optimization of TCPNs.Implements an event-driven timed state space with AI heuristic search algorithms.Aimed at supporting flexible decision making process with algorithm portfolio.Comparative study of nine search algorithms on real system demonstrates tool efficiency. The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.Peer Reviewe",TIMSPAT - Reachability graph search-based optimization tool for colored Petri net-based scheduling,,,,,core
322315502,1985-01-01T08:00:00,"How to design VLSI architectures systematically and how to partition an algorithm for solving it on fixed size VLSI architectures are very important issues in VLSI design. By using proposed approach--space-time domain expansion and the computational model, we can solve both above problems easily. Many examples, such as VLSI implementations of vector inner-product, matrix-vector multiplication, matrix multiplication, convolution, comparison in relational database, Fast Fourier Transformation (FFT), and pattern recognition and image processing algorithms are discussed. This approach can be applied at different levels. This approach has no restriction on the dimensionality of the processing arrays. It can handle multi-directional dataflow and control-flow cases, so can design new architectures. One difficulty in making pattern recognition and image processing systems practically feasible, and hence more popularly used, is the requirement of computer time and storage. For solving this problem, VLSI implementation of PRIP algorithms is very attractive, because of the recursive nature of many PRIP algorithms and high speed, high reliability and low (even no) local memory requirement of VLSI architectures. VLSI implementations of hierarchical scene matching, string-matching, dynamic time-warp pattern-matching and hand-written symbol recognition are proposed. By using extensive pipelining and parallel techniques, the computation can be speeded up greatly. They will find many applications in the areas of pattern recognition, image processing and artificial intelligence, particularly, speech processing, office automation and signature recognition. This will promote the real-time Knowledge processing. The verifications of proposed architectures are also given",Space-time domain expansion approach to VLSI and its application to pattern recognition and image-processing,,'Purdue University (bepress)',,,core
6285086,,"This year the 4th WSEAS International Conference on COMPUTER ENGINEERING and APPLICATIONS (CEA '10) was held at Harvard University, Cambridge, USA, January 27-29, 2010. The conference remains faithful to its original idea of providing a platform to discuss network architecture, network design software, mobile networks and mobile services, digital broadcasting, e-commerce, optical networks, hacking, trojian horses, viruses, worms, spam, information security, standards of information security: aes, ipsec, high-tech crime prevention, real-time operating systems, hardware engineering, supercomputing, artificial intelligence, microprocessors, microcomputers, antennas and radars, lightwave technology, numerical methods for electromagnetics, aerospace systems, atm networks, military communications, cyber-science and cyber-space, mathematical logic and computers, image, video and internet technologies, web-based education, law aspects related to informatics etc. with participants from all over the world, both from academia and from industry.cyber-science and cyber-space; mathematical logic and computers; image; video and internet technologies; web-based education",Recent Advances in Computer Engineering and Applications,,,,,core
42829936,"Oct 1, 1988","Over the years demand on space systems have been increased tremendously and this trend will continue for the near future. The enhanced capabilities of space systems, however, can only be met with increased complexity and sophistication of onboard and ground systems, and artificial intelligence and expert system concepts have a significant role in space applications. Expert systems could facilitate decision making, improved fault diagnosis and repair, enhanced performance and less reliance on ground support. However, some requirements have to be fulfilled before practical use of flight-worthy expert systems for onboard (and ground) operations. This paper discusses some of the characteristics and important considerations in design, development, implementation and use of expert systems for real-life space applications. Further, it describes a typical life cycle of expert system development and its usage",Considerations in development of expert systems for real-time space applications,https://core.ac.uk/download/pdf/42829936.pdf,,,,core
237018740,,"This thesis studies the application of multi-agent learning in complex domains where safety and sustainability are crucial. We target some of the main obstacles in the deployment of multi-agent learning techniques in such domains. These obstacles consist of modelling complex environments with multi-agent interaction, designing robust learning processes and modelling adversarial agents. The main goal of using modern multi-agent learning methods is to improve the effectiveness of behaviour in such domains, and hence increase sustainability and security. This thesis investigates three complex real-world domains: space debris removal, critical domains with risky states and spatial security domains such as illegal rhino poaching. We first tackle the challenge of modelling a complex multi-agent environment. The focus is on the space debris removal problem, which poses a major threat to the sustainability of earth orbit. We develop a high-fidelity space debris simulator that allows us to simulate the future evolution of the space debris environment. Using the data from the simulator we propose a surrogate model, which enables fast evaluation of different strategies chosen by the space actors. We then analyse the dynamics of strategic decision making among multiple space actors, comparing different models of agent interaction: static vs. dynamic and centralised vs. decentralised. The outcome of our work can help future decision makers to design debris removal strategies, and consequently mitigate the threat of space debris. Next, we study how we can design a robust learning process in critical domains with risky states, where destabilisation of local components can lead to severe impact on the whole network. We propose a novel robust operator κ which can be combined with reinforcement learning methods, leading to learning safe policies, mitigating the threat of external attack, or failure in the system. Finally, we investigate the challenge of learning an effective behaviour while facing adversarial attackers in spatial security domains such as illegal rhino poaching. We assume that such attackers can be occasionally observed. Our approach consists of combining Bayesian inference with temporal difference learning, in order to build a model of the attacker behaviour. Our method can effectively use the partial observability of the attacker’s location and approximate the performance of a full observability case. This thesis therefore presents novel methods and tackles several important obstacles in deploying multi-agent learning algorithms in the real-world, which further narrows the reality gap between theoretical models and real-world applications",Multi-Agent Learning for Security and Sustainability,https://core.ac.uk/download/237018740.pdf,,10.17638/03058825,,core
42829617,"JAN 1, 1986","In a software system the size of the Space Station Software Support Environment (SSE), no one software development or implementation methodology is presently powerful enough to provide safe, reliable, maintainable, cost effective real time or near real time software. In an environment that must survive one of the most harsh and long life times, software must be produced that will perform as predicted, from the first time it is executed to the last. Many of the software challenges that will be faced will require strategies borrowed from Artificial Intelligence (AI). AI is the only development area mentioned as an example of a legitimate reason for a waiver from the overall requirement to use the Ada programming language for software development. The limits are defined of the applicability of the Ada language Ada Programming Support Environment (of which the SSE is a special case), and software engineering to AI solutions by describing a scenario that involves many facets of AI methodologies",Artificial intelligence and the space station software support environment,https://core.ac.uk/download/pdf/42829617.pdf,,,,core
78898986,,"This study presents a computational fluid dynamic (CFD) study of Dimethyl Ether (DME) gas adsorptive separation and steam reforming (DME-SR) in a large scale Circulating Fluidized Bed (CFB) reactor. The CFD model is based on Eulerian-Eulerian dispersed flow and solved using commercial software (ANSYS FLUENT). Hydrogen is currently receiving increasing interest as an alternative source of clean energy and has high potential applications, including the transportation sector and power generation. Computational fluid dynamic (CFD) modelling has attracted considerable recognition in the engineering sector consequently leading to using it as a tool for process design and optimisation in many industrial processes. In most cases, these processes are difficult or expensive to conduct in lab scale experiments. The CFD provides a cost effective methodology to gain detailed information up to the microscopic level. The main objectives in this project are to: (i) develop a predictive model using ANSYS FLUENT (CFD) commercial code to simulate the flow hydrodynamics, mass transfer, reactions and heat transfer in a large scale dual fluidized bed system for combined gas separation and steam reforming processes (ii) implement a suitable adsorption models in the CFD code, through a user defined function, to predict selective separation of a gas from a mixture (iii) develop a model for dimethyl ether steam reforming (DME-SR) to predict hydrogen production (iv) carry out detailed parametric analysis in order to establish ideal operating conditions for future industrial application. The project has originated from a real industrial case problem in collaboration with the industrial partner Dow Corning (UK) and jointly funded by the Engineering and Physical Research Council (UK) and Dow Corning. The research examined gas separation by adsorption in a bubbling bed, as part of a dual fluidized bed system. The adsorption process was simulated based on the kinetics derived from the experimental data produced as part of a separate PhD project completed under the same fund. The kinetic model was incorporated in FLUENT CFD tool as a pseudo-first order rate equation; some of the parameters for the pseudo-first order kinetics were obtained using MATLAB. The modelling of the DME adsorption in the designed bubbling bed was performed for the first time in this project and highlights the novelty in the investigations. The simulation results were analysed to provide understanding of the flow hydrodynamic, reactor design and optimum operating condition for efficient separation. Bubbling bed validation by estimation of bed expansion and the solid and gas distribution from simulation agreed well with trends seen in the literatures. Parametric analysis on the adsorption process demonstrated that increasing fluidizing velocity reduced adsorption of DME. This is as a result of reduction in the gas residence time which appears to have much effect compared to the solid residence time. The removal efficiency of DME from the bed was found to be more than 88%. Simulation of the DME-SR in FLUENT CFD was conducted using selected kinetics from literature and implemented in the model using an in-house developed user defined function. The validation of the kinetics was achieved by simulating a case to replicate an experimental study of a laboratory scale bubbling bed by Vicente et al [1]. Good agreement was achieved for the validation of the models, which was then applied in the DME-SR in the large scale riser section of the dual fluidized bed system. This is the first study to use the selected DME-SR kinetics in a circulating fluidized bed (CFB) system and for the geometry size proposed for the project. As a result, the simulation produced the first detailed data on the spatial variation and final gas product in such an industrial scale fluidized bed system. The simulation results provided insight in the flow hydrodynamic, reactor design and optimum operating condition. The solid and gas distribution in the CFB was observed to show good agreement with literatures. The parametric analysis showed that the increase in temperature and steam to DME molar ratio increased the production of hydrogen due to the increased DME conversions, whereas the increase in the space velocity has been found to have an adverse effect. Increasing temperature between 200 oC to 350 oC increased DME conversion from 47% to 99% while hydrogen yield increased substantially from 11% to 100%. The CO2 selectivity decreased from 100% to 91% due to the water gas shift reaction favouring CO at higher temperatures. The higher conversions observed as the temperature increased was reflected on the quantity of unreacted DME and methanol concentrations in the product gas, where both decreased to very low values of 0.27 mol% and 0.46 mol% respectively at 350 °C. Increasing the steam to DME molar ratio from 4 to 7.68 increased the DME conversion from 69% to 87%, while the hydrogen yield increased from 40% to 59%. The CO2 selectivity decreased from 100% to 97%. The decrease in the space velocity from 37104 ml/g/h to 15394 ml/g/h increased the DME conversion from 87% to 100% while increasing the hydrogen yield from 59% to 87%. The parametric analysis suggests an operating condition for maximum hydrogen yield is in the region of 300 oC temperatures and Steam/DME molar ratio of 5. The analysis of the industrial sponsor’s case for the given flow and composition of the gas to be treated suggests that 88% of DME can be adsorbed from the bubbling and consequently producing 224.4t/y of hydrogen in the riser section of the dual fluidized bed system. The process also produces 1458.4t/y of CO2 and 127.9t/y of CO as part of the product gas. The developed models and parametric analysis carried out in this study provided essential guideline for future design of DME-SR at industrial level and in particular this work has been of tremendous importance for the industrial collaborator in order to draw conclusions and plan for future potential implementation of the process at an industrial scale",Computationsl modelling of dimethyl ether separation and steam reforming in fluidized bed reactors,https://core.ac.uk/download/78898986.pdf,,10.48780/publications.aston.ac.uk.00028743,,core
96575110,,"This research explores a newfangled approach for the notion that architecture is

no longer concerned with the organization of space and matter, but rather a

system of systems with self-organizational behavior and progressively complex

programmatic interventions. Such system represents an Internet of Things (IoT)

paradigm that has the capacity to self-organize a pre-defined architecture

acclimating dynamically to the demands of the environment through a process of

self-awareness and re-configurability.

This paper is an attempt to develop spaces that bring computation into the

physical world by introducing artificial intelligence into building systems so as to

communicate, exchange information, and allow right responses and decisions

within a sustainable manner. Environments with embedded computational

systems and adaptive reconfiguration behavior will be precisely studied.

With an intensive focus on smart, self reconfiguring architecture that has

embraced kinetic motion as an approach for environmental adaptation and

responsiveness. This study addresses the networked organization of sensory

systems that incorporate computational platforms in relation to user’s desires,

where architecture turns into an interactive intermediary between human and

computation.

Correspondingly, the authors are particularly aiming to propose three conceptual

frameworks for delivering a smart system at the architectural scale which is

capable of reconfiguring and interacting constantly in real time, precisely

focusing on an initial implementation for the first framework utilizing an

experimental customized prototype, while the other two frameworks would be

posteriorly studied through future work.

Thus the IoT will foster a rapid development in intelligent systems which would

directly introduce an advanced technological leap forward in accommodating a

new way of demonstrating human-computer interaction within a novel

architectural design approach",A self-aware paradigm for autonomous architectural systems within the Internet of Things,https://core.ac.uk/download/96575110.pdf,'Informa UK Limited',10.1080/17445760.2017.1390098,,core
390096842,,"This paper proposes a novel unsupervised metric learning approach to detect anomalous/novel objects. Existing object detection approaches either cannot detect novel categories or require human annotations even in a small scale (such as few-shot learning). To overcome this, especially for robotic applications where human annotations are not available, this project leverages unsupervised representation learning and unsupervised metric learning to discover feature prototypes of unknown fine-grained categories i.e. clusters in the low-dimensional embedding space. Specifically, the proposed approach leverages deep clustering and self-reconstruction to learn feature prototypes for normal objects. More importantly, we interpolate the latent features and generate pseudo anomalous examples to learn the embedding space of good compactness and sparseness to learn a discriminative embedding space, facilitating distinguishing anomalous examples from normal

ones. The learned prototypes can be further used to infer the

probability of novel objects using the metric distance to the prototypes. The proposed unsupervised learning approach is

also integrated with a Region Proposal Network as a detection

pipeline and real-time detection is achieved. This paper uses the StreetHazards dataset of CAOS benchmark for training and evaluation and comparison experiments are implemented to demonstrate the effectiveness of the proposed approach",Zero-shot anomalous object detection using unsupervised metric learning,,,,,core
235571910,1990-11-29T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH TRADING COMPANY
STORE NEWS PHONES: OFFICE 758-3033 GROCERIES 758-3535 HARDWARE 758-3065 UPSTAIRS & STORE NEWS 758-6377
STORE HOURS: Monday, Tuesday, Wednesday, Thursday, Friday, Saturday.......................8 a.m. to'6 p.m.
THURSDAY, NOVEMBER 29, 1990 ................. MAGRATH, ALBERTA
************************************»***>[**********************************
HARDWARE DEPARTMENT
************************************* Î- ^+*********************************
BEAT THE
CHRISTMAS RUSH
Card
REGULAR
$89.99
Pack. REG. $12
a good selection of Christinas Cards
1
G.E 4 event, 14 day timer.
Remote control. CAOfIQR
Speed scan stop
action
CHRISTMAS CARDS
have
Card
Regular
36 Card
Regular
40 Card
Regular
Pack.
$4.99 - Christinas Special
Pack.
$6.99. CHRISTMAS SPECIAL
Pack
$9.99. CHRISTMAS SPECIAL
fresh cut
Home-Line 1 cu. ft. Multiwave
Microwave. White, Almond
REG. $299.99 ËidQ99
SAVE $50.00 _____
499
& AM/FM
W RADIO
Old Fashioned AM/FM
Radio set in woodtone
cabinet.
platters
Christmas Turkey
Platters. Large
size. REGULAR $17.99
HH CHRISTMAS
SPECIAL
Our Christmas Trees
will be in Monday,
December 3rd. The
same good quality
trees at our usual
low prices.
HOME-LINE
MICROWAVE OVEN
a
Home-Line Classi
Microwave.
REGULAR $189.99
CHRISTMAS SPECIAL ....
WE HAVE A GOOD SELECTION OF MICROWAVE STANDS
We received the following news
items from Dr. Steele Brewerton about his
sons Byrne and Courtney.
Byrne Rhys Brewerton earned a Master’s
Degree in English with honors from the Un­iversity
of Texas, San Antonio (1990). He
is teaching in the American Culture and
Language progam, Office of Extended Ed­ucation
at the California State Univer­sity
in San Bernardino, California since
September 1990. He lives at 1030 North­park
Blvd., San Bernardino, Calif. 92407,
Phone (714)882-0510 with his wife Carol,
son Michael and two daughters Christabel
and Marta.
Captain Cortney Steele Brewerton,
USAF - Two months after getting posted
back to U.S.A, after a four year tour of
duty in Great Britain, Captain Brewerton
was deployed with his unit to Saudi Arabia
on August 19/90.
Courtney and wife Alissa's address
is Box 1000, England Airforce Base,
Alexandria, La. 71311.
The boys would love to hear from
their old neighbors and friends.
Steele and Lauree send greetings to
all in Magrath.
MAGRATH RODS GUN CLUB TURKEY BINGOS
Magrath Rod & Gun Club will hold their
annual Christmas Turkey Bingos on the
following Mondays and Fridays in Dec­ember:
December 3,7,10 and 14th.
They will be held at 7 p.m. each night
in the Magrath Trading Company Hall.
MAGRATH ROD & GUN CLUB TURKEY SHOOT
will open in the Magrath Trading
Company Hall Monday December 3rd and
continue through to December 22nd.
LOST: 1 pair of Ballet Slippers in red
draw string bag. Finder please call
758-6338.
Ladies7 Dresses
THIS WEEKEND SALE
FRIDAY & SATURDAY, NOVEMBER
30th & DECEMBER 1st.
2 RACKS OF LADIES DRESSES-IN
A VARIETY OF STYLES,
FABRICS, COLORS AND SIZES.
YOUR
CHOICE EA
Books
We have a good select­ion
of L.D.S. Books -
Pictures - Cassette Tapes
(Music & Spoken Word)
for Christmas Gift
giving.
MAGRATH Bantams played 2 games during the last
week. An exhibition game against Fort Macleod with
Magrath winning 17-1. A regular scheduled league
game was played against Brooks when Magrath clobb­ered
Brooks No. team by a 15-2 score.
Scoring for Magrath: Marc Heggie (5-1) Daniel
Baker (3) Jason Ripley (2-4) Tom Bingham (2) John
Ripley (l-2_ Robert Brandt (1-1) Ken Wright (1)
Tyler Harris 2 assists, Trevor Dunlop 1 assist.
Magrath displayed commendable restraint in
both games due to the ""chippy” plays from the
opposition. Keep up the good work gentlemen.
FOR SALE: 1989 Chev Silverado 4x4. Extended Cab,
short box. Air, tilt, cruise, power windows, locks,
48,000 km. $16,750 Truck only; $17,250 with ""Cougar""
cab. Please call Ralph, 758-6682.
FOR RENT: Office Space. Phone 758­3418.
We are excited and proud to announce
the graduation of Cst. Ronald Scott
Beck from the Royal Canadian Mounted
Police. He was sent to Montreal in Nov­ember
1989 for 6 months of training
in the R.C.M.P. Academy in Regina,
Sask. The weekend graduation exer­cises
were very impressive, a once-in-
a lifetime experience.
Those who travelled to Regina for
the graduation were Ron & Kathy Beck
and his brothers Wes and Gavin. Also
Arthur, Diane & Joan Wocknitz; Earl
& Elsie Wocknitz and Mrs. Mary Malec.
Scott and his wife Janice (nee
Wocknitz) will be moving to Colonsay,
Sask, in early December. Colonsay is
a small town of 500 people, 35 miles
southeast of Saskatoon.
We would like to thank everyone
who made the long trip to Regina to
share in this special day.
Cst. Scott & Janice Beck.
YOUR____C 0 M M U N I T Y'‘S__ FUTURE
SECO _N_D____MEETING
cONE_AND BRING A_FRIEND
PLACE: Magrath Senior's Centre
DATE & TIME"" Wednesday, December 5th, 1990
7:00 p.m.
REASON: SO THIS TOWN WILL DEVELOP WITH
YOUR INPUT!!
THIS IS YOUR CHANCE TO QUIT ""JUST COMPLAIN­ING""
..................AND HAVE A REAL EFFECT.
SEE YOU THERE
7:00 P.M. - WEDNESDAY, DECEMBER %, 1990.
WANTED: Nintendo games. Phone 758-3418.
Students home during the American Thanksgiv­ing
holiday included Rosealee Grusendorf, Karla
Jones, Billie Jo Whitt, Laura Jane Wright, Toby
Barnett? Trina Kerr.
Jennifer Kerr was home from Edmonton.
FOR CHRISTMAS
LOUNGERS
WEAR
PRICED
FROM
PRICED
FROM
SLEEPWEAR
Nightgowns in a variety
of styles and fabrics in­cluding
Brushed Nylon,
Nylon tricot, Polyester
Cotton and Rayon. Select
La short gown or long
S& length.
SLACKS
A nice selection of Slacks in sizes
from 7 to 44. **
Assorted colors 5
& styles. From UP_______
HEADWEAR
MITTS & GLOVES
We have just received a shipment
of those popular stretch
Gloves & Mitts. Also
headbands. Snazzy colors M
1 ■
// r
[■
r 7 V
k
V
Flannelettes
New Flannelettes have just
arrived including Rose and
Red. Just in time for your
Christmas sewing.
A restaurant enterprise in Magrath requires
a temporary Supervisor from January 2, 1991 to
work 38 hours per week evenings from 4:00 p.m.
on, and all day Saturday from 12 noon. Rate of
pay is $1,410.00 per month. Allpy with resume
to Box 499, Magrath before December 14, 1990.
PRICED FROM
A restaurant enterprise in Magrath has
a temporary position available for a Mark­eting
Person to conduct market surveys and
product sampling. This position is avail­able
January 2, 1991 for 1 month involving
approximately 6 hours per day 5 days per
week. Rate of pay is $8.57 per hour.
Apply with resume to Box 499, Magrath
before December 14, 1990.
CHRISTMAS
FELT
New Felt Squares in
Red, White & Green.
79* PRICED AT > ■ ea
Christmas
Knitting Yarns
Christmas Yarns in White, Red,
Green and variagated. Just
in time for your
Christmas crafts. & UP
""JUST FOR YOU HAIRSTYLES"" 5th ANNUAL
CHRISTMAS PERM SPECIAL - November 20th
to December 15th. $35.95 includes cut.
Seniors - $31.95 includes cut. Long
Hair extra. Book now so you're looking
great for the Festive Holiday Season.
Phone 758-6350.
Charlotte Hofer of Del Bonita will
be joining Laurie at ""Just For You Hair­styles""
Thursdays, Fridays and Satur­days.
Charlotte has 4 years experience
in hair styling. She invites her friends
and former customers to drop in to ""Just
For You’/
AVON OPEN HOUSE - and Stock Reduction
Sale - November 30th and December 1
from 9 a.m. to 8 p.m. at the Doug &
Betty Loose farm (2 miles east of the
cemetery and 1 mile north). Still
time to order for Christmas.
CAKES BY RITA: For Christmas I will be
doing decorated Gingerbread cookies -
4"" ones 50c each, 7"" ones $1 each.
Please order early as I am leaving
December 11th to spend /Christmas with
one of my daughters. 1 have cutters
for Christmas trees, Santa Claus,
Snowmen, Bears, Gingerbread boys &
girls, Rocking horses, etc. Christmassy
cake pants I have are Wee Angel, Santa,
Rudolph, Bells, Christmas Tree, Candle,
Noel, Toy Soldier, Holiday House and
Nativity Scene. I would do these
cakes as near to my leaving time as
possible, they freeze very well. For
more information phone Rita Olson,
758-6315.
GARDSTON CARPET CLEANERS - has now
moved to Magrath,, For your Carpet and
Upholstery Shampooing call: 758-6525.
Free estimates. Guaranteed satisfact­ion.
ANNE CAMPBELL SINGERS 22nd ANNUAL
SINGING TREE ""An Old Fashioned
Christmas"" featuring Southminster
Mini Choir, Southminster Junior Girls
Choir, Anne Campbell Singers, Linnet
Singers. Directed by Dr. Anne Camp­bell,
Shannon Little. Guest Artist
Dale Ketcheson, guitarist. Southmins­ter
Church Sanctuary, Saturday, Dec­ember
1 at 7:00 p.m. Sunday, December
2 at 3:00 p.m. Reception following
Sunday performace. Admission: Adults
$5.00; Ages 12 & under $3.00. Tickets
available from choir members, at the
church office, Leisters Music or at
the door.
HOLLAND INSURANCE (Magrath) Office
will be closed for Christmas holidays
on December 24,25 & 26 and also on
New Year's Day, January 1, 1991.
MOBILE DENTURE CLINIC - George Torre­Alba
will have his Mobile Denture
Clinic at Diamond Willow Terrace,
FOR RENT: 1 bedroom suite at Four
Plex. $325/month plus electricity
Phone 758-3107 or 758-6212.
NON STICK
like tins.
MUFFIN TINS - fibreglass-
6 muffin size. $1.19
Hardware Dept.
Monday, December 10th at 6 p.m. For
further information - Mobile Phone
1-554-5017.
Few men can fool their wives as easily
as they did their mothers.
ORANGES
PRODUCE
■ r n ■ U lf
SATSUMA, 8 lb
APPLES SPARTAN
LETTUCE .................................................
Tomatoes ....................... 88*
Bagged Onions 3 lb. bags
****************************************************************************
CHINOOK_HEALTH_UNIT_NEWS
ALLERGY_ALERT_Z_WALNUTS - ""Host­ess""
Traditional Nanaimo Bars.
The Health Protection Branch of
Health and Welfare Canada is ad­vising
consumers allergic to wal­nuts
not to eat ""Hostess"" Tradi­tional
Nanaimo Bars sold in re­tail
outlets as a snack item and
distributed by Weston Bakeries
Ltd. The product, of which app­roximately
120 were sold, is
packaged in 300 gram plastic
trays and was distributed in
Saskatchewan and Alberta.
The Health Protection Branch
investigation indicates that wal­nuts
were found mixed in the pro­duct
and that the product labels
do not list walnuts as an ingred­ient.
This product is consider­ed
a health risk ONLY for those
individuals who have an allergy
t o walnut s .
Consumers may return this
product to the point of purchase.
The Health Protection Branch
is monitoring the effectiveness
of the company voluntary recall.
For more information call
the Environmental Health Officer
at your local Chinook Health
Unit.
We have a nice assortment of
Gloves, Mitts, Headbands, Ear
Muffs & Dickies - Upstairs.
Members of the John & Myrna
Cook family home during the weekend
were Shannon & Brad Sabey and fam­ily
of Alix, AB, Lana & Craig Hansen
and Erin and Jennifer Cook of Provo,
Utah.
They attended the 75th birthday
celebration of Grandmother Fern
Cook Moors held in the Seniors'
Centre Saturday, November 24th.
MAGRATH_AT0M_""B2_NEWS
On Saturday, November 24/90, during an
exhibition game at our local rink, the Mag-rath
Mounties suffered a disappointing 8-0
loss to the Lethbridge Atom ""B'.' The action
was fast paced and a little rough ( a total
of 8 penalties). Our guys played with a
lot of heart, but just ran out of.steam dur­ing
the second period. The absence of Mich­ael
Balderson and Jeremy Bree was felt. Roy
Kapcsos made his debut in net doing an exc­eptional
job against the Lethbridge sharp­shooters
with 21 saves. Better luck next
week boys.
ON THE SIDELINES: Pre-game Coach Kim Chris­tensen
telling a slightly talkative Kurt
Robinson that if he really hurried he may
be ready for next Wednesday s practise.
Hockey Moms Deb Robinson and Joy Christen­sen
making their time-box debut and doing
an outstanding job once all the ""wrinkles
were ironed out.
AGRICULTURAL-SOCIETY annual meeting
All Ag. Society Members are invited to
attend the Annual Meeting Friday, December
7th at 7:30 p.m. at the Arena. All assoc­iated
groups and their members such as the
Rodeo Club, Roping Club, Fair Committee,
Garden Club, Riding Club, etc. should also
be represented.
Directions for next year's program
will be discussed and Elections for the
next year's Board of Directors will also
be held.
ATTENTION: Am looking for someone to
play Country Music - as a past time,
week nights. If interested call Perry
at 758-6481.
An oldster on the highway was bewildered
by the missile-like speeds of all the other cars.
He drove at his usual rate, slower than the min­imum.
A police cruiser soon overtook him. ""I
suppose you know why I stopped you, the police­man
said. ""Sure do,"" the old timer replied. ""I'm
the only one you could catch.""
MAGRATH_MINOR_HOCKEY PEE_WEE_2.B2 The Magrath Maver­icks played the Taber Blackhawks Saturday, November 17/90. The Mavericks 5, the Blackhawks 3. lst_Periodj_ Matt Johnson (18:26) unassisted. Bart Bullock (6:44) assist to Tommy Lee.
2nd Period: Tyler Ferguson (19:15) assist to Arlin Silverman. Matt Johnson (7:29) assists to Kinn Wetherelt and Terry Anderson. 3rd_Period£ Matt Johnson (11:10) unassisted.
The win was a good example of team effort and good coaching. Thanks to the three refrees who did an excellent job of keeping this game fair and under control.
S.R.
MAG^xTH_GENERAL HOSPITAL NEWS
We would like to let the Community know how much we appreciate people tak­ing non-emergency situations to the doc­tor's clinics; and we appreciate that the doctors have been willing to see all non-emergency patients at their clinics.
Also, we would like to remind you that we are still open for emergency situations to be brought to the hospital.
At the Magrath General Hospital we were facing a deficit of 12%. We have now reduced that deficit to 5% and will continue to work at reducing this through the co-operation of all staff and the community. We also appreciate all dona­tions that have been made to the hospital Stuart B. Norton, Administrator.
On Saturday November 24th, Magrath Novice played a League game at home against Kainai and lost 12-6. Scoring for Magrath were: Adam Mehew (3-1) Tanner Godionton 2, Mason Dahl 1. Dustin Ririe and Rhonda Strafe 1 assist each. Brian Bullock was in net.
That same afternoon Novice travelled to Lethbridge for an Exhibition game and played a strong Lethbridge team losing 13-3. Goal getters were: Adam Mehew (1) Tanner Godionton (1-1) Mason Dahl (1). Dustin Ririe had 1 assist. Darrin Harding played in goal.
controll-the
FOR SALE: Sega Genesis with
er and 5 games. $200. Ph. 758-3771.
BAKE SALE: Friends of the Magrath Public Library Foundation will hold a Bake Sale, Saturday, December 8th, 10 a.m. to 2 p.m. in the Magrath Trading Company Hall. All donations will be gratefully accepted. Please call a member of the Executive for more information - Ann Fazikos, 758­6425, Laural Bennett, 758-6222 or Marie Stevenson, 758-3540. This is the first Fund Raiser of the Organ­ization and your help will be grate­fully accepted.
SASH's Gommerical Kitchen is taking orders for Christmas baking. We would be pleased to make your mince­meat pies and Christmas cakes on a pre-order basis. Please call 758­3160 to place your orders or drop into our facility on Main Street.
SASHAS VOCATIONAL SERVICES PROGRAM will make up your party trays with Christmas season. Please place orders specifying type of tray re­quired at least 4 days in advance. Call 758-3160, 758-3334 or place
your order at SASH's office on Main Street. We are sorry but we cannot offer trays which include fruit .
Beautiful warm fall weather was the order of the day for the wedding of Debi Bly and Delin Watmough in the Idaho Falls Temple on October 31, 1990. Close
family members who made the journey south to witness the happy occasion were par­ents Rodney and Jacquie Bly, Lawrence and Kala Watmought; grandparents Roy and Ollie Ericksen; brothers and sisters Kathy, Amy, Heather and John Bly; trevor and Stace Watmough .
Following a session of picture tak­ing in the beautiful fall sunshine, a family dinner was held at the Westbank Inn. Dinner and reception for the new Mr. and Mrs. Watmough were held at the Magrath Stake Center on Saturday, Nov­ember 3rd. The bride, attended by her sisters Kathy, Amy and Heather, wore a Victorian wedding dress complimented by groom's western style tuxedo.
The groom's attendants were his
brothers Trevor and Stacey and the bride's brother John.
A short program followed the rec­eption with Charles Bly as Master of Ceremonies and numbers provided by friends and relatives.
Debi and Delin have taken up resi­dence in Lethbridge where they are both employed.
ATTENTION: For Christmas House Cleaning call Audra Foggin, Ph. 758-6832.
I am providing a Day Home for your children's care. My home is Government approved and you can receive a Govern­ment subsidy. I will watch your child­ren full or part time and on a shift work basis. For an interview with me, please call me, Kimberlee, 758-6367.
CHS TMAS _ C AR E - A- V AN
Once again ""Just For You Hairstyles will be collecting Toys and non perish­able Food Items, etc. for those less fortunate residents of the area. This is in association with 1570 Radio and the Salvation Army.
Deadline date to have items dropped off at ""Just For You Hairstyles"" is December 12th.
To ruin the day for a grouch, smile at him.Cmeat~>
Grade A Turkeys 3-8 kg............................................ 4 39 lb JOGrG
Beef Short Ribs
PORK
SIDERIBS
1“
1“
qsi
Vkg
439 » kg
ACON MAYFAIR 550 g
COOKED HAN MAYFAIR 375 g ............................. 3«9
Sandwich Meats SCHNEIDERS ""JUST FOR KIDS"" LUNCHEON MEATS, 125 g.................. 119
**************************************************************************
GALBRAITH TOURS LT IK - presents Florida
and the Bahamas. Air, Land and Cruise -
February 7th to February 16, 1991.
Anchors Away!! Call Evelyn at 758-3640.
_NQZA.s_
SENIORS - Friday, November 30th is our
Christmas Pot Luck dinner. Come and
start the Christmas celebrations early.
Dinner at 12 noon.
CHRISTMAS CRAFT SALE & ""NEW TO YOU""
Come, get your Stocking Stuffers and
like-new stuffed toys, etc. for your
children at affordable prices. See
you at the Magrath Trading Company
Hall, Saturday, December 1, 9 to 5.
MAGRATH GENERAL HOSPITAL 1st ANNUAL
BAKE & CRAFT SALE - will be held in
the Hospital lobby Saturday, December
1st from 9 a.m. to 4 p.m. Everyone
is invited to attend.
EARL MALDANER CONSTRUCTION - Finish
carpentry, Renovations, Additions,
Farm buildings. Phone 752-4913,
Box 504, Raymond, AB. TOK 2S0.
THANK YOU: I would like to thank the
Doctors, Nurses and Staff in the Mag­rath
and Lethbridge hospitals for the
care I received while a patient. My
thanks also to my visitors. I app­reciated
it. Evelyn Bourne.
B & R CONSTRUCTION
Your complete Home Specialists.
Custom homes, additions, remodelling,
cabinetry, tiling, flooring, land­scaping,
shingling and concrete work.
Affordable and reliable. Please give
Rod Brandham a call at 758-6812.
MAGRATH KINDERGARTEN-The
Kindergarten needs 30 tin cans
with plastic lids - Koolaid, Slim
Fast, etc. - by the end of November.
If you have any please call Janet Voth,
758-6273.
Magrath Hospital Auxiliary Dec­ember
Meeting will be held Thursday,
December 13th at 12:30 p.m. at the
home of Alma Whitt. This is the
Christmas Pot Luck dinner meeting.
MAGRATH LDS 4th WARD CHRISTMAS PARTY
DINNER THEATRE will be held Thursday,
December 6th. Supper at 6:30 p.m.
The play ""The Rented Christmas"" at
7:30. Everyone welcome.
MAGRATH U.C.W. ANNUAL CHRISTMAS POT­LUCK
DINNER - will be held Monday,
December 3rd at 7 p.m. in the Church
Hall. $3.00 Gift exchange item.
Please remember your Secret Pal
Christmas cards and reveal your names.
Husbands are invited for K.P. duties
and supper.
FOR SALE: Brand new 1200 gallon fiber­glass
Water Tank. Offers. Phone 758­6288
.
FOR APPLIANCE REPAIRS - G.E., Maytag,
Admiral, Whirl Pool, Inglis, Moffat,
etc. (no Kenmore) please call Tai at
752-3866.
THERAPY
Patient: ""My conscience has been
troubling me.""
Psychiatrist: ""So now you want me to
strengthen your willpower?""
Patient: ""No - weaken my conscience.
ICE CREAM •“ 2 litre ¿99
Yogurt ALPHA 175 g
Bread «. ». e.......
Peanut Butter > kg
59°
389
COFFEE W. F. 300 g
Mushroom Soup w,f‘ gream of mushr°°m °r chicken n°°dle 284 mi
J99
59*
APPLE JUICE DAIRY MAID 1 litre
Drinks KOOL AID KOOLERS #?@%) ML....
Cheer ULTRA CONCENTRATE 4 litre .
Dishwasher Powder ELECTRASOL 1.4 Kg
GARBAGE BAGS GLAD 10 pk 249
PAPER TOWELS
Bathroom Tissue
W.F. 2 pk
W.F. 8 pk
Cheez Whiz KRAFT 1 kg
CHEESE SLICES KRAFT 1 kg
299
549
ß«
FUN FRUITS SUNKIST 153 g 259
Meat Pie 99c
Ice Cream WEST BEST 4 litre
W. F.
SUNKIST
39","Magrath Store News (November 29, 1990)",,J. A. Ririe,,,core
235571283,1980-09-18T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH TR ADING CO.
STORE NEWS OFFICE 758-3033 GROCERIES 758-357S nnv mnna -vrq non _______
STM HOOTS,
THURSDAY. SEPTEMBER 18th, 1980 .............i
*********************************^ot****Tfilill,, h^*************^*****
.LADTee ready-to-wear
******************* * ** ** w * * * T-w* * * * * * * ******** * **** * **** * **** * ** * ****** * ** *
Foamtread Slippers
SEE OUR SELECTION
OF MATERNITY WEAR
INCLUDING DRESSES
& JUMPERS TOO.
End of Summer
Sale F
Ladies one aqd two piece Dresses in the
very latest fabrics $nd up-to-the minute fashions.
Now clearing during these last
days of Summer. 1/ RRIA»
MORE HAVE BEEN ADDED TO THE RACK yO P|||Rr
REAL BARGAINS ON THIS SALE RACK «2G
1/3
COSTUME
JEWELLERY
Maternity
Wear 1 Maternity Jeans,
and Slacks for
""Mother-to-Be""
A choice of styles
and colors.
Flattering Chains
in Gold and Silver
so popular this
Season to wear with
your Ensembles.
Available in 15"",
and 18"" lengths.
Ideal gift
items too.
PRICED
Ladies Houseslippers
in Enchantment and
Chatelaine styles.
Very comfortable to
wear around the house
as well as when you
are relaxing.
FAU. Gardening
85 litres
WHILE THEY LAST
A_NEW CONCEPT IN WHEELBARROWS
deal for moving loads over lawns, soft ground,
sand and gravel. Features include seam-ite?
X Tray With a caPacity of approx­imately
3 cu. ft. All steel ""X"" frame
leg assembly.
See on display in our Hardware window.
PRICED AT ..
ERIE
purpose WHEELBARROW
General Purpose Wheelbarrows with
tubular steel frame. Has drawn seamless
tray, one piece tubular steel frame, square
se design. x"" frame leg assemblv. Roller­hearing
wheel, dust sealed. Grease' fitting
for easy lubrication. y
X3PcuITft 124 litres (4*4 cu- - Wet
Spring of 1981 Price is over $100.00.
, e M.T.D, WHEELBARROW
j.b cu. ft. with seamless leak-proof
Jra£' ^4' "" 360' rolled edge, clioped
?-° bandies. 4,'x8"" Wheel with ball
bearings. Pneumatic two ply 1G""x4""
tire. ’
Save the $5.00 Assembly Price
We have several put together and’
ready to go at the Knock Down Price
1981 Spring Price $79.95
SAVE NOW AT
Garaant utility Wheelbarrow
A compact model
for small jobs.
Metal handles &
seamless tray.
Unassembled.
(The last of the
$25.00 Wheelbarrow)
Garden tools
LEAF & LAWN RAKES SHOVELS
HOLLOW BACK CONSTRUCTION
light digging. A qual-
$9.95
ELORAL SHOVEL : Light-weighti
Blade size is
6 x8V. Just right for
digging gladioli bulbs,
etc' $3.09
BOW RAKE: 14 teeth,
forged steel one piece
head’ $8.09
^ALL_SIZE_GARDEN_SHOVEL : For lady
gardeners and n ~ J
ity shovel.
COMET BRUME RAKE: Efficient
Sweeping action to pick up small­est
-jrass clippings, leaves, etc.
PRICED AT.......... $5,98
¿PRINS-BRACED RAKE.’ Sturdiest on the
marker. 22 temoeied steel teeth
angled for 2"" raking depth.
elastic lawelrake : Light -sweep feel
bamboo, flexibilitv of sorinn
Men's work shirts
Men's G.W.G. Twill Work
Shirts in popular Khaki
or Brown shades. Never
press.
REGULAR $16.95
13”
WE HAVE A GOOD STOCK
OF FALL & WINTER CAPS
IN WOOLS, VELOURS AND
TWILLS.
""PATIO"" SALE: at the home
of Doris & Arvid Larson,
Space 28, North Trailer .
Court - Clothes, Household
goods, decorations & items
too numerous to mention.
Come & get in on the bargains
Saturday, Sept. 27th, 2-5
Doris Larson, Olive
’Train & Norma Arnold.
p .m
YOU: Our sincere
to the Fire Department
and Neighbors for
THANK
thanks
Friends
their prompt response in
helping us to put
fire last Tuesday
The Dan. Atwood
out our
af ternoon.
Family.
FOR SALE: 2 bedroom house on treed
SPORT JACKETS
CORDUROY SPORT BLAZERS
Two popular shades - Beige
or Chocolate Brown. Sizes
left include 36, 38, 42.
REG. $89.00
Clearing At
1 RACK OF MEN'S TIES
IN ALL THE POPULAR
COLORS, PATTERNS AND
PLAINS.
men’s ties
1 acre lot. Phone 758-3556.
CLEARING YOUR GARDEN? If you've got
garden wastes you’d like to get rid
of, give us a call. You stack 'em,
we'll pack 'em away. Ph. 758-6239
Another Harris Family Service.
LESSONS IN ARTEX PAINTING - by
June Kolasko, Thursdays 1:30 - 3:30
p.m. Drop In Centre. Everyone wel­come
.
NOTICE: Anyone interested in taking
Beginner’s Classical Guitar Lessons
contact Derek Hansen. Ph. 758-6851.
WANTED: Ride to Salt Lake at Con­ference
time. Rose Gurney, Paone
758-3015.
ATTENTION: Anyone interested in
singing in a Community Christmas
Contata contact Rose Gurney or
Mary Baker.
WANTED: Set of Bunk beds. Phone
758-3752.
WANTED TO RENT: 2 or 3 bedroom house,
in Magrath. R. Emmelcamp. Phone
345-4771, Collect.
DRAWERS
Men's Cotton knit
and Thermal knit
Drawers for Winter.
Get your winter
supply now while we
have a full size
range in stock.
$6.95 - $8.95
***********
Magrath Trading Co: Thank you for donat­ing
the trophy for the Jr. Western Equit­ation
Class in the Horse Show. I was
reel happy to win this class because
there was some real tough competition.
Thanks again. Sincerely, Jess Hartung.
WANTED: Caretaker for Raquet Ball Court
and Dressing Rooms. Contact Town Office.
WANTED: Someone to teach Accordian
or Guitar lessons. Ph. 758-6750.
FOR SALE: 1977 Leisure Craft Tent
Trailer. Sleeps 6. Stove, tank, ice
box electrical. Excellent condition.
Ph. 758-6369.'
WANTED TO BUY: Child's Playpen and
crib. Must be in good condition.
Phone 758-3154. Jacque Ac k'r o y d .
WORK NEEDED: A 14 year old boy will­ing
to do odd jobs. Saving for band.
Call Larry Webster, 758-3424. Thank
you. ?
WANTED: Used Metranome in good cond­ition.
Ph. 758-3057.
NOTICE: I will clear up your gardens
for you. Ph. 758-3057.
NOTICE: A book of Mystery, a book
of Living, a book of Life after
Living. Come study the Bible and
related Gospels. Ph. 758-6288.
There are bigger things in life
than money. Bills, for instance.
Fabric 1
r
V
New Photo Albums
are now in. Will
take refills. Get
your pictures sorted
PHOTO ALBUMS
(REGULAR $6.95)
LIGHT BLUE DENIM
Ideal for making your
Sport-Outfits, Skirts
Jeans, as well as for
Kiddies wear.60"" wide
REGULAR $6.95
395
Cachet Gift Set ■
5 PIECE GIFT SETS
Cachet or Wind Song
Bubble Bath, Lotion,
After Shower Cologne,
Spray Cologne, Soap,
heat plastic pouch.
in
REGULAR $15.50 9.95
Toys for Toddlers
MUSICAL TOY
Bells or Hearts. Pull
cord to wind up. Nursery
songs.
REGULAR $6.99
Priscilla
more private
*****************
Brockway will take a few
voice or piano students
if lessons can be scheduled on Mondays
during school hours. Adults welcome!
ATTENTION: Community Women’s Chorus
meets Thursday at 7 p.m. in the High
School Band Room. New members will
be taken at this' time.
NOTICE: Community Band will meet
Thursday at 8 p.m. in the High School
Band Room. For information phone Steve
Brockway, 758-6402, or merely show up
at the Band Room for the practise
Thursday evening.
Will babtsit in my home Monday - Fri­day.
Ph. 758-3154. Jacque Ackroyd
MAGRATH GH SCHOOL F0.0TBAEL.1 SCHEDHLE
WadnMday. Oct. • .
Magrath Zeniths at Raymond
Comets •
Thursday, Oct a .
Cardatori Cougars, at Coaldale
Spartana ' '
Friday, Oct 10
St. Mary's Warriors at Claresholm
Cobras
Bye—Vulcan Golden Hawks '
Friday, Oct. 17 '
Claresholm Cobras at Cardston
Cougars
Vulcan Golden Hawks at Raymond
Comets
Coaldale'Spartans at Magrath
Zeniths
Bye—St. Mary's Warriors
Saturday, Oct. 25 '
Playoffs Begin
(Game time 1 p,m.)
Team.No. 2 vs Team No. 7
Team No. 3 vs Team No. 6
Team No. 4 vs Team No 5
Bye—First place team
Saturday, Nov. 1
Semi-Finals
(Game Timo 1 p.m.)
Team No. 1 vs lowest ranked team
Remaining two teams meet In other
Saturday, Nov. 8
Championship game at home of
.highest ranked team
Western Eight schedule
LEAGUE SCHEDULE
Friday, Sept 12
Raymond Comets at St. Mary's
Warriors
Magrath Zeniths at Cardston
Cougars
Coaldale Spartans at Vulcan Golden
Hawks
Bye—Claresholm Cobras
Friday, Sept, is
Vulcan Golden Hawks at Clarsholm
Cobras
Coaldale Spartans at Raymond
Comets '
Cardston Cougars-at St. Mary's
Warriors
Bye—Magrath Zeniths
Tuesday, Sept 23
Raymond Comets at Cardston
Cougars
St. Mary’s Warriors at Vulcan Golden
Hawks .
Claresholm Cobras at Magrath
Zeniths
Bye—Coaldale Spartans
Friday, Sept 28
Claresholm Cobras at Coaldale
Spartans
Magrath Zeniths at St. Mary's
Warriors
Cardston Cougars at Vulcan Golden
Hawks
Bye—Raymond Comets
Friday, Oct 3
Raymond Comets at Claresholm
Cobras
St. Mary's Warriors at Coaldale
Spartans
Vulcan Golden Hawks at Magrath
Zeniths
Bye—Cardston Cougars
the exception of the playoffs
games will all take place on Saturday
afternoons, kick off time at one p.m.
The league will begin Friday, Sept.
12th and continues through until play­offs
Saturday, Oct. 25th. The semi­finals
are slated for Saturday, Nov.
1 and the championship match, at the
home of the highest ranked team, will
go Saturday, Nov. 8th.
Conference games
get under way at
5 p.m. in September.
4:30 p.m. the rest
of the season with
. . i. Playoff
POST OFFICE NOTICE: Do to the re­scheduling
of the arrival and depart­ure
of mail, the following Post Office
hours are being implemented: Wicket
Service - Mondays through Fridays
0830 to 1700; Saturday 0900 to 1300.
We request all despatching of local
mail be in the Post Office by 1615
or 4:15 p.m.
SENIOR CITIZENS: On Friday, Sept. 26,
at 1 p.m. there will be a Pot Luck
Dinner at the Garden City Senior Cit­izens’
Drop In Center, in the Lounge.
Guests will pay $1.00 each, and bring
an item of food: vegetable, or salad,
or dessert.
Hostesses will be Ione ,Krhbiel
and Eda Kosma.
At the program following the
dinner, medallions will be presented
to citizens of 75 years and over who
have applied for them.
ELDERS' QUORUM MOVIE - September 19th
7 p.m. Magrath Cultural Hall. ""The
Cat from Outer Space"". Adults $1.50,
Students 75d, Children under 12, 50d.
MAGRATH CERAMICS OPENING — Wednesday,
Oct. 1st. Adults 1 - 3:30 p.m. and
7 - 9:30 p.m. Children 3:30 - 5:30
p.m. Wednesdays and Thursdays. Fees -
$6.00 Adults, $3.00 Students. Phone
758-3229 for further information.
Isobell Holladay
Never Press Coveralls
in Blue or Brown.
Light weight.
REGULAR $29.95
öOVERNfftENr
VftMT&D
j
THEV TUST WRITE SIGNS PA
THEY NEVER REAP TU EM*
Men's Turtle Neck
T Shirts styled by
Stanfields. Long
Sleeves. Shades of
White, Brown, Red
Blue, etc.
REGULAR $18.00
1 TABLE OF BOYS '
IN ASSORTED STYLES.
NOW IS THE TIME TO
STOCK UP ON YOUR
WINTER NEEDS.
REGULAR $17.95
INSULATED COVERALLS
styled by Workmate.
Quilted lining,
polyester shell.
Dark green
PRICED AT
shade.
..... $69.95
; Rubber
Western Boots WA Urrà
Western boot style
Rubber Boots, lined.
1 pr per customer _
REG. $14.95 8s
A ""Salute to the North­west
Territories"" Tour to
commemorate an all paved high­way
from Coutts to the North
West Territories as well' as
celebrating Alberta's 75th
birthday has been completed.
The tour was open to everyone
in Alberta who wished to part­icipate,
with buses provided
for senior citizens.
Those from Magrath who
took part were Burns & Iöla
Harker, Lottie Harker, Winnie
Ehlert, Lyda Poulsen, Mary
Dudley, Lucille Seward and
Olive Gruninger.
The tour started at Coutts
August 26th and ended September
9th at Stony Plain. It encomp­assed
Alberta visiting all
13 zones, a distance of 3500
^lles Pjlus 1000 miles by plane
in the N.W.T. as the group
was flown to Yellow Knife and
Fort Smith and back from Hay
River. ‘
All of us had a grand and
glorious time.
Alberta is a wonderful
province with many museums,
fishing lakes, beaches and
beautiful scenery to be seen.
L . S .
A man's job is his best
friend. It clothes and feeds
his wife and children, pays the
rent and supplies the wherewithal
to develop and become cultivated.
The least a man can do in return
is to love his job...............
If you ask any successful
man the reason for his making
good, he will tell you that first
and foremost it is because he
likes his work; indeed, he is
wrapped up in it. He walks his
work; he talks his work; he is
entirely inseparable from his
work, and that is the way every
man worth his salt ought to be
if he wants to make of his work
what it should be, and make of
himself what he wants to be.
Bedspreads
blankets by Tex made’
REGULAR $49.95
(No Returns)
Mr. and Mrs. Rob Rollingson
and son John have been transferred
to Rimbey, Alberta where Rob will
be working for the Bank’ of Montreal
as the Administrations Manager.
Flannelette Blankets
designed by Texmade. ’
Have attractive colored
SIZE: 70 x 90.
Pretty two-tone Chenille Bedspreads '
in size 93x103. Look luxurious,
wash like a dream
Mr. and Mrs. Earl Gurney, their
daughter Mrs. Myrna Law and grand­daughter
Mrs. Debbie Rollingson and
great grandson John Rollingson and Mrs.
Ila Tuff have returned from Utah where
they attended the wedding of Miss Leslee
Gurney, daughter of Mr. and Mrs. Martin
Gurney of Orem, Utah.
Mr. and Mrs. Ewald Kruger have as
their guests his brother Mr. Robert
Kruger and daughter Rita and her fiance
Mr. Ludwig Ludecke all pf Hardenberg,
Germany.
Mr. and Mrs. Loren Meldrum and
daughters Fern and Jane of Brigham City,
Utah are visiting relatives Mr. and Mrs.
Lloyd Meldrum and Mr. and Mrs. Lawrence
McClain.
Mr. and Mrs. Gary Heaton and sons
left at the weekend to make their home
at Innisfail, Alberta.
Mr. and Mrs. Jay Christensen of
Calgary and Mr. and Mrs. Pete Harris
of Corpus Christi, Texas were visitors
during the weekend of Jay’s father, Mr.
Eiden Christensen at Diamond Willow
Terrace.
Mr. and Mrs. Hall Poulsen attended
the recent funeral of the late Clifford
TOWELS
Terry Hand Towels
with screen printed
floral design.
Assorted colors.
REGULAR $3.95
2«
SAYELLE
YARN
A knitting favorite
as well as for your
handicraft needs.
Assorted colors.
REGULAR $1.00 89* Hood in Calgary, beloved husband of
Haz^l Hood and brother-in-law of Mrs.
Poulsen.
Mr. and Mrs. L. B. Tanner returned
home Sunday from Mesa, Arizona, where
they served a two year Mission for the
Magrath Third Ward L.D.S. Church.
Word was received Monday evening
of the passing of Marion Macleod of
Provo, Utah, daughter of Mr. and Mrs.
Tom Norton of Provo, and formerly of
Magrath. Mrs. Macleod passed away
Monday afternoon following a lengthy
illness.
She is also survived by her loving
husband, Don, one daughter and 5 sons;
her twin sister Marilyn Haskell of
Lovell, Wyoming; sister Jane of Salt
Lake City; brothers Keith of Magrath
and Al of California. She was pre­deceased
by a sister Helen.
TABLECLOTHS
Perma Press Table Cloths with
soil release. Floral patterns
in an assortment of
colors. 54x72. 14“ REGULAR $21.95
Mr. and Mrs. Dick Sabey wish to
announce the forthcoming.marriage of
their daughter Mary, to Ned Moon of
Burley, Idaho. The wedding will take
place in the Logan Temple, September
19th. Friends and relatives are in­vited
to an Open House honoring the
happy couple on September 27th from
7 to 9 p.m. at the Heritage Hall
(Main St.) Milk River.
BONELESS ,
L V M P I C 500 G
BEEF ,
Smoked Hams swifts .
Hound Steak i LB
LB
LB
I a
2««
J 69
HAM ho C
*************************************
.. P $ TRICT HOME ECONOMIST REP ORT
n —-------------- ---------- ------- Alberta Agri­culture
and Lethbridge Community Coll-
K S MEATY
. . .LB c
Housewives Conference: Alberta Agri-pillât.**/*
— _ J ▼ . • « . .
eg£ are offering the Housewives Con­ference
on Sept. 30 - Oct. 2nd at
aferton Park. The program is aimed
at 'Southern Alberta housewives to
ma|ce them more aware of the options
and opportunities available to them
in their homes and communities. For
more information and registration
orms, drop into the Agriculture Off-icç
at Cardston or call 653-4461
Adolescent Issues- This workshop is
designed for parents and other int­erested
in or working with adoles­cents.
Information and discussion on
suçh topics as parent-adolescent
communication, the development of
the adolescent, sexuality, alcohol
and drug abuse will be explored.
Place - Provincial Building, Cardston.
Time: 9:00 a.m. to 4:00 p.m.
Cost - $5.00.
To register and for further informat-tiqn
contact or drop into the Agri­culture
Office or call 653-4461.
Other courses which final dates
have not been set for include:
Timfe Saver Sewing; Pattern Altera­tions;
Ski Wear; Money Management,
lease inquire if there are any
areas that want certain courses.
£athy Bosse, D. H. E.
*************************** 1
DISTRICT AGRICULTURIST NEWS
FOR SALE: Feed grain oats & barley.
Ph. ; 758-3278, Nick Nykyforuk.
FOR SALE:
lances.
2 bikes, a couple of appl-
Ph. 758-3219.
May
May
May
— - - X~ ~ ““ J V. J- d
The rains fall soft upon your fields,
. and
Until we meet again, may God hold you
in the palm of His hand.
-Ancient Irish Toast.
the
the
the
road rise to meet
wind be always at
you,
__ your back,
sun shine warm upon your face 5
Warble Treatment Pays! A return from
ro $45 on an investment of one
dollar is pretty good return on any­body
s money. This is the return that
a cattleman who treats his cattle for
warbles in the fall can expect to make
says Alberta Agriculture’s livestock
pest control specialist, Dr. All Khan.
He reports that 917 feeder calves
(weighing an average of 415 pounds)
that were treated with a commercially
available pour-on, spot-on or spray
in an experiment conducted at the ;
University of Nebraska gained an av­erage
of 1.04 pounds per day compared
with only 0.92 pounds per day for $
similar group of untreated animals.
On this basis, if these animals
had been pre-conditioned for 200
days before they went into the feed­lot,
they would have gained 24 extra
pounds. It is not hard to calculate
what these 24 extra pounds would bd
worth at today’s prices.
Dr. Khan also reports that trials
which he has conducted in Alberta
show that treating cattle in Octobep
rather than in November produces '
much better results.
D.L. Steed & John Knapp. D.A.’s.
PRE SCHOOL STORYHOUR- at the Magrath
Public Library will begin again Wed.
Sept. 24th from 10 - 11 a.m. Pre­schoolers
and older through Kinder­garten
age. Books may be taken home
(2 per child) at the end of each week’s
session providing the child holds a
current library card - 25c. This unit
will last 6 weeks through October 29
at a $1.00 fee per child. Only 15
children will be accepted but guests
may be brought at any time at a charge
of 25c. To register your children '
call Iris English, 758-3751.
One good thing about telling the truth
is that you don't have to remember
what you said.
FALL SPECIALS
W. F. 1# PARCHMENT
Orange Juice MINUTE MAID 355 ML .........
HEINZ 568 ML ».
3 MINUTE
H E I N Z IN TOMATO SAUCE 398 ML
Butter SQUIRREL 1KG
Bounce Sheets FABRIC SOFTENER 40'S
PAPER TOWELS KLEENEX 2 PACK .
Vegetable Soup HEINZ 284 ML ............................... 3i89‘
HANDI PACK BOX J 95
Hearts i8s each 88(
Câbbôg® LOCAL
COOKING ONIONS 3# bag","Magrath Store News (September 18, 1980)",,J. A. Ririe,,,core
4406546,1977-04-01T00:00:00,"Key Words and Phrases: real-time, compacting, garbage collection, list processing, virtual memory, file or database management, storage management, storage allocation, LISP, CDR-coding, reference counting.
CR Categories: 3.50, 3.60, 373, 3.80, 4.13, 24.32, 433, 4.35, 4.49
This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-75-C-0522.A real-time list processing system is one in which the time required by each elementary list operation (CONS, CAR, CDR, RPLACA, RPLACD, EQ, and ATOM in LISP) is bounded by a (small) constant. Classical list processing systems such as LISP do not have this property because a call to CONS may invoke the garbage collector which requires time proportional to the number of accessible cells to finish. The space requirement of a classical LISP system with N accessible cells under equilibrium conditions is (1.5+μ)N or (1+μ)N, depending upon whether a stack is required for the garbage collector, where μ>0 is typically less than 2.
A list processing system is presented which:
1) is real-time--i.e. T(CONS) is bounded by a constant independent of the number of cells in use;
2) requires space (2+2μ)N, i.e. not more than twice that of a classical system;
3) runs on a serial computer without a time-sharing clock;
4) handles directed cycles in the data structures;
5) is fast--the average time for each operation is about the same as with normal garbage collection;
6) compacts--minimizes the working set;
7) keeps the free pool in one contiguous block--objects of nonuniform size pose no problem;
8) uses one phase incremental collection--no separate mark, sweep, relocate phases;
9) requires no garbage collector stack;
10) requires no ""mark bits"", per se;
11) is simple--suitable for microcoded implementation.
Extensions of the system to handle a user program stack, compact list representation (""CDR-coding""), arrays of non-uniform size, and hash linking are discussed. CDR-coding is shown to reduce memory requirements for N LISP cells to ≈(I+μ)N. Our system is also compared with another approach to the real-time storage management problem, reference counting, and reference counting is shown to be neither competitive with our system when speed of allocation is critical, nor compatible, in the sense that a system with both forms of garbage collection is worse than our pure one.MIT Artificial Intelligence Laboratory
Department of Defense Advanced Research Projects Agenc",List Processing in Real Time on a Serial Computer,https://core.ac.uk/download/4406546.pdf,MIT Artificial Intelligence Laboratory,,,core
