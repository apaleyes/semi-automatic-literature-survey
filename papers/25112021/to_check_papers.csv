doi,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1109/AERO.2018.8396807,to_check,2018 IEEE Aerospace Conference,IEEE,2018-03-10 00:00:00,ieeexplore,Learning safe recovery trajectories with deep neural networks for unmanned aerial vehicles,https://ieeexplore.ieee.org/document/8396807/,"Unmanned vehicles that use vision sensors for perception to aid autonomous flight are a highly popular area of research. However, these systems are often prone to failures that are often hard to model. Previous work has focused on using deep learning to detect these failures. In this work, we build on these failure detection systems and develop a pipeline that learns to identify the correct trajectory to execute that restores the vision system and the unmanned vehicle to a safe state. The key challenge with using a deep learning pipeline for this problem is the limited amount of training data available from a real world system. Ideally one requires millions of data points to sufficiently train a model from scratch. However, this is not feasible for an unmanned aerial vehicle. The dataset we operate with is limited to 400-500 points. To sufficiently learn from such a small dataset we leverage the idea of transfer learning and non linear dimensionality reduction. We deploy our pipeline on an unmanned aerial vehicle flying autonomously through outdoor clutter (in a GPS denied environment) and show that we are able to achieve long durations of safe autonomous flight.",autonomous vehicle
10.1109/SSCI.2018.8628895,to_check,2018 IEEE Symposium Series on Computational Intelligence (SSCI),IEEE,2018-11-21 00:00:00,ieeexplore,Example Mining for Incremental Learning in Medical Imaging,https://ieeexplore.ieee.org/document/8628895/,"Incremental Learning is well known machine learning approach wherein the weights of the learned model are dynamically and gradually updated to generalize on new unseen data without forgetting the existing knowledge. Incremental learning proves to be time as well as resource-efficient solution for deployment of deep learning algorithms in real world as the model can automatically and dynamically adapt to new data as and when annotated data becomes available. The development and deployment of Computer Aided Diagnosis (CAD) tools in medical domain is another scenario, where incremental learning becomes very crucial as collection and annotation of a comprehensive dataset spanning over multiple pathologies and imaging machines might take years. However, not much has so far been explored in this direction. In the current work, we propose a robust and efficient method for incremental learning in medical imaging domain. Our approach makes use of Hard Example Mining technique (which is commonly used as a solution to heavy class imbalance) to automatically select a subset of dataset to fine-tune the existing network weights such that it adapts to new data while retaining existing knowledge. We develop our approach for incremental learning of our already under test model for detecting dental caries. Further, we apply our approach to one publicly available dataset and demonstrate that our approach reaches the accuracy of training on entire dataset at once, while availing the benefits of incremental learning scenario.",health
10.1109/MLSP.2018.8516927,to_check,2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP),IEEE,2018-09-20 00:00:00,ieeexplore,SINGLE-CHANNEL EEG CLASSIFICATION BY MULTI-CHANNEL TENSOR SUBSPACE LEARNING AND REGRESSION,https://ieeexplore.ieee.org/document/8516927/,"The classification of brain states using neural recordings such as electroencephalography (EEG) finds applications in both medical and non-medical contexts, such as detecting epileptic seizures or discriminating mental states in brain-computer interfaces, respectively. Although this endeavor is well-established, existing solutions are typically restricted to lab or hospital conditions because they operate on recordings from a set of EEG electrodes that covers the whole head. By contrast, a true breakthrough for these applications would be the deployment `in the real world', by means of wearable devices that encompass just one (or a few) channels. Such a reduction of the available information inevitably makes the classification task more challenging. We tackle this issue by means of a multilinear subspace learning step (using data from multiple channels during training) and subsequently solving a regression problem with a low-rank structure to classify new trials (using data from only a single channel during testing). We demonstrate the feasibility of this approach on EEG data recorded during a mental arithmetic task.",health
10.1007/s10916-021-01783-y,to_check,Journal of Medical Systems,Springer,2021-11-02 00:00:00,springer,Machine Learning for Health: Algorithm Auditing & Quality Control,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10916-021-01783-y,"Developers proposing new machine learning for health (ML4H) tools often pledge to match or even surpass the performance of existing tools, yet the reality is usually more complicated. Reliable deployment of ML4H to the real world is challenging as examples from diabetic retinopathy or Covid-19 screening show. We envision an integrated framework of algorithm auditing and quality control that provides a path towards the effective and reliable application of ML systems in healthcare. In this editorial, we give a summary of ongoing work towards that vision and announce a call for participation to the special issue  Machine Learning for Health: Algorithm Auditing & Quality Control in this journal to advance the practice of ML4H auditing.",health
10.1109/IJCNN.2016.7727278,to_check,2016 International Joint Conference on Neural Networks (IJCNN),IEEE,2016-07-29 00:00:00,ieeexplore,Augmenting adaptation with retrospective model correction for non-stationary regression problems,https://ieeexplore.ieee.org/document/7727278/,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. We address a scenario when selective deployment of these adaptive mechanisms is possible. In this case, deploying each adaptive mechanism results in different candidate models, and only one of these candidates is chosen to make predictions on the subsequent data. After observing the error of each of candidate, it is possible to revert the current model to the one which had the least error. We call this strategy retrospective model correction. In this work we aim to investigate the benefits of such approach. As a vehicle for the investigation we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to changes in the data. Using real world data from the process industry we show empirically that the retrospective model correction is indeed beneficial for the predictive accuracy, especially for the weaker adaptive mechanisms.",industry
10.1109/HPEC.2019.8916576,to_check,2019 IEEE High Performance Extreme Computing Conference (HPEC),IEEE,2019-09-26 00:00:00,ieeexplore,Deploying AI Frameworks on Secure HPC Systems with Containers.,https://ieeexplore.ieee.org/document/8916576/,"The increasing interest in the usage of Artificial Intelligence (AI) techniques from the research community and industry to tackle “real world” problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high level scripting languages or frameworks such as TensorFlow and the installation processes often require connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API's such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",industry
10.1109/ISGT.2016.7781159,to_check,2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT),IEEE,2016-09-09 00:00:00,ieeexplore,Large-scale detection of non-technical losses in imbalanced data sets,https://ieeexplore.ieee.org/document/7781159/,"Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",industry
10.1109/IJCNN.2015.7280779,to_check,2015 International Joint Conference on Neural Networks (IJCNN),IEEE,2015-07-17 00:00:00,ieeexplore,On sequences of different adaptive mechanisms in non-stationary regression problems,https://ieeexplore.ieee.org/document/7280779/,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. These mechanisms are usually deployed in a prescribed order which does not change. In this work we investigate and provide a comparative analysis of the effects of using a flexible order of adaptive mechanisms' deployment resulting in varying adaptation sequences. As a vehicle for this comparison, we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to the changes in data. Using real world data from the process industry we demonstrate that such flexible deployment of available adaptive methods embedded in a cross-validatory framework can benefit the predictive accuracy over time.",industry
http://arxiv.org/abs/1905.10090v1,to_check,arxiv,arxiv,2019-05-24 08:45:56+00:00,arxiv,Deploying AI Frameworks on Secure HPC Systems with Containers,http://arxiv.org/abs/1905.10090v1,"The increasing interest in the usage of Artificial Intelligence techniques
(AI) from the research community and industry to tackle ""real world"" problems,
requires High Performance Computing (HPC) resources to efficiently compute and
scale complex algorithms across thousands of nodes. Unfortunately, typical data
scientists are not familiar with the unique requirements and characteristics of
HPC environments. They usually develop their applications with high-level
scripting languages or frameworks such as TensorFlow and the installation
process often requires connection to external systems to download open source
software during the build. HPC environments, on the other hand, are often based
on closed source applications that incorporate parallel and distributed
computing API's such as MPI and OpenMP, while users have restricted
administrator privileges, and face security restrictions such as not allowing
access to external systems. In this paper we discuss the issues associated with
the deployment of AI frameworks in a secure HPC environment and how we
successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",industry
http://arxiv.org/abs/2110.04003v1,to_check,arxiv,arxiv,2021-10-08 09:59:12+00:00,arxiv,Learning to Centralize Dual-Arm Assembly,http://arxiv.org/abs/2110.04003v1,"Even though industrial manipulators are widely used in modern manufacturing
processes, deployment in unstructured environments remains an open problem. To
deal with variety, complexity and uncertainty of real world manipulation tasks
a general framework is essential. In this work we want to focus on assembly
with humanoid robots by providing a framework for dual-arm peg-in-hole
manipulation. As we aim to contribute towards an approach which is not limited
to dual-arm peg-in-hole, but dual-arm manipulation in general, we keep modeling
effort at a minimum. While reinforcement learning has shown great results for
single-arm robotic manipulation in recent years, research focusing on dual-arm
manipulation is still rare. Solving such tasks often involves complex modeling
of interaction between two manipulators and their coupling at a control level.
In this paper, we explore the applicability of model-free reinforcement
learning to dual-arm manipulation based on a modular approach with two
decentralized single-arm controllers and a single centralized policy. We reduce
modeling effort to a minimum by using sparse rewards only. We demonstrate the
effectiveness of the framework on dual-arm peg-in-hole and analyze sample
efficiency and success rates for different action spaces. Moreover, we compare
results on different clearances and showcase disturbance recovery and
robustness, when dealing with position uncertainties. Finally we zero-shot
transfer policies trained in simulation to the real-world and evaluate their
performance.",industry
http://arxiv.org/abs/1602.08350v2,to_check,arxiv,arxiv,2016-02-26 14:49:29+00:00,arxiv,Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets,http://arxiv.org/abs/1602.08350v2,"Non-technical losses (NTL) such as electricity theft cause significant harm
to our economies, as in some countries they may range up to 40% of the total
electricity distributed. Detecting NTLs requires costly on-site inspections.
Accurate prediction of NTLs for customers using machine learning is therefore
crucial. To date, related research largely ignore that the two classes of
regular and non-regular customers are highly imbalanced, that NTL proportions
may change and mostly consider small data sets, often not allowing to deploy
the results in production. In this paper, we present a comprehensive approach
to assess three NTL detection models for different NTL proportions in large
real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and
Support Vector Machine. This work has resulted in appreciable results that are
about to be deployed in a leading industry solution. We believe that the
considerations and observations made in this contribution are necessary for
future smart meter research in order to report their effectiveness on
imbalanced and large real world data sets.",industry
10.1016/j.enconman.2021.113856,to_check,Energy Conversion and Management,scopus,2021-04-01,sciencedirect,The mutual benefits of renewables and carbon capture: Achieved by an artificial intelligent scheduling strategy,https://api.elsevier.com/content/abstract/scopus_id/85101129959,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system.",industry
10.1109/ICECCE49384.2020.9179349,to_check,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",IEEE,2020-06-13 00:00:00,ieeexplore,A cloud based smart recycling bin for in-house waste classification,https://ieeexplore.ieee.org/document/9179349/,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations.",smart cities
10.1109/MOCAST49295.2020.9200283,to_check,2020 9th International Conference on Modern Circuits and Systems Technologies (MOCAST),IEEE,2020-09-09 00:00:00,ieeexplore,A cloud based smart recycling bin for waste classification,https://ieeexplore.ieee.org/document/9200283/,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low.",smart cities
http://arxiv.org/abs/1805.00361v1,to_check,arxiv,arxiv,2018-04-30 17:36:14+00:00,arxiv,"Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt
  for Mobile and Embedded Applications",http://arxiv.org/abs/1805.00361v1,"Computer vision performances have been significantly improved in recent years
by Convolutional Neural Networks(CNN). Currently, applications using CNN
algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs
or FPGAs. However, power consumption, speed, accuracy, memory footprint, and
die size should all be taken into consideration for mobile and embedded
applications. Domain Specific Architecture (DSA) for CNN is the efficient and
practical solution for CNN deployment and implementation. We designed and
produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra
power-efficient performance of 9.3TOPS/Watt and with all processing done in the
internal memory instead of outside DRAM. It classifies 224x224 RGB image inputs
at more than 140fps with peak power consumption at less than 300mW and an
accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is
reconfigurable to support CNN model coefficients of various layer sizes and
layer types, including convolution, depth-wise convolution, short-cut
connections, max pooling, and ReLU. Furthermore, in order to better support
real-world deployment for various application scenarios, especially with
low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also
designed algorithms to fully utilize the CNN-DSA accelerator efficiently by
reducing the dependency on external accelerator computation resources,
including implementation of Fully-Connected (FC) layers within the accelerator
and compression of extracted features from the CNN-DSA accelerator. Live demos
with our CNN-DSA accelerator on mobile and embedded systems show its
capabilities to be widely and practically applied in the real world.",smart cities
http://arxiv.org/abs/2103.08022v1,to_check,arxiv,arxiv,2021-03-14 20:13:06+00:00,arxiv,"Success Weighted by Completion Time: A Dynamics-Aware Evaluation
  Criteria for Embodied Navigation",http://arxiv.org/abs/2103.08022v1,"We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner.",smart cities
http://arxiv.org/abs/1805.08692v1,to_check,arxiv,arxiv,2018-05-04 15:07:29+00:00,arxiv,"Assessing a mobile-based deep learning model for plant disease
  surveillance",http://arxiv.org/abs/1805.08692v1,"Convolutional neural network models (CNNs) have made major advances in
computer vision tasks in the last five years. Given the challenge in collecting
real world datasets, most studies report performance metrics based on available
research datasets. In scenarios where CNNs are to be deployed on images or
videos from mobile devices, models are presented with new challenges due to
lighting, angle, and camera specifications, which are not accounted for in
research datasets. It is essential for assessment to also be conducted on real
world datasets if such models are to be reliably integrated with products and
services in society. Plant disease datasets can be used to test CNNs in real
time and gain insight into real world performance. We train a CNN object
detection model to identify foliar symptoms of diseases (or lack thereof) in
cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app
and test its performance on mobile images and video of 720 diseased leaflets in
an agricultural field in Tanzania. Within each disease category we test two
levels of severity of symptoms - mild and pronounced, to assess the model
performance for early detection of symptoms. In both severities we see a
decrease in the F-1 score for real world images and video. The F-1 score
dropped by 32% for pronounced symptoms in real world images (the closest data
to the training data) due to a drop in model recall. If the potential of
smartphone CNNs are to be realized our data suggest it is crucial to consider
tuning precision and recall performance in order to achieve the desired
performance in real world settings. In addition, the varied performance related
to different input data (image or video) is an important consideration for the
design of CNNs in real world applications.",smart cities
http://arxiv.org/abs/1807.05211v1,to_check,arxiv,arxiv,2018-07-11 11:05:12+00:00,arxiv,"Learning Deployable Navigation Policies at Kilometer Scale from a Single
  Traversal",http://arxiv.org/abs/1807.05211v1,"Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",smart cities
http://arxiv.org/abs/1910.09667v1,to_check,arxiv,arxiv,2019-10-21 21:44:15+00:00,arxiv,"Combining Benefits from Trajectory Optimization and Deep Reinforcement
  Learning",http://arxiv.org/abs/1910.09667v1,"Recent breakthroughs both in reinforcement learning and trajectory
optimization have made significant advances towards real world robotic system
deployment. Reinforcement learning (RL) can be applied to many problems without
needing any modeling or intuition about the system, at the cost of high sample
complexity and the inability to prove any metrics about the learned policies.
Trajectory optimization (TO) on the other hand allows for stability and
robustness analyses on generated motions and trajectories, but is only as good
as the often over-simplified derived model, and may have prohibitively
expensive computation times for real-time control. This paper seeks to combine
the benefits from these two areas while mitigating their drawbacks by (1)
decreasing RL sample complexity by using existing knowledge of the problem with
optimal control, and (2) providing an upper bound estimate on the
time-to-arrival of the combined learned-optimized policy, allowing online
policy deployment at any point in the training process by using the TO as a
worst-case scenario action. This method is evaluated for a car model, with
applicability to any mobile robotic system. A video showing policy execution
comparisons can be found at https://youtu.be/mv2xw83NyWU .",smart cities
10.1109/AIVR.2018.00018,to_check,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2018-12-12 00:00:00,ieeexplore,A Compensation Method of Two-Stage Image Generation for Human-AI Collaborated In-Situ Fashion Design in Augmented Reality Environment,https://ieeexplore.ieee.org/document/8613637/,"In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real human computer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design.",multimedia
10.1109/ACIE51979.2021.9381092,to_check,2021 IEEE Asia Conference on Information Engineering (ACIE),IEEE,2021-01-31 00:00:00,ieeexplore,End-to-End Behavior Simulation for Multi-Access Edge Computing,https://ieeexplore.ieee.org/document/9381092/,"Multi-Access edge computing (MEC) is an emerging network architecture that enables cloud computing at the edge of the network characterized by ultra-low latency, high-bandwidth, and direct access to real-time network information. However, the network heterogeneity (4G, 5G, WiFi, etc.) and quality requirement diversity (Internet of Things, Artificial Intelligence, Augmented Reality, etc.) make the deployment of MEC ever more complicated and expensive. In this paper, we introduce a novel end-to-end simulation tool- Behavior Simulation from User Equipment to Edge and Cloud (BSUEEC)- which can offload computing, communication, and traffic simulation to different simulators for the deployment and resource planning of MEC, applicable to various real world scenarios. It's a co-simulation tool that can: 1) seamlessly integrate with heterogeneous access network simulators via a self-developed adapter, 2) extend the simulation from UE(user equipments)-to-edge to UE-to-multi-layer-MEC, even the Cloud 3) assign the UE with real traffic patterns, 4) consider the interaction among access network, MEC/Cloud processing, and road traffic in one simulation. The effectiveness and applicability of our simulation framework (“BSUEEC”) three Vehicle to Everything (V2X) scenarios.",multimedia
10.1109/CVPR.2019.01165,to_check,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2019-06-20 00:00:00,ieeexplore,Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation,https://ieeexplore.ieee.org/document/8953924/,"There has been an increasing interest in 3D indoor navigation, where a robot in an environment moves to a target according to an instruction. To deploy a robot for navigation in the physical world, lots of training data is required to learn an effective policy. It is quite labour intensive to obtain sufficient real environment data for training robots while synthetic data is much easier to construct by render-ing. Though it is promising to utilize the synthetic environments to facilitate navigation training in the real world, real environment are heterogeneous from synthetic environment in two aspects. First, the visual representation of the two environments have significant variances. Second, the houseplans of these two environments are quite different. There-fore two types of information,i.e. visual representation and policy behavior, need to be adapted in the reinforce mentmodel. The learning procedure of visual representation and that of policy behavior are presumably reciprocal. We pro-pose to jointly adapt visual representation and policy behavior to leverage the mutual impacts of environment and policy. Specifically, our method employs an adversarial feature adaptation model for visual representation transfer anda policy mimic strategy for policy behavior imitation. Experiment shows that our method outperforms the baseline by 19.47% without any additional human annotations.",multimedia
10.1109/ICRA40945.2020.9196582,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,https://ieeexplore.ieee.org/document/9196582/,"Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.",multimedia
10.1109/IROS40897.2019.8967592,to_check,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2019-11-08 00:00:00,ieeexplore,Can a Robot Become a Movie Director? Learning Artistic Principles for Aerial Cinematography,https://ieeexplore.ieee.org/document/8967592/,"Aerial filming is constantly gaining importance due to the recent advances in drone technology. It invites many intriguing, unsolved problems at the intersection of aesthetical and scientific challenges. In this work, we propose a deep reinforcement learning agent which supervises motion planning of a filming drone by making desirable shot mode selections based on aesthetical values of video shots. Unlike most of the current state-of-the-art approaches that require explicit guidance by a human expert, our drone learns how to make favorable viewpoint selections by experience. We propose a learning scheme that exploits aesthetical features of retrospective shots in order to extract a desirable policy for better prospective shots. We train our agent in realistic AirSim simulations using both a hand-crafted reward function as well as reward from direct human input. We then deploy the same agent on a real DJI M210 drone in order to test the generalization capability of our approach to real world conditions. To evaluate the success of our approach in the end, we conduct a comprehensive user study in which participants rate the shot quality of our methods. Videos of the system in action can be seen at https://youtu.be/qmVw6mfyEmw.",multimedia
10.1109/ACII52823.2021.9597437,to_check,2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII),IEEE,2021-10-01 00:00:00,ieeexplore,Towards Noise Robust Speech Emotion Recognition Using Dynamic Layer Customization,https://ieeexplore.ieee.org/document/9597437/,"Robustness to environmental noise is important to creating automatic speech emotion recognition systems that are deployable in the real world. In this work, we experiment with two paradigms, one where we can anticipate noise sources that will be seen at test time and one where we cannot. In our first experiment, we assume that we have advance knowledge of the noise conditions that will be seen at test time. We show that we can use this knowledge to create ""expert"" feature encoders for each noise condition. If the noise condition is unchanging, data can be routed to a single encoder to improve robustness. However, if the noise source is variant, this paradigm is too restrictive. In-stead, we introduce a new approach, dynamic layer customization (DLC), that allows the data to be dynamically routed to noise-matched encoders and then recombined. Critically, this process maintains temporal order, enabling extensions for multimodal models that generally benefit from long-term context. In our second experiment, we investigate whether partial knowledge of noise seen at test time can still be used to train systems that generalize well to unseen noise conditions using state-of-the-art domain adaptation algorithms. We find that DLC enables performance increases in both cases, highlighting the utility of mixture-of-expert approaches, domain adaptation methods and DLC to noise robust automatic speech emotion recognition.",multimedia
http://arxiv.org/abs/1912.06321v2,to_check,arxiv,arxiv,2019-12-13 04:29:38+00:00,arxiv,"Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World
  Performance?",http://arxiv.org/abs/1912.06321v2,"Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",multimedia
http://arxiv.org/abs/2105.05873v1,to_check,arxiv,arxiv,2021-05-12 18:00:14+00:00,arxiv,Out of the Box: Embodied Navigation in the Real World,http://arxiv.org/abs/2105.05873v1,"The research field of Embodied AI has witnessed substantial progress in
visual navigation and exploration thanks to powerful simulating platforms and
the availability of 3D data of indoor and photorealistic environments. These
two factors have opened the doors to a new generation of intelligent agents
capable of achieving nearly perfect PointGoal Navigation. However, such
architectures are commonly trained with millions, if not billions, of frames
and tested in simulation. Together with great enthusiasm, these results yield a
question: how many researchers will effectively benefit from these advances? In
this work, we detail how to transfer the knowledge acquired in simulation into
the real world. To that end, we describe the architectural discrepancies that
damage the Sim2Real adaptation ability of models trained on the Habitat
simulator and propose a novel solution tailored towards the deployment in
real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot
equipped with a single Intel RealSense camera. Different from previous work,
our testing scene is unavailable to the agent in simulation. The environment is
also inaccessible to the agent beforehand, so it cannot count on scene-specific
semantic priors. In this way, we reproduce a setting in which a research group
(potentially from other fields) needs to employ the agent visual navigation
capabilities as-a-Service. Our experiments indicate that it is possible to
achieve satisfying results when deploying the obtained model in the real world.
Our code and models are available at https://github.com/aimagelab/LoCoNav.",multimedia
http://arxiv.org/abs/1904.02579v2,to_check,arxiv,arxiv,2019-04-04 14:30:09+00:00,arxiv,"Can a Robot Become a Movie Director? Learning Artistic Principles for
  Aerial Cinematography",http://arxiv.org/abs/1904.02579v2,"Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human expert, our drone learns how to make
favorable viewpoint selections by experience. We propose a learning scheme that
exploits aesthetical features of retrospective shots in order to extract a
desirable policy for better prospective shots. We train our agent in realistic
AirSim simulations using both a hand-crafted reward function as well as reward
from direct human input. We then deploy the same agent on a real DJI M210 drone
in order to test the generalization capability of our approach to real world
conditions. To evaluate the success of our approach in the end, we conduct a
comprehensive user study in which participants rate the shot quality of our
methods. Videos of the system in action can be seen at
https://youtu.be/qmVw6mfyEmw.",multimedia
http://arxiv.org/abs/1709.04909v1,to_check,arxiv,arxiv,2017-09-14 17:54:05+00:00,arxiv,Shared Learning : Enhancing Reinforcement in $Q$-Ensembles,http://arxiv.org/abs/1709.04909v1,"Deep Reinforcement Learning has been able to achieve amazing successes in a
variety of domains from video games to continuous control by trying to maximize
the cumulative reward. However, most of these successes rely on algorithms that
require a large amount of data to train in order to obtain results on par with
human-level performance. This is not feasible if we are to deploy these systems
on real world tasks and hence there has been an increased thrust in exploring
data efficient algorithms. To this end, we propose the Shared Learning
framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving
this, we look into some principles of transfer learning which aim to study the
benefits of information exchange across tasks in reinforcement learning and
adapt transfer to learning our value function estimates in a novel manner. In
this paper, we consider the special case of transfer between the value function
estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further
empirically demonstrate how our proposed framework can help in speeding up the
learning process in $Q$-ensembles with minimum computational overhead on a
suite of Atari 2600 Games.",multimedia
http://arxiv.org/abs/2109.14549v1,to_check,arxiv,arxiv,2021-09-29 16:48:05+00:00,arxiv,"Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay
  Randomization",http://arxiv.org/abs/2109.14549v1,"Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.",multimedia
http://arxiv.org/abs/1910.09430v2,to_check,arxiv,arxiv,2019-10-21 15:06:03+00:00,arxiv,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,http://arxiv.org/abs/1910.09430v2,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.de",multimedia
10.1109/IROS40897.2019.8968004,to_check,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2019-11-08 00:00:00,ieeexplore,Long Range Neural Navigation Policies for the Real World,https://ieeexplore.ieee.org/document/8968004/,"Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies - a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving 0.80 success rate over long navigation runs and outperforming SLAM-based models in the same settings.",robotics
http://arxiv.org/abs/2106.09357v1,to_check,arxiv,arxiv,2021-06-17 10:20:45+00:00,arxiv,"Cat-like Jumping and Landing of Legged Robots in Low-gravity Using Deep
  Reinforcement Learning",http://arxiv.org/abs/2106.09357v1,"In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.",robotics
http://arxiv.org/abs/1903.09870v2,to_check,arxiv,arxiv,2019-03-23 19:36:11+00:00,arxiv,Long Range Neural Navigation Policies for the Real World,http://arxiv.org/abs/1903.09870v2,"Learned Neural Network based policies have shown promising results for robot
navigation. However, most of these approaches fall short of being used on a
real robot due to the extensive simulated training they require. These
simulations lack the visuals and dynamics of the real world, which makes it
infeasible to deploy on a real robot. We present a novel Neural Net based
policy, NavNet, which allows for easy deployment on a real robot. It consists
of two sub policies -- a high level policy which can understand real images and
perform long range planning expressed in high level commands; a low level
policy that can translate the long range plan into low level commands on a
specific platform in a safe and robust manner. For every new deployment, the
high level policy is trained on an easily obtainable scan of the environment
modeling its visuals and layout. We detail the design of such an environment
and how one can use it for training a final navigation policy. Further, we
demonstrate a learned low-level policy. We deploy the model in a large office
building and test it extensively, achieving $0.80$ success rate over long
navigation runs and outperforming SLAM-based models in the same settings.",robotics
10.1016/j.ijfoodmicro.2015.03.010,to_check,International Journal of Food Microbiology,scopus,2015-07-02,sciencedirect,A strategy to establish food safety model repositories,https://api.elsevier.com/content/abstract/scopus_id/84926308733,"Transferring the knowledge of predictive microbiology into real world food manufacturing applications is still a major challenge for the whole food safety modelling community. To facilitate this process, a strategy for creating open, community driven and web-based predictive microbial model repositories is proposed. These collaborative model resources could significantly improve the transfer of knowledge from research into commercial and governmental applications and also increase efficiency, transparency and usability of predictive models. To demonstrate the feasibility, predictive models of Salmonella in beef previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models were made publicly available in a Food Safety Model Repository within the OpenML for Predictive Modelling in Food community project. Three different approaches were used to create new models in the model repositories: (1) all information relevant for model re-implementation is available in a scientific publication, (2) model parameters can be imported from tabular parameter collections and (3) models have to be generated from experimental data or primary model parameters. All three approaches were demonstrated in the paper. The sample Food Safety Model Repository is available via: http://sourceforge.net/projects/microbialmodelingexchange/files/models and the PMM-Lab software can be downloaded from http://sourceforge.net/projects/pmmlab/. This work also illustrates that a standardized information exchange format for predictive microbial models, as the key component of this strategy, could be established by adoption of resources from the Systems Biology domain.",industry
10.1016/j.eswa.2018.06.037,to_check,Expert Systems with Applications,scopus,2018-12-15,sciencedirect,Two-echelon logistics delivery and pickup network optimization based on integrated cooperation and transportation fleet sharing,https://api.elsevier.com/content/abstract/scopus_id/85049438774,"The optimization of the two-echelon logistics delivery and pickup network (2E-LDPN) is a strategical and tactical task which can efficiently be achieved by establishing cooperative alliances. Under the coordination of logistics services providers or logistics facilities of the existing network, high operating costs caused by cross and long distance transportation can be reduced via the inclusive reorganization of the entire network. In order to minimize the total cost, this study simultaneously considers semitrailer truck and vehicle sharing, and establishes a linear mathematical model capable of interpreting real world practices under single or multiple alliances scenarios. An Improved Particle Swarm Optimization (IPSO) algorithm and the Ant Colony Optimization (ACO) algorithm are reasonably combined into a hybrid meta-heuristics to solve the cooperative 2E-LDPN optimization problem. This algorithm combines the merits of IPSO and ACO with local and global search capabilities, and redistributes customer zones on the basis of region partitioning solutions in order to rationalize transportation activities. Finally, an Improved Shapley value model is applied to guarantee profits allocation's fairness and is proved reliable in term of alliance stability. Empirical results out of a case study in Chongqing city show that the IPSO–ACO hybrid algorithm is superior to three well-known algorithms on the cost solution and the number of iterations. Using the Improved Shapley value model and strictly monotonic path (SMP) selection principles, optimal adhesion sequences for two alliances and a grand alliance are yielded. The implemented transition from two sub-alliances based network to the grand alliance is in line with real-world's practices and provides decision makers with a useful tool for the design of cooperative alliances. In addition, the proposed cooperation strategy and profit allocation method enable companies to increase cost savings and the logistics network's efficiency. Besides, semitrailer truck and vehicle sharing as feature of collaboration conditional clauses reduces the size of transportation fleets, and promotes greener logistics operations.",smart cities
10.1109/ROBOT.1997.606781,to_check,Proceedings of International Conference on Robotics and Automation,IEEE,1997-04-25 00:00:00,ieeexplore,Real-time pose estimation of 3D objects from camera images using neural networks,https://ieeexplore.ieee.org/document/606781/,This paper deals with the problem of obtaining a rough estimate of three dimensional object position and orientation from a single two dimensional camera image. Such an estimate is required by most 3-D to 2-D registration and tracking methods that can efficiently refine an initial value by numerical optimization to precisely recover 3-D pose. However the analytic computation of an initial pose guess requires the solution of an extremely complex correspondence problem that is due to the large number of topologically distinct aspects that arise when a three dimensional opaque object is imaged by a camera. Hence general analytic methods fail to achieve real-time performance and most tracking and registration systems are initialized interactively or by ad hoc heuristics. To overcome these limitations we present a novel method for approximate object pose estimation that is based on a neural net and that can easily be implemented in real-time. A modification of Kohonen's self-organizing feature map is systematically trained with computer generated object views such that it responds to a preprocessed image with one or more sets of object orientation parameters. The key idea proposed here is to choose network topology in accordance with the representation of 3-D orientation. Experimental results from both simulated and real images demonstrate that a pose estimate within the accuracy requirements can be found in more than 81% of all cases. The current implementation operates at 10 Hz on real world images.,multimedia
10.1109/IJCNN.1993.716991,to_check,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",IEEE,1993-10-29 00:00:00,ieeexplore,Learning goal-directed navigation as attractor dynamics for a sensory motor system. (An experiment by the mobile robot YAMABICO),https://ieeexplore.ieee.org/document/716991/,"This paper describes experimental results based on the authors' prior-proposed scheme: learning of sensory-based, goal-directed behavior. The scheme was implemented on the mobile robot ""YAMABICO"" and learning of a set of goal-directed navigations were conducted. The experiment assumed that the robot receives no global information such as position nor prior environment model. Instead, the robot was trained to learn adequate maneuvering in the adopted workspace by building a correct mapping between a spatio-temporal sequence of sensory inputs and maneuvering outputs on a neural structure. The experimental results showed that sufficient training generated rigid dynamical structure of a fixed point and limit cycling in the sensory-based state space, which realized robust navigations of homing and cyclic routing even against certain changes of environment as well as miscellaneous noises in the real world.",robotics
10.1016/j.cogsys.2017.08.002,to_check,Cognitive Systems Research,scopus,2018-01-01,sciencedirect,A computational cognitive framework of spatial memory in brains and robots,https://api.elsevier.com/content/abstract/scopus_id/85034106426,"Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible.
                  We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models.
                  We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.",robotics
10.1109/ICTAI.2019.00220,to_check,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2019-11-06 00:00:00,ieeexplore,Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning,https://ieeexplore.ieee.org/document/8995417/,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",autonomous vehicle
10.1109/ICARCV.2006.345471,to_check,"2006 9th International Conference on Control, Automation, Robotics and Vision",IEEE,2006-12-08 00:00:00,ieeexplore,Terrain Modeling Using Machine Learning Methods,https://ieeexplore.ieee.org/document/4150400/,"The problem of terrain modeling is basically a type of function approximation problem. This type of problem has been widely studied in the soft computing community. In recent years, neural networks have been successfully applied to surface reconstruction and classification problems involving scattered data. However, due to the iterative nature of training a neural network, the resulting high cost in computational time limits the implementation of machine learning based methods in many real world applications (for example, navigation applications in unmanned aerial vehicles) that require fast generation of terrain models. A recently proposed machine learning method, the extreme learning machine (ELM), is able to train single-layer feed forward neural networks with excellent speed and good generalization. In this paper, we present terrain modeling using various machine learning methods, and we compare the performances of these methods with ELM. We also present a comparison of terrain modeling performances between ELM and the popular choice of terrain and surface modeling technique, the Delaunay triangulation with linear interpolation. Our results show that machine learning using ELM offers a potential solution to terrain modeling problems with good performances",autonomous vehicle
http://arxiv.org/abs/1411.6326v1,to_check,arxiv,arxiv,2014-11-24 02:09:59+00:00,arxiv,Vision and Learning for Deliberative Monocular Cluttered Flight,http://arxiv.org/abs/1411.6326v1,"Cameras provide a rich source of information while being passive, cheap and
lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work
we present the first implementation of receding horizon control, which is
widely used in ground vehicles, with monocular vision as the only sensing mode
for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a
number of contributions: novel coupling of perception and control via relevant
and diverse, multiple interpretations of the scene around the robot, leveraging
recent advances in machine learning to showcase anytime budgeted cost-sensitive
feature selection, and fast non-linear regression for monocular depth
prediction. We empirically demonstrate the efficacy of our novel pipeline via
real world experiments of more than 2 kms through dense trees with a quadrotor
built from off-the-shelf parts. Moreover our pipeline is designed to combine
information from other modalities like stereo and lidar as well if available.",autonomous vehicle
http://arxiv.org/abs/2001.03864v1,to_check,arxiv,arxiv,2020-01-12 06:06:03+00:00,arxiv,"Learning to drive via Apprenticeship Learning and Deep Reinforcement
  Learning",http://arxiv.org/abs/2001.03864v1,"With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.",autonomous vehicle
10.1109/SOCA.2015.18,to_check,2015 IEEE 8th International Conference on Service-Oriented Computing and Applications (SOCA),IEEE,2015-10-21 00:00:00,ieeexplore,Automated Process Adaptation in Cyber-Physical Domains with the SmartPM System (Short Paper),https://ieeexplore.ieee.org/document/7399092/,"Cyber Physical Systems (CPSs) refer to a new generation of embedded ICT systems (PCs, smartphones, sensors, actuators, etc.) that are interconnected and collaborating to provide users with a wide range of innovative applications and services. Many application domains, e.g., Emergency management, factories of the future, personalized healthcare, just to name a few, require the definition, design and development of systems able to carry out complex processes that coordinate the services offered by the CPS in the ""physical"" real world. The physical world, however, is not entirely predictable, and such processes must be robust to unexpected conditions and adaptable to unanticipated exceptions. This demands a more flexible approach in process design and enactment, recognizing that in real-world environments it is not adequate to assume that all possible recovery activities can be predefined for dealing with the exceptions that can ensue. In this paper, we tackle the above issue and we propose an approach and a process management system implementation, called SmartPM, for automatically adapting processes enacted in cyber-physical domains in case of unanticipated exceptions and exogenous events.",health
10.1109/ICACITE51222.2021.9404749,to_check,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),IEEE,2021-03-05 00:00:00,ieeexplore,Artificial Intelligence and Robotics: Impact &amp; Open issues of automation in Workplace,https://ieeexplore.ieee.org/document/9404749/,"In engineering province robotics is one of the cognitive perspective to human communication or it concern with synod of perception of action. In Today's Tech World Artificial Intelligence is an essential tool which provides effective analytical business solutions &amp; plays significant role in the domain of robotics and have several similarities like human behavior which may drive the real world. This paper shows the significant blend of Artificial Intelligence and robotics which transform entire industries, technological improvement of robotics application &amp; utilization. It also focuses on different aspects of targets like marketing, home appliances, medical science, Smart agriculture and many more which includes open issues and technological challenges arises by this combination and conclude that robotics with AI can work in real world with real objects. Further AI based robotics are very important area in economics and organizational consequence, implementation of automation in any organizational design give impact on overall economy and infrastructure provide a wider direction for further research on Robotics and IoT are two terms each covering a myriad of technologies and concepts.",health
10.1038/s41746-020-00318-y,to_check,npj Digital Medicine,Nature,2020-08-21 00:00:00,springer,Developing a delivery science for artificial intelligence in healthcare,https://www.nature.com/articles/s41746-020-00318-y,"Artificial Intelligence (AI) has generated a large amount of excitement in healthcare, mostly driven by the emergence of increasingly accurate machine learning models. However, the promise of AI delivering scalable and sustained value for patient care in the real world setting has yet to be realized. In order to safely and effectively bring AI into use in healthcare, there needs to be a concerted effort around not just the creation, but also the delivery of AI. This AI “delivery science” will require a broader set of tools, such as design thinking, process improvement, and implementation science, as well as a broader definition of what AI will look like in practice, which includes not just machine learning models and their predictions, but also the new systems for care delivery that they enable. The careful design, implementation, and evaluation of these AI enabled systems will be important in the effort to understand how AI can improve healthcare.",health
10.1016/j.neucom.2016.09.005,to_check,Neurocomputing,scopus,2017-05-10,sciencedirect,Wood moisture content prediction using feature selection techniques and a kernel method,https://api.elsevier.com/content/abstract/scopus_id/84996497604,"Wood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (
                        MC
                     ) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting 
                        MC
                      from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the 
                        MC
                      of wood. The relationship between the reflection coefficient measurements and the 
                        MC
                      is studied. Based upon this relationship, 
                        MC
                      predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible.",industry
10.1016/j.asoc.2011.05.011,to_check,Applied Soft Computing Journal,scopus,2011-12-01,sciencedirect,Credit risk evaluation using neural networks: Emotional versus conventional models,https://api.elsevier.com/content/abstract/scopus_id/80053571498,"Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. Artificial neural networks (NNs) have been considered to be accurate tools for credit analysis among others in the credit industry. Lately, emotional neural networks (EmNNs) have been suggested and applied successfully for pattern recognition. In this paper we investigate the efficiency of EmNNs and compare their performance to conventional NNs when applied to credit risk evaluation. In total 12 neural networks; based equally on emotional and conventional neural models; are arbitrated under three learning schemes to classify whether a credit application is approved or declined. The learning schemes differ in the ratio of training-to-validation data used during training and testing the neural networks. The emotional and conventional neural models are trained using real world credit application cases from the Australian credit approval datasets which has 690 cases; each case with 14 numerical attributes; based on which an application is accepted or rejected. The performance of the 12 neural networks will be evaluated using certain criteria. Experimental results suggest that both emotional and conventional neural models can be used effectively for credit risk evaluations, however the emotional models outperform their conventional counterparts in decision making speed and accuracy, thus, making them ideal for implementation in fast automatic processing of credit applications.",industry
10.1109/ICNN.1994.375034,to_check,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),IEEE,1994-07-02 00:00:00,ieeexplore,The ordering-oriented Hopfield network,https://ieeexplore.ieee.org/document/375034/,"The travelling salesman problem (TSP) is a well known problem which can be solved using Hopfield networks. The TSP solution with Hopfield networks is based on the uniqueness constraint. This is, each city must be visited once and only once while trying to minimize the travelling distance. But in real world applications, usually there are other equally important constraints needing to be considered. For example, an ordering constraint on how cities are to be visited. This paper proposes a modified Hopfield neural network architecture that can solve a new class of optimization problem, called ""the picking stone problem (PSP)"". The PSP requires not only the uniqueness but also the ordering constraints. The neural network implementation to solve PSP tends to turn on neurons which satisfy the ordering constraint and this constraint is essential in solving the stereo correspondence problem in binocular vision and can be applied to many other pattern recognition problems. In this paper the authors define the PSP, formulate its computational complexity, propose the ordering-oriented neural network architecture, discuss the performance of the proposed network by the traditional random initialization method, and propose a new initialization method to improve the performance of the network.&lt;<ETX>&gt;</ETX>",smart cities
10.1109/BigData50022.2020.9377975,to_check,2020 IEEE International Conference on Big Data (Big Data),IEEE,2020-12-13 00:00:00,ieeexplore,FlightSense: A Spoofer Detection and Aircraft Identification System using Raw ADS-B Data,https://ieeexplore.ieee.org/document/9377975/,"We introduce a robust neural network based method for classifying aircraft, using raw I/Q data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) data from airplanes. ADS-B has become the de-facto standard for air traffic control and forms the basis of the Next Generation Air Transportation System (NextGen). Although ADS-B is at the core of modern day air traffic control, the standard lacks basic security features such as encryption and authentication. As a result, it is possible to spoof ADS-B data and in the process create unprecedented operational havoc in the skies. In this work we propose FlightSense: a robust adversarial learning based system for filtering out spoofed ADS-B data and subsequent identification of airplanes operating in the airspace from the filtered signal. We use the framework of a generative adversarial network (GAN) for our implementation, which is end-to-end in that it uses the raw I/Q signal data as input and no preprocessing steps are required. We present experiments and results to demonstrate the efficacy of our methods using a real world standardized ADS-B dataset.",smart cities
http://arxiv.org/abs/1411.3895v1,to_check,arxiv,arxiv,2014-11-14 13:11:32+00:00,arxiv,"Learning Fuzzy Controllers in Mobile Robotics with Embedded
  Preprocessing",http://arxiv.org/abs/1411.3895v1,"The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance.",smart cities
10.1109/ICPR48806.2021.9412909,to_check,2020 25th International Conference on Pattern Recognition (ICPR),IEEE,2021-01-15 00:00:00,ieeexplore,SAILenv: Learning in Virtual Visual Environments Made Simple,https://ieeexplore.ieee.org/document/9412909/,"Recently, researchers in Machine Learning algorithms, Computer Vision scientists, engineers and others, showed a growing interest in 3D simulators as a mean to artificially create experimental settings that are very close to those in the real world. However, most of the existing platforms to interface algorithms with 3D environments are often designed to setup navigation-related experiments, to study physical interactions, or to handle ad-hoc cases that are not thought to be customized, sometimes lacking a strong photorealistic appearance and an easy-to-use software interface. In this paper, we present a novel platform, SAILenv, that is specifically designed to be simple and customizable, and that allows researchers to experiment visual recognition in virtual 3D scenes. A few lines of code are needed to interface every algorithm with the virtual world, and non-3D-graphics experts can easily customize the 3D environment itself, exploiting a collection of photorealistic objects. Our framework yields pixel-level semantic and instance labeling, depth, and, to the best of our knowledge, it is the only one that provides motion-related information directly inherited from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We perform experiments using a state-of-the-art object detector trained on real-world images, showing that it is able to recognize the photorealistic 3D objects of our environment. The computational burden of the optical flow compares favourably with the estimation performed using modern GPU-based convolutional networks or more classic implementations. We believe that the scientific community will benefit from the easiness and high-quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.",multimedia
10.1109/ICCAS.2013.6703875,to_check,"2013 13th International Conference on Control, Automation and Systems (ICCAS 2013)",IEEE,2013-10-23 00:00:00,ieeexplore,Active relearning for robust on-road vehicle detection and tracking,https://ieeexplore.ieee.org/document/6703875/,This paper aims to introduce a novel robust real time system capable of rapidly detecting and tracking vehicles in a video stream using a monocular vision system. The framework used for this purpose is an actively relearned implementation of the Haar-like feature based Viola-Jones classifier capable of classifying image frame regions as a vehicle or non-vehicle. A passively trained supervised system (based on Adaboost) is initially built by cascading a set of weak classifiers working with Rectangular Haar-like features. An actively learned model is then generated from the initial passive classifier by querying misclassified instances when the model is evaluated on an independent dataset. This classifier is integrated with a Lucas-Kanade Optical Flow Tracker and an empirical distance estimation algorithm to evolve the system into a complete real-time detection and tracking system. The built model is then evaluated extensively on static as well as real world data and results are presented.,multimedia
10.1109/ICTAI.2011.89,to_check,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,IEEE,2011-11-09 00:00:00,ieeexplore,Multi-agent Simulation Design Driven by Real Observations and Clustering Techniques,https://ieeexplore.ieee.org/document/6103379/,"The multi-agent simulation consists in using a set of interacting agents to reproduce the dynamics and the evolution of the phenomena that we seek to simulate. It is considered now as an alternative to classical simulations based on analytical models. But, its implementation remains difficult, particularly in terms of behaviors extraction and agents modelling. This task is usually performed by the designer who has some expertise and available observation data on the process. In this paper, we propose a novel way to make use of the observations of real world agents to model simulated agents. The modelling is based on clustering techniques. Our approach is illustrated through an example in which the behaviors of agents are extracted as trajectories and destinations from video sequences analysis. This methodology is investigated with the aim to apply it, in particular, in a retail space simulation for the evaluation of marketing strategies. This paper presents experiments of our methodology in the context of a public area modelling.",multimedia
10.1109/IJCNN.2008.4634309,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,A new intelligent digital right management technique for E-learning content,https://ieeexplore.ieee.org/document/4634309/,"The digitalization of e-learning sources makes it an easy target for frauds, conterfeiting and content stealing. In this paper we present a new technique to deal with the security problems of e-learning content, its authentication and digital right management. The proposed technique is done by inserting a digital logo image, which serves as watermark signals, in the audio stream of e-learning material. This technique is based on modulated complex lapped transform that was selected for its audio reconstruction properties and the extraction of the watermark is performed using an independent component analysis algorithm. To demonstrate the effectiveness of the proposed method, a real world implementation has been done and the algorithm shows quite good visual and audible quality in watermarked content, as well as a high robustness against common signal processing attacks.",multimedia
http://arxiv.org/abs/2110.04697v1,to_check,arxiv,arxiv,2021-10-10 03:51:39+00:00,arxiv,"An Augmented Reality Platform for Introducing Reinforcement Learning to
  K-12 Students with Robots",http://arxiv.org/abs/2110.04697v1,"Interactive reinforcement learning, where humans actively assist during an
agent's learning process, has the promise to alleviate the sample complexity
challenges of practical algorithms. However, the inner workings and state of
the robot are typically hidden from the teacher when humans provide feedback.
To create a common ground between the human and the learning robot, in this
paper, we propose an Augmented Reality (AR) system that reveals the hidden
state of the learning to the human users. This paper describes our system's
design and implementation and concludes with a discussion on two directions for
future work which we are pursuing: 1) use of our system in AI education
activities at the K-12 level; and 2) development of a framework for an AR-based
human-in-the-loop reinforcement learning, where the human teacher can see
sensory and cognitive representations of the robot overlaid in the real world.",multimedia
http://arxiv.org/abs/2007.08224v2,to_check,arxiv,arxiv,2020-07-16 09:50:23+00:00,arxiv,SAILenv: Learning in Virtual Visual Environments Made Simple,http://arxiv.org/abs/2007.08224v2,"Recently, researchers in Machine Learning algorithms, Computer Vision
scientists, engineers and others, showed a growing interest in 3D simulators as
a mean to artificially create experimental settings that are very close to
those in the real world. However, most of the existing platforms to interface
algorithms with 3D environments are often designed to setup navigation-related
experiments, to study physical interactions, or to handle ad-hoc cases that are
not thought to be customized, sometimes lacking a strong photorealistic
appearance and an easy-to-use software interface. In this paper, we present a
novel platform, SAILenv, that is specifically designed to be simple and
customizable, and that allows researchers to experiment visual recognition in
virtual 3D scenes. A few lines of code are needed to interface every algorithm
with the virtual world, and non-3D-graphics experts can easily customize the 3D
environment itself, exploiting a collection of photorealistic objects. Our
framework yields pixel-level semantic and instance labeling, depth, and, to the
best of our knowledge, it is the only one that provides motion-related
information directly inherited from the 3D engine. The client-server
communication operates at a low level, avoiding the overhead of HTTP-based data
exchanges. We perform experiments using a state-of-the-art object detector
trained on real-world images, showing that it is able to recognize the
photorealistic 3D objects of our environment. The computational burden of the
optical flow compares favourably with the estimation performed using modern
GPU-based convolutional networks or more classic implementations. We believe
that the scientific community will benefit from the easiness and high-quality
of our framework to evaluate newly proposed algorithms in their own customized
realistic conditions.",multimedia
10.1016/j.autcon.2019.103012,to_check,Automation in Construction,scopus,2020-02-01,sciencedirect,On-demand monitoring of construction projects through a game-like hybrid application of BIM and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85075291648,"While unavoidable, inspections, progress monitoring, and comparing as-planned with as-built conditions in construction projects do not readily add tangible intrinsic value to the end-users. In large-scale construction projects, the process of monitoring the implementation of every single part of buildings and reflecting them on the BIM models can become highly labour intensive and error-prone, due to the vast amount of data produced in the form of schedules, reports and photo logs. In order to address the mentioned methodological and technical gap, this paper presents a framework and a proof of concept prototype for on-demand automated simulation of construction projects, integrating some cutting edge IT solutions, namely image processing, machine learning, BIM and Virtual Reality. This study utilised the Unity game engine to integrate data from the original BIM models and the as-built images, which were processed via various computer vision techniques. These methods include object recognition and semantic segmentation for identifying different structural elements through supervised training in order to superimpose the real world images on the as-planned model. The proposed framework leads to an automated update of the 3D virtual environment with states of the construction site. This framework empowers project managers and stockholders with an advanced decision-making tool, highlighting the inconsistencies in an effective manner. This paper contributes to body knowledge by providing a technical exemplar for the integration of ML and image processing approaches with immersive and interactive BIM interfaces, the algorithms and program codes of which can help replicability of these approaches by other scholars.",multimedia
10.1109/INCOS.2009.51,to_check,2009 International Conference on Intelligent Networking and Collaborative Systems,IEEE,2009-11-06 00:00:00,ieeexplore,An Architecture for Adaptive Collaboration Support Guided by Learning Design,https://ieeexplore.ieee.org/document/5369352/,"A CSCL environment provides support to manage collaborative tasks. However, these systems do not usually provide the personalization features required to adapt the learning experience to the student needs, a drawback that can affect the collaboration objective and ultimately a successful learning. To alleviate this disadvantage we propose an architecture that provides adaptive collaboration support for a CSCL environment framed in an open and standards-based LMS. Our proposal combines adaptation rules defined in IMS Learning Design specification and dynamic support through recommendations via an accessible and adaptive guidance system. The implementation offers CSCL courses following a methodology called Collaborative Logical Framework. This system has been tested on a real world scenario at the Madrid Science Week 2009.",science
10.1109/ROMAN.2006.314387,to_check,ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication,IEEE,2006-09-08 00:00:00,ieeexplore,Adaptive Social Skills for Robots Interacting with Virtual Characters in Real Worlds,https://ieeexplore.ieee.org/document/4107778/,"We propose the implementation of a new interaction type that allows the creation of adaptive social relationships between robots and virtual characters in a real world environment, using reinforcement learning. We present the implementation of a storytelling scenario, which results in an immersion experience for the robot. The robot is able to interact and learn dynamically from the virtual character",science
http://arxiv.org/abs/2007.10784v2,to_check,arxiv,arxiv,2020-07-16 21:14:45+00:00,arxiv,Fast Neural Models for Symbolic Regression at Scale,http://arxiv.org/abs/2007.10784v2,"Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public.",science
http://arxiv.org/abs/2110.06196v1,to_check,arxiv,arxiv,2021-10-12 17:49:46+00:00,arxiv,GraPE: fast and scalable Graph Processing and Embedding,http://arxiv.org/abs/2110.06196v1,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",science
10.1016/j.asoc.2021.107792,to_check,Applied Soft Computing,scopus,2021-11-01,sciencedirect,Sentiment classification using attention mechanism and bidirectional long short-term memory network,https://api.elsevier.com/content/abstract/scopus_id/85112745936,"We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.",science
10.1109/TSMC.2020.2967936,to_check,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2021-12-01 00:00:00,ieeexplore,Deep Q-Learning With Q-Matrix Transfer Learning for Novel Fire Evacuation Environment,https://ieeexplore.ieee.org/document/8989970/,"Deep reinforcement learning (RL) is achieving significant success in various applications like control, robotics, games, resource management, and scheduling. However, the important problem of emergency evacuation, which clearly could benefit from RL, has been largely unaddressed. Indeed, emergency evacuation is a complex task that is difficult to solve with RL. An emergency situation is highly dynamic, with a lot of changing variables and complex constraints that make it challenging to solve. Also, there is no standard benchmark environment available that can be used to train RL agents for evacuation. A realistic environment can be complex to design. In this article, we propose the first fire evacuation environment to train RL agents for evacuation planning. The environment is modeled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty, and bottlenecks. The implementation of our environment is in the OpenAI gym format, to facilitate future research. We also propose a new RL approach that entails pretraining the network weights of a DQN-based agent [DQN/Double-DQN (DDQN)/Dueling-DQN] to incorporate information on the shortest path to the exit. We achieved this by using tabular <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-learning to learn the shortest path on the building model’s graph. This information is transferred to the network by deliberately overfitting it on the <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions due to fire spread, bottlenecks, and uncertainty. We perform comparisons of the proposed approach with state-of-the-art RL algorithms like DQN, DDQN, Dueling-DQN, PPO, VPG, state-action-reward-state-action (SARSA), actor–critic method, and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN-based models. Finally, our model is tested on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. In order to reduce the action space, we propose a strategy that involves one step simulation. That is, an action importance vector is added to the final output of the pretrained DQN and acts like an attention mechanism. Using this strategy, the action space is reduced by 90.1%. In this manner, the model is able to deal with large action spaces. Hence, our model achieves near optimal performance on the real world emergency environment.",robotics
10.1109/56.802,to_check,IEEE Journal on Robotics and Automation,IEEE,1988-08-01 00:00:00,ieeexplore,Dynamic multi-sensor data fusion system for intelligent robots,https://ieeexplore.ieee.org/document/802/,"The objective of the authors is to develop an intelligent robot workstation capable of integrating data from multiple sensors. The investigation is based on a Unimation PUMA 560 robot and various external sensors. These include overhead vision, eye-in-hand vision, proximity, tactile array, position, force/torque, cross-fire, overload, and slip-sensing devices. The efficient fusion of data from different sources will enable the machine to respond promptly in dealing with the 'real world'. Towards this goal, the general paradigm of a sensor data fusion system has been developed, and some simulation results, as well as results from the actual implementation of certain concepts of sensor data fusion, have been demonstrated.&lt;<ETX>&gt;</ETX>",robotics
10.1109/ICRA40945.2020.9197209,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning,https://ieeexplore.ieee.org/document/9197209/,"The challenges of multi-robot navigation in dynamic environments lie in uncertainties in obstacle complexities, partially observation of robots, and policy implementation from simulations to the real world. This paper presents a cooperative approach to address the multi-robot navigation problem (MRNP) under dynamic environments using a deep reinforcement learning (DRL) framework, which can help multiple robots jointly achieve optimal paths despite a certain degree of obstacle complexities. The novelty of this work includes threefold: (1) developing a cooperative architecture that robots can exchange information with each other to select the optimal target locations; (2) developing a DRL based framework which can learn a navigation policy to generate the optimal paths for multiple robots; (3) developing a training mechanism based on dynamics randomization which can make the policy generalized and achieve the maximum performance in the real world. The method is tested with Gazebo simulations and 4 differential drive robots. Both simulation and experiment results validate the superior performance of the proposed method in terms of success rate and travel time when compared with the other state-of-art technologies.",robotics
10.1109/CDC.2006.377499,to_check,Proceedings of the 45th IEEE Conference on Decision and Control,IEEE,2006-12-15 00:00:00,ieeexplore,Path Generation Using Matrix Representations of Previous Robot State Data,https://ieeexplore.ieee.org/document/4178112/,"Humans learn by repetition and using past experiences. It is possible for robots to act in a similar fashion. By representing past path traversal experiences with matrices, a new path can be generated without relying on calculations of complex dynamics or control laws. This paper presents one approach for allowing robots to use past experience to generate new paths and control actions. This approach relies on using several matrices to associate each new input value with previous robot states. An example is provided and analyzed which shows a successful simulated implementation of this approach. In addition a real world test of the approach was conducted which demonstrates that the implementation not only generates new paths, but does so fast enough to be feasible for real time systems",robotics
10.1109/CBS46900.2019.9114416,to_check,2019 IEEE International Conference on Cyborg and Bionic Systems (CBS),IEEE,2019-09-20 00:00:00,ieeexplore,"Motion Prediction of Virtual Patterns, Human Hand Motions, and a simplified Hand Manipulation Task with Hierarchical Temporal Memory",https://ieeexplore.ieee.org/document/9114416/,"In this paper we utilize Numenta's Hierarchical Temporal Memory implementation NuPIC for online visual motion pattern prediction and test its performance on virtual animations as well as real world human motion data. For evaluation we run a series of progressively more complex experiments testing specific capabilities: Prediction of fixed-time noise-free motion animations, prediction of protocol-directed tasks with real-world camera captured human motion data, and lastly prediction of repetitive tasks performed without a strict protocol. Results show that the presented setup is able to predict time sequenced images as well as highly variable human motions increasingly well over several iterations. Limits are faced for non sequential variable hand motion execution: Here, predictions are made but do not improve in quality over time. The network runs online in real time and can be transferred to different tasks without expert knowledge. These characteristics qualify the setup for human robot interaction scenarios without the need for verified prediction accuracy.",robotics
10.1016/j.neunet.2013.01.019,to_check,Neural Networks,scopus,2013-11-01,sciencedirect,Realtime cerebellum: A large-scale spiking network model of the cerebellum that runs in realtime using a graphics processing unit,https://api.elsevier.com/content/abstract/scopus_id/84884150994,"The cerebellum plays an essential role in adaptive motor control. Once we are able to build a cerebellar model that runs in realtime, which means that a computer simulation of 1 s in the simulated world completes within 1 s in the real world, the cerebellar model could be used as a realtime adaptive neural controller for physical hardware such as humanoid robots. In this paper, we introduce “Realtime Cerebellum (RC)”, a new implementation of our large-scale spiking network model of the cerebellum, which was originally built to study cerebellar mechanisms for simultaneous gain and timing control and acted as a general-purpose supervised learning machine of spatiotemporal information known as reservoir computing, on a graphics processing unit (GPU). Owing to the massive parallel computing capability of a GPU, RC runs in realtime, while reproducing qualitatively the same simulation results of the Pavlovian delay eyeblink conditioning with the previous version. RC is adopted as a realtime adaptive controller of a humanoid robot, which is instructed to learn a proper timing to swing a bat to hit a flying ball online. These results suggest that RC provides a means to apply the computational power of the cerebellum as a versatile supervised learning machine towards engineering applications.",robotics
10.1109/HPCC/SmartCity/DSS.2019.00281,to_check,2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),IEEE,2019-08-12 00:00:00,ieeexplore,Asynchronous Block Coordinate Descent Method for Large-Scale Nonconvex Problem in Real World Study,https://ieeexplore.ieee.org/document/8855671/,"Here comes the era of big data in healthcare, and this era will transform medicine and especially oncology. In the last decade, there has been a growing interest in the potential benefits and the relevance of real world studies (RWS) with to the rapid development of data mining algorithms, as well as large-scale accumulated real world datasets. However, it is beyond the ability of traditional data mining algorithms to solve such large-scale RWS problems due to the large amount of data need to be processed and the difficulty in the problems solving, especially the nonconvex optimization problems. In many cases for these nonconvex problems, the goal is to find a reasonable local minimum, and the main concern is that the iterative updates are easily trapped in saddle points or bad local optima points. In this paper, we aim at minimizing the sum of a smooth nonlinear function and a block-separable nonconvex function involving a large number of block variables subjected to inequality constraints, which covers a wide range of interesting applications appearing in distributed optimization, statistics learning, etc. However, solving such a nonconvex nonlinear optimization problem associated with inequality constraints remains as a challenging task. This is because: 1) it is still an open problem about how to escape from saddle points and bad local optima points using a block coordinate descent method to deal with the nonconvex nonlinear optimization functions; 2) it can be extremely expensive to compute the entire block variables when the problem scale is large. Therefore, we propose a novel parallel first-order optimization method, called as Asynchronous block coordinate descent with Time Perturbation (ATP), which utilizing the time perturbation technique that escapes from saddle points and local optima points. We present a randomized block variable selection rule of the proposed method, as well as the iteration complexity. Experiments conducted on typical machine learning problems in RWS, i.e., folded concave linear regression, validate the efficacy of our optimization method. In particular, the experiment results demonstrate that time perturbation could greatly help the proposed algorithm escape from saddle points and local optima in the nonconvex part, which provides a promising way to tackle nonconvex optimization problems employing block coordinate descent.",health
10.1109/ICIP40778.2020.9191024,to_check,2020 IEEE International Conference on Image Processing (ICIP),IEEE,2020-10-28 00:00:00,ieeexplore,A GAN Based Multi-Contrast Modalities Medical Image Registration Approach,https://ieeexplore.ieee.org/document/9191024/,"Most current multi modalities medical image registration approaches are concerned about registering one modality image to another. However, in the real world, medical image registration may be involved in multiple modes, not just two specific modalities. To this end, we propose a multi-contrast modalities medical image registration modal (Star-Reg net). It uses a single generator and discriminator for all contrasts of registrations amount several modalities. Furthermore, the proposed approach is trained in an unsupervised way, which alleviates the requirement of manual annotation data. The experiment on the IXI dataset demonstrates the Star-Reg net effectiveness in multi-contrast modalities medical image registration.",health
10.1109/BIBM.2013.6732766,to_check,2013 IEEE International Conference on Bioinformatics and Biomedicine,IEEE,2013-12-21 00:00:00,ieeexplore,An experimental research for automatic classification of unbalanced single-channel protein sub-cellular location fluorescence image set,https://ieeexplore.ieee.org/document/6732766/,"To Model the protein sub-cellular localization pattern which responds to drugs' treatment is an important problem for medical applications named as high content screening (HCS) with bio-imaging and machine learning. Traditionally, at least, three channels' images have to be retrieved for auto-focusing and segmentation on DNA channel, background correction on auto-fluorescence channel and protein sub-cellular localization analysis on GFP channel which is a time consuming and error accumulating procedure. An automatic classification of Single-channel Protein Sub-cellular Location Fluorescence Images without segmentation is desired for speeding up the pattern analysis. But in the real world, the data imbalance often occurred and affected the classification accuracy. By now there are a variety of approaches proposed for improved the classification accuracy including the re-sampling and reweighting. This experiment engaged in comparing existed solutions for further research.",health
10.1109/ICRTIT.2013.6844173,to_check,2013 International Conference on Recent Trends in Information Technology (ICRTIT),IEEE,2013-07-27 00:00:00,ieeexplore,Clustering of lung cancer data using Foggy K-means,https://ieeexplore.ieee.org/document/6844173/,"In the medical field, huge data is available, which leads to the need of a powerful data analysis tool for extraction of useful information. Several studies have been carried out in data mining field to improve the capability of data analysis on huge datasets. Cancer is one of the most fatal diseases in the world. Lung Cancer with high rate of accurance is one of the serious problems and biggest killing disease in India. Prediction of occurance of the lung cancer is very difficult because it depends upon multiple attributes which could not be analyzedeasily. In this paper a real time lung cancer dataset is taken from SGPGI (Sanjay Gandhi Post Graduate Institute of Medical Sciences) Lucknow. A realtime dataset is always associated with its obvious challenges such as missing values, highly dimensional, noise, and outlier, which is not suitable for efficient classification. A clustering approach is an alternative solution to analyze the data in an unsupervised manner. In this current research work main focus is to develop a novel approach to create accurate clusters of desired real time datasets called Foggy K-means clustering. The result of the experiment indicates that foggy k-means clustering algorithm gives better result on real datasets as compared to simple k-means clustering algorithm and provides a better solution to the real world problem.",health
10.1109/TCSS.2014.2377811,to_check,IEEE Transactions on Computational Social Systems,IEEE,2014-06-01 00:00:00,ieeexplore,Behavioral Analysis of Insider Threat: A Survey and Bootstrapped Prediction in Imbalanced Data,https://ieeexplore.ieee.org/document/7010900/,"The problem of insider threat is receiving increasing attention both within the computer science community as well as government and industry. This paper starts by presenting a broad, multidisciplinary survey of insider threat capturing contributions from computer scientists, psychologists, criminologists, and security practitioners. Subsequently, we present the behavioral analysis of insider threat (BAIT) framework, in which we conduct a detailed experiment involving 795 subjects on Amazon Mechanical Turk (AMT) in order to gauge the behaviors that real human subjects follow when attempting to exfiltrate data from within an organization. In the real world, the number of actual insiders found is very small, so supervised machine-learning methods encounter a challenge. Unlike past works, we develop bootstrapping algorithms that learn from highly imbalanced data, mostly unlabeled, and almost no history of user behavior from an insider threat perspective. We develop and evaluate seven algorithms using BAIT and show that they can produce a realistic (and acceptable) balance of precision and recall.",industry
http://arxiv.org/abs/2007.09881v1,to_check,arxiv,arxiv,2020-07-20 04:17:30+00:00,arxiv,"DeepCO: Offline Combinatorial Optimization Framework Utilizing Deep
  Learning",http://arxiv.org/abs/2007.09881v1,"Combinatorial optimization serves as an essential part in many modern
industrial applications. A great number of the problems are offline setting due
to safety and/or cost issues. While simulation-based approaches appear
difficult to realise for complicated systems, in this research, we propose
DeepCO, an offline combinatorial optimization framework utilizing deep
learning. We also design an offline variation of Travelling Salesman Problem
(TSP) to model warehouse operation sequence optimization problem for
evaluation. With only limited historical data, novel proposed distribution
regularized optimization method outperforms existing baseline method in offline
TSP experiment reducing route length by 5.7% averagely and shows great
potential in real world problems.",industry
10.1016/j.conengprac.2021.104957,to_check,Control Engineering Practice,scopus,2022-01-01,sciencedirect,A flexible manufacturing assembly system with deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85117832781,"Traditional assembly line requires a significant amount of designs from engineers, especially in the case of multi-species and small-lot production. Recently, intelligent algorithms based on reinforcement learning are proposed to address this issue. However, the lower success rate and safety reasons limit their industrial applications. In this article, we proposed a systematic solution, including the automatic planning of assembly motions and the monitoring system of the production lines. In the planning stage, we built the digital twin model of the assembly line, then trained a deep reinforcement learning agent to assembly the workpieces. In the production stage, the digital twin model is used to monitor the assembly lines and predict failures. To validate the system we proposed, we conducted a peg-in-hole assembly experiment, and reached a 90% success rate for a single assembly attempt. During the whole experiment, no collision happens in the real world.",industry
10.1109/ICIST52614.2021.9440573,to_check,2021 11th International Conference on Information Science and Technology (ICIST),IEEE,2021-05-23 00:00:00,ieeexplore,Spatiotemporal Multi-Graph Convolutional Network for Taxi Demand Prediction,https://ieeexplore.ieee.org/document/9440573/,"Taxi demand prediction plays an important role in ITS (Intelligent Transportation System). This task is challenging due to the complex spatiotemporal correlations and semantic trends between different locations. Existing work tried to solve this problem by exploiting a variety of spatiotemporal models based on deep learning. However, we observe that more semantic pair-wise correlations among possibly distant roads are also critical for taxi demand prediction. To combine the spatiotemporal correlations with semantic correlations in the traffic network, this paper proposed an end-to-end framework called DeepTDP. First, we defined five kinds of spatial and semantic correlations, which are modeled into multi location graphs and fused by multi-graph convolutional network. Second, LSTM in encoder-decoder network is utilized to capture temporal correlation between future taxi demand values. Besides, a cross-entropy loss function based on error correction is designed to generate taxi demand predictions. Third, we apply a word embedding technique to reduce the dimension of decoded vector in output layer. Finally, we evaluate DeepTDP on two real world traffic datasets, the experiment results demonstrate effectiveness of our approach in comparison with variants of self and other baselines.",smart cities
http://arxiv.org/abs/1802.02203v4,to_check,arxiv,arxiv,2018-01-23 08:21:04+00:00,arxiv,"Automatic construction of Chinese herbal prescription from tongue image
  via CNNs and auxiliary latent therapy topics",http://arxiv.org/abs/1802.02203v4,"The tongue image provides important physical information of humans. It is of
great importance for diagnoses and treatments in clinical medicine. Herbal
prescriptions are simple, noninvasive and have low side effects. Thus, they are
widely applied in China. Studies on the automatic construction technology of
herbal prescriptions based on tongue images have great significance for deep
learning to explore the relevance of tongue images for herbal prescriptions, it
can be applied to healthcare services in mobile medical systems. In order to
adapt to the tongue image in a variety of photographic environments and
construct herbal prescriptions, a neural network framework for prescription
construction is designed. It includes single/double convolution channels and
fully connected layers. Furthermore, it proposes the auxiliary therapy topic
loss mechanism to model the therapy of Chinese doctors and alleviate the
interference of sparse output labels on the diversity of results. The
experiment use the real world tongue images and the corresponding prescriptions
and the results can generate prescriptions that are close to the real samples,
which verifies the feasibility of the proposed method for the automatic
construction of herbal prescriptions from tongue images. Also, it provides a
reference for automatic herbal prescription construction from more physical
information.",smart cities
10.1109/AIVR50618.2020.00046,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,Avatars rendering and its effect on perceived realism in Virtual Reality,https://ieeexplore.ieee.org/document/9319086/,"Immersive virtual environments have proven to be a plausible platform to be used by multiple disciplines to simulate different types of scenarios and situations at a low cost. When participants are immersed in a virtual environment experience presence, they are more likely to behave as if they were in the real world. Improving the level of realism should provide a more compelling scenario so that users will experience higher levels of presence, and thus be more likely to behave as if they were in the real world. This paper presents preliminary results of an experiment in which participants navigate through two versions of the same scenario with different levels of realism of both the environment and the avatars. Our current results, from a between subjects experiment, show that the reported levels of quality in the visualization are not significantly different, which means that other aspects of the virtual environment and/or avatars must be taken into account in order to improve the perceived level of realism.",multimedia
10.1109/SOLI48380.2019.8955074,to_check,"2019 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)",IEEE,2019-11-08 00:00:00,ieeexplore,3D Deep Learning for 3D Printing of Tooth Model,https://ieeexplore.ieee.org/document/8955074/,"Recently, with the development of both 3D sensors and 3D virtual network that bring the needs of interaction with the real world, many 3D applications burst out. However, it is difficult to understanding these three-dimensional scenes with a fixed program. Then, a data-driven method is required to process these 3D data, which brings a strong demand of 3D Deep Learning in 3D data. Towards this goal, with an end-to-end deep learning, the experiment is based on PointNet++, a well proposed method for feature extraction. The experiment optimizes the network structure and parameters to improve the classification results. Finally, the network is applied to tooth model for classification and identification so that the dental model can be found from different perspectives.",multimedia
10.1109/MLSP52302.2021.9596140,to_check,2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP),IEEE,2021-10-28 00:00:00,ieeexplore,Self-Trained Video Anomaly Detection Based on Teacher-Student Model,https://ieeexplore.ieee.org/document/9596140/,"Anomaly detection in videos is a challenging problem in computer vision. Most existing methods need supervised information to train their models, which limits their applications in real world scenario. Therefore, self-trained methods which do not need manually labels receive increasing attentions recently. In this paper, we propose a novel self-trained video anomaly detection method based on teacher-student model. The teacher-student architecture can significantly improve the performance of self-trained video anomaly detection by utilizing the unlabeled samples. We test our method on two surveillance datasets. Experiment results show that our method achieves better performance than state-of-the-art unsupervised methods on both datasets and achieves comparable performance as semi-supervised methods, which experimentally proves the effectiveness of our method.",multimedia
10.1109/ICCCI.2014.6921771,to_check,2014 International Conference on Computer Communication and Informatics,IEEE,2014-01-05 00:00:00,ieeexplore,Efficient Approximate Membership Localization using P-Prune algorithm in blogs,https://ieeexplore.ieee.org/document/6921771/,"Approximate Membership Localization (AML) is concerned with locating non-overlapped substrings thus avoiding redundancies. This overcomes the drawback of Approximate Membership Extraction (AME) process which has low efficiency for real world application. An algorithm called P-Prune is used in Blog search. This prunes most of the overlapped redundant substrings before generating them. Here we use the opinion retrieval scheme which analyses the viewers' comments on Blog contents. Our experimental study on blogs reveals the efficiency of P-Prune over AME method. We also work AML in application to a proposed Blog search framework, a search-based approach joining two tables using dictionary-based entity recognition from blogs. Apart from the advantage of AML over AME, the experiment also proves the efficiency of the search-based approach. This algorithm can be extended to video blogs (vlog) also.",multimedia
10.1109/IVS.2019.8814117,to_check,2019 IEEE Intelligent Vehicles Symposium (IV),IEEE,2019-06-12 00:00:00,ieeexplore,Virtual World Bridges the Real Challenge: Automated Data Generation for Autonomous Driving,https://ieeexplore.ieee.org/document/8814117/,"In autonomous driving research, one of the bottlenecks is the shortage of a well-annotated dataset to train deep neural networks for object detection. Specifically, a dataset focusing on harsh weather conditions is insufficient. The purpose of this research is to explore the power of utilizing synthetic data for training object detection deep neural networks under harsh weather conditions. We introduce a state-of-the-art automated pipeline to collect synthetic images from a high realism video game and generate training data which can be used for training an autonomous driving object detection neural network. We use our synthetic dataset, KITTI, and Cityscapes to train three separate object detection neural networks and employ the PASCAL object detection criteria to evaluate each neural networks' performance. The results from the experiment indicate that the neural network trained by our synthetic dataset outperforms its counterparts and achieves higher average precision (AP) in detecting images under harsh weather conditions. The result sheds a light on employing synthetic data to resolve the challenges in the real world.",multimedia
10.1109/IST48021.2019.9010169,to_check,2019 IEEE International Conference on Imaging Systems and Techniques (IST),IEEE,2019-12-10 00:00:00,ieeexplore,Classification System with Capability to Reject Unknowns,https://ieeexplore.ieee.org/document/9010169/,"In this paper, we propose a novel method for object classification with capability to reject unknown inputs. In the real world application such as an image-recognition-based checkout system, it is crucial to reject unknown inputs while correctly classifying registered objects. Conventional deep-learning-based classification systems with softmax output suffer from overconfident score on unknown objects. We tackled the problem by the following two approaches. First, we incorporated a metric-learning-based method proposed for face verification into object classification. Second, we utilize available unregistered objects (known unknowns) in the training phase by proposing a novel “Margined Unknown Loss”. In the experiment, we showed the effectiveness of the proposed method by confirming that it outperformed conventional softmax-based approaches which also use the known unknowns, on two datasets, MNIST dataset and a retail product dataset, in terms of Recall at a low false positive rate.",multimedia
10.1109/SII.2013.6776750,to_check,Proceedings of the 2013 IEEE/SICE International Symposium on System Integration,IEEE,2013-12-17 00:00:00,ieeexplore,Learning and association of synaesthesia phenomenon using deep neural networks,https://ieeexplore.ieee.org/document/6776750/,"Robots are required to process multimodal information because the information in the real world comes from various modal inputs. However, there exist only a few robots integrating multimodal information. Humans can recognize the environment effectively by cross-modal processing. We focus on modeling synaesthesia phenomenon known to be a cross-modal perception of humans. Recently, deep neural networks (DNNs) have gained more attention and successfully applied to process high-dimensional data composed not only of single modality but also of multimodal information. We introduced DNNs to construct multimodal association model which can reconstruct one modality from the other modality. Our model is composed of two DNNs: one for image compression and the other for audio-visual sequential learning. We tried to reproduce synaesthesia phenomenon by training our model with the multimodal data acquired from psychological experiment. Cross-modal association experiment showed that our model can reconstruct the same or similar images from sound as synaesthetes, those who experience synaesthesia. The analysis of middle layers of DNNs representing multimodal features implied that DNNs self-organized the difference of perception between individual synaesthetes.",multimedia
10.1109/ICASSP.2018.8461837,to_check,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2018-04-20 00:00:00,ieeexplore,A Joint Multi-Task Learning Framework for Spoken Language Understanding,https://ieeexplore.ieee.org/document/8461837/,"Spoken language understanding (SLU), which mainly involves intent prediction and slot filling, is a core component of a spoken dialogue system. Usually, intent determination and slot filling are carried out independently. Recently, joint learning of intent determination and slot filling has been proved effective in SLU. In this paper, we propose a novel joint multi-task learning framework for SLU, which predicts user intent and slot label via shared LSTM architecture, as well as next word's part of speech (POS) via neural language model. The proposed model exploits the correlation among different tasks and makes full advantage of all supervised signals. We conduct experiments on popular benchmark ATIS dataset, which consists of rich dialogues collected from real world. The experiment results show that our model achieves state-of-the-art in terms of several popular metrics.",multimedia
http://arxiv.org/abs/2108.10972v1,to_check,arxiv,arxiv,2021-08-24 22:02:27+00:00,arxiv,Domain Adaptation for Real-World Single View 3D Reconstruction,http://arxiv.org/abs/2108.10972v1,"Deep learning-based object reconstruction algorithms have shown remarkable
improvements over classical methods. However, supervised learning based methods
perform poorly when the training data and the test data have different
distributions. Indeed, most current works perform satisfactorily on the
synthetic ShapeNet dataset, but dramatically fail in when presented with real
world images. To address this issue, unsupervised domain adaptation can be used
transfer knowledge from the labeled synthetic source domain and learn a
classifier for the unlabeled real target domain. To tackle this challenge of
single view 3D reconstruction in the real domain, we experiment with a variety
of domain adaptation techniques inspired by the maximum mean discrepancy (MMD)
loss, Deep CORAL, and the domain adversarial neural network (DANN). From these
findings, we additionally propose a novel architecture which takes advantage of
the fact that in this setting, target domain data is unsupervised with regards
to the 3D model but supervised for class labels. We base our framework off a
recent network called pix2vox. Results are performed with ShapeNet as the
source domain and domains within the Object Dataset Domain Suite (ODDS) dataset
as the target, which is a real world multiview, multidomain image dataset. The
domains in ODDS vary in difficulty, allowing us to assess notions of domain gap
size. Our results are the first in the multiview reconstruction literature
using this dataset.",multimedia
http://arxiv.org/abs/2004.00137v1,to_check,arxiv,arxiv,2020-03-31 22:02:38+00:00,arxiv,Revisiting Few-shot Activity Detection with Class Similarity Control,http://arxiv.org/abs/2004.00137v1,"Many interesting events in the real world are rare making preannotated
machine learning ready videos a rarity in consequence. Thus, temporal activity
detection models that are able to learn from a few examples are desirable. In
this paper, we present a conceptually simple and general yet novel framework
for few-shot temporal activity detection based on proposal regression which
detects the start and end time of the activities in untrimmed videos. Our model
is end-to-end trainable, takes into account the frame rate differences between
few-shot activities and untrimmed test videos, and can benefit from additional
few-shot examples. We experiment on three large scale benchmarks for temporal
activity detection (ActivityNet1.2, ActivityNet1.3 and THUMOS14 datasets) in a
few-shot setting. We also study the effect on performance of different amount
of overlap with activities used to pretrain the video classification backbone
and propose corrective measures for future works in this domain. Our code will
be made available.",multimedia
10.1109/ICDMW.2009.26,to_check,2009 IEEE International Conference on Data Mining Workshops,IEEE,2009-12-06 00:00:00,ieeexplore,An Effective Network Partitioning Algorithm Based on Two-Point Diffusing Strategy,https://ieeexplore.ieee.org/document/5360433/,"The network modeling and analysis have played important roles in fields of physics, sociology, biology, and computer science. Recently, community structure has been considered as an important character for complex networks, and its detection can bring great benefit in real world affairs. In the paper, a new heuristic algorithm based on two-point diffusing strategy is proposed. At first, two pseudo-core points are identified according to the clue of the longest path in a network. Then, two embryonic communities and an undecided node set are generated through performing diffusing operation on such two points. Subsequently, an experience rule is used to classify the undecided nodes to form the final community structure. In addition, the effectiveness and efficiency are validated by comparison experiments with four real-world networks. The experiment results show that our TPD algorithm can yield better community partition results and shorter computing time than the existing classical community detecting algorithms.",science
10.1109/ICCT.2018.8600029,to_check,2018 IEEE 18th International Conference on Communication Technology (ICCT),IEEE,2018-10-11 00:00:00,ieeexplore,A Social Bots Detection Model Based on Deep Learning Algorithm,https://ieeexplore.ieee.org/document/8600029/,"With the development of the Internet, social bots are increasingly spreading on social platforms. Therefore, an effective detection algorithm is demanded to detect these social bot accounts that endanger social networks. In this paper, a social bots detection model based on deep learning algorithm (DeBD) is proposed. The model mainly includes three layers. The first layer is the joint content feature extraction layer, which focuses on the feature extraction of the tweets content and the relationship between them. The second layer is the tweet metadata temporal feature extraction layer, which regards the tweet metadata as temporal information and uses this temporal information as the input of the LSTM to extract the user social activity temporal feature. The third layer is the feature fusing layer, which fuses the extracted joint content features with the temporal features to detect social bots. To evaluate the effectiveness of the DeBD model, we conducted experiments on three different types of new social bot data sets from the real world and the experiment results also demonstrate the effectiveness of our proposed model.",science
10.1145/3341161.3342939,to_check,2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),IEEE,2019-08-30 00:00:00,ieeexplore,Strengthening Social Networks Analysis by Networks Fusion,https://ieeexplore.ieee.org/document/9073317/,"The relationship extraction and fusion of networks are the hotspots of current research in social network mining. Most previous work is based on single-source data. However, the relationships portrayed by single-source data are not sufficient to characterize the relationships of the real world. To solve this problem, a Semi-supervised Fusion framework for Multiple Network (SFMN), using gradient boosting decision tree algorithm (GBDT) to fuse the information of multi-source networks into a single network, is proposed in this paper. Our framework aims to take advantage of multi-source networks fusion to enhance the accuracy of the network construction. The experiment shows that our method optimizes the structural and community accuracy of social networks which makes our framework outperforms several state-of-the-art methods.",science
10.1109/SMC.2017.8122654,to_check,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",IEEE,2017-10-08 00:00:00,ieeexplore,Implementation of human-robot VQA interaction system with dynamic memory networks,https://ieeexplore.ieee.org/document/8122654/,"One of the major functions of intelligent robots such as social or home service robots is to interact with users in natural language. Moving on from simple conversation or retrieval of data stored in computer memory, we present a new Human-Robot Interaction (HRI) system which can understand and reason over environment around the user and provide information about it in a natural language. For its intelligent interaction, we integrated Dynamic Memory Networks (DMN), a deep learning network for Visual Question Answering (VQA). For its hardware, we built a robotic head platform with a tablet PC and a 3 DOF neck. Through an experiment where the user and the robot had question answering interaction in our customized environment and in real time, the feasibility our proposed system was validated, and the effectiveness of deep learning application in real world as well as a new insight on human robot interaction was demonstrated.",science
10.1109/ICTAI.2015.85,to_check,2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2015-11-11 00:00:00,ieeexplore,Policies for Contextual Bandit Problems with Count Payoffs,https://ieeexplore.ieee.org/document/7372181/,"The contextual bandit problem has been of major interest in the last few years. This corresponds to a sequential decision process where an agent has to choose at each iteration an action to perform, according to some knowledge about the decision environment and the current available actions, with the aim to maximize a cumulative amount of rewards over time. Many instances of the problem exist, depending on the kind of rewards we collect - real, binary, natural - and various algorithms are known to be efficient for some of these instances, either empirically or theoretically. In this paper we focus on the case of count payoffs, which corresponds to bandit problems where rewards are integer rewards, potentially unbounded. Based on a Bayesian Poisson regression model, we propose two new contextual bandit algorithms for this particular case with several concrete applications in real life: an Upper Confidence Bound algorithm and a Thompson Sampling strategy. Our approaches present the advantage to remain analytically tractable and computationally efficient. We experiment the algorithms on both simulated data and a real world scenario of spread maximization on a social network.",science
10.1109/ACCESS.2020.2979012,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Dynamic Bayesian Network Approach for Analysing Topic-Sentiment Evolution,https://ieeexplore.ieee.org/document/9026898/,"Sentiment analysis is one of the key tasks of natural language understanding. Sentiment Evolution models the dynamics of sentiment orientation over time. It can help people have a more profound and deep understanding of opinion and sentiment implied in user generated content. Existing work mainly focuses on sentiment classification, while the analysis of how the sentiment orientation of a topic has been influenced by other topics or the dynamic interaction of topics from the aspect of sentiment has been ignored. In this paper, we propose to construct a Gaussian Process Dynamic Bayesian Network to model the dynamics and interactions of the sentiment of topics on social media such as Twitter. We use Dynamic Bayesian Networks to model time series of the sentiment of related topics and learn relationships between them. The network model itself applies Gaussian Process Regression to model the sentiment at a given time point based on related topics at previous time. We conducted experiments on a real world dataset that was crawled from Twitter with 9.72 million tweets. The experiment demonstrates a case study of analysing the sentiment dynamics of topics related to the event Brexit.",science
10.1007/s42979-021-00864-6,to_check,SN Computer Science,Nature,2021-09-18 00:00:00,springer,Explaining Causal Influence of External Factors on Incidence Rate of Covid-19,https://www.nature.com/articles/s42979-021-00864-6,"Classical susceptible–infected–removed model with constant transmission rate and removal rate may not capture real world dynamics of epidemic due to complex influence of multiple external factors on the spread and spatio-temporal variation of transmission rate. Also, explainability of a model is of prime necessity to understand the influence of multiple factors on transmission rate. Thus, we modified discrete global susceptible–infected–removed model with time-varying transmission rate, recovery rate and multiple spatially local models. We have derived the criteria for disease-free equilibrium within a specific time period. A convolutional LSTM model is created and trained to map multiple spatiotemporal features to transmission rate. The model achieved 8.39% mean absolute percent error in terms of cumulative infection cases in each locality in a region in USA for a 10-day prediction period. Comparison with current state of the art methods reveals performance superiority of the proposed method. A perturbation-based spatio-temporal model interpretation method is proposed which generates spatio-temporal local interpretations. Global interpretations are generated by statistically accumulating the local interpretations. Global interpretations of transmission rate for a region in USA shows consistent positive influence of population density, whereas, temperature and humidity have very minor influence. An experiment with what-if scenario reveals locality specific quick identification of positive cases, rapid isolation and improving healthcare facilities are keys to rapid convergence to disease-free equilibrium. A long-term forecasting of 160 days predicts new infection cases in a region in USA with a median error of 455 cases.",science
10.1016/j.ijhcs.2018.12.011,to_check,International Journal of Human Computer Studies,scopus,2019-05-01,sciencedirect,UrbanSocialRadar: A place-aware social matching model for estimating serendipitous interaction willingness in Korean cultural context,https://api.elsevier.com/content/abstract/scopus_id/85059456392,"In the era of perpetual digital connectedness, information and communication technology has significantly altered the way people communicate and interact with each other. Nonetheless, the computer-mediated communication should only complement offline communication rather than substituting it, as the resultant online ties are not as strong as face-to-face ties. In an effort to understand the motives in making offline social interactions real and ultimately to predict willingness to engage in serendipitous interactions with people encountered in a public place, we propose a place-aware social matching model driven by interpersonal factors (i.e., similarity, complementarity, and intimacy) and socio-spatial factors (i.e., place sociability, information acquisition expectancy, and perceived personal space in a place). Through a web-based social matching survey experiment (N = 1139 matches from 99 participants in Korea) based on a bogus stranger paradigm, we examine the interrelationship between those factors and the interaction willingness using a series of multiple regression analyses and build a prediction model by devising predictive features based on several machine learning models. From this, we find that both factors have statistically significant influence on interaction willingness, yet interpersonal factors have a higher relative importance than the socio-spatial factors. The interesting point is that the predictive power of these factors varies according to the place characteristics and the level of interaction willingness. We also empirically test the predictability of the model built from the controlled lab experiment through real-world experiments. The results reveal that the proposed model predicts interaction willingness in a real world with under 21% error rate within the Korean cultural context. Findings have implications for the design of mobile social networking systems that endeavor to facilitate serendipitous interactions.",science
10.1109/RCAR52367.2021.9517387,to_check,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),IEEE,2021-07-19 00:00:00,ieeexplore,A Depthwise Separable Convolution Based 6D Pose Estimation Network by Efficient 2D-3D Feature Fusion,https://ieeexplore.ieee.org/document/9517387/,"Precise 6D pose estimation of the target object is an essential prerequisite for robots to understand the real world. Previous 6D pose estimation methods based on 3D data usually have problems such as long model training time, imperfect feature extraction, redundant network model parameters, and complicated follow-up processing steps. This paper proposes a 2D-3D feature fusion module that could enhance feature extraction for the 6D pose estimation network. Furthermore, we compress the size of model parameters by adopting depthwise separable convolution to accelerate training speed and to reduce memory consumption. The experiment results on LineMOD dataset show the effectiveness of our method. Our method achieves on par or better performance than the state-of-art methods for 6D pose estimation and reduces model training time and the number of model parameters simultaneously.",robotics
10.1109/TASE.2020.3043636,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2021-07-01 00:00:00,ieeexplore,An Ergodic Measure for Active Learning From Equilibrium,https://ieeexplore.ieee.org/document/9312988/,"This article develops KL-ergodic exploration from equilibrium (KL-E<sup>3</sup>), a method for robotic systems to integrate stability into actively generating informative measurements through ergodic exploration. Ergodic exploration enables robotic systems to indirectly sample from informative spatial distributions globally, avoiding local optima, and without the need to evaluate the derivatives of the distribution against the robot dynamics. Using a hybrid systems theory, we derive a controller that allows a robot to exploit equilibrium policies (i.e., policies that solve a task) while allowing the robot to explore and generate informative data using an ergodic measure that can extend to high-dimensional states. We show that our method is able to maintain Lyapunov attractiveness with respect to the equilibrium task while actively generating data for learning tasks such, as Bayesian optimization, model learning, and off-policy reinforcement learning. In each example, we show that our proposed method is capable of generating an informative distribution of data while synthesizing smooth control signals. We illustrate these examples using simulated systems and provide simplification of our method for real-time online learning in robotic systems. <italic>Note to Practitioners</italic>—Robotic systems need to adapt to sensor measurements and learn to exploit an understanding of the world around them such that they can truly begin to experiment in the real world. Standard learning methods do not have any restrictions on how the robot can explore and learn, making the robot dynamically volatile. Those that do are often too restrictive in terms of the stability of the robot, resulting in a lack of improved learning due to poor data collection. Applying our method would allow robotic systems to be able to adapt online without the need for human intervention. We show that considering both the dynamics of the robot and the statistics of where the robot has been, we are able to naturally encode where the robot needs to explore and collect measurements for efficient learning that is dynamically safe. With our method, we are able to effectively learn while being energetically efficient compared with state-of-the-art active learning methods. Our approach accomplishes such tasks in a single execution of the robotic system, i.e., the robot does not need human intervention to reset it. Future work will consider multiagent robotic systems that actively learn and explore in a team of collaborative robots.",robotics
10.1109/LRA.2021.3116703,to_check,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,Learning Robot Exploration Strategy With 4D Point-Clouds-Like Information as Observations,https://ieeexplore.ieee.org/document/9555230/,"Being able to explore unknown environments is a requirement for fully autonomous robots. Many learning-based methods have been proposed to learn an exploration strategy. In the frontier-based exploration, learning algorithms tend to learn the optimal or near-optimal frontier to explore. Most of these methods represent the environments as fixed size images and take these as inputs to neural networks. However, the size of environments is usually unknown, which makes these methods fail to generalize to real world scenarios. To address this issue, we present a novel state representation method based on 4D point-clouds-like information, including the locations, frontier, and distance information. We also design a neural network that can process these 4D point-clouds-like information and generate the estimated value for each frontier. Then this neural network is trained using the typical reinforcement learning framework. We test the performance of our proposed method by comparing it with other five methods and test its scalability on maps that are much larger than maps in the training set. The experiment results demonstrate that our proposed method needs shorter average traveling distances to explore whole environments and can be adopted in maps with arbitrarily sizes.",robotics
10.1109/ICNSC.2009.4919323,to_check,"2009 International Conference on Networking, Sensing and Control",IEEE,2009-03-29 00:00:00,ieeexplore,Navigation and path search for roving robot using reinforcement learning,https://ieeexplore.ieee.org/document/4919323/,"The robot technology is rapidly developing in recent years. In connection with this technology, a robot activity is expected in various places or various environments. Therefore, this study describes 1) how the location of the destination of the robot in real world is measured based on the image obtained by one camera and 2) how the robot is navigated to the destination where a user points out on the display, on which the forward scene is imaged. The cases where there are some obstacles on the way to the destination are considered. The roving robot tries to find the shortest way to the destination based on the information on the locations of the obstacles and the destination by using the reinforcement learning, which is a hopeful candidate in the autonomous control technique. In addition, the measurement method for the indicated location based on the image is described, the simulation result for the path search by using the reinforcement leaning is shown, and the experiment result of navigation in real field is shown. Finally, the main conclusions are summarized.",robotics
10.1109/ICCAS.2008.4694375,to_check,"2008 International Conference on Control, Automation and Systems",IEEE,2008-10-17 00:00:00,ieeexplore,Development of Automatic Process Control system with simulation in PECVD system,https://ieeexplore.ieee.org/document/4694375/,"We develop the automatic process control system to maximize the number of available process chambers by controlling the time to start cleaning each chamber. Firstly, to achieve the purpose we build the controller model which creates and changes the pseudo RPSC counter properly by controlling other factors: unit priority, robot sequence etc. Secondly, for testing the control system we design one of the representative PECVD systems, and experiment the simulated system with the APC system and without it under various conditions. Finally, we optimize the system with the evolution strategy. As a result, not only major performance indicators; cycle time and throughput but also the variations of them have improved than before adapting the system. The contribution of this research is to present the practical solution concerned with various and complex issues in the real world.",robotics
10.1109/IROS.2010.5651719,to_check,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2010-10-22 00:00:00,ieeexplore,Learning interaction protocols using Augmented Baysian Networks applied to guided navigation,https://ieeexplore.ieee.org/document/5651719/,"Research in robot navigation usually concentrates on implementing navigation algorithms that allow the robot to navigate without human aid. In many real world situations, it is desirable that the robot is able to understand natural gestures from its user or partner and use this understanding to guide its navigation. Some algorithms already exist for learning natural gestures and/or their associated actions but most of these systems does not allow the robot to automatically generate the associated controller that allows it to actually navigate in the real environment. Furthermore, a technique is needed to combine the gestures/actions learned from interacting with multiple users or partners. This paper resolves these two issues and provides a complete system that allows the robot to learn interaction protocols and act upon them using only unsupervised learning techniques and enables it to combine the protocols learned from multiple users/partners. The proposed approach is general and can be applied to other interactive tasks as well. This paper also provides a real world experiment involving 18 subjects and 72 sessions that supports the ability of the proposed system to learn the needed gestures and to improve its knowledge of different gestures and their associations to actions over time.",robotics
10.1109/WCICA.2006.1713788,to_check,2006 6th World Congress on Intelligent Control and Automation,IEEE,2006-06-23 00:00:00,ieeexplore,Tele-Lightsaber - A Kind of Competitive Teleoperation<sup>*</sup>,https://ieeexplore.ieee.org/document/1713788/,"To improve the performance of telerobot system, which takes complex tasks with strong interaction under unexpected environment, a branch of teleoperation, competitive teleoperation, has been deeply studied. A kind of attack-defend competitive teleoperation experiment, which name Tele-Lightsaber (TLS), has been designed on TTRP (teleoperation/telegame robot platform) to imitate the action of two intelligent agents in real world. The design of TLS is presented in detail in this paper",robotics
10.1109/LRA.2021.3108510,to_check,IEEE Robotics and Automation Letters,IEEE,2021-10-01 00:00:00,ieeexplore,Online Learning of Unknown Dynamics for Model-Based Controllers in Legged Locomotion,https://ieeexplore.ieee.org/document/9525285/,"The performance of a model-based controller can severely suffer when its model inaccurately represents the real world dynamics. We propose to learn a time-varying, locally linear residual model along the robot's current trajectory, to compensate for the prediction errors of the controller's model. Supervised learning is performed online, as the robot is running in the unknown environment, using data collected from its immediate past. We theoretically investigate our method in its general formulation, then apply it to a bipedal controller derived from the full-order dynamics of virtual constraints, and a quadrupedal controller derived from a simplified model of contact forces. For a biped in simulation, our method consistently outperforms the baseline and a recent learning-based method. We also experiment with a 12 kg quadruped in simulation and real world, where the baseline fails to walk with 10 kg of payload but our method succeeds.",robotics
10.1109/ICMLA51294.2020.00201,to_check,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),IEEE,2020-12-17 00:00:00,ieeexplore,Defending Against Localized Adversarial Attacks on Edge-Deployed Monocular Depth Estimators,https://ieeexplore.ieee.org/document/9356303/,"Estimation of depth from a single image is an important scene understanding task in computer vision. With the advent of Deep Learning and Convolutional Neural Networks, staggeringly high accuracies have been achieved in this task. With advancements in model optimization, it has been possible to deploy these models on edge devices, allowing for efficient depth estimation in safety-critical applications in robots, rovers, drones and even self-driving vehicles. However, these models are susceptible to attacks from malicious adversaries, which aim to distort the output of the model for a seemingly clean image by adding minute perturbations. In the real-world scenario, the most plausible attack is the adversarial patch, which can be printed and used as a physical adversarial attack against Deep Learning models. In the case of Monocular Depth Estimation, we show that small adversarial patches, which range from 0.7% to 5% of the image size, greatly worsen model performance. It is thus essential that these models are made robust using defense mechanisms, to defend against malicious inputs while also not reducing performance on clean images. Moreover, it is essential that the defense mechanism be computationally efficient, for real-time inference on edge devices. In this work, we propose the first defense mechanism against adversarial patches for a regression network, in the context of Monocular Depth Estimation on an edge device. The defense mechanism adds very little overhead time of 38 milliseconds on a Raspberry Pi 3 Model B, maintaining performance on clean images while also achieving near clean image levels of performance on adversarial inputs.",autonomous vehicle
http://arxiv.org/abs/1712.04248v2,to_check,arxiv,arxiv,2017-12-12 11:36:26+00:00,arxiv,"Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box
  Machine Learning Models",http://arxiv.org/abs/1712.04248v2,"Many machine learning algorithms are vulnerable to almost imperceptible
perturbations of their inputs. So far it was unclear how much risk adversarial
perturbations carry for the safety of real-world machine learning applications
because most methods used to generate such perturbations rely either on
detailed model information (gradient-based attacks) or on confidence scores
such as class probabilities (score-based attacks), neither of which are
available in most real-world scenarios. In many such cases one currently needs
to retreat to transfer-based attacks which rely on cumbersome substitute
models, need access to the training data and can be defended against. Here we
emphasise the importance of attacks which solely rely on the final model
decision. Such decision-based attacks are (1) applicable to real-world
black-box models such as autonomous cars, (2) need less knowledge and are
easier to apply than transfer-based attacks and (3) are more robust to simple
defences than gradient- or score-based attacks. Previous attacks in this
category were limited to simple models or simple datasets. Here we introduce
the Boundary Attack, a decision-based attack that starts from a large
adversarial perturbation and then seeks to reduce the perturbation while
staying adversarial. The attack is conceptually simple, requires close to no
hyperparameter tuning, does not rely on substitute models and is competitive
with the best gradient-based attacks in standard computer vision tasks like
ImageNet. We apply the attack on two black-box algorithms from Clarifai.com.
The Boundary Attack in particular and the class of decision-based attacks in
general open new avenues to study the robustness of machine learning models and
raise new questions regarding the safety of deployed machine learning systems.
An implementation of the attack is available as part of Foolbox at
https://github.com/bethgelab/foolbox .",autonomous vehicle
http://arxiv.org/abs/2009.11722v1,to_check,arxiv,arxiv,2020-09-23 09:23:29+00:00,arxiv,"Cloud2Edge Elastic AI Framework for Prototyping and Deployment of AI
  Inference Engines in Autonomous Vehicles",http://arxiv.org/abs/2009.11722v1,"Self-driving cars and autonomous vehicles are revolutionizing the automotive
sector, shaping the future of mobility altogether. Although the integration of
novel technologies such as Artificial Intelligence (AI) and Cloud/Edge
computing provides golden opportunities to improve autonomous driving
applications, there is the need to modernize accordingly the whole prototyping
and deployment cycle of AI components. This paper proposes a novel framework
for developing so-called AI Inference Engines for autonomous driving
applications based on deep learning modules, where training tasks are deployed
elastically over both Cloud and Edge resources, with the purpose of reducing
the required network bandwidth, as well as mitigating privacy issues. Based on
our proposed data driven V-Model, we introduce a simple yet elegant solution
for the AI components development cycle, where prototyping takes place in the
cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment
and evaluation on the target ECUs (Electronic Control Units) is performed as
Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework
is demonstrated using two real-world use-cases of AI inference engines for
autonomous vehicles, that is environment perception and most probable path
prediction.",autonomous vehicle
http://arxiv.org/abs/2012.10706v4,to_check,arxiv,arxiv,2020-12-19 14:53:56+00:00,arxiv,Siamese Anchor Proposal Network for High-Speed Aerial Tracking,http://arxiv.org/abs/2012.10706v4,"In the domain of visual tracking, most deep learning-based trackers highlight
the accuracy but casting aside efficiency. Therefore, their real-world
deployment on mobile platforms like the unmanned aerial vehicle (UAV) is
impeded. In this work, a novel two-stage Siamese network-based method is
proposed for aerial tracking, \textit{i.e.}, stage-1 for high-quality anchor
proposal generation, stage-2 for refining the anchor proposal. Different from
anchor-based methods with numerous pre-defined fixed-sized anchors, our
no-prior method can 1) increase the robustness and generalization to different
objects with various sizes, especially to small, occluded, and fast-moving
objects, under complex scenarios in light of the adaptive anchor generation, 2)
make calculation feasible due to the substantial decrease of anchor numbers. In
addition, compared to anchor-free methods, our framework has better performance
owing to refinement at stage-2. Comprehensive experiments on three benchmarks
have proven the superior performance of our approach, with a speed of around
200 frames/s.",autonomous vehicle
http://arxiv.org/abs/2012.15717v2,to_check,arxiv,arxiv,2020-12-31 17:15:09+00:00,arxiv,"FGraDA: A Dataset and Benchmark for Fine-Grained Domain Adaptation in
  Machine Translation",http://arxiv.org/abs/2012.15717v2,"Previous research for adapting a general neural machine translation (NMT)
model into a specific domain usually neglects the diversity in translation
within the same domain, which is a core problem for domain adaptation in
real-world scenarios. One representative of such challenging scenarios is to
deploy a translation system for a conference with a specific topic, e.g.,
global warming or coronavirus, where there are usually extremely less resources
due to the limited schedule. To motivate wider investigation in such a
scenario, we present a real-world fine-grained domain adaptation task in
machine translation (FGraDA). The FGraDA dataset consists of Chinese-English
translation task for four sub-domains of information technology: autonomous
vehicles, AI education, real-time networks, and smart phone. Each sub-domain is
equipped with a development set and test set for evaluation purposes. To be
closer to reality, FGraDA does not employ any in-domain bilingual training data
but provides bilingual dictionaries and wiki knowledge base, which can be
easier obtained within a short time. We benchmark the fine-grained domain
adaptation task and present in-depth analyses showing that there are still
challenging problems to further improve the performance with heterogeneous
resources.",autonomous vehicle
http://arxiv.org/abs/1810.09729v1,to_check,arxiv,arxiv,2018-10-23 08:51:54+00:00,arxiv,"Design Challenges of Multi-UAV Systems in Cyber-Physical Applications: A
  Comprehensive Survey, and Future Directions",http://arxiv.org/abs/1810.09729v1,"Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a
wide range of innovative applications that can fundamentally change the way
cyber-physical systems (CPSs) are designed. CPSs are a modern generation of
systems with synergic cooperation between computational and physical potentials
that can interact with humans through several new mechanisms. The main
advantages of using UAVs in CPS application is their exceptional features,
including their mobility, dynamism, effortless deployment, adaptive altitude,
agility, adjustability, and effective appraisal of real-world functions anytime
and anywhere. Furthermore, from the technology perspective, UAVs are predicted
to be a vital element of the development of advanced CPSs. Therefore, in this
survey, we aim to pinpoint the most fundamental and important design challenges
of multi-UAV systems for CPS applications. We highlight key and versatile
aspects that span the coverage and tracking of targets and infrastructure
objects, energy-efficient navigation, and image analysis using machine learning
for fine-grained CPS applications. Key prototypes and testbeds are also
investigated to show how these practical technologies can facilitate CPS
applications. We present and propose state-of-the-art algorithms to address
design challenges with both quantitative and qualitative methods and map these
challenges with important CPS applications to draw insightful conclusions on
the challenges of each application. Finally, we summarize potential new
directions and ideas that could shape future research in these areas.",autonomous vehicle
http://arxiv.org/abs/2111.06123v1,to_check,arxiv,arxiv,2021-11-11 10:01:01+00:00,arxiv,"Spatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision
  Prediction",http://arxiv.org/abs/2111.06123v1,"In autonomous vehicles (AVs), early warning systems rely on collision
prediction to ensure occupant safety. However, state-of-the-art methods using
deep convolutional networks either fail at modeling collisions or are too
expensive/slow, making them less suitable for deployment on AV edge hardware.
To address these limitations, we propose sg2vec, a spatio-temporal scene-graph
embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term
Memory (LSTM) layers to predict future collisions via visual scene perception.
We demonstrate that sg2vec predicts collisions 8.11% more accurately and 39.07%
earlier than the state-of-the-art method on synthesized datasets, and 29.47%
more accurately on a challenging real-world collision dataset. We also show
that sg2vec is better than the state-of-the-art at transferring knowledge from
synthetic datasets to real-world driving datasets. Finally, we demonstrate that
sg2vec performs inference 9.3x faster with an 88.0% smaller model, 32.4% less
power, and 92.8% less energy than the state-of-the-art method on the
industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for
implementation on the edge.",autonomous vehicle
http://arxiv.org/abs/2003.01886v1,to_check,arxiv,arxiv,2020-03-04 04:35:22+00:00,arxiv,"Efficient statistical validation with edge cases to evaluate Highly
  Automated Vehicles",http://arxiv.org/abs/2003.01886v1,"The widescale deployment of Autonomous Vehicles (AV) seems to be imminent
despite many safety challenges that are yet to be resolved. It is well known
that there are no universally agreed Verification and Validation (VV)
methodologies to guarantee absolute safety, which is crucial for the acceptance
of this technology. Existing standards focus on deterministic processes where
the validation requires only a set of test cases that cover the requirements.
Modern autonomous vehicles will undoubtedly include machine learning and
probabilistic techniques that require a much more comprehensive testing regime
due to the non-deterministic nature of the operating design domain. A rigourous
statistical validation process is an essential component required to address
this challenge. Most research in this area focuses on evaluating system
performance in large scale real-world data gathering exercises (number of miles
travelled), or randomised test scenarios in simulation.
  This paper presents a new approach to compute the statistical characteristics
of a system's behaviour by biasing automatically generated test cases towards
the worst case scenarios, identifying potential unsafe edge cases.We use
reinforcement learning (RL) to learn the behaviours of simulated actors that
cause unsafe behaviour measured by the well established RSS safety metric. We
demonstrate that by using the method we can more efficiently validate a system
using a smaller number of test cases by focusing the simulation towards the
worst case scenario, generating edge cases that correspond to unsafe
situations.",autonomous vehicle
http://arxiv.org/abs/2011.07699v1,to_check,arxiv,arxiv,2020-11-16 02:56:13+00:00,arxiv,"Efficient falsification approach for autonomous vehicle validation using
  a parameter optimisation technique based on reinforcement learning",http://arxiv.org/abs/2011.07699v1,"The widescale deployment of Autonomous Vehicles (AV) appears to be imminent
despite many safety challenges that are yet to be resolved. It is well-known
that there are no universally agreed Verification and Validation (VV)
methodologies guarantee absolute safety, which is crucial for the acceptance of
this technology. The uncertainties in the behaviour of the traffic participants
and the dynamic world cause stochastic reactions in advanced autonomous
systems. The addition of ML algorithms and probabilistic techniques adds
significant complexity to the process for real-world testing when compared to
traditional methods. Most research in this area focuses on generating
challenging concrete scenarios or test cases to evaluate the system performance
by looking at the frequency distribution of extracted parameters as collected
from the real-world data. These approaches generally employ Monte-Carlo
simulation and importance sampling to generate critical cases. This paper
presents an efficient falsification method to evaluate the System Under Test.
The approach is based on a parameter optimisation problem to search for
challenging scenarios. The optimisation process aims at finding the challenging
case that has maximum return. The method applies policy-gradient reinforcement
learning algorithm to enable the learning. The riskiness of the scenario is
measured by the well established RSS safety metric, euclidean distance, and
instance of a collision. We demonstrate that by using the proposed method, we
can more efficiently search for challenging scenarios which could cause the
system to fail in order to satisfy the safety requirements.",autonomous vehicle
10.1109/CHASE.2016.18,to_check,"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",IEEE,2016-06-29 00:00:00,ieeexplore,Improving Tuberculosis Diagnostics Using Deep Learning and Mobile Health Technologies among Resource-Poor and Marginalized Communities,https://ieeexplore.ieee.org/document/7545842/,"Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Perú, a densely occupied urban community and high-burden TB.",health
10.1109/ACCESS.2018.2789918,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,Energy-Efficient Architecture for Wireless Sensor Networks in Healthcare Applications,https://ieeexplore.ieee.org/document/8247183/,"The need to deploy wireless sensor networks (WSNs) for real-world applications, such as mobile multimedia for healthcare organizations, is increasing spectacularly. However, the energy problem remains one of the core barriers preventing an increase in investment in this technology. In this paper, we propose a new technique to resolve the problems due to limited energy sources. Using a quaternary transceiver (in the architecture on a sensor node), instead of a binary one, which will use the amplitude/phase, modulator/demodulator units to increase the number of bits transmitted per symbol. The system will reduce the consumption of energy in the transmission phase due to the increased bits transmitted per symbol. Moreover, neural network static random access memory (NN-SRAM) implementation in a clusteringbased system for energy-constrained WSNs is proposed. The scheme reduces the total amount of energy consumption in storage and transmissions during the data dissemination process. Through simulation results based on MATLAB and Spice software tools, it is shown that the neural network static random access memory implementation in a clustering-based system reduces the energy consumption of the entire system by about 76.99%.",health
10.1109/TVCG.2019.2934631,to_check,IEEE Transactions on Visualization and Computer Graphics,IEEE,2020-01-01 00:00:00,ieeexplore,Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics,https://ieeexplore.ieee.org/document/8812988/,"Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.",health
10.1109/CVPRW50498.2020.00378,to_check,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),IEEE,2020-06-19 00:00:00,ieeexplore,Debiasing Skin Lesion Datasets and Models? Not So Fast,https://ieeexplore.ieee.org/document/9150714/,"Data-driven models are now deployed in a plethora of real-world applications - including automated diagnosis - but models learned from data risk learning biases from that same data. When models learn spurious correlations not found in real-world situations, their deployment for critical tasks, such as medical decisions, can be catastrophic. In this work we address this issue for skin-lesion classification models, with two objectives: finding out what are the spurious correlations exploited by biased networks, and debiasing the models by removing such spurious correlations from them. We perform a systematic integrated analysis of 7 visual artifacts (which are possible sources of biases exploitable by networks), employ a state-of-the-art technique to prevent the models from learning spurious correlations, and propose datasets to test models for the presence of bias. We find out that, despite interesting results that point to promising future research, current debiasing methods are not ready to solve the bias issue for skin-lesion models.",health
10.1109/ACCESS.2020.3022039,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Lightweight Convolutional Neural Network for Real and Apparent Age Estimation in Unconstrained Face Images,https://ieeexplore.ieee.org/document/9187238/,"Real and apparent age estimation of human face has attracted increased attention due to its numerous real-world applications. Different intelligent application scenarios can benefit from these computer-based systems that predict the ages of people correctly. Automatic apparent age system is particularly useful in medical diagnosis, facial beauty product development, movie role casting, the effect of plastic surgery, and anti-aging treatment. Predicting the real and apparent age of people has been quite difficult for both machines and humans. More recently, Deep learning with Convolutional Neural Networks (CNNs) methods have been extensively used for these classification task. It has incomparable advantages in extracting discriminative image features from human faces. However, many of the existing CNN-based methods are designed to be deeper and larger with more complex layers that makes it challenging to deploy on mobile devices with resource-constrained features. Therefore, we design a lightweight CNN model of fewer layers to estimate the real and apparent age of individuals from unconstrained real-time face images that can be deployed on mobile devices. The experimental results, when analyzed for classification accuracy on FG-NET, MORPH-II and APPA-REAL, with large-scale face images containing both real and apparent age annotations, show that our model obtains a state-of-the-art performance in both real and apparent age classification when compared to state-of-the-art methods. The new results and model size, therefore, confirm the usefulness of the model on resource-constrained mobile devices.",health
10.1109/DSAA.2019.00059,to_check,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),IEEE,2019-10-08 00:00:00,ieeexplore,Maximum Relevance and Minimum Redundancy Feature Selection Methods for a Marketing Machine Learning Platform,https://ieeexplore.ieee.org/document/8964172/,"In machine learning applications for online product offerings and marketing strategies, there are often hundreds or thousands of features available to build such models. Feature selection is one essential method in such applications for multiple objectives: improving the prediction accuracy by eliminating irrelevant features, accelerating the model training and prediction speed, reducing the monitoring and maintenance workload for feature data pipeline, and providing better model interpretation and diagnosis capability. However, selecting an optimal feature subset from a large feature space is considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum Relevance) feature selection framework solves this problem by selecting the relevant features while controlling for the redundancy within the selected features. This paper describes the approach to extend, evaluate, and implement the mRMR feature selection methods for classification problem in a marketing machine learning platform at Uber that automates creation and deployment of targeting and personalization models at scale. This study first extends the existing mRMR methods by introducing a non-linear feature redundancy measure and a model-based feature relevance measure. Then an extensive empirical evaluation is performed for eight different feature selection methods, using one synthetic dataset and three real-world marketing datasets at Uber to cover different use cases. Based on the empirical results, the selected mRMR method is implemented in production for the marketing machine learning platform. A description of the production implementation is provided and an online experiment deployed through the platform is discussed.",health
http://arxiv.org/abs/1908.05376v1,to_check,arxiv,arxiv,2019-08-15 00:06:23+00:00,arxiv,"Maximum Relevance and Minimum Redundancy Feature Selection Methods for a
  Marketing Machine Learning Platform",http://arxiv.org/abs/1908.05376v1,"In machine learning applications for online product offerings and marketing
strategies, there are often hundreds or thousands of features available to
build such models. Feature selection is one essential method in such
applications for multiple objectives: improving the prediction accuracy by
eliminating irrelevant features, accelerating the model training and prediction
speed, reducing the monitoring and maintenance workload for feature data
pipeline, and providing better model interpretation and diagnosis capability.
However, selecting an optimal feature subset from a large feature space is
considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum
Relevance) feature selection framework solves this problem by selecting the
relevant features while controlling for the redundancy within the selected
features. This paper describes the approach to extend, evaluate, and implement
the mRMR feature selection methods for classification problem in a marketing
machine learning platform at Uber that automates creation and deployment of
targeting and personalization models at scale. This study first extends the
existing mRMR methods by introducing a non-linear feature redundancy measure
and a model-based feature relevance measure. Then an extensive empirical
evaluation is performed for eight different feature selection methods, using
one synthetic dataset and three real-world marketing datasets at Uber to cover
different use cases. Based on the empirical results, the selected mRMR method
is implemented in production for the marketing machine learning platform. A
description of the production implementation is provided and an online
experiment deployed through the platform is discussed.",health
http://arxiv.org/abs/1907.07296v4,to_check,arxiv,arxiv,2019-07-17 00:50:37+00:00,arxiv,"Explaining Vulnerabilities to Adversarial Machine Learning through
  Visual Analytics",http://arxiv.org/abs/1907.07296v4,"Machine learning models are currently being deployed in a variety of
real-world applications where model predictions are used to make decisions
about healthcare, bank loans, and numerous other critical tasks. As the
deployment of artificial intelligence technologies becomes ubiquitous, it is
unsurprising that adversaries have begun developing methods to manipulate
machine learning models to their advantage. While the visual analytics
community has developed methods for opening the black box of machine learning
models, little work has focused on helping the user understand their model
vulnerabilities in the context of adversarial attacks. In this paper, we
present a visual analytics framework for explaining and exploring model
vulnerabilities to adversarial attacks. Our framework employs a multi-faceted
visualization scheme designed to support the analysis of data poisoning attacks
from the perspective of models, data instances, features, and local structures.
We demonstrate our framework through two case studies on binary classifiers and
illustrate model vulnerabilities with respect to varying attack strategies.",health
http://arxiv.org/abs/2107.10230v4,to_check,arxiv,arxiv,2021-07-21 17:28:46+00:00,arxiv,"Multi-institution encrypted medical imaging AI validation without data
  sharing",http://arxiv.org/abs/2107.10230v4,"Adoption of artificial intelligence medical imaging applications is often
impeded by barriers between healthcare systems and algorithm developers given
that access to both private patient data and commercial model IP is important
to perform pre-deployment evaluation. This work investigates a framework for
secure, privacy-preserving and AI-enabled medical imaging inference using
CrypTFlow2, a state-of-the-art end-to-end compiler allowing cryptographically
secure 2-party Computation (2PC) protocols between the machine learning model
vendor and target patient data owner. A common DenseNet-121 chest x-ray
diagnosis model was evaluated on multi-institutional chest radiographic imaging
datasets both with and without CrypTFlow2 on two test sets spanning seven sites
across the US and India, and comprising 1,149 chest x-ray images. We measure
comparative AUROC performance between secure and insecure inference in multiple
pathology classification tasks, and explore model output distributional shifts
and resource constraints introduced by secure model inference. Secure inference
with CrypTFlow2 demonstrated no significant difference in AUROC for all
diagnoses, and model outputs from secure and insecure inference methods were
distributionally equivalent. The use of CrypTFlow2 may allow off-the-shelf
secure 2PC between healthcare systems and AI model vendors for medical imaging,
without changes in performance, and can facilitate scalable pre-deployment
infrastructure for real-world secure model evaluation without exposure to
patient data or model IP.",health
http://arxiv.org/abs/2001.11363v1,to_check,arxiv,arxiv,2020-01-29 17:23:16+00:00,arxiv,"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the
  Wild",http://arxiv.org/abs/2001.11363v1,"In recent years, significant attention has been devoted towards integrating
deep learning technologies in the healthcare domain. However, to safely and
practically deploy deep learning models for home health monitoring, two
significant challenges must be addressed: the models should be (1) robust
against noise; and (2) compact and energy-efficient. We propose REST, a new
method that simultaneously tackles both issues via 1) adversarial training and
controlling the Lipschitz constant of the neural network through spectral
regularization while 2) enabling neural network compression through sparsity
regularization. We demonstrate that REST produces highly-robust and efficient
models that substantially outperform the original full-sized models in the
presence of noise. For the sleep staging task over single-channel
electroencephalogram (EEG), the REST model achieves a macro-F1 score of 0.67
vs. 0.39 achieved by a state-of-the-art model in the presence of Gaussian noise
while obtaining 19x parameter reduction and 15x MFLOPS reduction on two large,
real-world EEG datasets. By deploying these models to an Android application on
a smartphone, we quantitatively observe that REST allows models to achieve up
to 17x energy reduction and 9x faster inference. We open-source the code
repository with this paper: https://github.com/duggalrahul/REST.",health
10.1016/j.cmpb.2020.105635,to_check,Computer Methods and Programs in Biomedicine,scopus,2020-10-01,sciencedirect,Data preprocessing for heart disease classification: A systematic literature review.,https://api.elsevier.com/content/abstract/scopus_id/85087500300,"Context
                  Early detection of heart disease is an important challenge since 17.3 million people yearly lose their lives due to heart diseases. Besides, any error in diagnosis of cardiac disease can be dangerous and risks an individual's life. Accurate diagnosis is therefore critical in cardiology. Data Mining (DM) classification techniques have been used to diagnosis heart diseases but still limited by some challenges of data quality such as inconsistencies, noise, missing data, outliers, high dimensionality and imbalanced data. Data preprocessing (DP) techniques were therefore used to prepare data with the goal of improving the performance of heart disease DM based prediction systems.
               
                  Objective
                  The purpose of this study is to review and summarize the current evidence on the use of preprocessing techniques in heart disease classification as regards: (1) the DP tasks and techniques most frequently used, (2) the impact of DP tasks and techniques on the performance of classification in cardiology, (3) the overall performance of classifiers when using DP techniques, and (4) comparisons of different combinations classifier-preprocessing in terms of accuracy rate.
               
                  Method
                  A systematic literature review is carried out, by identifying and analyzing empirical studies on the application of data preprocessing in heart disease classification published in the period between January 2000 and June 2019. A total of 49 studies were therefore selected and analyzed according to the aforementioned criteria.
               
                  Results
                  The review results show that data reduction is the most used preprocessing task in cardiology, followed by data cleaning. In general, preprocessing either maintained or improved the performance of heart disease classifiers. Some combinations such as (ANN + PCA), (ANN + CHI) and (SVM + PCA) are promising terms of accuracy. However the deployment of these models in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of interpretation.",health
10.1109/ICSPCC.2018.8567797,to_check,"2018 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)",IEEE,2018-09-16 00:00:00,ieeexplore,Attack Detection for Wireless Enterprise Network: a Machine Learning Approach,https://ieeexplore.ieee.org/document/8567797/,"An increasing number of enterprises are adopting wireless technology to deploy networks. However, wireless enterprise networks are more vulnerable than wired networks because of the broadcast feature. Thus, illegal attacks such as data theft and information forgery seriously threaten the property and information security of users and enterprises; these phenomena are attracting increasing attention from both academia and industry. Additionally, effectively detecting the attacks in the wireless enterprise networks is one of todays most important and challenging problems, especially in Wi-Fi networks, as attacks become increasingly covert and diverse. Fortunately, WiFi networks produce large amounts of data, providing copious big data for researchers. In this paper, using the Aegean Wi-Fi Intrusion Dataset (AWID), which is derived from the real-world Wi-Fi network, we introduce machine learning to detect network attacks. To significantly increase the training and convergence speeds, we deploy two-dimensional data cleaning and select 18 useful attributes from the original set of 154. Then, we introduce support vector machine (SVM) to detect attacks based on the cleaned dataset. The detection accuracy for flooding attacks, injection attacks, and normal data reached 89.18%, 87.34%, and 99.88% respectively. To the best of our knowledge, this is the first study to introduce a two-dimensional data cleaning method with an SVM to improve the detection accuracy for attacks. Finally, our detection results are comparable with the existing studies; however, our method operates with simpler data attributes with faster and more efficient training speed.",industry
10.1109/CCEM.2018.00025,to_check,2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),IEEE,2018-11-24 00:00:00,ieeexplore,Bodhisattva - Rapid Deployment of AI on Containers,https://ieeexplore.ieee.org/document/8648632/,"Cloud-based machine learning is becoming increasingly important in all verticals of the industry as all organizations want to leverage ML and AI to solve real-world problems of emerging markets. But, incorporating these services into business solutions is a goliath task, mainly due to the sheer effort necessary to go from development to deployment. We present a novel idea that enables users to easily specify, create, train and rapidly deploy machine learning models through a scalable Machine-Learning-as-a-Service (MLaaS) offering. The MLaaS is provided as an end-to-end microservice suite in a container-based PaaS environment for web applications on the cloud. Our implementation provides an intuitive web-based GUI for tenants to consume these services in a few quick steps. The utility of our service is demonstrated by training ML models for various use cases and comparing them on factors like time-to-deploy, resource usage and training metrics.",industry
10.1109/VLSID.2018.26,to_check,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),IEEE,2018-01-10 00:00:00,ieeexplore,Special Session: Design of Energy-Efficient and Reliable VLSI Systems: A Data-Driven Perspective,https://ieeexplore.ieee.org/document/8326889/,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The amount of data generated and collected across computing platforms every day is not only enormous, but growing at an exponential rate. Advanced data analytics and machine learning techniques have become increasingly essential to analyze and extract meaning from such “Big Data”. These techniques can be very useful to detect patterns and trends to improve the operational behavior of computing systems, but they also introduce a number of outstanding challenges: (1) How can we design and deploy data analytics mechanisms to improve energy-efficiency and reliability in IoT and mobile devices, without introducing significant software overheads? (2) How to leverage emerging technologies (e.g.,3D integration) to design energy-efficient and reliable manycore systems for big data computing? (3) How to use machine learning and data mining techniques for effective design space exploration of computing systems, and enable adaptive control to improve energy-efficiency? (4) How can data analytics detect anomalies and increase robustness in the network backbone of emerging large-scale networking systems? To address these outstanding challenges, out-of-the-box approaches need to be explored. In this special session, we will discuss these outstanding problems and describe far-reaching solutions applicable across the interconnected ecosystem of IoT and mobile devices, manycore chips, datacenters, and networks. The special session brings together speakers with unique insights on applying data analytics and machine learning to real-world problems to achieve the most sought after features on multi-scale computing platforms, viz. intelligent data mining, energyefficiency, and robustness. By integrating data analytics and machine learning algorithms, statistical modeling, embedded hardware and software design, and cloud computing content, this session will engage a broad section of Embedded and VLSI Design conference attendees. This special session is targeted towards university researchers/professors, students, industry professionals, and embedded/VLSI system designers. This session will attract newcomers who want to learn how to apply data analytics to solve problems in computing systems, as well as experienced researchers looking for exciting new directions in embedded systems, VLSI design, EDA algorithms, and multi-scale computing.",industry
10.1109/SenSysML50931.2020.00007,to_check,2020 IEEE Second Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML),IEEE,2020-04-21 00:00:00,ieeexplore,Welcome Message from SenSys-ML'20 Chairs,https://ieeexplore.ieee.org/document/9111711/,"We are very excited to welcome you to the second ACM/IEEE International Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML 2020). SenSys-ML’20 will be held in conjunction with the CPS-IoT Week 2020 and focuses on work that combines sensor signals from the physical world with machine learning, particularly in ways that are distributed to the device or use edge and fog computing. The development and deployment of ML at the very edge remains a technological challenge constrained by computing, memory, energy, network bandwidth, and data privacy and security limitations. This is especially true for battery-operated devices and always-on use cases and applications. In recent years this has gained attention from both academia and industry and many TinyML initiatives have been started focusing both in hardware and software advancements. This workshop will provide a forum for sensing, networking and machine learning researchers to present and share their latest research on building machine learning-enabled sensor systems. Sensys-ML focuses on providing extensive feedback on Work-In-Progress papers involving machine learning (TinyML/ UltraML) on sensor systems. Many papers were submitted from multiple countries, and four papers were selected for publication. This year our articles had themes including techniques for collecting low-resolution images from thermal cameras for human activity recognition, VAE-based approach for privacy preservation of sensor data, deep model compression techniques, and novel GRU based shallow Neural Networks. We see advancement and contributions made by research work will have a significant impact on real-world applications and future research directions.",industry
10.1109/TAI.2021.3057027,to_check,IEEE Transactions on Artificial Intelligence,IEEE,2020-12-01 00:00:00,ieeexplore,ZJU-Leaper: A Benchmark Dataset for Fabric Defect Detection and a Comparative Study,https://ieeexplore.ieee.org/document/9346038/,"Fabric inspection plays an important role in the process of quality control in textile manufacturing. There is a growing demand in the textile industry to leverage computer vision technology for more efficient quality control in the hope that it will replace the traditional labor-intensive inspection by naked eyes. However, there is an underlying viewpoint in most existing fabric datasets that automatic defect detection is a traditional image classification problem, thus more samples help better, which lacks enough consideration about the problem itself and real application environments. After deep communication with users, we find these facts that an assembly line usually has only a few fixed texture fabrics for a long period, users prefer fast deployment and easily upgradable model to a general model and long-time tuning, and users hope the process of collecting samples, annotating, and deployment affects assembly lines as little as possible. This implies that defect detection is different from popular deep learning problems. Multiple-stage models and fast training become more attractive since users are able to train and deploy models by themselves according to the real conditions of samples that can be obtained. Based on this analysis, we propose a new fabric dataset “ZJU-Leaper”. It provides a series of task settings in accordance with the progressive strategy dealing with the problem, from only normal samples to many defective samples with precise annotations, to facilitate real-world application. To avoid misleading information and inconsistency issues associated with the prior evaluation metrics, we propose a new evaluation protocol by experimental analysis of task-specific indexes, which can tell a truthful comparison between different inspection methods. We also offer some novel solutions to address the new challenges of our dataset, as part of the baseline experiments. It is our hope that ZJU-Leaper can accelerate the research of automated visual inspection and empower the practitioners with more opportunities for manufacturing automation in the textile industry.",industry
10.1109/MCDM.2007.369428,to_check,2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making,IEEE,2007-04-05 00:00:00,ieeexplore,A Review of Two Industrial Deployments of Multi-criteria Decision-making Systems at General Electric,https://ieeexplore.ieee.org/document/4222994/,"Two industrial deployments of multi-criteria decision-making systems at General Electric are reviewed from the perspective of their multi-criteria decision-making component similarities and differences. The motivation is to present a framework for multi-criteria decision-making system development and deployment. The first deployment is a financial portfolio management system that integrates hybrid multi-objective optimization and interactive Pareto frontier decision-making techniques to optimally allocate financial assets while considering multiple measures of return and risk, and numerous regulatory constraints. The second deployment is a power plant management system that integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and automated decision-making based on Pareto frontier techniques. The integrated approach, embedded in a real-time plant optimization and control software environment dynamically optimizes emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant",industry
10.1109/ICCC51575.2020.9345104,to_check,2020 IEEE 6th International Conference on Computer and Communications (ICCC),IEEE,2020-12-14 00:00:00,ieeexplore,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,https://ieeexplore.ieee.org/document/9345104/,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",industry
10.1109/TII.2020.3007407,to_check,IEEE Transactions on Industrial Informatics,IEEE,2021-08-01 00:00:00,ieeexplore,Industrial Cyber-Physical Systems-Based Cloud IoT Edge for Federated Heterogeneous Distillation,https://ieeexplore.ieee.org/document/9134802/,"Deep convoloutional networks have been widely deployed in modern cyber-physical systems performing different visual classification tasks. As the fog and edge devices have different computing capacity and perform different subtasks, models trained for one device may not be deployable on another. Knowledge distillation technique can effectively compress well trained convolutional neural networks into light-weight models suitable to different devices. However, due to privacy issue and transmission cost, manually annotated data for training the deep learning models are usually gradually collected and archived in different sites. Simply training a model on powerful cloud servers and compressing them for particular edge devices failed to use the distributed data stored at different sites. This offline training approach is also inefficient to deal with new data collected from the edge devices. To overcome these obstacles, in this article, we propose the heterogeneous brain storming (HBS) method for object recognition tasks in real-world Internet of Things (IoT) scenarios. Our method enables flexible bidirectional federated learning of heterogeneous models trained on distributed datasets with a new “brain storming” mechanism and optimizable temperature parameters. In our comparison experiments, this HBS method outperformed multiple state-of-the-art single-model compression methods, as well as the newest multinetwork knowledge distillation methods with both homogeneous and heterogeneous classifiers. The ablation experiment results proved that the trainable temperature parameter into the conventional knowledge distillation loss can effectively ease the learning process of student networks in different methods. To the best of authors' knowledge, this is the first IoT-oriented method that allows asynchronous bidirectional heterogeneous knowledge distillation in deep networks.",industry
10.1109/JIOT.2019.2912022,to_check,IEEE Internet of Things Journal,IEEE,2019-08-01 00:00:00,ieeexplore,Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things,https://ieeexplore.ieee.org/document/8693904/,"It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",industry
10.1109/JIOT.2019.2963635,to_check,IEEE Internet of Things Journal,IEEE,2020-05-01 00:00:00,ieeexplore,Toward Edge-Based Deep Learning in Industrial Internet of Things,https://ieeexplore.ieee.org/document/8948000/,"As a typical application of the Internet of Things (IoT), the Industrial IoT (IIoT) connects all the related IoT sensing and actuating devices ubiquitously so that the monitoring and control of numerous industrial systems can be realized. Deep learning, as one viable way to carry out big-data-driven modeling and analysis, could be integrated in IIoT systems to aid the automation and intelligence of IIoT systems. As deep learning requires large computation power, it is commonly deployed in cloud servers. Thus, the data collected by IoT devices must be transmitted to the cloud for training process, contributing to network congestion and affecting the IoT network performance as well as the supported applications. To address this issue, in this article, we leverage the fog/edge computing paradigm and propose an edge computing-based deep learning model, which utilizes edge computing to migrate the deep learning process from cloud servers to edge nodes, reducing data transmission demands in the IIoT network and mitigating network congestion. Since edge nodes have limited computation ability compared to servers, we design a mechanism to optimize the deep learning model so that its requirements for computational power can be reduced. To evaluate our proposed solution, we design a testbed implemented in the Google cloud and deploy the proposed convolutional neural network (CNN) model, utilizing a real-world IIoT data set to evaluate our approach.<sup>1</sup> Our experimental results confirm the effectiveness of our approach, which cannot only reduce the network traffic overhead for IIoT but also maintain the classification accuracy in comparison with several baseline schemes.<sup>1</sup>Certain commercial equipment, instruments, or materials are identified in this article in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.",industry
10.1109/CNSC.2014.6906671,to_check,2014 First International Conference on Networks & Soft Computing (ICNSC2014),IEEE,2014-08-20 00:00:00,ieeexplore,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,https://ieeexplore.ieee.org/document/6906671/,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",industry
10.1109/TIM.2021.3092518,to_check,IEEE Transactions on Instrumentation and Measurement,IEEE,2021-01-01 00:00:00,ieeexplore,Pipeline Safety Early Warning by Multifeature-Fusion CNN and LightGBM Analysis of Signals From Distributed Optical Fiber Sensors,https://ieeexplore.ieee.org/document/9541184/,"Energy pipelines are the backbones of global energy systems. Monitoring their safety and automatically identifying and locating third-party damage events are crucial to energy supply. However, most traditional methods lack in-depth consideration of distributed fiber signals and have not been tested on real-world long-distance pipelines, making it difficult to deploy them in operating long-distance pipelines. In this study, we utilize a novel real-time machine-learning method based on phase-sensitive optical time domain reflectometer technology to monitor the safety of oil and gas pipelines. Specifically, we build a multifeature-fusion convolutional neural network and LightGBM fusion model based on two novel complementary spatiotemporal features. The method was applied to a large amount of data collected from real-world oil–gas transportation pipelines of the China National Petroleum Corporation. The proposed method could accurately locate and identify third-party damage events in real-time under conditions of strong noise and various types of system hardware, and could effectively handle signal drift in the time and space dimensions. Our methodology has been deployed at real long-distance energy pipeline sites and our work will contribute to energy pipeline safety and energy supply security. Furthermore, the proposed solution could be generalized to other fields, such as industrial inspection, measurement, and monitoring.",industry
http://arxiv.org/abs/2011.09926v2,to_check,arxiv,arxiv,2020-11-18 16:20:28+00:00,arxiv,Challenges in Deploying Machine Learning: a Survey of Case Studies,http://arxiv.org/abs/2011.09926v2,"In recent years, machine learning has received increased interest both as an
academic research field and as a solution for real-world business problems.
However, the deployment of machine learning models in production systems can
present a number of issues and concerns. This survey reviews published reports
of deploying machine learning solutions in a variety of use cases, industries
and applications and extracts practical considerations corresponding to stages
of the machine learning deployment workflow. Our survey shows that
practitioners face challenges at each stage of the deployment. The goal of this
paper is to layout a research agenda to explore approaches addressing these
challenges.",industry
http://arxiv.org/abs/1908.08998v2,to_check,arxiv,arxiv,2019-08-13 10:15:39+00:00,arxiv,AIBench: An Industry Standard Internet Service AI Benchmark Suite,http://arxiv.org/abs/1908.08998v2,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.",industry
http://arxiv.org/abs/2102.08936v2,to_check,arxiv,arxiv,2021-02-17 18:51:51+00:00,arxiv,"Deep Learning Anomaly Detection for Cellular IoT with Applications in
  Smart Logistics",http://arxiv.org/abs/2102.08936v2,"The number of connected Internet of Things (IoT) devices within
cyber-physical infrastructure systems grows at an increasing rate. This poses
significant device management and security challenges to current IoT networks.
Among several approaches to cope with these challenges, data-based methods
rooted in deep learning (DL) are receiving an increased interest. In this
paper, motivated by the upcoming surge of 5G IoT connectivity in industrial
environments, we propose to integrate a DL-based anomaly detection (AD) as a
service into the 3GPP mobile cellular IoT architecture. The proposed
architecture embeds autoencoder based anomaly detection modules both at the IoT
devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing
between the system responsiveness and accuracy. We design, integrate,
demonstrate and evaluate a testbed that implements the above service in a
real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT)
mobile operator network.",industry
http://arxiv.org/abs/2002.05648v3,to_check,arxiv,arxiv,2020-02-01 01:15:39+00:00,arxiv,Politics of Adversarial Machine Learning,http://arxiv.org/abs/2002.05648v3,"In addition to their security properties, adversarial machine-learning
attacks and defenses have political dimensions. They enable or foreclose
certain options for both the subjects of the machine learning systems and for
those who deploy them, creating risks for civil liberties and human rights. In
this paper, we draw on insights from science and technology studies,
anthropology, and human rights literature, to inform how defenses against
adversarial attacks can be used to suppress dissent and limit attempts to
investigate machine learning systems. To make this concrete, we use real-world
examples of how attacks such as perturbation, model inversion, or membership
inference can be used for socially desirable ends. Although the predictions of
this analysis may seem dire, there is hope. Efforts to address human rights
concerns in the commercial spyware industry provide guidance for similar
measures to ensure ML systems serve democratic, not authoritarian ends",industry
http://arxiv.org/abs/2109.13602v1,to_check,arxiv,arxiv,2021-09-28 10:23:46+00:00,arxiv,"SafetyNet: Safe planning for real-world self-driving vehicles using
  machine-learned policies",http://arxiv.org/abs/2109.13602v1,"In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios.",industry
http://arxiv.org/abs/2008.06448v1,to_check,arxiv,arxiv,2020-08-14 16:17:54+00:00,arxiv,"Loghub: A Large Collection of System Log Datasets towards Automated Log
  Analytics",http://arxiv.org/abs/2008.06448v1,"Logs have been widely adopted in software system development and maintenance
because of the rich system runtime information they contain. In recent years,
the increase of software size and complexity leads to the rapid growth of the
volume of logs. To handle these large volumes of logs efficiently and
effectively, a line of research focuses on intelligent log analytics powered by
AI (artificial intelligence) techniques. However, only a small fraction of
these techniques have reached successful deployment in industry because of the
lack of public log datasets and necessary benchmarking upon them. To fill this
significant gap between academia and industry and also facilitate more research
on AI-powered log analytics, we have collected and organized loghub, a large
collection of log datasets. In particular, loghub provides 17 real-world log
datasets collected from a wide range of systems, including distributed systems,
supercomputers, operating systems, mobile systems, server applications, and
standalone software. In this paper, we summarize the statistics of these
datasets, introduce some practical log usage scenarios, and present a case
study on anomaly detection to demonstrate how loghub facilitates the research
and practice in this field. Up to the time of this paper writing, loghub
datasets have been downloaded over 15,000 times by more than 380 organizations
from both industry and academia.",industry
http://arxiv.org/abs/2010.12837v3,to_check,arxiv,arxiv,2020-10-24 08:37:04+00:00,arxiv,"XDM: Improving Sequential Deep Matching with Unclicked User Behaviors
  for Recommender System",http://arxiv.org/abs/2010.12837v3,"Deep learning-based sequential recommender systems have recently attracted
increasing attention from both academia and industry. Most of industrial
Embedding-Based Retrieval (EBR) system for recommendation share the similar
ideas with sequential recommenders. Among them, how to comprehensively capture
sequential user interest is a fundamental problem. However, most existing
sequential recommendation models take as input clicked or purchased behavior
sequences from user-item interactions. This leads to incomprehensive user
representation and sub-optimal model performance, since they ignore the
complete user behavior exposure data, i.e., items impressed yet unclicked by
users. In this work, we attempt to incorporate and model those unclicked item
sequences using a new learning approach in order to explore better sequential
recommendation technique. An efficient triplet metric learning algorithm is
proposed to appropriately learn the representation of unclicked items. Our
method can be simply integrated with existing sequential recommendation models
by a confidence fusion network and further gain better user representation. The
offline experimental results based on real-world E-commerce data demonstrate
the effectiveness and verify the importance of unclicked items in sequential
recommendation. Moreover we deploy our new model (named XDM) into EBR of
recommender system at Taobao, outperforming the deployed previous generation
SDM.",industry
http://arxiv.org/abs/1911.05771v1,to_check,arxiv,arxiv,2019-11-13 19:25:53+00:00,arxiv,"Machine Learning Based Network Vulnerability Analysis of Industrial
  Internet of Things",http://arxiv.org/abs/1911.05771v1,"It is critical to secure the Industrial Internet of Things (IIoT) devices
because of potentially devastating consequences in case of an attack. Machine
learning and big data analytics are the two powerful leverages for analyzing
and securing the Internet of Things (IoT) technology. By extension, these
techniques can help improve the security of the IIoT systems as well. In this
paper, we first present common IIoT protocols and their associated
vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the
utilization of machine learning in countering these susceptibilities. Following
that, a literature review of the available intrusion detection solutions using
machine learning models is presented. Finally, we discuss our case study, which
includes details of a real-world testbed that we have built to conduct
cyber-attacks and to design an intrusion detection system (IDS). We deploy
backdoor, command injection, and Structured Query Language (SQL) injection
attacks against the system and demonstrate how a machine learning based anomaly
detection system can perform well in detecting these attacks. We have evaluated
the performance through representative metrics to have a fair point of view on
the effectiveness of the methods.",industry
http://arxiv.org/abs/2010.09254v1,to_check,arxiv,arxiv,2020-10-19 06:48:40+00:00,arxiv,Query-aware Tip Generation for Vertical Search,http://arxiv.org/abs/2010.09254v1,"As a concise form of user reviews, tips have unique advantages to explain the
search results, assist users' decision making, and further improve user
experience in vertical search scenarios. Existing work on tip generation does
not take query into consideration, which limits the impact of tips in search
scenarios. To address this issue, this paper proposes a query-aware tip
generation framework, integrating query information into encoding and
subsequent decoding processes. Two specific adaptations of Transformer and
Recurrent Neural Network (RNN) are proposed. For Transformer, the query impact
is incorporated into the self-attention computation of both the encoder and the
decoder. As for RNN, the query-aware encoder adopts a selective network to
distill query-relevant information from the review, while the query-aware
decoder integrates the query information into the attention computation during
decoding. The framework consistently outperforms the competing methods on both
public and real-world industrial datasets. Last but not least, online
deployment experiments on Dianping demonstrate the advantage of the proposed
framework for tip generation as well as its online business values.",industry
http://arxiv.org/abs/2109.13375v1,to_check,arxiv,arxiv,2021-09-27 22:37:55+00:00,arxiv,"Automated Estimation of Construction Equipment Emission using Inertial
  Sensors and Machine Learning Models",http://arxiv.org/abs/2109.13375v1,"The construction industry is one of the main producers of greenhouse gasses
(GHG). Quantifying the amount of air pollutants including GHG emissions during
a construction project has become an additional project objective to
traditional metrics such as time, cost, and safety in many parts of the world.
A major contributor to air pollution during construction is the use of heavy
equipment and thus their efficient operation and management can substantially
reduce the harm to the environment. Although the on-road vehicle emission
prediction is a widely researched topic, construction equipment emission
measurement and reduction have received very little attention. This paper
describes the development and deployment of a novel framework that uses machine
learning (ML) methods to predict the level of emissions from heavy construction
equipment monitored via an Internet of Things (IoT) system comprised of
accelerometer and gyroscope sensors. The developed framework was validated
using an excavator performing real-world construction work. A portable emission
measurement system (PEMS) was employed along with the inertial sensors to
record data including the amount of CO, NOX, CO2, SO2, and CH4 pollutions
emitted by the equipment. Different ML algorithms were developed and compared
to identify the best model to predict emission levels from inertial sensors
data. The results showed that Random Forest with the coefficient of
determination (R2) of 0.94, 0.91 and 0.94 for CO, NOX, CO2, respectively was
the best algorithm among different models evaluated in this study.",industry
http://arxiv.org/abs/2110.00086v1,to_check,arxiv,arxiv,2021-09-30 20:56:37+00:00,arxiv,On the Trustworthiness of Tree Ensemble Explainability Methods,http://arxiv.org/abs/2110.00086v1,"The recent increase in the deployment of machine learning models in critical
domains such as healthcare, criminal justice, and finance has highlighted the
need for trustworthy methods that can explain these models to stakeholders.
Feature importance methods (e.g. gain and SHAP) are among the most popular
explainability methods used to address this need. For any explainability
technique to be trustworthy and meaningful, it has to provide an explanation
that is accurate and stable. Although the stability of local feature importance
methods (explaining individual predictions) has been studied before, there is
yet a knowledge gap about the stability of global features importance methods
(explanations for the whole model). Additionally, there is no study that
evaluates and compares the accuracy of global feature importance methods with
respect to feature ordering. In this paper, we evaluate the accuracy and
stability of global feature importance methods through comprehensive
experiments done on simulations as well as four real-world datasets. We focus
on tree-based ensemble methods as they are used widely in industry and measure
the accuracy and stability of explanations under two scenarios: 1) when inputs
are perturbed 2) when models are perturbed. Our findings provide a comparison
of these methods under a variety of settings and shed light on the limitations
of global feature importance methods by indicating their lack of accuracy with
and without noisy inputs, as well as their lack of stability with respect to:
1) increase in input dimension or noise in the data; 2) perturbations in models
initialized by different random seeds or hyperparameter settings.",industry
http://arxiv.org/abs/2001.05375v1,to_check,arxiv,arxiv,2020-01-15 15:30:29+00:00,arxiv,"AAAI FSS-19: Human-Centered AI: Trustworthiness of AI Models and Data
  Proceedings",http://arxiv.org/abs/2001.05375v1,"To facilitate the widespread acceptance of AI systems guiding decision-making
in real-world applications, it is key that solutions comprise trustworthy,
integrated human-AI systems. Not only in safety-critical applications such as
autonomous driving or medicine, but also in dynamic open world systems in
industry and government it is crucial for predictive models to be
uncertainty-aware and yield trustworthy predictions. Another key requirement
for deployment of AI at enterprise scale is to realize the importance of
integrating human-centered design into AI systems such that humans are able to
use systems effectively, understand results and output, and explain findings to
oversight committees.
  While the focus of this symposium was on AI systems to improve data quality
and technical robustness and safety, we welcomed submissions from broadly
defined areas also discussing approaches addressing requirements such as
explainable models, human trust and ethical aspects of AI.",industry
http://arxiv.org/abs/2007.15215v1,to_check,arxiv,arxiv,2020-07-30 03:54:32+00:00,arxiv,"Learner's Dilemma: IoT Devices Training Strategies in Collaborative Deep
  Learning",http://arxiv.org/abs/2007.15215v1,"With the growth of Internet of Things (IoT) and mo-bile edge computing,
billions of smart devices are interconnected to develop applications used in
various domains including smart homes, healthcare and smart manufacturing. Deep
learning has been extensively utilized in various IoT applications which
require huge amount of data for model training. Due to privacy requirements,
smart IoT devices do not release data to a remote third party for their use. To
overcome this problem, collaborative approach to deep learning, also known as
Collaborative DeepLearning (CDL) has been largely employed in data-driven
applications. This approach enables multiple edge IoT devices to train their
models locally on mobile edge devices. In this paper,we address IoT device
training problem in CDL by analyzing the behavior of mobile edge devices using
a game-theoretic model,where each mobile edge device aims at maximizing the
accuracy of its local model at the same time limiting the overhead of
participating in CDL. We analyze the Nash Equilibrium in anN-player static game
model. We further present a novel cluster-based fair strategy to approximately
solve the CDL game to enforce mobile edge devices for cooperation. Our
experimental results and evaluation analysis in a real-world smart home
deployment show that 80% mobile edge devices are ready to cooperate in CDL,
while 20% of them do not train their local models collaboratively.",industry
10.1016/j.ifacol.2020.12.2866,to_check,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Reinforcement learning for dual-resource constrained scheduling,https://api.elsevier.com/content/abstract/scopus_id/85107805245,"This paper proposes using reinforcement learning to solve scheduling problems where two types of resources of limited availability must be allocated. The goal is to minimize the makespan of a dual-resource constrained flexible job shop scheduling problem. Efficient practical implementation is very valuable to industry, yet it is often only solved combining heuristics and expert knowledge. A framework for training a reinforcement learning agent to schedule diverse dual-resource constrained job shops is presented. Comparison with other state-of-the-art approaches is done on both simpler and more complex instances that the ones used for training. Results show the agent produces competitive solutions for small instances that can outperform the implemented heuristic if given enough time. Other extensions are needed before real-world deployment, such as deadlines and constraining resources to work shifts.",industry
10.1016/j.simpat.2016.08.007,to_check,Simulation Modelling Practice and Theory,scopus,2017-02-01,sciencedirect,Intelligent simulation: Integration of SIMIO and MATLAB to deploy decision support systems to simulation environment,https://api.elsevier.com/content/abstract/scopus_id/84996798684,"Discrete-event simulation is a decision support tool which enables practitioners to model and analyze their own system behavior. Although simulation packages are capable of mimicking most tasks in a real-world system, there are some decision-making activities, which are beyond the reach of simulation packages. The Application Programmers Interface (API) of SIMIO provides a wide range of opportunities for researchers to develop their own logic and apply it during the simulation run. This paper illustrates how to deploy MATLAB, as a computational tool coupled with SIMIO, as a simulation package by using a new user-defined step instance named “CallMATLAB”. A manufacturing system case study is introduced where the CallMATLAB step instance is used to create an Iterative Optimization-based Simulation (IOS) model. This model is created to evaluate the performance of different optimizers. The benefits of this hybridization for other industries, including healthcare systems, supply chain management systems, and project management problems are discussed.",industry
10.1109/TII.2018.2884951,to_check,IEEE Transactions on Industrial Informatics,IEEE,2019-03-01 00:00:00,ieeexplore,3-D Deployment Optimization for Heterogeneous Wireless Directional Sensor Networks on Smart City,https://ieeexplore.ieee.org/document/8558102/,"The development of smart cities and the emergence of three-dimensional (3-D) urban terrain data have introduced new requirements and issues to the research on the 3-D deployment of wireless sensor networks. We study the deployment issue of heterogeneous wireless directional sensor networks in 3-D smart cities. Traditionally, studies on the deployment problem of WSNs focus on omnidirectional sensors on a 2-D plane or in full 3-D space. Based on 3-D urban terrain data, we transform the deployment problem into a multiobjective optimization problem, in which objectives of Coverage, Connectivity Quality, and Lifetime, as well as the Connectivity and Reliability constraints, are simultaneously considered. A graph-based 3-D signal propagation model employing the line-of-sight concept is used to calculate the signal path loss. Novel distributed parallel multiobjective evolutionary algorithms (MOEAs) are also proposed. For verification, real-world and artificial urban terrains are utilized. In comparison with other state-of-the-art MOEAs, the novel algorithms could more effectively and more efficiently address the deployment problem in terms of optimization performance and operation time.",smart cities
10.1109/TII.2020.3009159,to_check,IEEE Transactions on Industrial Informatics,IEEE,2021-06-01 00:00:00,ieeexplore,Data-Augmentation-Based Cellular Traffic Prediction in Edge-Computing-Enabled Smart City,https://ieeexplore.ieee.org/document/9140397/,"With the massive deployment of 5G cellular infrastructures, traffic prediction has become an indispensable part of the cellular resource management system in order to provide reliable and fast communication services that can meet the increasing quality-of-service requirements of smart city. A promising approach for handling this problem is to introduce intelligent methods to implement a highly effective and efficient cellular traffic prediction model. Meanwhile, integrating the multiaccess edge computing framework in 5G cellular networks facilitates the application of intelligent traffic prediction models by enabling their implementation at the network edge. However, the data shortage and privacy issues may still be obstacles for training a robust and accurate prediction model at the edge. To address these issues, we propose a data-augmentation-based cellular traffic prediction model (ctGAN-S2S), where an effective data augmentation submodel based on generative adversarial networks is proposed to improve the prediction performance while protecting data privacy, and a long-short-term-memory-based sequence-to-sequence submodel is used to achieve the flexible multistep cellular traffic prediction. The experimental results on a real-world city-scale cellular traffic dataset reveal that our ctGAN-S2S model achieves up to 48.49% improvement of the prediction accuracy compared to four typical reference models.",smart cities
10.1109/BRAINS49436.2020.9223311,to_check,2020 2nd Conference on Blockchain Research & Applications for Innovative Networks and Services (BRAINS),IEEE,2020-09-30 00:00:00,ieeexplore,Blockchain framework for real-time streaming data generated in image sensor networks for smart monitoring,https://ieeexplore.ieee.org/document/9223311/,"The smart city concept is attracting increasing attention from society. Smart monitoring, which enables the detection and prevention of road traffic accidents, is one promising application of the smart city concept. The deployment of three-dimensional (3D) image sensor networks formed by light detection and ranging (LIDAR) devices interconnected via a network is a key enabler for smart monitoring. Data collected by image sensor networks for smart monitoring are sensitive because the usage of the data is often related to public safety or law enforcement. Managing sensor data using the blockchain technology is one way to address these sensitivity concerns, as a blockchain network helps prevent data from being tampered with even by the administrators of the system. However, prior works have not considered how to handle streaming data such as image sensor data generated by LIDAR devices in real-time, which means there is a risk of overflow in the network if the data are handled frame by frame. In response to this issue, we propose a blockchain framework for real-time monitoring in a smart city using image sensor networks. Our key concept with this framework is to aggregate hash values converted from multiple frames of image sensor data into one hash value. The proposed framework reduces the number of data `writes' on a blockchain network, thus preventing any overflow. Our framework also enables the estimation of the optimal number of aggregated hash values that minimizes delay while avoiding overflow. Measurements taken in actual environments using a real-world LIDAR dataset demonstrated the effectiveness of the proposed framework.",smart cities
10.1109/IJCNN48605.2020.9207457,to_check,2020 International Joint Conference on Neural Networks (IJCNN),IEEE,2020-07-24 00:00:00,ieeexplore,Crowd Flow Forecasting with Multi-Graph Neural Networks,https://ieeexplore.ieee.org/document/9207457/,"Crowd flow forecasting is of great significance for urban traffic management and personal travel planning. Due to the complexity of the urban geographic structure and the highly nonlinear temporal and spatial dependence on crowd flow, accurate forecasting becomes very challenging. Recent research works usually divided cities into regions of the same size and coded as heat-maps, for cities with complex terrain, heat-maps contain many invalid data, which have a negative effect on the acquisition of spatial dependence. In order to decrease the effect, we encode the crowd flow into graphs and propose a multi-graph neural network based model to solve the crowd flow forecasting problem. We first construct two K-NN graphs by the Euclidean distance and the Pearson correlation coefficient respectively and the spatial dependence is captured through the spatial block composed of Graph Attention Networks(GAT) and ChebNet, then another ChebNet is deployed to fuse the spatial dependency of the two graphs. Afterward, we adapted a LSTM to capture the temporal dependence of all regions separately and use self-attention mechanism and fully connected layer to (a) the receptive filed of CNN's filget prediction results. Extensive experiment results based on two real-world datasets demonstrate that our model achieves an important performance on other baselines.",smart cities
10.1109/ICDE51399.2021.00160,to_check,2021 IEEE 37th International Conference on Data Engineering (ICDE),IEEE,2021-04-22 00:00:00,ieeexplore,An Empirical Experiment on Deep Learning Models for Predicting Traffic Data,https://ieeexplore.ieee.org/document/9458663/,"To tackle ever-increasing city traffic congestion problems, researchers have proposed deep learning models to aid decision-makers in the traffic control domain. Although the proposed models have been remarkably improved in recent years, there are still questions that need to be answered before deploying models. For example, it is difficult to figure out which models provide state-of-the-art performance, as recently proposed models have often been evaluated with different datasets and experiment environments. It is also difficult to determine which models would work when traffic conditions change abruptly (e.g., rush hour). In this work, we conduct two experiments to answer the two questions. In the first experiment, we conduct an experiment with the state-of-the-art models and the identical public datasets to compare model performance under a consistent experiment environment. We then extract a set of temporal regions in the datasets, whose speeds change abruptly and use these regions to explore model performance with difficult intervals. The experiment results indicate that Graph-WaveNet and GMAN show better performance in general. We also find that prediction models tend to have varying performances with data and intervals, which calls for in-depth analysis of models on difficult intervals for real-world deployment.",smart cities
10.1109/ICWS.2017.76,to_check,2017 IEEE International Conference on Web Services (ICWS),IEEE,2017-06-30 00:00:00,ieeexplore,Early Air Pollution Forecasting as a Service: An Ensemble Learning Approach,https://ieeexplore.ieee.org/document/8029817/,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system.",smart cities
10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00066,to_check,"2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",IEEE,2020-12-19 00:00:00,ieeexplore,Constructing Optimal Sparse Decision Tree for Analying I-SIG System Attack,https://ieeexplore.ieee.org/document/9443756/,"The Intelligent Traffic Signal System(I-SIG), has been deployed in states including New York, California and Florida by the US Department of Transportation, improving the traffic mobility, connectivity and efficiency in real-world intersections. Unfortunately, a new vulnerability to cause a congestion attack was revealed in 2018, which requires emerging security analysis and reinforcement. From high-dimension traffic features, to further evaluate attack consequences and analyze explainable feature influences, we propose an optimal sparse decision tree(OSDT)-based approach. Through massive feature engineering around attack performing, we finally define and determine 29 features, and collect 195 high-quality samples from VISSIM platform. This is the first standard dataset for statistical I-SIG analysis. Experiment shows that, facing random-deleted sparse traffic feature values, the OSDT can better explain the feature influence with a 78.6% prediction accuracy, compared to other state-of-art decision tree models.",smart cities
10.1109/CCNC49032.2021.9369651,to_check,2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2021-01-12 00:00:00,ieeexplore,Wireless Channel Quality Prediction using Sparse Gaussian Conditional Random Fields,https://ieeexplore.ieee.org/document/9369651/,"Accurate wireless channel quality prediction over 4G LTE networks continues to be an important problem as future channel predictions are widely leveraged to meet the strict requirements of applications such as 360-degree video, ARlVR, and online games. The availability of large amounts of wireless channel data, the increase in computational power and the advancements in the field of machine learning provide us the opportunity to design learning-based approaches to address the channel quality prediction problem. In this paper, we design discriminative sequence-to-sequence probabilistic graphical models, specifically sparse Gaussian Conditional Random Fields (GCRF) models to accurately predict future channel quality variations in 4G LTE networks based on past channel quality data. In contrast to prior work that has primarily focused on designing parsimonious Markovian models or computationally-intensive deep learning models, the sparse GCRF models designed here provide superior performance while being highly interpretable and computationally efficient, thus making them an ideal choice for practical deployment. To validate the efficacy of our sparse GCRF model, we compare its performance (i.e., root mean squared error and mean absolute error) with i) linear regression and ii) ARIMA and iii) the state-of-the-art deep learning model on real-world 4G LTE channel quality data collected under varying levels of user mobility for two cellular operators and observe that the GCRF model provides significantly higher performance improvement.",smart cities
10.1109/TNSM.2019.2948764,to_check,IEEE Transactions on Network and Service Management,IEEE,2019-12-01 00:00:00,ieeexplore,An NFV-Based Service Framework for IoT Applications in Edge Computing Environments,https://ieeexplore.ieee.org/document/8880517/,"Emerging Internet of Things (IoT) applications share the same characteristics of involving multiple processing components (i.e., function modules) and requiring a massive amount of data to be processed with low latency. To meet these needs, edge/fog computing has been proposed for next-generation mobile networks to migrate the computing from the cloud to the edge of the network. Thanks to the development of Network Functions Virtualization (NFV), with which edge computing platform can virtualize function modules and deploy them on any edge devices to provide flexible services on the edge networks. However, such platform would need to deal with complicated function module calling relationship (i.e., call graph) of applications and user mobility, and both are not thoroughly considered by existing works of NFV and edge computing. In this paper, based on our previous idea of virtual local-hub (VLH), we propose a complete design of edge computing framework, which applies NFV technology on edge computing environment for IoT applications. To handle the complicated call graphs of IoT applications with better resource utilization, the VLH framework adapts the technologies of container-based virtualization and microservice architecture, which enables remote function module sharing on the edge computing environment. The framework includes the heuristic algorithm for function module allocation with the objective of minimizing total bandwidth consumption. We also present a design of protocols for system operations and mobility handling in the framework. Then we implement the framework on commodity hardware as a testbed. Via simulations under a large-scale environment with practical settings and experiments on the testbed under real-world scenarios, we demonstrate and verify the effectiveness and practicability of the proposed VLH framework for IoT application service provision.",smart cities
http://arxiv.org/abs/1902.05377v1,to_check,arxiv,arxiv,2019-02-06 08:22:18+00:00,arxiv,UrbanFM: Inferring Fine-Grained Urban Flows,http://arxiv.org/abs/1902.05377v1,"Urban flow monitoring systems play important roles in smart city efforts
around the world. However, the ubiquitous deployment of monitoring devices,
such as CCTVs, induces a long-lasting and enormous cost for maintenance and
operation. This suggests the need for a technology that can reduce the number
of deployed devices, while preventing the degeneration of data accuracy and
granularity. In this paper, we aim to infer the real-time and fine-grained
crowd flows throughout a city based on coarse-grained observations. This task
is challenging due to two reasons: the spatial correlations between coarse- and
fine-grained urban flows, and the complexities of external impacts. To tackle
these issues, we develop a method entitled UrbanFM based on deep neural
networks. Our model consists of two major parts: 1) an inference network to
generate fine-grained flow distributions from coarse-grained inputs by using a
feature extraction module and a novel distributional upsampling module; 2) a
general fusion subnet to further boost the performance by considering the
influences of different external factors. Extensive experiments on two
real-world datasets, namely TaxiBJ and HappyValley, validate the effectiveness
and efficiency of our method compared to seven baselines, demonstrating the
state-of-the-art performance of our approach on the fine-grained urban flow
inference problem.",smart cities
http://arxiv.org/abs/2111.02149v1,to_check,arxiv,arxiv,2021-11-03 11:37:11+00:00,arxiv,"Deployment Optimization for Shared e-Mobility Systems with Multi-agent
  Deep Neural Search",http://arxiv.org/abs/2111.02149v1,"Shared e-mobility services have been widely tested and piloted in cities
across the globe, and already woven into the fabric of modern urban planning.
This paper studies a practical yet important problem in those systems: how to
deploy and manage their infrastructure across space and time, so that the
services are ubiquitous to the users while sustainable in profitability.
However, in real-world systems evaluating the performance of different
deployment strategies and then finding the optimal plan is prohibitively
expensive, as it is often infeasible to conduct many iterations of
trial-and-error. We tackle this by designing a high-fidelity simulation
environment, which abstracts the key operation details of the shared e-mobility
systems at fine-granularity, and is calibrated using data collected from the
real-world. This allows us to try out arbitrary deployment plans to learn the
optimal given specific context, before actually implementing any in the
real-world systems. In particular, we propose a novel multi-agent neural search
approach, in which we design a hierarchical controller to produce tentative
deployment plans. The generated deployment plans are then tested using a
multi-simulation paradigm, i.e., evaluated in parallel, where the results are
used to train the controller with deep reinforcement learning. With this closed
loop, the controller can be steered to have higher probability of generating
better deployment plans in future iterations. The proposed approach has been
evaluated extensively in our simulation environment, and experimental results
show that it outperforms baselines e.g., human knowledge, and state-of-the-art
heuristic-based optimization approaches in both service coverage and net
revenue.",smart cities
http://arxiv.org/abs/2109.13963v1,to_check,arxiv,arxiv,2021-09-28 18:09:29+00:00,arxiv,"Smart at what cost? Characterising Mobile Deep Neural Networks in the
  wild",http://arxiv.org/abs/2109.13963v1,"With smartphones' omnipresence in people's pockets, Machine Learning (ML) on
mobile is gaining traction as devices become more powerful. With applications
ranging from visual filters to voice assistants, intelligence on mobile comes
in many forms and facets. However, Deep Neural Network (DNN) inference remains
a compute intensive workload, with devices struggling to support intelligence
at the cost of responsiveness.On the one hand, there is significant research on
reducing model runtime requirements and supporting deployment on embedded
devices. On the other hand, the strive to maximise the accuracy of a task is
supported by deeper and wider neural networks, making mobile deployment of
state-of-the-art DNNs a moving target.
  In this paper, we perform the first holistic study of DNN usage in the wild
in an attempt to track deployed models and match how these run on widely
deployed devices. To this end, we analyse over 16k of the most popular apps in
the Google Play Store to characterise their DNN usage and performance across
devices of different capabilities, both across tiers and generations.
Simultaneously, we measure the models' energy footprint, as a core cost
dimension of any mobile deployment. To streamline the process, we have
developed gaugeNN, a tool that automates the deployment, measurement and
analysis of DNNs on devices, with support for different frameworks and
platforms. Results from our experience study paint the landscape of deep
learning deployments on smartphones and indicate their popularity across app
developers. Furthermore, our study shows the gap between bespoke techniques and
real-world deployments and the need for optimised deployment of deep learning
models in a highly dynamic and heterogeneous ecosystem.",smart cities
http://arxiv.org/abs/2105.05504v1,to_check,arxiv,arxiv,2021-05-12 08:28:12+00:00,arxiv,"An Empirical Experiment on Deep Learning Models for Predicting Traffic
  Data",http://arxiv.org/abs/2105.05504v1,"To tackle ever-increasing city traffic congestion problems, researchers have
proposed deep learning models to aid decision-makers in the traffic control
domain. Although the proposed models have been remarkably improved in recent
years, there are still questions that need to be answered before deploying
models. For example, it is difficult to figure out which models provide
state-of-the-art performance, as recently proposed models have often been
evaluated with different datasets and experiment environments. It is also
difficult to determine which models would work when traffic conditions change
abruptly (e.g., rush hour). In this work, we conduct two experiments to answer
the two questions. In the first experiment, we conduct an experiment with the
state-of-the-art models and the identical public datasets to compare model
performance under a consistent experiment environment. We then extract a set of
temporal regions in the datasets, whose speeds change abruptly and use these
regions to explore model performance with difficult intervals. The experiment
results indicate that Graph-WaveNet and GMAN show better performance in
general. We also find that prediction models tend to have varying performances
with data and intervals, which calls for in-depth analysis of models on
difficult intervals for real-world deployment.",smart cities
http://arxiv.org/abs/2101.11800v1,to_check,arxiv,arxiv,2021-01-28 03:30:04+00:00,arxiv,"AdaSpring: Context-adaptive and Runtime-evolutionary Deep Model
  Compression for Mobile Applications",http://arxiv.org/abs/2101.11800v1,"There are many deep learning (e.g., DNN) powered mobile and wearable
applications today continuously and unobtrusively sensing the ambient
surroundings to enhance all aspects of human lives. To enable robust and
private mobile sensing, DNN tends to be deployed locally on the
resource-constrained mobile devices via model compression. The current practice
either hand-crafted DNN compression techniques, i.e., for optimizing
DNN-relative performance (e.g., parameter size), or on-demand DNN compression
methods, i.e., for optimizing hardware-dependent metrics (e.g., latency),
cannot be locally online because they require offline retraining to ensure
accuracy. Also, none of them have correlated their efforts with runtime
adaptive compression to consider the dynamic nature of the deployment context
of mobile applications. To address those challenges, we present AdaSpring, a
context-adaptive and self-evolutionary DNN compression framework. It enables
the runtime adaptive DNN compression locally online. Specifically, it presents
the ensemble training of a retraining-free and self-evolutionary network to
integrate multiple alternative DNN compression configurations (i.e., compressed
architectures and weights). It then introduces the runtime search strategy to
quickly search for the most suitable compression configurations and evolve the
corresponding weights. With evaluation on five tasks across three platforms and
a real-world case study, experiment outcomes show that AdaSpring obtains up to
3.1x latency reduction, 4.2 x energy efficiency improvement in DNNs, compared
to hand-crafted compression techniques, while only incurring <= 6.2ms
runtime-evolution latency.",smart cities
http://arxiv.org/abs/2005.00953v1,to_check,arxiv,arxiv,2020-05-03 00:12:38+00:00,arxiv,"Deep Generative Adversarial Residual Convolutional Networks for
  Real-World Super-Resolution",http://arxiv.org/abs/2005.00953v1,"Most current deep learning based single image super-resolution (SISR) methods
focus on designing deeper / wider models to learn the non-linear mapping
between low-resolution (LR) inputs and the high-resolution (HR) outputs from a
large number of paired (LR/HR) training data. They usually take as assumption
that the LR image is a bicubic down-sampled version of the HR image. However,
such degradation process is not available in real-world settings i.e. inherent
sensor noise, stochastic noise, compression artifacts, possible mismatch
between image degradation process and camera device. It reduces significantly
the performance of current SISR methods due to real-world image corruptions. To
address these problems, we propose a deep Super-Resolution Residual
Convolutional Generative Adversarial Network (SRResCGAN) to follow the
real-world degradation settings by adversarial training the model with
pixel-wise supervision in the HR domain from its generated LR counterpart. The
proposed network exploits the residual learning by minimizing the energy-based
objective function with powerful image regularization and convex optimization
techniques. We demonstrate our proposed approach in quantitative and
qualitative experiments that generalize robustly to real input and it is easy
to deploy for other down-scaling operators and mobile/embedded devices.",smart cities
http://arxiv.org/abs/1904.01735v1,to_check,arxiv,arxiv,2019-04-03 01:29:48+00:00,arxiv,"Multi-Modal Generative Adversarial Network for Short Product Title
  Generation in Mobile E-Commerce",http://arxiv.org/abs/1904.01735v1,"Nowadays, more and more customers browse and purchase products in favor of
using mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are
usually inclined to describe redundant and over-informative product titles to
attract attentions from customers, it is important to concisely display short
product titles on limited screen of mobile phones. To address this discrepancy,
previous studies mainly consider textual information of long product titles and
lacks of human-like view during training and evaluation process. In this paper,
we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short
product title generation in E-Commerce, which innovatively incorporates image
information and attribute tags from product, as well as textual information
from original long titles. MM-GAN poses short title generation as a
reinforcement learning process, where the generated titles are evaluated by the
discriminator in a human-like view. Extensive experiments on a large-scale
E-Commerce dataset demonstrate that our algorithm outperforms other
state-of-the-art methods. Moreover, we deploy our model into a real-world
online E-Commerce environment and effectively boost the performance of click
through rate and click conversion rate by 1.66% and 1.87%, respectively.",smart cities
http://arxiv.org/abs/2007.03639v3,to_check,arxiv,arxiv,2020-07-07 17:19:56+00:00,arxiv,Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,http://arxiv.org/abs/2007.03639v3,"Since the past few decades, human trajectory forecasting has been a field of
active research owing to its numerous real-world applications: evacuation
situation analysis, deployment of intelligent transport systems, traffic
operations, to name a few. Early works handcrafted this representation based on
domain knowledge. However, social interactions in crowded environments are not
only diverse but often subtle. Recently, deep learning methods have
outperformed their handcrafted counterparts, as they learned about human-human
interactions in a more generic data-driven fashion. In this work, we present an
in-depth analysis of existing deep learning-based methods for modelling social
interactions. We propose two knowledge-based data-driven methods to effectively
capture these social interactions. To objectively compare the performance of
these interaction-based forecasting models, we develop a large scale
interaction-centric benchmark TrajNet++, a significant yet missing component in
the field of human trajectory forecasting. We propose novel performance metrics
that evaluate the ability of a model to output socially acceptable
trajectories. Experiments on TrajNet++ validate the need for our proposed
metrics, and our method outperforms competitive baselines on both real-world
and synthetic datasets.",smart cities
http://arxiv.org/abs/2010.05842v2,to_check,arxiv,arxiv,2020-10-12 16:46:40+00:00,arxiv,Remote Electrical Tilt Optimization via Safe Reinforcement Learning,http://arxiv.org/abs/2010.05842v2,"Remote Electrical Tilt (RET) optimization is an efficient method for
adjusting the vertical tilt angle of Base Stations (BSs) antennas in order to
optimize Key Performance Indicators (KPIs) of the network. Reinforcement
Learning (RL) provides a powerful framework for RET optimization because of its
self-learning capabilities and adaptivity to environmental changes. However, an
RL agent may execute unsafe actions during the course of its interaction, i.e.,
actions resulting in undesired network performance degradation. Since the
reliability of services is critical for Mobile Network Operators (MNOs), the
prospect of performance degradation has prohibited the real-world deployment of
RL methods for RET optimization. In this work, we model the RET optimization
problem in the Safe Reinforcement Learning (SRL) framework with the goal of
learning a tilt control strategy providing performance improvement guarantees
with respect to a safe baseline. We leverage a recent SRL method, namely Safe
Policy Improvement through Baseline Bootstrapping (SPIBB), to learn an improved
policy from an offline dataset of interactions collected by the safe baseline.
Our experiments show that the proposed approach is able to learn a safe and
improved tilt update policy, providing a higher degree of reliability and
potential for real-world network deployment.",smart cities
http://arxiv.org/abs/2110.07206v1,to_check,arxiv,arxiv,2021-10-14 08:03:33+00:00,arxiv,"Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad
  Weather",http://arxiv.org/abs/2110.07206v1,"Visual perception in autonomous driving is a crucial part of a vehicle to
navigate safely and sustainably in different traffic conditions. However, in
bad weather such as heavy rain and haze, the performance of visual perception
is greatly affected by several degrading effects. Recently, deep learning-based
perception methods have addressed multiple degrading effects to reflect
real-world bad weather cases but have shown limited success due to 1) high
computational costs for deployment on mobile devices and 2) poor relevance
between image enhancement and visual perception in terms of the model ability.
To solve these issues, we propose a task-driven image enhancement network
connected to the high-level vision task, which takes in an image corrupted by
bad weather as input. Specifically, we introduce a novel low memory network to
reduce most of the layer connections of dense blocks for less memory and
computational cost while maintaining high performance. We also introduce a new
task-driven training strategy to robustly guide the high-level task model
suitable for both high-quality restoration of images and highly accurate
perception. Experiment results demonstrate that the proposed method improves
the performance among lane and 2D object detection, and depth estimation
largely under adverse weather in terms of both low memory and accuracy.",smart cities
http://arxiv.org/abs/2108.00505v1,to_check,arxiv,arxiv,2021-08-01 17:33:04+00:00,arxiv,"DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in
  Highways",http://arxiv.org/abs/2108.00505v1,"Vehicle trajectory prediction is an essential task for enabling many
intelligent transportation systems. While there have been some promising
advances in the field, there is a need for new agile algorithms with smaller
model sizes and lower computational requirements. This article presents
DeepTrack, a novel deep learning algorithm customized for real-time vehicle
trajectory prediction in highways. In contrast to previous methods, the vehicle
dynamics are encoded using Agile Temporal Convolutional Networks (ATCNs) to
provide more robust time prediction with less computation. ATCN also uses
depthwise convolution, which reduces the complexity of models compared to
existing approaches in terms of model size and operations. Overall, our
experimental results demonstrate that DeepTrack achieves comparable accuracy to
state-of-the-art trajectory prediction models but with smaller model sizes and
lower computational complexity, making it more suitable for real-world
deployment.",smart cities
http://arxiv.org/abs/2003.12228v1,to_check,arxiv,arxiv,2020-03-27 04:10:06+00:00,arxiv,Mechanism Design for Wireless Powered Spatial Crowdsourcing Networks,http://arxiv.org/abs/2003.12228v1,"Wireless power transfer (WPT) is a promising technology to prolong the
lifetime of the sensors and communication devices, i.e., workers, in completing
crowdsourcing tasks by providing continuous and cost-effective energy supplies.
In this paper, we propose a wireless powered spatial crowdsourcing framework
which consists of two mutually dependent phases: task allocation phase and data
crowdsourcing phase. In the task allocation phase, we propose a Stackelberg
game based mechanism for the spatial crowdsourcing platform to efficiently
allocate spatial tasks and wireless charging power to each worker. In the data
crowdsourcing phase, the workers may have an incentive to misreport its real
working location to improve its utility, which causes adverse effects to the
spatial crowdsourcing platform. To address this issue, we present three
strategyproof deployment mechanisms for the spatial crowdsourcing platform to
place a mobile base station, e.g., vehicle or robot, which is responsible for
transferring the wireless power and collecting the crowdsourced data. As the
benchmark, we first apply the classical median mechanism and evaluate its
worst-case performance. Then, we design a conventional strategyproof deployment
mechanism to improve the expected utility of the spatial crowdsourcing platform
under the condition that the workers' locations follow a known geographical
distribution. For a more general case with only the historical location data
available, we propose a deep learning based strategyproof deployment mechanism
to maximize the spatial crowdsourcing platform's utility. Extensive
experimental results based on synthetic and real-world datasets reveal the
effectiveness of the proposed framework in allocating tasks and charging power
to workers while avoiding the dishonest worker's manipulation.",smart cities
http://arxiv.org/abs/2009.10031v1,to_check,arxiv,arxiv,2020-09-21 17:12:33+00:00,arxiv,Training Production Language Models without Memorizing User Data,http://arxiv.org/abs/2009.10031v1,"This paper presents the first consumer-scale next-word prediction (NWP) model
trained with Federated Learning (FL) while leveraging the Differentially
Private Federated Averaging (DP-FedAvg) technique. There has been prior work on
building practical FL infrastructure, including work demonstrating the
feasibility of training language models on mobile devices using such
infrastructure. It has also been shown (in simulations on a public corpus) that
it is possible to train NWP models with user-level differential privacy using
the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models
with DP-FedAvg in a real-world production environment on a heterogeneous fleet
of mobile phones requires addressing numerous challenges. For instance, the
coordinating central server has to keep track of the devices available at the
start of each round and sample devices uniformly at random from them, while
ensuring \emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL
work of which we are aware, for the first time we demonstrate the deployment of
a differentially private mechanism for the training of a production neural
network in FL, as well as the instrumentation of the production training
infrastructure to perform an end-to-end empirical measurement of unintended
memorization.",smart cities
10.1016/j.sysarc.2021.102016,to_check,Journal of Systems Architecture,scopus,2021-05-01,sciencedirect,SimEdgeIntel: A open-source simulation platform for resource management in edge intelligence,https://api.elsevier.com/content/abstract/scopus_id/85099788069,"To meet the challenges posed by the explosive growth of mobile traffic, data caching at the network edge has been considered a key technology in future mobile networks, while the potential of device-to-device (D2D) communications in areas such as traffic offloading is also of great interest. Existing work does not have a network switching mechanism, which would ensure load balancing and improve quality of service are also ignored. Existing simulators perform poorly in terms of algorithmic compatibility, and require a high level of coding ability which are difficult to get started. In this paper, an edge simulator called SimEdgeIntel is presented for resource management that opens up detailed configuration options, enabling researchers quickly deploy mobile with edge intelligence. It supports researchers to customize the development of mobility models, caching algorithms and switching strategies. The interface-oriented system architecture helps researchers achieve cross-platform and cross-language algorithm import with machine learning techniques. In the experimental section, we perform a comprehensive evaluation of SimEdgeIntel based on real-world tracing, proving its scalability and effectiveness in terms of cache hit rate, delivery latency, and backhaul traffic, and evaluating its performance in terms of CPU and memory, respectively.",smart cities
10.1016/j.procs.2019.09.181,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,A quantum genetic algorithm for pickup and delivery problems with coalition formation,https://api.elsevier.com/content/abstract/scopus_id/85076257816,"With the “last mile” of the delivery process being the most expensive phase, autonomous package delivery systems are gaining traction as they aim for faster and cheaper delivery of goods to city, urban and rural destinations. This interest is further fueled by the emergence of e-commerce, where many applications can benefit from autonomous package delivery solutions. However, the environment stochasticity, variability and task complexity for autonomous operation make it difficult to deploy such systems in real-world applications without the incorporation of advanced machine learning and optimization algorithms. Moving away from designing a “one size fits all” agent to solve the outdoor package delivery problem and considering ad-hoc teams of agents trained within a data-driven framework could provide the answer. In this work, we propose a delivery scheduling algorithm for heterogeneous multi-agent systems using the pickup and delivery problem (PDP) formulation. Specifically, a 3-index mixed integer program-based PDP that allows coalition formation (PDP-CF) among agents is derived to allow multi-agent PDP schedules. We propose a quantum genetic algorithm to solve for the schedule since it is can better handle the large computational complexity of PDP-CF. Multiple PDP scenario simulations show the merits of the proposed approach.",smart cities
10.1016/S0952-1976(01)00010-0,to_check,Engineering Applications of Artificial Intelligence,scopus,2001-06-01,sciencedirect,Opportunistic planning for a fleet of transportation robots,https://api.elsevier.com/content/abstract/scopus_id/0035366212,"The Dynamic Transportation-Planning Problem (DTPP) embodies a class of real-world applications that involve the reactive routing and scheduling of a fleet of vehicles in response to dynamically changing transportation demands. Examples include mobile robots in a warehouse, taxis in an urban road network, or aeroplanes for medical evacuation. In contrast with the Vehicle Routing Problem, for which a plethora of techniques is available, few approaches exist that permit the efficient deployment of a large number of vehicles in a changing environment. This paper highlights the characteristic features of the problem, reviews possible approaches and existing techniques, and proposes a heuristic solution to the DTPP using a Blackboard-based approach. The resulting application is an intelligent transportation planning system (ITPS) for a fleet of automated robot taxis, based on the generic assumption-based truth maintained Blackboard shell (GATMBS) and comprising traffic simulation models as well as various monitoring and problem-solving strategies for assignment and routing. The Blackboard architecture supports the dynamic alteration of planned routes in response to changes in traffic conditions and passenger requests. A prototype of the ITPS has been validated in simulation using a small fleet of robot taxis with randomly generated road networks, background traffic load, and passenger requests.",smart cities
10.1109/OJCOMS.2021.3122844,to_check,IEEE Open Journal of the Communications Society,IEEE,2021-01-01 00:00:00,ieeexplore,A Deep Neural Network-Based Multi-Label Classifier for SLA Violation Prediction in a Latency Sensitive NFV Application,https://ieeexplore.ieee.org/document/9592636/,"Recent advancements in the domain of Network Function Virtualization (NFV), and rollout of next-generation networks have necessitated the requirement for the upkeep of latency-critical application architectures in future networks and communications. While Cloud service providers recognize the evolving mission-critical requirements in latency sensitive verticals such as autonomous driving, multimedia, gaming, telecommunications, and virtual reality, there is a wide gap to bridge the Quality of Service (QoS) constraints for the end-user experience. Most latency-critical services are over-provisioned on all fronts to offer reliability, which is inefficient towards scalability in the long run. To address this, we propose a strategy to model frequent violations on the application level as a multi-output target to enable more complex decision-making in the management of virtualised communication networks. In this work, we utilize data from a real-world deployment to configure and draft a realistic set of Service Level Objectives (SLOs) for a voice based NFV application, and develop a deep neural network based multi-label classification methodology to identify and predict multiple categories of SLO breaches associated with an application state. With this, we aim to gain granular SLA and SLO violation insights, enabling us to study and mitigate their impact and inform precision in drafting proactive scaling policies. We further compare the performance against a set of multi-label compatible machine learning classifiers, and address class imbalance in a multi-label setup. We perform a comprehensive evaluation to assess the performance on example-based, label-based and ranking-based measures, and demonstrate the suitability of deep learning in such a use-case.",multimedia
10.1109/CVPR42600.2020.00203,to_check,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2020-06-19 00:00:00,ieeexplore,AANet: Adaptive Aggregation Network for Efficient Stereo Matching,https://ieeexplore.ieee.org/document/9157353/,"Despite the remarkable progress made by learning based stereo matching algorithms, one key challenge remains unsolved. Current state-of-the-art stereo models are mostly based on costly 3D convolutions, the cubic computational complexity and high memory consumption make it quite expensive to deploy in real-world applications. In this paper, we aim at completely replacing the commonly used 3D convolutions to achieve fast inference speed while maintaining comparable accuracy. To this end, we first propose a sparse points based intra-scale cost aggregation method to alleviate the well-known edge-fattening issue at disparity discontinuities. Further, we approximate traditional cross-scale cost aggregation algorithm with neural network layers to handle large textureless regions. Both modules are simple, lightweight, and complementary, leading to an effective and efficient architecture for cost aggregation. With these two modules, we can not only significantly speed up existing top-performing models (e.g., 41× than GC-Net, 4× than PSMNet and 38× than GANet), but also improve the performance offast stereo models (e.g., StereoNet). We also achieve competitive results on Scene Flow and KITTI datasets while running at 62ms, demonstrating the versatility and high efficiency of the proposed method. Our fullframework is available at https: //github.com/haofeixu/aanet.",multimedia
10.1109/ICCV.2019.00988,to_check,2019 IEEE/CVF International Conference on Computer Vision (ICCV),IEEE,2019-11-02 00:00:00,ieeexplore,Mesh R-CNN,https://ieeexplore.ieee.org/document/9008508/,"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.",multimedia
10.1109/ICMEW46912.2020.9106051,to_check,2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),IEEE,2020-07-10 00:00:00,ieeexplore,Automatic Key Moment Extraction and Highlights Generation Based on Comprehensive Soccer Video Understanding,https://ieeexplore.ieee.org/document/9106051/,"The massive growth of sports video has resulted in a need for automatic generation of sports highlights that are comparable in quality to the hand-edited highlights produced by editors. Many methods have been applied to this task and have achieved some positive results. Unlike previous works ignoring the multi-modality information, we propose a novel system that leverages visual and audio information derived from the soccer video. The proposed system involves three crucial tasks which can be jointly used to produce highlights automatically, i.e. play-back detection, soccer event recognition and commentator emotion classification. We introduce a new dataset of 460 soccer games totaling 700 hours with a benchmark for three tasks. Making use of recent progress in deep learning, we further provide strong baselines on three tasks. The experiments on the proposed dataset demonstrate state-of-the-art performance on each independent task. The real-world deployment shows that this system can be useful for soccer games to find and extract soccer video highlights.",multimedia
10.1109/BIGCOM.2018.00037,to_check,2018 4th International Conference on Big Data Computing and Communications (BIGCOM),IEEE,2018-08-09 00:00:00,ieeexplore,Geo-Edge: Geographical Resource Allocation on Edge Caches for Video-on-Demand Streaming,https://ieeexplore.ieee.org/document/8488647/,"Geographical information has shown great potential in optimizing the resource allocation for Video-on-Demand (VoD) systems, e.g., the VoD service provider can allocate more bandwidth/computing resources to certain regions where more user requests are generated. Recently, the deployment of edge caches close to client users has become a cost-effective solution to the large-scale VoD system, as popular video content can be placed in edge caches so as to reduce the response latency of user requests, and save the bandwidth consumption on CDN edge servers. In this paper, we propose an edge-cache-assisted VoD system, named Geo-edge. The system develops a machine learning approach-a modified LSTM (Long Short Term Memory) network-to predict the amount and distribution of requests to each video, and proactively place appropriate video resources in the buffer of edge caches. We conduct emulations over the traces of real-world video sessions in ten different autonomous systems in the backbone network of China. Results show that the length of the path between source and clients could decrease by 29% with the help of edge caches, while Geo-Edge can significantly save the expenditure of CDN bandwidth under two typical charging policies of Internet service providers (i.e., a reduction of 45% when charged by overall throughput, or 35% when charged by peak bandwidth consumption).",multimedia
10.1109/JIOT.2019.2944889,to_check,IEEE Internet of Things Journal,IEEE,2020-03-01 00:00:00,ieeexplore,JointRec: A Deep-Learning-Based Joint Cloud Video Recommendation Framework for Mobile IoT,https://ieeexplore.ieee.org/document/8854245/,"In the era of Internet of Things (IoT), watching videos on mobile devices has been a popular application in our daily life. How to recommend videos to users is one of the most concerned problem for Internet video service providers (IVSPs). In order to provide better recommendation service to users, they deploy cloud servers in a geo-distributed manner. Each server is responsible for analyzing a local area of user data. Therefore, these cloud servers form information islands and the characteristics of data present nonindependent and identically distribution (non-i.i.d). In this scenario, it is difficult to provide accurate video recommendation service to the minority of users in each area. To tackle this issue, we propose JointRec, a deep learning-based joint cloud video recommendation framework. JointRec integrates the JointCloud architecture into mobile IoT and achieves federated training among distributed cloud servers. Specifically, we first design a dual-convolutional probabilistic matrix factorization (Dual-CPMF) model to conduct video recommendation. Based on this model, each cloud can recommend videos by exploiting the user's profiles and description of videos that users rate, thereby providing more accurate video recommendation services. Then, we present a federated recommendation algorithm which enables each cloud to share their weights and train a model cooperatively. Furthermore, considering the heavy communication costs in the process of federated training, we combine low-rank matrix factorization and 8-bit quantization method to reduce uplink communication costs and network bandwidth. We validate the proposed approach on the real-world data set, and the experimental results indicate the effectiveness of our proposed approach.",multimedia
10.1109/ICC.2018.8422858,to_check,2018 IEEE International Conference on Communications (ICC),IEEE,2018-05-24 00:00:00,ieeexplore,ATDPS: An Adaptive Time-Dependent Push Strategy in Hybrid CDN-P2P VoD System,https://ieeexplore.ieee.org/document/8422858/,"Online video service has become an emerging application of Internet, playing an important and indispensable role in our daily life. Aiming at relieving the heavy burden on CDN servers, many video service providers deploy a hybrid CDN-P2P system. To minimize the cost of the ""last mile delivery"" of CDN servers, the system should be able to dispatch video files that receive a large amount of view requests during peak hours to P2P network in advance. However, to accurately predict whether a video will gain popularity is difficult due to the uncertainty of people's viewing tendency. In this paper, we reveal the correlation between the popularity of videos in the daytime and the popularity in peak hours at night based on statistics collected in a real-world large commercial VoD system. Based on our findings, we propose Adaptive Time-Dependent Push Strategy (ATDPS), a scalable push strategy that implements adaptive time-dependent techniques in the hybrid CDN-P2P system. A deep neural network is leveraged to predict the demand of hot videos during peak hours at night. Besides, the system adaptively adjusts the time-dependent strategy of pushing the content of hot videos to the P2P network according to the seed scarcity of the videos in different time periods. Simulations over historical data and pilot deployment on smart routers show that our ATDPS can significantly decrease the addressed bandwidth consumption at peak hours.",multimedia
10.1109/ICMLA51294.2020.00030,to_check,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),IEEE,2020-12-17 00:00:00,ieeexplore,Efficient and Compact Convolutional Neural Network Architectures for Non-temporal Real-time Fire Detection,https://ieeexplore.ieee.org/document/9356284/,"Automatic visual fire detection is used to complement traditional fire detection sensor systems (smoke/heat). In this work, we investigate different Convolutional Neural Network (CNN) architectures and their variants for the non-temporal real-time bounds detection of fire pixel regions in video (or still) imagery. Two reduced complexity compact CNN architectures (NasNet-A-OnFire and ShuffleNetV2-OnFire) are proposed through experimental analysis to optimise the computational efficiency for this task. The results improve upon the current state-of-the-art solution for fire detection, achieving an accuracy of 95% for full-frame binary classification and 97% for superpixel localisation. We notably achieve a classification speed up by a factor of 2.3× for binary classification and 1.3× for superpixel localisation, with runtime of 40 fps and 18 fps respectively, outperforming prior work in the field presenting an efficient, robust and real-time solution for fire region detection. Subsequent implementation on low-powered devices (Nvidia Xavier-NX, achieving 49 fps for full-frame classification via ShuffleNetV2-OnFire) demonstrates our architectures are suitable for various real-world deployment applications.",multimedia
10.1109/APSIPAASC47483.2019.9023182,to_check,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),IEEE,2019-11-21 00:00:00,ieeexplore,Gun Detection in Surveillance Videos using Deep Neural Networks,https://ieeexplore.ieee.org/document/9023182/,"The ongoing epidemic of gun violence worldwide has compelled various agencies, businesses and consumers to deploy closed-circuit television (CCTV) surveillance cameras in attempt to combat this epidemic. An active-based CCTV system extends this platform to autonomously detect potential firearms within a video surveillance perspective. However, autonomously detecting a firearm across varying CCTV camera angles, depth and illumination represents an arduous task which has seen limited success using existing deep neural networks models. This challenge is in part due to the lack of available contextual hand gun information from CCTV images, which remains unresolved. As such, this paper introduces a novel large scale dataset of hand guns which were captured using a CCTV camera. This dataset serves to substantially improve the state-of-the-art in representation learning of hand guns within a surveillance perspective. The proposed dataset consist of 250 recorded CCTV videos with a total of 5500 images. Each annotated CCTV image realistically captures the presence of a hand gun under 1) varying outdoor and indoor conditions, and 2) different resolutions representing variable scales and depth of a gun relative to a cameras sensor. The proposed dataset is used to train a single-stage object detector using a multi-level feature pyramid network (i.e. M2Det). The trained network is then validated using images from the UCF crime video dataset which contains real-world gun violence. Experimental results indicate that the proposed dataset increases the average precision of gun detection at different scales by as much as 18% when compared to existing approaches in firearms detection.",multimedia
10.1109/TNNLS.2013.2248094,to_check,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2014-01-01 00:00:00,ieeexplore,PCA Feature Extraction for Change Detection in Multidimensional Unlabeled Data,https://ieeexplore.ieee.org/document/6479367/,"When classifiers are deployed in real-world applications, it is assumed that the distribution of the incoming data matches the distribution of the data used to train the classifier. This assumption is often incorrect, which necessitates some form of change detection or adaptive classification. While there has been a lot of work on change detection based on the classification error monitored over the course of the operation of the classifier, finding changes in multidimensional unlabeled data is still a challenge. Here, we propose to apply principal component analysis (PCA) for feature extraction prior to the change detection. Supported by a theoretical example, we argue that the components with the lowest variance should be retained as the extracted features because they are more likely to be affected by a change. We chose a recently proposed semiparametric log-likelihood change detection criterion that is sensitive to changes in both mean and variance of the multidimensional distribution. An experiment with 35 datasets and an illustration with a simple video segmentation demonstrate the advantage of using extracted features compared to raw data. Further analysis shows that feature extraction through PCA is beneficial, specifically for data with multiple balanced classes.",multimedia
10.1007/978-3-642-10467-1_42,to_check,Advances in Multimedia Information Processing - PCM 2009,Springer,2009-01-01 00:00:00,springer,A Neural Network Based Framework for Audio Scene Analysis in Audio Sensor Networks,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-642-10467-1_42,"In recent years, the audio sensor networks have been paid much attention. One of the most important applications of audio sensor networks is audio scene analysis. In this paper, we present a neural network based framework for analyzing the audio scene in the audio sensor networks. In the proposed framework, basic audio events are modeled and detected by Hidden Markov Models (HMMs) in the audio sensor nodes. The cluster head collects the sensory information in its cluster, and then a neural network based approach is proposed to discover the high-level semantic content of the audio context. With the neural network based approach, human knowledge and machine learning are effectively combined together in the semantic inference. That is, the model parameters are learned by statistical learning and then modified manually based on the prior knowledge. We deploy the proposed framework on an audio sensor network and do a series of experiments to evaluate its performance. The experimental results show that our method can work well in the complex real-world situations.",multimedia
http://arxiv.org/abs/2109.09828v1,to_check,arxiv,arxiv,2021-09-20 20:17:40+00:00,arxiv,iRNN: Integer-only Recurrent Neural Network,http://arxiv.org/abs/2109.09828v1,"Recurrent neural networks (RNN) are used in many real-world text and speech
applications. They include complex modules such as recurrence,
exponential-based activation, gate interaction, unfoldable normalization,
bi-directional dependence, and attention. The interaction between these
elements prevents running them on integer-only operations without a significant
performance drop. Deploying RNNs that include layer normalization and attention
on integer-only arithmetic is still an open problem. We present a
quantization-aware training method for obtaining a highly accurate integer-only
recurrent neural network (iRNN). Our approach supports layer normalization,
attention, and an adaptive piecewise linear approximation of activations, to
serve a wide range of RNNs on various applications. The proposed method is
proven to work on RNN-based language models and automatic speech recognition.
Our iRNN maintains similar performance as its full-precision counterpart, their
deployment on smartphones improves the runtime performance by $2\times$, and
reduces the model size by $4\times$.",multimedia
http://arxiv.org/abs/2101.02000v1,to_check,arxiv,arxiv,2021-01-06 13:15:21+00:00,arxiv,Weakly-Supervised Multi-Face 3D Reconstruction,http://arxiv.org/abs/2101.02000v1,"3D face reconstruction plays a very important role in many real-world
multimedia applications, including digital entertainment, social media,
affection analysis, and person identification. The de-facto pipeline for
estimating the parametric face model from an image requires to firstly detect
the facial regions with landmarks, and then crop each face to feed the deep
learning-based regressor. Comparing to the conventional methods performing
forward inference for each detected instance independently, we suggest an
effective end-to-end framework for multi-face 3D reconstruction, which is able
to predict the model parameters of multiple instances simultaneously using
single network inference. Our proposed approach not only greatly reduces the
computational redundancy in feature extraction but also makes the deployment
procedure much easier using the single network model. More importantly, we
employ the same global camera model for the reconstructed faces in each image,
which makes it possible to recover the relative head positions and orientations
in the 3D scene. We have conducted extensive experiments to evaluate our
proposed approach on the sparse and dense face alignment tasks. The
experimental results indicate that our proposed approach is very promising on
face alignment tasks without fully-supervision and pre-processing like
detection and crop. Our implementation is publicly available at
\url{https://github.com/kalyo-zjl/WM3DR}.",multimedia
http://arxiv.org/abs/2111.01777v1,to_check,arxiv,arxiv,2021-11-02 17:53:54+00:00,arxiv,"A Framework for Real-World Multi-Robot Systems Running Decentralized
  GNN-Based Policies",http://arxiv.org/abs/2111.01777v1,"Graph Neural Networks (GNNs) are a paradigm-shifting neural architecture to
facilitate the learning of complex multi-agent behaviors. Recent work has
demonstrated remarkable performance in tasks such as flocking, multi-agent path
planning and cooperative coverage. However, the policies derived through
GNN-based learning schemes have not yet been deployed to the real-world on
physical multi-robot systems. In this work, we present the design of a system
that allows for fully decentralized execution of GNN-based policies. We create
a framework based on ROS2 and elaborate its details in this paper. We
demonstrate our framework on a case-study that requires tight coordination
between robots, and present first-of-a-kind results that show successful
real-world deployment of GNN-based policies on a decentralized multi-robot
system relying on Adhoc communication. A video demonstration of this case-study
can be found online. https://www.youtube.com/watch?v=COh-WLn4iO4",multimedia
http://arxiv.org/abs/2009.05835v3,to_check,arxiv,arxiv,2020-09-12 17:37:36+00:00,arxiv,"How Much Can We Really Trust You? Towards Simple, Interpretable Trust
  Quantification Metrics for Deep Neural Networks",http://arxiv.org/abs/2009.05835v3,"A critical step to building trustworthy deep neural networks is trust
quantification, where we ask the question: How much can we trust a deep neural
network? In this study, we take a step towards simple, interpretable metrics
for trust quantification by introducing a suite of metrics for assessing the
overall trustworthiness of deep neural networks based on their behaviour when
answering a set of questions. We conduct a thought experiment and explore two
key questions about trust in relation to confidence: 1) How much trust do we
have in actors who give wrong answers with great confidence? and 2) How much
trust do we have in actors who give right answers hesitantly? Based on insights
gained, we introduce the concept of question-answer trust to quantify
trustworthiness of an individual answer based on confident behaviour under
correct and incorrect answer scenarios, and the concept of trust density to
characterize the distribution of overall trust for an individual answer
scenario. We further introduce the concept of trust spectrum for representing
overall trust with respect to the spectrum of possible answer scenarios across
correctly and incorrectly answered questions. Finally, we introduce
NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite
of metrics aligns with past social psychology studies that study the
relationship between trust and confidence. Leveraging these metrics, we
quantify the trustworthiness of several well-known deep neural network
architectures for image recognition to get a deeper understanding of where
trust breaks down. The proposed metrics are by no means perfect, but the hope
is to push the conversation towards better metrics to help guide practitioners
and regulators in producing, deploying, and certifying deep learning solutions
that can be trusted to operate in real-world, mission-critical scenarios.",multimedia
http://arxiv.org/abs/2010.08833v1,to_check,arxiv,arxiv,2020-10-17 17:48:04+00:00,arxiv,"Efficient and Compact Convolutional Neural Network Architectures for
  Non-temporal Real-time Fire Detection",http://arxiv.org/abs/2010.08833v1,"Automatic visual fire detection is used to complement traditional fire
detection sensor systems (smoke/heat). In this work, we investigate different
Convolutional Neural Network (CNN) architectures and their variants for the
non-temporal real-time bounds detection of fire pixel regions in video (or
still) imagery. Two reduced complexity compact CNN architectures
(NasNet-A-OnFire and ShuffleNetV2-OnFire) are proposed through experimental
analysis to optimise the computational efficiency for this task. The results
improve upon the current state-of-the-art solution for fire detection,
achieving an accuracy of 95% for full-frame binary classification and 97% for
superpixel localisation. We notably achieve a classification speed up by a
factor of 2.3x for binary classification and 1.3x for superpixel localisation,
with runtime of 40 fps and 18 fps respectively, outperforming prior work in the
field presenting an efficient, robust and real-time solution for fire region
detection. Subsequent implementation on low-powered devices (Nvidia Xavier-NX,
achieving 49 fps for full-frame classification via ShuffleNetV2-OnFire)
demonstrates our architectures are suitable for various real-world deployment
applications.",multimedia
http://arxiv.org/abs/2105.05092v1,to_check,arxiv,arxiv,2021-05-11 14:44:12+00:00,arxiv,"DeepLight: Robust & Unobtrusive Real-time Screen-Camera Communication
  for Real-World Displays",http://arxiv.org/abs/2105.05092v1,"The paper introduces a novel, holistic approach for robust Screen-Camera
Communication (SCC), where video content on a screen is visually encoded in a
human-imperceptible fashion and decoded by a camera capturing images of such
screen content. We first show that state-of-the-art SCC techniques have two key
limitations for in-the-wild deployment: (a) the decoding accuracy drops rapidly
under even modest screen extraction errors from the captured images, and (b)
they generate perceptible flickers on common refresh rate screens even with
minimal modulation of pixel intensity. To overcome these challenges, we
introduce DeepLight, a system that incorporates machine learning (ML) models in
the decoding pipeline to achieve humanly-imperceptible, moderately high SCC
rates under diverse real-world conditions. Deep-Light's key innovation is the
design of a Deep Neural Network (DNN) based decoder that collectively decodes
all the bits spatially encoded in a display frame, without attempting to
precisely isolate the pixels associated with each encoded bit. In addition,
DeepLight supports imperceptible encoding by selectively modulating the
intensity of only the Blue channel, and provides reasonably accurate screen
extraction (IoU values >= 83%) by using state-of-the-art object detection DNN
pipelines. We show that a fully functional DeepLight system is able to robustly
achieve high decoding accuracy (frame error rate < 0.2) and moderately-high
data goodput (>=0.95Kbps) using a human-held smartphone camera, even over
larger screen-camera distances (approx =2m).",multimedia
http://arxiv.org/abs/2103.14749v4,to_check,arxiv,arxiv,2021-03-26 21:54:36+00:00,arxiv,"Pervasive Label Errors in Test Sets Destabilize Machine Learning
  Benchmarks",http://arxiv.org/abs/2103.14749v4,"We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
https://labelerrors.com and all label errors can be reproduced by
https://github.com/cleanlab/label-errors.",multimedia
http://arxiv.org/abs/2005.07115v6,to_check,arxiv,arxiv,2020-05-14 16:33:13+00:00,arxiv,CoSimGNN: Towards Large-scale Graph Similarity Computation,http://arxiv.org/abs/2005.07115v6,"The ability to compute similarity scores between graphs based on metrics such
as Graph Edit Distance (GED) is important in many real-world applications, such
as 3D action recognition and biological molecular identification. Computing
exact GED values is typically an NP-hard problem and traditional algorithms
usually achieve an unsatisfactory trade-off between accuracy and efficiency.
Recently, Graph Neural Networks (GNNs) provide a data-driven solution for this
task, which is more efficient while maintaining prediction accuracy in small
graph (around 10 nodes per graph) similarity computation. Existing GNN-based
methods, which either respectively embed two graphs (lack of low-level
cross-graph interactions) or deploy cross-graph interactions for whole graph
pairs (redundant and time-consuming), are still not able to achieve competitive
results when the number of nodes in graphs increases. In this paper, we focus
on similarity computation for large-scale graphs and propose the
""embedding-coarsening-matching"" framework, which first embeds and coarsens
large graphs to coarsened graphs with denser local topology and then deploys
fine-grained interactions on the coarsened graphs for the final similarity
scores.",multimedia
http://arxiv.org/abs/2004.09548v1,to_check,arxiv,arxiv,2020-04-20 18:07:55+00:00,arxiv,AANet: Adaptive Aggregation Network for Efficient Stereo Matching,http://arxiv.org/abs/2004.09548v1,"Despite the remarkable progress made by learning based stereo matching
algorithms, one key challenge remains unsolved. Current state-of-the-art stereo
models are mostly based on costly 3D convolutions, the cubic computational
complexity and high memory consumption make it quite expensive to deploy in
real-world applications. In this paper, we aim at completely replacing the
commonly used 3D convolutions to achieve fast inference speed while maintaining
comparable accuracy. To this end, we first propose a sparse points based
intra-scale cost aggregation method to alleviate the well-known edge-fattening
issue at disparity discontinuities. Further, we approximate traditional
cross-scale cost aggregation algorithm with neural network layers to handle
large textureless regions. Both modules are simple, lightweight, and
complementary, leading to an effective and efficient architecture for cost
aggregation. With these two modules, we can not only significantly speed up
existing top-performing models (e.g., $41\times$ than GC-Net, $4\times$ than
PSMNet and $38\times$ than GA-Net), but also improve the performance of fast
stereo models (e.g., StereoNet). We also achieve competitive results on Scene
Flow and KITTI datasets while running at 62ms, demonstrating the versatility
and high efficiency of the proposed method. Our full framework is available at
https://github.com/haofeixu/aanet .",multimedia
http://arxiv.org/abs/1906.01308v1,to_check,arxiv,arxiv,2019-06-04 10:03:08+00:00,arxiv,"Towards better Validity: Dispersion based Clustering for Unsupervised
  Person Re-identification",http://arxiv.org/abs/1906.01308v1,"Person re-identification aims to establish the correct identity
correspondences of a person moving through a non-overlapping multi-camera
installation. Recent advances based on deep learning models for this task
mainly focus on supervised learning scenarios where accurate annotations are
assumed to be available for each setup. Annotating large scale datasets for
person re-identification is demanding and burdensome, which renders the
deployment of such supervised approaches to real-world applications infeasible.
Therefore, it is necessary to train models without explicit supervision in an
autonomous manner. In this paper, we propose an elegant and practical
clustering approach for unsupervised person re-identification based on the
cluster validity consideration. Concretely, we explore a fundamental concept in
statistics, namely \emph{dispersion}, to achieve a robust clustering criterion.
Dispersion reflects the compactness of a cluster when employed at the
intra-cluster level and reveals the separation when measured at the
inter-cluster level. With this insight, we design a novel Dispersion-based
Clustering (DBC) approach which can discover the underlying patterns in data.
This approach considers a wider context of sample-level pairwise relationships
to achieve a robust cluster affinity assessment which handles the complications
may arise due to prevalent imbalanced data distributions. Additionally, our
solution can automatically prioritize standalone data points and prevents
inferior clustering. Our extensive experimental analysis on image and video
re-identification benchmarks demonstrate that our method outperforms the
state-of-the-art unsupervised methods by a significant margin. Code is
available at https://github.com/gddingcs/Dispersion-based-Clustering.git.",multimedia
http://arxiv.org/abs/2110.06674v1,to_check,arxiv,arxiv,2021-10-13 12:18:09+00:00,arxiv,Truthful AI: Developing and governing AI that does not lie,http://arxiv.org/abs/2110.06674v1,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI ""lies"" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.",multimedia
10.1016/j.isprsjprs.2021.04.006,to_check,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2021-07-01,sciencedirect,Aerial scene understanding in the wild: Multi-scene recognition via prototype-based memory networks,https://api.elsevier.com/content/abstract/scopus_id/85106241924,"Aerial scene recognition is a fundamental visual task and has attracted an increasing research interest in the last few years. Most of current researches mainly deploy efforts to categorize an aerial image into one scene-level label, while in real-world scenarios, there often exist multiple scenes in a single image. Therefore, in this paper, we propose to take a step forward to a more practical and challenging task, namely multi-scene recognition in single images. Moreover, we note that manually yielding annotations for such a task is extraordinarily time- and labor-consuming. To address this, we propose a prototype-based memory network to recognize multiple scenes in a single image by leveraging massive well-annotated single-scene images. The proposed network consists of three key components: 1) a prototype learning module, 2) a prototype-inhabiting external memory, and 3) a multi-head attention-based memory retrieval module. To be more specific, we first learn the prototype representation of each aerial scene from single-scene aerial image datasets and store it in an external memory. Afterwards, a multi-head attention-based memory retrieval module is devised to retrieve scene prototypes relevant to query multi-scene images for final predictions. Notably, only a limited number of annotated multi-scene images are needed in the training phase. To facilitate the progress of aerial scene recognition, we produce a new multi-scene aerial image (MAI) dataset. Experimental results on variant dataset configurations demonstrate the effectiveness of our network. Our dataset and codes are publicly available
                        1
                     
                     
                        1
                        
                           https://github.com/Hua-YS/Prototype-based-Memory-Network.
                     .",multimedia
10.1016/j.csl.2020.101180,to_check,Computer Speech and Language,scopus,2021-07-01,sciencedirect,Identification of related languages from spoken data: Moving from off-line to on-line scenario,https://api.elsevier.com/content/abstract/scopus_id/85098984452,"The accelerating flow of information we encounter around the world today makes many companies deploy speech recognition systems that, to an ever-growing extent, process data on-line rather than off-line. These systems, e.g., for real-time 24/7 broadcast transcription, often work with input-stream data containing utterances in more than one language. This multilingual data can correctly be transcribed in real-time only if the language used is identified with just a small latency for each input frame. For this purpose, a novel approach to on-line spoken language identification is proposed in this work. Its development is documented within a series of consecutive experiments starting in the off-line mode for 11 Slavic languages, going through artificially prepared multilingual data for the on-line scenario, and ending with real bilingual TV programs containing utterances in mutually similar Czech and Slovak. The resulting scheme that we propose operates frame-by-frame; it takes in a multilingual stream of speech frames and outputs a stream of the corresponding language labels. It utilizes a weighted finite-state transducer as a decoder, which smooths the output from a language classifier fed by multilingual and augmented bottleneck features. An essential factor from the accuracy point of view is that these features, as well as the classifier itself, are based on deep neural network architectures that allow the modeling of long-term time dependencies. The obtained results show that our scheme allows us to determine the language spoken in real-world bilingual TV shows with an average latency of around 2.5 seconds and with an increase in word error rate by a mere 2.9% over the reference 18.1% value yielded by using manually prepared language labels.",multimedia
10.1109/ACCESS.2020.2968131,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Resonance - An Intelligence Analysis Framework for Social Connection Inference via Mining Co-Occurrence Patterns Over Multiplex Trajectories,https://ieeexplore.ieee.org/document/8963942/,"With the rapid development of the Internet of Things(IoT), in the last decades, law enforcement agencies have deployed extensive sensor networks for public safety purposes. Diverse kinds of trajectories from the sensor networks provide an unprecedented opportunity for intelligence analysis. The geographic co-movement pattern has rarely been used by the police force to infer social connections, although it has been prevalent in other fields. The previous studies have mainly focused on a singular form of trajectories with exact co-locations, and the spread of the co-locations is over-looked. In this paper, we propose a novel framework for detecting co-occurrence patterns over multiplex trajectories. Firstly, We constructed the foundation for the discovery of co-occurrence events, namely space-time resonance honeycomb. It consists of multiple polygonal zones over sensor networks. Secondly, we transform all trajectories into a series of space-time prisms, and co-location activities are recorded using a sliding window approach. Thirdly, we propose a novel feature: Geo-Spread, which captures the extent of the co-location spread. In the end, we combine multiple features and employ Random Forest to predict social connections. We conduct extensive experiments on both the public dataset and the real-world surveillance dataset. Experiment results on all datasets prove the effectiveness of the proposed framework by outperforming the state-of-the-art methods.",science
10.1145/3341161.3343690,to_check,2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),IEEE,2019-08-30 00:00:00,ieeexplore,Concept Drift in Bias and Sensationalism Detection: An Experimental Study,https://ieeexplore.ieee.org/document/9073558/,"Due to easy dissemination of news in social media and the Web, there has been an increasing rise of disinformation on important political issues like elections in recent years. Computational solutions for automatic bias and sensationalism detection for news articles can have tremendous impact if used in the right way. Because news is an ever-shifting domain, concept drift is an issue that must be dealt with in any real-world computational news classification system that relies on features and trained machine learning models. Yet, an empirical study of concept drift in such systems, especially popular systems released recently as open-source and used within organizations, has been lacking thus far. This short paper reports results on an empirical study specifically designed to assess concept drift, using an open-source, popular computational news classification system, on real news data crawled from the Web. We find that even a gap of two years (2017 vs. 2019) can lead to significant concept drift, a far narrower gap than observed in traditional machine learning domains, making deployment of pre-trained or openly available computational news classification models an ethically suspect issue.",science
10.1109/ICRA.2018.8460968,to_check,2018 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2018-05-25 00:00:00,ieeexplore,Socially Compliant Navigation Through Raw Depth Inputs with Generative Adversarial Imitation Learning,https://ieeexplore.ieee.org/document/8460968/,"We present an approach for mobile robots to learn to navigate in dynamic environments with pedestrians via raw depth inputs, in a socially compliant manner. To achieve this, we adopt a generative adversarial imitation learning (GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our approach overcomes the disadvantages of previous methods, as they heavily depend on the full knowledge of the location and velocity information of nearby pedestrians, which not only requires specific sensors, but also the extraction of such state information from raw sensory input could consume much computation time. In this paper, our proposed GAIL-based model performs directly on raw depth inputs and plans in real-time. Experiments show that our GAIL-based approach greatly improves the safety and efficiency of the behavior of mobile robots from pure behavior cloning. The real-world deployment also shows that our method is capable of guiding autonomous vehicles to navigate in a socially compliant manner directly through raw depth inputs. In addition, we release a simulation plugin for modeling pedestrian behaviors based on the social force model.",science
10.1109/ICPR48806.2021.9413200,to_check,2020 25th International Conference on Pattern Recognition (ICPR),IEEE,2021-01-15 00:00:00,ieeexplore,Temporal Collaborative Filtering with Graph Convolutional Neural Networks,https://ieeexplore.ieee.org/document/9413200/,"Temporal collaborative filtering (TCF) methods aim at modelling non-static aspects behind recommender systems, such as the dynamics in users' preferences and social trends around items. State-of-the-art TCF methods employ recurrent neural networks (RNNs) to model such aspects. These methods deploy matrix-factorization-based (MF -based) approaches to learn the user and item representations. Recently, graph-neural-network-based (GNN-based) approaches have shown improved performance in providing accurate recommendations over traditional MF -based approaches in non-temporal CF settings. Motivated by this, we propose a novel TCF method that leverages GNNs to learn user and item representations, and RNNs to model their temporal dynamics. A challenge with this method lies in the increased data sparsity, which negatively impacts obtaining meaningful quality representations with GNNs. To overcome this challenge, we train a GNN model at each time step using a set of observed interactions accumulated time-wise. Comprehensive experiments on real-world data show the improved performance obtained by our method over several state-of-the-art temporal and non-temporal CF models.",science
10.1109/ACCESS.2019.2935200,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Mapping Consumer Sentiment Toward Wireless Services Using Geospatial Twitter Data,https://ieeexplore.ieee.org/document/8796368/,"Hyper-dense wireless network deployment is one of the popular solutions to meeting high capacity requirement for 5G delivery. However, current operator understanding of consumer satisfaction comes from call centers and base station quality-of-service (QoS) reports with poor geographic accuracy. The dramatic increase in geo-tagged social media posts adds a new potential to understand consumer satisfaction towards target-specific quality-of-experience (QoE) topics. In our paper, we focus on evaluating users' opinion on wireless service-related topics by applying natural language processing (NLP) to geo-tagged Twitter data. Current generalized sentiment detection methods with generalized NLP corpora are not topic specific. Here, we develop a novel wireless service topic-specific sentiment framework, yielding higher targeting accuracy than generalized NLP frameworks. To do so, we first annotate a new sentiment corpus called SignalSentiWord (SSW) and compare its performance with two other popular corpus libraries, AFINN and SentiWordNet. We then apply three established machine learning methods, namely: Naïve Bayes (NB), Support Vector Machine (SVM), and Recurrent Neural Network (RNN) to build our topic-specific sentiment classifier. Furthermore, we discuss the capability of SSW to filter noisy and high-frequency irrelevant words to improve the performance of machine learning algorithms. Finally, the real-world testing results show that our proposed SSW improves the performance of NLP significantly.",science
10.1109/TCSS.2019.2950589,to_check,IEEE Transactions on Computational Social Systems,IEEE,2019-12-01 00:00:00,ieeexplore,Topo2Vec: A Novel Node Embedding Generation Based on Network Topology for Link Prediction,https://ieeexplore.ieee.org/document/8915749/,"Link prediction of a scale-free network has become relevant for problems relating to social network analysis, recommendation system, and in the domain of bioinformatics. In recently proposed approaches, the sampling of nodes of a network is done by simulating random walk. The generated node samples are used to train a neural network to learn the contextual information through an embedding vector. This method has gained popularity as the embedding vector is produced from the primitive node adjacency information and has achieved an outstanding performance. In this article, a naive and scalable approach for generating the node samples based on the principle of goal-oriented greedy searching has been proposed. The generated node samples have low noisy structures, which can represent the relation of edges of the network in a better way, compared to the state-of-the-art methods. Consequently, better representation of feature embedding of nodes of the network is generated from the samples. The learned feature vectors are used for solving the link prediction problem using a pairwise kernel support vector machine (SVM) classifier, which is computationally and spatially expensive in nature. Therefore, as an alternative, we have chosen to deploy a random forest (RF) classifier with a new algebraic operation to obtain the symmetric pairwise feature representation of a node pair. We demonstrated the efficacy of the proposed Topo2vec by testing it against the state-of-the-art network context generation algorithms in several real-world networks. Altogether, the proposed Topo2vec algorithm is a new method for solving the link prediction problem with entirely different settings.",science
http://arxiv.org/abs/2103.13452v1,to_check,arxiv,arxiv,2021-03-24 19:11:58+00:00,arxiv,"A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based
  Finger Control",http://arxiv.org/abs/2103.13452v1,"Objective: Deep learning-based neural decoders have emerged as the prominent
approach to enable dexterous and intuitive control of neuroprosthetic hands.
Yet few studies have materialized the use of deep learning in clinical settings
due to its high computational requirements. Methods: Recent advancements of
edge computing devices bring the potential to alleviate this problem. Here we
present the implementation of a neuroprosthetic hand with embedded deep
learning-based control. The neural decoder is designed based on the recurrent
neural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a
compacted yet powerful edge computing platform for deep learning inference.
This enables the implementation of the neuroprosthetic hand as a portable and
self-contained unit with real-time control of individual finger movements.
Results: The proposed system is evaluated on a transradial amputee using
peripheral nerve signals (ENG) with implanted intrafascicular microelectrodes.
The experiment results demonstrate the system's capabilities of providing
robust, high-accuracy (95-99%) and low-latency (50-120 msec) control of
individual finger movements in various laboratory and real-world environments.
Conclusion: Modern edge computing platforms enable the effective use of deep
learning-based neural decoders for neuroprosthesis control as an autonomous
system. Significance: This work helps pioneer the deployment of deep neural
networks in clinical applications underlying a new class of wearable biomedical
devices with embedded artificial intelligence.",science
http://arxiv.org/abs/2010.06425v1,to_check,arxiv,arxiv,2020-10-13 14:38:40+00:00,arxiv,"Temporal Collaborative Filtering with Graph Convolutional Neural
  Networks",http://arxiv.org/abs/2010.06425v1,"Temporal collaborative filtering (TCF) methods aim at modelling non-static
aspects behind recommender systems, such as the dynamics in users' preferences
and social trends around items. State-of-the-art TCF methods employ recurrent
neural networks (RNNs) to model such aspects. These methods deploy
matrix-factorization-based (MF-based) approaches to learn the user and item
representations. Recently, graph-neural-network-based (GNN-based) approaches
have shown improved performance in providing accurate recommendations over
traditional MF-based approaches in non-temporal CF settings. Motivated by this,
we propose a novel TCF method that leverages GNNs to learn user and item
representations, and RNNs to model their temporal dynamics. A challenge with
this method lies in the increased data sparsity, which negatively impacts
obtaining meaningful quality representations with GNNs. To overcome this
challenge, we train a GNN model at each time step using a set of observed
interactions accumulated time-wise. Comprehensive experiments on real-world
data show the improved performance obtained by our method over several
state-of-the-art temporal and non-temporal CF models.",science
10.1016/j.neucom.2018.08.009,to_check,Neurocomputing,scopus,2018-11-17,sciencedirect,Evaluation of deep neural networks for traffic sign detection systems,https://api.elsevier.com/content/abstract/scopus_id/85052108235,"Traffic sign detection systems constitute a key component in trending real-world applications, such as autonomous driving, and driver safety and assistance. This paper analyses the state-of-the-art of several object-detection systems (Faster R-CNN, R-FCN, SSD, and YOLO V2) combined with various feature extractors (Resnet V1 50, Resnet V1 101, Inception V2, Inception Resnet V2, Mobilenet V1, and Darknet-19) previously developed by their corresponding authors. We aim to explore the properties of these object-detection models which are modified and specifically adapted to the traffic sign detection problem domain by means of transfer learning. In particular, various publicly available object-detection models that were pre-trained on the Microsoft COCO dataset are fine-tuned on the German Traffic Sign Detection Benchmark dataset. The evaluation and comparison of these models include key metrics, such as the mean average precision (mAP), memory allocation, running time, number of floating point operations, number of parameters of the model, and the effect of traffic sign image sizes. Our findings show that Faster R-CNN Inception Resnet V2 obtains the best mAP, while R-FCN Resnet 101 strikes the best trade-off between accuracy and execution time. YOLO V2 and SSD Mobilenet merit a special mention, in that the former achieves competitive accuracy results and is the second fastest detector, while the latter, is the fastest and the lightest model in terms of memory consumption, making it an optimal choice for deployment in mobile and embedded devices.",science
10.1016/j.inffus.2017.05.003,to_check,Information Fusion,scopus,2018-03-01,sciencedirect,A Social-aware online short-text feature selection technique for social media,https://api.elsevier.com/content/abstract/scopus_id/85019594702,"Large-scale text categorisation in social environments, characterised by the high dimensionality of feature spaces, is one of the most relevant problems in machine learning and data mining nowadays. Short-texts, which are posted at unprecedented rates, accentuate both the importance of learning tasks and the challenges posed by such large feature space. A collection of social media short-texts does not only provide textual information but also topological information given by the relationships between posts and their authors. The linked nature of social data causes new complementary data dimensions to be added to the feature space, which, at the same time, becomes sparser. Additionally, in the context of social media, posts usually arrive simultaneously in streams, which hinders the deployment of efficient traditional feature selection techniques that assume a feature space fully known in advance. Hence, efficient and scalable online feature selection becomes an important requirement in numerous large-scale social applications. This work presents an online feature selection technique for high-dimensional data based on the integration of two information sources, social and content-based, for the real-time classification of short-text streams coming from social media. It focuses on discovering implicit relations amongst new posts, already known ones and their corresponding authors to identify groups of socially related posts. Then, each discovered group is represented by a set of non-redundant and relevant textual features. Finally, such features are used to train different learning models for classifying newly arriving posts. Extensive experiments conducted on real-world short-texts demonstrate that the proposed approach helps to improve classification results when compared to state-of-the-art and traditional online feature selection techniques.",science
10.1109/LRA.2019.2894216,to_check,IEEE Robotics and Automation Letters,IEEE,2019-04-01 00:00:00,ieeexplore,VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control,https://ieeexplore.ieee.org/document/8620258/,"In this letter, we deal with the reality gap from a novel perspective, targeting transferring deep reinforcement learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as first, no extra transfer steps are required during the expensive training of DRL agents in simulation; second, the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; and third, the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.",robotics
10.1109/ICRA.2011.5980435,to_check,2011 IEEE International Conference on Robotics and Automation,IEEE,2011-05-13 00:00:00,ieeexplore,Autonomous learning of vision-based layered object models on mobile robots,https://ieeexplore.ieee.org/document/5980435/,"Although mobile robots are increasingly being used in real-world applications, the ability to robustly sense and interact with the environment is still missing. A key requirement for the widespread deployment of mobile robots is the ability to operate autonomously by learning desired environmental models and revising the learned models in response to environmental changes. This paper presents an approach that enables a mobile robot to autonomously learn layered models for environmental objects using temporal, local and global visual cues. A temporal assessment of image gradient features is used to detect candidate objects, which are then modeled using color distribution statistics and a spatial representation of gradient features. The robot incrementally revises the learned models and uses them for object recognition and tracking based on a matching scheme comprising a spatial similarity measure and second order distribution statistics. All algorithms are implemented and tested on a wheeled robot platform in dynamic indoor environments.",robotics
10.1109/ICRA.2011.5980333,to_check,2011 IEEE International Conference on Robotics and Automation,IEEE,2011-05-13 00:00:00,ieeexplore,To look or not to look: A hierarchical representation for visual planning on mobile robots,https://ieeexplore.ieee.org/document/5980333/,"Mobile robots are increasingly being used in real-world applications due to the ready availability of high fidelity sensors and the development of sophisticated information processing algorithms. However, one key challenge to the widespread deployment of mobile robots equipped with multiple sensors and processing algorithms is the ability to autonomously tailor sensing and information processing to the task at hand. This paper poses this challenge as the task of planning under uncertainty, and more specifically as an instance of probabilistic sequential decision-making. A novel hierarchy of partially observable Markov decision processes (POMDPs) is incorporated, which uses constrained-convolutional policies and automatic belief propagation to achieve efficient and reliable operation on mobile robots. All algorithms are implemented and evaluated on simulated and physical robot platforms for the task of searching for target objects in dynamic indoor environments.",robotics
10.1109/FUZZ48607.2020.9177654,to_check,2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),IEEE,2020-07-24 00:00:00,ieeexplore,AI-FML Agent for Robotic Game of Go and AIoT Real-World Co-Learning Applications,https://ieeexplore.ieee.org/document/9177654/,"In this paper, we propose an AI-FML agent for robotic game of Go and AIoT real-world co-learning applications. The fuzzy machine learning mechanisms are adopted in the proposed model, including fuzzy markup language (FML)-based genetic learning (GFML), eXtreme Gradient Boost (XGBoost), and a seven-layered deep fuzzy neural network (DFNN) with backpropagation learning, to predict the win rate of the game of Go as Black or White. This paper uses Google AlphaGo Master sixty games as the dataset to evaluate the performance of the fuzzy machine learning, and the desired output dataset were predicted by Facebook AI Research (FAIR) ELF Open Go AI bot. In addition, we use IEEE 1855 standard for FML to describe the knowledge base and rule base of the Open Go Darkforest (OGD) prediction platform in order to infer the win rate of the game. Next, the proposed AI-FML agent publishes the inferred result to communicate with the robot Kebbi Air based on MQTT protocol to achieve the goal of human and smart machine co-learning. From Sept. 2019 to Jan. 2020, we introduced the AI-FML agent into the teaching and learning fields in Taiwan. The experimental results show the robots and students can co-learn AI tools and FML applications effectively. In addition, XGBoost outperforms the other machine learning methods but DFNN has the most obvious progress after learning. In the future, we hope to deploy the AI-FML agent to more available robot and human co-learning platforms through the established AI-FML International Academy in the world.",robotics
10.1109/RO-MAN47096.2020.9223341,to_check,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),IEEE,2020-09-04 00:00:00,ieeexplore,Context Dependent Trajectory Generation using Sequence-to-Sequence Models for Robotic Toilet Cleaning,https://ieeexplore.ieee.org/document/9223341/,"A robust, easy-to-deploy robot for service tasks in a real environment is difficult to construct. Record-and-playback (R&amp;P) is a method used to teach motor-skills to robots for performing service tasks. However, R&amp;P methods do not scale to challenging tasks where even slight changes in the environment, such as localization errors, would either require trajectory modification or a new demonstration. In this paper, we propose a Sequence-to-Sequence (Seq2Seq) based neural network model to generate robot trajectories in configuration space given a context variable based on real-world measurements in Cartesian space. We use the offset between a target pose and the actual pose after localization as the context variable. The model is trained using a few expert demonstrations collected using teleoperation. We apply our proposed method to the task of toilet cleaning where the robot has to clean the surface of a toilet bowl using a compliant end-effector in a constrained toilet setting. In the experiments, the model is given a novel offset context and it generates a modified robot trajectory for that context. We demonstrate that our proposed model is able to generate trajectories for unseen setups and the executed trajectory results in cleaning of the toilet bowl.",robotics
10.1109/ISCSIC.2017.28,to_check,2017 International Symposium on Computer Science and Intelligent Controls (ISCSIC),IEEE,2017-10-22 00:00:00,ieeexplore,An Adaptive 2D Tracking Approach for Person Following Robot,https://ieeexplore.ieee.org/document/8294176/,"In this paper, we present a 2D appearance vision based tracking approach for human following robot. Generally, existing methods have high cost of computing and requirements of hardware which makes the difficulty of employing the function on service robot. Hence, minimizing the cost of tracking with slight loss of precision can benefit this area. We focus on approach based on 2D image data which reduce tracking into two dimensions and can minimize the cost of computation. Our approach presents a corporate strategy which utilizes central consensus of correspondence for 2D feature points pairwise to track and employ a semi-supervised learning detector to update appearance change. To overcome difficulties from environment change, we set an enhancing process for feature points and background segmentation through depth information. We carefully evaluate our approach with common challenges of visual tracking in static view and deploy a dynamic view a real-world following task. The experiment results illustrate that our tracking approach works against common risks at 2D appearance tracking and properly follows the user obtaining 25 fps performance on mobile platform.",robotics
10.1109/ICRA40945.2020.9197159,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference,https://ieeexplore.ieee.org/document/9197159/,"Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.",robotics
10.1109/TVT.2019.2952926,to_check,IEEE Transactions on Vehicular Technology,IEEE,2020-01-01 00:00:00,ieeexplore,Mechanism Design for Wireless Powered Spatial Crowdsourcing Networks,https://ieeexplore.ieee.org/document/8895988/,"Wireless power transfer (WPT) is a promising technology to prolong the lifetime of the sensors and communication devices, i.e., workers, in completing crowdsourcing tasks by providing continuous and cost-effective energy supplies. In this paper, we propose a wireless powered spatial crowdsourcing framework which consists of two mutually dependent phases: task allocation phase and data crowdsourcing phase. In the task allocation phase, we propose a Stackelberg game based mechanism for the spatial crowdsourcing platform to efficiently allocate spatial tasks and wireless charging power to each worker. In the data crowdsourcing phase, the workers may have an incentive to misreport its real working location to improve its utility, which causes adverse effects to the spatial crowdsourcing platform. To address this issue, we present three strategyproof deployment mechanisms for the spatial crowdsourcing platform to place a mobile base station, e.g., vehicle or robot, which is responsible for transferring the wireless power and collecting the crowdsourced data. As the benchmark, we first apply the classical median mechanism and evaluate its worst-case performance. Then, we design a conventional strategyproof deployment mechanism to improve the expected utility of the spatial crowdsourcing platform under the condition that the workers' locations follow a known geographical distribution. For a more general case with only the historical location data available, we propose a deep learning based strategyproof deployment mechanism to maximize the spatial crowdsourcing platform's utility. Extensive experimental results based on synthetic and real-world datasets reveal the effectiveness of the proposed framework in allocating tasks and charging power to workers while avoiding the dishonest worker's manipulation.",robotics
10.1007/s42979-021-00817-z,to_check,SN Computer Science,Nature,2021-08-18 00:00:00,springer,Deep Reinforcement Learning of Map-Based Obstacle Avoidance for Mobile Robot Navigation,https://www.nature.com/articles/s42979-021-00817-z,"Autonomous and safe navigation in complex environments without collisions is particularly important for mobile robots. In this paper, we propose an end-to-end deep reinforcement learning method for mobile robot navigation with map-based obstacle avoidance. Using the experience collected in the simulation environment, a convolutional neural network is trained to predict the proper steering operation of the robot based on its egocentric local grid maps, which can accommodate various sensors and fusion algorithms. We use dueling double DQN with prioritized experienced replay technology to update parameters of the network and integrate curriculum learning techniques to enhance its performance. The trained deep neural network is then transferred and executed on a real-world mobile robot to guide it to avoid local obstacles for long-range navigation. The qualitative and quantitative evaluations of the new approach were performed in simulations and real robot experiments. The results show that the end-to-end map-based obstacle avoidance model is easy to deploy, without any fine-tuning, robust to sensor noise, compatible with different sensors, and better than other related DRL-based models in many evaluation indicators.",robotics
10.1007/978-3-319-70407-4_39,to_check,The Semantic Web: ESWC 2017 Satellite Events,Springer,2017-01-01 00:00:00,springer,Making Sense of Indoor Spaces Using Semantic Web Mining and Situated Robot Perception,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-70407-4_39,"Intelligent Autonomous Robots deployed in human environments must have understanding of the wide range of possible semantic identities associated with the spaces they inhabit – kitchens, living rooms, bathrooms, offices, garages, etc. We believe robots should learn this information through their own exploration and situated perception in order to uncover and exploit structure in their environments – structure that may not be apparent to human engineers, or that may emerge over time during a deployment. In this work, we combine semantic web-mining and situated robot perception to develop a system capable of assigning semantic categories to regions of space. This is accomplished by looking at web-mined relationships between room categories and objects identified by a Convolutional Neural Network trained on 1000 categories. Evaluated on real-world data, we show that our system exhibits several conceptual and technical advantages over similar systems, and uncovers semantic structure in the environment overlooked by ground-truth annotators.",robotics
10.1016/j.robot.2021.103891,to_check,Robotics and Autonomous Systems,scopus,2021-12-01,sciencedirect,Hybrid autonomous controller for bipedal robot balance with deep reinforcement learning and pattern generators[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85116222985,"Recovering after an abrupt push is essential for bipedal robots in real-world applications within environments where humans must collaborate closely with robots. There are several balancing algorithms for bipedal robots in the literature, however most of them either rely on hard coding or power-hungry algorithms. We propose a hybrid autonomous controller that hierarchically combines two separate, efficient systems, to address this problem. The lower-level system is a reliable, high-speed, full state controller that was hardcoded on a microcontroller to be power efficient. The higher-level system is a low-speed reinforcement learning controller implemented on a low-power onboard computer. While one controller offers speed, the other provides trainability and adaptability. An efficient control is then formed without sacrificing adaptability to new dynamic environments. Additionally, as the higher-level system is trained via deep reinforcement learning, the robot could learn after deployment, which is ideal for real-world applications. The system’s performance is validated with a real robot recovering after a random push in less than 5 s, with minimal steps from its initial positions. The training was conducted using simulated data.",robotics
10.1016/j.actaastro.2017.07.038,to_check,Acta Astronautica,scopus,2017-11-01,sciencedirect,Self-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning,https://api.elsevier.com/content/abstract/scopus_id/85026883328,"Although machine learning holds an enormous promise for autonomous space robots, it is currently not employed because of the inherent uncertain outcome of learning processes. In this article we investigate a learning mechanism, Self-Supervised Learning (SSL), which is very reliable and hence an important candidate for real-world deployment even on safety-critical systems such as space robots. To demonstrate this reliability, we introduce a novel SSL setup that allows a stereo vision equipped robot to cope with the failure of one of its cameras. The setup learns to estimate average depth using a monocular image, by using the stereo vision depths from the past as trusted ground truth. We present preliminary results from an experiment on the International Space Station (ISS) performed with the MIT/NASA SPHERES VERTIGO satellite. The presented experiments were performed on October 8th, 2015 on board the ISS. The main goals were (1) data gathering, and (2) navigation based on stereo vision. First the astronaut Kimiya Yui moved the satellite around the Japanese Experiment Module to gather stereo vision data for learning. Subsequently, the satellite freely explored the space in the module based on its (trusted) stereo vision system and a pre-programmed exploration behavior, while simultaneously performing the self-supervised learning of monocular depth estimation on board. The two main goals were successfully achieved, representing the first online learning robotic experiments in space. These results lay the groundwork for a follow-up experiment in which the satellite will use the learned single-camera depth estimation for autonomous exploration in the ISS, and are an advancement towards future space robots that continuously improve their navigation capabilities over time, even in harsh and completely unknown space environments.",robotics
10.1109/IDT52577.2021.9497540,to_check,2021 International Conference on Information and Digital Technologies (IDT),IEEE,2021-06-24 00:00:00,ieeexplore,Phishing Detection for Secure Operations of UAVs,https://ieeexplore.ieee.org/document/9497540/,"Unmanned Aerial Vehicles (UAV) or drones are used in various domains more and more, including military operations, monitoring, rescue of victims, and transport. Often, UAV resources are developed as web services so that they can be accessed anywhere on the Internet through the World Wide Web. However, this makes them vulnerable to phishing activities of criminals who may try to access these resources and other sensitive information. Therefore, the development of a phishing detection tool based on data mining is presented in this paper. It consists of a browser extension monitoring visited webpages and a backend communicating with the browser extension for the purposes of executing some specific tasks. The browser extension is implemented in JavaScript and the ReactJS framework, and it contains an implementation of classifications with a Bayesian network, decision tree, nearest neighbor classifier and neural network. The backend uses PHP, Python scripts and the Apache HTTP Server. In addition, a browser extension is implemented so that data about webpages can be collected and this data is used for the creation of data mining models. Experimental validation with 10-fold cross-validation and through the browsing of real-world websites show promising results in phishing detection.",autonomous vehicle
10.1016/j.jmsy.2011.09.002,to_check,Journal of Manufacturing Systems,scopus,2012-04-01,sciencedirect,Intelligent evaluation of supplier bids using a hybrid technique in distributed supply chains,https://api.elsevier.com/content/abstract/scopus_id/84858340427,"The main idea of this research is to devise the smart module to pick the best supplier bid(s) automatically. The hybrid model is composed of three useful tools: fuzzy logic, AHP, and QFD. The approach has been carefully implemented and verified via a real-world case study in a medium-to-large industry manufacturing vehicle tires and other rubber products. A collection of 12 assessment criteria classified into two categories have been considered. Eight factors are derived from customer suggestions and the other four are design specifications required to manufacture the product. The main outcomes are: a hybrid autonomous model to evaluate supplier bids without direct human intervention; devising a hybrid three-module method and overcoming complexity of computations in resulting algorithm by means of agents; outlining the best criteria to assess suppliers; evaluating the suppliers based on voice of customer during all stages of the process; and discussing analysis, design, and implementation issues of the evaluation agent. The paper includes implications for development of an integrated total system for supply chain coordination. The most important advantages of this work over earlier researches on supplier selection are: implementation of an autonomous assessment mechanism using intelligent agents for the first time, making the best out of three widely applied methodologies all at once, evaluation process mainly based on features of customer order, coordination of supply job based on a bidding system, and portal-mediated operation and control.",autonomous vehicle
10.1109/ACCESS.2020.3007928,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,An Improved Marine Predators Algorithm With Fuzzy Entropy for Multi-Level Thresholding: Real World Example of COVID-19 CT Image Segmentation,https://ieeexplore.ieee.org/document/9136648/,"Medical imaging techniques play a critical role in diagnosing diseases and patient healthcare. They help in treatment, diagnosis, and early detection. Image segmentation is one of the most important steps in processing medical images, and it has been widely used in many applications. Multi-level thresholding (MLT) is considered as one of the simplest and most effective image segmentation techniques. Traditional approaches apply histogram methods; however, these methods face some challenges. In recent years, swarm intelligence methods have been leveraged in MLT, which is considered an NP-hard problem. One of the main drawbacks of the SI methods is when searching for optimum solutions, and some may get stuck in local optima. This because during the run of SI methods, they create random sequences among different operators. In this study, we propose a hybrid SI based approach that combines the features of two SI methods, marine predators algorithm (MPA) and moth-?ame optimization (MFO). The proposed approach is called MPAMFO, in which, the MFO is utilized as a local search method for MPA to avoid trapping at local optima. The MPAMFO is proposed as an MLT approach for image segmentation, which showed excellent performance in all experiments. To test the performance of MPAMFO, two experiments were carried out. The first one is to segment ten natural gray-scale images. The second experiment tested the MPAMFO for a real-world application, such as CT images of COVID-19. Therefore, thirteen CT images were used to test the performance of MPAMFO. Furthermore, extensive comparisons with several SI methods have been implemented to examine the quality and the performance of the MPAMFO. Overall experimental results confirm that the MPAMFO is an efficient MLT approach that approved its superiority over other existing methods.",health
10.1109/ICPHYS.2018.8390779,to_check,2018 IEEE Industrial Cyber-Physical Systems (ICPS),IEEE,2018-05-18 00:00:00,ieeexplore,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,https://ieeexplore.ieee.org/document/8390779/,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",industry
10.1109/IEEM45057.2020.9309776,to_check,2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),IEEE,2020-12-17 00:00:00,ieeexplore,Job Shop Scheduling Problem Neural Network Solver with Dispatching Rules,https://ieeexplore.ieee.org/document/9309776/,"Job Shop Scheduling Problem (JSSP) is an optimization problem in computer science and operations research. Many problems in real-world manufacturing processes can be translated into JSSP. In recent years, Machine Learning has shown great promises in solving optimization problems and can be used to solve JSSP instances. In this paper, an Artificial Neural Network (ANN) was designed and trained to solve JSSP instances using the priority of the operations as the learning output. Dispatching rules were implemented to break ties during the decoding of the priorities. Our experiment results showed that a hybrid algorithm that combines the best of ANN with dispatching rules and standalone dispatching rule-based heuristic outperforms previously reported results.",industry
10.1109/MSM49833.2020.9202398,to_check,2020 International Conference Mechatronic Systems and Materials (MSM),IEEE,2020-07-03 00:00:00,ieeexplore,Collaborative Robot System for Playing Chess,https://ieeexplore.ieee.org/document/9202398/,"In recent years, number of collaborative robots industrial applications has made a significant increasment. Implementation of collaborative robots is a safe and effective way for designing robot-human cooperation systems. Combined with constantly developing artificial intelligence, collaborative systems are actually able to solve complex problems that require some sort of intelligence. For humans, board games are a good example of the visualization of robot intelligence. Such systems require estimation and detection of board and pieces in manipulator workspace, some kind of decision-making algorithms and robot control system to move pieces. The flagship of such systems are chess playing robots. The chess game has a defined and easy to understand set of rules which makes it interesting example of intelligent robotics systems application. In this paper, we present an implementation of collaborative robots for chess playing system which was designed to play against human or another robot. The system is able to track state of the game via camera, calculate the optimal move using implemented decision-making algorithm, detect illegal moves and execute pick-and-place task to physically move pieces. We test the developed system in a real-world setup and provide experimental results documenting the performance of proposed approach.",industry
http://arxiv.org/abs/2007.01097v1,to_check,arxiv,arxiv,2020-07-01 08:47:46+00:00,arxiv,"PrototypeML: A Neural Network Integrated Design and Development
  Environment",http://arxiv.org/abs/2007.01097v1,"Neural network architectures are most often conceptually designed and
described in visual terms, but are implemented by writing error-prone code.
PrototypeML is a machine learning development environment that bridges the
dichotomy between the design and development processes: it provides a highly
intuitive visual neural network design interface that supports (yet abstracts)
the full capabilities of the PyTorch deep learning framework, reduces model
design and development time, makes debugging easier, and automates many
framework and code writing idiosyncrasies. In this paper, we detail the deep
learning development deficiencies that drove the implementation of PrototypeML,
and propose a hybrid approach to resolve these issues without limiting network
expressiveness or reducing code quality. We demonstrate the real-world benefits
of a visual approach to neural network design for research, industry and
teaching. Available at https://PrototypeML.com",industry
10.1109/TIE.2019.2905808,to_check,IEEE Transactions on Industrial Electronics,IEEE,2020-03-01 00:00:00,ieeexplore,"Design, Implementation, and Evaluation of a Neural-Network-Based Quadcopter UAV System",https://ieeexplore.ieee.org/document/8676108/,"In this paper, a quadcopter unmanned aerial vehicle (UAV) system based on neural-network enhanced dynamic inversion control is proposed for multiple real-world application scenarios. A sigma-pi neural network (SPNN) is used as the compensator to reduce the model error and improve the system performance in the presence of the uncertainties of UAV dynamics, payload, and environment. Besides, we present a technical framework for fast and robust implementation of multipurpose UAV systems and develop a testbed for the evaluation of UAV control system by using a high-precision optical motion capture system. Both simulation results and experiment results demonstrate that the SPNN can reduce the inversion errors related to UAV parameter uncertainties as well as tracking errors related to unknown disturbances and unmodeled dynamics. With the help of an online neural network (NN) learning mechanism, the entire system can achieve much higher accuracy in attitude and trajectory control than that achieved by conventional proportional-integral derivative based control systems under varying flight conditions.",autonomous vehicle
http://arxiv.org/abs/1710.06270v2,to_check,arxiv,arxiv,2017-10-17 13:38:16+00:00,arxiv,"Procedural Modeling and Physically Based Rendering for Synthetic Data
  Generation in Automotive Applications",http://arxiv.org/abs/1710.06270v2,"We present an overview and evaluation of a new, systematic approach for
generation of highly realistic, annotated synthetic data for training of deep
neural networks in computer vision tasks. The main contribution is a procedural
world modeling approach enabling high variability coupled with physically
accurate image synthesis, and is a departure from the hand-modeled virtual
worlds and approximate image synthesis methods used in real-time applications.
The benefits of our approach include flexible, physically accurate and scalable
image synthesis, implicit wide coverage of classes and features, and complete
data introspection for annotations, which all contribute to quality and cost
efficiency. To evaluate our approach and the efficacy of the resulting data, we
use semantic segmentation for autonomous vehicles and robotic navigation as the
main application, and we train multiple deep learning architectures using
synthetic data with and without fine tuning on organic (i.e. real-world) data.
The evaluation shows that our approach improves the neural network's
performance and that even modest implementation efforts produce
state-of-the-art results.",autonomous vehicle
http://arxiv.org/abs/1911.03565v1,to_check,arxiv,arxiv,2019-11-08 22:28:53+00:00,arxiv,"Vision-Based Lane-Changing Behavior Detection Using Deep Residual Neural
  Network",http://arxiv.org/abs/1911.03565v1,"Accurate lane localization and lane change detection are crucial in advanced
driver assistance systems and autonomous driving systems for safer and more
efficient trajectory planning. Conventional localization devices such as Global
Positioning System only provide road-level resolution for car navigation, which
is incompetent to assist in lane-level decision making. The state of art
technique for lane localization is to use Light Detection and Ranging sensors
to correct the global localization error and achieve centimeter-level accuracy,
but the real-time implementation and popularization for LiDAR is still limited
by its computational burden and current cost. As a cost-effective alternative,
vision-based lane change detection has been highly regarded for affordable
autonomous vehicles to support lane-level localization. A deep learning-based
computer vision system is developed to detect the lane change behavior using
the images captured by a front-view camera mounted on the vehicle and data from
the inertial measurement unit for highway driving. Testing results on
real-world driving data have shown that the proposed method is robust with
real-time working ability and could achieve around 87% lane change detection
accuracy. Compared to the average human reaction to visual stimuli, the
proposed computer vision system works 9 times faster, which makes it capable of
helping make life-saving decisions in time.",autonomous vehicle
10.1109/JBHI.2020.3037027,to_check,IEEE Journal of Biomedical and Health Informatics,IEEE,2021-06-01 00:00:00,ieeexplore,Detecting Medical Misinformation on Social Media Using Multimodal Deep Learning,https://ieeexplore.ieee.org/document/9253994/,"In 2019, outbreaks of vaccine-preventable diseases reached the highest number in the US since 1992. Medical misinformation, such as antivaccine content propagating through social media, is associated with increases in vaccine delay and refusal. Our overall goal is to develop an automatic detector for antivaccine messages to counteract the negative impact that antivaccine messages have on the public health. Very few extant detection systems have considered multimodality of social media posts (images, texts, and hashtags), and instead focus on textual components, despite the rapid growth of photo-sharing applications (e.g., Instagram). As a result, existing systems are not sufficient for detecting antivaccine messages with heavy visual components (e.g., images) posted on these newer platforms. To solve this problem, we propose a deep learning network that leverages both visual and textual information. A new semantic- and task-level attention mechanism was created to help our model to focus on the essential contents of a post that signal antivaccine messages. The proposed model, which consists of three branches, can generate comprehensive fused features for predictions. Moreover, an ensemble method is proposed to further improve the final prediction accuracy. To evaluate the proposed model's performance, a real-world social media dataset that consists of more than 30,000 samples was collected from Instagram between January 2016 and October 2019. Our 30 experiment results demonstrate that the final network achieves above 97% testing accuracy and outperforms other relevant models, demonstrating that it can detect a large amount of antivaccine messages posted daily. The implementation code is available at https://github.com/wzhings/antivaccine_detection.",health
10.1109/ASONAM.2018.8508264,to_check,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),IEEE,2018-08-31 00:00:00,ieeexplore,Interactive Kernel Dimension Alternative Clustering on GPUs,https://ieeexplore.ieee.org/document/8508264/,"Machine learning has seen tremendous growth in recent years thanks to two key advances in technology: massive data generation and highly-parallel accelerator architectures. The rate that data is being generated is exploding across multiple domains, including medical research, environmental science, web-search, and e-commerce. Many of these advances have benefited from emergent web-based applications, and improvements in data storage and sensing technologies. Innovations in parallel accelerator hardware, such as GPUs, has made it possible to process massive amounts of data in a timely fashion. Given these advanced data acquisition technology and hardware, machine learning researchers are equipped to generate and sift through much larger and complex datasets quickly. In this work, we focus on accelerating Kernel Dimension Alternative Clustering algorithms using GPUs. We conduct a thorough performance analysis by using both synthetic and real-world datasets, while also modifying both the structure of the data, and the size of the datasets. Our GPU implementation reduces execution time from minutes to seconds, which enables us to develop a web-based application for users to, interactively, view alternative clustering solutions.",health
10.1109/TCIAIG.2013.2285651,to_check,IEEE Transactions on Computational Intelligence and AI in Games,IEEE,2014-03-01 00:00:00,ieeexplore,DeepQA Jeopardy! Gamification: A Machine-Learning Perspective,https://ieeexplore.ieee.org/document/6632881/,"DeepQA is a large-scale natural language processing (NLP) question-and-answer system that responds across a breadth of structured and unstructured data, from hundreds of analytics that are combined with over 50 models, trained through machine learning. After the 2011 historic milestone of defeating the two best human players in the Jeopardy! game show, the technology behind IBM Watson, DeepQA, is undergoing gamification into real-world business problems. Gamifying a business domain for Watson is a composite of functional, content, and training adaptation for nongame play. During domain gamification for medical, financial, government, or any other business, each system change affects the machine-learning process. As opposed to the original Watson Jeopardy!, whose class distribution of positive-to-negative labels is 1:100, in adaptation the computed training instances, question-and-answer pairs transformed into true-false labels, result in a very low positive-to-negative ratio of 1:100 000. Such initial extreme class imbalance during domain gamification poses a big challenge for the Watson machine-learning pipelines. The combination of ingested corpus sets, question-and-answer pairs, configuration settings, and NLP algorithms contribute toward the challenging data state. We propose several data engineering techniques, such as answer key vetting and expansion, source ingestion, oversampling classes, and question set modifications to increase the computed true labels. In addition, algorithm engineering, such as an implementation of the Newton-Raphson logistic regression with a regularization term, relaxes the constraints of class imbalance during training adaptation. We conclude by empirically demonstrating that data and algorithm engineering are complementary and indispensable to overcome the challenges in this first Watson gamification for real-world business problems.",health
10.1109/ICAC.2017.21,to_check,2017 IEEE International Conference on Autonomic Computing (ICAC),IEEE,2017-07-21 00:00:00,ieeexplore,Ananke: A Q-Learning-Based Portfolio Scheduler for Complex Industrial Workflows,https://ieeexplore.ieee.org/document/8005354/,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",health
10.1007/s00345-021-03879-z,to_check,World Journal of Urology,Springer,2021-11-13 00:00:00,springer,Personalized application of machine learning algorithms to identify pediatric patients at risk for recurrent ureteropelvic junction obstruction after dismembered pyeloplasty,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00345-021-03879-z,"Purpose To develop a model that predicts whether a child will develop a recurrent obstruction after pyeloplasty, determine their survival risk score, and expected time to re-intervention using machine learning (ML). Methods We reviewed patients undergoing pyeloplasty from 2008 to 2020 at our institution, including all children and adolescents younger than 18 years. We developed a two-stage machine learning model from 34 clinical fields, which included patient characteristics, ultrasound findings, and anatomical variation. We fit and trained with a logistic lasso model for binary cure model and subsequent survival model. Feature importance on the model was determined with post-selection inference. Performance metrics included area under the receiver-operating-characteristic (AUROC), concordance, and leave-one-out cross validation. Results A total of 543 patients were identified, with a median preoperative and postoperative anteroposterior diameter of 23 and 10 mm, respectively. 39 of 232 patients included in the survival model required re-intervention. The cure and survival models performed well with a leave-one-out cross validation AUROC and concordance of 0.86 and 0.78, respectively. Post-selective inference showed that larger anteroposterior diameter at the second post-op follow-up, and anatomical variation in the form of concurrent anomalies were significant model features predicting negative outcomes. The model can be used at https://sickkidsurology.shinyapps.io/PyeloplastyReOpRisk/ . Conclusion Our ML-based model performed well in predicting the risk of and time to re-intervention after pyeloplasty. The implementation of this ML-based approach is novel in pediatric urology and will likely help achieve personalized risk stratification for patients undergoing pyeloplasty. Further real-world validation is warranted.",health
10.1186/s12911-021-01634-3,to_check,BMC Medical Informatics and Decision Making,BioMed Central,2021-10-02 00:00:00,springer,A framework for validating AI in precision medicine: considerations from the European ITFoC consortium,https://www.biomedcentral.com/openurl?doi=10.1186/s12911-021-01634-3,"Background Artificial intelligence (AI) has the potential to transform our healthcare systems significantly. New AI technologies based on machine learning approaches should play a key role in clinical decision-making in the future. However, their implementation in health care settings remains limited, mostly due to a lack of robust validation procedures. There is a need to develop reliable assessment frameworks for the clinical validation of AI. We present here an approach for assessing AI for predicting treatment response in triple-negative breast cancer (TNBC), using real-world data and molecular -omics data from clinical data warehouses and biobanks. Methods The European “ITFoC (Information Technology for the Future Of Cancer)” consortium designed a framework for the clinical validation of AI technologies for predicting treatment response in oncology. Results This framework is based on seven key steps specifying: (1) the intended use of AI, (2) the target population, (3) the timing of AI evaluation, (4) the datasets used for evaluation, (5) the procedures used for ensuring data safety (including data quality, privacy and security), (6) the metrics used for measuring performance, and (7) the procedures used to ensure that the AI is explainable. This framework forms the basis of a validation platform that we are building for the “ITFoC Challenge”. This community-wide competition will make it possible to assess and compare AI algorithms for predicting the response to TNBC treatments with external real-world datasets. Conclusions The predictive performance and safety of AI technologies must be assessed in a robust, unbiased and transparent manner before their implementation in healthcare settings. We believe that the consideration of the ITFoC consortium will contribute to the safe transfer and implementation of AI in clinical settings, in the context of precision oncology and personalized care.",health
10.1007/978-3-030-81850-0_17,to_check,The Health Information Workforce,Springer,2021-01-01 00:00:00,springer,Working as a Health AI Specialist,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-81850-0_17,"Artificial intelligence and the sub-field of machine learning offer the potential to deliver data-driven healthcare solutions that can improve patient care and increase efficiency in healthcare services. Despite this, the methods and models are new and complicated, to those who work in healthcare. This chapter explores the implementation of such solutions in healthcare settings, through five real-world case studies of experts applying this technology in a variety of clinical settings and at different stages of implementation. These cases highlight the challenges and opportunities posed by implementing artificial intelligence and data-driven solutions, and the lessons learnt from colleagues pioneering its adoption in the healthcare sector.",health
10.1186/s12916-019-1382-x,to_check,BMC Medicine,BioMed Central,2019-07-17 00:00:00,springer,Beyond the hype of big data and artificial intelligence: building foundations for knowledge and wisdom,https://www.biomedcentral.com/openurl?doi=10.1186/s12916-019-1382-x,"Big data, coupled with the use of advanced analytical approaches, such as artificial intelligence (AI), have the potential to improve medical outcomes and population health. Data that are routinely generated from, for example, electronic medical records and smart devices have become progressively easier and cheaper to collect, process, and analyze. In recent decades, this has prompted a substantial increase in biomedical research efforts outside traditional clinical trial settings. Despite the apparent enthusiasm of researchers, funders, and the media, evidence is scarce for successful implementation of products, algorithms, and services arising that make a real difference to clinical care. This article collection provides concrete examples of how “big data” can be used to advance healthcare and discusses some of the limitations and challenges encountered with this type of research. It primarily focuses on real-world data, such as electronic medical records and genomic medicine, considers new developments in AI and digital health, and discusses ethical considerations and issues related to data sharing. Overall, we remain positive that big data studies and associated new technologies will continue to guide novel, exciting research that will ultimately improve healthcare and medicine—but we are also realistic that concerns remain about privacy, equity, security, and benefit to all.",health
10.1007/978-3-030-35231-8_65,to_check,Advanced Data Mining and Applications,Springer,2019-01-01 00:00:00,springer,An Anti-fraud Framework for Medical Insurance Based on Deep Learning,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-35231-8_65,"Given rising medical costs, medical expense control has become an important task in the healthcare domain. To solve the shortage of medical reimbursement mechanisms based on medical service items, single-disease payment models have been extensively studied. However, the approach of payment via a single-disease model is also flawed, and fraud may occur. Herein, we present an anti-fraud framework for medical insurance based on deep learning to automatically identify suspicious medical records, ensure the effective implementation of single-disease charges, and reduce the workload of medical insurance auditors. The framework first predicts the probabilities of diseases according to patients’ chief complaints and then evaluates whether the disease codes written in medical records are reasonable via the predicted probabilities; finally, medical records with unreasonable disease codes are selected as abnormal cases for manual auditing. We conduct experiments on a real-world dataset from a large hospital and demonstrate that our model can play an effective role in anti-fraud for medical insurance.",health
http://arxiv.org/abs/1803.04873v2,to_check,arxiv,arxiv,2018-03-13 15:17:30+00:00,arxiv,"Using Convolutional Neural Networks for Determining Reticulocyte
  Percentage in Cats",http://arxiv.org/abs/1803.04873v2,"Recent advances in artificial intelligence (AI), specifically in computer
vision (CV) and deep learning (DL), have created opportunities for novel
systems in many fields. In the last few years, deep learning applications have
demonstrated impressive results not only in fields such as autonomous driving
and robotics, but also in the field of medicine, where they have, in some
cases, even exceeded human-level performance. However, despite the huge
potential, adoption of deep learning-based methods is still slow in many areas,
especially in veterinary medicine, where we haven't been able to find any
research papers using modern convolutional neural networks (CNNs) in medical
image processing. We believe that using deep learning-based medical imaging can
enable more accurate, faster and less expensive diagnoses in veterinary
medicine. In order to do so, however, these methods have to be accessible to
everyone in this field, not just to computer scientists. To show the potential
of this technology, we present results on a real-world task in veterinary
medicine that is usually done manually: feline reticulocyte percentage. Using
an open source Keras implementation of the Single-Shot MultiBox Detector (SSD)
model architecture and training it on only 800 labeled images, we achieve an
accuracy of 98.7% at predicting the correct number of aggregate reticulocytes
in microscope images of cat blood smears. The main motivation behind this paper
is to show not only that deep learning can approach or even exceed human-level
performance on a task like this, but also that anyone in the field can
implement it, even without a background in computer science.",health
http://arxiv.org/abs/1905.02940v1,to_check,arxiv,arxiv,2019-05-08 07:26:27+00:00,arxiv,"A new direction to promote the implementation of artificial intelligence
  in natural clinical settings",http://arxiv.org/abs/1905.02940v1,"Artificial intelligence (AI) researchers claim that they have made great
`achievements' in clinical realms. However, clinicians point out the so-called
`achievements' have no ability to implement into natural clinical settings. The
root cause for this huge gap is that many essential features of natural
clinical tasks are overlooked by AI system developers without medical
background. In this paper, we propose that the clinical benchmark suite is a
novel and promising direction to capture the essential features of the
real-world clinical tasks, hence qualifies itself for guiding the development
of AI systems, promoting the implementation of AI in real-world clinical
practice.",health
10.1016/j.ymssp.2021.107955,to_check,Mechanical Systems and Signal Processing,scopus,2021-12-01,sciencedirect,A novel percussion-based method for multi-bolt looseness detection using one-dimensional memory augmented convolutional long short-term memory networks,https://api.elsevier.com/content/abstract/scopus_id/85104439984,"In the past decade, bolt looseness detection has attracted much attention. Compared to common approaches that require the implementation of constant-contact sensors, several percussion-based methods have demonstrated their superiorities, including low-cost and easy-to-operate, in detecting bolt looseness. However, some drawbacks may impede the further real-world application of percussion-based methods in detecting bolt looseness. First, current percussion-based methods depend on hand-crafted features, which require the extensive experience of operators. In addition, the ability of current percussion-based methods in anti-noising and adaptability is unknown, since no related investigation has been conducted. Moreover, only single-bolt looseness is considered in the current percussion-based investigation. With these deficiencies in mind, in this paper, we propose a novel percussion-based method that uses a newly developed one-dimensional memory augmented convolutional long short-term memory (1D-MACLSTM) networks. Via the convolutional operation in the 1D-MACLSTM, we can avoid manual feature extraction, and the long short-term memory (LSTM) controller backed by external memory can enhance the ability of anti-noising and adaptability. Finally, three case studies are conducted on a pair of typical multi-bolt connections to verify the effectiveness of the proposed method, which has better performance than current percussion-based methods, particularly in a noisy environment and new scenarios.",health
10.1016/j.jbi.2021.103922,to_check,Journal of Biomedical Informatics,scopus,2021-11-01,sciencedirect,Predicting potential palliative care beneficiaries for health plans: A generalized machine learning pipeline,https://api.elsevier.com/content/abstract/scopus_id/85116551006,"Recognizing that palliative care improves the care quality and reduces the healthcare costs for individuals in their end of life, health plan providers strive to better enroll the appropriate target population for palliative care. Current research has not adequately addressed challenges related to proactively select potential palliative care beneficiaries from a population health perspective. This study presents a Generalized Machine Learning Pipeline (GMLP) to predict palliative needs in patients using administrative claims data. The GMLP has five steps: data cohort creation, feature engineering, predictive modeling, scoring beneficiaries, and model maintenance. It encapsulates principles of population health management, business domain knowledge, and machine learning (ML) process knowledge with an innovative data pull strategy. The GMLP was applied in a regional health plan using a data cohort of 17,197 patients. Multiple ML models were turned and evaluated against a custom performance metric based on the business requirement. The best model was an AdaBoost model with a precision of 71.43% and a recall of 67.98%. The post-implementation evaluation of the GMLP showed that it increased the recall of high mortality risk patients, improved their quality of life, and reduced the overall cost. The GMLP is a novel approach that can be applied agnostically to the data and specific ML algorithms. To the best of our knowledge, it is the first attempt to continuously score palliative care beneficiaries using administrative data. The GMLP and its use case example presented in the paper can serve as a methodological guide for different health plans and healthcare policymakers to apply ML in solving real-world clinical challenges, such as palliative care management and other similar risk-stratified care management workflows.",health
10.1016/j.ymssp.2020.107510,to_check,Mechanical Systems and Signal Processing,scopus,2021-06-16,sciencedirect,Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,https://api.elsevier.com/content/abstract/scopus_id/85100211264,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model.",health
10.1016/j.eswa.2020.113920,to_check,Expert Systems with Applications,scopus,2021-03-01,sciencedirect,BIDI: A classification algorithm with instance difficulty invariance,https://api.elsevier.com/content/abstract/scopus_id/85090363699,"In artificial intelligence, an expert/intelligent systems can emulate the decision-making ability of human experts. A good classification algorithm can provide significant assistance to expert/intelligent systems in solving a variety of practical problems. In classification, the “hard” instances may be outliers or noisy instances that are difficult to learn, which may confuse the classifier and induce the overfitting problem in the case of placing much emphasis on them. In fact, the difficulty of instances is crucial for improving the generalization and credibility of classification. Unfortunately, nearly all the existing classifiers ignore this important information. In this paper, the classification difficulty of each instance is introduced from a statistical perspective, which is an inherent characteristic of the instance itself. Then, a new classification algorithm named “boosting with instance difficulty invariance (BIDI)” is proposed by incorporating the classification difficulty of instances. The BIDI conforms to the human cognition that easy instances are misclassified with a lower probability than difficult ones, and performs better with respect to generalization. The key insight of BIDI can provide relevant guidance for researchers to improve the generalization and credibility of classifiers in the expert systems of decision support systems. Experimental results demonstrate the effectiveness of BIDI in real-world data sets, indicating that it has great potential for solving many classification tasks of expert systems such as disease diagnosis and credit card fraud detection. Although the classification difficulty has strong statistical significance, its implementation remains computationally expensive. A fast method demonstrating rationality and feasibility is also proposed to approximate instances’ classification difficulty.",health
10.1016/j.jclepro.2020.123365,to_check,Journal of Cleaner Production,scopus,2020-12-20,sciencedirect,An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode,https://api.elsevier.com/content/abstract/scopus_id/85089891280,"The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers’ maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions.",health
10.1016/j.berh.2020.101559,to_check,Best Practice and Research: Clinical Rheumatology,scopus,2020-10-01,sciencedirect,Innovations to improve access to musculoskeletal care,https://api.elsevier.com/content/abstract/scopus_id/85088788188,"Innovation is a form of realising a new way of doing something, often ignoring traditional wisdom, in order to meet new challenges. Globally, particularly in emerging economies, the high burden of musculoskeletal conditions and their contribution to multimorbidity continue to rise, as does the gap for services to deliver essential care. There is a growing need to find solutions to this challenge and deliver person-centred and integrated care, wherein empowering patients with the capacity for self-management is critical. Whilst there is an abundance of information available online to support consumer education, the number of sources for credible medical information is diluted by uninformed anecdotal social media solutions. Even with the provision of high-quality information, behavioural change does not necessarily follow, and more robust educational approaches are required.
                  In this chapter, we examine innovation, its management and the strategic directions required to improve musculoskeletal healthcare at macro (policy), meso (service delivery) and micro (clinical practice) levels. We discuss the critical role of consumer agency (patients and their families/carers) in driving innovation and the need to leverage this through empowerment by education.
                  We provide a snapshot of real-world examples of innovative practices including capacity building in consumer and interprofessional musculoskeletal education and practice; recommendations to transform the access and delivery of integrated, person-centred care; and initiatives in musculoskeletal care and implementation of models of care, enabled by digital health solutions including telehealth, remote monitoring, artificial intelligence, blockchain technology and big data. We provide emerging evidence for how innovation can support systems' strengthening and build capacity to support improved access to ‘right’ musculoskeletal care, and explore some of the ways to best manage innovations.
                  We conclude with recommended systematic steps to establish required leadership, collaboration, research, networking, dissemination, implementation and evaluation of future innovations in musculoskeletal health and care.",health
10.1016/j.jacr.2019.06.009,to_check,Journal of the American College of Radiology,scopus,2019-10-01,sciencedirect,Bending the Artificial Intelligence Curve for Radiology: Informatics Tools From ACR and RSNA,https://api.elsevier.com/content/abstract/scopus_id/85071398084,"Artificial intelligence (AI) will reshape radiology over the coming years. The radiology community has a strong history of embracing new technology for positive change, and AI is no exception. As with any new technology, rapid, successful implementation faces several challenges that will require creation and adoption of new integration technology. Use cases important to real-world application of AI are described, including clinical registries, AI research, AI product validation, and computer assistance for radiology reporting. Furthermore, the informatics technologies required for successful implementation of the use cases are described, including open Computer-Assisted Radiologist Decision Support, ACR Assist, ACR Data Science Institute use cases, common data elements (radelement.org), RadLex (radlex.org), LOINC/RSNA RadLex Playbook (loinc.org), and Radiology Report Templates (radreport.org).",health
10.1109/PacificVis.2018.00026,to_check,2018 IEEE Pacific Visualization Symposium (PacificVis),IEEE,2018-04-13 00:00:00,ieeexplore,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,https://ieeexplore.ieee.org/document/8365986/,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",industry
10.1109/BigData.2017.8258076,to_check,2017 IEEE International Conference on Big Data (Big Data),IEEE,2017-12-14 00:00:00,ieeexplore,Empirical evaluations of active learning strategies in legal document review,https://ieeexplore.ieee.org/document/8258076/,"One type of machine learning, text classification, is now regularly applied in the legal matters involving voluminous document populations because it can reduce the time and expense associated with the review of those documents. One form of machine learning - Active Learning - has drawn attention from the legal community because it offers the potential to make the machine learning process even more effective. Active Learning, applied to legal documents, is considered a new technology in the legal domain and is continuously applied to all documents in a legal matter until an insignificant number of relevant documents are left for review. This implementation is slightly different than traditional implementations of Active Learning where the process stops once achieving acceptable model performance. The purpose of this paper is twofold: (i) to question whether Active Learning actually is a superior learning methodology and (ii) to highlight the ways that Active Learning can be most effectively applied to real legal industry data. Unlike other studies, our experiments were performed against large data sets taken from recent, real-world legal matters covering a variety of areas. We conclude that, although these experiments show the Active Learning strategy popularly used in legal document review can quickly identify informative training documents, it becomes less effective over time. In particular, our findings suggest this most popular form of Active Learning in the legal arena, where the highest-scoring documents are selected as training examples, is in fact not the most efficient approach in most instances. Ultimately, a different Active Learning strategy may be best suited to initiate the predictive modeling process but not to continue through the entire document review.",industry
10.1109/CCGRID.2017.22,to_check,"2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",IEEE,2017-05-17 00:00:00,ieeexplore,Energy Model for Low-Power Cluster,https://ieeexplore.ieee.org/document/7973809/,"Energy efficiency in high performance computing (HPC) systems is a relevant issue nowadays, which is approached from multiple edges and components (network, I/O, resource management, etc). HPC industry turned its focus towards embedded and low-power computational infrastructures (of RISC architecture processors) to improve energy efficiency, therefore, we use an ARM-based cluster, known as millicluster, designed to achieve high energy efficiency with low power. We provide a model for energy consumption estimation based on experimental data, obtained of measurements performed during a benchmarking process that represents a real-world workload, such as scientific computing algorithms of artificial intelligence. The energy model enables power prediction of tasks in low-power nodes with high accuracy, and its implementation in a job scheduling algorithm of HPC, facilitates the optimization of energy consumption and performance metrics at the same time.",industry
10.1109/BigData.2018.8622065,to_check,2018 IEEE International Conference on Big Data (Big Data),IEEE,2018-12-13 00:00:00,ieeexplore,Parallel Large-Scale Neural Network Training For Online Advertising,https://ieeexplore.ieee.org/document/8622065/,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",industry
10.1109/TASE.2012.2198057,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2012-07-01 00:00:00,ieeexplore,Neural-Network-Based Optimal Control for a Class of Unknown Discrete-Time Nonlinear Systems Using Globalized Dual Heuristic Programming,https://ieeexplore.ieee.org/document/6203617/,"In this paper, a neuro-optimal control scheme for a class of unknown discrete-time nonlinear systems with discount factor in the cost function is developed. The iterative adaptive dynamic programming algorithm using globalized dual heuristic programming technique is introduced to obtain the optimal controller with convergence analysis in terms of cost function and control law. In order to carry out the iterative algorithm, a neural network is constructed first to identify the unknown controlled system. Then, based on the learned system model, two other neural networks are employed as parametric structures to facilitate the implementation of the iterative algorithm, which aims at approximating at each iteration the cost function and its derivatives and the control law, respectively. Finally, a simulation example is provided to verify the effectiveness of the proposed optimal control approach. Note to Practitioners-The increasing complexity of the real-world industry processes inevitably leads to the occurrence of nonlinearity and high dimensions, and their mathematical models are often difficult to build. How to design the optimal controller for nonlinear systems without the requirement of knowing the explicit model has become one of the main foci of control practitioners. However, this problem cannot be handled by only relying on the traditional dynamic programming technique because of the ""curse of dimensionality"". To make things worse, the backward direction of solving process of dynamic programming precludes its wide application in practice. Therefore, in this paper, the iterative adaptive dynamic programming algorithm is proposed to deal with the optimal control problem for a class of unknown nonlinear systems forward-in-time. Moreover, the detailed implementation of the iterative ADP algorithm through the globalized dual heuristic programming technique is also presented by using neural networks. Finally, the effectiveness of the control strategy is illustrated via simulation study.",industry
10.1109/83.791960,to_check,IEEE Transactions on Image Processing,IEEE,1999-10-01 00:00:00,ieeexplore,Real-time DSP implementation for MRF-based video motion detection,https://ieeexplore.ieee.org/document/791960/,"This paper describes the real time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling, MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach.",industry
http://arxiv.org/abs/2111.09478v1,to_check,arxiv,arxiv,2021-11-18 02:18:27+00:00,arxiv,"Software Engineering for Responsible AI: An Empirical Study and
  Operationalised Patterns",http://arxiv.org/abs/2111.09478v1,"Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.",industry
http://arxiv.org/abs/1909.10270v1,to_check,arxiv,arxiv,2019-09-23 10:37:59+00:00,arxiv,"Pose Estimation for Texture-less Shiny Objects in a Single RGB Image
  Using Synthetic Training Data",http://arxiv.org/abs/1909.10270v1,"In the industrial domain, the pose estimation of multiple texture-less shiny
parts is a valuable but challenging task. In this particular scenario, it is
impractical to utilize keypoints or other texture information because most of
them are not actual features of the target but the reflections of surroundings.
Moreover, the similarity of color also poses a challenge in segmentation. In
this article, we propose to divide the pose estimation process into three
stages: object detection, features detection and pose optimization. A
convolutional neural network was utilized to perform object detection.
Concerning the reliability of surface texture, we leveraged the contour
information for estimating pose. Since conventional contour-based methods are
inapplicable to clustered metal parts due to the difficulties in segmentation,
we use the dense discrete points along the metal part edges as semantic
keypoints for contour detection. Afterward, we exploit both keypoint
information and CAD model to calculate the 6D pose of each object in view. A
typical implementation of deep learning methods not only requires a large
amount of training data, but also relies on intensive human labor for labeling
the datasets. Therefore, we propose an approach to generate datasets and label
them automatically. Despite not using any real-world photos for training, a
series of experiments showed that the algorithm built on synthetic data perform
well in the real environment.",industry
http://arxiv.org/abs/1904.01719v1,to_check,arxiv,arxiv,2019-04-03 00:56:14+00:00,arxiv,"Empirical Evaluations of Active Learning Strategies in Legal Document
  Review",http://arxiv.org/abs/1904.01719v1,"One type of machine learning, text classification, is now regularly applied
in the legal matters involving voluminous document populations because it can
reduce the time and expense associated with the review of those documents. One
form of machine learning - Active Learning - has drawn attention from the legal
community because it offers the potential to make the machine learning process
even more effective. Active Learning, applied to legal documents, is considered
a new technology in the legal domain and is continuously applied to all
documents in a legal matter until an insignificant number of relevant documents
are left for review. This implementation is slightly different than traditional
implementations of Active Learning where the process stops once achieving
acceptable model performance. The purpose of this paper is twofold: (i) to
question whether Active Learning actually is a superior learning methodology
and (ii) to highlight the ways that Active Learning can be most effectively
applied to real legal industry data. Unlike other studies, our experiments were
performed against large data sets taken from recent, real-world legal matters
covering a variety of areas. We conclude that, although these experiments show
the Active Learning strategy popularly used in legal document review can
quickly identify informative training documents, it becomes less effective over
time. In particular, our findings suggest this most popular form of Active
Learning in the legal arena, where the highest-scoring documents are selected
as training examples, is in fact not the most efficient approach in most
instances. Ultimately, a different Active Learning strategy may be best suited
to initiate the predictive modeling process but not to continue through the
entire document review.",industry
http://arxiv.org/abs/2006.06082v3,to_check,arxiv,arxiv,2020-06-10 21:54:27+00:00,arxiv,Towards Integrating Fairness Transparently in Industrial Applications,http://arxiv.org/abs/2006.06082v3,"Numerous Machine Learning (ML) bias-related failures in recent years have led
to scrutiny of how companies incorporate aspects of transparency and
accountability in their ML lifecycles. Companies have a responsibility to
monitor ML processes for bias and mitigate any bias detected, ensure business
product integrity, preserve customer loyalty, and protect brand image.
Challenges specific to industry ML projects can be broadly categorized into
principled documentation, human oversight, and need for mechanisms that enable
information reuse and improve cost efficiency. We highlight specific roadblocks
and propose conceptual solutions on a per-category basis for ML practitioners
and organizational subject matter experts. Our systematic approach tackles
these challenges by integrating mechanized and human-in-the-loop components in
bias detection, mitigation, and documentation of projects at various stages of
the ML lifecycle. To motivate the implementation of our system -- SIFT (System
to Integrate Fairness Transparently) -- we present its structural primitives
with an example real-world use case on how it can be used to identify potential
biases and determine appropriate mitigation strategies in a participatory
manner.",industry
http://arxiv.org/abs/2106.06150v1,to_check,arxiv,arxiv,2021-06-11 03:30:25+00:00,arxiv,Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs,http://arxiv.org/abs/2106.06150v1,"Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.",industry
10.1109/SBESC49506.2019.9046078,to_check,2019 IX Brazilian Symposium on Computing Systems Engineering (SBESC),IEEE,2019-11-22 00:00:00,ieeexplore,A Linked Data-Based Semantic Information Model for Smart Cities,https://ieeexplore.ieee.org/document/9046078/,"Smart cities typically involve a myriad of inter-connected systems intended to promote better management of urban and natural resources of cities, thereby contributing to the improve the quality of life of citizens. The heterogeneity of domains, systems, data, and relationships among them requires defining a data model able to express information in a flexible, extensible way while promoting interoperability between systems and applications. Furthermore, smart city systems can benefit from georeferenced information to allow for more effective actions over the real-world urban space. Aiming at tackling challenges related to data heterogeneity while considering georeferenced information, this work introduces LGeoSIM, a semantic-based information model for smart cities as means of fostering interoperability and powerful automated reasoning upon unambiguous information. LGeoSIM relies on the recent NGSI-LD Specification, thereby encompassing the principles of Linked Data to allow semantically defining information through ontologies and their interconnection. This paper also presents an implementation of LGeoSIM within Smart Geo Layers, a geographic-layered data middleware platform conceived to integrate data provided by heterogeneous sources in a smart city environment.",smart cities
10.1109/ICDE.2016.7498348,to_check,2016 IEEE 32nd International Conference on Data Engineering (ICDE),IEEE,2016-05-20 00:00:00,ieeexplore,Mercury: Metro density prediction with recurrent neural network on streaming CDR data,https://ieeexplore.ieee.org/document/7498348/,"Telecommunication companies possess mobility information of their phone users, containing accurate locations and velocities of commuters travelling in public transportation system. Although the value of telecommunication data is well believed under the smart city vision, there is no existing solution to transform the data into actionable items for better transportation, mainly due to the lack of appropriate data utilization scheme and the limited processing capability on massive data. This paper presents the first ever system implementation of real-time public transportation crowd prediction based on telecommunication data, relying on the analytical power of advanced neural network models and the computation power of parallel streaming analytic engines. By analyzing the feeds of caller detail record (CDR) from mobile users in interested regions, our system is able to predict the number of metro passengers entering stations, the number of waiting passengers on the platforms and other important metrics on the crowd density. New techniques, including geographical-spatial data processing, weight-sharing recurrent neural network, and parallel streaming analytical programming, are employed in the system. These new techniques enable accurate and efficient prediction outputs, to meet the real-world business requirements from public transportation system.",smart cities
10.1109/IV48863.2021.9575140,to_check,2021 IEEE Intelligent Vehicles Symposium (IV),IEEE,2021-07-17 00:00:00,ieeexplore,Urban Traffic Surveillance (UTS): A fully probabilistic 3D tracking approach based on 2D detections,https://ieeexplore.ieee.org/document/9575140/,"Urban Traffic Surveillance (UTS) is a surveillance system based on a monocular and calibrated video camera that detects vehicles in an urban traffic scenario with dense traffic on multiple lanes and vehicles performing sharp turning maneuvers. UTS then tracks the vehicles using a 3D bounding box representation and a physically reasonable 3D motion model relying on an unscented Kalman filter based approach. Since UTS recovers positions, shape and motion information in a three-dimensional world coordinate system, it can be employed to recognize diverse traffic violations or to supply intelligent vehicles with valuable traffic information. We build on YOLOv3 as a detector yielding 2D bounding boxes and class labels for each vehicle. A 2D detector renders our system much more independent to different camera perspectives as a variety of labeled training data is available. This allows for a good generalization while also being more hardware efficient. The task of 3D tracking based on 2D detections is supported by integrating class specific prior knowledge about the vehicle shape. We quantitatively evaluate UTS using self generated synthetic data and ground truth from the CARLA simulator, due to the non-existence of datasets with an urban vehicle surveillance setting and labeled 3D bounding boxes. Additionally, we give a qualitative impression of how UTS performs on real-world data. Our implementation is capable of operating in real time on a reasonably modern workstation. To the best of our knowledge, UTS is to date the only 3D vehicle tracking system in a surveillance scenario (static camera observing moving targets).",smart cities
10.1109/TITS.2019.2935152,to_check,IEEE Transactions on Intelligent Transportation Systems,IEEE,2020-09-01 00:00:00,ieeexplore,T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction,https://ieeexplore.ieee.org/document/8809901/,"Accurate and real-time traffic forecasting plays an important role in the intelligent traffic system and is of great significance for urban traffic planning, traffic management, and traffic control. However, traffic forecasting has always been considered an “open” scientific issue, owing to the constraints of urban road network topological structure and the law of dynamic change with time. To capture the spatial and temporal dependences simultaneously, we propose a novel neural network-based traffic forecasting method, the temporal graph convolutional network (T-GCN) model, which is combined with the graph convolutional network (GCN) and the gated recurrent unit (GRU). Specifically, the GCN is used to learn complex topological structures for capturing spatial dependence and the gated recurrent unit is used to learn dynamic changes of traffic data for capturing temporal dependence. Then, the T-GCN model is employed to traffic forecasting based on the urban road network. Experiments demonstrate that our T-GCN model can obtain the spatio-temporal correlation from traffic data and the predictions outperform state-of-art baselines on real-world traffic datasets. Our tensorflow implementation of the T-GCN is available at https://www.github.com/lehaifeng/T-GCN.",smart cities
10.1109/SMC.2014.6974301,to_check,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",IEEE,2014-10-08 00:00:00,ieeexplore,Cloud aided safety-based route planning,https://ieeexplore.ieee.org/document/6974301/,"This paper proposes a novel multi-objective route planning approach within the framework of a Vehicle-to-Cloud-to-Vehicle (V2C2V) architecture. Time and road risk index (RRI) are both considered as metrics. To evaluate road segment risk, an accident database from the Highway Safety Information System (HSIS) is processed to build a comprehensive road risk assessment model. Route planning is formulated as a multi-objective network flow problem and further reduced to a Mixed Integer Programming (MIP) problem. A real-world case study, route planning through the city of Columbus, Ohio, is presented. The Vehicle-to-Cloud-to-Vehicle (V2C2V) based implementation of our safety-based route planning approach is proposed to facilitate access to real-time information and computing resources.",smart cities
10.1109/IEEECONF49454.2021.9382646,to_check,2021 IEEE/SICE International Symposium on System Integration (SII),IEEE,2021-01-14 00:00:00,ieeexplore,Development and Testing of Garbage Detection for Autonomous Robots in Outdoor Environments,https://ieeexplore.ieee.org/document/9382646/,"In Japan, there is a growing concern about labor shortages due to the declining birthrate and aging population, and there are high expectations for robots to help solve such social problems and create industries. However, due to the prohibition of public road tests in Japan, there are few examples of actual applications of robots. Therefore, considerations and problems in the practical application of robots are still unclear. In this paper, by focusing on the implementation of garbage collection technology, we have developed an autonomous garbage collection robot using deep learning. In addition, we have verified the usefulness of our garbage detection technology in outdoor environments by conducting actual demonstrations at HANEDA INNOVATION CITY, which is a large-scale commercial and business complex belonged private property, Utsunomiya University, and Nakanoshima Challenge 2019, which is a field of demonstration experiment in the outdoor environment. Our garbage detector was designed to detect cans, plastic bottles, and lunch boxes automatically. Through experiments on test data and outdoor experiments in the real-world, we have confirmed that our detector has a 95.6% Precision and 96.8% Recall. Conparisons to other state-of-the-art detectors are also presented.",smart cities
10.1109/INFOCOM41043.2020.9155232,to_check,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,IEEE,2020-07-09 00:00:00,ieeexplore,MABSTA: Collaborative Computing over Heterogeneous Devices in Dynamic Environments,https://ieeexplore.ieee.org/document/9155232/,"Collaborative computing, leveraging resource on multiple wireless-connected devices, enables complex applications that a single device cannot support individually. However, the problem of assigning tasks over devices becomes challenging in the dynamic environments encountered in real-world settings, considering that the resource availability and channel conditions change over time in unpredictable ways due to mobility and other factors. In this paper, we formulate the task assignment problem as an online learning problem using an adversarial multi-armed bandit framework. We propose MABSTA, a novel algorithm that learns the performance of unknown devices and channel qualities continually through exploratory probing and makes task assignment decisions by exploiting the gained knowledge. The implementation of MABSTA, based on Gibbs Sampling approach, is computational-light and offers competitive performance in different scenarios on the trace-data obtained from a wireless IoT testbed. Furthermore, we prove that MABSTA is 1-competitive compared to the best offline assignment for any dynamic environment without stationarity assumptions, and demonstrate the polynomial-time algorithm for the exact implementation of the sampling process. To the best of our knowledge, MABSTA is the first online learning algorithm tailored to this class of problems.",smart cities
10.1109/TETC.2021.3070422,to_check,IEEE Transactions on Emerging Topics in Computing,IEEE,2021-09-01 00:00:00,ieeexplore,A Study of the Effects and Benefits of Custom-Precision Mathematical Libraries for HPC Codes,https://ieeexplore.ieee.org/document/9395219/,"Mathematical libraries are typically developed for use with the fixed-width data-paths on processors and target common floating-point formats such as IEEE <monospace>binary32</monospace> and <monospace>binary64</monospace>. To address the increasing energy consumption and throughput requirements of HPC, scientific computing and AI applications, libraries and hardware implementations now provide new floating-point formats, allowing mathematical function evaluations with different performance and accuracy trade-offs. In this article we present a methodology and its associated proof-of-concept tool to evaluate the benefits of custom accuracy of mathematical library calls in HPC and scientific computations. First, our tool collects for each call-site of a mathematical function the input- and output-data profile. Then, using a heuristic exploration algorithm, we estimate the minimal required accuracy by rounding the result to lower precisions. The data profile and accuracy measurement per call-site is used to speculatively select the mathematical function implementation with the most appropriate accuracy for a given scenario. We have tested the methodology with the Intel <monospace>MKL</monospace> Vector Math (<monospace>VM</monospace>) library, leveraging the predefined accuracy levels. We demonstrate the benefits of our approach on two real-world applications: <monospace>SGP4</monospace>, a satellite tracking application, and <monospace>PATMOS</monospace>, a Monte Carlo neutron transport code. The robustness of the methodology is estimated by measuring the numerical accuracy of the resulting optimized code, against user-defined criteria. We experiment and discuss generalization across data-sets and finally propose a speculative runtime implementation for <monospace>PATMOS</monospace>. The experiment provides an insight into the performance improvements achievable by leveraging the control of per-function call-site accuracy-mode execution of the Intel compiler <monospace>SVML</monospace> library. We show benefits from 13 to 55 percent in time reduction for the <monospace>PATMOS</monospace> use case.",smart cities
http://arxiv.org/abs/1811.05320v3,to_check,arxiv,arxiv,2018-11-12 03:30:03+00:00,arxiv,T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction,http://arxiv.org/abs/1811.05320v3,"Accurate and real-time traffic forecasting plays an important role in the
Intelligent Traffic System and is of great significance for urban traffic
planning, traffic management, and traffic control. However, traffic forecasting
has always been considered an open scientific issue, owing to the constraints
of urban road network topological structure and the law of dynamic change with
time, namely, spatial dependence and temporal dependence. To capture the
spatial and temporal dependence simultaneously, we propose a novel neural
network-based traffic forecasting method, the temporal graph convolutional
network (T-GCN) model, which is in combination with the graph convolutional
network (GCN) and gated recurrent unit (GRU). Specifically, the GCN is used to
learn complex topological structures to capture spatial dependence and the
gated recurrent unit is used to learn dynamic changes of traffic data to
capture temporal dependence. Then, the T-GCN model is employed to traffic
forecasting based on the urban road network. Experiments demonstrate that our
T-GCN model can obtain the spatio-temporal correlation from traffic data and
the predictions outperform state-of-art baselines on real-world traffic
datasets. Our tensorflow implementation of the T-GCN is available at
https://github.com/lehaifeng/T-GCN.",smart cities
http://arxiv.org/abs/1108.0123v1,to_check,arxiv,arxiv,2011-07-31 01:55:21+00:00,arxiv,Lean Algebraic Multigrid (LAMG): Fast Graph Laplacian Linear Solver,http://arxiv.org/abs/1108.0123v1,"Laplacian matrices of graphs arise in large-scale computational applications
such as machine learning; spectral clustering of images, genetic data and web
pages; transportation network flows; electrical resistor circuits; and elliptic
partial differential equations discretized on unstructured grids with finite
elements. A Lean Algebraic Multigrid (LAMG) solver of the linear system Ax=b is
presented, where A is a graph Laplacian. LAMG's run time and storage are linear
in the number of graph edges. LAMG consists of a setup phase, in which a
sequence of increasingly-coarser Laplacian systems is constructed, and an
iterative solve phase using multigrid cycles. General graphs pose algorithmic
challenges not encountered in traditional applications of algebraic multigrid.
LAMG combines a lean piecewise-constant interpolation, judicious node
aggregation based on a new node proximity definition, and an energy correction
of the coarse-level systems. This results in fast convergence and substantial
overhead and memory savings. A serial LAMG implementation scaled linearly for a
diverse set of 1666 real-world graphs with up to six million edges. This
multilevel methodology can be fully parallelized and extended to eigenvalue
problems and other graph computations.",smart cities
http://arxiv.org/abs/1108.1310v2,to_check,arxiv,arxiv,2011-08-05 12:04:00+00:00,arxiv,"Lean Algebraic Multigrid (LAMG): Fast Graph Laplacian Linear Solver
  (Journal Version)",http://arxiv.org/abs/1108.1310v2,"Laplacian matrices of graphs arise in large-scale computational applications
such as semi-supervised machine learning; spectral clustering of images,
genetic data and web pages; transportation network flows; electrical resistor
circuits; and elliptic partial differential equations discretized on
unstructured grids with finite elements. A Lean Algebraic Multigrid (LAMG)
solver of the symmetric linear system Ax=b is presented, where A is a graph
Laplacian. LAMG's run time and storage are empirically demonstrated to scale
linearly with the number of edges.
  LAMG consists of a setup phase during which a sequence of
increasingly-coarser Laplacian systems is constructed, and an iterative solve
phase using multigrid cycles. General graphs pose algorithmic challenges not
encountered in traditional multigrid applications. LAMG combines a lean
piecewise-constant interpolation, judicious node aggregation based on a new
node proximity measure (the affinity), and an energy correction of coarse-level
systems. This results in fast convergence and substantial setup and memory
savings. A serial LAMG implementation scaled linearly for a diverse set of 3774
real-world graphs with up to 47 million edges, with no parameter tuning. LAMG
was more robust than the UMFPACK direct solver and Combinatorial Multigrid
(CMG), although CMG was faster than LAMG on average. Our methodology is
extensible to eigenproblems and other graph computations.",smart cities
10.1016/j.psym.2011.07.004,to_check,Psychosomatics,scopus,2012-01-01,sciencedirect,Impact of Integrated and Measurement-Based Depression Care: Clinical Experience in an HIV Clinic,https://api.elsevier.com/content/abstract/scopus_id/84859559400,"Background
                  Just as in heart disease and diabetes, depression in HIV/AIDS is associated with negative outcomes. While randomized trials have shown the efficacy of treatment for depression in HIV/AIDS, the implementation of evidence-based treatments in real-world settings remains a challenge.
               
                  Objective
                  The objective of this study was to assess the effectiveness of a collaborative, measurement-based approach to depression care, including psychopharmacologic and ancillary psychological therapies in patients with HIV/AIDS and to examine whether or not effective depression treatment would also improve virologic and immunologic outcomes.
               
                  Methods
                  This was a retrospective chart review of patients referred for depression to a co-located psychiatry consultation service embedded within an infectious diseases outpatient clinic at an urban tertiary hospital. Data extracted at initial assessment and at last appointment included: axis I diagnosis, whether the patient was on an antidepressant, whether the patient was on a stimulant, BDI-II score, HIV RNA level, and CD4 cell count.
               
                  Results
                  One hundred twenty-four patient charts were included. Pre- vs. post-treatment analyses revealed significant reductions in depression (average BDI-II score of 23 to 15.7, p = 0.00001) and HIV RNA (14.1 K to 4 K copies/mL, p = 0 .003), and significant increases in CD4 count (518 to 592 cells/μL, p = 0.001). Additionally, more participants were prescribed antidepressants and stimulants at post- vs. pre-treatment.
               
                  Conclusion
                  Taking a collaborative, measurement-based approach to depression care appears to be an effective method for improving depression, virologic, and immunologic outcomes in depressed patients with HIV/AIDS illness.",smart cities
10.1109/90.759332,to_check,IEEE/ACM Transactions on Networking,IEEE,1999-02-01 00:00:00,ieeexplore,A QoS-provisioning neural fuzzy connection admission controller for multimedia high-speed networks,https://ieeexplore.ieee.org/document/759332/,"This paper proposes a neural fuzzy approach for connection admission control (CAC) with QoS guarantee in multimedia high-speed networks. Fuzzy logic systems have been successfully applied to deal with traffic-control-related problems and have provided a robust mathematical framework for dealing with real-world imprecision. However, there is no clear and general technique to map domain knowledge on traffic control onto the parameters of a fuzzy logic system. Neural networks have learning and adaptive capabilities that can be used to construct intelligent computational algorithms for traffic control. However, the knowledge embodied in conventional methods is difficult to incorporate into the design of neural networks. The proposed neural fuzzy connection admission control (NFCAC) scheme is an integrated method that combines the linguistic control capabilities of a fuzzy logic controller and the learning abilities of a neural network. It is an intelligent implementation so that it can provide a robust framework to mimic experts' knowledge embodied in existing traffic control techniques and can construct efficient computational algorithms for traffic control. We properly choose input variables and design the rule structure for the NFCAC controller so that it can have robust operation even under dynamic environments. Simulation results show that compared with a conventional effective-bandwidth-based CAC, a fuzzy-logic-based CAC, and a neural-net-based CAC, the proposed NFCAC can achieve superior system utilization, high learning speed, and simple design procedure, while keeping the QoS contract.",multimedia
10.1109/TKDE.2003.1161587,to_check,IEEE Transactions on Knowledge and Data Engineering,IEEE,2003-01-01 00:00:00,ieeexplore,Semantic abstractions in the multimedia domain,https://ieeexplore.ieee.org/document/1161587/,"Information searching by exactly matching content is traditionally a strong point of machine searching; this is not, however, how human memory works and is rarely satisfactory for advanced retrieval tasks in any domain-multimedia in particular, where the presentational aspects can be equally important to the semantic information content. A combined abstraction of the conceptual and presentational characteristics of multimedia applications, leading on the one hand to their conceptual structure (with classic semantics of the real-world modeled by entities, relationships, and attributes) and on the other to the presentational structure (including media type, logical structure, temporal synchronization, spatial (on the screen) ""synchronization"" and interactive behavior) is developed in this paper. Multimedia applications are construed as consisting of ""Presentational Units:"" elementary (a media object with play duration and screen position), and composite (recursive structures of PUs in the temporal, spatial, and logical dimensions). The fundamental concept introduced is that of Semantic Multimedia Abstractions (SMA): qualitative abstract descriptions of multimedia applications in terms of their conceptual and presentational properties at an adjustable level of abstraction. SMAs, which could be viewed as metadata, form an abstract space to be queried. A detailed study of possible abstractions (from multimedia applications to SMAs and SMA-to-SMA), a definition and query language for Semantic Multimedia Abstractions (SMA-L) and the corresponding SMA model (equivalent to extended OMT), as well as an implementation of a system capable of wrapping the presentational structure of XML-based documents complete this work, whose contribution lays in the classically fruitful boundary between AI, software engineering, and database research.",multimedia
10.1109/AERO50100.2021.9438232,to_check,2021 IEEE Aerospace Conference (50100),IEEE,2021-03-13 00:00:00,ieeexplore,A Pipeline for Vision-Based On-Orbit Proximity Operations Using Deep Learning and Synthetic Imagery,https://ieeexplore.ieee.org/document/9438232/,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations.",multimedia
10.1109/ICCE-ASIA.2017.8309327,to_check,2017 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia),IEEE,2017-10-07 00:00:00,ieeexplore,Adverse weather simulation for training neural networks,https://ieeexplore.ieee.org/document/8309327/,"Convolutional neural networks generally require considerable amount of data for training to perform adequately well in all real-world scenarios. Many times, the data for all scenarios is hard to collect and ground truth annotation is also a challenge. Similar problem exists in training networks for the autonomous vehicles given the diverse weather conditions where these cars are expected to be driven. Thus, a synthetic data generation model is imperative and we go about building a weather simulation framework. This framework is intended to generate weather conditions over different driving scenarios. To start with, we go about implementing a completely configurable rain/fog/windshield simulation model. The scope of this framework, however is much more than these three models. Apart from refining these models further as and when need, we intend to build in mechanisms to simulate more diverse weather conditions within this framework. There are multiple challenges in the implementation of these models. To begin with, we need a mechanism to simulate diverse weather conditions in a driving environment. One method could be to simulate the entire 3D environment, with the roads, automobiles, and an artificial world, but this approach would be extremely challenging both in terms of the realism that can be achieved, and in terms of the time it would take for the implementation. Another method is to overlay the rain/fog effect on top of pre-rendered videos. This 2D overlaying technique is a practical solution, since there exist many driving videos at our disposal. In this paper, we outline methods to implement this effectively, and showcase the results obtained in training a Neural Network with this approach.",multimedia
10.1109/INAGENTSYS.2014.7005719,to_check,"2014 International Conference on Intelligent Autonomous Agents, Networks and Systems",IEEE,2014-08-21 00:00:00,ieeexplore,Implementation of multi-agent system using fuzzy logic towards shopping center simulation,https://ieeexplore.ieee.org/document/7005719/,"A computer simulation tool has a purpose to simulate real-world phenomenon using mathematical and computational calculations to run the simulations and provide a detailed report as the result of the simulation. By using a computer simulation tool, users can retrieve information relevant to the simulation area in a relatively short time. In this paper, the author presents a prototype of multi-agent simulation tool for shopping centers. The shopping centers and all its components are presented in a simulated 3D environment. The simulation tool is built by using Unity3D engine for building 3D environment and running the simulation. The shopping behavior of the agents in the simulation is performed with a fuzzy logic calculation with the basis of the agent's knowledge. This simulation tool is tested by experts for its ability to simulate and gives accurate information of the simulation. The result of the simulation shows that the implementation of the multi-agent system in shopping centers can show the general idea of people's activities in shopping centers.",multimedia
10.1109/TSMC.2018.2830099,to_check,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2019-07-01 00:00:00,ieeexplore,Efficient Deep CNN-Based Fire Detection and Localization in Video Surveillance Applications,https://ieeexplore.ieee.org/document/8385121/,"Convolutional neural networks (CNNs) have yielded state-of-the-art performance in image classification and other computer vision tasks. Their application in fire detection systems will substantially improve detection accuracy, which will eventually minimize fire disasters and reduce the ecological and social ramifications. However, the major concern with CNN-based fire detection systems is their implementation in real-world surveillance networks, due to their high memory and computational requirements for inference. In this paper, we propose an original, energy-friendly, and computationally efficient CNN architecture, inspired by the SqueezeNet architecture for fire detection, localization, and semantic understanding of the scene of the fire. It uses smaller convolutional kernels and contains no dense, fully connected layers, which helps keep the computational requirements to a minimum. Despite its low computational needs, the experimental results demonstrate that our proposed solution achieves accuracies that are comparable to other, more complex models, mainly due to its increased depth. Moreover, this paper shows how a tradeoff can be reached between fire detection accuracy and efficiency, by considering the specific characteristics of the problem of interest and the variety of fire data.",multimedia
10.1109/ISMSIT50672.2020.9254863,to_check,2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),IEEE,2020-10-24 00:00:00,ieeexplore,Personalized Quality of Experience (QOE) Management using Data Driven Architecture in 5G Wireless Networks,https://ieeexplore.ieee.org/document/9254863/,"the aim of this paper is to Personalized Quality of Experience (QOE) Management using Data driven Architecture in 5G Wireless Networks that consume less resources. The proposed research will be the part of the overall research project, which focuses on addressing a problem that many organizations experience that introduce an Enterprise Architecture to support the integration of different services across the enterprise. With the rapid growth in mobile network usage and video streaming being the most popular service, Quality of Experience of video in mobile networks is of extreme importance to both service providers and their customers. The ability to effectively predict Quality of Experience of video is key for QoE adaptation and higher levels of customer satisfaction. In this work machine learning algorithms were used to create models that predict QoE with network QoS parameters, including wireless-specific and 5Gspecific parameters. An 5G simulation that reflects the current mobile traffic landscape was created to obtain the data set for training. An objective tool for video QoE evaluation was used to gather QoE data necessary to train the prediction models. Support Vector Machines, Random Forest, Gradient Boosted Trees and Neural Networks were chosen as the machine learning algorithms for Quality of Experience prediction, and it was shown that they achieve high accuracy. Influence of wireless-specific parameters on QoE prediction was also investigated, and it was discovered that they are suitable for use in Quality of Experience prediction models.The problem is that; organizations do not know where they either have or may encounter weaknesses in their Enterprise Architecture with Data Driven Architecture (DDA). The framework presented is based on concepts from Wireless Networks with Driven Architecture will be designed to support both Transitional Gap Analysis (TGA) and Comparative Gap Analysis (CGA). TGA is supported by comparing a baseline Data Driven Architecture (DDA) to a target QoE where both DDA have been defined from the management perspective. DDA is facilitated by mapping a QoE to two or more 5G networks. The research methodology used in the paper is design science research for the QoE management based 5G network. The QOE for implementation of 5th generation network and apply it in many different real-world organizations. The goal of the paper is to present a framework in the form an implementation and management model, called QOE, that visualizes the gaps (weaknesses) in proposed or existing enterprise architectures and to support a comparative analysis process for different a5Grnative solution approaches. a set of requirements on the QOE management can be presented and the frameworks are applied on Matlab for implementation.",multimedia
10.1109/IJCNN.2008.4633922,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,Stable reciprocal image associations in cognitive systems,https://ieeexplore.ieee.org/document/4633922/,"Sensory inputs such as visual images or audio spectrograms can act as symbols in a new cognitive model. The stability of direct image association operators allows the discrete bit patterns in a general-purpose symbol processing system to be replaced with continuous real-world signals. Analogous to an SR flip-flop, two reciprocal images recursively connected by association processors, can ldquolockrdquo each other in place. A computational model of the Brodmann areas, whose boundaries are defined by the thickness of the exterior and interior lamina in cerebral cortex, closely resembles this structure. The recurrence between the cells in the cortical columns allows local connections in small regions to form overall global image associations. An implementation, based on neurotransmitter field theory, demonstrates the stability of the reciprocal-image attractors.",multimedia
10.1109/ICASSP.2019.8683618,to_check,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2019-05-17 00:00:00,ieeexplore,Introducing Undergraduates to Pattern Recognition and Machine Learning through Speech Processing,https://ieeexplore.ieee.org/document/8683618/,This paper describes an educational project experience that achieves a software implementation and performance analysis of a blind signal to noise ratio (SNR) estimation system for noisy speech. The system is based on a pattern recognition paradigm and no clean speech reference signal is available. It is a product of the faculty's research and funded by a government contract. The faculty's research on a real-world issue in speech processing has been converted into an undergraduate project. Assessment results show that the project is viewed very favorably by students. Target versus control group results show that the target group feels better qualified for graduate study and career options in digital signal processing.,multimedia
http://arxiv.org/abs/1510.03727v1,to_check,arxiv,arxiv,2015-10-13 15:06:03+00:00,arxiv,SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes,http://arxiv.org/abs/1510.03727v1,"We present an open-source, real-time implementation of SemanticPaint, a
system for geometric reconstruction, object-class segmentation and learning of
3D scenes. Using our system, a user can walk into a room wearing a depth camera
and a virtual reality headset, and both densely reconstruct the 3D scene and
interactively segment the environment into object classes such as 'chair',
'floor' and 'table'. The user interacts physically with the real-world scene,
touching objects and using voice commands to assign them appropriate labels.
These user-generated labels are leveraged by an online random forest-based
machine learning algorithm, which is used to predict labels for previously
unseen parts of the scene. The entire pipeline runs in real time, and the user
stays 'in the loop' throughout the process, receiving immediate feedback about
the progress of the labelling and interacting with the scene as necessary to
refine the predicted segmentation.",multimedia
http://arxiv.org/abs/1612.05571v1,to_check,arxiv,arxiv,2016-12-16 17:57:15+00:00,arxiv,Delta Networks for Optimized Recurrent Network Computation,http://arxiv.org/abs/1612.05571v1,"Many neural networks exhibit stability in their activation patterns over time
in response to inputs from sensors operating under real-world conditions. By
capitalizing on this property of natural signals, we propose a Recurrent Neural
Network (RNN) architecture called a delta network in which each neuron
transmits its value only when the change in its activation exceeds a threshold.
The execution of RNNs as delta networks is attractive because their states must
be stored and fetched at every timestep, unlike in convolutional neural
networks (CNNs). We show that a naive run-time delta network implementation
offers modest improvements on the number of memory accesses and computes, but
optimized training techniques confer higher accuracy at higher speedup. With
these optimizations, we demonstrate a 9X reduction in cost with negligible loss
of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on
the large Wall Street Journal speech recognition benchmark even existing
networks can be greatly accelerated as delta networks, and a 5.7x improvement
with negligible loss of accuracy can be obtained through training. Finally, on
an end-to-end CNN trained for steering angle prediction in a driving dataset,
the RNN cost can be reduced by a substantial 100X.",multimedia
http://arxiv.org/abs/1806.01759v2,to_check,arxiv,arxiv,2018-06-05 15:56:24+00:00,arxiv,"Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point
  Clouds",http://arxiv.org/abs/1806.01759v2,"Deep learning systems extensively use convolution operations to process input
data. Though convolution is clearly defined for structured data such as 2D
images or 3D volumes, this is not true for other data types such as sparse
point clouds. Previous techniques have developed approximations to convolutions
for restricted conditions. Unfortunately, their applicability is limited and
cannot be used for general point clouds. We propose an efficient and effective
method to learn convolutions for non-uniformly sampled point clouds, as they
are obtained with modern acquisition techniques. Learning is enabled by four
key novelties: first, representing the convolution kernel itself as a
multilayer perceptron; second, phrasing convolution as a Monte Carlo
integration problem, third, using this notion to combine information from
multiple samplings at different levels; and fourth using Poisson disk sampling
as a scalable means of hierarchical point cloud learning. The key idea across
all these contributions is to guarantee adequate consideration of the
underlying non-uniform sample distribution function from a Monte Carlo
perspective. To make the proposed concepts applicable to real-world tasks, we
furthermore propose an efficient implementation which significantly reduces the
GPU memory required during the training process. By employing our method in
hierarchical network architectures we can outperform most of the
state-of-the-art networks on established point cloud segmentation,
classification and normal estimation benchmarks. Furthermore, in contrast to
most existing approaches, we also demonstrate the robustness of our method with
respect to sampling variations, even when training with uniformly sampled data
only. To support the direct application of these concepts, we provide a
ready-to-use TensorFlow implementation of these layers at
https://github.com/viscom-ulm/MCCNN",multimedia
10.1016/j.ecoinf.2021.101475,to_check,Ecological Informatics,scopus,2021-12-01,sciencedirect,Automated feature-specific tree species identification from natural images using deep semi-supervised learning,https://api.elsevier.com/content/abstract/scopus_id/85119019677,"Prior work on plant species classification predominantly focuses on building models from isolated plant attributes. Hence, there is a need for tools that can assist in species identification in the natural world. We present a novel and robust two-fold approach capable of identifying trees in a real-world natural setting. Additionally, we leverage unlabelled data through deep semi-supervised learning and demonstrate superior performance to supervised learning. Our single-GPU implementation for feature recognition uses minimal annotated data and achieves accuracies of 93.96% and 93.11% for leaves and bark, respectively. Further, we extract feature-specific datasets of 50 species by employing this technique. Finally, our semi-supervised species classification method attains 94.04% top-5 accuracy for leaves and 83.04% top-5 accuracy for bark.",multimedia
10.1016/j.patrec.2021.06.020,to_check,Pattern Recognition Letters,scopus,2021-10-01,sciencedirect,Bangla Sign alphabet recognition with zero-shot and transfer learning,https://api.elsevier.com/content/abstract/scopus_id/85111317099,"Bangla, being the fifth most spoken language in the world has its own distinct sign language with two methods (one-handed and two-handed) of representation. However, a standard automatic recognition system of Bangla sign language (BdSL) is still to be achieved. Though widely studied and explored by researchers in the past years, certain unaddressed issues like identifying unseen signs and both types of BdSL or lack of evaluation of the models in versatile environmental conditions demarcate the real-world implementation of the automatic recognition of BdSL. To find a probable solution to the shortcomings in the existing works, this paper proposes two approaches based on conventional transfer learning and contemporary Zero-shot learning (ZSL) for automatic BdSL alphabet recognition of both seen and unseen data. The performance of the proposed system is evaluated for both types of Bangla sign representations as well as on a large dataset with 35,149 images from over 350 subjects, varying in terms of backgrounds, camera angle, light contrast, skin tone, hand size, and orientation. For the ZSL approach, a new semantic descriptor dedicated to BdSL is created and a split of the dataset into seen and unseen classes is proposed. Our model achieved 68.21%, 91.57%, and 54.34% of harmonic mean accuracy, seen accuracy, and zero-shot accuracy with six unseen classes respectively. For the transfer learning-based approach, we found pre-trained DenseNet201 architecture to be the best performing feature extractor and Linear Discriminant Analysis as the best classifier with an overall accuracy of 93.68% on the large dataset after conducting quantitative experimentation on 18 CNN architectures and 21 classifiers. The satisfactory result from our models supports its very probative potential to serve extensively for the hearing and speaking impaired community.",multimedia
10.1016/j.surg.2020.09.040,to_check,Surgery (United States),scopus,2021-05-01,sciencedirect,The future surgical training paradigm: Virtual reality and machine learning in surgical education,https://api.elsevier.com/content/abstract/scopus_id/85097383738,"Surgical training has undergone substantial change in the last few decades. As technology and patient complexity continues to increase, demands for novel approaches to ensure competency have arisen. Virtual reality systems augmented with machine learning represents one such approach. The ability to offer on-demand training, integrate checklists, and provide personalized, surgeon-specific feedback is paving the way to a new era of surgical training. Machine learning algorithms that improve over time as they acquire more data will continue to refine the education they provide. Further, fully immersive simulated environments coupled with machine learning analytics provide real-world training opportunities in a safe atmosphere away from the potential to harm patients. Careful implementation of these technologies has the potential to increase access and improve quality of surgical training and patient care and are poised to change the landscape of current surgical training. Herein, we describe the current state of virtual reality coupled with machine learning for surgical training, future directions, and existing limitations of this technology.",multimedia
10.1109/ICTAI.2018.00055,to_check,2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2018-11-07 00:00:00,ieeexplore,Legal Reasoning in Answer Set Programming,https://ieeexplore.ieee.org/document/8576053/,"Answer Set Programming is a declarative problem solving approach, initially tailored to modelling problems in the area of Knowledge Representation and Reasoning. In this article, we provide a knowledge-based system, capable of representing and reasoning about legal knowledge in the context of Answer Set Programming - thus, modelling non-monotonicity that is inherent in legal arguments. The work, although limited to a specific indicative domain, namely, university regulations, has a variety of extensions. The overall approach constitutes a representative implementation of the Answer Set Programming's modelling methodology, as well as an enhancing of the bond between Artificial Intelligence and Legal Science, bringing us a step closer to a successful development of an automated legal reasoning system for real-world applications.",science
10.1109/ICDMW51313.2020.00082,to_check,2020 International Conference on Data Mining Workshops (ICDMW),IEEE,2020-11-20 00:00:00,ieeexplore,SynC: A Copula based Framework for Generating Synthetic Data from Aggregated Sources,https://ieeexplore.ieee.org/document/9346329/,"A synthetic dataset is a data object that is generated programmatically, and it may be valuable to creating a single dataset from multiple sources when direct collection is difficult or costly. Although it is a fundamental step for many data science tasks, an efficient and standard framework is absent. In this paper, we study a specific synthetic data generation task called downscaling, a procedure to infer high-resolution, harder-to-collect information (e.g., individual level records) from many low-resolution, easy-to-collect sources, and propose a multi-stage framework called SynC (Synthetic Data Generation via Gaussian Copula). For given low-resolution datasets, the central idea of SynC is to fit Gaussian copula models to each of the low-resolution datasets in order to correctly capture dependencies and marginal distributions, and then sample from the fitted models to obtain the desired high-resolution subsets. Predictive models are then used to merge sampled subsets into one, and finally, sampled datasets are scaled according to low-resolution marginal constraints. We make four key contributions in this work: 1) propose a novel framework for generating individual level data from aggregated data sources by combining state-of-the-art machine learning and statistical techniques, 2) perform simulation studies to validate SynC's performance as a synthetic data generation algorithm, 3) demonstrate its value as a feature engineering tool, as well as an alternative to data collection in situations where gathering is difficult through two real-world datasets, 4) release an easy-to-use framework implementation for reproducibility and scalability at the production level that easily incorporates new data.",science
10.1109/WIIAT.2008.424,to_check,2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,IEEE,2008-12-12 00:00:00,ieeexplore,Autonomy-Oriented Computing for Web Intelligence and Brain Informatics,https://ieeexplore.ieee.org/document/4740421/,"In this talk, we will discuss the roles and potential implications of autonomy-oriented computing (AOC) in/to the future of Web intelligence (WI) and brain informatics (BI). Generally speaking, AOC is a methodology for self-organized computing that is well suited for two types of applications: (i) to characterize the working mechanisms that lead to certain emergent behavior in natural and artificial complex systems (e.g., phenomena in ldquoWeb Sciencerdquo, and the dynamics of social networks and neural systems), and (ii) to develop solutions to large-scale, distributed computational problems (e.g., distributed scalable scientific or social computing, and collective intelligence). AOC emphasizes the modeling of autonomous entities or agents that locally interact following certain nature or real-world inspired behavioral rules, resulting in some self-organized behavior of the entities and/or their nonlinearly aggregated effects. Computing based on interacting entities and their self-organization can offer several means as well as advantages for WI and BI development, such as natural formulation, distributed implementation, scalable performance, robustness, and behavioral or first-principle understanding.",science
10.1109/CIC48465.2019.00029,to_check,2019 IEEE 5th International Conference on Collaboration and Internet Computing (CIC),IEEE,2019-12-14 00:00:00,ieeexplore,Event Detection in Noisy Streaming Data with Combination of Corroborative and Probabilistic Sources,https://ieeexplore.ieee.org/document/8998489/,"Global physical event detection has traditionally relied on dense coverage of physical sensors around the world; while this is an expensive undertaking, there have not been alternatives until recently. The ubiquity of social networks and human sensors in the field provides a tremendous amount of real-time, live data about true physical events from around the world. However, while such human sensor data have been exploited for retrospective large-scale event detection such as hurricanes or earthquakes, there has been limited to no success in exploiting this rich resource for general physical event detection. Prior implementation approaches have suffered from the concept drift phenomenon, where real-world data exhibits continuous, unknown, and unbounded changes in its data distribution, making static machine learning models ineffective in the long term. We propose and implement an end-to-end collaborative drift adaptive system that integrates corroborative and probabilistic sources to deliver real-time predictions. Furthermore, our system is adaptive to concept drift and performs automated continuous learning to maintain high performance. We demonstrate our approach in a real-time demo available online for landslide disaster detection, with extensibility to other real-world physical events such as flooding, wildfires, hurricanes, and earthquakes.",science
10.1109/ICITR51448.2020.9310890,to_check,2020 5th International Conference on Information Technology Research (ICITR),IEEE,2020-12-04 00:00:00,ieeexplore,Hybrid Approach and Architecture to Detect Fake News on Twitter in Real-Time using Neural Networks,https://ieeexplore.ieee.org/document/9310890/,"Fake news has been a key issue since the dawn of social media. Currently, we are at a stage where it is merely impossible to differentiate between real and fake news. This directly and indirectly affects people's decision patterns and makes us question the credibility of the news shared via social media platforms. Twitter is one of the leading social networks in the world by active users. There has been an exponential spread of fake news on Twitter in the recent past. In this paper, we will discuss the implementation of a browser extension which will identify fake news on Twitter using deep learning models with a focus on real-world applicability, architectural stability and scalability of such a solution. Experimental results show that the proposed browser extension has an accuracy of 86% accuracy in fake news detection. To the best of our knowledge, our work is the first of its kind to detect fake news on Twitter real-time using a hybrid approach and evaluate using real users.",science
10.1109/IPDPS47924.2020.00100,to_check,2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS),IEEE,2020-05-22 00:00:00,ieeexplore,PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network,https://ieeexplore.ieee.org/document/9139807/,"Inspired by the successes of convolutional neural networks (CNN) in computer vision, the convolutional operation has been moved beyond low-dimension grids (e.g., images) to high-dimensional graph-structured data (e.g., web graphs, social networks), leading to graph convolutional network (GCN). And GCN has been gaining popularity due to its success in real-world applications such as recommendation, natural language processing, etc. Because neural network and graph propagation have high computation complexity, GPUs have been introduced to both neural network training and graph processing. However, it is notoriously difficult to perform efficient GCN computing on data parallel hardware like GPU due to the sparsity and irregularity in graphs. In this paper, we present PCGCN, a novel and general method to accelerate GCN computing by taking advantage of the locality in graphs. We experimentally demonstrate that real-world graphs usually have the clustering property that can be used to enhance the data locality in GCN computing. Then, PCGCN proposes to partition the whole graph into chunks according to locality and process subgraphs with a dual-mode computing strategy which includes a selective and a full processing methods for sparse and dense subgraphs, respectively. Compared to existing state-of-the-art implementations of GCN on real-world and synthetic datasets, our implementation on top of TensorFlow achieves up to 8.8× speedup over the fastest one of the baselines.",science
10.1109/TPDS.2021.3064942,to_check,IEEE Transactions on Parallel and Distributed Systems,IEEE,2021-09-01 00:00:00,ieeexplore,BALS: Blocked Alternating Least Squares for Parallel Sparse Matrix Factorization on GPUs,https://ieeexplore.ieee.org/document/9373912/,"Matrix factorization on sparse matrices has been proven to be an effective approach for data mining and machine learning. However, the prior parallel implementations for matrix factorization fail to capture the internal social property embedded in real-world use cases. This article presents an efficient implementation of the alternative least squares (ALS) algorithm called BALSbuilt on top of a new sparse matrix format for parallel matrix factorization. The BALS storage format organizes the sparse matrix into 2D tiles to avoid repeated data loads and improve data reuses. We further propose a data reordering technique to sort sparse matrices according to nonzeros. The experimental results show that BALS can yield a superior performance than state-of-the-art implementations, i.e., our BALS generally runs faster than Gates' implementation over different latent feature sizes, with a speedup of up to 2.08× on K20C, 3.72× on TITAN X and 3.13× on TITAN RTX. When compared with alternative matrix factorization algorithms, our BALS consistently outperforms CDMF, cuMF_CCD, and cuMF_SGD over various latent feature sizes and datasets. The reordering technique can provide an extra improvement of up to 23.68 percent on K20C, 19.87 percent on TITAN X and 20.38 percent on TITAN RTX.",science
10.1007/978-3-642-41981-2_116-1,to_check,Handbook of Mobile Teaching and Learning,Springer,2019-01-01 00:00:00,springer,Enhancing Student Learning Experience with Practical Big Data Analytics Techniques,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-642-41981-2_116-1,"With the enormous growing volumes and varieties of consumer data, generated by various desktop, mobile, and Internet of Things applications, e-commerce, and other resources, and the recent advances of computational processing and data storage technologies, big data analytics has become an increasingly important tool of transforming large quantities of digital data into meaningful insight and decisions (see “Characteristics of Mobile Teaching and Learning”). The concept of big data has been around for decades, but it has only become a hot buzzword in the last few years, and its broad applications nowadays have been enthusiastically embraced by financial service providers, retailers, insurers, manufacturers, healthcare organizations, universities, and other enterprises. To meet the great demand for data scientists and engineers from almost every sector of industry, business, and government, several universities have recently started graduate programs in data science or data analytics. However, the number of undergraduate programs that have integrated big data analytics courses into their curricula still remains very small. In this chapter, the author describes the design, implementation, and evaluation of two data analytics courses, Introduction to Data Mining and Introduction to Artificial Neural Networks, which have been developed and included in the undergraduate computer science program at the University of San Diego (USD). Since the spring of 2011, both courses have been offered as upper-division electives on a regular basis, and it has been a very successful learning experience for both the instructor and the students. The courses include, in addition to the coverage on key data analytics concepts, principles, and applications, a unique student lecture series, programming projects, and research activities to engage students in active learning. The author has also recently been offering the data mining course as a study abroad program in China, integrated with additional guest lectures by data scientists from the host institutions and field trips to visit top information technology firms in the host country. The author’s experience has shown that big data analytics can be successfully taught at the undergraduate level, and in fact, students enrolled in the courses have learned a great deal of data analytics techniques and have been able to apply them to solve many real-world problems.",science
10.1007/978-981-13-2766-7_116,to_check,Handbook of Mobile Teaching and Learning,Springer,2019-01-01 00:00:00,springer,Enhancing Student Learning Experience with Practical Big Data Analytics Techniques,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-13-2766-7_116,"With the enormous growing volumes and varieties of consumer data, generated by various desktop, mobile, and Internet of Things applications, e-commerce, and other resources, and the recent advances of computational processing and data storage technologies, big data analytics has become an increasingly important tool of transforming large quantities of digital data into meaningful insight and decisions (see Chap. 2, “Characteristics of Mobile Teaching and Learning” ). The concept of big data has been around for decades, but it has only become a hot buzzword in the last few years, and its broad applications nowadays have been enthusiastically embraced by financial service providers, retailers, insurers, manufacturers, healthcare organizations, universities, and other enterprises. To meet the great demand for data scientists and engineers from almost every sector of industry, business, and government, several universities have recently started graduate programs in data science or data analytics. However, the number of undergraduate programs that have integrated big data analytics courses into their curricula still remains very small. In this chapter, the author describes the design, implementation, and evaluation of two data analytics courses, Introduction to Data Mining and Introduction to Artificial Neural Networks, which have been developed and included in the undergraduate computer science program at the University of San Diego (USD). Since the spring of 2011, both courses have been offered as upper-division electives on a regular basis, and it has been a very successful learning experience for both the instructor and the students. The courses include, in addition to the coverage on key data analytics concepts, principles, and applications, a unique student lecture series, programming projects, and research activities to engage students in active learning. The author has also recently been offering the data mining course as a study abroad program in China, integrated with additional guest lectures by data scientists from the host institutions and field trips to visit top information technology firms in the host country. The author’s experience has shown that big data analytics can be successfully taught at the undergraduate level, and in fact, students enrolled in the courses have learned a great deal of data analytics techniques and have been able to apply them to solve many real-world problems.",science
http://arxiv.org/abs/1609.02664v1,to_check,arxiv,arxiv,2016-09-09 06:04:17+00:00,arxiv,"Machine Learning with Guarantees using Descriptive Complexity and SMT
  Solvers",http://arxiv.org/abs/1609.02664v1,"Machine learning is a thriving part of computer science. There are many
efficient approaches to machine learning that do not provide strong theoretical
guarantees, and a beautiful general learning theory. Unfortunately, machine
learning approaches that give strong theoretical guarantees have not been
efficient enough to be applicable. In this paper we introduce a logical
approach to machine learning. Models are represented by tuples of logical
formulas and inputs and outputs are logical structures. We present our
framework together with several applications where we evaluate it using SAT and
SMT solvers. We argue that this approach to machine learning is particularly
suited to bridge the gap between efficiency and theoretical soundness. We
exploit results from descriptive complexity theory to prove strong theoretical
guarantees for our approach. To show its applicability, we present experimental
results including learning complexity-theoretic reductions rules for board
games. We also explain how neural networks fit into our framework, although the
current implementation does not scale to provide guarantees for real-world
neural networks.",science
http://arxiv.org/abs/2012.13968v1,to_check,arxiv,arxiv,2020-12-27 16:03:32+00:00,arxiv,"Detecting Medical Misinformation on Social Media Using Multimodal Deep
  Learning",http://arxiv.org/abs/2012.13968v1,"In 2019, outbreaks of vaccine-preventable diseases reached the highest number
in the US since 1992. Medical misinformation, such as antivaccine content
propagating through social media, is associated with increases in vaccine delay
and refusal. Our overall goal is to develop an automatic detector for
antivaccine messages to counteract the negative impact that antivaccine
messages have on the public health. Very few extant detection systems have
considered multimodality of social media posts (images, texts, and hashtags),
and instead focus on textual components, despite the rapid growth of
photo-sharing applications (e.g., Instagram). As a result, existing systems are
not sufficient for detecting antivaccine messages with heavy visual components
(e.g., images) posted on these newer platforms. To solve this problem, we
propose a deep learning network that leverages both visual and textual
information. A new semantic- and task-level attention mechanism was created to
help our model to focus on the essential contents of a post that signal
antivaccine messages. The proposed model, which consists of three branches, can
generate comprehensive fused features for predictions. Moreover, an ensemble
method is proposed to further improve the final prediction accuracy. To
evaluate the proposed model's performance, a real-world social media dataset
that consists of more than 30,000 samples was collected from Instagram between
January 2016 and October 2019. Our 30 experiment results demonstrate that the
final network achieves above 97% testing accuracy and outperforms other
relevant models, demonstrating that it can detect a large amount of antivaccine
messages posted daily. The implementation code is available at
https://github.com/wzhings/antivaccine_detection.",science
http://arxiv.org/abs/1910.06840v3,to_check,arxiv,arxiv,2019-10-15 14:58:54+00:00,arxiv,A Hybrid Compact Neural Architecture for Visual Place Recognition,http://arxiv.org/abs/1910.06840v3,"State-of-the-art algorithms for visual place recognition, and related visual
navigation systems, can be broadly split into two categories:
computer-science-oriented models including deep learning or image
retrieval-based techniques with minimal biological plausibility, and
neuroscience-oriented dynamical networks that model temporal properties
underlying spatial navigation in the brain. In this letter, we propose a new
compact and high-performing place recognition model that bridges this divide
for the first time. Our approach comprises two key neural models of these
categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by
brain architectures of fruit flies, Drosophila melanogaster, and (2) a
one-dimensional continuous attractor neural network (CANN). The resulting
FlyNet+CANN network incorporates the compact pattern recognition capabilities
of our FlyNet model with the powerful temporal filtering capabilities of an
equally compact CANN, replicating entirely in a hybrid neural implementation
the functionality that yields high performance in algorithmic localization
approaches like SeqSLAM. We evaluate our model, and compare it to three
state-of-the-art methods, on two benchmark real-world datasets with small
viewpoint variations and extreme environmental changes - achieving 87% AUC
results under day to night transitions compared to 60% for Multi-Process
Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times
faster, respectively.",science
http://arxiv.org/abs/2108.11579v1,to_check,arxiv,arxiv,2021-08-26 05:00:27+00:00,arxiv,Modeling Item Response Theory with Stochastic Variational Inference,http://arxiv.org/abs/2108.11579v1,"Item Response Theory (IRT) is a ubiquitous model for understanding human
behaviors and attitudes based on their responses to questions. Large modern
datasets offer opportunities to capture more nuances in human behavior,
potentially improving psychometric modeling leading to improved scientific
understanding and public policy. However, while larger datasets allow for more
flexible approaches, many contemporary algorithms for fitting IRT models may
also have massive computational demands that forbid real-world application. To
address this bottleneck, we introduce a variational Bayesian inference
algorithm for IRT, and show that it is fast and scalable without sacrificing
accuracy. Applying this method to five large-scale item response datasets from
cognitive science and education yields higher log likelihoods and higher
accuracy in imputing missing data than alternative inference algorithms. Using
this new inference approach we then generalize IRT with expressive Bayesian
models of responses, leveraging recent advances in deep learning to capture
nonlinear item characteristic curves (ICC) with neural networks. Using an
eigth-grade mathematics test from TIMSS, we show our nonlinear IRT models can
capture interesting asymmetric ICCs. The algorithm implementation is
open-source, and easily usable.",science
http://arxiv.org/abs/1911.09281v1,to_check,arxiv,arxiv,2019-11-21 04:19:16+00:00,arxiv,"Event Detection in Noisy Streaming Data with Combination of
  Corroborative and Probabilistic Sources",http://arxiv.org/abs/1911.09281v1,"Global physical event detection has traditionally relied on dense coverage of
physical sensors around the world; while this is an expensive undertaking,
there have not been alternatives until recently. The ubiquity of social
networks and human sensors in the field provides a tremendous amount of
real-time, live data about true physical events from around the world. However,
while such human sensor data have been exploited for retrospective large-scale
event detection, such as hurricanes or earthquakes, they has been limited to no
success in exploiting this rich resource for general physical event detection.
  Prior implementation approaches have suffered from the concept drift
phenomenon, where real-world data exhibits constant, unknown, unbounded changes
in its data distribution, making static machine learning models ineffective in
the long term. We propose and implement an end-to-end collaborative drift
adaptive system that integrates corroborative and probabilistic sources to
deliver real-time predictions. Furthermore, out system is adaptive to concept
drift and performs automated continuous learning to maintain high performance.
We demonstrate our approach in a real-time demo available online for landslide
disaster detection, with extensibility to other real-world physical events such
as flooding, wildfires, hurricanes, and earthquakes.",science
http://arxiv.org/abs/1810.03190v1,to_check,arxiv,arxiv,2018-10-07 17:59:49+00:00,arxiv,"Scalable Solutions for Automated Single Pulse Identification and
  Classification in Radio Astronomy",http://arxiv.org/abs/1810.03190v1,"Data collection for scientific applications is increasing exponentially and
is forecasted to soon reach peta- and exabyte scales. Applications which
process and analyze scientific data must be scalable and focus on execution
performance to keep pace. In the field of radio astronomy, in addition to
increasingly large datasets, tasks such as the identification of transient
radio signals from extrasolar sources are computationally expensive. We present
a scalable approach to radio pulsar detection written in Scala that
parallelizes candidate identification to take advantage of in-memory task
processing using Apache Spark on a YARN distributed system. Furthermore, we
introduce a novel automated multiclass supervised machine learning technique
that we combine with feature selection to reduce the time required for
candidate classification. Experimental testing on a Beowulf cluster with 15
data nodes shows that the parallel implementation of the identification
algorithm offers a speedup of up to 5X that of a similar multithreaded
implementation. Further, we show that the combination of automated multiclass
classification and feature selection speeds up the execution performance of the
RandomForest machine learning algorithm by an average of 54% with less than a
2% average reduction in the algorithm's ability to correctly classify pulsars.
The generalizability of these results is demonstrated by using two real-world
radio astronomy data sets.",science
http://arxiv.org/abs/1904.07998v2,to_check,arxiv,arxiv,2019-04-16 22:10:19+00:00,arxiv,"SynC: A Unified Framework for Generating Synthetic Population with
  Gaussian Copula",http://arxiv.org/abs/1904.07998v2,"Synthetic population generation is the process of combining multiple
socioeconomic and demographic datasets from different sources and/or
granularity levels, and downscaling them to an individual level. Although it is
a fundamental step for many data science tasks, an efficient and standard
framework is absent. In this study, we propose a multi-stage framework called
SynC (Synthetic Population via Gaussian Copula) to fill the gap. SynC first
removes potential outliers in the data and then fits the filtered data with a
Gaussian copula model to correctly capture dependencies and marginal
distributions of sampled survey data. Finally, SynC leverages predictive models
to merge datasets into one and then scales them accordingly to match the
marginal constraints. We make three key contributions in this work: 1) propose
a novel framework for generating individual level data from aggregated data
sources by combining state-of-the-art machine learning and statistical
techniques, 2) demonstrate its value as a feature engineering tool, as well as
an alternative to data collection in situations where gathering is difficult
through two real-world datasets, 3) release an easy-to-use framework
implementation for reproducibility, and 4) ensure the methodology is scalable
at the production level and can easily incorporate new data.",science
http://arxiv.org/abs/2102.10477v1,to_check,arxiv,arxiv,2021-02-20 23:45:24+00:00,arxiv,"Neural Sampling Machine with Stochastic Synapse allows Brain-like
  Learning and Inference",http://arxiv.org/abs/2102.10477v1,"Many real-world mission-critical applications require continual online
learning from noisy data and real-time decision making with a defined
confidence level. Probabilistic models and stochastic neural networks can
explicitly handle uncertainty in data and allow adaptive learning-on-the-fly,
but their implementation in a low-power substrate remains a challenge. Here, we
introduce a novel hardware fabric that implements a new class of stochastic NN
called Neural-Sampling-Machine that exploits stochasticity in synaptic
connections for approximate Bayesian inference. Harnessing the inherent
non-linearities and stochasticity occurring at the atomic level in emerging
materials and devices allows us to capture the synaptic stochasticity occurring
at the molecular level in biological synapses. We experimentally demonstrate
in-silico hybrid stochastic synapse by pairing a ferroelectric field-effect
transistor -based analog weight cell with a two-terminal stochastic selector
element. Such a stochastic synapse can be integrated within the
well-established crossbar array architecture for compute-in-memory. We
experimentally show that the inherent stochastic switching of the selector
element between the insulator and metallic state introduces a multiplicative
stochastic noise within the synapses of NSM that samples the conductance states
of the FeFET, both during learning and inference. We perform network-level
simulations to highlight the salient automatic weight normalization feature
introduced by the stochastic synapses of the NSM that paves the way for
continual online learning without any offline Batch Normalization. We also
showcase the Bayesian inferencing capability introduced by the stochastic
synapse during inference mode, thus accounting for uncertainty in data. We
report 98.25%accuracy on standard image classification task as well as
estimation of data uncertainty in rotated samples.",science
http://arxiv.org/abs/2009.09471v1,to_check,arxiv,arxiv,2020-09-20 16:36:25+00:00,arxiv,"SYNC: A Copula based Framework for Generating Synthetic Data from
  Aggregated Sources",http://arxiv.org/abs/2009.09471v1,"A synthetic dataset is a data object that is generated programmatically, and
it may be valuable to creating a single dataset from multiple sources when
direct collection is difficult or costly. Although it is a fundamental step for
many data science tasks, an efficient and standard framework is absent. In this
paper, we study a specific synthetic data generation task called downscaling, a
procedure to infer high-resolution, harder-to-collect information (e.g.,
individual level records) from many low-resolution, easy-to-collect sources,
and propose a multi-stage framework called SYNC (Synthetic Data Generation via
Gaussian Copula). For given low-resolution datasets, the central idea of SYNC
is to fit Gaussian copula models to each of the low-resolution datasets in
order to correctly capture dependencies and marginal distributions, and then
sample from the fitted models to obtain the desired high-resolution subsets.
Predictive models are then used to merge sampled subsets into one, and finally,
sampled datasets are scaled according to low-resolution marginal constraints.
We make four key contributions in this work: 1) propose a novel framework for
generating individual level data from aggregated data sources by combining
state-of-the-art machine learning and statistical techniques, 2) perform
simulation studies to validate SYNC's performance as a synthetic data
generation algorithm, 3) demonstrate its value as a feature engineering tool,
as well as an alternative to data collection in situations where gathering is
difficult through two real-world datasets, 4) release an easy-to-use framework
implementation for reproducibility and scalability at the production level that
easily incorporates new data.",science
10.1016/j.dss.2021.113665,to_check,Decision Support Systems,scopus,2021-01-01,sciencedirect,A constraint programming model for making recommendations in personal process management: A design science research approach,https://api.elsevier.com/content/abstract/scopus_id/85115634506,"Decision-making in everyday life has an essential role in effectively completing personal tasks and processes. The complexity of these processes and the resulting cognitive load of managing them may vary significantly. To decrease the cognitive load created by such decision-making efforts and to obtain better outcomes, recommendation systems carry significant potential. In order to investigate the benefits provided by decision support systems (DSS) in personal process management (PPM), we first build a constraint programming (CP) model and a prototype context-aware-mobile application employing this CP model. Then, we evaluate the application and the model via two exemplary real-world scenarios. The scenarios form the core of the experiments conducted with 50 participants. We compare the participants’ planning performances with and without the PPM system with quantitative metrics such as planning times and scenario objective values. In addition, System Usability Scale (SUS) questionnaires and open-ended questions provide qualitative evaluation results. Throughout the study, we apply the Design Science Research methodology to rigorously conduct research activities by proof of concept, proof of use, and proof of value. The empirical results clearly show that our proposed model for PPM is effective, and the developed prototype solution generates positive participant comments as well as a high SUS score. Overall, the prototype PPM system with CP implementation leads to better planning in less time in the planning phase, and it lets the user do fast replanning in the execution phase, which is invaluable in dynamically changing situations such as daily activities.",science
10.1016/j.knosys.2018.03.016,to_check,Knowledge-Based Systems,scopus,2018-07-01,sciencedirect,Enhancing user creativity: Semantic measures for idea generation,https://api.elsevier.com/content/abstract/scopus_id/85044259734,"Human creativity generates novel ideas to solve real-world problems. This thereby grants us the power to transform the surrounding world and extend our human attributes beyond what is currently possible. Creative ideas are not just new and unexpected, but are also successful in providing solutions that are useful, efficient and valuable. Thus, creativity optimizes the use of available resources and increases wealth. The origin of human creativity, however, is poorly understood, and semantic measures that could predict the success of generated ideas are currently unknown. Here, we analyze a dataset of design problem-solving conversations in real-world settings by using 49 semantic measures based on WordNet 3.1 and demonstrate that a divergence of semantic similarity, an increased information content, and a decreased polysemy predict the success of generated ideas. The first feedback from clients also enhances information content and leads to a divergence of successful ideas in creative problem solving. These results advance cognitive science by identifying real-world processes in human problem solving that are relevant to the success of produced solutions and provide tools for real-time monitoring of problem solving, student training and skill acquisition. A selected subset of information content (IC Sánchez–Batet) and semantic similarity (Lin/Sánchez–Batet) measures, which are both statistically powerful and computationally fast, could support the development of technologies for computer-assisted enhancements of human creativity or for the implementation of creativity in machines endowed with general artificial intelligence.",science
10.1109/IROS.2005.1545040,to_check,2005 IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2005-08-06 00:00:00,ieeexplore,Broker: an interprocess communication solution for multi-robot systems,https://ieeexplore.ieee.org/document/1545040/,"We describe in this paper a novel implementation of the interprocess communication (IPC) technology, called Broker, in support of the development and the operation of a complex robot system. We view each robot system as a collection of processes that need to exchange information, e.g. motion commands and sensory data, in a flexible and convenient fashion, without affecting each other's operations in case of a process's scheduled termination or unexpected failure. We argue that the IPC technology provides an ideal framework for this purpose, and we carefully make our design decisions about its implementation based on the needs of robotics applications. Broker is programming language, operating system, and hardware platform independent and has served us well in a RoboCup project and collective robotics experiments, in both simulation and real-world environments.",robotics
10.1109/ITSC.2018.8569569,to_check,2018 21st International Conference on Intelligent Transportation Systems (ITSC),IEEE,2018-11-07 00:00:00,ieeexplore,ShadowCam: Real-Time Detection of Moving Obstacles Behind A Corner For Autonomous Vehicles,https://ieeexplore.ieee.org/document/8569569/,"Moving obstacles occluded by corners are a potential source for collisions in mobile robotics applications such as autonomous vehicles. In this paper, we address the problem of anticipating such collisions by proposing a vision-based detection algorithm for obstacles which are outside of a vehicle's direct line of sight. Our method detects shadows of obstacles hidden around corners and automatically classifies these unseen obstacles as “dynamic” or “static”. We evaluate our proposed detection algorithm on real-world corners and a large variety of simulated environments to assess generalizability in different challenging surface and lighting conditions. The mean classification accuracy on simulated data is around 80% and on real-world corners approximately 70%. Additionally, we integrate our detection system on a full-scale autonomous wheelchair and demonstrate its feasibility as an additional safety mechanism through real-world experiments. We release our real-time-capable implementation of the proposed ShadowCam algorithm and the dataset containing simulated and real-world data under an open-source license.",robotics
10.1109/TASE.2019.2940543,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2020-04-01 00:00:00,ieeexplore,Robust Visual Localization in Dynamic Environments Based on Sparse Motion Removal,https://ieeexplore.ieee.org/document/8855084/,"Visual localization has been well studied in recent decades and applied in many fields as a fundamental capability in robotics. However, the success of the state of the arts usually builds on the assumption that the environment is static. In dynamic scenarios where moving objects are present, the performance of the existing visual localization systems degrades a lot due to the disturbance of the dynamic factors. To address this problem, we propose a novel sparse motion removal (SMR) model that detects the dynamic and static regions for an input frame based on a Bayesian framework. The similarity between the consecutive frames and the difference between the current frame and the reference frame are both considered to reduce the detection uncertainty. After the detection process is finished, the dynamic regions are eliminated while the static ones are fed into a feature-based visual simultaneous localization and mapping (SLAM) system for further visual localization. To verify the proposed method, both qualitative and quantitative experiments are performed and the experimental results have demonstrated that the proposed model can significantly improve the accuracy and robustness for visual localization in dynamic environments.&lt;;/p&gt;&lt;;p&gt;&lt;;italic&gt;Note to Practitioners&lt;;/italic&gt;-This article was motivated by the visual localization problem in dynamic environments. Visual localization is well applied in many robotic fields such as path planning and exploration as the basic capability for a mobile robot. In the GPS-denied environments, one robot needs to localize itself through perceiving the unknown environment based on a visual sensor. In real-world scenes, the existence of the moving objects will significantly degrade the localization accuracy, which makes the robot implementation unreliable. In this article, an SMR model is designed to handle this problem. Once receiving a frame, the proposed model divides it into dynamic and static regions through a Bayesian framework. The dynamic regions are eliminated, while the static ones are maintained and fed into a feature-based visual SLAM system for further visual localization. The proposed method greatly improves the localization accuracy in dynamic environments and guarantees the robustness for robotic implementation.",robotics
10.1109/ICRA.2015.7139395,to_check,2015 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2015-05-30 00:00:00,ieeexplore,RoboSherlock: Unstructured information processing for robot perception,https://ieeexplore.ieee.org/document/7139395/,"We present RoboSherlock, an open source software framework for implementing perception systems for robots performing human-scale everyday manipulation tasks. In RoboSherlock, perception and interpretation of realistic scenes is formulated as an unstructured information management (UIM) problem. The application of the UIM principle supports the implementation of perception systems that can answer task-relevant queries about objects in a scene, boost object recognition performance by combining the strengths of multiple perception algorithms, support knowledge-enabled reasoning about objects and enable automatic and knowledge-driven generation of processing pipelines. We demonstrate the potential of the proposed framework by three feasibility studies of systems for real-world scene perception that have been built on top of RoboSherlock.",robotics
10.1109/JIOT.2020.2979413,to_check,IEEE Internet of Things Journal,IEEE,2020-08-01 00:00:00,ieeexplore,Adversarial Learning-Enabled Automatic WiFi Indoor Radio Map Construction and Adaptation With Mobile Robot,https://ieeexplore.ieee.org/document/9031749/,"Location-based service (LBS) has become an indispensable part of our daily lives. Realizing accurate LBS in indoor environments is still a challenging task. WiFi fingerprinting-based indoor positioning system (IPS) has achieved encouraging results recently, but the time and labor overhead of constructing a dense WiFi radio map remains the key bottleneck that hinders it for real-world large-scale implementation. In this article, we propose WiGAN an automatic fine-grained indoor ratio map construction and the adaptation scheme empowered by the Gaussian process regression conditioned least-squares generative adversarial networks (GPR-GANs) with a mobile robot. First, we develop a mobile robotic platform that constructs the spatial map and radio map simultaneously in the easily accessed free space. GPR-GAN first establishes a Gaussian process regression (GPR) model using the real received signal strength (RSS) measurements collected by our robotic platform via LiDAR SLAM in the free space. Then, the outputs of the GPR are adopted as the input of GAN's generator. The learning objective of GAN is to synthesize realistic RSS data in a constrained space where it has not been covered and model the irregular RSS distributions in complex indoor environments. Real-world experiments were conducted in a real-world indoor environment, which confirms the feasibility, high accuracy, and superiority of WiGAN over existing solutions in terms of both RSS estimation accuracy and localization accuracy.",robotics
10.1109/TCYB.2013.2275291,to_check,IEEE Transactions on Cybernetics,IEEE,2013-10-01 00:00:00,ieeexplore,Real-Time Multiple Human Perception With Color-Depth Cameras on a Mobile Robot,https://ieeexplore.ieee.org/document/6583249/,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.",robotics
10.1016/j.compag.2019.104973,to_check,Computers and Electronics in Agriculture,scopus,2019-10-01,sciencedirect,Deep learning-based visual recognition of rumex for robotic precision farming,https://api.elsevier.com/content/abstract/scopus_id/85071398904,"In this paper we address the problem of recognising the Broad-leaved dock (Rumex obtusifolius L.) in grasslands from high-resolution 2D images. We discuss and present the determining factors for developing and implementing weed visual recognition algorithms using deep learning. This analysis, leads to the formulation of the proposed algorithm. Our implementation exploits Transfer Learning techniques for deep learning-based feature extraction, in combination with a classifier for weed recognition. A prototype robotic platform has been used to make available an image dataset from a dairy farm containing broad-leaved docks. The evaluation of the proposed algorithm on this dataset shows that it outperforms competing weed/plant recognition methods in recognition accuracy, while producing low false-positive rates under real-world operation conditions.",robotics
10.1016/j.procs.2018.11.110,to_check,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Compositional models for VQA: Can neural module networks really count?,https://api.elsevier.com/content/abstract/scopus_id/85059483400,"Large neural networks trained in an end-to-end fashion usually fail to generalize over novel inputs which were not included in the training data. In contrast, biologically-inspired compositional models offer a more robust solution due to adaptive chaining of logical operations performed by specialized modules. In this paper, we present an implementation of a cognitive architecture based on the End-to-End Module Networks (N2NMNs) model [9] in the humanoid robot Pepper. The architecture is focused on the Visual Question Answering task (VQA), in which the robot answers questions regarding the seen image in natural language. We trained the system on the synthetic CLEVR dataset [10] and tested it on both synthetic images and real-world situations with CLEVR-like objects. We compare between the results and discuss the decrease of accuracy in real-world situations. Furthermore, we propose a new evaluation method, in which we test whether the model’s results for counting objects in each category is consistent with the overall number of seen objects. In summary, our results show that the current visual reasoning models are still far from being applicable in everyday life.",robotics
10.1016/j.bica.2015.06.007,to_check,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,"NARLE: Neurocognitive architecture for the autonomous task recognition, learning, and execution",https://api.elsevier.com/content/abstract/scopus_id/84941177769,"Robots controlled by the state of the art cognitive architectures are still far behind animals in their capabilities to learn complex skills and autonomously adapt to unexpected circumstances. The neurocognitive architecture proposed in this paper addresses the problem of learning and execution of hierarchical behaviors and complex skills. Learning is addressed both on the level of individual elementary behaviors and goal-directed sequences of actions. The proposed architecture comprises a Dynamic Neural Fields (DNFs) implementation of the low-level elementary behaviors and a Functional System Network (FSN) tying these behaviors in goal-directed sequences. The DNF framework enables a continuous, dynamical representation of perceptual features and motor parameters, which may be directly coupled to the robot’s sensors and motors. Attractor states and instabilities of the DNFs account for segregation of cognitive states and mark behaviorally relevant events in the continuous flow of sensorimotor dynamics. The FSN, in its turn, comprises dynamical elements that can be arranged in a multilayered network by a learning process, in which new layers and elementary behaviors are added on demand. In our architecture, the FSN controls adaptation processes in the already acquired neural-dynamic elementary behaviors, as well as formation of new elementary behaviors. Combination of the DNF and FSN frameworks in a neurocognitive architecture NARLE enables pervasive learning both on the level of individual behaviors and goal-directed sequence, contributing to the progress towards more adaptive intelligent robotic systems, capable to learn new tasks and extend their behavioral repertoire in stochastic real-world environments.",robotics
10.1109/ITSC.2018.8569575,to_check,2018 21st International Conference on Intelligent Transportation Systems (ITSC),IEEE,2018-11-07 00:00:00,ieeexplore,Deep Traffic Light Detection for Self-driving Cars from a Large-scale Dataset,https://ieeexplore.ieee.org/document/8569575/,"Traffic lights perception problem is one of the key challenges for autonomous vehicle controllers in urban areas. While a number of approaches for traffic light detection have been proposed, these methods often require a prior knowledge of map and/or show high false positive rates. Recent successes suggest that deep neural networks will be widely used in self-driving cars, but current public datasets do not provide sufficient amount of labels for training such large deep neural networks. In this paper, we developed a two-step computational method that can detect traffic lights from images in a real-time manner. The first step exploits a deep neural object detection architecture to fine true traffic light candidates. In the second step, a point-based reward system is used to eliminate false traffic lights out of the candidates. To evaluate the proposed approach, we collected a human-annotated large-scale traffic lights dataset (over 60 hours). We also designed a real-world experiment with an instrumented self-driving vehicle and observed that the proposed method was able to handle false traffic lights substantially better compared with the baseline considered.",autonomous vehicle
10.1016/j.compag.2020.105909,to_check,Computers and Electronics in Agriculture,scopus,2021-01-01,sciencedirect,State and parameter estimation of the AquaCrop model for winter wheat using sensitivity informed particle filter,https://api.elsevier.com/content/abstract/scopus_id/85098062759,"Crop models play a paramount role in providing quantitative information on crop growth and field management. However, its prediction performance degrades significantly in the presence of unknown, uncertain parameters and noisy measurements. Consequently, simultaneous state and parameter estimation (SSPE) for crop model is required to maximize its potentials. This work aims to develop an integrated dynamic SSPE framework for the AquaCrop model by leveraging constrained particle filter, crop sensitivity analysis and UAV remote sensing. Both Monte Carlo simulation and one winter wheat experimental case study are performed to validate the proposed framework. It is shown that: (i) the proposed framework with state/parameter bound and parameter sensitivity information outperforms conventional particle filter and constrained particle filter in both state and parameter estimation in Monte Carlo simulations; (ii) in real-world experiment, the proposed approach achieves the smallest root mean squared error for canopy cover estimation among the three algorithms by using day forward-chaining validation method.",autonomous vehicle
10.1016/j.ijpe.2016.01.003,to_check,International Journal of Production Economics,scopus,2016-04-01,sciencedirect,Using an agent-based neural-network computational model to improve product routing in a logistics facility,https://api.elsevier.com/content/abstract/scopus_id/84960121547,"This study tests whether a simplified neural-network computational model can make routing decisions in a logistics facility more efficiently than five ׳intelligent׳ routing heuristics from the logistics literature. The experiment uses a real-world simulation scenario based on the Hamburg Harbor Car Terminal, a logistic site faced with managing approximately 46,500 car-routing decisions on a yearly basis. The simulation environment has been built based on a data set provided by the Terminal operator to reflect a real-world case. The simulation results show that the percent-improvement of the neural-net model׳s performance is 48% better than that of the best routing heuristic tested in previous studies. To test the applicability of the method with more complex logistic scenarios, we relaxed the sequence constraints for routing in a subsequent simulations study. If logistic complexity in terms of more freedom in decision-making is increased, the neural net model׳s percent-improvement performance of routing decisions is around three times better than the best-performing heuristic.",autonomous vehicle
10.1109/ICTAI.2018.00161,to_check,2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2018-11-07 00:00:00,ieeexplore,Context Enhancement for Linear Contextual Multi-Armed Bandits,https://ieeexplore.ieee.org/document/8576159/,"Contextual Multi-Armed Bandit (CMAB) algorithms such as LinUCB (Linear Upper Confidence Bound) or Contextual Thompson Sampling (CTS) base their resolution on the assumption that there exists a linear dependency between the expected reward of an action and its context. Since context constrains such sequential decision problems, it seems unavoidable to work, as far as possible, with the most relevant context features. This article first sheds light on some contextual issues that can be encountered by real-world applications, and then proposes a new method of context enhancement, called Individual Context Enrichment: ICE, to be combined with CMABs. ICE allows CMAB algorithms to rely on additional relevant context features that are computed according to the previously obtained individual accuracy of users when in given contexts which we define as user-context pairs. Basically, ICE classifies user-context pairs according to their individual accuracy, and uses the obtained classes to enrich the original context. To be effective, our method requires regular users, thus it is particularly interesting in the case of applications having identifiable subscribers e.g., recommender systems, clinical trial or mobile health. We experiment and discuss our method, which shows better results on several real-life datasets in terms of global accuracy and cumulative regrets than any other original competitive Multi-Armed Bandit (MAB) or CMAB methods.",health
10.1109/COMPSAC48688.2020.0-226,to_check,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE,2020-07-17 00:00:00,ieeexplore,ReRe: A Lightweight Real-Time Ready-to-Go Anomaly Detection Approach for Time Series,https://ieeexplore.ieee.org/document/9202855/,"Anomaly detection is an active research topic in many different fields such as intrusion detection, network monitoring, system health monitoring, IoT healthcare, etc. However, many existing anomaly detection approaches require either human intervention or domain knowledge, and may suffer from high computation complexity, consequently hindering their applicability in real-world scenarios. Therefore, a lightweight and ready-to-go approach that is able to detect anomalies in real-time is highly sought-after. Such an approach could be easily and immediately applied to perform time series anomaly detection on any commodity machine. The approach could provide timely anomaly alerts and by that enable appropriate countermeasures to be undertaken as early as possible. With these goals in mind, this paper introduces ReRe, which is a Real-time Ready-to-go proactive Anomaly Detection algorithm for streaming time series. ReRe employs two lightweight Long Short-Term Memory (LSTM) models to predict and jointly determine whether or not an upcoming data point is anomalous based on short-term historical data points and two long-term self-adaptive thresholds. Our experiment based on real-world time-series datasets demonstrates the good performance of ReRe in real-time anomaly detection without requiring human intervention or domain knowledge.",health
10.1109/ACCESS.2021.3083064,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,A Framework for Anomaly Identification Applied on Fall Detection,https://ieeexplore.ieee.org/document/9439497/,"Automatic systems to monitor people and subsequently improve people's lives have been emerging in the last few years, and currently, they are capable of identifying many activities of daily living (ADLs). An important field of research in this context is the monitoring of health risks and the identification of falls. It is estimated that every year, one in three persons older than 65 years will fall, and fall events are associated with high mortality rates among the elderly. We propose an anomaly identification framework to detect falls, which incorporates a spatial-temporal convolutional graph network (ST-GCN) as a feature extractor and uses an encoder process to reconstruct ADLs and identify falls as anomalies. As the publicly available fall datasets are few and generally unbalanced, training a reliable model using approaches that need explicit labeling is challenging. Thus, a focus on learning without external supervision is desirable. Treating a fall as an exception of ADLs allows us to recognize falls as anomalies without explicit labels. Given its modular architecture, our framework can robustly represent visual information and use the encoder's reconstruction error to identify falls as anomalies. We assess our framework's ability to recognize falls by training it with only ADLs. We perform three types of experiments: single dataset training and evaluation that consists of separate 90% of the data to train the model 5% to adjust the model, and the rest to the test. A joint dataset experiment, where we combine two datasets to increase the number of samples our model is trained on, and a cross-dataset evaluation, where we train on one dataset and evaluate using another one. Besides presenting state-of-the-art results on our experiments, particularly on the cross-dataset one, the model also presents a low number of false events, which makes it an ideal candidate for real-world application.",health
10.1109/ICPR48806.2021.9412907,to_check,2020 25th International Conference on Pattern Recognition (ICPR),IEEE,2021-01-15 00:00:00,ieeexplore,Improving Model Accuracy for Imbalanced Image Classification Tasks by Adding a Final Batch Normalization Layer: An Empirical Study,https://ieeexplore.ieee.org/document/9412907/,"Some real-world domains, such as Agriculture and Healthcare, comprise early-stage disease indications whose recording constitutes a rare event, and yet, whose precise detection at that stage is critical. In this type of highly imbalanced classification problems, which encompass complex features, deep learning (DL) is much needed because of its strong detection capabilities. At the same time, DL is observed in practice to favor majority over minority classes and consequently suffer from inaccurate detection of the targeted early-stage indications. To simulate such scenarios, we artificially generate skewness (99% vs. 1%) for certain plant types out of the PlantVillage dataset as a basis for classification of scarce visual cues through transfer learning. By randomly and unevenly picking healthy and unhealthy samples from certain plant types to form a training set, we consider a base experiment as fine-tuning ResNet34 and VGG19 architectures and then testing the model performance on a balanced dataset of healthy and unhealthy images. We empirically observe that the initial F1 test score jumps from 0.29 to 0.95 for the minority class upon adding a final Batch Normalization (BN) layer just before the output layer in VGG19. We demonstrate that utilizing an additional BN layer before the output layer in modern CNN architectures has a considerable impact in terms of minimizing the training time and testing error for minority classes in highly imbalanced data sets. Moreover, when the final BN is employed, minimizing the loss function may not be the best way to assure a high F1 test score for minority classes in such problems. That is, the network might perform better even if it is not `confident' enough while making a prediction; leading to another discussion about why softmax output is not a good uncertainty measure for DL models. We also report on the corroboration of these findings on the ISIC Skin Cancer as well as the Wall Crack datasets.",health
10.23919/FRUCT49677.2020.9211072,to_check,2020 27th Conference of Open Innovations Association (FRUCT),IEEE,2020-09-09 00:00:00,ieeexplore,Speech Enhancement Using Dilated Wave-U-Net: an Experimental Analysis,https://ieeexplore.ieee.org/document/9211072/,"Speech enhancement is a relevant component in many real-world applications such as hearing aid devices, mobile telecommunications, and healthcare applications. In this paper, we investigate on the Dilated Wave-U-Net model: a recently proposed end-to-end neural speech enhancement approach based on the Wave-U-Net architecture. We evaluate the performance of the model on two datasets: the public VCTK dataset, and a contaminated version of Librispeech dataset. In particular, we experiment on using alternative losses based on the MSE loss, L1 norm and on a combination of L1 and MSE losses. Results show that the Dilated Wave-U-Net architecture outperforms other state-of-the-art methods in terms of intelligibility and quality metrics on both datasets and that MSE loss is the most performing one.",health
10.1109/TCYB.2019.2909925,to_check,IEEE Transactions on Cybernetics,IEEE,2021-02-01 00:00:00,ieeexplore,Automatic Construction of Chinese Herbal Prescriptions From Tongue Images Using CNNs and Auxiliary Latent Therapy Topics,https://ieeexplore.ieee.org/document/8705645/,"The tongue image provides important physical information of humans. It is of great importance for diagnoses and treatments in clinical medicine. Herbal prescriptions are simple, noninvasive, and have low side effects. Thus, they are widely applied in China. Studies on the automatic construction technology of herbal prescriptions based on tongue images have great significance for deep learning to explore the relevance of tongue images for herbal prescriptions, it can be applied to healthcare services in mobile medical systems. In order to adapt to the tongue image in a variety of photographic environments and construct herbal prescriptions, a neural network framework for prescription construction is designed. It includes single/double convolution channels and fully connected layers. Furthermore, it proposes the auxiliary therapy topic loss mechanism to model the therapy of Chinese doctors and alleviate the interference of sparse output labels on the diversity of results. The experiment use the real-world tongue images and the corresponding prescriptions and the results can generate prescriptions that are close to the real samples, which verifies the feasibility of the proposed method for the automatic construction of herbal prescriptions from tongue images. Also, it provides a reference for automatic herbal prescription construction from more physical information.",health
10.1109/TITB.2007.900808,to_check,IEEE Transactions on Information Technology in Biomedicine,IEEE,2008-07-01 00:00:00,ieeexplore,Mining Unexpected Temporal Associations: Applications in Detecting Adverse Drug Reactions,https://ieeexplore.ieee.org/document/4358892/,"In various real-world applications, it is very useful mining unanticipated episodes where certain event patterns unexpectedly lead to outcomes, e.g., taking two medicines together sometimes causing an adverse reaction. These unanticipated episodes are usually unexpected and infrequent, which makes existing data mining techniques, mainly designed to find frequent patterns, ineffective. In this paper, we propose unexpected temporal association rules (UTARs) to describe them. To handle the unexpectedness, we introduce a new interestingness measure, residual-leverage, and develop a novel case-based exclusion technique for its calculation. Combining it with an event-oriented data preparation technique to handle the infrequency, we develop a new algorithm MUTARC to find pairwise UTARs. The MUTARC is applied to generate adverse drug reaction (ADR) signals from real-world healthcare administrative databases. It reliably shortlists not only six known ADRs, but also another ADR, flucloxacillin possibly causing hepatitis, which our algorithm designers and experiment runners have not known before the experiments. The MUTARC performs much more effectively than existing techniques. This paper clearly illustrates the great potential along the new direction of ADR signal generation from healthcare administrative databases.",health
10.1109/ICDM.2002.1184022,to_check,"2002 IEEE International Conference on Data Mining, 2002. Proceedings.",IEEE,2002-12-12 00:00:00,ieeexplore,Improving medical/biological data classification performance by wavelet preprocessing,https://ieeexplore.ieee.org/document/1184022/,"Many real-world datasets contain noise which could degrade the performances of learning algorithms. Motivated from the success of wavelet denoising techniques in image data, we explore a general solution to alleviate the effect of noisy data by wavelet preprocessing for medical/biological data classification. Our experiments are divided into two categories: one is of different classification algorithms on a specific database, and the other is of a specific classification algorithm (decision tree) on different databases. The experiment results show that the wavelet denoising of noisy data is able to improve the accuracies of those classification methods, if the localities of the attributes are strong enough.",health
10.1109/IC2E48712.2020.00017,to_check,2020 IEEE International Conference on Cloud Engineering (IC2E),IEEE,2020-04-24 00:00:00,ieeexplore,Realising Edge Analytics for Early Prediction of Readmission: A Case Study,https://ieeexplore.ieee.org/document/9096429/,"The post-discharge support is increasingly suggested for stroke patients to be discharged earlier and start rehabilitation at home. Considering that stroke patients usually have a high chance of recurrence, a good prognostic program is essential to improve diagnostic capabilities while reducing readmission rate to further save medical sources. In this context, various machine learning methods have been leveraged to obtain diagnostic findings and guide further treatments. However, those approaches mainly focus on performing analysis using a single data source obtained from the hospital, which could ignore the information complementarity between different groups of features and several subtle and discrete differences of physical interpretation among them. In this paper, we propose an Edge-based system design for post-stroke surveillance and warning prediction, called PSMART (Post-Stroke Mobile Auxiliary Rudiment Treatment), for processing enriched pathogenic factors of ischemic stroke from multi-sensors (views) to make readmission warning predictions. Our approach can considerably enrich the distinctive features from raw data, as well as exploit the consistency and complementary proprieties of different views, leading to better learning results. We evaluate the performance of the proposed approach on a real-world dataset, and the accuracy can reach up to 98.98%. Moreover, experiment results also show that our proposed approach can provide better accuracy when compared to the single-view ones.",health
10.1109/CVPRW.2008.4563020,to_check,2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,IEEE,2008-06-28 00:00:00,ieeexplore,Stratified regularity measures with Jensen-Shannon divergence,https://ieeexplore.ieee.org/document/4563020/,"This paper proposes a stratified regularity measure: a novel entropic measure to describe data regularity as a function of data domain stratification. Jensen-Shannon divergence is used to compute a set-similarity of intensity distributions derived from stratified data. We prove that derived regularity measures form a continuum as a function of the stratificationpsilas granularity and also upper-bounded by the Shannon entropy. This enables to interpret it as a generalized Shannon entropy with an intuitive spatial parameterization. This measure is applied as a novel feature extraction method for a real-world medical image analysis problem. The proposed measure is employed to describe ground-glass lung nodules whose shape and intensity distribution tend to be more irregular than typical lung nodules. Derived descriptors are then incorporated into a machine learning-based computer-aided detection system. Our ROC experiment resulted in 83% success rate with 5 false positives per patient, demonstrating an advantage of our approach toward solving this clinically significant problem.",health
10.1109/TSC.2018.2868750,to_check,IEEE Transactions on Services Computing,IEEE,2021-08-01 00:00:00,ieeexplore,PDLM: Privacy-Preserving Deep Learning Model on Cloud with Multiple Keys,https://ieeexplore.ieee.org/document/8454831/,"Deep learning has aroused a lot of attention and has been used successfully in many domains, such as accurate image recognition and medical diagnosis. Generally, the training of models requires large, representative datasets, which may be collected from a large number of users and contain sensitive information (e.g., users' photos and medical information). The collected data would be stored and computed by service providers (SPs) or delegated to an untrusted cloud. The users can neither control how it will be used, nor realize what will be learned from it, which make the privacy issues prominent and severe. To solve the privacy issues, one of the most popular approaches is to encrypt users' data with their public keys. However, this technique inevitably leads to another challenge that how to train the model based on multi-key encrypted data. In this paper, we propose a novel privacy-preserving deep learning model, namely PDLM, to apply deep learning over the encrypted data under multiple keys. In PDLM, lots of users contribute their encrypted data to SP to learn a specific model. We adopt an effective privacy-preserving calculation toolkit to achieve the training process based on stochastic gradient descent (SGD) in a privacy-preserving manner. We also prove that our PDLM can achieve users' privacy preservation and analyze the efficiency of PDLM in theory. Finally, we conduct an experiment to evaluate PDLM over two real-world datasets and empirical results demonstrate that our PDLM can effectively and efficiently train the model in a privacy-preserving way.",health
10.1109/ICHI52183.2021.00046,to_check,2021 IEEE 9th International Conference on Healthcare Informatics (ICHI),IEEE,2021-08-12 00:00:00,ieeexplore,Can recurrent models know more than we do?,https://ieeexplore.ieee.org/document/9565803/,"Model interpretation is an active research area, aiming to unravel the black box of deep learning models. One common approach, saliency, leverages the gradients of the model to produce a per-input map highlighting the features most important for a correct prediction. However, saliency faces challenges in recurrent models due to the “vanishing saliency” problem: gradients decay significantly towards earlier time steps. We alleviate this problem and improve the quality of saliency maps by augmenting recurrent models with an attention mechanism. We validate our methodology on synthetic data and compare these results to previous work. This synthetic experiment quantitatively validate that our methodology effectively captures the underlying signal of the input data. To show that our work is valid in a real-world setting, we apply it to functional magnetic resonance imaging (fMRI) data consisting of individuals with and without a diagnosis of schizophrenia. fMRI is notoriously complicated and a perfect candidate to show that our method works even for complex, high-dimensional data. Specifically, we use our methodology to find the relevant temporal information of the subjects and connect our findings to current and past research.",health
10.1109/JBHI.2020.2974425,to_check,IEEE Journal of Biomedical and Health Informatics,IEEE,2020-09-01 00:00:00,ieeexplore,Inaccurate Labels in Weakly-Supervised Deep Learning: Automatic Identification and Correction and Their Impact on Classification Performance,https://ieeexplore.ieee.org/document/9000595/,"In data-driven deep learning-based modeling, data quality may substantially influence classification performance. Correct data labeling for deep learning modeling is critical. In weakly-supervised learning, a challenge lies in dealing with potentially inaccurate or mislabeled training data. In this paper, we proposed an automated methodological framework to identify mislabeled data using two metric functions, namely, Cross-entropy Loss that indicates divergence between a prediction and ground truth, and Influence function that reflects the dependence of a model on data. After correcting the identified mislabels, we measured their impact on the classification performance. We also compared the mislabeling effects in three experiments on two different real-world clinical questions. A total of 10,500 images were studied in the contexts of clinical breast density category classification and breast cancer malignancy diagnosis. We used intentionally flipped labels as mislabels to evaluate the proposed method at a varying proportion of mislabeled data included in model training. We also compared the effects of our method to two published schemes for breast density category classification. Experiment results show that when the dataset contains 10% of mislabeled data, our method can automatically identify up to 98% of these mislabeled data by examining/checking the top 30% of the full dataset. Furthermore, we show that correcting the identified mislabels leads to an improvement in the classification performance. Our method provides a feasible solution for weakly-supervised deep learning modeling in dealing with inaccurate labels.",health
10.1016/j.bspc.2018.05.013,to_check,Biomedical Signal Processing and Control,scopus,2018-08-01,sciencedirect,Multiple-feature-branch convolutional neural network for myocardial infarction diagnosis using electrocardiogram,https://api.elsevier.com/content/abstract/scopus_id/85047257304,"Generally, 12-lead electrocardiogram (ECG) is widely used in MI diagnosis. It has two unique attributes namely integrity and diversity. But most of the previous studies on automated MI diagnosis algorithm didn’t utilize these two attributes simultaneously. In this paper, a novel Multiple-Feature-Branch Convolutional Neural Network (MFB-CNN) is proposed for automated MI detection and localization using ECG. Each independent feature branch of the MFB-CNN corresponds to a certain lead. Individual features of a lead can be learned by a feature branch, exploiting the diversity among the 12 leads. Global fully-connected softmax layer can exploit the integrity, summarizing all the feature branches. Based on deep learning framework, no hand-designed features are required for analysis. Furthermore, patient-specific paradigm is adopted to manage the inter-patient variability, which is a significant challenge for automated diagnosis. Also, class-based experiment (regardless of the inter-patient variability) is performed. The proposed algorithm is evaluated using the ECG data from PTB diagnostic database. It can achieve a good performance in MI diagnosis. For class-based MI detection and localization, the average accuracies are up to 99.95% and 99.81%, respectively; for patient-specific experiment, the average accuracies of MI detection and localization are 98.79% and 94.82%, respectively. Considering its excellent performance, the MFB-CNN can be applied to computer-aided diagnosis platform to assist the real-world MI detection and localization.",health
10.1109/MDM.2019.00-49,to_check,2019 20th IEEE International Conference on Mobile Data Management (MDM),IEEE,2019-06-13 00:00:00,ieeexplore,Synchronization-Free GPS Spoofing Detection with Crowdsourced Air Traffic Control Data,https://ieeexplore.ieee.org/document/8788766/,"GPS-dependent localization, navigation and air traffic control (ATC) applications have had a significant impact on the modern aviation industry. However, the lack of encryption and authentication makes GPS vulnerable to spoofing attacks with the purpose of hijacking aerial vehicles or threatening air safety. In this paper, we propose GPS-Probe, a GPS spoofing detection algorithm that leverages the ATC messages that are periodically broadcasted by aerial vehicles. By continuously analyzing the received signal strength indicator (RSSI) and the timestamps at server (TSS) of the ATC messages, which are monitored by multiple ground sensors, GPS-Probe constructs a machine learning enabled framework to estimate the real position of the target aerial vehicle and to detect whether or not the position data is compromised by GPS spoofing attacks. Unlike existing techniques, GPS-Probe neither requires any updates of the GPS infrastructure nor updates of the GPS receivers. More importantly, it releases the requirement on time synchronization of the ground sensors distributed around the world. Using the real-world ATC data crowdsourced by the OpenSky Network, our experiment results show that GPS-Probe can achieve the detection accuracy and precision, of 81.7% and 85.3% respectively on average, and up to 89.7% and 91.5% respectively at the best.",industry
10.1109/TSP.2014.2339799,to_check,IEEE Transactions on Signal Processing,IEEE,2014-09-01 00:00:00,ieeexplore,Evolutionary Dynamics of Information Diffusion Over Social Networks,https://ieeexplore.ieee.org/document/6856208/,"Current social networks are of extremely large-scale generating tremendous information flows at every moment. How information diffuses over social networks has attracted much attention from both industry and academics. Most of the existing works on information diffusion analysis are based on machine learning methods focusing on social network structure analysis and empirical data mining. However, the network users' decisions, actions, and socio-economic interactions are generally ignored by most of existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks. Specifically, we derive the information diffusion dynamics in complete networks, uniform degree, and nonuniform degree networks, with the highlight of two special networks, the Erdös-Rényi random network and the Barabási-Albert scale-free network. We find that the dynamics of information diffusion over these three kinds of networks are scale-free and all the three dynamics are same with each other when the network scale is sufficiently large. To verify our theoretical analysis, we perform simulations for the information diffusion over synthetic networks and real-world Facebook networks. Moreover, we also conduct an experiment on a Twitter hashtags dataset, which shows that the proposed game theoretic model can well fit and predict the information diffusion over real social networks.",industry
10.1109/ACCESS.2020.2973336,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,An Effective Discrete Artificial Bee Colony Algorithm for Scheduling an Automatic-Guided-Vehicle in a Linear Manufacturing Workshop,https://ieeexplore.ieee.org/document/8995549/,"This paper deals with a new automatic guided vehicle (AGV) scheduling problem from the material handling process in a linear manufacturing workshop. The problem is to determine a sequence of Cells for AGV to travel to minimize the standard deviation of the waiting time of the Cells and the total travel distance of AGV. For this purpose, we first propose an integer linear programming model based on a comprehensive investigation. Then, we present an improved nearest-neighbor-based heuristic so as to fast generate a good solution in view of the problem-specific characteristics. Next, we propose an effective discrete artificial bee colony algorithm with some novel and advanced techniques including a heuristic-based initialization, six neighborhood structures and a new evolution strategy in the onlooker bee phase. Finally, the proposed algorithms are empirically evaluated based on several typical instances from the real-world linear manufacturing workshop. A comprehensive and thorough experiment shows that the presented algorithm produces superior results which are also demonstrated to be statistically significant than the existing algorithms.",industry
10.1109/ICNSC48988.2020.9238123,to_check,"2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)",IEEE,2020-11-02 00:00:00,ieeexplore,A Novel Reinforcement-Learning-Based Approach to Scientific Workflow Scheduling,https://ieeexplore.ieee.org/document/9238123/,"Recently, the Cloud Computing paradigm is becoming increasingly popular in supporting large-scale and complex workflow applications. The workflow scheduling problem, which refers to finding the most suitable resource for each task of the workflow to meet user defined quality of service (QoS), attracts considerable research attention. Multi-objective optimization algorithms in workflow scheduling have many limitations, e.g., the encoding schemes in most existing heuristic-based scheduling algorithms require prior experts' knowledge and thus they can be ineffective when scheduling workflows upon dynamic cloud infrastructures with real-time. To address this problem, we propose a novel Reinforcement-Learning-Based algorithm to multi-workflow scheduling over IaaS clouds. The proposed algorithm aims at optimizing make-span and dwell time and is to achieve a unique set of correlated equilibrium solution. In the experiment, our algorithm is evaluated for famous scientific workflow templates and real-world industrial IaaS cloud platforms by a simulation process and we compare our algorithm to the current state-of-the-art heuristic algorithms, e.g., NSGA-II, MOPSO, GTBGA. The result shows that our algorithm performs better than compared algorithm.",industry
10.1109/ICASSP.2019.8683088,to_check,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2019-05-17 00:00:00,ieeexplore,Learning Discriminative Features in Sequence Training without Requiring Framewise Labelled Data,https://ieeexplore.ieee.org/document/8683088/,"In this work, we try to answer two questions: Can deeply learned features with discriminative power benefit an ASR system's robustness to acoustic variability? And how to learn them without requiring framewise labelled sequence training data? As existing methods usually require knowing where the labels occur in the input sequence, they have so far been limited to many real-world sequence learning tasks. We propose a novel method which simultaneously models both the sequence discriminative training and the feature discriminative learning within a single network architecture, so that it can learn discriminative deep features in sequence training that obviates the need for presegmented training data. Our experiment in a realistic industrial ASR task shows that, without requiring any specific fine-tuning or additional complexity, our proposed models have consistently outperformed state-of-the-art models and significantly reduced Word Error Rate (WER) under all test conditions, and especially with highest improvements under unseen noise conditions, by relative 12.94%, 8.66% and 5.80%, showing our proposed models can generalize better to acoustic variability.",industry
http://arxiv.org/abs/2002.01711v4,to_check,arxiv,arxiv,2020-02-05 10:25:02+00:00,arxiv,"A Reinforcement Learning Framework for Time-Dependent Causal Effects
  Evaluation in A/B Testing",http://arxiv.org/abs/2002.01711v4,"A/B testing, or online experiment is a standard business strategy to compare
a new product with an old one in pharmaceutical, technological, and traditional
industries. Major challenges arise in online experiments where there is only
one unit that receives a sequence of treatments over time. In those
experiments, the treatment at a given time impacts current outcome as well as
future outcomes. The aim of this paper is to introduce a reinforcement learning
framework for carrying A/B testing, while characterizing the long-term
treatment effects. Our proposed testing procedure allows for sequential
monitoring and online updating, so it is generally applicable to a variety of
treatment designs in different industries. In addition, we systematically
investigate the theoretical properties (e.g., asymptotic distribution and
power) of our testing procedure. Finally, we apply our framework to both
synthetic datasets and a real-world data example obtained from a ride-sharing
company to illustrate its usefulness.",industry
http://arxiv.org/abs/2007.09712v1,to_check,arxiv,arxiv,2020-07-19 16:47:26+00:00,arxiv,"Deep Anomaly Detection for Time-series Data in Industrial IoT: A
  Communication-Efficient On-device Federated Learning Approach",http://arxiv.org/abs/2007.09712v1,"Since edge device failures (i.e., anomalies) seriously affect the production
of industrial products in Industrial IoT (IIoT), accurately and timely
detecting anomalies is becoming increasingly important. Furthermore, data
collected by the edge device may contain the user's private data, which is
challenging the current detection approaches as user privacy is calling for the
public concern in recent years. With this focus, this paper proposes a new
communication-efficient on-device federated learning (FL)-based deep anomaly
detection framework for sensing time-series data in IIoT. Specifically, we
first introduce a FL framework to enable decentralized edge devices to
collaboratively train an anomaly detection model, which can improve its
generalization ability. Second, we propose an Attention Mechanism-based
Convolutional Neural Network-Long Short Term Memory (AMCNN-LSTM) model to
accurately detect anomalies. The AMCNN-LSTM model uses attention
mechanism-based CNN units to capture important fine-grained features, thereby
preventing memory loss and gradient dispersion problems. Furthermore, this
model retains the advantages of LSTM unit in predicting time series data.
Third, to adapt the proposed framework to the timeliness of industrial anomaly
detection, we propose a gradient compression mechanism based on Top-\textit{k}
selection to improve communication efficiency. Extensive experiment studies on
four real-world datasets demonstrate that the proposed framework can accurately
and timely detect anomalies and also reduce the communication overhead by 50\%
compared to the federated learning framework that does not use a gradient
compression scheme.",industry
http://arxiv.org/abs/1711.05098v1,to_check,arxiv,arxiv,2017-11-14 14:20:56+00:00,arxiv,Web Robot Detection in Academic Publishing,http://arxiv.org/abs/1711.05098v1,"Recent industry reports assure the rise of web robots which comprise more
than half of the total web traffic. They not only threaten the security,
privacy and efficiency of the web but they also distort analytics and metrics,
doubting the veracity of the information being promoted. In the academic
publishing domain, this can cause articles to be faulty presented as prominent
and influential. In this paper, we present our approach on detecting web robots
in academic publishing websites. We use different supervised learning
algorithms with a variety of characteristics deriving from both the log files
of the server and the content served by the website. Our approach relies on the
assumption that human users will be interested in specific domains or articles,
while web robots crawl a web library incoherently. We experiment with features
adopted in previous studies with the addition of novel semantic characteristics
which derive after performing a semantic analysis using the Latent Dirichlet
Allocation (LDA) algorithm. Our real-world case study shows promising results,
pinpointing the significance of semantic features in the web robot detection
problem.",industry
http://arxiv.org/abs/1312.0317v1,to_check,arxiv,arxiv,2013-12-02 03:21:28+00:00,arxiv,Evolutionary Dynamics of Information Diffusion over Social Networks,http://arxiv.org/abs/1312.0317v1,"Current social networks are of extremely large-scale generating tremendous
information flows at every moment. How information diffuse over social networks
has attracted much attention from both industry and academics. Most of the
existing works on information diffusion analysis are based on machine learning
methods focusing on social network structure analysis and empirical data
mining. However, the dynamics of information diffusion, which are heavily
influenced by network users' decisions, actions and their socio-economic
interactions, is generally ignored by most of existing works. In this paper, we
propose an evolutionary game theoretic framework to model the dynamic
information diffusion process in social networks. Specifically, we derive the
information diffusion dynamics in complete networks, uniform degree and
non-uniform degree networks, with the highlight of two special networks,
Erd\H{o}s-R\'enyi random network and the Barab\'asi-Albert scale-free network.
We find that the dynamics of information diffusion over these three kinds of
networks are scale-free and the same with each other when the network scale is
sufficiently large. To verify our theoretical analysis, we perform simulations
for the information diffusion over synthetic networks and real-world Facebook
networks. Moreover, we also conduct experiment on Twitter hashtags dataset,
which shows that the proposed game theoretic model can well fit and predict the
information diffusion over real social networks.",industry
10.1016/j.cie.2021.107733,to_check,Computers and Industrial Engineering,scopus,2021-12-01,sciencedirect,SiteForge: Detecting and localizing forged images on microblogging platforms using deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85117254237,"Microblogging applications are currently used to disseminate information with concise text and images. Nevertheless, they are also the largest platform for circulating forged images. Forged images are digital photographs that have been modified to deceive or distort the information they communicate. These manipulated images posted on microblogging apps like Twitter often create biased user emotions leading to harmful consequences like religious feuds or riots. Microblogging platforms and fact-checking industry are investing in artificial intelligence solutions to detect these forged images on time. Many image forensic techniques are proposed earlier, but their effectiveness falls short on real-world images shared over microblogging sites. As forged images shared over these platforms are typically altered using multiple manipulation techniques, it is hard for forensic techniques to detect them. This paper proposes a customized convolutional neural network with an attention mechanism to spot fake images shared over microblogging platforms. Deep learning convolutional networks learn the intrinsic feature set of images and can detect the forged images. To handle multiple manipulations in an image, the applied attention mechanism focuses on the most relevant image region to learn the inherent feature sets. The model utilizes High-pass filters from the image processing domain to initialize kernel weights of the neural network. This supports the proposed model to converge faster and achieve better accuracy. The pooling layers are designed to specifically handle images from microblogging sites. The solution is universal and can detect complex tampering scenarios like text-editing, face-swapping, copy-move, splicing and mirroring. Local Interpretable Model-agnostic Explanations (LIME) is utilized to localize the manipulated region in a forged image. LIME also adds interpretability and confidence to the proposed model, a common concern in deep learning models. The model is verified against the publicly available CASIA 2.0 dataset. An accuracy score of 94.7% is achieved, which is better than the previous state-of-art papers in fake image detection. In order to test the model on real-world images published on Twitter, a recent dataset is built from an Indian viewpoint. The model achieves a modest accuracy of 83.2% over the real-world Twitter dataset. The experiment proves that the proposed model can accurately detect the forged images over social platforms. It can be utilized in the fact-checking field to improve manual efforts. It will also support manual fact-checkers in swift decision making.",industry
10.1016/j.cie.2019.106246,to_check,Computers and Industrial Engineering,scopus,2020-02-01,sciencedirect,A Parallel Gated Recurrent Units (P-GRUs) network for the shifting lateness bottleneck prediction in make-to-order production system,https://api.elsevier.com/content/abstract/scopus_id/85077469749,"In the make-to-order production system, the lateness bottleneck is the constraint of just-in-time management and orders on-time delivery. Since the dynamic nature of the manufacturing system, the bottleneck frequently shifts and influences the stability during the production runs. Therefore, predicting the bottleneck allows operators to foresee the future production status and to make proactive decision towards a balanced-line. Based on the large volumes of manufacturing data collected by Internet of Things (IoT), a novel Parallel gated recurrent units (P-GRUs) network with main inputs and auxiliary inputs are particularly developed for shifting bottleneck prediction. The designed P-GRUs can capture the temporal correlations of shifting bottlenecks and depict the production status simultaneously to make accurate bottleneck prediction. The P-GRUs model is applied in a large-scale production system to validate the performance and demonstrate the practical impacts. Finally, the experiment results from both real-world production as well as simulation environment show that the P-GRUs model yields better performance than benchmark models, including Autoregressive integrated moving average model (ARIMA), vanilla Recurrent nueral network (RNN), Deep neural network (DNN), and regular GRUs network.",industry
10.1016/j.eswa.2015.12.027,to_check,Expert Systems with Applications,scopus,2016-06-01,sciencedirect,Semi-supervised support vector regression based on self-training with label uncertainty: An application to virtual metrology in semiconductor manufacturing,https://api.elsevier.com/content/abstract/scopus_id/84955137019,"Dataset size continues to increase and data are being collected from numerous applications. Because collecting labeled data is expensive and time consuming, the amount of unlabeled data is increasing. Semi-supervised learning (SSL) has been proposed to improve conventional supervised learning methods by training from both unlabeled and labeled data. In contrast to classification problems, the estimation of labels for unlabeled data presents added uncertainty for regression problems. In this paper, a semi-supervised support vector regression (SS-SVR) method based on self-training is proposed. The proposed method addresses the uncertainty of the estimated labels for unlabeled data. To measure labeling uncertainty, the label distribution of the unlabeled data is estimated with two probabilistic local reconstruction (PLR) models. Then, the training data are generated by oversampling from the unlabeled data and their estimated label distribution. The sampling rate is different based on uncertainty. Finally, expected margin-based pattern selection (EMPS) is employed to reduce training complexity. We verify the proposed method with 30 regression datasets and a real-world problem: virtual metrology (VM) in semiconductor manufacturing. The experiment results show that the proposed method improves the accuracy by 8% compared with conventional supervised SVR, and the training time for the proposed method is 20% shorter than that of the benchmark methods.",industry
10.1109/INFOCOM42981.2021.9488713,to_check,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,IEEE,2021-05-13 00:00:00,ieeexplore,Multi-Agent Reinforcement Learning for Urban Crowd Sensing with For-Hire Vehicles,https://ieeexplore.ieee.org/document/9488713/,"Recently, vehicular crowd sensing (VCS) that leverages sensor-equipped urban vehicles to collect city-scale sensory data has emerged as a promising paradigm for urban sensing. Nowadays, a wide spectrum of VCS tasks are carried out by for-hire vehicles (FHVs) due to various hardware and software constraints that are difficult for private vehicles to satisfy. However, such FHV-enabled VCS systems face a fundamental yet unsolved problem of striking a balance between the order-serving and sensing outcomes. To address this problem, we propose a novel graph convolutional cooperative multi-agent reinforcement learning (GCC-MARL) framework, which helps FHVs make distributed routing decisions that cooperatively optimize the system-wide global objective. Specifically, GCC-MARL meticulously assigns credits to agents in the training process to effectively stimulate cooperation, represents agents' actions by a carefully chosen statistics to cope with the variable agent scales, and integrates graph convolution to capture useful spatial features from complex large-scale urban road networks. We conduct extensive experiments with a real-world dataset collected in Shenzhen, China, containing around 1 million trajectories and 50 thousand orders of 553 taxis per-day from June 1st to 30th, 2017. Our experiment results show that GCC-MARL outperforms state-of-the-art baseline methods in order-serving revenue, as well as sensing coverage and quality.",smart cities
10.1109/IGARSS.2019.8898701,to_check,IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium,IEEE,2019-08-02 00:00:00,ieeexplore,Partial 3D Object Retrieval and Completeness Evaluation for Urban Street Scene,https://ieeexplore.ieee.org/document/8898701/,"3D objects detected from real-world data are usually incomplete in different degrees. Objects with different degrees of incompleteness should be treated and processed separately. This paper proposes a framework for partial 3D object retrieval and completeness evaluation in an urban street scene based on mobile laser scanning (MLS) point cloud data. The framework consists of three parts. A deep learning method is first used to detect objects from 3D point cloud data. Then, for each detected object, the most similar object in the reference dataset, which contains complete objects, is obtained by a partial 3D shape retrieval method. Last, a completeness evaluation of the detected object is conducted by calculating the completeness index that reflects the integrity of the detected object, and a missing part prediction is given to guide further completion. The proposed framework is validated on the public dataset KITTI and our own point cloud dataset. The experiment includes 3D detection, the partial 3D shape retrieval, and the completeness evaluation. Results show the good performance of the object detection and partial shape retrieval, also a reasonable evaluation of objects completeness.",smart cities
10.1109/ICNNSP.2003.1281074,to_check,"International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003",IEEE,2003-12-17 00:00:00,ieeexplore,Using object classification to improve urban traffic monitoring system,https://ieeexplore.ieee.org/document/1281074/,"This paper presents an algorithm for classifying moving objects in real-world traffic scenes. Spatial and Temporal information provided by region segmenting and tracking is used for moving object classification. In order to achieve real-time requirement, the proposed approach uses the classification metrics that are computationally inexpensive and makes use of simplifying assumption that there are two kinds of objects: vehicle(including motorcycle, car, bus, and truck) and human (including the pedestrian and bicycler). Using the classification statistics, we successfully reduces the occlusion effect. The experiment results show that the object classification algorithm can obviously improve the performance of urban traffic monitoring system, such as, the accuracy of vehicles counting and average speed measuring, and rarely degrades the system processing speed.",smart cities
10.1109/PDCAT.2017.00039,to_check,"2017 18th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)",IEEE,2017-12-20 00:00:00,ieeexplore,A Deep Learning Based Framework for Power Demand Forecasting with Deep Belief Networks,https://ieeexplore.ieee.org/document/8327087/,"Power demand forecasting plays a very important role in many electricity-required industries, such as modern high-speed railways or urban railways. Accurate forecasting will guarantee that electrical equipments such as electric traction systems for trains work under safe, robust and efficient status. Recently, many studies adopt the learning-based methods to achieve the prediction of power demand. However, most of the studies use the traditional classification or clustering algorithms which may not satisfy the requirements of accuracy and efficiency due to the complex features in smart grid. In this paper, we focus on solving the power demand forecasting problem based on deep learning structures. We first propose a deep learning based framework for power demand forecasting with Deep Belief Network (DBN). Then, we use an algorithm called Adaboost to combine weak learners with strong learners, which can increase the accuracy significantly in real-world scenarios. The prediction of the load status is realized by analyzing the information of historical distribution transformer load, weather, electricity population and some other related information. It is also worth noting that the training process of these DBN networks can be parallel, which effectively shorten the processing time and provide the possible of real-time predicting. Our experiment on real-world data from the electrical company shows results that the deep leaning based methods can increase the accuracy of forecasting and significantly shorten the prediction time.",smart cities
10.1109/ITSC48978.2021.9564544,to_check,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),IEEE,2021-09-22 00:00:00,ieeexplore,Mining the Graph Representation of Traffic Speed Data for Graph Convolutional Neural Network,https://ieeexplore.ieee.org/document/9564544/,"Deep learning algorithms are considered as the best-fit methods to deal with spatial-temporal attributes of short-term traffic predictions in recent years. Further, the graph-based Graph Convolutional Network (GCN) models are widely used to handle the spatial dependence of roads in urban networks. This paper aims to explore the spatial graph representation of urban networks for GCN models. Specifically, a data-driven spatial graph representation scheme is established to measure the complex non-linear relationships among roads, together with the local and non-local impacts of urban traffics. This spatial graph representation is then combined with Sequence to Sequence structure to present a multi-input and multi-output network-wide traffic prediction model (SGDE-S2S model). A sensitive test is carried out to select the optimal threshold value of the most relevant roads to every target road. Then the SGDE-S2S model and some other baseline models are tested on real-world traffic speed data of Chengdu, China. The experiment results confirm that the SGDE-S2S model can well capture the intrinsic relationships of roads without the need for topological adjacent information and performs the best in all multi-step predictions.",smart cities
10.1109/ACCESS.2019.2917286,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,A Method to Mine Movement Patterns Between Zones: A Case Study of Subway Commuters in Shanghai,https://ieeexplore.ieee.org/document/8717986/,"As identifying people's movements across zones can improve our understanding of transportation patterns and recommend strategies for urban planning such as precise locations for targeted advertisements, residential zoning, and transportation development. However, when the amount of data is large or the relationships between data are complex, traditional algorithms for movement patterns between zones become ineffective. We propose a new agglomeration algorithm, namely, the density-based movement patterns between zones (DBMPZ), to mine spatial clustering of movement patterns. To validate the proposed algorithm, we use a real-world dataset of subway commuters in Shanghai and some synthetic datasets to identify movement patterns between zones. The experiment results show that the proposed algorithm can effectively mine movement patterns between zones with high precision, effectiveness, and efficiency. In addition, the proposed algorithm can also play an important role in other regions or types of transportation dataset by modifying the clustering procedure.",smart cities
10.1109/TKDE.2016.2550436,to_check,IEEE Transactions on Knowledge and Data Engineering,IEEE,2016-08-01 00:00:00,ieeexplore,A General Multi-Context Embedding Model for Mining Human Trajectory Data,https://ieeexplore.ieee.org/document/7447767/,"The proliferation of location-based social networks, such as Foursquare and Facebook Places, offers a variety of ways to record human mobility, including user generated geo-tagged contents, check-in services, and mobile apps. Although trajectory data is of great value to many applications, it is challenging to analyze and mine trajectory data due to the complex characteristics reflected in human mobility, which is affected by multiple contextual information. In this paper, we propose a Multi-Context Trajectory Embedding Model, called MC-TEM, to explore contexts in a systematic way. MC-TEM is developed in the distributed representation learning framework, and it is flexible to characterize various kinds of useful contexts for different applications. To the best of our knowledge, it is the first time that the distributed representation learning methods apply to trajectory data. We formally incorporate multiple context information of trajectory data into the proposed model, including user-level, trajectory-level, location-level, and temporal contexts. All the context information is represented in the same embedding space. We apply MC-TEM to two challenging tasks, namely location recommendation and social link prediction. We conduct extensive experiments on three real-world datasets. Extensive experiment results have demonstrated the superiority of our MC-TEM model over several state-of-the-art methods.",smart cities
http://arxiv.org/abs/2109.05225v1,to_check,arxiv,arxiv,2021-09-11 09:04:35+00:00,arxiv,"Space Meets Time: Local Spacetime Neural Network For Traffic Flow
  Forecasting",http://arxiv.org/abs/2109.05225v1,"Traffic flow forecasting is a crucial task in urban computing. The challenge
arises as traffic flows often exhibit intrinsic and latent spatio-temporal
correlations that cannot be identified by extracting the spatial and temporal
patterns of traffic data separately. We argue that such correlations are
universal and play a pivotal role in traffic flow. We put forward spacetime
interval learning as a paradigm to explicitly capture these correlations
through a unified analysis of both spatial and temporal features. Unlike the
state-of-the-art methods, which are restricted to a particular road network, we
model the universal spatio-temporal correlations that are transferable from
cities to cities. To this end, we propose a new spacetime interval learning
framework that constructs a local-spacetime context of a traffic sensor
comprising the data from its neighbors within close time points. Based on this
idea, we introduce spacetime neural network (STNN), which employs novel
spacetime convolution and attention mechanism to learn the universal
spatio-temporal correlations. The proposed STNN captures local traffic
patterns, which does not depend on a specific network structure. As a result, a
trained STNN model can be applied on any unseen traffic networks. We evaluate
the proposed STNN on two public real-world traffic datasets and a simulated
dataset on dynamic networks. The experiment results show that STNN not only
improves prediction accuracy by 15% over state-of-the-art methods, but is also
effective in handling the case when the traffic network undergoes dynamic
changes as well as the superior generalization capability.",smart cities
10.1016/j.neucom.2021.08.073,to_check,Neurocomputing,scopus,2021-11-13,sciencedirect,Balanced distortion and perception in single-image super-resolution based on optimal transport in wavelet domain,https://api.elsevier.com/content/abstract/scopus_id/85114383530,"Single image super-resolution (SISR) is a classic ill-posed problem in computer vision. In recent years, deep-learning-based (DL-based) models have achieved promising results with the SISR problem. However, most existing methods suffer from an intrinsic trade-off between distortion and perceptual quality. To satisfy the requirements in different real-world situations, the balance of distortion and visual quality for image super-resolution is a critical issue. In DL-based models, the uses of hybrid loss (i.e., the combination of the distortion loss and the perceptual loss) and network interpolation are two common approaches to balancing the distortion and perceptual quality of super-resolved images. However, these two kinds of methods lack flexibility and hold strict constraints on network architectures. In this paper, we propose an image-fusion interpolation method for image super-resolution, which can balance the distortion and visual quality of super-resolved images, based on the optimal transport theory in the wavelet domain. The advantage of our proposed method is that it can be applied to any pretrained DL-based model, without any requirement from the network architecture and parameters. In addition, our proposed method is parameter-free and can run fast without using a GPU. Compared with existing state-of-the-art SISR methods, experiment results show that our proposed method can achieve a better balance between the distortion and visual quality in super-resolved images.",smart cities
10.1016/j.trc.2021.103156,to_check,Transportation Research Part C: Emerging Technologies,scopus,2021-06-01,sciencedirect,Efficient dispatching for on-demand ride services: Systematic optimization via Monte-Carlo tree search,https://api.elsevier.com/content/abstract/scopus_id/85105698494,"Efficient dispatching for on-demand ride services is essential to the ride-sourcing platform, passengers, and drivers in a competitive ride-sourcing market. The existing first-come-first-serve (FCFS) dispatching method is a myopic mechanism that sought to minimize individual-level waiting time. To improve the passenger-vehicle matching efficiency and reduce the cancelation rate, a searching tree structure is developed to deal with the vehicle dispatching problem considering vehicles still in service. This paper formulates a distinctive systematic dispatching model from the passengers' perspective and presents new dispatching rules to match passengers and vehicles. Based on an improved tree policy and efficient branch reduction policy, we customize and enhance the reinforcement learning approach, Monte-Carlo Tree Search (MCTS), to solve the multi-period sequential dispatching problem and enable efficient dispatches. The computational Complexity of MCTS is analytically derived. As benchmarks, the Hungarian algorithm, CPLEX, greedy algorithm, and two global optimization algorithms are employed. We compare these algorithms by using both simulation and real-world city-scale on-demand ride-sourcing data. Both numerical and city-scale experiment results show that the improved MCTS dispatching approach increases the percentages of satisfied passengers and dispatched drivers, and reduces the number of unmatched passengers and cumulative waiting time compared to the benchmark algorithms. The results shed light on modeling urban dispatching problems with available vehicles in both forward-looking and backward-looking time periods. This paper demonstrates that improved MCTS performs well in solving a multi-period sequential optimization problem in real-world city-scale applications.",smart cities
10.1016/j.trc.2020.102851,to_check,Transportation Research Part C: Emerging Technologies,scopus,2021-01-01,sciencedirect,DNEAT: A novel dynamic node-edge attention network for origin-destination demand prediction,https://api.elsevier.com/content/abstract/scopus_id/85097168126,"The ride-hailing service platforms have grown tremendously around the world and attracted a wide range of research interests. A key to ride-hailing service platforms is how to realize accurate and reliable demand prediction. However, most of the existing studies focus on the region-level demand prediction while only a few attempts to address the problem of origin–destination (OD) demand prediction. In this paper, from the graph aspects, we construct the dynamic OD graphs to describe the ride-hailing demand data. We propose a novel neural architecture named the Dynamic Node-Edge Attention Network (DNEAT) to address the unique challenges of OD demand prediction from the demand generation and attraction perspectives. Different from previous studies, in DNEAT, we develop a new neural layer, named k-hop temporal node-edge attention layer (
                        k
                     -TNEAT), to capture the temporal evolution of node topologies in dynamic OD graphs instead of the pre-defined relationships among regions. We evaluate our model on two real-world ride-hailing demand datasets (from Chengdu, China, and New York City). The experiment results show that the proposed model outperforms six baseline models and is more robust to demand data with high sparsity.",smart cities
10.1016/j.jhydrol.2018.05.003,to_check,Journal of Hydrology,scopus,2018-08-01,sciencedirect,Addressing the incorrect usage of wavelet-based hydrological and water resources forecasting models for real-world applications with best practices and a new forecasting framework,https://api.elsevier.com/content/abstract/scopus_id/85048519861,"Many recent studies propose wavelet-based hydrological and water resources forecasting models that have been incorrectly developed and that cannot properly be used for real-world forecasting problems. The incorrect development of these wavelet-based forecasting models occurs during wavelet decomposition (the process of extracting high- and low-frequency information into different sub-time series known as wavelet and scaling coefficients, respectively) and as a result introduces error into the forecast model inputs. The source of this error is due to the boundary condition that is associated with wavelet decomposition (and the wavelet and scaling coefficients) and is linked to three main issues: 1) using ‘future data’ (i.e., data from the future that is not available); 2) inappropriately selecting decomposition levels and wavelet filters; and 3) not carefully partitioning calibration and validation data. By not addressing these boundary conditions during wavelet decomposition, incorrectly developed wavelet-based forecasting models often result in much better performance than what is realistically achievable. We demonstrate that the discrete wavelet transform (DWT) multiresolution analysis (DWT-MRA) and maximal overlap discrete wavelet transform (MODWT) multiresolution analysis (MODWT-MRA), two commonly adopted wavelet decomposition methods used in the development of hydrological and water resources wavelet-based forecasting models, suffer from these boundary conditions and cannot be used properly for real-world forecasting. However, by following a proposed set of best (correct) practices, we show that the MODWT and à trous algorithm (AT) can be used to correctly forecast target (e.g., hydrological and water resources) processes in real-world scenarios. In this vein, we contribute a set of best practices, which focusses on deriving “boundary-corrected” wavelet and scaling coefficients from time series data, overcoming the boundary condition issues and providing hydrological and water resources modellers with a justified and coherent strategy for developing wavelet-based forecasting models that may be used for real-world forecasting problems. We coalesce these best practices into a new forecasting framework named Wavelet Data-Driven Forecasting Framework (WDDFF) that uses a combination of input variable selection and data-driven models to convert “boundary-corrected” wavelet and scaling coefficients into forecasts of a target process. Through a real-world urban water demand forecasting experiment in Montreal, Canada, we demonstrate the superiority of WDDFF against benchmark forecasting models such as (non-wavelet-based) random walk, multiple linear regression, extreme learning machine, and second-order Volterra series models. For the same case study, we also show how the WDDFF provides realistic and accurate forecasts while a recently proposed wavelet-based forecasting model that adopts the (invalid) MODWT-MRA for wavelet decomposition provides incorrect and unrealistic forecasts. We conclude that WDDFF is a useful tool for forecasting real-world hydrological and water resources processes that overcomes the limitations of many earlier wavelet-based forecasting methods and should be explored further for forecasting different processes such as streamflow, rainfall, evaporation, etc.",smart cities
10.1016/j.trc.2018.01.008,to_check,Transportation Research Part C: Emerging Technologies,scopus,2018-03-01,sciencedirect,Reinforcement learning approach for coordinated passenger inflow control of urban rail transit in peak hours,https://api.elsevier.com/content/abstract/scopus_id/85044656834,"In peak hours, when the limited transportation capacity of urban rail transit is not adequate enough to meet the travel demands, the density of the passengers waiting at the platform can exceed the critical density of the platform. Coordinated passenger inflow control strategy is required to adjust/meter the inflow volume and relieve some of the demand pressure at crowded metro stations so as to ensure both operational efficiency and safety at such stations for all passengers. However, such strategy is usually developed by the operation staff at each station based on their practical working experience. As such, the best strategy/decision cannot always be made and sometimes can even be highly undesirable due to their inability to account for the dynamic performance of all metro stations in the entire rail transit network. In this paper, a new reinforcement learning-based method is developed to optimize the inflow volume during a certain period of time at each station with the aim of minimizing the safety risks imposed on passengers at the metro stations. Basic principles and fundamental components of the reinforcement learning, as well as the reinforcement learning-based problem-specific algorithm are presented. The simulation experiment carried out on a real-world metro line in Shanghai is constructed to test the performance of the approach. Simulation results show that the reinforcement learning-based inflow volume control strategy is highly effective in minimizing the safety risks by reducing the frequency of passengers being stranded. Additionally, the strategy also helps to relieve the passenger congestion at certain stations.",smart cities
10.1016/j.is.2015.09.001,to_check,Information Systems,scopus,2016-04-01,sciencedirect,Labeling sensing data for mobility modeling,https://api.elsevier.com/content/abstract/scopus_id/84957559944,"In urban environments, sensory data can be used to create personalized models for predicting efficient routes and schedules on a daily basis; and also at the city level to manage and plan more efficient transport, and schedule maintenance and events. Raw sensory data is typically collected as time-stamped sequences of records, with additional activity annotations by a human, but in machine learning, predictive models view data as labeled instances, and depend upon reliable labels for learning. In real-world sensor applications, human annotations are inherently sparse and noisy. This paper presents a methodology for preprocessing sensory data for predictive modeling in particular with respect to creating reliable labeled instances. We analyze real-world scenarios and the specific problems they entail, and experiment with different approaches, showing that a relatively simple framework can ensure quality labeled data for supervised learning. We conclude the study with recommendations to practitioners and a discussion of future challenges.",smart cities
10.1016/j.procs.2015.07.305,to_check,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Building efficient probability transition matrix using machine learning from big data for personalized route prediction,https://api.elsevier.com/content/abstract/scopus_id/84939178951,"Personalized route prediction is an important technology in many applications related to intelligent vehicles and transportation systems. Current route prediction technologies used in many general navigation systems are, by and large, based on either the shortest or the fastest route selection. Personal traveling route prediction is a very challenging big data problem, as trips getting longer and variations in routes growing. It is particularly challenging for real-time in-vehicle applications, since many embedded processors have limited memory and computational power. In this paper we present a machine learning algorithm for modeling route prediction based on a Markov chain model, and a route prediction algorithm based on a probability transition matrix. We also present two data reduction algorithms, one is developed to map large GPS based trips to a compact link-based standard route representation, and another a machine learning algorithm to significantly reduce the size of a probability transition matrix. The proposed algorithms are evaluated on real-world driving trip data collected in four months, where the data collected in the first three months are used as training and the data in the fourth month are used as testing. Our experiment results show that the proposed personal route prediction system generated more than 91% prediction accuracy in average among the test trips. The data reduction algorithm gave about 8:1 reduction in link-based standard route representation and 23:1 in reducing the size of probability transition matrix.",smart cities
10.1109/VR.2017.7892238,to_check,2017 IEEE Virtual Reality (VR),IEEE,2017-03-22 00:00:00,ieeexplore,Acoustic VR in the mouth: A real-time speech-driven visual tongue system,https://ieeexplore.ieee.org/document/7892238/,"We propose an acoustic-VR system that converts acoustic signals of human language (Chinese) to realistic 3D tongue animation sequences in real time. It is known that directly capturing the 3D geometry of the tongue at a frame rate that matches the tongue's swift movement during the language production is challenging. This difficulty is handled by utilizing the electromagnetic articulography (EMA) sensor as the intermediate medium linking the acoustic data to the simulated virtual reality. We leverage Deep Neural Networks to train a model that maps the input acoustic signals to the positional information of pre-defined EMA sensors based on 1,108 utterances. Afterwards, we develop a novel reduced physics-based dynamics model for simulating the tongue's motion. Unlike the existing methods, our deformable model is nonlinear, volume-preserving, and accommodates collision between the tongue and the oral cavity (mostly with the jaw). The tongue's deformation could be highly localized which imposes extra difficulties for existing spectral model reduction methods. Alternatively, we adopt a spatial reduction method that allows an expressive subspace representation of the tongue's deformation. We systematically evaluate the simulated tongue shapes with real-world shapes acquired by MRI/CT. Our experiment demonstrates that the proposed system is able to deliver a realistic visual tongue animation corresponding to a user's speech signal.",multimedia
10.1109/IWCMC51323.2021.9498886,to_check,2021 International Wireless Communications and Mobile Computing (IWCMC),IEEE,2021-07-02 00:00:00,ieeexplore,Residual Transformer Network for 3D Objects Classification,https://ieeexplore.ieee.org/document/9498886/,"Three-dimensional point cloud is an efficient and flexible representation of three-dimensional structures. Recently, neural network algorithm has shown superior performance in the tasks of three-dimensional point cloud classification and segmentation. However, the good results shown in these tasks were experimentally obtained using complete and aligned 3D point cloud data, whereas real-world 3D point cloud data is missing and unaligned. The key to the difficulty of learning non- aligned point cloud data is how to obtain invariance of geometric transformation. To solve this problem, we propose a new spatial transformation network named residual space transformation network to process point cloud data. Different from the existing spatial transformation networks, the network is inspired by traditional image and point cloud alignment algorithms and residual learning, and predicts the three-dimensional space transformation through a multi-step estimation method. The experiment shows that compared with the baseline, the network performs better in the classification and attitude prediction of missing point cloud data.",multimedia
10.1109/ICRA48506.2021.9561572,to_check,2021 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2021-06-05 00:00:00,ieeexplore,Unsupervised Learning of 3D Scene Flow from Monocular Camera<sup>*</sup>,https://ieeexplore.ieee.org/document/9561572/,"Scene flow represents the motion of points in the 3D space, which is the counterpart of the optical flow that represents the motion of pixels in the 2D image. However, it is difficult to obtain the ground truth of scene flow in the real scenes, and recent studies are based on synthetic data for training. Therefore, how to train a scene flow network with unsupervised methods based on real-world data shows crucial significance. A novel unsupervised learning method for scene flow is proposed in this paper, which utilizes the images of two consecutive frames taken by monocular camera without the ground truth of scene flow for training. Our method realizes the goal that training scene flow network with real-world data, which bridges the gap between training data and test data and broadens the scope of available data for training. Unsupervised learning of scene flow in this paper mainly consists of two parts: (i) depth estimation and camera pose estimation, and (ii) scene flow estimation based on four different loss functions. Depth estimation and camera pose estimation obtain the depth maps and camera pose between two consecutive frames, which provide further information for the next scene flow estimation. After that, we used depth consistency loss, dynamic-static consistency loss, Chamfer loss, and Laplacian regularization loss to carry out unsupervised training of the scene flow network. To our knowledge, this is the first paper that realizes the unsupervised learning of 3D scene flow from monocular camera. The experiment results on KITTI show that our method for unsupervised learning of scene flow meets great performance compared to traditional methods Iterative Closest Point (ICP) and Fast Global Registration (FGR). The source code is available at: https://github.com/IRMVLab/3DUnMonoFlow.",multimedia
10.1109/EMBC.2018.8512516,to_check,2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),IEEE,2018-07-21 00:00:00,ieeexplore,Estimating Mini Mental State Examination Scores using Game-Specific Performance Values: A Preliminary Study,https://ieeexplore.ieee.org/document/8512516/,"Individuals with permanent cognitive impairment need to be evaluated and monitored. There exists a number of clinically validated cognitive assessment tools, but they often need to be administered by trained therapists in clinical settings. This serves as a major barrier for frequent, longitudinal monitoring of cognitive function. In this work, we introduce Neuro-World, a collection of innovative 3D mobile games, that allows one to self-administer the assessment of his/her cognitive function. The game performance is analyzed and converted into a clinically-accepted measure of cognitive function, specifically the Mini Mental State Examination (MMSE) score, improving the translational impact of the system in real-world clinical settings. To validate the feasibility of our approach, we collected game-specific performance data from 12 post-stroke patients, which was used to train a supervised machine learning model to estimate the corresponding MMSE score. Our experiment results showed a normalized root mean square error of 5.3% between the actual and estimated MMSE scores. This study enables new clinical and research opportunities for accurate longitudinal assessment of cognitive function via an interactive means of playing mobile games.",multimedia
10.1109/LRA.2020.3004325,to_check,IEEE Robotics and Automation Letters,IEEE,2020-07-01 00:00:00,ieeexplore,Cross-View Semantic Segmentation for Sensing Surroundings,https://ieeexplore.ieee.org/document/9123682/,"Sensing surroundings plays a crucial role in human spatial perception, as it extracts the spatial configuration of objects as well as the free space from the observations. To facilitate the robot perception with such a surrounding sensing capability, we introduce a novel visual task called Cross-view Semantic Segmentation as well as a framework named View Parsing Network (VPN) to address it. In the cross-view semantic segmentation task, the agent is trained to parse the first-view observations into a top-down-view semantic map indicating the spatial location of all the objects at pixel-level. The main issue of this task is that we lack the real-world annotations of top-down-view data. To mitigate this, we train the VPN in 3D graphics environment and utilize the domain adaptation technique to transfer it to handle real-world data. We evaluate our VPN on both synthetic and real-world agents. The experimental results show that our model can effectively make use of the information from different views and multi-modalities to understanding spatial information. Our further experiment on a LoCoBot robot shows that our model enables the surrounding sensing capability from 2D image input. Code and demo videos can be found at https://view-parsing-network.github.io.",multimedia
10.1109/TVCG.2019.2927477,to_check,IEEE Transactions on Visualization and Computer Graphics,IEEE,2021-01-01 00:00:00,ieeexplore,Evaluating Balance Recovery Techniques for Users Wearing Head-Mounted Display in VR,https://ieeexplore.ieee.org/document/8758372/,"Room-scale 3D position tracking enables users to explore a virtual environment by physically walking, which improves comfort and the level of immersion. However, when users walk with their eyesight blocked by a head-mounted display, they may unexpectedly lose their balance and fall if they bump into real-world obstacles or unintentionally shift their center of mass outside the margin of stability. This paper evaluates balance recovery methods and intervention timing during the use of VR with the assumption that the onset of a fall is given. Our experiment followed the tether-release protocol during clinical research and induced a fall while a subject was engaged in a secondary 3D object selection task. The experiment employed a two-by-two design that evaluated two assistive techniques, i.e., video-see-through and auditory warning at two different timings, i.e., at fall onset and 500ms prior to fall onset. The data from 17 subjects showed that video-see-through triggered 500 ms before the onset of fall can effectively help users recover from falls. Surprisingly, video-see-through at fall onset has a significant negative impact on balance recovery and produces similar results to those of the baseline condition (no intervention).",multimedia
10.1109/ICME.2019.00058,to_check,2019 IEEE International Conference on Multimedia and Expo (ICME),IEEE,2019-07-12 00:00:00,ieeexplore,360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming,https://ieeexplore.ieee.org/document/8784927/,"Tile-based 360-degree video (360 video) streaming, employed with adaptive bitrate (ABR) algorithms, is a promising approach to offer high video quality of experience (QoE) within limited network bandwidth. Existing ABR algorithms, however, fail to achieve optimal performance in real-world fluctuated network conditions as they heavily rely on unbiased bandwidth predictions. Recently, reinforcement learning (RL) has shown promising potential in generating better ABR algorithms in 2D video streaming. However, unlike existed work in 2D video streaming, directly applying RL in the tile-based 360 video streaming is infeasible due to the resulting exponential decision space. To overcome these limitations, we propose in this paper 360SRL, an improved ABR algorithm employing Sequential RL (360SRL). Firstly, we reduce the decision space of 360SRL from exponential to linear by introducing a sequential ABR decision structure, thus making it feasible to be employed with RL. Secondly, instead of relying on accurate bandwidth predictions, 360SRL learns to make ABR decisions solely through observations of the resulting QoE performance of past decisions. Finally, we compare 360SRL to state-of-the-art ABR algorithms using trace-driven experiments. The experiment results demonstrate that 360SRL outperforms state-of-the-art algorithms with around 12% improvement in average QoE.",multimedia
10.1109/IJCNN52387.2021.9533460,to_check,2021 International Joint Conference on Neural Networks (IJCNN),IEEE,2021-07-22 00:00:00,ieeexplore,CMVCG: Non-autoregressive Conditional Masked Live Video Comments Generation Model,https://ieeexplore.ieee.org/document/9533460/,"The blooming of live comment videos leads to the need of automatic live video comment generating task. Previous works focus on autoregressive live video comments generation and can only generate comments by giving the first word of the target comment. However, in some scenes, users need to generate comments by their given prompt keywords, which can't be solved by the traditional live video comment generation methods. In this paper, we propose a Transformer based non-autoregressive conditional masked live video comments generation model called CMVCG model. Our model considers not only the visual and textual context of the comments, but also time and color information. To predict the position of the given prompt keywords, we also introduce a keywords position predicting module. By leveraging the conditional masked language model, our model achieves non-autoregressive live video comment generation. Furthermore, we collect and introduce a large-scale real-world live video comment dataset called Bili-22 dataset. We evaluate our model in two live comment datasets and the experiment results present that our model outperforms the state-of-the-art models in most of the metrics.",multimedia
10.1109/INFOCOM41043.2020.9155492,to_check,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,IEEE,2020-07-09 00:00:00,ieeexplore,PERM: Neural Adaptive Video Streaming with Multi-path Transmission,https://ieeexplore.ieee.org/document/9155492/,"The multi-path transmission techniques enable multiple paths to maximize resource usage and increase throughput in transmission, which have been installed over mobile devices in recent years. For video streaming applications, compared to the single-path transmission, the multi-path techniques can establish multiple subflows simultaneously to extend the available bandwidth for streaming high-quality videos in mobile devices. Existing adaptive video streaming systems have difficulty in harnessing multi-path scheduling and balancing the tradeoff between the quality of experience (QoE) and quality of service (QoS) concerns. In this paper, we propose an actor-critic network based on Periodical Experience Replay for Multi-path video streaming (PERM). Specifically, PERM employs two actor modules and a critic module: the two actor modules respectively assign the path usage of each subflow and select bitrates for the next chunk of the video, while the critic module predicts the overall objectives. We conduct trace-driven emulation and real-world testbed experiment to examine the performance of PERM, and results show that PERM outperforms state-of-the-art multi-path and single path streaming systems, with an improvement of 10%- 15% on the QoE and QoS metrics.",multimedia
10.1109/TKDE.2018.2885520,to_check,IEEE Transactions on Knowledge and Data Engineering,IEEE,2020-03-01 00:00:00,ieeexplore,Personalized Video Recommendation Using Rich Contents from Videos,https://ieeexplore.ieee.org/document/8567986/,"Video recommendation has become an essential way of helping people explore the massive videos and discover the ones that may be of interest to them. In the existing video recommender systems, the models make the recommendations based on the user-video interactions and single specific content features. When the specific content features are unavailable, the performance of the existing models will seriously deteriorate. Inspired by the fact that rich contents (e.g., text, audio, motion, and so on) exist in videos, in this paper, we explore how to use these rich contents to overcome the limitations caused by the unavailability of the specific ones. Specifically, we propose a novel general framework that incorporates arbitrary single content feature with user-video interactions, named as collaborative embedding regression (CER) model, to make effective video recommendation in both in-matrix and out-of-matrix scenarios. Our extensive experiments on two real-world large-scale datasets show that CER beats the existing recommender models with any single content feature and is more time efficient. In addition, we propose a priority-based late fusion (PRI) method to gain the benefit brought by the integrating the multiple content features. The corresponding experiment shows that PRI brings real performance improvement to the baseline and outperforms the existing fusion methods.",multimedia
10.1109/ACCESS.2019.2947067,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Toward Edge-Assisted Video Content Intelligent Caching With Long Short-Term Memory Learning,https://ieeexplore.ieee.org/document/8866710/,"Nowadays video content has contributed to the majority of Internet traffic, which brings great challenge to the network infrastructure. Fortunately, the emergence of edge computing has provided a promising way to reduce the video load on the network by caching contents closer to users.But caching replacement algorithm is essential for the cache efficiency considering the limited cache space under existing edge-assisted network architecture. To investigate the challenges and opportunities inside, we first measure the performance of five state-of-the-art caching algorithms based on three real-world datasets. Our observation shows that state-of-the-art caching replacement algorithms suffer from following weaknesses: 1) the rule-based replacement approachs (e.g., LFU,LRU) cannot adapt under different scenarios; 2) data-driven forecast approaches only work efficiently on specific scenarios or datasets, as the extracted features working on one dataset may not work on another one. Motivated by these observations and edge-assisted computation capacity, we then propose an edge-assisted intelligent caching replacement framework LSTM-C based on deep Long Short-Term Memory network, which contains two types of modules: 1) four basic modules manage the coordination among content requests, content replace, cache space, service management; 2) three learning-based modules enable the online deep learning to provide intelligent caching strategy. Supported by this design, LSTM-C learns the pattern of content popularity at long and short time scales as well as determines the cache replacement policy. Most important, LSTM-C represents the request pattern with built-in memory cells, thus requires no data pre-processing, pre-programmed model or additional information. Our experiment results show that LSTM-C outperforms state-of-the-art methods in cache hit rate on three real-traces of video requests. When the cache size is limited, LSTM-C outperforms baselines by 20%~32% in cache hit rate. We also show that the training and predicting time of one iteration are $8.6~ms$ and $300~\mu s$ on average respectively, which are fast enough for online operations.",multimedia
10.1109/HRI.2010.5453172,to_check,2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI),IEEE,2010-03-05 00:00:00,ieeexplore,FusionBot: A barista robot: Fusionbot serving coffees to visitors during technology exhibition event,https://ieeexplore.ieee.org/document/5453172/,"Summary form of only given: This video shows a service robot named FusionBot autonomously serving coffees to visitors on their request, which occurred during two days-long experiment in TechFest 2008 event. The coffee serving task involves taking coffee order from a visitor, identifying a cup and smart coffee machine, moving towards the coffee machine, communicating with the coffee machine and fetching the coffee cup to the visitor. The main purpose of this experiment is to explore and demonstrate the utility of an interactive service robot in smart home environment, thereby improving the quality of human life. Before conducting the experiments, visitors were given general procedural instructions and simple introduction on how the FusionBot works. Visitors then performed experiment tasks, i.e., ordering a cup of coffee. Thereafter, the visitors were asked to fill out the satisfaction questionnaires to find out their reaction and perception on the FusionBot. Of just over 100 survey questionnaires handed out, sixty eight (68) valid responses (i.e. 68%) were received. Over all, with regards to the FusionBot task satisfaction, more than half of respondents were satisfied with what the FusionBot can do. Nearly one quarter of the respondents indicated that it was not easy to communicate with the FusionBot. This could be due to occurrence of various background noises, which were falsely picked up by the FusionBot as speech input from the visitor. Similarly, less than one quarter indicated that it was not easy to learn how to use the FusionBot. This could be due to the not knowing what to do with the FusionBot and not knowing what the FusionBot does. The experiment was successful in two main dimensions; (1) the robot demonstrated the ability to interact with visitors and perform challenging real-world task autonomously, and (2) It provided some evidence towards the feasibility of using autonomous service robot and smart coffee machine to serve drink in a reception/home or acting as a host in an organization. While preliminary, the experiment also suggests that while developing a service robot; (1) static appearance is very important, (2) requires robust speech recognition and vision understanding, and finally (3) requires comprehensive training on speech and vision with respective data.",multimedia
10.1109/CVPRW.2017.206,to_check,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),IEEE,2017-07-26 00:00:00,ieeexplore,RATM: Recurrent Attentive Tracking Model,https://ieeexplore.ieee.org/document/8014940/,"We present an attention-based modular neural framework for computer vision. The framework uses a soft attention mechanism allowing models to be trained with gradient descent. It consists of three modules: a recurrent attention module controlling where to look in an image or video frame, a feature-extraction module providing a representation of what is seen, and an objective module formalizing why the model learns its attentive behavior. The attention module allows the model to focus computation on task-related information in the input. We apply the framework to several object tracking tasks and explore various design choices. We experiment with three data sets, bouncing ball, moving digits and the real-world KTH data set. The proposed RATM performs well on all three tasks and can generalize to related but previously unseen sequences from a challenging tracking data set.",multimedia
10.23919/EUSIPCO.2019.8902932,to_check,2019 27th European Signal Processing Conference (EUSIPCO),IEEE,2019-09-06 00:00:00,ieeexplore,Referenceless Performance Evaluation of Audio Source Separation using Deep Neural Networks,https://ieeexplore.ieee.org/document/8902932/,"Current performance evaluation for audio source separation depends on comparing the processed or separated signals with reference signals. Therefore, common performance evaluation toolkits are not applicable to real-world situations where the ground truth audio is unavailable. In this paper, we propose a performance evaluation technique that does not require reference signals in order to assess separation quality. The proposed technique uses a deep neural network (DNN) to map the processed audio into its quality score. Our experiment results show that the DNN is capable of predicting the sources-to-artifacts ratio from the blind source separation evaluation toolkit [1] for singing-voice separation without the need for reference signals.",multimedia
10.1109/ICASSP39728.2021.9413575,to_check,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2021-06-11 00:00:00,ieeexplore,Bandwidth Extension is All You Need,https://ieeexplore.ieee.org/document/9413575/,"Speech generation and enhancement have seen recent breakthroughs in quality thanks to deep learning. These methods typically operate at a limited sampling rate of 16-22kHz due to computational complexity and available datasets. This limitation imposes a gap between the output of such methods and that of high-fidelity (≥44kHz) real-world audio applications. This paper proposes a new bandwidth extension (BWE) method that expands 8-16kHz speech signals to 48kHz. The method is based on a feed-forward WaveNet architecture trained with a GAN-based deep feature loss. A mean-opinion-score (MOS) experiment shows significant improvement in quality over state-of-the-art BWE methods. An AB test reveals that our 16-to-48kHz BWE is able to achieve fidelity that is typically indistinguishable from real high-fidelity recordings. We use our method to enhance the output of recent speech generation and denoising methods, and experiments demonstrate significant improvement in sound quality over these baselines. We propose this as a general approach to narrow the gap between generated speech and recorded speech, without the need to adapt such methods to higher sampling rates.",multimedia
10.1109/TIP.2021.3053398,to_check,IEEE Transactions on Image Processing,IEEE,2021-01-01 00:00:00,ieeexplore,Adversarial Training for Solving Inverse Problems in Image Processing,https://ieeexplore.ieee.org/document/9337199/,"Inverse problems are a group of important mathematical problems that aim at estimating source data x and operation parameters z from inadequate observations y. In the image processing field, most recent deep learning-based methods simply deal with such problems under a pixel-wise regression framework (from y to x) while ignoring the physics behind. In this paper, we re-examine these problems under a different viewpoint and propose a novel framework for solving certain types of inverse problems in image processing. Instead of predicting x directly from y, we train a deep neural network to estimate the degradation parameters z under an adversarial training paradigm. We show that if the degradation behind satisfies some certain assumptions, the solution to the problem can be improved by introducing additional adversarial constraints to the parameter space and the training may not even require pair-wise supervision. In our experiment, we apply our method to a variety of real-world problems, including image denoising, image deraining, image shadow removal, non-uniform illumination correction, and underdetermined blind source separation of images or speech signals. The results on multiple tasks demonstrate the effectiveness of our method.",multimedia
10.1007/s11416-020-00376-6,to_check,Journal of Computer Virology and Hacking Techniques,Springer,2021-06-01 00:00:00,springer,Audio signal processing for Android malware detection and family identification,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11416-020-00376-6,"Mobile malware is increasing in complexity and maliciousness, with particular regard to the malicious samples targeting the Android platform, currently the most widespread operating system for mobile devices. In this scenario antimalware technologies are not able to detect the so-called zero-day malware, because they are able to detect mobile malware only once their malicious signature is stored in the antimalware repository (i.e., the so-called signature based approach). From these considerations, in this paper an approach for detecting Android malware is proposed. Moreover the proposed approach aims to detect the belonging family of the malicious sample under analysis. We represent the executable of the application in term of audio file and, exploiting audio signal processing techniques, we extract a set of numerical features from each sample. Thus, we build several machine learning models and we evaluate their effectiveness in terms of malware detection and family identification. We experiment the method we propose on a data-set composed by 50,000 Android real-world samples (24,553 malicious among 71 families and 25,447 legitimate), by reaching an accuracy equal to 0.952 in Android malware detection and of 0.922 in family detection.",multimedia
10.1007/978-3-030-80568-5_37,to_check,Proceedings of the 22nd Engineering Applications of Neural Networks Conference,Springer,2021-01-01 00:00:00,springer,Using Artificial Neural Network to Provide Realistic Lifting Capacity in the Mobile Crane Simulation,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80568-5_37,"Simulations are often used for training novice operators to avoid accidents, while they are still polishing their skills. To ensure the experience gained in the simulation be applicable in real-world scenarios, the simulation has to be made as realistic as possible. This paper investigated how to make the lifting capacity of a virtual mobile crane behave similarly like its real counterpart. We initially planned to use information from the load charts, which document how the lifting capacity of a mobile crane works, but the data in the load charts were very limited. To mitigate this issue, we trained an artificial neural network (ANN) using 90% of random data from two official load charts of a real mobile crane. The trained model could predict the lifting capacity based on the real-time states of the boom length, the load radius, and the counterweight of the virtual mobile crane. To evaluate the accuracy of the ANN predictions, we conducted a real-time experiment inside the simulation, where we compared the lifting capacity predicted by the ANN and the remaining 10% of the data from the load charts. The results showed that the ANN could predict the lifting capacity with small deviation rates. The deviation rates also had no significant impact on the lifting capacity, except when both boom length and load radius were approaching their maximum states. Therefore, the predicted lifting capacity generated by the ANN could be assumed to be close enough to the values in the load charts.",multimedia
http://arxiv.org/abs/2011.04424v2,to_check,arxiv,arxiv,2020-11-05 13:49:55+00:00,arxiv,"Playing optical tweezers with deep reinforcement learning: in virtual,
  physical and augmented environments",http://arxiv.org/abs/2011.04424v2,"Reinforcement learning was carried out in a simulated environment to learn
continuous velocity control over multiple motor axes. This was then applied to
a real-world optical tweezers experiment with the objective of moving a
laser-trapped microsphere to a target location whilst avoiding collisions with
other free-moving microspheres. The concept of training a neural network in a
virtual environment has significant potential in the application of machine
learning for experimental optimization and control, as the neural network can
discover optimal methods for problem solving without the risk of damage to
equipment, and at a speed not limited by movement in the physical environment.
As the neural network treats both virtual and physical environments
equivalently, we show that the network can also be applied to an augmented
environment, where a virtual environment is combined with the physical
environment. This technique may have the potential to unlock capabilities
associated with mixed and augmented reality, such as enforcing safety limits
for machine motion or as a method of inputting observations from additional
sensors.",multimedia
http://arxiv.org/abs/2109.09929v1,to_check,arxiv,arxiv,2021-09-21 03:12:23+00:00,arxiv,"A Unified Approach of Detecting Misleading Images via Tracing its
  Instances on Web and Analysing its Past Context for the Verification of
  Content",http://arxiv.org/abs/2109.09929v1,"The verification of multimedia content over social media is one of the
challenging and crucial issues in the current scenario and gaining prominence
in an age where user-generated content and online social web platforms are the
leading sources in shaping and propagating news stories. As these sources allow
users to share their opinions without restriction, opportunistic users often
post misleading/ unreliable content on social media such as Twitter, Facebook,
etc. At present, to lure users towards the news story, the text is often
attached with some multimedia content (images/videos/audios). Verifying these
contents to maintain the credibility and reliability of social media
information is of paramount importance. Motivated by this, we proposed a
generalized system that supports the automatic classification of images into
credible or misleading. In this paper, we investigated machine learning-based
as well as deep learning-based approaches utilized to verify misleading
multimedia content, where the available image traces are used to identify the
credibility of the content. The experiment is performed on the real-world
dataset (Media-eval-2015 dataset) collected from Twitter. It also demonstrates
the efficiency of our proposed approach and features using both Machine and
Deep Learning Model (Bi-directional LSTM). The experiment result reveals that
the Microsoft bings image search engine is quite effective in retrieving titles
and performs better than our study's Google image search engine. It also shows
that gathering clues from attached multimedia content (image) is more effective
than detecting only posted content-based features.",multimedia
http://arxiv.org/abs/2103.11799v1,to_check,arxiv,arxiv,2021-03-14 16:11:30+00:00,arxiv,DeepHate: Hate Speech Detection via Multi-Faceted Text Representations,http://arxiv.org/abs/2103.11799v1,"Online hate speech is an important issue that breaks the cohesiveness of
online social communities and even raises public safety concerns in our
societies. Motivated by this rising issue, researchers have developed many
traditional machine learning and deep learning methods to detect hate speech in
online social platforms automatically. However, most of these methods have only
considered single type textual feature, e.g., term frequency, or using word
embeddings. Such approaches neglect the other rich textual information that
could be utilized to improve hate speech detection. In this paper, we propose
DeepHate, a novel deep learning model that combines multi-faceted text
representations such as word embeddings, sentiments, and topical information,
to detect hate speech in online social platforms. We conduct extensive
experiments and evaluate DeepHate on three large publicly available real-world
datasets. Our experiment results show that DeepHate outperforms the
state-of-the-art baselines on the hate speech detection task. We also perform
case studies to provide insights into the salient features that best aid in
detecting hate speech in online social platforms.",multimedia
http://arxiv.org/abs/1811.00454v1,to_check,arxiv,arxiv,2018-11-01 15:50:42+00:00,arxiv,"Referenceless Performance Evaluation of Audio Source Separation using
  Deep Neural Networks",http://arxiv.org/abs/1811.00454v1,"Current performance evaluation for audio source separation depends on
comparing the processed or separated signals with reference signals. Therefore,
common performance evaluation toolkits are not applicable to real-world
situations where the ground truth audio is unavailable. In this paper, we
propose a performance evaluation technique that does not require reference
signals in order to assess separation quality. The proposed technique uses a
deep neural network (DNN) to map the processed audio into its quality score.
Our experiment results show that the DNN is capable of predicting the
sources-to-artifacts ratio from the blind source separation evaluation toolkit
without the need for reference signals.",multimedia
http://arxiv.org/abs/2104.02017v1,to_check,arxiv,arxiv,2021-04-05 17:12:51+00:00,arxiv,Self-Supervised Learning for Personalized Speech Enhancement,http://arxiv.org/abs/2104.02017v1,"Speech enhancement systems can show improved performance by adapting the
model towards a single test-time speaker. In this personalization context, the
test-time user might only provide a small amount of noise-free speech data,
likely insufficient for traditional fully-supervised learning. One way to
overcome the lack of personal data is to transfer the model parameters from a
speaker-agnostic model to initialize the personalized model, and then to
finetune the model using the small amount of personal speech data. This
baseline marginally adapts over the scarce clean speech data. Alternatively, we
propose self-supervised methods that are designed specifically to learn
personalized and discriminative features from abundant in-the-wild noisy, but
still personal speech recordings. Our experiment shows that the proposed
self-supervised learning methods initialize personalized speech enhancement
models better than the baseline fully-supervised methods, yielding superior
speech enhancement performance. The proposed methods also result in a more
robust feature set under the real-world conditions: compressed model sizes and
fewness of the labeled data.",multimedia
http://arxiv.org/abs/2109.13630v1,to_check,arxiv,arxiv,2021-09-28 11:47:12+00:00,arxiv,Unsupervised Diffeomorphic Surface Registration and Non-Linear Modelling,http://arxiv.org/abs/2109.13630v1,"Registration is an essential tool in image analysis. Deep learning based
alternatives have recently become popular, achieving competitive performance at
a faster speed. However, many contemporary techniques are limited to volumetric
representations, despite increased popularity of 3D surface and shape data in
medical image analysis. We propose a one-step registration model for 3D
surfaces that internalises a lower dimensional probabilistic deformation model
(PDM) using conditional variational autoencoders (CVAE). The deformations are
constrained to be diffeomorphic using an exponentiation layer. The one-step
registration model is benchmarked against iterative techniques, trading in a
slightly lower performance in terms of shape fit for a higher compactness. We
experiment with two distance metrics, Chamfer distance (CD) and Sinkhorn
divergence (SD), as specific distance functions for surface data in real-world
registration scenarios. The internalised deformation model is benchmarked
against linear principal component analysis (PCA) achieving competitive results
and improved generalisability from lower dimensions.",multimedia
http://arxiv.org/abs/1807.07501v3,to_check,arxiv,arxiv,2018-07-19 15:42:26+00:00,arxiv,Noise Adaptive Speech Enhancement using Domain Adversarial Training,http://arxiv.org/abs/1807.07501v3,"In this study, we propose a novel noise adaptive speech enhancement (SE)
system, which employs a domain adversarial training (DAT) approach to tackle
the issue of a noise type mismatch between the training and testing conditions.
Such a mismatch is a critical problem in deep-learning-based SE systems. A
large mismatch may cause a serious performance degradation to the SE
performance. Because we generally use a well-trained SE system to handle
various unseen noise types, a noise type mismatch commonly occurs in real-world
scenarios. The proposed noise adaptive SE system contains an
encoder-decoder-based enhancement model and a domain discriminator model.
During adaptation, the DAT approach encourages the encoder to produce
noise-invariant features based on the information from the discriminator model
and consequentially increases the robustness of the enhancement model to unseen
noise types. Herein, we regard stationary noises as the source domain (with the
ground truth of clean speech) and non-stationary noises as the target domain
(without the ground truth). We evaluated the proposed system on TIMIT
sentences. The experiment results show that the proposed noise adaptive SE
system successfully provides significant improvements in PESQ (19.0%), SSNR
(39.3%), and STOI (27.0%) over the SE system without an adaptation.",multimedia
http://arxiv.org/abs/2010.11162v1,to_check,arxiv,arxiv,2020-10-21 17:28:56+00:00,arxiv,In-the-wild Drowsiness Detection from Facial Expressions,http://arxiv.org/abs/2010.11162v1,"Driving in a state of drowsiness is a major cause of road accidents,
resulting in tremendous damage to life and property. Developing robust,
automatic, real-time systems that can infer drowsiness states of drivers has
the potential of making life-saving impact. However, developing drowsiness
detection systems that work well in real-world scenarios is challenging because
of the difficulties associated with collecting high-volume realistic drowsy
data and modeling the complex temporal dynamics of evolving drowsy states. In
this paper, we propose a data collection protocol that involves outfitting
vehicles of overnight shift workers with camera kits that record their faces
while driving. We develop a drowsiness annotation guideline to enable humans to
label the collected videos into 4 levels of drowsiness: `alert', `slightly
drowsy', `moderately drowsy' and `extremely drowsy'. We experiment with
different convolutional and temporal neural network architectures to predict
drowsiness states from pose, expression and emotion-based representation of the
input video of the driver's face. Our best performing model achieves a macro
ROC-AUC of 0.78, compared to 0.72 for a baseline model.",multimedia
10.1016/j.ecoinf.2021.101240,to_check,Ecological Informatics,scopus,2021-07-01,sciencedirect,Multi-class fish stock statistics technology based on object classification and tracking algorithm,https://api.elsevier.com/content/abstract/scopus_id/85106889350,"The development of intensive aquaculture has increased the need for video-based underwater monitoring technology to generate statistics on multi-class fish. However, the complex marine environment, e.g., light fluctuations, shape deformations, similar appearance of fish, and occlusions, makes this a challenging task. Therefore, there are relatively few studies in this field. This paper proposes a real-time multi-class fish stock statistics method (RMCF). The accuracy of fish stock statistics has reached 95.6% over the previous best approach. The proposed method uses YOLOv4 as a backbone network and a parallel two-branch structure based on deep learning to perform real-time detection and tracking of fish in a real marine ranch environment. The two-branch structure contains detection and tracking branches, where the detection branch detects fish species and improves tracking accuracy and online tracking time. The tracking branch tracks the fish and making a number statistics. Finally, we combine the detection and tracking branches to generate multi-class fish stock statistics. Here, the detection branch helps the tracking branch realize multi-class tracking. With the tracking results, we further analyze the changing trends of different fish over time. Compared to state-of-the-art video tracking and detection methods, the experiment results demonstrate the proposed method provides better fish detection and tracking performance in a complex real-world marine environment.",multimedia
10.1016/j.neunet.2021.02.015,to_check,Neural Networks,scopus,2021-07-01,sciencedirect,End-to-end novel visual categories learning via auxiliary self-supervision,https://api.elsevier.com/content/abstract/scopus_id/85101873827,"Semi-supervised learning has largely alleviated the strong demand for large amount of annotations in deep learning. However, most of the methods have adopted a common assumption that there is always labeled data from the same class of unlabeled data, which is impractical and restricted for real-world applications. In this research work, our focus is on semi-supervised learning when the categories of unlabeled data and labeled data are disjoint from each other. The main challenge is how to effectively leverage knowledge in labeled data to unlabeled data when they are independent from each other, and not belonging to the same categories. Previous state-of-the-art methods have proposed to construct pairwise similarity pseudo labels as supervising signals. However, two issues are commonly inherent in these methods: (1) All of previous methods are comprised of multiple training phases, which makes it difficult to train the model in an end-to-end fashion. (2) Strong dependence on the quality of pairwise similarity pseudo labels limits the performance as pseudo labels are vulnerable to noise and bias. Therefore, we propose to exploit the use of self-supervision as auxiliary task during model training such that labeled data and unlabeled data will share the same set of surrogate labels and overall supervising signals can have strong regularization. By doing so, all modules in the proposed algorithm can be trained simultaneously, which will boost the learning capability as end-to-end learning can be achieved. Moreover, we propose to utilize local structure information in feature space during pairwise pseudo label construction, as local properties are more robust to noise. Extensive experiments have been conducted on three frequently used visual datasets, i.e., CIFAR-10, CIFAR-100 and SVHN, in this paper. Experiment results have indicated the effectiveness of our proposed algorithm as we have achieved new state-of-the-art performance for novel visual categories learning for these three datasets.",multimedia
10.1016/j.eswa.2020.113695,to_check,Expert Systems with Applications,scopus,2020-12-15,sciencedirect,Genetic state-grouping algorithm for deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85087883558,"Although Reinforcement learning has already been considered one of the most important and well-known techniques of machine learning, its applicability remains limited in the real-world problems due to its long initial learning time and unstable learning. Especially, the problem of an overwhelming number of the branching factors under real-time constraint still stays unconquered, demanding a new method for the next generation of reinforcement learning. In this paper, we propose Genetic State-Grouping Algorithm based on deep reinforcement learning. The core idea is to divide the entire set of states into a few state groups. Each group consists of states that are mutually similar, thus representing their common features. The state groups are then processed with the Genetic Optimizer, which finds outstanding actions. These steps help the Deep Q Network avoid excessive exploration, thereby contributing to the significant reduction of initial learning time. The experiment on the real-time fighting video game (FightingICE) shows the effectiveness of our proposed approach.",multimedia
10.1109/MLHPC49564.2019.00013,to_check,2019 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC),IEEE,2019-11-18 00:00:00,ieeexplore,DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in Spatiotemporal Systems,https://ieeexplore.ieee.org/document/8950692/,"Extracting actionable insight from complex unlabeled scientific data is an open challenge and key to unlocking data-driven discovery in science. Complementary and alternative to supervised machine learning approaches, unsupervised physics-based methods based on behavior-driven theories hold great promise. Due to computational limitations, practical application on real-world domain science problems has lagged far behind theoretical development. However, powerful modern supercomputers provide the opportunity to narrow the gap between theory and practical application. We present our first step towards bridging this divide - DisCo - a high-performance distributed workflow for the behavior-driven local causal state theory. DisCo provides a scalable unsupervised physics-based representation learning method that decomposes spatiotemporal systems into their structurally relevant components, which are captured by the latent local causal state variables. In several firsts we demonstrate the efficacy of DisCo in capturing physically meaningful coherent structures from observational and simulated scientific data. To the best of our knowledge, DisCo is also the first application software developed entirely in Python to scale to over 1000 machine nodes, providing good performance along with ensuring domain scientists' productivity. Our capstone experiment, using newly developed and optimized DisCo workflow and libraries, performs unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data, processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64% strong-scaling efficiency. This enables us to achieve state-of-the-art unsupervised segmentation of coherent spatiotemporal structures in complex fluid flows.",science
10.1109/NCCA.2015.25,to_check,2015 IEEE Fourth Symposium on Network Cloud Computing and Applications (NCCA),IEEE,2015-06-12 00:00:00,ieeexplore,A Cloud-Assisted Framework for Bag-of-Features Tagging in Social Networks,https://ieeexplore.ieee.org/document/7340054/,"Recently, Bag-of-Features Tagging is proven to be an alternative to discover user connections from user shared images in social networks. This approach used unsupervised clustering to classify the user shared images and then correlate similar user, which is computationally intensive for real-world applications. This paper introduces a cloud-assisted framework to improve the efficiency and scalability of Bag-of-Features Tagging. The framework distributes the computation of the unsupervised clustering, the profile learning process and also the similarity calculation. The experiment proves how a scalable cloud-assisted framework outperforms a stand-alone machine with different parameters on a real social network dataset, Skyrock.",science
10.1109/IJCNN52387.2021.9534075,to_check,2021 International Joint Conference on Neural Networks (IJCNN),IEEE,2021-07-22 00:00:00,ieeexplore,Attentional Social Recommendation System with Graph Convolutional Network,https://ieeexplore.ieee.org/document/9534075/,"As an effective deep representation learning technique for graph, graph convolutional network (GCN) has recently been widely applied to obtain better embedding of vertex. The existing studies have successfully explored user-item interaction via GCN for recommendation task and proved its effectiveness. We argue that a significant limitation of these methods is that social relation, which has been proven to impose positive effects for recommendation in many loss optimization models, has received relatively little scrutiny in GCN. Thus, the resultant embeddings are insufficient to model the potential social propagation effect. In our work, we propose to obtain embeddings by using the neighborhood propagation mechanism on two coupled graph neural networks, i.e., user-item interaction graph and social relation graph, which is capable of capturing the interplay between the item taste of user and user connection. In particular, to address the challenge that different factors on neighborhood propagation process make different contributions for the embedding, we develop a new social recommendation framework with hierarchical attention(i.e., neighbor-level and graph-level attention) Attentional Social Recommendation system (ASR). This allows much flexibility for adaptively acquiring relative importance for different factors. Extensive experiment on two real-world datasets not only show the superior performance of our proposed model over the baselines, but also demonstrate the effectiveness of simultaneous neighborhood propagation on two graphs.",science
10.1109/ICTAI50040.2020.00079,to_check,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2020-11-11 00:00:00,ieeexplore,Probabilistic Decision Modeling in Social Networks,https://ieeexplore.ieee.org/document/9288207/,"Bayesian approaches have been successfully applied in social network analysis to study group behaviors such as online information dissemination and voting pattern. The focus has been on estimating the structure and strength of peer influence and its impact on the decisions of an individual. Less attention has been given to incorporating contextual information and individuals' hidden characteristics (or bias). In this work, we examine the social dynamics where social influence and contextual information play pivotal roles in driving one's decision. We design a probabilistic graphical model called CLAP to understand users' decision behavior in a social network, with an emphasis on both social-level and individual-level factors. To this end, the proposed model introduces hidden bias states associated with each actor and jointly estimates each actor's hidden bias state together with the social influence network. We demonstrate the effectiveness of CLAP on two types of social networks, a real-world US Congress network where senators vote on new bills, and online Twitter networks where users debate on the effectiveness of vaccine and lockdown policy during COVID-19. The experiment results show that CLAP outperforms state-of-the-art game theoretic approaches in predicting user decision. Further, the estimated social influence networks by CLAP has high edge homogeniety ratios.",science
10.1109/ICWS53863.2021.00024,to_check,2021 IEEE International Conference on Web Services (ICWS),IEEE,2021-09-10 00:00:00,ieeexplore,SRaSLR: A Novel Social Relation Aware Service Label Recommendation Model,https://ieeexplore.ieee.org/document/9590240/,"With the rapid development of new technologies such as cloud, edge and mobile computing, the number and diversity of available services are dramatically exploding and services have become increasingly important to people's daily work and life. As a consequence, using service label recommendation techniques to automatically categorize services plays a crucial role in many service computing tasks, such as service discovery, service composition, and service organization. There have been many service label recommendation studies that have achieved remarkable performance. However, these studies mainly focus on using the text information in service profiles to recommend labels for services while overlooking those social relations that widely exist among services. We argue that such social relations can help to obtain more precise recommendation results. In this paper, we propose a novel Social Relation aware Service Label Recommendation model called SRaSLR, which combines text information in service profiles and social network relations among services. A deep learning based model is constructed based on feature fusion of the two perspectives. We conduct extensive experiments on the real-world Programmable Web dataset, and the experiment results show that SRaSLR yields better performance than existing methods. Additionally, we discuss how service social network affects service label recommendation performance based on the experiment results.",science
10.1109/CSCWD49262.2021.9437878,to_check,2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD),IEEE,2021-05-07 00:00:00,ieeexplore,Sentiment Evolution in Social Network Based on Joint Pre-training Model,https://ieeexplore.ieee.org/document/9437878/,"Sentiment analysis is one of the key tasks of natural language understanding. Most of sentiment analysis researches revolve around sentiment classification of subjective texts. However, research in the field of sentiment evolution analysis for complex interactive texts are notable. Sentiment evolution models the dynamics of sentiment orientation over time, it can predict the stage of event development. In this paper, we propose a sentiment evolution method based on a joint model to analyze the dynamics and interactions of individual sentiment on social media such as Weibo. The model contains two modules, sentiment encoder module based on pre-training model and time series prediction module based on Long Short-Term Memory(LSTM). We conducted experiments on real-world datasets which were crawled from Weibo. The experiment demonstrated a case study that analyzed the sentiment dynamics of topics related to COVID-19. Experimental results show that our method achieve an accuracy of 88.0%, which are about 14.7% higher than the existing methods.",science
10.1109/IJCNN.2019.8852080,to_check,2019 International Joint Conference on Neural Networks (IJCNN),IEEE,2019-07-19 00:00:00,ieeexplore,Social Network Polluting Contents Detection through Deep Learning Techniques,https://ieeexplore.ieee.org/document/8852080/,"Nowadays social networks are widespread used not only to enable users to share comments with other users but also as tool from which is possible to extract knowledge. As a matter of fact, social networks are increasingly considered to understand the opinion trend about a politician or related to a certain event that occurred: in general social networks have been proved useful to understand the public opinion from both governments and companies. In addition, also from the end users point of view it is difficult to identify real contents. This is the reason why in last years we are witnessing a growing interest in tools for analyzing big data gathered from social networks in order to find common opinions. In this context, content polluters on social networks make the opinion mining process difficult to browse valuable contents. In this paper we propose a method aimed to discriminate between pollute and real information from a semantic point of view. We exploit a combination of word embedding and deep learning techniques to categorize semantic similarities between (pollute and real) linguistic sentences. We experiment the proposed method on a dataset composed of real-world sentences gathered from the Twitter social network obtaining interesting results in terms of precision and recall.",science
10.1109/SMC.2018.00406,to_check,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",IEEE,2018-10-10 00:00:00,ieeexplore,Apparel Goods Recommender System Based on Image Shape Features Extracted by a CNN,https://ieeexplore.ieee.org/document/8616402/,"Recommender system is an information-filtering tool used in solving the problem that the user's preference in information overload. In recent years, some algorithms have been combined with some side information (i.e., item description documents, user reviews, and social networks), and rating prediction accuracy has been significantly improved. However, for fashionable goods, such as apparel and shoes that are important for designing, the contextual information of items is insufficient, and their image shape feature should be considered. Currently, no such recommender system is available to use this feature of image shape. This study proposes a novel probabilistic model using the image shape feature that integrates a convolutional neural network into the probabilistic matrix factorization. The experiment conducted on two real-world datasets corroborates that our model outperforms the other recommendation models.",science
10.1109/ICDIM.2018.8847052,to_check,2018 Thirteenth International Conference on Digital Information Management (ICDIM),IEEE,2018-09-26 00:00:00,ieeexplore,Attention Based Neural Architecture for Rumor Detection with Author Context Awareness,https://ieeexplore.ieee.org/document/8847052/,"The prevalence of social media has made information sharing possible across the globe. The downside, unfortunately, is the wide spread of misinformation. Methods applied in most previous rumor classifiers give an equal weight, or attention, to words in the microblog, and do not take the context beyond microblog contents into account; therefore, the accuracy becomes plateaued. In this research, we propose an ensemble neural architecture to detect rumor on Twitter. The architecture incorporates word attention and context from the author to enhance the classification performance. In particular, the word-level attention mechanism enables the architecture to put more emphasis on important words when constructing the text representation. To derive further context, microblog posts composed by individual authors are exploited since they can reflect style and characteristics in spreading information, which are significant cues to help classify whether the shared content is rumor or legitimate news. The experiment on the real-world Twitter dataset collected from two well-known rumor tracking websites demonstrates promising results.",science
10.1109/ICACI.2019.8778469,to_check,2019 Eleventh International Conference on Advanced Computational Intelligence (ICACI),IEEE,2019-06-09 00:00:00,ieeexplore,Intelligent Prediction of Vulnerability Severity Level Based on Text Mining and XGBboost,https://ieeexplore.ieee.org/document/8778469/,"Vulnerabilities have always been important factors threatening the security of information systems. The endless vulnerabilities pose a huge threat to the social economy and public privacy. The vulnerability database provides abundant materials for researchers to study the threat of vulnerabilities, while mining the text information of the database and obtaining valuable information can help to grasp the severity level of the vulnerability. Based on the textual description of vulnerabilities in the database, we first use text mining to extract main features. Then we utilize principal component analysis to gather sparse features which take sparse characteristic into consideration. Finally we use XGBoost to intelligently predict the severity level of vulnerabilities and compare them with the results of other machine learning methods based on same extracted features. The experiment on real-world vulnerability text description show the effectiveness of our method.",science
10.1109/TKDE.2020.2966971,to_check,IEEE Transactions on Knowledge and Data Engineering,IEEE,2021-09-01 00:00:00,ieeexplore,CAPER: Context-Aware Personalized Emoji Recommendation,https://ieeexplore.ieee.org/document/8960434/,"With the popularity of social platforms, emoji appears and becomes extremely popular with a large number of users. It expresses more beyond plaintexts and makes the content more vivid. Using appropriate emojis in messages and microblog posts makes you lovely and friendly. Recently, emoji recommendation becomes a significant task since it is hard to choose the appropriate one from thousands of emoji candidates. In this paper, we propose a Context-Aware Personalized Emoji Recommendation (CAPER) model fusing the contextual information and the personal information. It is to learn latent factors of contextual and personal information through a score-ranking matrix factorization framework. The personal factors such as user preference, user gender, and the current time can make the recommended emojis meet users' individual needs. Moreover, we consider the co-occurrence factors of the emojis which could improve the recommendation accuracy. We conduct a series of experiments on the real-world datasets, and experiment results show better performance of our model than existing methods, demonstrating the effectiveness of the considering contextual and personal factors.",science
10.1109/ACCESS.2018.2869463,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,Graph Analytics on Manycore Memory Systems,https://ieeexplore.ieee.org/document/8458129/,"Graphs are ubiquitous, and graph analytics has been widely adopted in many big data applications such as social computation and natural language processing, as well as web-search and recommendation systems. Prior research focuses on processing large-scale graphs on distributed environments or a single multi-core machine with several terabytes of RAM. Increasing complex memory systems and on-chip interconnects are developed to mitigate the data movement bottlenecks in manycore processors such as Xeon Phi KNL CPU with heterogeneous memory, with up to 72 dual-core tiles. This paper presents a detailed study on the characteristics of manycore memory systems and their impact on the efficiency of graph analytics. Based on this paper, we introduce Ants, the first graph analytics platform on manycore memory systems. First, Ants differentially allocates graph data according to their access patterns and the behavior of heterogeneous memory. Second, to reduce excessive memory access and ease congestion on interconnects and memory controllers, Ants develops a fine-grained and effective task partitioning strategy for many cores. A detailed experiment on a 64 dual-core tile machine shows that Ants outperforms the state-of-the-art graph analytics platform-Ligra by up to 8.97X for real-world graphs.",science
http://arxiv.org/abs/2106.08064v1,to_check,arxiv,arxiv,2021-06-15 11:42:05+00:00,arxiv,"Generating Contrastive Explanations for Inductive Logic Programming
  Based on a Near Miss Approach",http://arxiv.org/abs/2106.08064v1,"In recent research, human-understandable explanations of machine learning
models have received a lot of attention. Often explanations are given in form
of model simplifications or visualizations. However, as shown in cognitive
science as well as in early AI research, concept understanding can also be
improved by the alignment of a given instance for a concept with a similar
counterexample. Contrasting a given instance with a structurally similar
example which does not belong to the concept highlights what characteristics
are necessary for concept membership. Such near misses have been proposed by
Winston (1970) as efficient guidance for learning in relational domains. We
introduce an explanation generation algorithm for relational concepts learned
with Inductive Logic Programming (\textsc{GeNME}). The algorithm identifies
near miss examples from a given set of instances and ranks these examples by
their degree of closeness to a specific positive instance. A modified rule
which covers the near miss but not the original instance is given as an
explanation. We illustrate \textsc{GeNME} with the well known family domain
consisting of kinship relations, the visual relational Winston arches domain
and a real-world domain dealing with file management. We also present a
psychological experiment comparing human preferences of rule-based,
example-based, and near miss explanations in the family and the arches domains.",science
http://arxiv.org/abs/2011.14172v1,to_check,arxiv,arxiv,2020-11-28 17:25:10+00:00,arxiv,"Thermodynamic Consistent Neural Networks for Learning Material
  Interfacial Mechanics",http://arxiv.org/abs/2011.14172v1,"For multilayer materials in thin substrate systems, interfacial failure is
one of the most challenges. The traction-separation relations (TSR)
quantitatively describe the mechanical behavior of a material interface
undergoing openings, which is critical to understand and predict interfacial
failures under complex loadings. However, existing theoretical models have
limitations on enough complexity and flexibility to well learn the real-world
TSR from experimental observations. A neural network can fit well along with
the loading paths but often fails to obey the laws of physics, due to a lack of
experimental data and understanding of the hidden physical mechanism. In this
paper, we propose a thermodynamic consistent neural network (TCNN) approach to
build a data-driven model of the TSR with sparse experimental data. The TCNN
leverages recent advances in physics-informed neural networks (PINN) that
encode prior physical information into the loss function and efficiently train
the neural networks using automatic differentiation. We investigate three
thermodynamic consistent principles, i.e., positive energy dissipation,
steepest energy dissipation gradient, and energy conservative loading path. All
of them are mathematically formulated and embedded into a neural network model
with a novel defined loss function. A real-world experiment demonstrates the
superior performance of TCNN, and we find that TCNN provides an accurate
prediction of the whole TSR surface and significantly reduces the violated
prediction against the laws of physics.",science
http://arxiv.org/abs/1907.11625v5,to_check,arxiv,arxiv,2019-07-08 19:59:40+00:00,arxiv,"Influence maximization in unknown social networks: Learning Policies for
  Effective Graph Sampling",http://arxiv.org/abs/1907.11625v5,"A serious challenge when finding influential actors in real-world social
networks is the lack of knowledge about the structure of the underlying
network. Current state-of-the-art methods rely on hand-crafted sampling
algorithms; these methods sample nodes and their neighbours in a carefully
constructed order and choose opinion leaders from this discovered network to
maximize influence spread in the (unknown) complete network. In this work, we
propose a reinforcement learning framework for network discovery that
automatically learns useful node and graph representations that encode
important structural properties of the network. At training time, the method
identifies portions of the network such that the nodes selected from this
sampled subgraph can effectively influence nodes in the complete network. The
realization of such transferable network structure based adaptable policies is
attributed to the meticulous design of the framework that encodes relevant node
and graph signatures driven by an appropriate reward scheme. We experiment with
real-world social networks from four different domains and show that the
policies learned by our RL agent provide a 10-36% improvement over the current
state-of-the-art method.",science
http://arxiv.org/abs/1909.11822v1,to_check,arxiv,arxiv,2019-09-25 23:52:57+00:00,arxiv,"DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in
  Spatiotemporal Systems",http://arxiv.org/abs/1909.11822v1,"Extracting actionable insight from complex unlabeled scientific data is an
open challenge and key to unlocking data-driven discovery in science.
Complementary and alternative to supervised machine learning approaches,
unsupervised physics-based methods based on behavior-driven theories hold great
promise. Due to computational limitations, practical application on real-world
domain science problems has lagged far behind theoretical development. We
present our first step towards bridging this divide - DisCo - a
high-performance distributed workflow for the behavior-driven local causal
state theory. DisCo provides a scalable unsupervised physics-based
representation learning method that decomposes spatiotemporal systems into
their structurally relevant components, which are captured by the latent local
causal state variables. Complex spatiotemporal systems are generally highly
structured and organize around a lower-dimensional skeleton of coherent
structures, and in several firsts we demonstrate the efficacy of DisCo in
capturing such structures from observational and simulated scientific data. To
the best of our knowledge, DisCo is also the first application software
developed entirely in Python to scale to over 1000 machine nodes, providing
good performance along with ensuring domain scientists' productivity. We
developed scalable, performant methods optimized for Intel many-core processors
that will be upstreamed to open-source Python library packages. Our capstone
experiment, using newly developed DisCo workflow and libraries, performs
unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data,
processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel
Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64%
strong-scaling efficiency.",science
10.1016/j.neucom.2021.03.076,to_check,Neurocomputing,scopus,2021-08-18,sciencedirect,Enhancing social recommendation via two-level graph attentional networks,https://api.elsevier.com/content/abstract/scopus_id/85104350458,"As an effective deep representation learning technique for graph data, graph convolutional network (GCN) has recently been widely applied to obtain better embedding of vertex. The existing studies have successfully explored user-item interaction via GCN for recommendation task and proved its effectiveness. We argue that a significant limitation of these methods is that social relation, which has been proven to impose positive effects for recommendation in many loss optimization models, has received relatively little scrutiny in GCN. Thus, the resultant embeddings are insufficient to model the potential social propagation effect.
                  In our work, we propose to obtain embeddings by using the neighborhood propagation mechanism on two coupled graphs, i.e. user-item interaction graph and social relation graph, which is capable of capturing the interplay between the user’s item taste and user’s friend relationship to integrate social effect into the embeddings. In particular, to address the challenge that different factors on neighborhood propagation process make different contributions for the embedding, we develop Attentional Social Recommendation system (ASR), a new social recommendation framework with hierarchical attention(i.e., neighbor-level and graph-level attention). This allows much flexibility for adaptively acquiring relative importance for different factors. Extensive experiment on three real-world datasets not only show the superior performance of our proposed model over the baselines, but also demonstrate the effectiveness of simultaneous neighborhood propagation on two graphs.",science
10.1016/j.knosys.2021.107069,to_check,Knowledge-Based Systems,scopus,2021-08-05,sciencedirect,Pair-wise ranking based preference learning for points-of-interest recommendation,https://api.elsevier.com/content/abstract/scopus_id/85105426715,"Recommending point-of-interest (POI) to users accurately is a hot topic in business. In the past, many researchers proposed recommendation models based on collaborative filtering or matrix factorization from the perspectives of time, geography, and social relationship. However, only a few studies have focused on user preference which is the key factor influencing user decision. This work focuses on studying the representation and mining of user preference from check-in data for POI recommendation. Pair-wise ranking is the common solution for implementing preference learning. However, traditional ways of constructing pair-wise data cut off the connections between multiple options in the decision process, affecting the effectiveness of preference learning. In this work, we change the ratio of negative to positive instance in pair-wise data from 1:1 to k:1 to ensure the data construction in line with the real decision making process. We propose a new negative sampling method taking the geographical distance and POI categorical distance into consideration jointly for enhancing the quality of training data. For our specialized pair-wise data, we propose a new optimization criterion for implementing effective preference learning. Finally, we conduct extensive experiments on two real-world datasets to validate the effectiveness of our proposed approach. The experiment results show that our approach outperforms the state-of-the-art models by at least 19.7% on F1-Score and 24.4% on nDCG. Additionally, our approach can be easily generalized to other domains, such as commodities, news, and movie recommendation.",science
10.1016/j.ejor.2018.09.025,to_check,European Journal of Operational Research,scopus,2019-03-16,sciencedirect,Individual-level social influence identification in social media: A learning-simulation coordinated method,https://api.elsevier.com/content/abstract/scopus_id/85054743386,"This study develops a learning-simulation coordinated method to perform individual-level causal inference and social influence identification in social media. This method uses machine learning models to predict user adoption behavior, uses simulation to infer unobservable potential outcomes, and uses a counterfactual framework to identify individual-level social influence. The method also uses an adjusting strategy to reduce the effect of homophily and correlated unobservables. Empirical results obtained on a synthetic dataset and a semi-synthetic dataset show that the proposed method performs better on causal inference at the individual and aggregate levels than competitive methods. The computational experiment using a real-world database considers three applications, i.e., new product adoption, repeated purchase and cross selling. The empirical results show that the proposed method performs well on identifying influential members. The results reveal that the global hubs and local central nodes of the versatile friend circles have similar influences on the adoption behavior of the followers.",science
10.1016/j.neucom.2018.01.065,to_check,Neurocomputing,scopus,2018-07-05,sciencedirect,Incorporating network structure with node contents for community detection on large networks using deep learning,https://api.elsevier.com/content/abstract/scopus_id/85044381671,"Community detection is an important task in social network analysis. In community detection, in general, there exist two types of the models that utilize either network topology or node contents. Some studies endeavor to incorporate these two types of models under the framework of spectral clustering for a better community detection. However, it was not successful to obtain a big achievement since they used a simple way for the combination. To reach a better community detection, it requires to realize a seamless combination of these two methods. For this purpose, we re-examine the properties of the modularity maximization and normalized-cut models and fund out a certain approach to realize a seamless combination of these two models. These two models seek for a low-rank embedding to represent of the community structure and reconstruct the network topology and node contents, respectively. Meanwhile, we found that autoencoder and spectral clustering have a similar framework in their low- rank matrix reconstruction. Based on this property, we proposed a new approach to seamlessly combine the models of modularity and normalized-cut via the autoencoder. The proposed method also utilized the advantages of the deep structure by means of deep learning. The experiment demonstrated that the proposed method can provide a nonlinearly deep representation for a large-scale network and reached an efficient community detection. The evaluation results showed that our proposed method outperformed the existing leading methods on nine real-world networks.",science
10.1016/j.actamat.2018.04.011,to_check,Acta Materialia,scopus,2018-06-15,sciencedirect,Sacrificing trap density to achieve short-delay and high-contrast mechanoluminescence for stress imaging,https://api.elsevier.com/content/abstract/scopus_id/85046027882,"Trap-controlled mechanoluminescence (ML) enables the direct observation of stress concentration of load-bearing objects through imaging the ML distribution, showing numerous prospects in stress detection, bio-imaging and optical displays. However, the applications of trap-controlled ML materials universally require long-time delay to fade the noise of symbiotic persistent luminescence (PersL) in order to achieve high-contrast ML images. In view of the difficulty to solve the PersL problem through individually eliminating the PersL traps, herein we propose a novel strategy of sacrificing trap density which decreases PersL and ML traps as a whole. By employing Sr2+ substitution to decrease the trap density of Ca2Nb2O7:Pr3+, we identify a novel composition of (Ca0.5Sr0.5)2Nb2O7:Pr3+ displaying short-delay and high-contrast ML images, and evaluate its practicability through a 2-dimensional in-situ imaging experiment of dynamic stress distribution. The underlying mechanism is ascribed to the greater decrease ratio of PersL intensity than ML intensity as a result of the larger detrapping rate of traps due to stress (leading to ML) than that due to thermal energy (PersL). Furthermore, multi-spectral investigations of (Ca,Sr)2Nb2O7:Pr3+ system reveal a distinctive electron transition process co-regulated by trap levels, charge transfer state and crystal field. The proposed strategy and the associated phosphors are expected to initiate the reconstruction of PersL-type ML materials and bring important implications for real-world stress imaging.",science
10.1109/ROBOT.2010.5509336,to_check,2010 IEEE International Conference on Robotics and Automation,IEEE,2010-05-07 00:00:00,ieeexplore,Reinforcement learning of motor skills in high dimensions: A path integral approach,https://ieeexplore.ieee.org/document/5509336/,"Reinforcement learning (RL) is one of the most general approaches to learning control. Its applicability to complex motor systems, however, has been largely impossible so far due to the computational difficulties that reinforcement learning encounters in high dimensional continuous state-action spaces. In this paper, we derive a novel approach to RL for parameterized control policies based on the framework of stochastic optimal control with path integrals. While solidly grounded in optimal control theory and estimation theory, the update equations for learning are surprisingly simple and have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a robot dog illustrates the functionality of our algorithm in a real-world scenario. We believe that our new algorithm, Policy Improvement with Path Integrals (PI<sup>2</sup>), offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL in robotics.",robotics
10.1109/DevLrn.2012.6400585,to_check,2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL),IEEE,2012-11-09 00:00:00,ieeexplore,Simultaneous concept formation driven by predictability,https://ieeexplore.ieee.org/document/6400585/,"This study is conducted in the context of developmental learning in embodied agents who have multiple data sources (sensors) at their disposal. We describe an online learning method that simultaneously discovers “meaningful” concepts in the associated processing streams, extending methods such as PCA, SOM or sparse coding to the multimodal case. In addition to the avoidance of redundancies in the concepts derived from single modalities, we claim that “meaningful” concepts are those who have statistical relations across modalities. This is a reasonable claim because measurements by different sensors often have common cause in the external world and therefore carry correlated information. To capture such cross-modal relations while avoiding redundancy of concepts, we propose a set of interacting self-organization processes which are modulated by local predictability. To validate the fundamental applicability of the method, we conduct a plausible simulation experiment with synthetic data and find that those concepts which are predictable from other modalities successively “grow”, i.e., become over-represented, whereas concepts that are not predictable become systematically under-represented. We conclude the article by a discussion of applicability in real-world robotics scenarios.",robotics
10.1109/ICRA48506.2021.9561586,to_check,2021 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2021-06-05 00:00:00,ieeexplore,Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots,https://ieeexplore.ieee.org/document/9561586/,"Object slip perception is essential for mobile manipulation robots to perform manipulation tasks reliably in the dynamic real-world. Traditional approaches to robot arms’ slip perception use tactile or vision sensors. However, mobile robots still have to deal with noise in their sensor signals caused by the robot’s movement in a changing environment. To solve this problem, we present an anomaly detection method that utilizes multisensory data based on a deep autoencoder model. The proposed framework integrates heterogeneous data streams collected from various robot sensors, including RGB and depth cameras, a microphone, and a force-torque sensor. The integrated data is used to train a deep autoencoder to construct latent representations of the multisensory data that indicate the normal status. Anomalies can then be identified by error scores measured by the difference between the trained encoder’s latent values and the latent values of reconstructed input data. In order to evaluate the proposed framework, we conducted an experiment that mimics an object slip by a mobile service robot operating in a real-world environment with diverse household objects and different moving patterns. The experimental results verified that the proposed framework reliably detects anomalies in object slip situations despite various object types and robot behaviors, and visual and auditory noise in the environment.",robotics
10.1109/SIBGRAPI51738.2020.00016,to_check,"2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)",IEEE,2020-11-10 00:00:00,ieeexplore,HTR-Flor: A Deep Learning System for Offline Handwritten Text Recognition,https://ieeexplore.ieee.org/document/9266005/,"In recent years, Handwritten Text Recognition (HTR) has captured a lot of attention among the researchers of the computer vision community. Current state-of-the-art approaches for offline HTR are based on Convolutional Recurrent Neural Networks (CRNNs) excel at scene text recognition. Unfortunately, deep models such as CRNNs, Recur-rent Neural Networks (RNNs) are likely to suffer from vanishing/exploding gradient problems when processing long text images, which are commonly found in scanned documents. Besides, they usually have millions of parameters which require huge amount of data, and computational resource. Recently, a new class of neural net-work architecture, called Gated Convolutional Neural Networks (Gated-CNN), has demonstrated potentials to complement CRNN methods in modeling. Therefore, in this paper, we present a new architecture for HTR, based on Gated-CNN, with fewer parameters and fewer layers, which is able to outperform the current state-of-the-art architectures for HTR. The experiment validates that the proposed model has statistically significant recognition results, surpassing previous HTR systems by an average of 33% over five important handwritten benchmark datasets. Moreover, the proposed model is able to achieve satisfactory recognition rates even in case of few training data. Finally, its compact architecture requires less computational resources, which can be applied for real-world applications that have hardware limitations, such as robots and smartphones.",robotics
10.1109/ICRA.2018.8463211,to_check,2018 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2018-05-25 00:00:00,ieeexplore,Intent-Aware Multi-Agent Reinforcement Learning,https://ieeexplore.ieee.org/document/8463211/,"This paper proposes an intent-aware multi-agent planning framework as well as a learning algorithm. Under this framework, an agent plans in the goal space to maximize the expected utility. The planning process takes the belief of other agents' intents into consideration. Instead of formulating the learning problem as a partially observable Markov decision process (POMDP), we propose a simple but effective linear function approximation of the utility function. It is based on the observation that for humans, other people's intents will pose an influence on our utility for a goal. The proposed framework has several major advantages: i) it is computationally feasible and guaranteed to converge. ii) It can easily integrate existing intent prediction and low-level planning algorithms. iii) It does not suffer from sparse feedbacks in the action space. We experiment our algorithm in a real-world problem that is non-episodic, and the number of agents and goals can vary over time. Our algorithm is trained in a scene in which aerial robots and humans interact, and tested in a novel scene with a different environment. Experimental results show that our algorithm achieves the best performance and human-like behaviors emerge during the dynamic process.",robotics
10.1109/IROS.1999.813009,to_check,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),IEEE,1999-10-21 00:00:00,ieeexplore,Adaptive behavior acquisition for a distributed autonomous swimming robot based on real-world learning,https://ieeexplore.ieee.org/document/813009/,"Proposes the construction of a ""strong"" autonomous mobile robot, which can acquire environment oriented behavior through learning, as a distributed autonomous system. It is thought that such a system has many advantages over other systems in terms of adaptability to the environment and so on. However, the potential of this type of system has yet to be demonstrated in experiments under real-world conditions. We conducted an experiment to determine whether a distributed autonomous swimming robot could acquire target-approaching behavior on a water surface which was set as the robot's work space. As a result, from a fairly simple coding, the robot acquired the reproducible target-approaching behavior using only local learning even in cases where a partial fault occurred, and the acquired actions also enabled the robot to approach the target in an environment with a narrow gate.",robotics
10.1007/s41315-021-00179-y,to_check,International Journal of Intelligent Robotics and Applications,Springer,2021-05-28 00:00:00,springer,Robot grasping in dense clutter via view-based experience transfer,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41315-021-00179-y,"To perform object grasping in dense clutter, we propose a novel algorithm for grasp detection. To obtain grasp candidates, we developed instance segmentation and view-based experience transfer as part of the algorithm. Subsequently, we established an algorithm for collision avoidance and stability analysis to determine the optimal grasp for robot grasping. The strategy for the view-based experience transfer was to first find the object view and then transfer the grasp experience onto the clutter scenario. This strategy has two advantages over existing learning-based methods for finding grasp candidates. (1) our approach can effectively exclude the influence of noise or occlusion on images and precisely detect grasps that are well aligned on each target object. (2) our approach can efficiently find out optimal grasps on each target object and has the flexibility of adjusting and redefining the grasp experience based on the type of target object. We evaluated our approach using some open-source datasets and with a real-world robot experiment, which involved a six-axis robot arm with a two-jaw parallel gripper and a Kinect V2 RGB-D camera. The experimental results show that our proposed approach can be generalized to objects with complex shape, and is able to grasp on dense clutter scenarios where different types of objects are in a bin. To demonstrate our grasping pipeline, a video is provided at https://youtu.be/gQ3SO6vtTpA .",robotics
10.1109/CSCI51800.2020.00117,to_check,2020 International Conference on Computational Science and Computational Intelligence (CSCI),IEEE,2020-12-18 00:00:00,ieeexplore,Self Driving Cars: All You Need to Know,https://ieeexplore.ieee.org/document/9458101/,"Self-driving cars are coming closer and closer to being fact not fiction, but are we ready for them? In this research we analyze the current status in place for self-driving cars. We address the gaps that need to be filled, and we identify the questions that need to be answered before having self-driving cars on the road becomes a reality. Towards this, we discuss four issues with self-driving: policies, safety, security, and psychological acceptability of users. Our research will help individuals understand different aspects of self-driving cars and their benefits and challenges. Our paper will educate policymakers on the areas that need to be addressed before we deploy self-driving cars on the roads in a larger scale.",autonomous vehicle
10.1007/978-3-030-90963-5_36,to_check,"HCI International 2021 - Late Breaking Papers: Multimodality, eXtended Reality, and Artificial Intelligence",Springer,2021-01-01 00:00:00,springer,Applying Human Cognition to Assured Autonomy,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-90963-5_36,"The scaled deployment of semi- and fully autonomous systems undeniably depends on assured autonomy. This reality, however, has become far more complex than expected because it necessarily demands an integrated tripartite solution not yet achieved: consensus-based standards and compliance across industry, scientific innovation within artificial intelligence R&D of explainability, and robust end-user education. In this is paper I present my human-centered approach to the design, development, and deployment of autonomous systems and break down how human factors such as cognitive and behavioral insights into how we think, feel, act, plan, make decisions, and problem-solve are foundational to assuring autonomy.",autonomous vehicle
10.1109/ACCESS.2021.3103680,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Human Centric Digital Transformation and Operator 4.0 for the Oil and Gas Industry,https://ieeexplore.ieee.org/document/9509417/,"Working at an oil and gas facility, such as a drilling rig, production facility, processing facility, or storage facility, involves various challenges, including health and safety risks. It is possible to leverage emerging digital technologies such as smart sensors, wearable or mobile devices, big data analytics, cloud computing, extended reality technologies, robotic systems, and drones to mitigate the challenges faced by oil and gas workers. While these technologies are not new to the oil and gas industry, most of its existing digital transformation initiatives follow business or process-centric approaches, in which the critical driver of the technology adoption is the enhancement of production, efficiency, and revenue. As a result, they may not address the challenges faced by the workers. As oil and gas workers are among the essential assets in the oil and gas industry, it is vital to address the challenges faced by these workers. This paper proposes a human-centric digital transformational framework for the oil and gas industry to deploy existing digital technologies to enhance their workers' health, safety, and working conditions. The paper outlines the critical challenges faced by oilfield workers, introduces a system architecture to implements a human-centric digital transformation, discusses the opportunities of the proposed framework, and summarizes the key impediment for the proposed framework.",health
10.1016/j.ejso.2020.04.010,to_check,European Journal of Surgical Oncology,scopus,2021-02-01,sciencedirect,Peroperative personalised decision support and analytics for colon cancer surgery- Short report,https://api.elsevier.com/content/abstract/scopus_id/85083856433,"Advanced instrumentation whether robotic or non-robotic- hasn't itself made for better surgery as all critical measures of operative success depend still on intraoperative surgeon judgement and decision-making. Computer assisted surgery, or digital surgery, refers to the combination of technology with real-time data during an operation and is often assumed to need new hardware platforms to become a reality. However, methods to support personalised surgical endeavour exist now and can be deployed today within standard laparoscopic paradigms. Here we describe in detail the rationale for the deployment of such assistance for surgical step-advancement in our current practice evolution from traditional proximal colon cancer resection to complete mesocolic excision focussing on personalised 3d anatomical display, intraoperative, quantificative fluorescence assessment of intracorporeal anastomoses and postoperative digital feedback to enable reflection and identify areas of technical improvement.",health
10.1109/IWOBI47054.2019.9114431,to_check,2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI),IEEE,2019-07-05 00:00:00,ieeexplore,Proof of Concept: Using Reinforcement Learning agent as an adversary in Serious Games,https://ieeexplore.ieee.org/document/9114431/,"This article focuses on simple rehabilitation video-game called Flying with Friends. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion entertainment industry in the form of computer games. The goal of this experiment was to provide a proof that such systems are viable option when it comes to artificial intelligence systems in serious video-games, but not limited to only serious ones. The solution described in this article, in cooperation with experts, is going to be deployed in a real rehabilitation environment.",industry
10.1109/ICCCN52240.2021.9522281,to_check,2021 International Conference on Computer Communications and Networks (ICCCN),IEEE,2021-07-22 00:00:00,ieeexplore,Realization of an Intrusion Detection use-case in ONAP with Acumos,https://ieeexplore.ieee.org/document/9522281/,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",industry
10.1109/ISIE45063.2020.9152441,to_check,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),IEEE,2020-06-19 00:00:00,ieeexplore,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,https://ieeexplore.ieee.org/document/9152441/,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",industry
10.23919/JCN.2020.100039,to_check,Journal of Communications and Networks,KICS,2020-12-01 00:00:00,ieeexplore,Special issue on 6G wireless systems,https://ieeexplore.ieee.org/document/9321190/,"While 5G is currently being deployed around the globe, research on 6G is under way aiming at addressing the coming challenges of drastic increase of wireless data traffic and support of other usage scenarios. 6G is expected to extend 5G capabilities even further. Higher bitrates (up to Tbps) and lower latency (less than 1ms) will allow introducing new services — such as pervasive edge intelligence, ultra-massive machine-type communications, extremely reliable low-latency communications, holographic rendering and high-precision communications — and meet more stringent requirements, especially in the following dimensions: energy efficiency; intelligence; spectral efficiency; security, secrecy and privacy; affordability; and customization. Artificial intelligence approaches and techniques, such as machine learning (of which deep learning and reinforcement learning are specific examples), and machine reasoning (which includes planning, scheduling, knowledge representation and reasoning, search and optimization), are the new fundamental enablers to operate networks more efficiently, enhance the overall end user experience and provide innovative service applications. Quantum Optics Computing (QOC) and Quantum Key Distribution (QKD) are almost ready for industrial applications. In particular, massive Internet of Things (mIoT), Industrial IoT (IloT), fully automated robotic platforms (which include control, perception, sensors and actuators, as well as the integration of other techniques into cyber-physical systems), vehicles and multisensory extended reality are examples of the new data-demanding applications, which will impose new performance targets and motivate 6G design and deployment.",industry
http://arxiv.org/abs/2103.03544v2,to_check,arxiv,arxiv,2021-03-05 08:52:31+00:00,arxiv,Challenges of engineering safe and secure highly automated vehicles,http://arxiv.org/abs/2103.03544v2,"After more than a decade of intense focus on automated vehicles, we are still
facing huge challenges for the vision of fully autonomous driving to become a
reality. The same ""disillusionment"" is true in many other domains, in which
autonomous Cyber-Physical Systems (CPS) could considerably help to overcome
societal challenges and be highly beneficial to society and individuals. Taking
the automotive domain, i.e. highly automated vehicles (HAV), as an example,
this paper sets out to summarize the major challenges that are still to
overcome for achieving safe, secure, reliable and trustworthy highly automated
resp. autonomous CPS. We constrain ourselves to technical challenges,
acknowledging the importance of (legal) regulations, certification,
standardization, ethics, and societal acceptance, to name but a few, without
delving deeper into them as this is beyond the scope of this paper. Four
challenges have been identified as being the main obstacles to realizing HAV:
Realization of continuous, post-deployment systems improvement, handling of
uncertainties and incomplete information, verification of HAV with machine
learning components, and prediction. Each of these challenges is described in
detail, including sub-challenges and, where appropriate, possible approaches to
overcome them. By working together in a common effort between industry and
academy and focusing on these challenges, the authors hope to contribute to
overcome the ""disillusionment"" for realizing HAV.",industry
10.1016/j.comcom.2020.01.018,to_check,Computer Communications,scopus,2020-02-01,sciencedirect,Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT,https://api.elsevier.com/content/abstract/scopus_id/85077781443,"The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.",industry
10.1109/RE.2019.00009,to_check,2019 IEEE 27th International Requirements Engineering Conference (RE),IEEE,2019-09-27 00:00:00,ieeexplore,Requirements We Live By,https://ieeexplore.ieee.org/document/8920530/,"Enlightened requirements engineering (RE) researchers and practitioners generally accept that RE is as much about understanding the world as it is about understanding the software and systems that will be built to inhabit that world. As a result, the RE field has fostered a multi-disciplinary following of researchers and practitioners who are prepared to engage deeply in application domains, to apply a range of technical and socio-technical skills to understand those domains, and to accept that the outcome of an effective RE process may not deliver a software system at all. The RE community has also developed, deployed, and evaluated a wide range of contributions that reflect such enlightenment: conceptual models that reflect the relationships between the world and the machine, domain models and scenarios that reflect understandings of problem domains, and enterprise models that reflect the organisations and processes that build and deploy systems. All these in addition to the models that capture the all-important behaviour of systems and software. It seems to me however that the RE discipline is at a crossroads. The mechanics of the discipline appear to be established - much of the published research is now empirical - or technical, but only in so far as it responds to technological advances elsewhere, such as mobile and ubiquitous technologies represented by the Internet of Things, richer application domains such as Industrie 4.0 and Smart Cities, or more advanced computational techniques that are maturing, such AI, machine learning, and blockchains. As a community, we reassure ourselves that our discipline is safe and thriving, after all RE is a “forever problem”: all systems we wish to build will have requirements, now and forever. But this is to be complacent. RE has no protected status to study and deploy requirements. The formal models we elicit, design, and build are increasingly deployable by other disciplines, as are the values that we seek our modern, AI-driven systems to embody. A new and potentially radical re-framing of our discipline may be needed, and I will speculate what this may look like. It may require letting go of what we have considered to be the boundaries of our discipline, while embracing new but fluid boundaries. I have advocated and explored “software without boundaries” as one such framing that challenges the separation of `world and the machine', not because I don't accept the separation of the `what' and the `how', the `indicative' and the `optative', or the `problem' and the `solution', but because the world we live in no longer accepts these separations. Society, more often than not, does not think of systems, of technology, or indeed of software; it thinks of ways of working, ways of interacting, ways of living. Requirements, such as they are, are `requirements we live by' not requirements of systems in the world. At an extreme, if one believes the AI hype, `the world and the machine' will increasingly be replaced by the `world in the machine'. Where does the RE community stand on this, and what can this community do to contribute to the framing and solving of this new reality? My own work in recent years has evolved to reflect the above. I still revisit, with some pride, the `RE Roadmap' that Steve Easterbrook and I published in 2000 - many of the fundamental RE principles we presented still hold today. But I cringe at how we missed the changing nature of the world in which we operate: a world populated by autonomous and adaptive systems, populated by big data and associated analytics, and populated by stakeholders whose multiple perspectives reflect a multitude of ethical and social values, not all of which are wholesome, and many of which are actively subversive or malicious. My own research on security and privacy requirements only scratches the surface of this evolving reality. I invite the RE community to reflect on how it frames its own research in this context.",smart cities
10.1109/TNSM.2020.3034482,to_check,IEEE Transactions on Network and Service Management,IEEE,2020-12-01 00:00:00,ieeexplore,Mobility Management With Transferable Reinforcement Learning Trajectory Prediction,https://ieeexplore.ieee.org/document/9244233/,"Future mobile networks will enable the massive deployment of mobile multimedia applications anytime and anywhere. In this context, mobility management schemes, such as handover and proactive multimedia service migration, will be essential to improve network performance. In this article, we propose a proactive mobility management approach based on group user trajectory prediction. Specifically, we introduce a mobile user trajectory prediction algorithm by combining the Long-Short Term Memory networks (LSTM) with Reinforcement Learning (RL) to automate the model training procedure. We further develop a group user trajectory predictor to reduce prediction calculation overheads of users with similar movement patterns. To validate the impact of the proposed mobility management approach, we present a virtual reality (VR) service migration scheme built on the top of the proactive handover mechanism that benefits from trajectory predictions. Experiment results validate our predictor's outstanding accuracy and its impacts on enhancing handover and service migration performance to provide quality of service assurance.",smart cities
10.1007/978-3-030-28925-6_1,to_check,3rd EAI International Conference on IoT in Urban Space,Springer,2020-01-01 00:00:00,springer,CityFlow: Supporting Spatial-Temporal Edge Computing for Urban Machine Learning Applications,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-28925-6_1,"A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities.",smart cities
http://arxiv.org/abs/2102.12165v1,to_check,arxiv,arxiv,2021-02-24 09:36:39+00:00,arxiv,"Efficient Low-Latency Dynamic Licensing for Deep Neural Network
  Deployment on Edge Devices",http://arxiv.org/abs/2102.12165v1,"Along with the rapid development in the field of artificial intelligence,
especially deep learning, deep neural network applications are becoming more
and more popular in reality. To be able to withstand the heavy load from
mainstream users, deployment techniques are essential in bringing neural
network models from research to production. Among the two popular computing
topologies for deploying neural network models in production are
cloud-computing and edge-computing. Recent advances in communication
technologies, along with the great increase in the number of mobile devices,
has made edge-computing gradually become an inevitable trend. In this paper, we
propose an architecture to solve deploying and processing deep neural networks
on edge-devices by leveraging their synergy with the cloud and the
access-control mechanisms of the database. Adopting this architecture allows
low-latency DNN model updates on devices. At the same time, with only one model
deployed, we can easily make different versions of it by setting access
permissions on the model weights. This method allows for dynamic model
licensing, which benefits commercial applications.",smart cities
http://arxiv.org/abs/2004.06049v2,to_check,arxiv,arxiv,2020-04-09 06:36:18+00:00,arxiv,"A Prospective Look: Key Enabling Technologies, Applications and Open
  Research Topics in 6G Networks",http://arxiv.org/abs/2004.06049v2,"The fifth generation (5G) mobile networks are envisaged to enable a plethora
of breakthrough advancements in wireless technologies, providing support of a
diverse set of services over a single platform. While the deployment of 5G
systems is scaling up globally, it is time to look ahead for beyond 5G systems.
This is driven by the emerging societal trends, calling for fully automated
systems and intelligent services supported by extended reality and haptics
communications. To accommodate the stringent requirements of their prospective
applications, which are data-driven and defined by extremely low-latency,
ultra-reliable, fast and seamless wireless connectivity, research initiatives
are currently focusing on a progressive roadmap towards the sixth generation
(6G) networks. In this article, we shed light on some of the major enabling
technologies for 6G, which are expected to revolutionize the fundamental
architectures of cellular networks and provide multiple homogeneous artificial
intelligence-empowered services, including distributed communications, control,
computing, sensing, and energy, from its core to its end nodes. Particularly,
this paper aims to answer several 6G framework related questions: What are the
driving forces for the development of 6G? How will the enabling technologies of
6G differ from those in 5G? What kind of applications and interactions will
they support which would not be supported by 5G? We address these questions by
presenting a profound study of the 6G vision and outlining five of its
disruptive technologies, i.e., terahertz communications, programmable
metasurfaces, drone-based communications, backscatter communications and
tactile internet, as well as their potential applications. Then, by leveraging
the state-of-the-art literature surveyed for each technology, we discuss their
requirements, key challenges, and open research problems.",smart cities
http://arxiv.org/abs/1404.1905v1,to_check,arxiv,arxiv,2014-04-04 17:31:17+00:00,arxiv,Developing a 21st Century Global Library for Mathematics Research,http://arxiv.org/abs/1404.1905v1,"Developing a 21st Century Global Library for Mathematics Research discusses
how information about what the mathematical literature contains can be
formalized and made easier to express, encode, and explore. Many of the tools
necessary to make this information system a reality will require much more than
indexing and will instead depend on community input paired with machine
learning, where mathematicians' expertise can fill the gaps of automatization.
This report proposes the establishment of an organization; the development of a
set of platforms, tools, and services; the deployment of an ongoing applied
research program to complement the development work; and the mobilization and
coordination of the mathematical community to take the first steps toward these
capabilities. The report recommends building on the extensive work done by many
dedicated individuals under the rubric of the World Digital Mathematical
Library, as well as many other community initiatives. Developing a 21st Century
Global Library for Mathematics envisions a combination of machine learning
methods and community-based editorial effort that makes a significantly greater
portion of the information and knowledge in the global mathematical corpus
available to researchers as linked open data through a central organizational
entity-referred to in the report as the Digital Mathematics Library. This
report describes how such a library might operate - discussing development and
research needs, role in facilitating discover and interaction, and establishing
partnerships with publishers.",smart cities
10.1109/ECAI.2017.8166500,to_check,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2017-07-01 00:00:00,ieeexplore,System using a hybrid application for virtual reality 3D drawing,https://ieeexplore.ieee.org/document/8166500/,"Smart devices have many sensors that capture the user data and use it for device control, improving the user experience, or send it through a computer network in order to be processed. The interaction between user and applications, especially in virtual reality, is made via motion and gestures with information offered by sensors such as accelerometer, compass or gyroscope. In order to reduce the complexity of developing this type of applications, it is possible to write hybrid applications that allow the programmer to code using web technologies then deploy and run the resulted applications natively on multiple operating systems. This paper presents the implementation of a virtual reality drawing hybrid application, created with the Telerik Mobile platform, which reads the compass and accelerometer data and sends the information to a server that creates a 3D representation using the Processing Development Environment.",multimedia
10.1109/HPCA51647.2021.00016,to_check,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),IEEE,2021-03-03 00:00:00,ieeexplore,Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,https://ieeexplore.ieee.org/document/9407116/,"Emerging AI-enabled applications such as augmented and virtual reality (AR/VR) leverage multiple deep neural network (DNN) models for various sub-tasks such as object detection, image segmentation, eye-tracking, speech recognition, and so on. Because of the diversity of the sub-tasks, the layers within and across the DNN models are highly heterogeneous in operation and shape. Diverse layer operations and shapes are major challenges for a fixed dataflow accelerator (FDA) that employs a fixed dataflow strategy on a single DNN accelerator substrate since each layer prefers different dataflows (computation order and parallelization) and tile sizes. Reconfigurable DNN accelerators (RDAs) have been proposed to adapt their dataflows to diverse layers to address the challenge. However, the dataflow flexibility in RDAs is enabled at the cost of expensive hardware structures (switches, interconnects, controller, etc.) and requires per-layer reconfiguration, which introduces considerable energy costs. Alternatively, this work proposes a new class of accelerators, heterogeneous dataflow accelerators (HDAs), which deploy multiple accelerator substrates (i.e., sub-accelerators), each supporting a different dataflow. HDAs enable coarser-grained dataflow flexibility than RDAs with higher energy efficiency and lower area cost comparable to FDAs. To exploit such benefits, hardware resource partitioning across sub-accelerators and layer execution schedule need to be carefully optimized. Therefore, we also present Herald, a framework for co-optimizing hardware partitioning and layer scheduling. Using Herald on a suite of AR/VR and MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which demonstrates 65.3% lower latency and 5.0% lower energy compared to the best fixed dataflow accelerators and 22.0% lower energy at the cost of 20.7% higher latency compared to a state-of-the-art reconfigurable DNN accelerator (RDA). The results suggest that HDA is an alternative class of Pareto-optimal accelerators to RDA with strength in energy, which can be a better choice than RDAs depending on the use cases.",multimedia
10.1109/ICMLA51294.2020.00193,to_check,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),IEEE,2020-12-17 00:00:00,ieeexplore,An embedded deep learning system for augmented reality in firefighting applications,https://ieeexplore.ieee.org/document/9356175/,"Firefighting is a dynamic activity, in which numerous operations occur simultaneously. Maintaining situational awareness (i.e., knowledge of current conditions and activities at the scene) is critical to the accurate decision-making necessary for the safe and successful navigation of a fire environment by firefighters. Conversely, the disorientation caused by hazards such as smoke and extreme heat can lead to injury or even fatality. This research implements recent advancements in technology such as deep learning, point cloud and thermal imaging, and augmented reality platforms to improve a firefighter's situational awareness and scene navigation through improved interpretation of that scene. We have designed and built a prototype embedded system that can leverage data streamed from cameras built into a firefighter's personal protective equipment (PPE) to capture thermal, RGB color, and depth imagery and then deploy already developed deep learning models to analyze the input data in real time. The embedded system analyzes and returns the processed images via wireless streaming, where they can be viewed remotely and relayed back to the firefighter using an augmented reality platform that visualizes the results of the analyzed inputs and draws the firefighter's attention to objects of interest, such as doors and windows otherwise invisible through smoke and flames.",multimedia
10.1109/ISCAIE.2018.8405457,to_check,2018 IEEE Symposium on Computer Applications & Industrial Electronics (ISCAIE),IEEE,2018-04-29 00:00:00,ieeexplore,Augmented reality enhanced computer aided learning for young children,https://ieeexplore.ieee.org/document/8405457/,"Learning to write can be exhausting for young children. In Traditional teaching, children with a different learning abilities are taught with the same rubric. This, in turn, impacts children that need extra attention to catch up with their pairs, which leads children to suffer right from the early learning stages. Traditional teaching methods also are so rigid that makes them unable to automatically identify those children with less abilities and in need of extra work. Hence, with the rapid development of ICT, an innovative learning methods are sought to be important to allow children to be taught with different rubrics. The aim of this research is to improve learning process for pre-school children via introducing Augmented Reality (AR) in the process which, in turn, simplify the learning process as well as identifying children abilities. The research introduces gamification to the process in order to ease the burden on children. Furthermore, we are trying to involve both school as well home to be part of the educational cycle that makes parents to be part of the learning/educational process of their young children. Augmented reality combined with pleasing sound make the learning more interactive and enjoyable. The outcome of this research also helps parents to keep track of their children's learning. The paper also describes the deployment of the application in a local schools as a pilot study so teachers can get feedback on student's learning curve and to fine tune the work further.",multimedia
10.1109/MILTECHS.2017.7988855,to_check,2017 International Conference on Military Technologies (ICMT),IEEE,2017-06-02 00:00:00,ieeexplore,Challenges in the implementation of autonomous systems into the battlefield,https://ieeexplore.ieee.org/document/7988855/,"The era of a system with some level of autonomous capacity is here for long time, however the deployment of fully autonomous system in military domain is still challenging. The main limitation in the AS operationalization are mentioned and proposals to these challenges are discussed. The ontology of the AS operationalization is elaborated and associations among the main terms is enlisted. The modeling and simulation experimental framework is designed to create a testing and verification environment for AS deployment. The key factor is to include standards of the robotic family and modeling and simulation as well. The standardized scenarios approach for AS deployment is mentioned. The perception of the AS is discussed and the idea of the augmented reality for autonomous system is introduced and related to the human behavior.",multimedia
10.1109/COMPSAC.2018.00152,to_check,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),IEEE,2018-07-27 00:00:00,ieeexplore,mmCNN: A Novel Method for Large Convolutional Neural Network on Memory-Limited Devices,https://ieeexplore.ieee.org/document/8377777/,"Deep learning recently has been widely used in many interactive application fields including but not limited to object recognition, speech recognition, natural language processing and so on. At the same time more and more attractive interactive applications (face recognition and augmented reality) are available on wearable and mobile devices. However, traditional deep learning methods such as CNN cost a lot of memory resources. This challenge makes it difficult to apply the powerful deep learning method on mobile memory limited platforms. In this paper we present a novel memory management strategy called mmCNN to solve this problem. This method helps us deploy a trained large size CNN on an any memory size platform including GPU, FPGA and memory-limited mobile devices. In our experiments, we run a feed-forward CNN process in an extremely small memory size (as low as 5MB) on a GPU platform. The result shows that our method saves more than 98% memory compared to a traditional CNN algorithm and further saves more than 90% compared to the sate-of-the-art related work ""vDNN"". Our work improve the computing scalability of interaction applications and break the memory bottleneck of using deep learning method on a memory-limited devices.",multimedia
10.1109/ICRA40945.2020.9197465,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning,https://ieeexplore.ieee.org/document/9197465/,"DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub<sup>2</sup>.",multimedia
10.1007/978-3-030-58197-8_7,to_check,5G and Beyond,Springer,2021-01-01 00:00:00,springer,6G Wireless Systems: Challenges and Opportunities,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58197-8_7,"The ongoing deployment of 5G cellular systems is continuously exposing the inherent limitations of this system, compared to its original premise as an enabler for Internet of Everything applications. These 5G drawbacks are spurring worldwide activities focused on defining the next-generation 6G wireless system that can truly integrate far-reaching applications ranging from autonomous systems to extended reality. To date, the fundamental architectural and performance components of 6G remain largely undefined and open to speculations. In this chapter, we present a holistic vision that identifies the main principles that can guide the design and development of a 6G system. In particular, we discuss, in detail, why 6G will not be a simple exploration of more spectrum at high-frequency bands such as terahertz frequencies, but it will rather be a convergence of a number of technological trends. 6G will also be largely driven by a new breed of exciting Internet of Everything services. To this end, we first outline the primary drivers of 6G systems, in terms of applications and accompanying technological trends. Then, we introduce a new set of service classes and expose their target 6G performance requirements. We then explore the enabling technologies for the introduced 6G services and define a comprehensive set of research problems that come hand in hand with the identified technologies. We conclude by providing our observations on the challenges and opportunities that will define the road toward 6G. Ultimately, this chapter can be used to stimulate comprehensive, out-of-the-box research around 6G technologies.",multimedia
10.1007/s00586-019-06054-6,to_check,European Spine Journal,Springer,2020-07-01 00:00:00,springer,Augmented reality and artificial intelligence-based navigation during percutaneous vertebroplasty: a pilot randomised clinical trial,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00586-019-06054-6,"Purpose To assess technical feasibility, accuracy, safety and patient radiation exposure of a novel navigational tool integrating augmented reality (AR) and artificial intelligence (AI), during percutaneous vertebroplasty of patients with vertebral compression fractures (VCFs). Material and methods This prospective parallel randomised open trial compared the trans-pedicular access phase of percutaneous vertebroplasty across two groups of 10 patients, electronically randomised, with symptomatic single-level VCFs. Trocar insertion was performed using AR/AI-guidance with motion compensation in Group A, and standard fluoroscopy in Group B. The primary endpoint was technical feasibility in Group A. Secondary outcomes included the comparison of Groups A and B in terms of accuracy of trocar placement (distance between planned/actual trajectory on sagittal/coronal fluoroscopic images); complications; time for trocar deployment; and radiation dose/fluoroscopy time. Results Technical feasibility in Group A was 100%. Accuracy in Group A was 1.68 ± 0.25 mm (skin entry point), and 1.02 ± 0.26 mm (trocar tip) in the sagittal plane, and 1.88 ± 0.28 mm (skin entry point) and 0.86 ± 0.17 mm (trocar tip) in the coronal plane, without any significant difference compared to Group B ( p  > 0.05). No complications were observed in the entire population. Time for trocar deployment was significantly longer in Group A (642 ± 210 s) than in Group B (336 ± 60 s; p  = 0.001). Dose–area product and fluoroscopy time were significantly lower in Group A (182.6 ± 106.7 mGy cm^2 and 5.2 ± 2.6 s) than in Group B (367.8 ± 184.7 mGy cm^2 and 10.4 ± 4.1 s; p  = 0.025 and 0.005), respectively. Conclusion AR/AI-guided percutaneous vertebroplasty appears feasible, accurate and safe, and facilitates lower patient radiation exposure compared to standard fluoroscopic guidance. Graphic abstract These slides can be retrieved under Electronic Supplementary Material.",multimedia
http://arxiv.org/abs/2009.10679v1,to_check,arxiv,arxiv,2020-09-22 16:55:44+00:00,arxiv,"An embedded deep learning system for augmented reality in firefighting
  applications",http://arxiv.org/abs/2009.10679v1,"Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.",multimedia
http://arxiv.org/abs/2012.10342v3,to_check,arxiv,arxiv,2020-12-18 16:40:32+00:00,arxiv,Exploring and Interrogating Astrophysical Data in Virtual Reality,http://arxiv.org/abs/2012.10342v3,"Scientists across all disciplines increasingly rely on machine learning
algorithms to analyse and sort datasets of ever increasing volume and
complexity. Although trends and outliers are easily extracted, careful and
close inspection will still be necessary to explore and disentangle detailed
behavior, as well as identify systematics and false positives. We must
therefore incorporate new technologies to facilitate scientific analysis and
exploration. Astrophysical data is inherently multi-parameter, with the
spatial-kinematic dimensions at the core of observations and simulations. The
arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as
well as the availability of versatile development tools for video games, has
enabled scientists to deploy such technology to effectively interrogate and
interact with complex data. In this paper we present development and results
from custom-built interactive VR tools, called the iDaVIE suite, that are
informed and driven by research on galaxy evolution, cosmic large-scale
structure, galaxy-galaxy interactions, and gas/kinematics of nearby galaxies in
survey and targeted observations. In the new era of Big Data ushered in by
major facilities such as the SKA and LSST that render past analysis and
refinement methods highly constrained, we believe that a paradigm shift to new
software, technology and methods that exploit the power of visual perception,
will play an increasingly important role in bridging the gap between
statistical metrics and new discovery. We have released a beta version of the
iDaVIE software system that is free and open to the community.",multimedia
http://arxiv.org/abs/1911.01562v1,to_check,arxiv,arxiv,2019-11-05 01:40:42+00:00,arxiv,"DeepRacer: Educational Autonomous Racing Platform for Experimentation
  with Sim2Real Reinforcement Learning",http://arxiv.org/abs/1911.01562v1,"DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ.",multimedia
10.1016/j.ascom.2021.100502,to_check,Astronomy and Computing,scopus,2021-10-01,sciencedirect,Exploring and interrogating astrophysical data in virtual reality,https://api.elsevier.com/content/abstract/scopus_id/85116926035,"Scientists across all disciplines increasingly rely on machine learning algorithms to analyse and sort datasets of ever increasing volume and complexity. Although trends and outliers are easily extracted, careful and close inspection will still be necessary to explore and disentangle detailed behaviour, as well as identify systematics and false positives. We must therefore incorporate new technologies to facilitate scientific analysis and exploration. Astrophysical data is inherently multi-parameter, with the spatial-kinematic dimensions at the core of observations and simulations. The arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as well as the availability of versatile development tools for video games, has enabled scientists to deploy such technology to effectively interrogate and interact with complex data. In this paper we present development and results from custom-built interactive VR tools, called the iDaVIE suite, that are informed and driven by research on galaxy evolution, cosmic large-scale structure, galaxy–galaxy interactions, and gas/kinematics of nearby galaxies in survey and targeted observations. In the new era of Big Data ushered in by major facilities such as the SKA and LSST that render past analysis and refinement methods highly constrained, we believe that a paradigm shift to new software, technology and methods that exploit the power of visual perception, will play an increasingly important role in bridging the gap between statistical metrics and new discovery. We have released a beta version of the iDaVIE software system that is free and open to the community.",multimedia
http://arxiv.org/abs/2110.15245v1,to_check,arxiv,arxiv,2021-10-28 16:04:01+00:00,arxiv,"From Machine Learning to Robotics: Challenges and Opportunities for
  Embodied Intelligence",http://arxiv.org/abs/2110.15245v1,"Machine learning has long since become a keystone technology, accelerating
science and applications in a broad range of domains. Consequently, the notion
of applying learning methods to a particular problem set has become an
established and valuable modus operandi to advance a particular field. In this
article we argue that such an approach does not straightforwardly extended to
robotics -- or to embodied intelligence more generally: systems which engage in
a purposeful exchange of energy and information with a physical environment. In
particular, the purview of embodied intelligent agents extends significantly
beyond the typical considerations of main-stream machine learning approaches,
which typically (i) do not consider operation under conditions significantly
different from those encountered during training; (ii) do not consider the
often substantial, long-lasting and potentially safety-critical nature of
interactions during learning and deployment; (iii) do not require ready
adaptation to novel tasks while at the same time (iv) effectively and
efficiently curating and extending their models of the world through targeted
and deliberate actions. In reality, therefore, these limitations result in
learning-based systems which suffer from many of the same operational
shortcomings as more traditional, engineering-based approaches when deployed on
a robot outside a well defined, and often narrow operating envelope. Contrary
to viewing embodied intelligence as another application domain for machine
learning, here we argue that it is in fact a key driver for the advancement of
machine learning technology. In this article our goal is to highlight
challenges and opportunities that are specific to embodied intelligence and to
propose research directions which may significantly advance the
state-of-the-art in robot learning.",science
10.1016/B978-0-12-816176-0.00045-4,to_check,Handbook of Medical Image Computing and Computer Assisted Intervention,scopus,2019-01-01,sciencedirect,Challenges in computer assisted interventions,https://api.elsevier.com/content/abstract/scopus_id/85082596227,"Challenges in design, implementation, clinical evaluation, and deployment of computer assisted intervention solutions are manifold. Some of these challenges will be discussed in this chapter.
               Computer assistance in both surgical procedures and radiology interventions aim at augmenting the clinicians with the overall objective of providing better clinical outcome. Multimodal imaging, robotics, artificial intelligence, and augmented reality play a major role in computer assisted interventions. After a brief analysis of the state-of-the-art and practice in this field, we discuss the challenges in design and development, as well as translation and deployment of the technology, from research projects motivated by clinical needs to solutions routinely used within clinical setups. We also consider the required training of surgeons and the surgical team as a major component for smooth and successful translation. We present simulation as an important tool not only for the design and development of computer assisted intervention solutions but also in their fast and smooth translation into daily practice.",science
10.1109/ICDMW.2019.00123,to_check,2019 International Conference on Data Mining Workshops (ICDMW),IEEE,2019-11-11 00:00:00,ieeexplore,Implementation of Mobile-Based Real-Time Heart Rate Variability Detection for Personalized Healthcare,https://ieeexplore.ieee.org/document/8955523/,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework.",health
10.1007/978-3-030-70761-3_9,to_check,Precision Medicine in Stroke,Springer,2021-01-01 00:00:00,springer,Future Application: Prognosis Determination,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-70761-3_9,"Precision medicine represents the future for improving outcome determination. Expanding precision medicine to the stroke area in order to identify prognostic markers is therefore crucial although challenging. The implementation of this concept in clinical practice is still far from being a reality and healthcare disparities could limit the acceleration of the translation from the science of precision medicine into clinical practice around the world. To achieve this goal, new approaches and strategies along with novel technologies, informatics, and identification of practical clinical paradigms need to be implemented. The great amount of data coming from clinical trials; routine care; databases; registries, including clinical, cognitive, and advanced neuroimaging data; omics (genomics/transcriptomics/proteomics/metabolomics); and clot composition analysis should be shared across centers, collected using standardized methods and a high-quality big data approach, and made available as a fertile ground for future studies. Furthermore, the use of artificial intelligence could allow the development of algorithms that, if validated, may guide stroke physicians in more precisely tailoring decision-making processes regarding patient selection and prognosis determination for each individual patient. This obviously needs expertise and a multidisciplinary approach including stroke clinicians and both clinical and basic researchers, but also data scientists, omics specialists, biostatisticians for adopting and implementing adequate statistical methodologies, epidemiologists, computer scientists, engineers, and experts in advanced analytics and artificial intelligence. Multicenter collaborative efforts should be put in place through the establishment of consortia and of adequate infrastructure for a proper and standardized data collection. What is certain is that a philosophic and paradigm shift in the stroke community and a transition to a superior management level should occur because individualized treatments based on prognosis prediction models incorporating precision medicine-based variables represent the real challenge for the future and the next frontier.",health
10.1093/iwc/iwv010,to_check,Interacting with Computers,OUP,2015-09-01 00:00:00,ieeexplore,Neuro-Fuzzy Physiological Computing to Assess Stress Levels in Virtual Reality Therapy,https://ieeexplore.ieee.org/document/8155481/,"This paper reports the design and assessment of a neuro-fuzzy model to support clinicians during virtual reality therapy. The implemented model is able to automatically recognize the perceived stress levels of the patients by analyzing physiological and behavioral data during treatment. The model, consisting of a self-organizing map and a fuzzy-rule-based module, was trained unobtrusively recording electrocardiogram, breath rate and activity during stress inoculation provided by the exposure to virtual environments. Twenty nurses were exposed to sessions simulating typical stressful situations experienced at their workplace. Four levels of stress severity were evaluated for each subject by gold standard clinical scales administered by trained personnel. The model's performances were discussed and compared with the main machine learning algorithms. The neuro-fuzzy model shows better performances in terms of stress level classification with 83% of mean recognition rate.RESEARCH HIGHLIGHTS Stress levels were predicted on the basis of physiological computing using a neuro-fuzzy model during virtual reality therapy. Features were extracted from ECG and respiration obtaining high accuracy and optimization of computational costs. The neuro-fuzzy model shows better performance than the more frequently adopted classifiers. This approach may enhance the use of physiological computing for stress treatment in clinical practice.",multimedia
10.1109/CBS.2018.8612261,to_check,2018 IEEE International Conference on Cyborg and Bionic Systems (CBS),IEEE,2018-10-27 00:00:00,ieeexplore,sEMG-Based Torque Estimation Using Time-Delay ANN for Control of an Upper-Limb Rehabilitation Robot,https://ieeexplore.ieee.org/document/8612261/,"Robotic-assisted rehabilitation of the upper limb following neurological injury can achieve best possible functional recovery when patients are engaged in the therapy. However, implementation of active training is still difficult as it's challenging to detect human motion intention online and impose corresponding robot control. This paper introduces a novel upper-limb rehabilitation robot, and proposes a sEMG-driven (sEMG: surface Electromyography) torque estimation model based on artificial neural networks (ANN). The robot has three DOFs, of which the first two DOFs adopt a planar parallel structure, and the wrist module has an exoskeleton form. In this study, we design an impedance controller and an admittance controller for the first two DOFs and the wrist module, respectively. Specifically, for the first two DOFs, the assistance/resistance force at the end-effector was controlled according to its motions and desired interaction impedance; for the wrist module, an sEMG armband was used to collect 8 channels of sEMG signals from the forearm muscles, and a time-delay ANN model was developed to estimate the wrist pronation/supination torque, based on which the wrist rotation was controlled according to the human motion intention. To overcome the overfitting problem, besides the experimental samples of wrist rotation, both resting and co-contraction samples were collected for training. Finally, combining with the design of a virtual reality game and force fields, the proposed methods were implemented and tested experimentally on the upper-limb rehabilitation robot.",multimedia
10.1109/ECTC32862.2020.00276,to_check,2020 IEEE 70th Electronic Components and Technology Conference (ECTC),IEEE,2020-06-30 00:00:00,ieeexplore,Extracting power supply current profile by using interposer-based low-noise probing technique for PDN design of high-density POP,https://ieeexplore.ieee.org/document/9159523/,"Firmly understanding the power supply current profile (PSCP) of various scenarios used in real use cases is essential for the simulation and design of power delivery network (PDN) of system-on-chip (SOC) to maximize processor's performance within limited cost budget for die, package, and system, because the low power hard-ware implementation of leading-edge SOC including high performance computing cores for video data processing, 3D graphics, augmented reality, artificial intelligence, and 5G data communication with battery powered portable electronic devices, whose primary concern is the low power consumption, has been concentrated on reducing the minimum allowable power supply voltage for high performance computing cores including CPU, GPU, NPU and CP. The objective of this work is presenting the method to precisely probe power supply voltage fluctuation (PSVF) of whole power domains for power supply current profile (PSCP) extraction of entire cores, for which the authors present an concrete analysis methodology, based on which a test interposer scheme targeted for probing core logic blocks at the proper position of PDN is implemented and demonstrated when in operation. The proposed low noise probing system for acquiring PSCP is constructed by a test interposer designed with rigorous PI analyses.",multimedia
10.1109/CCECE.2012.6335012,to_check,2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),IEEE,2012-05-02 00:00:00,ieeexplore,"Realtime HDR (High Dynamic Range) video for eyetap wearable computers, FPGA-based seeing aids, and glasseyes (EyeTaps)",https://ieeexplore.ieee.org/document/6335012/,"Realtime video HDR (High Dynamic Range) is presented in the context of a seeing aid designed originally for task-specific use (e.g. electric arc welding). It can also be built into regular eyeglasses to help people see better in everyday life. Our prototype consists of an EyeTap (electric glasses) welding helmet, with a wearable computer upon which are implemented a set of image processing algorithms that implement realtime HDR (High Dynamic Range) image processing together with applications such as mediated reality, augmediatedTM, and augmented reality. The HDR video system runs in realtime and processes 120 frames per second, in groups of three frames or four frames (e.g. a set of four differently exposed images captured every thirtieth of a second). The processing method, for implementation on FPGAs (Field Programmable Gate Arrays), achieves a realtime performance for creating HDR video using our novel compositing methods, and runs on a miniature self-contained battery-operated head-worn circuit board, without the need for a host computer. The result is an essentially self-contained miniaturizable hardware HDR camera system that could be built into smaller eyeglass frames, for use in various wearable computing and mediated/ aug-mediated reality applications, as well as to help people see better in their everyday lives.",multimedia
10.1109/ROBIO.2009.5420526,to_check,2009 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2009-12-23 00:00:00,ieeexplore,Learning-based action planning for real-time robot telecontrol with binocular vision in enhanced reality environment,https://ieeexplore.ieee.org/document/5420526/,"Action planning is one of the pivot issues in robot telecontrol, in which the action instructions are often given by the controller from remote site with the help of vision systems. In this paper, we present a learning-based strategy for action planning in robot telecontrol, in which the parameters of sophisticated actions of the remote robot equipped with a binocular vision system could be pre-scheduled with a virtual robot at the control terminal. The remote robot will then be 'taught' with the scheduled action plan with a series of parameter sets obtained form try-outs with the virtual robot and object in the enhanced environment, thus implementing dedicated actions assigned correctly. The action planning process is implemented within a enhanced reality environment, in which both the virtual and the real robot will be displayed simultaneously for the purpose of being deeply immersed. Experiment results demonstrate that the proposed method is capable of promoting the action precision of the remote robot, and effective and valid to designated applications, where action precision plays a critical role.",robotics
10.23919/CISTI.2017.7975750,to_check,2017 12th Iberian Conference on Information Systems and Technologies (CISTI),IEEE,2017-06-24 00:00:00,ieeexplore,UAV simulator for grown-up people quality of life enhancement,https://ieeexplore.ieee.org/document/7975750/,This paper presents the development of a virtual reality simulator for the management of a UAV (Unmanned Aerial Vehicle) focused on improving the quality of life of grown-up people. The present research has collected characteristics of gestures and physical movements from users made by other related research in order to study the same interaction within a virtual world. Through this research a smaller number of gestures were created improving the user learning curve without affecting the usability. The following implementation uses a client-server architecture composed of 2 Raspberry Pi devices and a Smartphone acting as a server the communication between them was achieved by employing Bluetooth Low Energy technology. The immersive virtual experience is accomplished by using Unity 3D and Google VR tools that allowed the design and display of a playful virtual environment as an approach to promote physical and cognitive skills such as spatial thinking and hand-eye coordination. By performing maneuvers through an aerial circuit filled with obstacles the proposed UAV simulator encourages motor and mental activity while the user is being entertained. The result is the improvement of the user quality of life by avoiding cognitive and physical sedentarism.,autonomous vehicle
http://arxiv.org/abs/1808.06352v1,to_check,arxiv,arxiv,2018-08-20 09:06:21+00:00,arxiv,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality",http://arxiv.org/abs/1808.06352v1,"Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",autonomous vehicle
10.1109/CIMCA.2006.133,to_check,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),IEEE,2006-12-01 00:00:00,ieeexplore,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,https://ieeexplore.ieee.org/document/4052645/,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",health
10.1109/ComPE49325.2020.9200139,to_check,2020 International Conference on Computational Performance Evaluation (ComPE),IEEE,2020-07-04 00:00:00,ieeexplore,Object Detection and Tracking Turret based on Cascade Classifiers and Single Shot Detectors,https://ieeexplore.ieee.org/document/9200139/,"The involvement of embedded systems and computer vision is increasing day by day in various segments of consumer market like industrial automation, traffic monitoring, medical imaging, modern appliance market, augmented reality systems, etc. These technologies are bound to make new developments in the domain of commercial and home security surveillance. Our project aims to make contributions to the domain of video surveillance by making use of embedded computer vision systems. Our implementation, built around the Raspberry Pi 4 SBC aims to utilize computer vision techniques like motion detection, face recognition, object detection, etc to segment the region of interest from the captured video footage. This technique is superior as compared to traditional surveillance systems as it requires minimum human interaction and intervention at the control room of such security systems. The proposed system is capable of sensing suspicious events like detection of an unknown face in the captured video or motion detection/object detection in a closed section of a building. Moreover, with the help of the turret mechanism built using servo motors, the camera integrated in the system is capable of having 360° rotation and can track a detected face or object of interest within its range. Apart from automated tracking, the system can also be manually controlled by the operator.",health
10.1109/ICOSP.2014.7014957,to_check,2014 12th International Conference on Signal Processing (ICSP),IEEE,2014-10-23 00:00:00,ieeexplore,Table of contents,https://ieeexplore.ieee.org/document/7014957/,The following topics are dealt with: digital signal processing; spectrum estimation &amp; modeling; TF spectrum analysis &amp; wavelet; adaptive filtering; array signal processing; hardware implementation; speech and audio coding; speech synthesis &amp; recognition; music information processing; speech quality improvements; flexible feature control; medical image processing; partial differential equation; video compression &amp; streaming; computer vision; virtual reality; multimedia &amp; human-computer interaction; statistic learning; pattern recognition; artificial intelligence; neural networks; communication signal processing; Internet and wireless communications; biometrics &amp; authentification; bio-medical &amp; cognitive science; security; radar signal processing; sonar signal processing and localization; and sensor networks.,health
10.1109/TLT.2014.2340878,to_check,IEEE Transactions on Learning Technologies,IEEE,2014-12-01 00:00:00,ieeexplore,A Novel Approach to Diagnosing Motor Skills,https://ieeexplore.ieee.org/document/6863666/,"The combination of virtual reality interactive systems and educational technologies have been used in the training of procedural tasks, but there is a lack of research with regard to providing specific assistance for acquiring motor skills. In this paper we present a novel approach to evaluating motor skills with an interactive intelligent learning system based on the ULISES framework. We describe the implementation of the different layers that ULISES is composed of in order to generate a diagnosis of trainees' motor skills. This diagnostic process takes into account the following characteristics of movement: coordination, poses, movement trajectories and the procedure followed in a sequence of movements. In order to validate our work we generated a model for the diagnosis of tennis-related motor skills and we conducted an experiment in which we interpreted and diagnosed tennis serves of several subjects and which shows promising results.",health
10.1109/ICCRD.2011.5764067,to_check,2011 3rd International Conference on Computer Research and Development,IEEE,2011-03-13 00:00:00,ieeexplore,Table of contents vol. 01,https://ieeexplore.ieee.org/document/5764067/,The following topics are dealt with: computer research and development; event driven programming; artificial intelligence; expert systems; algorithm analysis; high performance computing; automated software engineering; human computer interaction; bioinformatics; scientific computing; image processing; information retrieval; compilers; interpreters; computational intelligence; computer architecture; embedded systems; computer animation; Internet; Web applications; communication/networking; knowledge data engineering; computer system implementation; logics; VLSI; mathematical software; information systems; computer based education; mathematical logic; mobile computing; computer games; multimedia applications; computer graphics; virtual reality; natural language processing; neural networks; computer modeling; parallel computing; distributed computing; computer networks; pattern recognition; computer security; computer simulation; computer vision; probability; statistics; performance evaluation; computer aided design/manufacturing; computing ethics; programming languages; problem complexity; control systems; physical sciences; engineering; discrete mathematics; reconfigurable computing systems; data communications; robotics; automation; system security; cryptography; data compression; data encryption; data mining; database systems; document processing; text processing; educational technology; digital library; technology management; digital signal processing; theoretical computer science; digital systems; logic design; ubiquitous computing; and visualizations.,industry
10.1109/JPROC.2018.2856739,to_check,Proceedings of the IEEE,IEEE,2018-11-01 00:00:00,ieeexplore,Navigating the Landscape for Real-Time Localization and Mapping for Robotics and Virtual and Augmented Reality,https://ieeexplore.ieee.org/document/8436423/,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",industry
http://arxiv.org/abs/1811.02213v1,to_check,arxiv,arxiv,2018-11-06 08:05:24+00:00,arxiv,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots",http://arxiv.org/abs/1811.02213v1,"One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility.",industry
10.1016/j.procir.2020.04.135,to_check,Procedia CIRP,scopus,2020-01-01,sciencedirect,Application of Artificial Intelligence to an Electrical Rewinding Factory Shop,https://api.elsevier.com/content/abstract/scopus_id/85091693237,"The evolution of artificial intelligence (AI) and big data resulted in the full potential realization of technologies through convergence. Tremendous acceptance, adoption and implementation of the United Nations Sustainable Development Goals (SDG) Agenda 2030, has resulted in original equipment manufacturers (OEM) developing various designs of rotary machines in a bid to improve energy efficiency, with more improvements expected in the coming decade. An effective technique to manage energy efficiency in the smart grid is through integration of demand side management, inclusive of optimization of rewinding of an electric motor in a machine shop. This paper aims to conceptualize application of AI and augmented reality (AR) towards process visibility of remanufacturing rotary machine stators by robotic vision. SLT is the triangulation methodology used in laser scanning for 3D modelling, and instantaneous condition assessment of the core. A pre-defined robotic path is used towards identification of features for range image acquisition. Therefore, the potential of industry 4.0 in resuscitation of end-of-life products through service remanufacturers by RE in a rewinding shop are presented.",industry
10.1109/AICAI.2019.8701282,to_check,2019 Amity International Conference on Artificial Intelligence (AICAI),IEEE,2019-02-06 00:00:00,ieeexplore,Role of Distributed Ledger Technology (DLT) to Enhance Resiliency in Internet of Things (IoT) Ecosystem,https://ieeexplore.ieee.org/document/8701282/,"So far Internet has connected humans and now with technological advancements it is inter connecting `Things'. With more globalization and technological advancement, The Internet of Things (IoT) has been matured into self sustaining and evolving technology that has the capacity to change the way how physical and cyber worlds interact. We can also say IoT is about anything that can connect everything. With Involvement of Global Corporations, new IoT-based systems are being proposed in almost every sector which humans have so far envisioned. The thoughts (or Science Fiction) which was once fictional and unbelievable are becoming reality, whatever we desire will be available at touch of our finger someday. Mankind is moving fast towards connected future, where not only autonomous vehicles but entire cities infrastructure will be completely internet connected to support rapid urbanization. To reap the full benefit of IoT, it is imperative that the infrastructure we depend upon is adequate to deliver the services envisioned and has the necessary resilience, robustness and security. IoT can be described as an large scale, heterogeneous, ultra-complex Ecosystem acting as bridge between cyber and physical worlds. Recently, the Distributed Ledger Technology (Blockchain and Block less-different implementation of Data Structures with cryptographic algorithmic functions) has gained much attention in IoT solutions from security perspective. In This Research Paper we have explained the concepts about the functioning of Distributed Ledger Technology with focus on how it can provide security (for System Resiliency) in IoT Ecosystem.",smart cities
10.1016/j.jnca.2020.102596,to_check,Journal of Network and Computer Applications,scopus,2020-06-01,sciencedirect,On the classification of fog computing applications: A machine learning perspective,https://api.elsevier.com/content/abstract/scopus_id/85082445495,"Currently, Internet applications running on mobile devices generate a massive amount of data that can be transmitted to a Cloud for processing. However, one fundamental limitation of a Cloud is the connectivity with end devices. Fog computing overcomes this limitation and supports the requirements of time-sensitive applications by distributing computation, communication, and storage services along the Cloud to Things (C2T) continuum, empowering potential new applications, such as smart cities, augmented reality (AR), and virtual reality (VR). However, the adoption of Fog-based computational resources and their integration with the Cloud introduces new challenges in resource management, which requires the implementation of new strategies to guarantee compliance with the quality of service (QoS) requirements of applications.
                  In this context, one major question is how to map the QoS requirements of applications on Fog and Cloud resources. One possible approach is to discriminate the applications arriving at the Fog into Classes of Service (CoS). This paper thus introduces a set of CoS for Fog applications which includes, the QoS requirements that best characterize these Fog applications. Moreover, this paper proposes the implementation of a typical machine learning classification methodology to discriminate Fog computing applications as a function of their QoS requirements. Furthermore, the application of this methodology is illustrated in the assessment of classifiers in terms of efficiency, accuracy, and robustness to noise. The adoption of a methodology for machine learning-based classification constitutes a first step towards the definition of QoS provisioning mechanisms in Fog computing. Moreover, classifying Fog computing applications can facilitate the decision-making process for Fog scheduler.",smart cities
10.1016/j.procs.2019.09.007,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Increase the interest in learning by implementing augmented reality: Case studies studying rail transportation.,https://api.elsevier.com/content/abstract/scopus_id/85073117730,"Learn a subject, for some people, might be an uninteresting and boring activity, especially when the subject to learn are difficult subjects to understand. Many methods used to change learning activities become more enjoyable and interested. This study proposed a new method in learning activities, by applied augmented reality technology in the learning process. The case study used in this paper are implementation the augmented reality in studied subjects related to train technology. In this study, author implement augmented reality on learning material, combines real and virtual things in one media, in this case a mobile device. The impact of implementation of augmented studied, at the end of experiment, author can conclude when implement augmented reality technology in learning material helps the learning process and increasing the impressive and fun factor in learning process and make the learning process more interested. Implementation of Augmented Reality in learning material gives more information about the object being studied, information about on shapes, textures, and provide more visualization for the object.",smart cities
10.1109/BICITS51482.2021.9509919,to_check,2021 1st Babylon International Conference on Information Technology and Science (BICITS),IEEE,2021-04-29 00:00:00,ieeexplore,Optical Impairment Compensation in Fiber Communication Systems Based on Artificial Intelligence: A Comprehensive Survey,https://ieeexplore.ieee.org/document/9509919/,"The global demand for high-speed communication has increased dramatically over the past few years when data beginning to dominating of the traffic according to the Cisco Visual Networking Index (VNI). Data traffic is triple between 2014 and 2020, mainly, due to developing applications that consume bandwidth such as cloud services, HD video, high quality of real-time video transmission, virtual- augmented reality (VR-AR), online- games (video games), exchange of multimedia via smartphones and the more like. In fact, in 2020, more than a million minutes of multimedia (video) content is transiting the IP network every second according to the VNI; and the demands will exceed the capability of the current (core) internet backbone systems, in which optical communications are the main infrastructure. In this paper, the focus was on reviewing the mechanisms used for the most important and most effective techniques used to increase the capacity of optical transmission systems, namely Nonlinear Compensation (NLC) which work to reduce the nonlinear impairments that represent the main intrinsic challenges and the main capacity limitations facing the optical systems. The traditional NLC techniques were determined based on the approximate solution of the Nonlinear Schrodinger Equation (NLSE) through Digital Back Propagation (DBP), or Split- step Fourier Method (SSFM). however, their implementation demands excessive signal processing resources, and high-level accurate knowledge. A completely new approach that uses artificial intelligence (AI) algorithms to identify and solve these impairments has been studied in this paper. Traditional NLC techniques are reviewed in the first part to mitigation the nonlinearities and estimate the quality of transmission (QoT). Whereas in the second part, we review the uses of AI techniques that have been studied in applications related to monitoring performance, reduce nonlinearity, and quantify QoT. Finally, this paper presents a summary with a conclusion and outlook for development and challenges in optical fiber communication systems where AI is predictable to represent a hot major role in the near future.",multimedia
10.1109/IIAI-AAI.2014.4,to_check,2014 IIAI 3rd International Conference on Advanced Applied Informatics,IEEE,2014-09-04 00:00:00,ieeexplore,Table of contents,https://ieeexplore.ieee.org/document/6913248/,The following topics are dealt with: data mining; Japanese WordNet synonym misplacement detection; social network; recommender system; sentiment analysis; workshop-based instruction; Japanese public libraries; machine learning methods; collaborative Web presentation support system; SMS4 ultracompact hardware implementation; wireless sensor networks; personalized public transportation recommendation system; adaptive user interface; NIS-Apriori algorithm; GetRNIA software tool; rough set-based rule generation; tree-Ga bump hunting; neural network model; weighted citation network analysis; sound proofing ventilation unit; touch interaction; mutually dependent Markov decision processes; ozone treatment; dynamic query optimization; big data; learner activity recognition; IoT-security approach; nutrition-based vegetable production; farm product cultivation; polynomial time mat learning; C-deterministic regular formal graph system; article abstract key expression extraction; English text comprehension; online social games; knowledge creation; knowledge utilization; online stock trading; customer behavior analysis; project-based collaborative learning; in-field mobile game-based learning activities; e-portfolio system design; self-regulated learning ontological model; mobile augmented reality based scaffolding platform; context-aware mobile Japanese conversation learning system; English writing error classification; image processing; outside-class learning; exercise-centric teaching materials; UML modeling; online historical document reading literacy; MMORPG-based learning environment; computer courses; undergraduate education; energy management system; higher education; decentralised auction-based bandwidth allocation; wireless networked control systems; resource scheduling algorithm; embedded cloud computing; Poisson distribution; Japanese seismic activity; suspect vehicle detection; 3D network traffic visualization; Web information retrieval; agent based disaster evacuation assist system; electroencephalogram; random number generator; multiagent simulations; multicore environment; CPU scheduler; multithreaded processes; reserve-price biddings; real-time traffic signal control; evolutionary computation; robot-assisted rehabilitation system; hybrid automata; Batik motif classification; color-texture-based feature extraction; backpropagation; multimedia storytelling; e-tourism service; Web mining; search engine; simulation-based e-learning mobile application software; library classification training system; WebQuest learning strategy; context-aware ubiquitous English learning; support vector machine; RFID tag ownership transfer protocol; cognitive linguistics; collaborative software engineering learning; write-access reduction method; NVM-DRAM hybrid memory; garbage collection; parallel indexing scheme; lazy-updating snoop cache protocol; distributed storage system; ITS application; software engineering education; ophthalmic multimodal imaging system; injected bug classification; secure live virtual machine migration; flash memory management; genetic programming; heterogeneous databases; time series similarity search; concurrency control program generation; incremental data migration; multidatabase system; software release time decision making; analytic hierarchy process; interactive genetic algorithm; biometric intelligence; talking robots; archaeological ruin analysis; GIS; optical wireless pedestrian-support systems; visual impairment; extreme programming; Japanese e-commerce Web sites; Chinese sign language animation; hearing-impaired people mammography inspection; geographical maps; electroculogram; XML element retrieval technique; image recognition; reinforcement learning; ECU formal verification; gasoline direct injection engines; earthquake disaster simulation; smart devices for autistic children; RoboCup rescue simulation; inductive logic programming; master-slave asynchronous evolutionary hybrid algorithm; VANET routing optimization; and Web image sharing services.,multimedia
10.1109/AIVR46125.2019.00024,to_check,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2019-12-11 00:00:00,ieeexplore,A Live Storytelling Virtual Reality System with Programmable Cartoon-Style Emotion Embodiment,https://ieeexplore.ieee.org/document/8942322/,"Virtual reality (VR) is a promising new medium for immersive storytelling. While previous research works on VR narrative have tried to engage audiences through nice scenes and interactivity, the emerging live streaming shows the role of a presenter, especially the conveyance of emotion, for promoting audience involvement and enjoyment. In this paper, to lower the requirement of emotion embodiment, we borrow experience from cartoon animation and comics, and propose a novel cartoon-style hybrid emotion embodiment model to increase a storyteller's presence during live performance, which contains an avatar with six basic emotions and auxiliary multimodal display to enhance emotion expressing. We further design and implement a system to teleoperate the embodiment model in VR for live storytelling. In particular, 1) we design a novel visual programming tool that allows users to customize emotional effects based on the emotion embodiment model; 2) we design a novel face tracking module to map presenters' emotional states to the avatar in VR. Our lightweight web-based implementation also makes the application very easy to use. We conduct two preliminary qualitative studies to explore the potential of the hybrid model and the storytelling system, including interviews with three experts and a workshop study with local secondary school students. Results show the potential of the VR storytelling system for education.",multimedia
10.1109/ICAIE53562.2021.00156,to_check,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),IEEE,2021-06-20 00:00:00,ieeexplore,A Review on the Application of Virtual Reality Technology in Ideological and Political Teaching,https://ieeexplore.ieee.org/document/9534508/,"In recent years, the application of virtual reality technology in different fields has attracted the attention of society and academic circles. Some scholars connect virtual reality technology with ideological and political teaching, so as to further explore the new ideas of ideological and political teaching. This paper analyzes the meaning, characteristics and functions of virtual reality technology, and puts forward its current difficulties in ideological and political courses. At the same time, this paper also points out the implementation principles, approaches and strategies of virtual reality technology in the ideological and political curriculum teaching, in order to enrich the teaching methods in the ideological and political curriculum, and lay a research foundation for improving the educational quality of the ideological and political curriculum.",multimedia
10.1109/VR.2019.8798186,to_check,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),IEEE,2019-03-27 00:00:00,ieeexplore,Virtual Reality and Photogrammetry for Improved Reproducibility of Human-Robot Interaction Studies,https://ieeexplore.ieee.org/document/8798186/,"Collecting data in robotics, especially human-robot interactions, traditionally requires a physical robot in a prepared environment, that presents substantial scalability challenges. First, robots provide many possible points of system failure, while the availability of human participants is limited. Second, for tasks such as language learning, it is important to create environments that provide interesting' varied use cases. Traditionally, this requires prepared physical spaces for each scenario being studied. Finally, the expense associated with acquiring robots and preparing spaces places serious limitations on the reproducible quality of experiments. We therefore propose a novel mechanism for using virtual reality to simulate robotic sensor data in a series of prepared scenarios. This allows for a reproducible dataset that other labs can recreate using commodity VR hardware. We demonstrate the effectiveness of this approach with an implementation that includes a simulated physical context, a reconstruction of a human actor, and a reconstruction of a robot. This evaluation shows that even a simple “sandbox” environment allows us to simulate robot sensor data, as well as the movement (e.g., view-port) and speech of humans interacting with the robot in a prescribed scenario.",multimedia
10.1109/VRW50115.2020.00132,to_check,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),IEEE,2020-03-26 00:00:00,ieeexplore,"[DC] Quality, Presence, and Emotions in Virtual Reality Communications",https://ieeexplore.ieee.org/document/9090552/,"This doctoral thesis looks for the identification and evaluation of the factors that allow to improve the QoE of a remote client in telepresence and virtual reality scenarios. Specifically, quality and socioemotional concepts such as social and spatial presence, empathy, and emotions of being in a completely different place, as well as communicate and interact with people who are in that place. The main goals of my research are the analysis of the methodologies to evaluate video quality and socioemotional concepts, the implementation of additional tools using ML techniques to improve the QoE, and finally, experiments in real use cases.",multimedia
10.1109/ACCESS.2020.3022644,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Dynamic Visual Communication Image Framing of Graphic Design in a Virtual Reality Environment,https://ieeexplore.ieee.org/document/9187835/,"This paper explores dynamic visual communication image framing for graphic design based on virtual reality algorithms; it defines corresponding feature representations by delineating layers of pixels, elements, relationships, planes, and applications; and it investigates methods for quantifying geometric features, perceptual features, and style features. The contents include extraction methods for element colors, calculation methods for layout perceptual features and color-matching perceptual features, and pairwise comparison methods for style features. By overfitting the distribution of geometric features in the data, the model can predict the probability density distribution of features such as element position and color under specific conditions to support the generation of flat images. To construct a prediction model, the sampling method of features, the model optimization method, and the data learning strategy are investigated. This thesis involves the design and implementation of a lossless/near-lossless compression system for high-frame-rate gaze camera image data, which is faced with the technical problems of high fidelity and strong real-time and reliable compression. The image single-frame lossless/near-loss-free compression ratio is generally low, and the compression ratio can be improved by using the correlation between image frames. In this paper, we study the application of lossless compression between image frames, the efficient computing structure of FPGA, and an onboard compression system.",multimedia
10.1109/ROBOT.2000.844768,to_check,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),IEEE,2000-04-28 00:00:00,ieeexplore,Application of automatic action planning for several work cells to the German ETS-VII space robotics experiments,https://ieeexplore.ieee.org/document/844768/,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",multimedia
10.23919/iLRN52045.2021.9459413,to_check,2021 7th International Conference of the Immersive Learning Research Network (iLRN),IEEE,2021-06-10 00:00:00,ieeexplore,Comparison of Direct and Vicarious VR Learning Experience: A Perspective from Accessibility and Equity,https://ieeexplore.ieee.org/document/9459413/,"A common challenge for adopting virtual reality (VR) in education is that limited VR devices are often shared among a large group of students. Consequently, there are two types of VR learners: Performers who acquire virtual learning experience through direct engagement in VR and observers who acquire such experience vicariously through observation. To explore the influence of learner type on VR learning, this study conducted a quasi-experiment with 53 elementary school students to examine the difference in VR learning experiences between the performers and the observers. The study results supported the observed VR learning experience as an adequate alternative to direct VR engagement as the observers demonstrated overall comparable learning patterns in reflection, emotion, engagement, and social interaction during the post-VR debriefing, except for the behaviors of recall and interpretation. The research findings can shed light on the issues of accessibility and equity in VR-based instruction and inform the design and implementation of large-scale VR educational programs.",multimedia
10.1109/AIVR50618.2020.00083,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,Eye Tracking Data Collection Protocol for VR for Remotely Located Subjects using Blockchain and Smart Contracts,https://ieeexplore.ieee.org/document/9319118/,"Eye tracking data collection in the virtual reality context is typically carried out in laboratory settings, which usually limits the number of participants or consumes at least several months of research time. In addition, under laboratory settings, subjects may not behave naturally due to being recorded in an uncomfortable environment. In this work, we propose a proof-of-concept eye tracking data collection protocol and its implementation to collect eye tracking data from remotely located subjects, particularly for virtual reality using Ethereum blockchain and smart contracts. With the proposed protocol, data collectors can collect high quality eye tracking data from a large number of human subjects with heterogeneous socio-demographic characteristics. The quality and the amount of data can be helpful for various tasks in datadriven human-computer interaction and artificial intelligence.",multimedia
10.1109/AIVR46125.2019.00057,to_check,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2019-12-11 00:00:00,ieeexplore,Live Emoji: A Live Storytelling VR System with Programmable Cartoon-Style Emotion Embodiment,https://ieeexplore.ieee.org/document/8942384/,"We introduce a novel cartoon-style hybrid emotion embodiment model for live storytelling in virtual reality (VR). It contains an avatar with six basic emotions and an auxiliary multimodal display to enhance the expression of emotions. We further design and implement a system to teleoperate the embodiment model in VR for live storytelling. Specifically, 1) we design a novel visual programming tool that allows users to customize emotional effects based on the emotion embodiment model; 2) we design a novel face tracking module to map presenters' emotional states to the avatar in VR. Our web-based implementation makes the application easy to use. This is an accompanying paper extracted from [1] for the demo session in IEEE AIVR 2019.",multimedia
10.1109/ISMAR50242.2020.00033,to_check,2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),IEEE,2020-11-13 00:00:00,ieeexplore,Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,https://ieeexplore.ieee.org/document/9284794/,"Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of 2.67° at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of 1.57° at 250 Hz using 800 mW.",multimedia
10.1109/AIVR50618.2020.00022,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,Photorealistic avatars to enhance the efficacy of Selfattachment psychotherapy,https://ieeexplore.ieee.org/document/9319121/,"We have designed, developed, and tested an Immersive virtual reality (VR) platform to practice the protocols of Self-attachment psychotherapy. We made use of customized photorealistic avatars for the implementation of both the high-end version (based on Facebook's Oculus) and the low-end version (based on Google's cardboard) of our platform. Under the Selfattachment therapeutic framework, the causes of mental disorders such as chronic anxiety and depression are traced back to the individual's insecure attachment with their primary caregiver during childhood and their subsequent problems in affect regulation. The conventional approach (without VR) to Selfattachment requires that the individual uses their childhood photographs to recall their childhood memories and then imagine that the child that they were is present with them. They thus establish a compassionate relationship with their childhood self and then, using love songs and dancing, create an affectional bond with them. Their adult self subsequently role plays a good parent and interacts with their imagined childhood self to perform various developmental and re-parenting activities. The goal is to enhance their capacities for self-regulation of emotion, which can lead them into earning secure attachment. It is hypothesized that our immersive virtual reality platform - which enables the users to interact with their customized 3D photorealistic childhood avatar - offers either a better alternative or at least a complementary visual tool to the conventional imaginal approach to Self-attachment. The platform was developed in Unity 3D, a cross-platform game engine, and takes advantage of the itSeez3D Avatar SDK for generating a customized photorealistic 3D avatar head from a 2D childhood image of the user. The platform also offers facial and body animations for some of the basic emotional states such as Happy, Sad, Scared and Joyful and it allows modifications to the avatar body (height/ width) and clothing color. A study to compare the use of the avatar-based approach (VR) to Self-attachment with the conventional photo-based approach showed promising results. Almost 85% of the participants reported that their photorealistic childhood avatar in VR was more relatable than their childhood photos. Both low-end and high-end VR based approaches were unanimously reported to be more effective than the conventional imaginal approach. Participants reported that the high-end version of the VR platform was more realistic and immersive than the low-end mobile VR version.",multimedia
10.1109/VR.2004.1310053,to_check,IEEE Virtual Reality 2004,IEEE,2004-03-31 00:00:00,ieeexplore,Resolving object references in multimodal dialogues for immersive virtual environments,https://ieeexplore.ieee.org/document/1310053/,"This paper describes the underlying concepts and the technical implementation of a system for resolving multi-modal references in virtual reality (VR). In this system the temporal and semantic relations intrinsic to referential utterances are expressed as a constraint satisfaction problem, where the propositional value of each referential unit during a multimodal dialogue updates incrementally the active set of constraints. As the system is based on findings of human cognition research it also regards, e.g., constraints implicitly assumed by human communicators. The implementation takes VR related real-time and immersive conditions into account and adapts its architecture to well known scene-graph based design patterns by introducing a so-called reference resolution engine. Regarding the conceptual work as well as regarding the implementation, special care has been taken to allow further refinements and modifications to the underlying resolving processes on a high level basis.",multimedia
10.1109/ICRA.2013.6631400,to_check,2013 IEEE International Conference on Robotics and Automation,IEEE,2013-05-10 00:00:00,ieeexplore,Robust real-time visual odometry for dense RGB-D mapping,https://ieeexplore.ieee.org/document/6631400/,"This paper describes extensions to the Kintinuous [1] algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused realtime surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.",multimedia
10.1109/OJCS.2020.3001839,to_check,IEEE Open Journal of the Computer Society,IEEE,2020-01-01 00:00:00,ieeexplore,An Instrument for Remote Kissing and Engineering Measurement of Its Communication Effects Including Modified Turing Test,https://ieeexplore.ieee.org/document/9119758/,"Various communication systems have been developed to integrate the haptic channel in digital communication. Future directions of such haptic technologies are moving towards realistic virtual reality applications and human-robot social interaction. With the digitisation of touch, robots equipped with touch sensors and actuators can communicate with humans on a more emotional and intimate level, such as sharing a hug or kiss just like humans do. This paper presents the design guideline, implementation and evaluations of a novel haptic kissing machine for smart phones - the Kissenger machine. The key novelties and contributions of the paper are: (i) A novel haptic kissing device for mobile phones, which uses dynamic perpendicular force stimulation to transmit realistic sensations of kissing in order to enhance intimacy and emotional connection of digital communication; (ii) Extensive evaluations of the Kissenger machine, including a lab experiment that compares mediated kissing with Kissenger to real kissing, a unique haptic Turing test that involves the first academic study of human-machine kiss, and a field study of the effects of Kissenger on long distance relationships. The first experiment showed that mediated kissing with Kissenger elicited similar ratings for pleasure, arousal and user experience as real kissing. Experiment 2 confirmed our hypothesis that interrogators have a higher chance of winning the Imitation Game (Turing test) when Kissenger is used during the game. Results from experiment 3 showed that long relationship couples who used Kissenger for a week experienced increased relationship satisfaction and decreased perceived stress.",multimedia
10.1109/TCIAIG.2013.2242072,to_check,IEEE Transactions on Computational Intelligence and AI in Games,IEEE,2013-06-01 00:00:00,ieeexplore,Thinking Penguin: Multimodal Brain–Computer Interface Control of a VR Game,https://ieeexplore.ieee.org/document/6418003/,"In this paper, we describe a multimodal brain-computer interface (BCI) experiment, situated in a highly immersive CAVE. A subject sitting in the virtual environment controls the main character of a virtual reality game: a penguin that slides down a snowy mountain slope. While the subject can trigger a jump action via the BCI, additional steering with a game controller as a secondary task was tested. Our experiment profits from the game as an attractive task where the subject is motivated to get a higher score with a better BCI performance. A BCI based on the so-called brain switch was applied, which allows discrete asynchronous actions. Fourteen subjects participated, of which 50% achieved the required performance to test the penguin game. Comparing the BCI performance during the training and the game showed that a transfer of skills is possible, in spite of the changes in visual complexity and task demand. Finally and most importantly, our results showed that the use of a secondary motor task, in our case the joystick control, did not deteriorate the BCI performance during the game. Through these findings, we conclude that our chosen approach is a suitable multimodal or hybrid BCI implementation, in which the user can even perform other tasks in parallel.",multimedia
10.1109/HOTCHIPS.2015.7477476,to_check,2015 IEEE Hot Chips 27 Symposium (HCS),IEEE,2015-08-25 00:00:00,ieeexplore,A low-power and real-time augmented reality processor for the next generation smart glasses,https://ieeexplore.ieee.org/document/7477476/,This article consists of a single slide from the authors' conference presentation. Topics covered include: hardware requirements of augmented reality in smart glasses systems; algorithm-hardware mapping and network congestion problem; neural network scheduler for 2-D-mesh network-on-chip; and chip and smart glass system implementation.,multimedia
10.23919/MIPRO.2019.8756928,to_check,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",IEEE,2019-05-24 00:00:00,ieeexplore,Utilizing Apple’s ARKit 2.0 for Augmented Reality Application Development,https://ieeexplore.ieee.org/document/8756928/,"When it comes to practical augmented reality applications, mobile platform tools are the most deserving. Thanks to the nature of mobile devices and their everyday usage, the ideal basis for this kind of content has inadvertently formed itself. Consequently, within the iOS development environment, Apple's Xcode program enables application development using the ARKit library which delivers a host of benefits. Amongst the plethora of advantages, this paper focuses on utilizing features such as the ability to measure distances between two points in space, horizontal and vertical plane detection, the ability to detect three-dimensional objects and utilize them as triggers, and the consolidated implementation of ARKit and MapKit libraries in conjunction with the Google Places API intended for displaying superimposed computer-generated content on iOS 11 and later iterations of Apple's mobile operating system.",multimedia
10.1109/IT48810.2020.9070689,to_check,2020 24th International Conference on Information Technology (IT),IEEE,2020-02-22 00:00:00,ieeexplore,A Private Blockchain Implementation Using Multichain Open Source Platform,https://ieeexplore.ieee.org/document/9070689/,"The impact of digital transformation is becoming visible in every aspect of our lives. Digital transformation strategies mostly rely on disruptive technologies such as Internet of Things, augmented reality, artificial intelligence, and blockchain. Blockchain provides decentralized, immutable, and trustless database distributed across all participants and it was initially proposed as distributed ledger for digital cryptocurrency. However, besides in the finance sector, blockchain technology is considered to be a major innovation enabler in various industries. The paper describes an implementation of a private blockchain using the Multichain open source platform with possible application to agrifood use cases such as food tracking and tracing, product lifecycle management, and counterfeit prevention.",multimedia
10.1109/FPL.2011.94,to_check,2011 21st International Conference on Field Programmable Logic and Applications,IEEE,2011-09-07 00:00:00,ieeexplore,Pattern Compression of FAST Corner Detection for Efficient Hardware Implementation,https://ieeexplore.ieee.org/document/6044867/,"This paper shows stream-oriented FPGA implementation of the machine-learned Features from Accelerated Segment Test (FAST) corner detection, which is used in the parallel tracking and mapping (PTAM) for augmented reality (AR). One of the difficulties of compact hardware implementation of the FAST corner detection is a matching process with a large number of corner patterns. We propose corner pattern compression methods focusing on discriminant division and pattern symmetry for rotation and inversion. This pattern compression enables implementation of the corner pattern matching with a combinational circuit. Our prototype implementation achieves real-time execution performance with 7-9% of available slices of a Virtex-5 FPGA.",multimedia
10.1109/TVCG.2019.2932276,to_check,IEEE Transactions on Visualization and Computer Graphics,IEEE,2019-11-01 00:00:00,ieeexplore,VPModel: High-Fidelity Product Simulation in a Virtual-Physical Environment,https://ieeexplore.ieee.org/document/8794580/,"In the development of a new product, the design team must describe the expected effects of the final products to potential users and stakeholders. However, existing prototyping tools can only present a product imperfectly, due to limitations at different levels. Specifically, the physical product model, which may be the product of 3D printing, could lack a visual interface; the presentation of the product through modeling software such as Rhinoceros 3D does not provide good realistic tactile perception; or the interface platforms, such as Axure RP, used to display the interactive effects differ from those to be used in the actual operation. Thus, we present the VPModel, a high-fidelity prototyping tool, able to integrate multiple prototyping methods simultaneously. It combines a touchable 3D-printed product model (3DPM) and a corresponding visualized virtual model, and the interactive interfaces are rendered synchronously in a mixed-reality device. Through the tangible, visual, and interactive demonstration, designers and normal users can each obtain a similar experience to the experience of the finished product. Furthermore, the VPModel also enhances design practices by enabling comparisons between modular models. However, the implementation of this system is a challenging task, which subsumes several fundamental problems as sub-tasks: object detection, real-time matching, hand-gesture detection and action recognition. To achieve the expected goals of the VPModel, this system uses physical hardware (a Microsoft MR HoloLens headset, a Leap Motion Controller, and a 3D printer) and existing machine learning algorithms. To evaluate our VPModel, we report the user experience of 16 participants, evaluated using a closed-ended questionnaire survey, a quantitative analysis of task performance, and a qualitative analysis of open-ended interviews. The results show a significant improvement in realism and enjoyment using the VPModel over the two traditional camera prototype approaches. In summary, the VPModel can be used to support design strategy and to convey design concepts fully and efficiently, which indicates a potential use for the VPModel in shortening product development cycles and reducing communication costs.",multimedia
http://arxiv.org/abs/1810.11359v4,to_check,arxiv,arxiv,2018-10-26 15:05:04+00:00,arxiv,"gpuRIR: A Python Library for Room Impulse Response Simulation with GPU
  Acceleration",http://arxiv.org/abs/1810.11359v4,"The Image Source Method (ISM) is one of the most employed techniques to
calculate acoustic Room Impulse Responses (RIRs), however, its computational
complexity grows fast with the reverberation time of the room and its
computation time can be prohibitive for some applications where a huge number
of RIRs are needed. In this paper, we present a new implementation that
dramatically improves the computation speed of the ISM by using Graphic
Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and
the computation of the images inside each RIR. Additional speedups were
achieved by exploiting the mixed precision capabilities of the newer GPUs and
by using lookup tables. We provide a Python library under GNU license that can
be easily used without any knowledge about GPU programming and we show that it
is about 100 times faster than other state of the art CPU libraries. It may
become a powerful tool for many applications that need to perform a large
number of acoustic simulations, such as training machine learning systems for
audio signal processing, or for real-time room acoustics simulations for
immersive multimedia systems, such as augmented or virtual reality.",multimedia
http://arxiv.org/abs/2009.06875v2,to_check,arxiv,arxiv,2020-09-15 05:50:13+00:00,arxiv,Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,http://arxiv.org/abs/2009.06875v2,"Gaze tracking is an essential component of next generation displays for
virtual reality and augmented reality applications. Traditional camera-based
gaze trackers used in next generation displays are known to be lacking in one
or multiple of the following metrics: power consumption, cost, computational
complexity, estimation accuracy, latency, and form-factor. We propose the use
of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to
traditional camera-based gaze tracking approaches while taking all of these
metrics into consideration. We begin by developing a rendering-based simulation
framework for understanding the relationship between light sources and a
virtual model eyeball. Findings from this framework are used for the placement
of LEDs and photodiodes. Our first prototype uses a neural network to obtain an
average error rate of 2.67{\deg} at 400Hz while demanding only 16mW. By
simplifying the implementation to using only LEDs, duplexed as light
transceivers, and more minimal machine learning model, namely a light-weight
supervised Gaussian process regression algorithm, we show that our second
prototype is capable of an average error rate of 1.57{\deg} at 250 Hz using 800
mW.",multimedia
http://arxiv.org/abs/2010.12570v3,to_check,arxiv,arxiv,2020-10-23 17:54:38+00:00,arxiv,"Eye Tracking Data Collection Protocol for VR for Remotely Located
  Subjects using Blockchain and Smart Contracts",http://arxiv.org/abs/2010.12570v3,"Eye tracking data collection in the virtual reality context is typically
carried out in laboratory settings, which usually limits the number of
participants or consumes at least several months of research time. In addition,
under laboratory settings, subjects may not behave naturally due to being
recorded in an uncomfortable environment. In this work, we propose a
proof-of-concept eye tracking data collection protocol and its implementation
to collect eye tracking data from remotely located subjects, particularly for
virtual reality using Ethereum blockchain and smart contracts. With the
proposed protocol, data collectors can collect high quality eye tracking data
from a large number of human subjects with heterogeneous socio-demographic
characteristics. The quality and the amount of data can be helpful for various
tasks in data-driven human-computer interaction and artificial intelligence.",multimedia
http://arxiv.org/abs/1208.6057v1,to_check,arxiv,arxiv,2012-08-30 00:58:22+00:00,arxiv,"Self-paced brain-computer interface control of ambulation in a virtual
  reality environment",http://arxiv.org/abs/1208.6057v1,"Objective: Spinal cord injury (SCI) often leaves affected individuals unable
to ambulate. Electroencephalogramme (EEG) based brain-computer interface (BCI)
controlled lower extremity prostheses may restore intuitive and able-body-like
ambulation after SCI. To test its feasibility, the authors developed and tested
a novel EEG-based, data-driven BCI system for intuitive and self-paced control
of the ambulation of an avatar within a virtual reality environment (VRE).
  Approach: Eight able-bodied subjects and one with SCI underwent the following
10-min training session: subjects alternated between idling and walking
kinaesthetic motor imageries (KMI) while their EEG were recorded and analysed
to generate subject-specific decoding models. Subjects then performed a
goal-oriented online task, repeated over 5 sessions, in which they utilised the
KMI to control the linear ambulation of an avatar and make 10 sequential stops
at designated points within the VRE.
  Main results: The average offline training performance across subjects was
77.2 +/- 9.5%, ranging from 64.3% (p = 0.00176) to 94.5% (p = 6.26*10^-23),
with chance performance being 50%. The average online performance was 8.4 +/-
1.0 (out of 10) successful stops and 303 +/- 53 sec completion time (perfect =
211 sec). All subjects achieved performances significantly different than those
of random walk (p < 0.05) in 44 of the 45 online sessions.
  Significance: By using a data-driven machine learning approach to decode
users' KMI, this BCIVRE system enabled intuitive and purposeful self-paced
control of ambulation after only a 10-minute training. The ability to achieve
such BCI control with minimal training indicates that the implementation of
future BCI-lower extremity prosthesis systems may be feasible.",multimedia
10.1016/j.compedu.2021.104338,to_check,Computers and Education,scopus,2021-12-01,sciencedirect,"Evaluation of four digital tools and their perceived impact on active learning, repetition and feedback in a large university class",https://api.elsevier.com/content/abstract/scopus_id/85115167061,"Large university classes often face challenges in enhancing active learning, repetition and feedback in the classroom which are essential for promoting student learning. In this study, we evaluated the implementation of digital tools (lecture recordings, question tool, classroom response system and virtual reality) regarding their perceived impact on active learning, repetition, and feedback in a large university class. The study applied a mixed methods design and collected data from a survey (95 students) and focus groups (11 students). The results show that students enjoyed using the tools because they enriched the lecture. However, students perceived differences regarding the impacts on active learning, repetition, and feedback. The perceived impacts of the classroom response system and the lecture recordings were rated high whereas the perceived impacts of the question tool and the VR modules were rated lower. Recommendations on how to use these digital tools in large classroom settings are provided.",multimedia
10.1016/j.entcom.2021.100404,to_check,Entertainment Computing,scopus,2021-05-01,sciencedirect,Using gestural emotions recognised through a neural network as input for an adaptive music system in virtual reality,https://api.elsevier.com/content/abstract/scopus_id/85100077108,"In this article, a head gesture recognition system is developed in order to identify emotional inputs and provide them to an adaptive music system (LitSens) in virtual reality applications, improving virtual presence in the process. Two iterations of this system, both founded on neural networks, are presented: the first one is based on a multi-layer perceptron, whereas the second one consists of a hybrid one-dimensional convolutional neural network. In both cases, the system is able to recognise fear by analysing head gestures. Whereas the first implementation is quicker when recognising this emotion, the second one is slower, but much more accurate, which makes it a better option overall for soundtrack adaptation. An experiment is then detailed, aimed towards validating the behaviour of a gestural recogniser when detecting fear in players. The results achieved through this validation are generally positive, but evince the need for an improvement in terms of system responsiveness.",multimedia
http://arxiv.org/abs/2010.16356v1,to_check,arxiv,arxiv,2020-10-30 16:25:26+00:00,arxiv,Cooperation dynamics of generalized reciprocity on complex networks,http://arxiv.org/abs/2010.16356v1,"Recent studies suggest that the emergence of cooperative behavior can be
explained by generalized reciprocity, a behavioral mechanism based on the
principle of ""help anyone if helped by someone"". In complex systems, the
cooperative dynamics is largely determined by the network structure which
dictates the interactions among neighboring individuals. Despite an abundance
of studies, the role of the network structure in in promoting cooperation
through generalized reciprocity remains an under-explored phenomenon. In this
doctoral thesis, we utilize basic tools from the dynamical systems theory, and
develop a unifying framework for investigating the cooperation dynamics of
generalized reciprocity on complex networks. We use this framework to present a
theoretical overview on the role of generalized reciprocity in promoting
cooperation in three distinct interaction structures: i) social dilemmas, ii)
multidimensional networks, and iii) fluctuating environments. The results
suggest that cooperation through generalized reciprocity always emerges as the
unique attractor in which the overall level of cooperation is maximized, while
simultaneously exploitation of the participating individuals is prevented. The
effect of the network structure is captured by a local centrality measure which
uniquely quantifies the propensity of the network structure to cooperation, by
dictating the degree of cooperation displayed both at microscopic and
macroscopic level. As a consequence, the implementation of our results may go
beyond explaining the evolution of cooperation. In particular, they can be
directly applied in domains that deal with the development of artificial
systems able to adequately mimic reality, such as reinforcement learning.",science
10.1007/s00345-019-03037-6,to_check,World Journal of Urology,Springer,2020-10-01 00:00:00,springer,Artificial intelligence and robotics: a combination that is changing the operating room,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00345-019-03037-6,"Purpose The aim of the current narrative review was to summarize the available evidence in the literature on artificial intelligence (AI) methods that have been applied during robotic surgery. Methods A narrative review of the literature was performed on MEDLINE/Pubmed and Scopus database on the topics of artificial intelligence, autonomous surgery, machine learning, robotic surgery, and surgical navigation, focusing on articles published between January 2015 and June 2019. All available evidences were analyzed and summarized herein after an interactive peer-review process of the panel. Literature review The preliminary results of the implementation of AI in clinical setting are encouraging. By providing a readout of the full telemetry and a sophisticated viewing console, robot-assisted surgery can be used to study and refine the application of AI in surgical practice. Machine learning approaches strengthen the feedback regarding surgical skills acquisition, efficiency of the surgical process, surgical guidance and prediction of postoperative outcomes. Tension-sensors on the robotic arms and the integration of augmented reality methods can help enhance the surgical experience and monitor organ movements. Conclusions The use of AI in robotic surgery is expected to have a significant impact on future surgical training as well as enhance the surgical experience during a procedure. Both aim to realize precision surgery and thus to increase the quality of the surgical care. Implementation of AI in master–slave robotic surgery may allow for the careful, step-by-step consideration of autonomous robotic surgery.",robotics
10.1007/978-3-030-35430-5_2,to_check,Pattern Recognition and Information Processing,Springer,2019-01-01 00:00:00,springer,Robots’ Vision Humanization Through Machine-Learning Based Artificial Visual Attention,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-35430-5_2,"If the main challenge of robotics during the industrial air of 19^th century has consisted of automating repetitive tasks and the sophistication of these machines through digitization of these robots throughout the 20^th century, the challenge of robotics in the current century will be to make cohabit humans and robots in the same living space. However, robots would not succeed in seamlessly integrate the humans’ universe without developing the ability of perceiving similarly to humans the environment that they are supposed to share with them. In such a context, fitting the skills of the natural vision is an appealing perspective for autonomous robotics dealing with and prospecting Human-Robot interaction. The main goal of the present article is to debate the plausibility and the reality of humanizing the robots behavior focusing the perception of the surrounding environment. An implementation of the developed concept on a real humanoid robot nourishes the presented results and the related discussions.",robotics
10.1049/cp.2018.1727,to_check,"IET Doctoral Forum on Biomedical Engineering, Healthcare, Robotics and Artificial Intelligence 2018 (BRAIN 2018)",IET,2018-11-04 00:00:00,ieeexplore,VR display of human brain vessel network extracted from time-of-flight MRI,https://ieeexplore.ieee.org/document/8826796/,"According to the report from the World Health Organization (WHO), vascular diseases became the most life-threatening diseases by 2015. Time of flight angiography (TOF) is a non-contrast MRI technique to visualize blood vessel in the human vascular system. However, diagnosis of the complex vascular network structures was difficult based on traditional (2D) display method owing to the complexity and variability of vascular network structures. In this study, we investigated the feasibility of displaying three-dimensional (3D) vessel network in virtual reality (VR) environment, using 3D TOF data. Also, the potential value of its clinical application was evaluated. The experiment results, 3D VR videos, showed that the intervention of VR technology is feasible for the 3D image display of cerebrovascular network.",health
10.1109/ACCESS.2019.2917277,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Air Quality Forecasting Based on Gated Recurrent Long Short Term Memory Model in Internet of Things,https://ieeexplore.ieee.org/document/8716687/,"With the continuous development of the Chinese economy and the gradual acceleration of urbanization, it has caused tremendous damage to the environment. The bad air environment seriously damages the physical and mental health of the people. The change in smog concentration will be affected by many realistic factors and exhibit nonlinear characteristics. The method proposed in this paper is to use the Internet of Things (IoT) technology to monitor the acquired data, process the data, and predict the next data using a neural network. The existing prediction models have limitations. They don't accurately capture the law between the concentration of haze and the factors affecting reality. It is difficult to accurately predict the nonlinear smog data. One algorithm proposed in this paper is a two-layer model prediction algorithm based on Long Short Term Memory Neural Network and Gated Recurrent Unit (LSTM&amp;GRU). We set a double-layer Recurrent Neural Network to predict the PM2.5 value. This model is an improvement and enhancement of the existing prediction method Long Short Term Memory (LSTM). The experiment integrates data monitored by the IoT node and information released by the national environmental protection department. First, the data of 96 consecutive hours in four cities were selected as the experimental samples. The experimental results are close to the true value. Then, we selected daily smog data from 2014/1/1 to 2018/1/1 as a train and test dataset. It contains smog data for 74 city sites. The first 70% of the data was used for training and the rest for testing. The results of this experiment show that our model can play a better prediction.",health
10.1109/ICCC51575.2020.9344971,to_check,2020 IEEE 6th International Conference on Computer and Communications (ICCC),IEEE,2020-12-14 00:00:00,ieeexplore,Federated Learning for Arrhythmia Detection of Non-IID ECG,https://ieeexplore.ieee.org/document/9344971/,"In this paper, a distributed arrhythmia detection algorithm based on electrocardiogram (ECG) is proposed for auxiliary diagnosis and treatment. ECG that contains tremendous cardiac rhythm information plays an important role in clinical treatment. Machine learning (ML) algorithms can effectively build the relationship between ECG and the underlying arrhythmia in it. Due to the privacy sensitivity of the ECG, we introduced a federated learning (FL)-based distributed algorithm that enables each medical institution to cooperatively train a arrhythmia detection algorithm locally. Compared with the traditional centralized ML algorithms, the use of FL-based algorithm does not need to collect all the local ECG of each medical institution to an external platform to perform centralized learning, and hence preventing the privacy from leakage. However, ECG collected from different medical institution is non-independent and identically distributed (non-IID) in reality, which will lead to non convergence of the FL-based algorithm. To address this challenge, we optimize the FL-based algorithm using a sharing strategy for partial ECG data of each medical institution combined with elastic weight consolidation (EWC) algorithm. Here, the sharing strategy, which makes each medical institution share ECG data to the central server while not share to other clients, could help build an initial FL model and EWC algorithm make the accuracy of the model trained by each medical institution not decline, therefore the proposed FL algorithm can achieve a trade-off between the privacy and model performance. The experiment results show that, compared with baseline FedAvg algorithm and FedCurv algorithm, the optimized FL-based algorithm is faster in convergence for IID ECG and achieves signicant improvement in terms of both recall and precision for non-IID ECG.",health
10.1109/ACCESS.2019.2957816,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Disease Prediction and Early Intervention System Based on Symptom Similarity Analysis,https://ieeexplore.ieee.org/document/8924757/,"With the development of computer technology, the electronic of medical data has become a reality. Now, how to analyze the data sufficiently to predict patient's disease and conduct early intervention has become a focused research direction. The patient's intuitive expression of feelings is also an aspect that cannot be ignored. Doctors record the pathological characteristics of patients in system. In the paper, we proposed a sentence similarity model to carry out symptom similarity analysis to achieve elementary disease prediction and early intervention, which makes use of word embedding and convolutional neural network (CNN) to extract a sentence vector that contains keyword information about the patient's feelings and symptoms. In order to increase the accuracy of sentence similarity computation, this model integrated syntactic tree and neural network into the computation process. Our main innovation is to use symptom similarity analysis model for disease prediction and early intervention. In addition, the SPO kernel is also one of the innovations. Finally, the results of experiment on Microsoft research paraphrase identification (MSRP) indicated that our model can achieve an excellent performance reached 83.9% in the terms of F1 and accuracy. Furthermore, we also conducted experiments on the data of the Semantic Textual Similarity task. Pearson correlation coefficient indicates that our result is closer to the gold standard scores, which illustrates that it can extract the key information of sentence well to realize the prediction of disease and carry out early intervention.",health
10.1007/s42979-021-00564-1,to_check,SN Computer Science,Nature,2021-04-25 00:00:00,springer,Decentralized Learning with Virtual Patients for Medical Diagnosis of Diabetes,https://www.nature.com/articles/s42979-021-00564-1,"Machine learning, applied to medical data, can uncover new knowledge and support medical practices. However, analyzing medical data by machine learning methods presents a trade-off between accuracy and privacy. To overcome the trade-off, we apply the data collaboration analysis method to medical data. This method using artificial dummy data enables analysis to compare distributed information without using the original data. The purpose of our experiment is to identify patients diagnosed with diabetes mellitus (DM), using 29,802 instances of real data obtained from the University of Tsukuba Hospital between 01/03/2013 and 30/09/2018. The whole data is divided into a number of datasets to simulate different hospitals. We propose the following improvements for the data collaboration analysis. (1) Making the dummy data which has a reality and (2) using non-linear reconverting functions into the comparable space. Both can be realized using the generative adversarial network (GAN) and Node2Vec, respectively. The improvement effects of dummy data with GAN scores more than 10% over the effects of dummy data with random numbers. Furthermore, the improvement effect of the re-conversion by Node2Vec with GAN anchor data scores about 20% higher than the linear method with random dummy data. Our results reveal that the data collaboration method with appropriate modifications, depending on data type, improves analysis performance.",health
10.1016/j.evopsy.2021.03.006,to_check,Evolution Psychiatrique,scopus,2021-05-01,sciencedirect,"From Digital Identity to Connected Personality, From Augmented Diagnostician to Virtual Caregiver: What Are the Challenges for the Psychology and the Psychiatry of the Future?",https://api.elsevier.com/content/abstract/scopus_id/85104125089,"Objectifs
                  Qui sommes-nous devenus, citoyens, patients, praticiens ? En quoi les moyens de communications et l’informatisation de notre société modifient-ils, intègrent-ils nos identités ? L’intelligence artificielle comprendrait-elle bientôt plus justement l’être humain dont elle s’émanciperait ?
               
                  Matériel et méthodes
                  Cheminons à partir de la lexicologie pour tenter de saisir, via le point de vue de la philosophie, l’identité contemporaine vers la notion d’« identité numérique » dont les incidents psychologiques normaux ou pathologiques entraînent ce que nous définissons « la personnalité numérique ». Puis, posant les bases d’une psychologie de l’identité contemporaine, nous envisageons comment « la psychologie » et « la psychiatrie » actuelles considèrent « la personnalité » du patient et, en retour, comment elles se définissent du point du vue du « praticien en ligne » ou du « chercheur connecté ».
               
                  Résultats
                  En échange de son utilisation « gratuite », l’action de l’internaute sur le Web 2.0 produit du contenu et alimente des bases de données, déclaratives ou non. En perte d’intimité au fur et à mesure que « ses » données ne lui appartiennent plus, l’identité du citoyen se décompose en fonctions des supports digitaux : site de rencontre amical, plateforme de liens amoureux, blog concernant un loisir ou un voyage, etc. Par le même mouvement, l’identité numérique se compose en autre-soi possédant une part d’intelligence artificielle pourvoyeuse de capacité d’existence propre. Plutôt que deux entités parallèlement différentiables, réelle ou augmentée, naît une identité hybride « réalistiquo-virtuelle ». Quelles conséquences normales ou pathologiques chez l’être humain ? Les tendances sociétales post-modernes issues du digital ou y trouvant expression peuvent entraîner, chez un individu donné, une exacerbation des traits de personnalité préalablement existants, voire des symptômes. Parallèlement, il arrive que les moyens de communication moderne deviennent une aide pour expérimenter le monde, majorer l’estime de soi, rêver favorablement ses phantasmes, se confier plus facilement à des « inconnu(e)s », etc. Mais dans tous les cas, chez le sujet souffrant, ou ne souffrant pas, préalablement à sa surexposition, de maladie neuropsychiatrique ou de trouble psychopathologique, il s’avère aujourd’hui scientifiquement documenté que la confrontation numérique accrue induit des atteintes neuropsychiques massives (affaiblissement de la mémoire de travail, des capacités d’attention et de concentration, des aptitudes à construire des opérations cognitives élaborées, etc.). Sur le plan psychopathologique, plutôt que la terminologie de « trouble de l’identité » ou une notion de « co-identités », le terme d’« identité trouble » nous paraît le mieux rendre compte de cette mutation du « moi » où la frontière entre réalité et virtualités s’amenuise : la dissociation prévaut. L’homme post-moderne et ses objets connectés ne font plus qu’un, mais cet « uniforme » apparaît constitué d’un patchwork de confettis identificatoires plus ou moins accolés, sans réelle harmonisation d’ensemble. La personnalité commune se marque d’hyperexpressivité et d’hyperémotivité, au détriment de la possibilité de contrôle des affects et du développement des capacités d’introspection. Contre le risque du vide, tend à se développer une contra-phobie par l’ordiphone, par l’objet lui-même, par la possibilité de contacter en permanence ses proches si nécessaire, et en retour rester toujours « disponible », ce qui alimente une forme d’égocentrisme addictogène. Résulte de ses évolutions, globalement dans la société, un affaiblissement des capacités langagières, et ainsi de réflexion, y compris pour l’espace clinique et scientifique.
               
                  Discussion
                  Pour les domaines de la psychologie et de la psychiatrie, s’associent actuellement deux évolutions : une velléité d’« objectivité-scientificité » et une numérisation de la relation patient–soignant. Du côté de la « science », la médecine objective « factuelle » s’intéresse de plus en plus à la pathologie aux dépens du sujet en souffrance, confondant signe et symptôme, glissant jusqu’à un niveau moléculaire, très en-deçà du patient, vers une psychiatrie ou une psychologie « post-clinique ». Qu’on veuille la promouvoir ou l’anéantir, du côté du clinicien ou du chercheur, la « subjectivité » est devenue un signifiant à la mode pour le domaine de la santé psychique. Ce retour actuel du « subjectif » prospère sur une sorte de peur de la subjectivité depuis la fin de la seconde guerre mondiale qui avait entraîné la nosographie américaine vers les « objectifs » des DSM (Manuel Diagnostique et Statistique des Troubles Psychiques publié par l’American Psychiatric Association depuis 1952). Mais plutôt qu’une connaissance validable, et/ou invariable concernant tel ou tel trouble psychique, le changement, la relativité des entités nosographiques d’une version à l’autre du manuel traduit, en miroir, la subjectivité d’une époque, ce que nous appelons « subjectivité sociétale ». Autant qu’elle témoigne de notre temps, la révolution bio-numérique s’imposera probablement dans une future édition de la nosographie : la validité diagnostique devrait se majorer par la définition précise de marqueurs biologiques et/ou neuroradiologiques, si ceux-ci participent à construire une théorie étiopathogénique des phénomènes psychiques observés. Cette orientation reste toutefois balbutiante : outre l’infime nombre de biomarqueurs identifiés, et surtout utilisables en pratique quotidienne, leurs liens de causalité ou de conséquentialité avec les symptômes ou le processus morbide restent le plus souvent incertains autant qu’ils sont fort divers et interreliés. Le chercheur en neurosciences vise à mesurer et analyser une multitude de données, intégrant en particulier les mimiques et les émotions authentifiables par caméra thermique, les mouvements des segments des corps et dynamiques des regards enregistrables par des capteurs, la standardisation des voix et des discours pour analyse par logiciel informatique de la prosodie, des signifiants employés, de la syntaxe… le tout s’intégrant dans un phénotypage digital de la souffrance. Pourra-t-on bientôt parler, en remplacement du psychologue ou du psychiatre, de « diagnosticien augmenté » ?
               
                  Conclusion
                  Apparaît-il actuellement hasardeux de faire confiance à un thérapeute entièrement virtuel… expérience déjà lancée il y a plus de 50 ans ! L’être humain est un « être de sens », or, selon le modèle de la clinique traumatique, le surgissement du tout-numérique peut entraîner un « effondrement du sens » générateur d’une tendance à la dissociation de la personnalité. Accordant le rétablissement des liens entre émotions, affects, comportements et cognitions, le langage parlé atténue puis fait disparaître la dissociation. Guidée par le praticien, cette parole thérapeutique est parfois qualifiée de « maïeutique », du nom de la science de l’accouchement : elle construit synchroniquement à son essence la pensée, et une prise de conscience de celle-ci, plutôt qu’elle n’en rendrait compte secondairement. Il s’agit d’une réinterprétation causale d’un sens compris ou plutôt « attribué » singulièrement par le sujet, après-coup, le passé revisité dans l’instant noue une synthèse, le hasard est transformé en destin. Le sujet qui parle réélabore son histoire vers une reconstruction sémantique, une densification de ses réseaux de signification. Reconquérant son être par la création d’un discours, de méandres véridiques comme fictionnels, la narration, voire la poétisation, offre l’illusion ponctuelle d’une meilleure cohérence, toujours relative, illusoire La parole thérapeutique et le discours sur celle-ci restent en devenir, inachevés, incertains autant que vivants, caractérisant une « post-psychothérapie », c’est-à-dire une psychothérapie et non pas une technique rééducative qui se trouverait figée dans des objectifs connus à l’avance. Les notions de faits et de réalité sont ici secondaires, non pas au sens de l’objectif, ni même du subjectif, mais du second degré, puis d’autres degrés successifs ou imbriqués portant l’effort intellectuel. Vers l’apaisement, si nous voulions amener la réflexion à son paroxysme, nous pourrions avancer qu’il suffirait de donner « n’importe quel sens », d’en choisir un quel qu’il soit, du côté du patient ou du praticien, sans qu’il ne soit nécessairement le même, témoignage d’une construction intersubjective formellement invalide.
               
                  Objectives
                  Who have we become, as citizens, patients, practitioners? How do the means of communication and the computerization of our society, its digitization, modify and integrate our identities? Can we assume that artificial intelligence will soon have a more accurate understanding of the human being from whom it will have emancipated itself?
               
                  Materials and methods
                  We move from lexicology to try to grasp, from the point of view of philosophy, a contemporary identity that is moving towards the notion of a “digital identity” whose normal or pathological psychological incidents lead to what we define as “the digital personality.” Then, laying the foundations for a contemporary psychology of identity, we consider how current “psychology” and “psychiatry” view the patient's “personality” and, in turn, how they define themselves from the point of view of “the patient,” or, inversely, from the point of view of the “online practitioner” or “connected researcher.”
               
                  Results
                  In exchange for its “free” use, the Internet user's action on Web 2.0 produces content and feeds databases, whether this is declared or not. Users’ privacy is lost, as “their” data no longer belongs to them; and citizens’ identity is broken down into digital media functions: a site for meeting friends, a dating platform, a blog about hobbies or travel, etc. At the same time, digital identity is made up of an other-self, including a part of artificial intelligence that provides capacity for its own existence. Rather than two parallel, differentiable entities, real or augmented, a “realistic-virtual” hybrid identity is born. What are the normal or pathological consequences for humans? Postmodern societal trends emerging from or finding expression in the digital can lead to an exacerbation of previously existing personality traits, or even symptoms, in a given individual. At the same time, it happens that the modern means of communication become an aid to experience the world, to increase self-esteem, to dream favorably about one's fantasies, to confide more easily in “strangers,” etc. But in all cases, in the subject suffering, or not suffering, prior to his overexposure, from a neuropsychiatric disease or a psychopathological disorder, it now turns out to be scientifically documented that the increased numerical confrontation induces massive neuropsychic damage (weakening working memory, attention and concentration skills, skills in constructing sophisticated cognitive operations, etc.). On the psychopathological level, rather than the terminology of “identity disorder” or a notion of “co-identities,” the term “identity elusive"" seems to us to best account for this mutation of the “me” where the border between reality and virtualities is shrinking: dissociation prevails. The postmodern human and its connected objects become one, but this “uniformity” appears to be made up of a patchwork of identifying confetti more or less joined together, without a real overall harmonization. The common personality is marked by hyperexpressiveness and hyperemotivity, to the detriment of the possibility of controlling affects and the development of introspective capacities. Against the risk of a vacuum, a contra-phobia tends to develop through the smartphone, by the object itself, by the possibility of constantly contacting relatives if necessary, and in return always remaining “available,” which fuels a form of addicting self-centeredness. The result of these developments, for society in general, is a weakening of language skills, and thus of reflection, including in the clinical and scientific space.
               
                  Discussion
                  For the areas of psychology and psychiatry, two developments are currently associated: a desire for “objectivity-scientificity” and a digitization of the patient–caregiver relationship. On the side of “science,” objective “factual” medicine is increasingly interested in pathology at the expense of the suffering subject, confusing sign and symptom, sliding down to a molecular level, far below the patient, towards psychiatry or postclinical psychology. Whether we want to promote it or destroy it, on the side of the clinician or the researcher, “subjectivity” has become a fashionable signifier in the field of mental health. This current return of the “subjective” thrives on a kind of fear of subjectivity present since the end of World War II, which had led American nosography towards the “objectives” of the DSM (Diagnostic and Statistical Manual of Mental Disorders, published by the American Psychiatric Association since 1952). But rather than a verifiable and/or invariable knowledge concerning a particular psychic disorder, the changes and the relativity of nosographic entities from one version of the manual to another provides us with a mirror image of the subjectivity of an era, which we propose to call “societal subjectivity.” As much as it is a product of our time, the bio-digital revolution will probably impose itself in a future edition of nosography: the diagnostic validity should be increased by the precise definition of biological and/or neuroradiological markers, if these participate in building an etiopathogenic theory of observed psychic phenomena. This orientation remains in its infancy, however: in addition to the tiny number of identified biomarkers, and above all, those that are usable in daily practice, their causal or consequential links with symptoms or with the morbid process remain most often uncertain, inasmuch as they are diverse and interrelated. The neuroscience researcher aims to measure and analyze a multitude of data, integrating, in particular, mimicry and emotions authenticated by thermal camera; movements of body segments and gaze dynamics recorded by sensors; the standardization of voices and speeches for computer software analysis of prosody, used signifiers, syntax… all of which is integrated into a digital phenotyping of suffering. Will we soon be able to speak, replacing the psychologist or the psychiatrist, of an “augmented diagnostician?”.
               
                  Conclusion
                  Does it currently appear risky to trust an entirely virtual therapist… an experiment already launched more than 50 years ago! The human being is a “being of meaning,” yet, according to the model of trauma, the emergence of the all-digital can lead to a “collapse of meaning,” generating a tendency to personality dissociation. Granting the reestablishment of the links between emotions, affects, behaviors, and cognitions, spoken language attenuates dissociation, then makes it disappear. Guided by the practitioner, this therapeutic word is sometimes qualified as “maieutics,” from the name of the science of childbirth: it builds thought synchronously to its essence, and an awareness of it, rather than nondisclosure, would account for it secondarily. It is a causal reinterpretation of a meaning understood or rather “attributed” singularly by the subject, after the fact: the past revisited in the present moment creates a synthesis, and chance is transformed into fate. The speaking subject re-elaborates her/his story towards a semantic reconstruction, a densification of her/his networks of signification. Reclaiming one's being by the creation of a discourse, of veridical as well as fictional meanders, narration, even poetization, offers the punctual illusion of a better coherence, always relative, illusory… Therapeutic speech and discourse about such speech–these are still being made, unfinished, uncertain, and alive. These are the characteristics of what we could a “post-psychotherapy,” that is, a psychotherapy and not a re-educational technique whose objectives would be fixed and known in advance. The notions of facts and reality are secondary here, not in the sense of the objective, nor even of the subjective, but of the second degree, then of other successive or overlapping degrees that require intellectual effort. Moving towards appeasement, if we wanted to bring the reflection to its paroxysm, we could advance that it would be enough to give “any meaning,” whatever it may be. This would apply both to the patient and to the practitioner, without each party's meaning necessarily being the same: a testimony to a formally invalid intersubjective construction.",health
10.1109/BigData.2018.8622004,to_check,2018 IEEE International Conference on Big Data (Big Data),IEEE,2018-12-13 00:00:00,ieeexplore,Correlated Anomaly Detection from Large Streaming Data,https://ieeexplore.ieee.org/document/8622004/,"Correlated anomaly detection (CAD) from streaming data is a type of group anomaly detection and an essential task in useful real-time data mining applications like botnet detection, financial event detection, industrial process monitor, etc. The primary approach for this type of detection in previous researches is based on principal score (PS) of divided batches or sliding windows by computing top eigenvalues of the correlation matrix, e.g. the Lanczos algorithm. However, this paper brings up the phenomenon of principal score degeneration for large data set, and then mathematically and practically prove current PS-based methods are likely to fail for CAD on large-scale streaming data even if the number of correlated anomalies grows with the data size at a reasonable rate; in reality, anomalies tend to be the minority of the data, and this issue can be more serious. We propose a framework with two novel randomized algorithms rPS and gPS for better detection of correlated anomalies from large streaming data of various correlation strength. The experiment shows high and balanced recall and estimated accuracy of our framework for anomaly detection from a large server log data set and a U.S. stock daily price data set in comparison to direct principal score evaluation and some other recent group anomaly detection algorithms. Moreover, our techniques significantly improve the computation efficiency and scalability for principal score calculation.",industry
http://arxiv.org/abs/2101.07594v1,to_check,arxiv,arxiv,2021-01-19 12:42:58+00:00,arxiv,"Real-Time Limited-View CT Inpainting and Reconstruction with Dual Domain
  Based on Spatial Information",http://arxiv.org/abs/2101.07594v1,"Low-dose Computed Tomography is a common issue in reality. Current reduction,
sparse sampling and limited-view scanning can all cause it. Between them,
limited-view CT is general in the industry due to inevitable mechanical and
physical limitation. However, limited-view CT can cause serious imaging problem
on account of its massive information loss. Thus, we should effectively utilize
the scant prior information to perform completion. It is an undeniable fact
that CT imaging slices are extremely dense, which leads to high continuity
between successive images. We realized that fully exploit the spatial
correlation between consecutive frames can significantly improve restoration
results in video inpainting. Inspired by this, we propose a deep learning-based
three-stage algorithm that hoist limited-view CT imaging quality based on
spatial information. In stage one, to better utilize prior information in the
Radon domain, we design an adversarial autoencoder to complement the Radon
data. In the second stage, a model is built to perform inpainting based on
spatial continuity in the image domain. At this point, we have roughly restored
the imaging, while its texture still needs to be finely repaired. Hence, we
propose a model to accurately restore the image in stage three, and finally
achieve an ideal inpainting result. In addition, we adopt FBP instead of
SART-TV to make our algorithm more suitable for real-time use. In the
experiment, we restore and reconstruct the Radon data that has been cut the
rear one-third part, they achieve PSNR of 40.209, SSIM of 0.943, while
precisely present the texture.",industry
10.1016/j.ergon.2019.102878,to_check,International Journal of Industrial Ergonomics,scopus,2020-01-01,sciencedirect,Relative Pointing Interface: A gesture interaction method based on the ability to divide space,https://api.elsevier.com/content/abstract/scopus_id/85075264255,"A new type of 3D gesture interface called Relative Pointing Interface (RelPoInt) is suggested in this study. RelPoInt allows users to select a virtual button by pointing to a relative position in a virtual 3D menu. This approach makes use of the distance or angle between the relative point and the reference point. Users can set the reference point freely and operate all the functions on the menu with a simple gesture. Microsoft Kinect was used to implement the RelPoInt in this study. To suggest a radial menu suitable for the RelPoInt, we conducted an experiment in which 20 participants were asked to point to virtual buttons, which were divided into four to eight regions. Errors and subjective assessment of easiness were measured at each treatment condition. RelPoInt can make users to manipulate functions without memorizing complicated gesture vocabularies, which is among the most serious pain points of using a gesture interface. Furthermore, RelPoInt does not require expensive gesture recognition equipment compared with finger recognition interfaces because the required resolution of interaction recognition is less sophisticated. It can be useful in applying 3D gesture interfaces to a variety of devices for smart home, virtual or augmented reality.
               
                  Relevance to industry
                  The proposed interface can be applied to various environments with low cost and easy to implement. Therefore, this interface is highly worthy of use in areas where gestures are useful to interact, such as IoT (Internet of Things) and VR (Virtual Reality)/AR (Augmented Reality).",industry
10.1109/ICNC.2010.5583711,to_check,2010 Sixth International Conference on Natural Computation,IEEE,2010-08-12 00:00:00,ieeexplore,ARMA-GRNN for passenger demand forecasting,https://ieeexplore.ieee.org/document/5583711/,"Passenger transport demand analysis is strategically important in mastering ever-changing market for each transport mode. In order to improve the predict accuracy in complex reality situation, the ARMA-GRNN technique is proposed to capture both the linear and nonlinear perspectives of the intercity passenger demand forecast problem. Taking flight demand from 1991 to 2008 in Beijing-Shanghai corridor as an example, the numerical experiment results demonstrate that after subtract from the linear part by AR, the GRNN network based on principal component analysis can effectively fit the non-linear section with maximum error 1.08%.",smart cities
http://arxiv.org/abs/2110.01863v1,to_check,arxiv,arxiv,2021-10-05 07:55:19+00:00,arxiv,"DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge
  Computing",http://arxiv.org/abs/2110.01863v1,"The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.",smart cities
http://arxiv.org/abs/2002.07325v2,to_check,arxiv,arxiv,2020-02-18 01:30:29+00:00,arxiv,"Decoding pedestrian and automated vehicle interactions using immersive
  virtual reality and interpretable deep learning",http://arxiv.org/abs/2002.07325v2,"To ensure pedestrian friendly streets in the era of automated vehicles,
reassessment of current policies, practices, design, rules and regulations of
urban areas is of importance. This study investigates pedestrian crossing
behaviour, as an important element of urban dynamics that is expected to be
affected by the presence of automated vehicles. For this purpose, an
interpretable machine learning framework is proposed to explore factors
affecting pedestrians' wait time before crossing mid-block crosswalks in the
presence of automated vehicles. To collect rich behavioural data, we developed
a dynamic and immersive virtual reality experiment, with 180 participants from
a heterogeneous population in 4 different locations in the Greater Toronto Area
(GTA). Pedestrian wait time behaviour is then analyzed using a data-driven Cox
Proportional Hazards (CPH) model, in which the linear combination of the
covariates is replaced by a flexible non-linear deep neural network. The
proposed model achieved a 5% improvement in goodness of fit, but more
importantly, enabled us to incorporate a richer set of covariates. A game
theoretic based interpretability method is used to understand the contribution
of different covariates to the time pedestrians wait before crossing. Results
show that the presence of automated vehicles on roads, wider lane widths, high
density on roads, limited sight distance, and lack of walking habits are the
main contributing factors to longer wait times. Our study suggested that, to
move towards pedestrian-friendly urban areas, national level educational
programs for children, enhanced safety measures for seniors, promotion of
active modes of transportation, and revised traffic rules and regulations
should be considered.",smart cities
http://arxiv.org/abs/1708.04001v1,to_check,arxiv,arxiv,2017-08-14 03:58:29+00:00,arxiv,"Group-driven Reinforcement Learning for Personalized mHealth
  Intervention",http://arxiv.org/abs/1708.04001v1,"Due to the popularity of smartphones and wearable devices nowadays, mobile
health (mHealth) technologies are promising to bring positive and wide impacts
on people's health. State-of-the-art decision-making methods for mHealth rely
on some ideal assumptions. Those methods either assume that the users are
completely homogenous or completely heterogeneous. However, in reality, a user
might be similar with some, but not all, users. In this paper, we propose a
novel group-driven reinforcement learning method for the mHealth. We aim to
understand how to share information among similar users to better convert the
limited user information into sharper learned RL policies. Specifically, we
employ the K-means clustering method to group users based on their trajectory
information similarity and learn a shared RL policy for each group. Extensive
experiment results have shown that our method can achieve clear gains over the
state-of-the-art RL methods for mHealth.",smart cities
10.1016/j.trc.2020.102962,to_check,Transportation Research Part C: Emerging Technologies,scopus,2021-03-01,sciencedirect,Decoding pedestrian and automated vehicle interactions using immersive virtual reality and interpretable deep learning,https://api.elsevier.com/content/abstract/scopus_id/85098936346,"To ensure pedestrian-friendly streets in the era of automated vehicles, reassessment of current policies, practices, design, rules and regulations of urban areas is of importance. This study investigates pedestrian crossing behaviour which, as an important element of urban dynamics, is expected to be affected by the presence of automated vehicles. For this purpose, an interpretable machine learning framework is proposed to explore factors affecting pedestrians’ wait time before crossing mid-block crosswalks in the presence of automated vehicles. To collect rich behavioural data, we developed a dynamic and immersive virtual reality experiment, with 180 participants from a heterogeneous population in 4 different locations in the Greater Toronto Area (GTA). Pedestrian wait time behaviour is then analysed using a data-driven Cox Proportional Hazards (CPH) model, in which the linear combination of the covariates is replaced by a flexible non-linear deep neural network. The proposed model achieved a 5% improvement in goodness of fit, but more importantly, enabled us to incorporate a richer set of covariates. A game theoretic based interpretability method is used to understand the contribution of different covariates to the time pedestrians wait before crossing. Results show that the presence of automated vehicles on roads, wider lane widths, high density on roads, limited sight distance, and lack of walking habits are the main contributing factors to longer wait times. Our study suggested that, to move towards pedestrian-friendly urban areas, educational programs for children, enhanced safety measures for seniors, promotion of active modes of transportation, and revised traffic rules and regulations should be considered.",smart cities
10.1016/j.trc.2019.02.010,to_check,Transportation Research Part C: Emerging Technologies,scopus,2019-08-01,sciencedirect,The effect of social influence and social interactions on the adoption of a new technology: The use of bike sharing in a student population,https://api.elsevier.com/content/abstract/scopus_id/85061773352,"The present study investigates how social influence and social interactions can affect the adoption of new technologies, using stated preference (SP) survey data combined with an “accelerated reality” experience of social interaction among the respondents. Specifically, the intention to use a pro-environmental transport mode (the bike sharing) during a public transport strike within a cohort of students has been analysed. Previous studies have modelled social influence effects using SP data by providing a hypothetical scenario with simulated interactions or information about social conformity processes (i.e. social adoption) during the survey. In our paper, in addition to the impact of assumed social norms, the effect of live/real social interactions is included in the survey. SP survey is developed to investigate the effect of Level-of-Service attributes on the hypothetical choices in the scenario of a public transport strike. Besides the pre-defined attributes characterising the alternatives in the SP design, the survey includes techniques to acquire information on conformity and social interactions. Specifically, the interviewees undertake a before and after stated preference experiment (SP1 and SP2), with a period of group discussion in between the two parts. This SP experiment involves different cognitive and interpersonal mechanisms, such as the functional information exchange on benefits and drawbacks of cycling and bike sharing. The aim is to establish whether hypothetical scenarios of social conformity are different from real/live social interactions and whether these social influence processes actually affect the individuals' mode choice. A joint SP1/SP2 mixed logit (ML) model has been estimated to explore the choice behaviour of individuals and allows us to incorporate the inertia/propensity to change behaviour between SP1 and SP2. Moreover, considering the “Reflexive Layers of Influence” (RLI) framework, the processes generated by social interactions (diffusion, translation and reflexivity) are measured and incorporated in the model. We finally show the effect of these social influence variables on the goodness-of-fit of the models and choice simulation for prediction. We also draw conclusions about the value of such enhanced choice models in understanding and predicting the impacts of social interactions on choice behaviour in the context of new transport technologies.",smart cities
10.1109/ICCSCE52189.2021.9530980,to_check,"2021 11th IEEE International Conference on Control System, Computing and Engineering (ICCSCE)",IEEE,2021-08-28 00:00:00,ieeexplore,A Comparative Investigation of Eye Fixation-based 4-Class Emotion Recognition in Virtual Reality Using Machine Learning,https://ieeexplore.ieee.org/document/9530980/,"Research on emotion recognition that relies purely on eye-tracking data is very limited although the usability of eye-tracking technology has great potential for emotional recognition. This paper proposes a novel approach for 4-class emotion classification using eye-tracking data solely in virtual reality (VR) with machine learning algorithms. We classify emotions into four specific classes using VR stimulus. Eye fixation data was used as the emotional-relevant feature in this investigation. A presentation of 360<sup>0</sup> videos, which contains four different sessions, was played in VR to evoke the user’s emotions. The eye-tracking data was collected and recorded using an add-on eye-tracker in the VR headset. Three classifiers were used in the experiment, which are k-nearest neighbor (KNN), random forest (RF), and support vector machine (SVM). The findings showed that RF has the best performance among the classifiers, and achieved the highest accuracy of 80.55%.",multimedia
10.1109/VRAIS.1995.512484,to_check,Proceedings Virtual Reality Annual International Symposium '95,IEEE,1995-03-15 00:00:00,ieeexplore,A vision-based head tracker for fish tank virtual reality-VR without head gear,https://ieeexplore.ieee.org/document/512484/,"A practical and robust head-position tracking method using computer vision is presented. By combining two simple image processing techniques, this tracker can, report the position of the user's head in real time. Whole image processing is performed by software running on normal mid-range workstations. This tracker can support desk top virtual reality (also referred to as ""fish tank VR""), thereby enabling a user to use a wide range of 3D systems Without having to the on any equipment. An experiment conducted by the author suggests this tracker can improve the human's ability in understanding complex 3D structures presented on the display.",multimedia
10.1109/TALE48869.2020.9368359,to_check,"2020 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)",IEEE,2020-12-11 00:00:00,ieeexplore,Analysis of Behavioral Patterns for Social Virtual Reality Based Active Learning,https://ieeexplore.ieee.org/document/9368359/,"The purpose of this study is to implement a virtual reality (VR) classroom by exchanging certain social signals which can be observed during an active learning (AL) setting. It is not clear how to do AL effectively in a VR classroom because it is difficult to exchange social signals on avatars. This paper describes a pilot experiment and suggested that social signals such as facing behavior, body tilted forward and backward, and hand movement on memo are important to convey the intention to the other participants. The outcomes are expected to be used to realize the social VR based AL.",multimedia
10.1109/ICAIE53562.2021.00033,to_check,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),IEEE,2021-06-20 00:00:00,ieeexplore,Interactive simulation teaching system of electrical engineering based on virtual reality,https://ieeexplore.ieee.org/document/9534584/,"The traditional simulation teaching system is limited by the system support software, which cannot realize good interactive teaching. To this end, an interactive simulation teaching system based on virtual reality for electrical engineering is designed. Based on the hardware consisting of ATmega328 chip as the core, sensors and surrounding circuits, the development tool OpenGL is used to draw 3D virtual teaching scenes. The system design is completed by using a double-layer hidden Markov chain to realize simultaneous interactive teaching of multiple paths. The comparison experiment shows that the teaching system has better interactivity, can effectively enhance students' learning interest, and has good user feedback.",multimedia
10.1109/AIVR50618.2020.00035,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,The Efficacy of a Virtual Reality-Based Mindfulness Intervention,https://ieeexplore.ieee.org/document/9319114/,"Mindfulness can be defined as increased awareness of and sustained attentiveness to the present moment. Recently, there has been a growing interest in the applications of mindfulness for empirical research in wellbeing and the use of virtual reality (VR) environments and 3D interfaces as a conduit for mindfulness training. Accordingly, the current experiment investigated whether a brief VR-based mindfulness intervention could induce a greater level of state mindfulness, when compared to an audio-based intervention and control group. Results indicated two mindfulness interventions, VR-based and audio-based, induced a greater state of mindfulness, compared to the control group. Participants in the VR-based mindfulness intervention group reported a greater state of mindfulness than those in the guided audio group, indicating the immersive mindfulness intervention was more robust. Collectively, these results provide empirical support for the efficaciousness of a brief VR-based mindfulness intervention in inducing a robust state of mindfulness in laboratory settings.",multimedia
10.1109/TVCG.2021.3067777,to_check,IEEE Transactions on Visualization and Computer Graphics,IEEE,2021-05-01 00:00:00,ieeexplore,Combining Dynamic Passive Haptics and Haptic Retargeting for Enhanced Haptic Feedback in Virtual Reality,https://ieeexplore.ieee.org/document/9382898/,"To provide immersive haptic experiences, proxy-based haptic feedback systems for virtual reality (VR) face two central challenges: (1) similarity, and (2) colocation. While to solve challenge (1), physical proxy objects need to be sufficiently similar to their virtual counterparts in terms of haptic properties, for challenge (2), proxies and virtual counterparts need to be sufficiently colocated to allow for seamless interactions. To solve these challenges, past research introduced, among others, two successful techniques: (a) Dynamic Passive Haptic Feedback (DPHF), a hardware-based technique that leverages actuated props adapting their physical state during the VR experience, and (b) Haptic Retargeting, a software-based technique leveraging hand redirection to bridge spatial offsets between real and virtual objects. Both concepts have, up to now, not ever been studied in combination. This paper proposes to combine both techniques and reports on the results of a perceptual and a psychophysical experiment situated in a proof-of-concept scenario focused on the perception of virtual weight distribution. We show that users in VR overestimate weight shifts and that, when DPHF and HR are combined, significantly greater shifts can be rendered, compared to using only a weight-shifting prop or unnoticeable hand redirection. Moreover, we find the combination of DPHF and HR to let significantly larger spatial dislocations of proxy and virtual counterpart go unnoticed by users. Our investigation is the first to show the value of combining DPHF and HR in practice, validating that their combination can better solve the challenges of similarity and colocation than the individual techniques can do alone.",multimedia
10.1109/TNSRE.2020.2999352,to_check,IEEE Transactions on Neural Systems and Rehabilitation Engineering,IEEE,2020-07-01 00:00:00,ieeexplore,Dual-Motor-Task of Catching and Throwing a Ball During Overground Walking in Virtual Reality,https://ieeexplore.ieee.org/document/9106414/,"Virtual Reality is a versatile platform to study human behavior in simulated environments and to develop interventions for functional rehabilitation. In this work, we designed a dual-task paradigm in a virtual environment where both tasks demand motor skills. Twenty-one healthy adults (mean age: 24.1 years) participated in this study. The experiment involved three conditions - normal overground walking, catch and throw a ball while standing, and catch and throw a ball while walking overground -all in the virtual environment. We investigated the dual-task gait characteristics and their correlations with outcomes from cognitive assessments. Results show that subjects walk conservatively with smaller stride lengths, larger stride widths and stride time while catching and throwing. However, they are able to throw the balls more accurately at the target and achieve higher scores. During the dual-task throw, we observed that the participants threw more balls during the stance phase of the gait when the foot was in the terminal stance and pre-swing region. During this region, the body has forward momentum. In addition, the changes in gait characteristics during dual-task throw correlate well with outcome measures in standardized cognitive tests. This study provides a new and engaging paradigm to analyze dual-motor-task cost in a virtual reality environment and it can be used as a basis to compare strategies adopted by different population groups with healthy young adults to execute coordinated motor tasks.",multimedia
10.1109/IJCNN.2010.5596835,to_check,The 2010 International Joint Conference on Neural Networks (IJCNN),IEEE,2010-07-23 00:00:00,ieeexplore,Driver's cognitive state classification toward brain computer interface via using a generalized and supervised technology,https://ieeexplore.ieee.org/document/5596835/,"Growing numbers of traffic accidents had become a serious social safety problem in recent years. The main factor of the high fatalities was the obvious decline of the driver's cognitive state in their perception, recognition and vehicle control abilities while being sleepy. The key to avoid the terrible consequents is to build a detecting system for ongoing assessment of driver's cognitive state. A quickly growing research, brain-computer interface (BCI), offers a solution offering great assistance to those who require alternative communicatory and control mechanisms. In this study, we propose an alertness/drowsiness classification system based on investigating electroencephalographic (EEG) brain dynamics in lane-keeping driving experiments in a virtual reality (VR) driving environment with a motion platform. The core of the classification system is composed of dimension reduction technique and classifier learning algorithm. In order to find the suitable method for better describing the data structure, we explore the performances using different feature extraction and feature selection methods with different classifiers. Experiment results show that the accuracy is over 80% in most combinations and even near 90% under Principal Component Analysis (PCA) and Nonparametric Weighted Feature Extraction (NWFE) going with Gaussian Maximum Likelihood classifier (ML) and k-Nearest-Neighbor classifier (kNN), respectively. In addition, this developed classification system can also solve the individual brain dynamic differences caused from different subjects and overcome the subject dependent limitation. The optimized solution with better accuracy performance out of all combinations can be considered to implement in the kernel brain-computer interface.",multimedia
10.1109/AIVR50618.2020.00030,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,Exploring the feasibility of mitigating VR-HMD-induced cybersickness using cathodal transcranial direct current stimulation,https://ieeexplore.ieee.org/document/9319054/,"Many head-mounted virtual reality display (VR-HMD) applications that involve moving visual environments (e.g., virtual rollercoaster, car and airplane driving) will trigger cybersickness (CS). Previous research Arshad et al. (2015) has explored the inhibitory effect of cathodal transcranial direct current stimulation (tDCS) on vestibular cortical excitability, applied to traditional motion sickness (MS), however its applicability to CS, as typically experienced in immersive VR, remains unknown. The presented double-blinded 2x2x3 mixed design experiment (independent variables: stimulation condition [cathodal/anodal]; timing of VR stimulus exposure [before/after tDCS]; sickness scenario [slight symptoms onset/moderate symptoms onset/recovery]) aims to investigate whether the tDCS protocol adapted from Arshad et al. (2015) is effective at delaying the onset of CS symptoms and/or accelerating recovery from them in healthy participants. Quantitative analysis revealed that the cathodal tDCS indeed delayed the onset of slight symptoms if compared to that in anodal condition. However, there are no significant differences in delaying the onset of moderate symptoms nor shortening time to recovery between the two stimulation types. Possible reasons for present findings are discussed and suggestions for future studies are proposed.",multimedia
10.1109/ICMLC.2004.1380515,to_check,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),IEEE,2004-08-29 00:00:00,ieeexplore,Image fusion based on human vision perception,https://ieeexplore.ieee.org/document/1380515/,"The self-control photo system (SCPS) synthetically combines virtual reality and computer technique in amusement photo systems. To achieve high quality image output, this paper presents a new image fusion method based on analysis of human vision perception. The experiment reveals that this method fully preserves valuable original image information and realizes smooth connection, which attributes to a satisfied vision impact.",multimedia
10.1109/IEMBS.2008.4650151,to_check,2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IEEE,2008-08-25 00:00:00,ieeexplore,Measurement of reaching movement with 6-DOF upper rehabilitation system “Robotherapist”,https://ieeexplore.ieee.org/document/4650151/,"In recent years, the needs for rehabilitation support systems are increasing, which use robot technology and virtual reality technology. Applying these technologies make efficient rehabilitation possible. We have developed 6-degrees-of-freedom (DOF) upper rehabilitation support system to evaluate synergy pattern of stroke survivors and to train stroke survivors, named “Robotherapist”. When stroke survivors who can move plural joints only along a certain constant pattern called synergy pattern do reaching movement, some of them cannot keep their posture of the arm normal, but can move their hand along the aim orbit. In this study, we experiment on a measurement of reaching movement with our system and make a model of movement peculiar to stroke survivors and a model of movement of healthy people. Additionally, we propose application software for reaching training with this model. In this paper, we report measurement of reaching movement and propose application software for reaching training with our system.",multimedia
10.1109/SNPD.2007.190,to_check,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",IEEE,2007-08-01 00:00:00,ieeexplore,One Solution for Accurate Collision Detection in Virtual Assembly Environment,https://ieeexplore.ieee.org/document/4287604/,"Virtual assembly system requires higher accuracy on collision detection than ordinary virtual reality application. Based on analyzing the requirement of virtual assembly on accurate collision detection, a kind of hierarchical accurate collision detection thought is proposed, which includes three levels: assembly environment level, polygon level and accurate geometry topology model level. Three ways for virtual assembly system to obtain accurate geometry topology model are given. A specific solution that synthetically uses geometry graph supporting toolkit to realize hierarchical accurate collision detection is presented. A comparison experiment manifests that above solution and approach are feasible and efficient.",multimedia
10.1109/ICECTECH.2011.5941831,to_check,2011 3rd International Conference on Electronics Computer Technology,IEEE,2011-04-10 00:00:00,ieeexplore,Radar-infrared sensor track correlation algorithm based on neural network fusion system,https://ieeexplore.ieee.org/document/5941831/,"Simultaneous capture of the texture and shape of a moving object in real time is expected to be applicable to various fields including virtual reality and object recognition. There are several difficulties must be overcome to develop a sensor able to achieve this feature: fast capturing of shape and the simultaneous capture of texture and shape. This paper presents a new type of neural network base fusion system infrared sensor, which is compatible to standard process. The proposed infrared sensor adopts a suspended n-well containing several p+/n-well diodes as infrared sensing element. The thermal analysis indicates that the sensor exhibits excellent steady-state and transient thermal property. In order to predigest the calculation, the measurement data are to be classified and selected. The fuzzy neuron network information fusion based on the T-S model is used to avoid obstacle for the mobile robot, which fully utilized the information coming from the sensors. Finally, the experiment with the autonomous robot proved that the method is really feasible and efficient.",multimedia
10.1109/ACCESS.2017.2782254,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,Evaluation of Presence in Virtual Environments: Haptic Vest and User’s Haptic Skills,https://ieeexplore.ieee.org/document/8186157/,"This paper presents the integration of a haptic vest with a multimodal virtual environment, consisting of video, audio, and haptic feedback, with the main objective of determining how users, who interact with the virtual environment, benefit from tactile and thermal stimuli provided by the haptic vest. Some experiments are performed using a game application of a train station after an explosion. The participants of this experiment have to move inside the environment, while receiving several stimuli to check if any improvement in presence or realism in that environment is reflected on the vest. This is done by comparing the experimental results with those similar scenarios, obtained without haptic feedback. These experiments are carried out by three groups of participants who are classified on the basis of their experience in haptics and virtual reality devices. Some differences among the groups have been found, which can be related to the levels of realism and synchronization of all the elements in the multimodal environment that fulfill the expectations and maximum satisfaction level. According to the participants in the experiment, two different levels of requirements are to be defined by the system to comply with the expectations of professional and conventional users.",multimedia
10.1109/JSEN.2019.2959639,to_check,IEEE Sensors Journal,IEEE,2020-04-01 00:00:00,ieeexplore,LSTM-Based Lower Limbs Motion Reconstruction Using Low-Dimensional Input of Inertial Motion Capture System,https://ieeexplore.ieee.org/document/8932413/,"Motion capture system has been widely used in virtual reality and rehabilitation area. This study proposed a data-driven method using low-dimensional input of inertial motion capture system to reconstruct human lower-limb motions. The long short-term memory (LSTM) neural network was used and an ensemble LSTM architecture was involved to improve reconstruction performance. Besides, the selection of optimal sensor configuration scheme and time-step parameters of LSTM network was discussed in detail. The reconstruction experiment shows that the method could get the lowest reconstruction joint angle root mean square (RMS) errors of 4.031° on separated motion dataset, and 5.105° on completely new dataset of synthetic motions using ensemble LSTM model with 18 base learner and three sensors units. The computational consumption test shows that the single and ensemble LSTM model spend 0.15ms and 0.91ms respectively to predict next frame. These findings demonstrate that the proposed method is effective and efficient for motions reconstruction of lower limbs.",multimedia
10.1109/ISMAR.2009.5336479,to_check,2009 8th IEEE International Symposium on Mixed and Augmented Reality,IEEE,2009-10-22 00:00:00,ieeexplore,Forked! A demonstration of physics realism in augmented reality,https://ieeexplore.ieee.org/document/5336479/,"In making fully immersive augmented reality (AR) applications, real and virtual objects will have to be seen to physically interact together in a realistic and believable way. This paper describes Forked! a system that has been developed to show how physical interactions between real and virtual objects can be simulated realistically and believably through appropriate use of a physics engine. The system allows users control a robotic forklift to manipulate virtual crates in an AR environment. The paper also describes a evaluation experiment in which it is shown that the physical interactions between the forklift and the virtual creates are realistic and believable enough to be comparable with the physical interactions between a forklift and real crates.",multimedia
10.1109/ICASSP39728.2021.9414088,to_check,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2021-06-11 00:00:00,ieeexplore,Geometry Consistency Of Augmented Reality Based On Semantics,https://ieeexplore.ieee.org/document/9414088/,"In augmented reality, for achieving geometric consistency in the perspective projection virtual-real, we propose a semantic consistency method to achieve the fusion between virtual and real objects with selected segmented objects in the real scene as references. The proposed framework maintains the three-dimensional structure of the scene by satisfying the global semantic map of the real scene. It takes the segmented objects in the scene as the basic unit, and executes the virtual and real fusion for ensuring the accuracy of the relative geometric position of the virtual objects. In addition, a multi-task network architecture is proposed to optimize the camera parameters based on the scene segmentation. The experiment results demonstrate the effectiveness of the proposed augmented reality geometric consistency framework, and confirm that our strategy has the capability of fusing the virtual and real geometric consistency.",multimedia
10.1109/ICRoM.2017.8466169,to_check,2017 5th RSI International Conference on Robotics and Mechatronics (ICRoM),IEEE,2017-10-27 00:00:00,ieeexplore,Mobile robot navigation based on Fuzzy Cognitive Map optimized with Grey Wolf Optimization Algorithm used in Augmented Reality,https://ieeexplore.ieee.org/document/8466169/,"This work presents a control technique for Mobile Robot Navigation using augmented reality (AR). This navigation technique is based on optimized Fuzzy Cognitive Map (FCM) and AR's Glyphs. AR's symbols are provided by the overhead camera. The patterns are made up of glyphs and a clear path. Six practical test are manipulated to examine the strength of optimizing FCM by a mobile robot for navigation with AR's symbols. The experiment examined the effectiveness of a Grey Wolf Optimization Algorithm (GWOA) in optimizing the FCM. Two practical experiments confirm that AR's Glyphs are an effective symbol for a robot to navigation in an unknown environment. A practical experiment reveals that a robot can use AR to manage its intended movement. Augmented reality, such as the Glyphs and a simplified map, are an effective tool for mobile robots to use in navigation in unknown environments. A prototype system is made to navigate the mobile robot by using AR and FCM.",multimedia
10.1109/SoutheastCon42311.2019.9020576,to_check,2019 SoutheastCon,IEEE,2019-04-14 00:00:00,ieeexplore,Pedagogical Innovative Research Endeavor: Visualization of Streamed Big Data through Augmented Reality,https://ieeexplore.ieee.org/document/9020576/,"The speed of technical and scientific innovation is accelerating much faster than humans can learn. Thus, there is a need for innovative pedagogical research to decrease this gap. Toward that goal, the approach of pedagogical component experiment was to design, develop, and measure the effectiveness of an augmented reality (AR) application for the visualization of streamed network traffic data to create an innovative pedagogical research environment and opportunities for learners. Specifically, this experiment was developed for the Microsoft HoloLens, with several AR models of data visualization. The effectiveness of this application was measured. Preliminary results showed that using augmented reality to visualize network data enhances comprehension of the data, though, results using statistical analysis were inconclusive, and further research is needed to determine whether using AR to visualize streaming data is more efficient than other methods of visualization. However, in spite of the outcome of this research experiment, a pedagogical environment was positively created for learners that most likely increased their involvement in research and learning.",multimedia
10.1109/ISMAR-Adjunct.2019.00-31,to_check,2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),IEEE,2019-10-18 00:00:00,ieeexplore,Real-Time Hand Model Estimation from Depth Images for Wearable Augmented Reality Glasses,https://ieeexplore.ieee.org/document/8951959/,"This work presents a hand model estimation method designed specifically with augmented reality (AR) glasses and 3D AR interface in mind. The proposed work is capable of estimating the 3D positions of all ten finger from a single depth image. By leveraging a low-dimensional hand model and exploiting hand geometries from an ego-centric view, we build a lightweight algorithm that is accurate, environment agnostic, and runs in real time on mobile hardware. One major consideration in our design for AR is that the user's hand is likely to interact with planar surfaces since they serve as ideal ""touchscreens"". As a result, our method will not fail to detect the hand even when the hand is in physical contact with a surface such as a table, wall, or even another palm. Our experiment shows using the CVAR database that the accuracy with clear background at 98% and with cluttered background at around 85%.",multimedia
10.1109/ACCESS.2017.2743746,to_check,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,An Augmented Reality Question Answering System Based on Ensemble Neural Networks,https://ieeexplore.ieee.org/document/8017380/,"This paper proposes a classification algorithm based on ensemble neural networks. In the training phase, the proposed algorithm uses a random number of training data to develop multiple random artificial neural network (ANN) models until those ANN models converge. Those models with lower accuracy than the threshold are filtered out. The remaining highly accurate models will be used to predict the output in the testing phase. Meanwhile, the accuracy of ANN models is presented as a weighting value in the testing phase. In the testing phase, the testing data are loaded into the selected ANN models to predict the output class. The output values are multiplied by the corresponding weighting values of ANN models. Then the weighted average of the outputs can be obtained. Finally, the predicted output is converted into the predicted class. We design an augmented reality question answering system (AR-QAS) applying and implementing the proposed algorithm on mobile devices. AR-QAS offers an interactive user interface and automatically replies according to user's queries. By comparing with the logistic regression method and the ANN method, the experiment results demonstrate that the proposed algorithm offers the highest accuracy.",multimedia
10.1109/ICMLC.2004.1384552,to_check,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),IEEE,2004-08-29 00:00:00,ieeexplore,Hand tracking in time-varying illumination,https://ieeexplore.ieee.org/document/1384552/,Tracking hand using a Webcam in daily environment always suffers from limited dynamic range and changing light conditions. This work presents a novel approach to generate steady hand segmentation from the videos captured by a Webcam in the time-varying illumination. Our approach consists of two parts: automatic gain control (AGC) during capture and motion of skin distribution estimating. A Markov model is exploited to estimate and predict the skin color distribution and camera parameters. We show that examples of segmented hand in a variety of lighting conditions. This method is used in our augmented reality map navigation system for bare hand control. The experiment shows this process can run in real time and the error rate is acceptable.,multimedia
10.1109/ROBIO.2016.7866501,to_check,2016 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2016-12-07 00:00:00,ieeexplore,Recent advances on application of deep learning for recovering object pose,https://ieeexplore.ieee.org/document/7866501/,"Recovering object pose is of great importance to many higher level tasks such as robotic manipulation, scene understanding and augmented reality to name a few. Following the recent major breakthroughs in many computer vision tasks made by the deep learning, intensive research to experiment with it also in the task of recovering object pose is conducting. This paper aims to review the state-of-the-art progress on deep learning based pose estimation methods. Firstly, we introduce some popular datasets together with their relevant attributes. Secondly, the deep learning based pose estimation methods are summarized and categorized, and detailed descriptions of representative methods are provided, and their pros and cons are examined. Thirdly, evaluation protocol and comparable performance of reviewed approaches are given. Finally, we highlight the advantages of deep learning based pose estimation methods and provide insights for future.",multimedia
10.1109/SNPD.2016.7515907,to_check,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE,2016-06-01 00:00:00,ieeexplore,A layered tone-mapping operator based on contrast enhanced adaptive histogram equalization,https://ieeexplore.ieee.org/document/7515907/,"Nowadays, high dynamic range (HDR) images have been widely used in photography, video processing and visual reality. In order to display HDR image on the ordinary low dynamic range devices, a technique named tone-mapping is introduced to reform its intensity value and produce a low dynamic range (LDR) image while keeping high contrast and rich detail. According to the work [1-5], the operators are always divided to two kinds of tone-mapping operators: spatially varying and spatially uniform. In the paper, we propose a new spatially varying operator based on the layered iCAM06 model and contrast enhanced adaptive histogram equalization. We first decompose the HDR image into base layer and detail layer, and then apply the contrast enhanced operator to both layers. Last, we experiment the operator on a series of HDR luminance image, and get satisfactory result with improved contrast and visual sensation, as well as lowered time cost with respect the original iCAM06 operator. The results are illustrated in detail and evaluated with contrast and entropy. It shows that our operator improves the contrast and details, as well as the visual sensation of all the LDR images comparing to the original operator.",multimedia
10.1109/CECNET.2011.5768706,to_check,"2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)",IEEE,2011-04-18 00:00:00,ieeexplore,The research application of inspection robot for the smart grid,https://ieeexplore.ieee.org/document/5768706/,"Catering to the new strategic project of smart grid, this paper introduces a design of high-voltage transmission inspection robot, which worked on earth wire and so that automatically inspection on the entire transmission line becomes the reality. Robot control system using hierarchical control structure, included remote management host, robot control ontology and motor drives. Under the auto-operation mode, it makes its own decisions for planning the sequence of operations according to knowledge data base without the upper layer's involvement. Using image recognition obstacle and expert data base, the combined method of using laser sensor over obstacles, the inspection robot autonomous obstacle had came true. The experiment and test results show that the inspection robot system has possessed the capabilities of navigation and inspection tasks on the power lines. It has a good application prospect.",multimedia
10.1109/ICACTE.2010.5579579,to_check,2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE),IEEE,2010-08-22 00:00:00,ieeexplore,Research on the algorithm of communication network speech enhancement based on BP neural network,https://ieeexplore.ieee.org/document/5579579/,"Speech is one of the best natural and convenient intercommunication manners among humankind. Nowadays, speech processing technologies have been broadly used in many applied fields. In this paper, the main research focus is on the study of speech enhancement and separation, which is one of the key technologies when we try to put the speech processing into reality. Firstly, we introduce speech single as well as the neural network elementary theory and propose based on the BP neural network speech enhancement system modeling method. Secondly, we integrate voice feature extraction and summarize the speech cepstrum and noise cepstrum valuation for neural network training and learning in order to eliminate the noise. Finally, the experiment proved this method surpass traditional the speech enhancement algorithm. The simulation result shows that this speech enhancement system design method can save the running time and the effect is good.",multimedia
10.1007/s00779-019-01225-0,to_check,Personal and Ubiquitous Computing,Springer,2019-07-17 00:00:00,springer,Gesture recognition algorithm based on image information fusion in virtual reality,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-019-01225-0,"Combining image information fusion theory with machine learning for biometric recognition is an important field in computer vision research in recent years. Based on this, a gesture recognition algorithm based on image information fusion in virtual reality is proposed. Firstly, it introduces the basic concepts and principles of virtual reality and information fusion technology, analyzes the characteristics and basic components of virtual environment system, points out the relationship between human and virtual environment and the impact of virtual environment on people, and gives a virtual reality. Then, the multi-sensor information fusion model of the virtual environment for gesture recognition is proposed. The membership degree and template matching algorithm are further selected for data correlation and gesture recognition in the fusion model. Finally, the design comparison experiment verifies the proposed method. The results show that the proposed multi-sensor information fusion model in the interactive virtual environment achieves the highest recognition success rate of 96.17% and is better than several comparison machine learning methods in recognition time.",multimedia
http://arxiv.org/abs/2010.09810v1,to_check,arxiv,arxiv,2020-10-19 19:40:29+00:00,arxiv,"Connections between Relational Event Model and Inverse Reinforcement
  Learning for Characterizing Group Interaction Sequences",http://arxiv.org/abs/2010.09810v1,"In this paper we explore previously unidentified connections between
relational event model (REM) from the field of network science and inverse
reinforcement learning (IRL) from the field of machine learning with respect to
their ability to characterize sequences of directed social interaction events
in group settings. REM is a conventional approach to tackle such a problem
whereas the application of IRL is a largely unbeaten path. We begin by
examining the mathematical components of both REM and IRL and find
straightforward analogies between the two methods as well as unique
characteristics of the IRL approach. We demonstrate the special utility of IRL
in characterizing group social interactions with an empirical experiment, in
which we use IRL to infer individual behavioral preferences based on a sequence
of directed communication events from a group of virtual-reality game players
interacting and cooperating to accomplish a shared goal. Our comparison and
experiment introduce fresh perspectives for social behavior analytics and help
inspire new research opportunities at the nexus of social network analysis and
machine learning.",multimedia
http://arxiv.org/abs/2005.10161v1,to_check,arxiv,arxiv,2020-05-20 16:09:57+00:00,arxiv,User Attention and Behaviour in Virtual Reality Art Encounter,http://arxiv.org/abs/2005.10161v1,"With the proliferation of consumer virtual reality (VR) headsets and creative
tools, content creators have started to experiment with new forms of
interactive audience experience using immersive media. Understanding user
attention and behaviours in virtual environment can greatly inform creative
processes in VR. We developed an abstract VR painting and an experimentation
system to study audience encounters through eye gaze and movement tracking. The
data from a user experiment with 35 participants reveal a range of user
activity patterns in art exploration. Deep learning models are used to study
the connections between behavioural data and audience background. New
integrated methods to visualise user attention as part of the artwork are also
developed as a feedback loop to the content creator.",multimedia
http://arxiv.org/abs/2101.01444v1,to_check,arxiv,arxiv,2021-01-05 10:34:35+00:00,arxiv,CycleGAN for Interpretable Online EMT Compensation,http://arxiv.org/abs/2101.01444v1,"Purpose: Electromagnetic Tracking (EMT) can partially replace X-ray guidance
in minimally invasive procedures, reducing radiation in the OR. However, in
this hybrid setting, EMT is disturbed by metallic distortion caused by the
X-ray device. We plan to make hybrid navigation clinical reality to reduce
radiation exposure for patients and surgeons, by compensating EMT error.
  Methods: Our online compensation strategy exploits cycle-consistent
generative adversarial neural networks (CycleGAN). 3D positions are translated
from various bedside environments to their bench equivalents. Domain-translated
points are fine-tuned to reduce error in the bench domain. We evaluate our
compensation approach in a phantom experiment.
  Results: Since the domain-translation approach maps distorted points to their
lab equivalents, predictions are consistent among different C-arm environments.
Error is successfully reduced in all evaluation environments. Our qualitative
phantom experiment demonstrates that our approach generalizes well to an unseen
C-arm environment.
  Conclusion: Adversarial, cycle-consistent training is an explicable,
consistent and thus interpretable approach for online error compensation.
Qualitative assessment of EMT error compensation gives a glimpse to the
potential of our method for rotational error compensation.",multimedia
10.1016/j.neulet.2020.135333,to_check,Neuroscience Letters,scopus,2020-10-15,sciencedirect,Virtual reality head-mounted goggles increase the body sway of young adults during standing posture,https://api.elsevier.com/content/abstract/scopus_id/85090012318,"The aim of this study was to investigate the effects of wearing virtual reality head-mounted goggles (VR) on body sway in young adults. We run two experiments, in which we compared the body sway while standing during the conditions of 1) wearing and non-wearing VR with eyes-opened (experiment #1), 2) wearing and no-wearing VR with eyes-closed (experiment #2), and 3) wearing VR with eyes-opened when the scene was turned on and off (experiment #2). Forty-four (experiment #1) and fifteen (experiment #2) young adults were instructed to remain as still as possible on a force plate for 60-s and performed three trials in each quiet standing condition. The center of pressure (CoP) displacement, mean velocity, root mean square (RMS), area and median frequency of sway were calculated in both experiments. In the experiment #1, wearing VR condition with eyes-opened largely increased the AP and ML CoP displacement, AP mean velocity, AP and ML RMS, and area (p < 0.05) compared to non-wearing VR with eyes-opened. In the experiment #2, no differences were found for any conditions (eyes-closed and eyes-opened with turned on and off VR scene). In conclusion, wearing VR head-mounted goggles increased body sway of young adults during standing postural task, when the individuals were with eyes-opened. However, the effects of wearing VR head-mounted goggles on body sway disappeared when the individuals were with eyes-closed or the google scene was turned off the scene compared to not wearing VR head-mounted goggles with eyes-closed or turned on scene, respectively.",multimedia
10.1109/LRA.2021.3070305,to_check,IEEE Robotics and Automation Letters,IEEE,2021-07-01 00:00:00,ieeexplore,Underwater Soft Robot Modeling and Control With Differentiable Simulation,https://ieeexplore.ieee.org/document/9392257/,"Underwater soft robots are challenging to model and control because of their high degrees of freedom and their intricate coupling with water. In this letter, we present a method that leverages the recent development in differentiable simulation coupled with a differentiable, analytical hydrodynamic model to assist with the modeling and control of an underwater soft robot. We apply this method to Starfish, a customized soft robot design that is easy to fabricate and intuitive to manipulate. Our method starts with data obtained from the real robot and alternates between simulation and experiments. Specifically, the simulation step uses gradients from a differentiable simulator to run system identification and trajectory optimization, and the experiment step executes the optimized trajectory on the robot to collect new data to be fed into simulation. Our demonstration on Starfish shows that proper usage of gradients from a differentiable simulator not only narrows down its simulation-to-reality gap but also improves the performance of an open-loop controller in real experiments.",robotics
10.1109/ICRA40945.2020.9197024,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving,https://ieeexplore.ieee.org/document/9197024/,In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.,autonomous vehicle
10.1109/MECO.2019.8760130,to_check,2019 8th Mediterranean Conference on Embedded Computing (MECO),IEEE,2019-06-14 00:00:00,ieeexplore,Facilitating Near Real Time Analytics on the Edge,https://ieeexplore.ieee.org/document/8760130/,"Internet of Things (IoT) is revolutionizing the way how information is processed and stored. Due to latency sensitive applications and huge amounts of data produced at the edge of the network, more and more data is processed where it is produced - namely on the edge. This development results in completely new network topologies where besides massive data centers we experience growing amount of so called micro data centers on the edge of the network. However, increasing complexity of multiple data centers necessary to execute an application represents a new challenge for the deployment and runtime operation of large scale applications like those in the area of smart cities, self-driving vehicles and tele medicine. The challenge thereby is to deploy application in a way to satisfy user requirements in form of different Quality of Service parameters (e.g., latency) but at the same time minimize energy consumption necessary to execute the application. In this talk we discuss several research challenges that arise when deploying near real time analytics on the edge of the network.",autonomous vehicle
10.1109/ICSSE52999.2021.9538460,to_check,2021 International Conference on System Science and Engineering (ICSSE),IEEE,2021-08-28 00:00:00,ieeexplore,Steering Angle Estimation for Self-driving Car Based on Enhanced Semantic Segmentation,https://ieeexplore.ieee.org/document/9538460/,"Common approaches for semantic segmentation using Convolutional Neural Networks (CNN) based around conventional U-shapes architectures were widely used. However, failures to retrieve global context information and memory issues made such models unable to compete against modern architectures considering accuracy and real-time capability. In this paper, an efficient method maintaining equivalent accuracy of a previous segmentation network and skillfully making a model more light-weighted for real-time inference was proposed. More concretely, we managed to alleviate five out of 17 million trainable parameters, which effectively reduce the amount of computation of the original PSPNet by 30% using the backbone of CSPNet. Our proposed network implementation achieved 73 mIoU scores on our custom dataset and reached 15 fps regarding real-time inference. We deployed the trained model on the multifunctional hardware and then connected it to a golf car to jointly navigate the natural environment and traffic sign detection task. Accordingly, the STM32 board and servo motor were used for controlling the steering wheel through a track-and-wheel drive system. As for the traffic sign detection task, we employed a small-size Yolov5 trained on the TT100K dataset running around 60fps and attained real-time performance with sufficient accuracy.",autonomous vehicle
10.1109/TDSC.2020.2967703,to_check,IEEE Transactions on Dependable and Secure Computing,IEEE,2021-12-01 00:00:00,ieeexplore,Leakage-Resilient Authenticated Key Exchange for Edge Artificial Intelligence,https://ieeexplore.ieee.org/document/8964439/,"Edge Artificial Intelligence (AI) is a timely complement of cloud-based AI. By introducing intelligence to the edge, it alleviates privacy concerns of streaming and storing data to the cloud, enables real-time operations where milliseconds matter, and brings AI services to remote areas with poor networking infrastructures. Security is a significant problem in Edge AI applications such as self-driving cars and intelligent healthcare. Since the edge devices are empowered to process data and take actions, attacking and compromising them can cause serious damage. However, the wide deployment of computationally limited devices in edge environments and the increasing happening of side-channel (or leakage) attacks pose critical challenges to security. This article thereby aims to enhance the security for Edge AI by designing and developing lightweight and leakage-resilient authenticated key exchange (LRAKE) protocols. Compared with available LRAKE protocols, the proposed protocols in this article can be effortless applied in some mainstreaming security and communication standards. Moreover, this article realizes prototypes and presents implementation details; and a use case of applying the proposed protocol in Bluetooth 5.0 is illustrated. The theoretical design and implementation details will provide a guidance of applying the LRAKE protocols in Edge AI applications.",autonomous vehicle
10.1109/CRV.2018.00024,to_check,2018 15th Conference on Computer and Robot Vision (CRV),IEEE,2018-05-10 00:00:00,ieeexplore,A Hierarchical Deep Architecture and Mini-batch Selection Method for Joint Traffic Sign and Light Detection,https://ieeexplore.ieee.org/document/8575742/,"Traffic light and sign detectors on autonomous cars are integral for road scene perception. The literature is abundant with deep learning networks that detect either lights or signs, not both, which makes them unsuitable for real-life deployment due to the limited graphics processing unit (GPU) memory and power available on embedded systems. The root cause of this issue is that no public dataset contains both traffic light and sign labels, which leads to difficulties in developing a joint detection framework. We present a deep hierarchical architecture in conjunction with a mini-batch proposal selection mechanism that allows a network to detect both traffic lights and signs from training on separate traffic light and sign datasets. Our method solves the overlapping issue where instances from one dataset are not labelled in the other dataset. We are the first to present a network that performs joint detection on traffic lights and signs. We measure our network on the Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small Traffic Lights benchmark for traffic light detection and show it outperforms the existing Bosch Small Traffic light state-of-the-art method. We focus on autonomous car deployment and show our network is more suitable than others because of its low memory footprint and real-time image processing time. Qualitative results can be viewed at https://youtu.be/ YmogPzBXOw.",autonomous vehicle
10.1109/IJCNN52387.2021.9533738,to_check,2021 International Joint Conference on Neural Networks (IJCNN),IEEE,2021-07-22 00:00:00,ieeexplore,CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor,https://ieeexplore.ieee.org/document/9533738/,"Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS) [1]. The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip [2]. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip.",autonomous vehicle
10.1109/VTC2020-Fall49728.2020.9348512,to_check,2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall),IEEE,2020-12-16 00:00:00,ieeexplore,Adaptive Deployment of UAV-Aided Networks Based on Hybrid Deep Reinforcement Learning,https://ieeexplore.ieee.org/document/9348512/,"Unmanned aerial vehicles (UAVs) can be used as air base stations to provide fast wireless connections for ground users. Due to their constraints on both mobility and energy consumption, a key problem is how to deploy UAVs adaptively in a geographic area with changing traffic demand of mobile users, while meeting the aforemetioned constraints. In this paper, we propose an adaptive deployment strategy for UAV-aided networks based on hybrid deep reinforcement learning, where a UAV can adjust its movement direction and distance to serve users who move randomly in the target area. Through hybrid deep reinforcement learning, UAVs can be trained offline to obtain the global state information and learn a completely distributed control strategy, with which each UAV only needs to take actions based on its observed state in the real deployment to be fully adaptive. Moreover, in order to improve the speed and effect of learning, we improve hybrid reinforcement learning, by adding genetic algorithms and TD-error-based resampling optimization mechanism. Simulation results show that the hybrid deep reinforcement learning algorithm has better efficiency and robustness in multi-UAV control, and has better performance in terms of coverage, energy consumption and average throughput, by which average throughput can be increased by 20% to 60%.",autonomous vehicle
10.1109/AECT47998.2020.9194188,to_check,2019 International Conference on Advances in the Emerging Computing Technologies (AECT),IEEE,2020-02-10 00:00:00,ieeexplore,Clustering Based UAV Base Station Positioning for Enhanced Network Capacity,https://ieeexplore.ieee.org/document/9194188/,"Unmanned aerial vehicles (UAVs) are expected to be deployed in a variety of applications in future mobile networks due to several advantages they bring over the deployment of ground base stations. However, despite the recent interest in UAVs in mobile networks, some issues still remain, such as determining the placement of multiple UAVs in different scenarios. In this paper we propose a solution to determine the optimal 3D position of multiple UAVs in a capacity enhancement use-case, or in other words, when the ground network cannot cope with the user traffic demand. For this scenario, real data from the city of Milan, provided by Telecom Italia is utilized to simulate an event. Based on that, a solution based on k-means, a machine learning technique, to position multiple UAVs is proposed and it is compared with two other baseline methods. Results demonstrate that the proposed solution is able to significantly outperform other methods in terms of users covered and quality of service.",autonomous vehicle
10.1109/GLOCOMW.2018.8644345,to_check,2018 IEEE Globecom Workshops (GC Wkshps),IEEE,2018-12-13 00:00:00,ieeexplore,Deployment and Movement for Multiple Aerial Base Stations by Reinforcement Learning,https://ieeexplore.ieee.org/document/8644345/,"A novel framework for Quality of experience (QoE)-driven deployment and movement of multiple unmanned aerial vehicles (UAVs) is proposed. The problem of joint non-concave 3D deployment and dynamic movement for maximizing the sum mean opinion score (MOS) of users is formulated, which is proved to be NP-hard. In an effort to solve this problem, we proposed a three-step approach to obtain 3D deployment and dynamic movement of multiple UAVs. More specifically, in the first step, GAK-means algorithm is invoked to obtain the cell partitioning of ground users. Secondly, Q-learning based deployment algorithm is proposed, in which each UAV is considered as an agent, making its own decision to obtain 3D position. In contrast to conventional genetic algorithm based learning algorithms, the proposed algorithm is capable of training the policy of making decision offline. Thirdly, Q-learning algorithm is invoked when the ground users roam. Unlike the other trajectory obtaining algorithms, the proposed approach enables each UAV learn its movement gradually through trials and errors, and updates the direction selection strategy until it reaches convergence. Numerical results reveal that the proposed 3D deployment scheme outperforms K-means algorithm and IGK algorithm with low complexity. Additionally, with the aid of proposed approach, 3D real-time dynamic movement of UAVs is obtained.",autonomous vehicle
10.1109/AICAS48895.2020.9073885,to_check,2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS),IEEE,2020-09-02 00:00:00,ieeexplore,Efficient Embedded Deep Neural-Network-based Object Detection Via Joint Quantization and Tiling,https://ieeexplore.ieee.org/document/9073885/,"Embedded visual AI is a growing trend in applications requiring low latency, real-time decision support, increased robustness and security. Visual object detection, a key task in visual data analytics, has enjoyed significant improvements in terms of capabilities and accuracy due to the emergence of Convolutional Neural Networks (CNNs). However, such complex paradigms require heavy computational resources that prevent their deployment on resource-constrained devices, and in particular, impose significant constraints in possible hardware accelerators geared towards such applications. In this work therefore, we investigate how a combination of techniques can lead to efficient visual AI pipelines for resource-constrained object detection. In particular we leverage an efficient search strategy based on a combination of pre-processing mechanisms, that reduce the processing demands of deep network as a counter measure for potential accuracy reduction caused by quantization. The proposed approach enables the detection of objects in higher resolution frames using quantized models, while maintaining the accuracy of full-precision CNN-based object detectors. We illustrate the impact on the accuracy and average processing time using quantization techniques and different tiling approaches on efficient object detection architectures; as a case study, we focus on Unmanned-Aerial- Vehicles (UAVs). Through the proposed methodology, hardware accelerator demands are thereby reduced, leading to both performance benefits and associated power savings.",autonomous vehicle
10.1109/GLOBECOM38437.2019.9014310,to_check,2019 IEEE Global Communications Conference (GLOBECOM),IEEE,2019-12-13 00:00:00,ieeexplore,Gated Recurrent Units Learning for Optimal Deployment of Visible Light Communications Enabled UAVs,https://ieeexplore.ieee.org/document/9014310/,"In this paper, the problem of optimizing the deployment of unmanned aerial vehicles (UAVs) equipped with visible light communication (VLC) capabilities is studied. In the studied model, the UAVs can simultaneously provide communications and illumination to service ground users. Ambient illumination increases the interference over VLC links while reducing the illumination threshold of the UAVs. Therefore, it is necessary to consider the illumination distribution of the target area for UAV deployment optimization. This problem is formulated as an optimization problem whose goal is to minimize the total transmit power while meeting the illumination and communication requirements of users. To solve this problem, an algorithm based on the machine learning framework of gated recurrent units (GRUs) is proposed. Using GRUs, the UAVs can model the longterm historical illumination distribution and predict the future illumination distribution. In order to reduce the complexity of the prediction algorithm while accurately predicting the illumination distribution, a Gaussian mixture model (GMM) is used to fit the illumination distribution of the target area at each time slot. Based on the predicted illumination distribution, the optimization problem is proved to be a convex optimization problem that can be solved by using duality. Simulations using real data from the Earth observations group (EOG) at NOAA/NCEI show that the proposed approach can achieve up to 22.1% reduction in transmit power compared to a conventional optimal UAV deployment that does not consider the illumination distribution. The results also show that UAVs must hover at areas having strong illumination, thus providing useful guidelines on the deployment of VLCenabled UAVs.",autonomous vehicle
10.1109/IROS40897.2019.8967722,to_check,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2019-11-08 00:00:00,ieeexplore,Informed Region Selection for Efficient UAV-based Object Detectors: Altitude-aware Vehicle Detection with CyCAR Dataset,https://ieeexplore.ieee.org/document/8967722/,"Deep Learning-based object detectors enhance the capabilities of remote sensing platforms, such as Unmanned Aerial Vehicles (UAVs), in a wide spectrum of machine vision applications. However, the integration of deep learning introduces heavy computational requirements, preventing the deployment of such algorithms in scenarios that impose low-latency constraints during inference, in order to make mission-critical decisions in real-time. In this paper, we address the challenge of efficient deployment of region-based object detectors in aerial imagery, by introducing an informed methodology for extracting candidate detection regions (proposals). Our approach considers information from the UAV on-board sensors, such as flying altitude and light-weight computer vision filters, along with prior domain knowledge to intelligently decrease the number of region proposals by eliminating false-positives at an early stage of the computation, reducing significantly the computational workload while sustaining the detection accuracy. We apply and evaluate the proposed approach on the task of vehicle detection. Our experiments demonstrate that state-of-the-art detection models can achieve up to 2.6x faster inference by employing our altitude-aware data-driven methodology. Alongside, we introduce and provide to the community a novel vehicle-annotated and altitude-stamped dataset of real UAV imagery, captured at numerous flying heights under a wide span of traffic scenarios.",autonomous vehicle
10.1109/ICCE.2018.8326145,to_check,2018 IEEE International Conference on Consumer Electronics (ICCE),IEEE,2018-01-14 00:00:00,ieeexplore,Optimized vision-directed deployment of UAVs for rapid traffic monitoring,https://ieeexplore.ieee.org/document/8326145/,"The flexibility and cost efficiency of traffic monitoring using Unmanned Aerial Vehicles (UAVs) has made such a proposition an attractive topic of research. To date, the main focus was placed on the types of sensors used to capture the data, and the alternative data processing options to achieve good monitoring performance. In this work we move a step further, and explore the deployment strategies that can be realized for rapid traffic monitoring over particular regions of the transportation network by considering a monitoring scheme that captures data from a visual sensor on-board the UAV, and subsequently analyzes it through a specific vision processing pipeline to extract network state information. These innovative deployment strategies can be used in real-time to assess traffic conditions, while for longer periods, to validate the underlying mobility models that characterise traffic patterns.",autonomous vehicle
10.1109/TWC.2020.3007804,to_check,IEEE Transactions on Wireless Communications,IEEE,2020-11-01 00:00:00,ieeexplore,Deep Learning for Optimal Deployment of UAVs With Visible Light Communications,https://ieeexplore.ieee.org/document/9140367/,"In this paper, the problem of dynamical deployment of unmanned aerial vehicles (UAVs) equipped with visible light communication (VLC) capabilities for optimizing the energy efficiency of UAV-enabled networks is studied. In the studied model, the UAVs can simultaneously provide communications and illumination to service ground users. Since ambient illumination increases the interference over VLC links while reducing the illumination threshold of the UAVs, it is necessary to consider the illumination distribution of the target area for UAV deployment optimization. This problem is formulated as an optimization problem which jointly optimizes UAV deployment, user association, and power efficiency while meeting the illumination and communication requirements of users. To solve this problem, an algorithm that combines the machine learning framework of gated recurrent units (GRUs) with convolutional neural networks (CNNs) is proposed. Using GRUs and CNNs, the UAVs can model the long-term historical illumination distribution and predict the future illumination distribution. Given the prediction of illumination distribution, the original nonconvex optimization problem can be divided into two sub-problems and is then solved using a low-complexity, iterative algorithm. Then, the proposed algorithm enables UAVs to determine the their deployment and user association to minimize the total transmit power. Simulation results using real data from the Earth observations group (EOG) at NOAA/NCEI show that the proposed approach can achieve up to 68.9% reduction in total transmit power compared to a conventional optimal UAV deployment that does not consider the illumination distribution and user association.",autonomous vehicle
10.1109/TVT.2019.2947078,to_check,IEEE Transactions on Vehicular Technology,IEEE,2019-12-01 00:00:00,ieeexplore,Deployment Optimization of UAV Relay for Malfunctioning Base Station: Model-Free Approaches,https://ieeexplore.ieee.org/document/8867956/,"Due to the advantages of high mobility, high maneuverability and high probability of line of sight (LoS) transmission, unmanned aerial vehicles (UAVs) have attracted much interest in assisting wireless communication systems. This paper considers a set of ground users cannot receive service from the base station because of a sudden base station malfunction, a UAV is deployed in the air to work as a relay to establish communication links between uncovered users and the neighboring base station. We are going to optimize the UAV relay deployment, aiming to maximize the capacity of the relay network. Considering the channel model and exact positions of ground users are priori unknown, the problem of deployment optimization can't be solved directly because of lacking parameters. Thus, considering multiple detecting UAVs are available and only one is available, model-free online deployment approaches are respectively proposed to solve this challenging problem. The optimal relay deployment is acquired via online learning and iteration without the knowledge of channel model and exact positions of the users but with the real-time measurement of relay capacity, thus the proposed model-free deployment approaches can be adaptive to the practical communication environment compared to the model-based optimization. Simulation results show that with the proposed model-free deployment approaches, relay network capacities are close to the optimum.",autonomous vehicle
10.1109/JSTARS.2020.2969809,to_check,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,IEEE,2020-01-01 00:00:00,ieeexplore,EmergencyNet: Efficient Aerial Image Classification for Drone-Based Emergency Monitoring Using Atrous Convolutional Feature Fusion,https://ieeexplore.ieee.org/document/9050881/,"Deep learning-based algorithms can provide state-of-the-art accuracy for remote sensing technologies such as unmanned aerial vehicles (UAVs)/drones, potentially enhancing their remote sensing capabilities for many emergency response and disaster management applications. In particular, UAVs equipped with camera sensors can operating in remote and difficult to access disaster-stricken areas, analyze the image and alert in the presence of various calamities such as collapsed buildings, flood, or fire in order to faster mitigate their effects on the environment and on human population. However, the integration of deep learning introduces heavy computational requirements, preventing the deployment of such deep neural networks in many scenarios that impose low-latency constraints on inference, in order to make mission-critical decisions in real time. To this end, this article focuses on the efficient aerial image classification from on-board a UAV for emergency response/monitoring applications. Specifically, a dedicated Aerial Image Database for Emergency Response applications is introduced and a comparative analysis of existing approaches is performed. Through this analysis a lightweight convolutional neural network architecture is proposed, referred to as EmergencyNet, based on atrous convolutions to process multiresolution features and capable of running efficiently on low-power embedded platforms achieving upto 20× higher performance compared to existing models with minimal memory requirements with less than 1% accuracy drop compared to state-of-the-art models.",autonomous vehicle
http://arxiv.org/abs/2104.14006v1,to_check,arxiv,arxiv,2021-04-28 20:24:10+00:00,arxiv,"EmergencyNet: Efficient Aerial Image Classification for Drone-Based
  Emergency Monitoring Using Atrous Convolutional Feature Fusion",http://arxiv.org/abs/2104.14006v1,"Deep learning-based algorithms can provide state-of-the-art accuracy for
remote sensing technologies such as unmanned aerial vehicles (UAVs)/drones,
potentially enhancing their remote sensing capabilities for many emergency
response and disaster management applications. In particular, UAVs equipped
with camera sensors can operating in remote and difficult to access
disaster-stricken areas, analyze the image and alert in the presence of various
calamities such as collapsed buildings, flood, or fire in order to faster
mitigate their effects on the environment and on human population. However, the
integration of deep learning introduces heavy computational requirements,
preventing the deployment of such deep neural networks in many scenarios that
impose low-latency constraints on inference, in order to make mission-critical
decisions in real time. To this end, this article focuses on the efficient
aerial image classification from on-board a UAV for emergency
response/monitoring applications. Specifically, a dedicated Aerial Image
Database for Emergency Response applications is introduced and a comparative
analysis of existing approaches is performed. Through this analysis a
lightweight convolutional neural network architecture is proposed, referred to
as EmergencyNet, based on atrous convolutions to process multiresolution
features and capable of running efficiently on low-power embedded platforms
achieving upto 20x higher performance compared to existing models with minimal
memory requirements with less than 1% accuracy drop compared to
state-of-the-art models.",autonomous vehicle
http://arxiv.org/abs/2107.13869v2,to_check,arxiv,arxiv,2021-07-29 10:11:36+00:00,arxiv,"Autonomous UAV Base Stations for Next Generation Wireless Networks: A
  Deep Learning Approach",http://arxiv.org/abs/2107.13869v2,"To address the ever-growing connectivity demands of wireless communications,
the adoption of ingenious solutions, such as Unmanned Aerial Vehicles (UAVs) as
mobile Base Stations (BSs), is imperative. In general, the location of a UAV
Base Station (UAV-BS) is determined by optimization algorithms, which have high
computationally complexities and place heavy demands on UAV resources. In this
paper, we show that a Convolutional Neural Network (CNN) model can be trained
to infer the location of a UAV-BS in real time. In so doing, we create a
framework to determine the UAV locations that considers the deployment of
Mobile Users (MUs) to generate labels by using the data obtained from an
optimization algorithm. Performance evaluations reveal that once the CNN model
is trained with the given labels and locations of MUs, the proposed approach is
capable of approximating the results given by the adopted optimization
algorithm with high fidelity, outperforming Reinforcement Learning (RL)-based
approaches. We also explore future research challenges and highlight key
issues.",autonomous vehicle
http://arxiv.org/abs/1912.00752v3,to_check,arxiv,arxiv,2019-11-28 03:03:24+00:00,arxiv,"Deep Learning for Optimal Deployment of UAVs with Visible Light
  Communications",http://arxiv.org/abs/1912.00752v3,"In this paper, the problem of dynamical deployment of unmanned aerial
vehicles (UAVs) equipped with visible light communication (VLC) capabilities
for optimizing the energy efficiency of UAV-enabled networks is studied. In the
studied model, the UAVs can simultaneously provide communications and
illumination to service ground users. Since ambient illumination increases the
interference over VLC links while reducing the illumination threshold of the
UAVs, it is necessary to consider the illumination distribution of the target
area for UAV deployment optimization. This problem is formulated as an
optimization problem which jointly optimizes UAV deployment, user association,
and power efficiency while meeting the illumination and communication
requirements of users. To solve this problem, an algorithm that combines the
machine learning framework of gated recurrent units (GRUs) with convolutional
neural networks (CNNs) is proposed. Using GRUs and CNNs, the UAVs can model the
long-term historical illumination distribution and predict the future
illumination distribution. Given the prediction of illumination distribution,
the original nonconvex optimization problem can be divided into two
sub-problems and is then solved using a low-complexity, iterative algorithm.
Then, the proposed algorithm enables UAVs to determine the their deployment and
user association to minimize the total transmit power. Simulation results using
real data from the Earth observations group (EOG) at NOAA/NCEI show that the
proposed approach can achieve up to 68.9% reduction in total transmit power
compared to a conventional optimal UAV deployment that does not consider the
illumination distribution and user association.",autonomous vehicle
http://arxiv.org/abs/1907.12650v2,to_check,arxiv,arxiv,2019-07-30 15:41:01+00:00,arxiv,"Beyond Safety Drivers: Staffing a Teleoperations System for Autonomous
  Vehicles",http://arxiv.org/abs/1907.12650v2,"Driverless vehicles promise a host of societal benefits including
dramatically improved safety, increased accessibility, greater productivity,
and higher quality of life. As this new technology approaches widespread
deployment, both industry and government are making provisions for
teleoperations systems, in which remote human agents provide assistance to
driverless vehicles. This assistance can involve real-time remote operation and
even ahead-of-time input via human-in-the-loop artificial intelligence systems.
In this paper, we address the problem of staffing such a remote support center.
Our analysis focuses on the tradeoffs between the total number of remote
agents, the reliability of the remote support system, and the resulting safety
of the driverless vehicles. By establishing a novel connection between queues
with large batch arrivals and storage processes, we determine the probability
of the system exceeding its service capacity. This connection drives our
staffing methodology. We also develop a numerical method to compute the exact
staffing level needed to achieve various performance measures. This moment
generating function based technique may be of independent interest, and our
overall staffing analysis may be of use in other applications that combine
human expertise and automated systems.",autonomous vehicle
http://arxiv.org/abs/2102.13253v1,to_check,arxiv,arxiv,2021-02-26 01:31:28+00:00,arxiv,"On the Visual-based Safe Landing of UAVs in Populated Areas: a Crucial
  Aspect for Urban Deployment",http://arxiv.org/abs/2102.13253v1,"Autonomous landing of Unmanned Aerial Vehicles (UAVs) in crowded scenarios is
crucial for successful deployment of UAVs in populated areas, particularly in
emergency landing situations where the highest priority is to avoid hurting
people. In this work, a new visual-based algorithm for identifying Safe Landing
Zones (SLZ) in crowded scenarios is proposed, considering a camera mounted on
an UAV, where the people in the scene move with unknown dynamics. To do so, a
density map is generated for each image frame using a Deep Neural Network, from
where a binary occupancy map is obtained aiming to overestimate the people's
location for security reasons. Then, the occupancy map is projected to the
head's plane, and the SLZ candidates are obtained as circular regions in the
head's plane with a minimum security radius. Finally, to keep track of the SLZ
candidates, a multiple instance tracking algorithm is implemented using Kalman
Filters along with the Hungarian algorithm for data association. Several
scenarios were studied to prove the validity of the proposed strategy,
including public datasets and real uncontrolled scenarios with people moving in
public squares, taken from an UAV in flight. The study showed promising results
in the search of preventing the UAV from hurting people during emergency
landing.",autonomous vehicle
http://arxiv.org/abs/2011.01840v1,to_check,arxiv,arxiv,2020-11-03 16:50:37+00:00,arxiv,"Distributional Reinforcement Learning for mmWave Communications with
  Intelligent Reflectors on a UAV",http://arxiv.org/abs/2011.01840v1,"In this paper, a novel communication framework that uses an unmanned aerial
vehicle (UAV)-carried intelligent reflector (IR) is proposed to enhance
multi-user downlink transmissions over millimeter wave (mmWave) frequencies. In
order to maximize the downlink sum-rate, the optimal precoding matrix (at the
base station) and reflection coefficient (at the IR) are jointly derived. Next,
to address the uncertainty of mmWave channels and maintain line-of-sight links
in a real-time manner, a distributional reinforcement learning approach, based
on quantile regression optimization, is proposed to learn the propagation
environment of mmWave communications, and, then, optimize the location of the
UAV-IR so as to maximize the long-term downlink communication capacity.
Simulation results show that the proposed learning-based deployment of the
UAV-IR yields a significant advantage, compared to a non-learning UAV-IR, a
static IR, and a direct transmission schemes, in terms of the average data rate
and the achievable line-of-sight probability of downlink mmWave communications.",autonomous vehicle
http://arxiv.org/abs/1909.07554v1,to_check,arxiv,arxiv,2019-09-17 02:22:09+00:00,arxiv,"Gated Recurrent Units Learning for Optimal Deployment of Visible Light
  Communications Enabled UAVs",http://arxiv.org/abs/1909.07554v1,"In this paper, the problem of optimizing the deployment of unmanned aerial
vehicles (UAVs) equipped with visible light communication (VLC) capabilities is
studied. In the studied model, the UAVs can simultaneously provide
communications and illumination to service ground users. Ambient illumination
increases the interference over VLC links while reducing the illumination
threshold of the UAVs. Therefore, it is necessary to consider the illumination
distribution of the target area for UAV deployment optimization. This problem
is formulated as an optimization problem whose goal is to minimize the total
transmit power while meeting the illumination and communication requirements of
users. To solve this problem, an algorithm based on the machine learning
framework of gated recurrent units (GRUs) is proposed. Using GRUs, the UAVs can
model the long-term historical illumination distribution and predict the future
illumination distribution. In order to reduce the complexity of the prediction
algorithm while accurately predicting the illumination distribution, a Gaussian
mixture model (GMM) is used to fit the illumination distribution of the target
area at each time slot. Based on the predicted illumination distribution, the
optimization problem is proved to be a convex optimization problem that can be
solved by using duality. Simulations using real data from the Earth
observations group (EOG) at NOAA/NCEI show that the proposed approach can
achieve up to 22.1% reduction in transmit power compared to a conventional
optimal UAV deployment that does not consider the illumination distribution.
The results also show that UAVs must hover at areas having strong illumination,
thus providing useful guidelines on the deployment of VLC-enabled UAVs.",autonomous vehicle
http://arxiv.org/abs/1806.07987v2,to_check,arxiv,arxiv,2018-06-20 21:12:43+00:00,arxiv,"A Hierarchical Deep Architecture and Mini-Batch Selection Method For
  Joint Traffic Sign and Light Detection",http://arxiv.org/abs/1806.07987v2,"Traffic light and sign detectors on autonomous cars are integral for road
scene perception. The literature is abundant with deep learning networks that
detect either lights or signs, not both, which makes them unsuitable for
real-life deployment due to the limited graphics processing unit (GPU) memory
and power available on embedded systems. The root cause of this issue is that
no public dataset contains both traffic light and sign labels, which leads to
difficulties in developing a joint detection framework. We present a deep
hierarchical architecture in conjunction with a mini-batch proposal selection
mechanism that allows a network to detect both traffic lights and signs from
training on separate traffic light and sign datasets. Our method solves the
overlapping issue where instances from one dataset are not labelled in the
other dataset. We are the first to present a network that performs joint
detection on traffic lights and signs. We measure our network on the
Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small
Traffic Lights benchmark for traffic light detection and show it outperforms
the existing Bosch Small Traffic light state-of-the-art method. We focus on
autonomous car deployment and show our network is more suitable than others
because of its low memory footprint and real-time image processing time.
Qualitative results can be viewed at https://youtu.be/_YmogPzBXOw",autonomous vehicle
http://arxiv.org/abs/2009.14349v3,to_check,arxiv,arxiv,2020-09-30 00:01:54+00:00,arxiv,"Computing Systems for Autonomous Driving: State-of-the-Art and
  Challenges",http://arxiv.org/abs/2009.14349v3,"The recent proliferation of computing technologies (e.g., sensors, computer
vision, machine learning, and hardware acceleration), and the broad deployment
of communication mechanisms (e.g., DSRC, C-V2X, 5G) have pushed the horizon of
autonomous driving, which automates the decision and control of vehicles by
leveraging the perception results based on multiple sensors. The key to the
success of these autonomous systems is making a reliable decision in real-time
fashion. However, accidents and fatalities caused by early deployed autonomous
vehicles arise from time to time. The real traffic environment is too
complicated for current autonomous driving computing systems to understand and
handle. In this paper, we present state-of-the-art computing systems for
autonomous driving, including seven performance metrics and nine key
technologies, followed by twelve challenges to realize autonomous driving. We
hope this paper will gain attention from both the computing and automotive
communities and inspire more research in this direction.",autonomous vehicle
http://arxiv.org/abs/2001.00048v1,to_check,arxiv,arxiv,2019-12-31 19:41:59+00:00,arxiv,"MIR-Vehicle: Cost-Effective Research Platform for Autonomous Vehicle
  Applications",http://arxiv.org/abs/2001.00048v1,"This paper illustrates the MIR (Mobile Intelligent Robotics) Vehicle: a
feasible option of transforming an electric ride-on-car into a modular Graphics
Processing Unit (GPU) powered autonomous platform equipped with the capability
that supports test and deployment of various intelligent autonomous vehicles
algorithms. To use a platform for research, two components must be provided:
perception and control. The sensors such as incremental encoders, an Inertial
Measurement Unit (IMU), a camera, and a LIght Detection And Ranging (LIDAR)
must be able to be installed on the platform to add the capability of
environmental perception. A microcontroller-powered control box is designed to
properly respond to the environmental changes by regulating drive and steering
motors. This drive-by-wire capability is controlled by a GPU powered laptop
computer where high-level perception algorithms are processed and complex
actions are generated by various methods including behavior cloning using deep
neural networks. The main goal of this paper is to provide an adequate and
comprehensive approach for fabricating a cost-effective platform that would
contribute to the research quality from the wider community. The proposed
platform is to use a modular and hierarchical software architecture where the
lower and simpler motor controls are taken care of by microcontroller programs,
and the higher and complex algorithms are processed by a GPU powered laptop
computer. The platform uses the Robot Operating System (ROS) as middleware to
maintain the modularity of the perceptions and decision-making modules. It is
expected that the level three and above autonomous vehicle systems and Advanced
Driver Assistance Systems (ADAS) can be tested on and deployed to the platform
with a decent real-time system behavior due to the capabilities and
affordability of the proposed platform.",autonomous vehicle
http://arxiv.org/abs/1712.02294v4,to_check,arxiv,arxiv,2017-12-06 17:20:21+00:00,arxiv,Joint 3D Proposal Generation and Object Detection from View Aggregation,http://arxiv.org/abs/1712.02294v4,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",autonomous vehicle
10.1016/j.treng.2021.100068,to_check,Transportation Engineering,scopus,2021-06-01,sciencedirect,Real-time traffic quantization using a mini edge artificial intelligence platform,https://api.elsevier.com/content/abstract/scopus_id/85111397458,"Traffic analysis is dependent on reliable and accurate datasets that quantify the vehicle composition, speed and traffic density over a long period of time. The utilisation of big data is required if equitable and efficient transportation networks are to be realised for smart, interconnected cities of the future. The rapid and widespread adoption of digital twins, IoT (Internet of Things), artificial intelligence and mini edge computing technologies serve as the catalyst to rapidly develop and deploy smart systems for real-time data acquisition of traffic in and around urban and metropolitan areas. This paper presents a proof of concept of a mini edge computing platform for real-time edge processing, which serves as a digital twin of a multi-lane freeway located in Pretoria, South Africa. Video data acquired from an Unmanned Aerial Vehicle (UAV) is processed using a neural network architecture designed for real-time object detection tracking of vehicles. The implementation successfully counted vehicles (cars and trucks) together with an estimation of the speed of each detected vehicle. These results compare favourably to the ground truth data with vehicle counting accuracies of 5% realised. Detection of sparse motorcycles and pedestrians were less than optimal. This proof of concept can be easily scaled and deployed over a wide geographic area. Integration of these cyber-physical assets can be incorporated into existing video monitoring systems or fused with optical sensors as a single data acquisition system.",autonomous vehicle
10.1016/j.procs.2021.02.012,to_check,Procedia Computer Science,scopus,2021-01-01,sciencedirect,The impact of the soft errors in convolutional neural network on GPUS: Alexnet as case study,https://api.elsevier.com/content/abstract/scopus_id/85105461752,"Convolutional Neural Networks (CNNs) have been increasingly deployed in many applications, including safety critical system such as healthcare and autonomous vehicles. Meanwhile, the vulnerability of CNN model to soft errors (e.g., caused by radiation induced) rapidly increases, thus reliability is crucial especially in real-time system. There are many traditional techniques for improve the reliability of the system, e.g., Triple Modular Redundancy, but these techniques incur high overheads, which makes them hard to deploy. In this paper, we experimentally evaluate the vulnerable parts of Alexnet mode (e.g., fault injector). Results show that FADD and LD are the top vulnerable instructions against soft errors for Alexnet model, both instructions generate at least 84% of injected faults as SDC errors. Thus, these the only parts of the Alexnet model that need to be hardened instead of using fully duplication solutions.",autonomous vehicle
10.1016/j.rse.2020.111717,to_check,Remote Sensing of Environment,scopus,2020-05-01,sciencedirect,Assessing the relationship between macro-faunal burrowing activity and mudflat geomorphology from UAV-based Structure-from-Motion photogrammetry,https://api.elsevier.com/content/abstract/scopus_id/85079899077,"Characterisation of the ecosystem functioning of mudflats requires insight on the morphology and facies of these coastal features, but also on biological processes that influence mudflat geomorphology, such as crab bioturbation and the formation of benthic biofilms, as well as their heterogeneity at cm or less scales. Insight into this fine scale of ecosystem functioning is also important as far as minimizing errors in upscaling are concerned. The realisation of high-resolution ground surveys of these mudflats without perturbing their surface is a real challenge. Here, we address this challenge using UAV-supported photogrammetry based on the Structure-from-Motion (SfM) workflow. We produced a Digital Surface Model (DSM) and an orthophotograph at 1 cm and 0.5 cm pixel resolutions, respectively, of a mudflat in French Guiana, and mapped and classed into different size ranges intricate morphological features, including crab burrow apertures, tidal drainage creeks and depressions. We also determined subtle facies and elevation changes and slopes, and the footprint of different degrees of benthic biofilm development. The results generated at this scale of photogrammetric analysis also enabled us to relate macrofaunal crab burrowing activity to various parameters, including mudflat elevation, spatial distribution and sizes of creeks and depressions, benthic biofilm distribution, and flooding duration. SfM photogrammetry offers interesting new perspectives in fine-scale characterisation of the geomorphology, benthic activity and degree of biofilm development of dynamic muddy intertidal environments that are generally difficult of access. The main shortcomings highlighted in this study are a drift of accuracy of the DSM outside areas of ground control points and the deployment of which perturb the mudflat morphology and biology, the water-logged or very wet surfaces which generate reconstruction artefacts through the sun glint effect, and the time-consuming task of manual interpretation of extraction of features such as crab burrow apertures. On-going developments in UAV positioning integrating RTK/PPK GPS solutions for image-georeferencing and precise orientation with high-quality inertial measurement units will limit the difficulties inherent to ground control points, while conduction of surveys during homogeneous cloudy conditions could reduce the sun-glint effect. Manual extraction of image features could be automated in the future through the use of deep-learning algorithms.",autonomous vehicle
10.1016/j.robot.2020.103472,to_check,Robotics and Autonomous Systems,scopus,2020-04-01,sciencedirect,Deploying MAVs for autonomous navigation in dark underground mine environments,https://api.elsevier.com/content/abstract/scopus_id/85079573394,"Operating Micro Aerial Vehicles (MAVs) in subterranean environments is becoming more and more relevant in the field of aerial robotics. Despite the large spectrum of technological advances in the field, flying in such challenging environments is still an ongoing quest that requires the combination of multiple sensor modalities like visual/thermal cameras as well as 3D and 2D lidars. Nevertheless, there exist cases in subterranean environments where the aim is to deploy fast and lightweight aerial robots for area reckoning purposes after an event (e.g. blasting in production areas). This work proposes a novel baseline approach for the navigation of resource constrained robots, introducing the aerial underground scout, with the main goal to rapidly explore unknown areas and provide a feedback to the operator. The main proposed framework focuses on the navigation, control and vision capabilities of the aerial platforms with low-cost sensor suites, contributing significantly towards real-life applications. The merit of the proposed control architecture is that it considers the flying platform as a floating object, composing a velocity controller on the 
                        x
                     , 
                        y
                      axes and altitude control to navigate along the tunnel. Two novel approaches make up the cornerstone of the proposed contributions for the task of navigation: (1) a vector geometry method based on 2D lidar, and (2) a Deep Learning (DL) method through a classification process based on an on-board image stream, where both methods correct the heading towards the center of the mine tunnel. Finally, the framework has been evaluated in multiple field trials in an underground mine in Sweden.",autonomous vehicle
10.1016/j.ifacol.2020.12.1459,to_check,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Deep learning based segmentation of fish in noisy forward looking MBES images,https://api.elsevier.com/content/abstract/scopus_id/85105082300,"In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments – where compute and power are restricted by size/cost – for testing and prototyping.",autonomous vehicle
10.1016/j.asoc.2018.12.013,to_check,Applied Soft Computing Journal,scopus,2019-03-01,sciencedirect,Online identification of a rotary wing Unmanned Aerial Vehicle from data streams,https://api.elsevier.com/content/abstract/scopus_id/85059117819,"Until now the majority of the neuro and fuzzy modeling and control approaches for rotary wing Unmanned Aerial Vehicles (UAVs), such as the quadrotor, have been based on batch learning techniques, therefore static in structure, and cannot adapt to rapidly changing environments. Implication of Evolving Intelligent System (EIS) based model-free data-driven techniques in fuzzy system are good alternatives, since they are able to evolve both their structure and parameters to cope with sudden changes in behavior, and performs perfectly in a single pass learning mode which is suitable for online real-time deployment. The Metacognitive Scaffolding Learning Machine (McSLM) is seen as a generalized version of EIS since the metacognitive concept enables the what-to-learn, how-to-learn, and when-to-learn scheme, and the scaffolding theory realizes a plug-and-play property which strengthens the online working principle of EISs. This paper proposes a novel online identification scheme, applied to a quadrotor using real-time experimental flight data streams based on McSLM, namely Metacognitive Scaffolding Interval Type 2 Recurrent Fuzzy Neural Network (McSIT2RFNN). Our proposed approach demonstrated significant improvements in both accuracy and complexity against some renowned existing variants of the McSLMs and EISs.",autonomous vehicle
10.1109/AICAS.2019.8771482,to_check,2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS),IEEE,2019-03-20 00:00:00,ieeexplore,Accelerating CNN-RNN Based Machine Health Monitoring on FPGA,https://ieeexplore.ieee.org/document/8771482/,"Emerging artificial intelligence brings new opportunities for embedded machine health monitoring systems. However, previous work mainly focus on algorithm improvement and ignore the software-hardware co-design. This paper proposes a CNN-RNN algorithm for remaining useful life (RUL) prediction, with hardware optimization for practical deployment. The CNN-RNN algorithm combines the feature extraction ability of CNN and the sequential processing ability of RNN, which shows 23%-53% improvement on the CMAPSS dataset. This algorithm also considers hardware implementation overhead and an FPGA based accelerator is developed. The accelerator adopts kernel-optimized design to utilize data reuse and reduce memory accesses. It enables real-time response and 5.89GOPs/W energy efficiency within small size and cost overhead. The FPGA implementation shows 15× CNN speedup and 9× overall speedup compared with the embedded processor Cortex-A9.",health
10.1109/ICOEI.2019.8862754,to_check,2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI),IEEE,2019-04-25 00:00:00,ieeexplore,Machine Learning based Health Prediction System using IBM Cloud as PaaS,https://ieeexplore.ieee.org/document/8862754/,"Adaptable Critical Patient Caring system is a key concern for hospitals in developing countries like Bangladesh. Most of the hospital in Bangladesh lack serving proper health service due to unavailability of appropriate, easy and scalable smart systems. The aim of this project is to build an adequate system for hospitals to serve critical patients with a real-time feedback method. In this paper, we propose a generic architecture, associated terminology and a classificatory model for observing critical patient's health condition with machine learning and IBM cloud computing as Platform as a service (PaaS). Machine Learning (ML) based health prediction of the patients is the key concept of this research. IBM Cloud, IBM Watson studio is the platform for this research to store and maintain our data and ml models. For our ml models, we have chosen the following Base Predictors: Naïve Bayes, Logistic Regression, KNeighbors Classifier, Decision Tree Classifier, Random Forest Classifier, Gradient Boosting Classifier, and MLP Classifier. For improving the accuracy of the model, the bagging method of ensemble learning has been used. The following algorithms are used for ensemble learning: Bagging Random Forest, Bagging Extra Trees, Bagging KNeighbors, Bagging SVC, and Bagging Ridge. We have developed a mobile application named “Critical Patient Management System - CPMS” for real-time data and information view. The system architecture is designed in such a way that the ml models can train and deploy in a real-time interval by retrieving the data from IBM Cloud and the cloud information can also be accessed through CPMS in a requested time interval. To help the doctors, the ml models will predict the condition of a patient. If the prediction based on the condition gets worse, the CPMS will send an SMS to the duty doctor and nurse for getting immediate attention to the patient. Combining with the ml models and mobile application, the project may serve as a smart healthcare solution for the hospitals.",health
10.1109/ACCESS.2019.2943381,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,AESGRU: An Attention-Based Temporal Correlation Approach for End-to-End Machine Health Perception,https://ieeexplore.ieee.org/document/8847332/,"Accurate and real-time perception of the operating status of rolling bearings, which constitute a key component of rotating machinery, is of vital significance. However, most existing solutions not only require substantial expertise to conduct feature engineering, but also seldom consider the temporal correlation of sensor sequences, ultimately leading to complex modeling processes. Therefore, we present a novel model, named Attention-based Equitable Segmentation Gated Recurrent Unit Networks (AESGRU), to improve diagnostic accuracy and model-building efficiency. Specifically, our proposed AESGRU consists of two modules, an equitable segmentation approach and an improved deep model. We first transform the original dataset into time-series segments with temporal correlation, so that the model enables end-to-end learning from the strongly correlated data. Then, we deploy a single-layer bidirectional GRU network, which is enhanced by attention mechanism, to capture the long-term dependency of sensor segments and focus limited attention resources on those informative sampling points. Finally, our experimental results show that the proposed approach outperforms previous approaches in terms of the accuracy.",health
10.1109/WETICE49692.2020.00032,to_check,2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),IEEE,2020-09-13 00:00:00,ieeexplore,A Learning based Secure Anomaly Detection for Healthcare Applications,https://ieeexplore.ieee.org/document/9338574/,"The wireless body sensor network (WBSN) is an emerging technology in the healthcare domain, which collects data from vital parameters of the patient's body, using small portable components such as sensors. It allows to continuously monitor patients without restricting their movements and to inform the health specialist of the evolution of their states. However, the deployment of new technologies in health applications without taking into account data security makes the privacy of patients vulnerable. Thus, these systems have insufficient performance for large and varied data sets, because they do not process information sufficiently diverse to cover all scenarios. Moreover, the interpretation of physiological signals is a tedious process in which human errors can be caused.To address these issues, we propose, in this paper, an effective health system that ensures the secure collection and transfer of patient data to the physician, to facilitate the diagnosis and the detection of life threatening diseases. Our main contribution is to offer real-time, high quality processing, learning and analysis capabilities, in a smart and secure system. For that, we have collected real patient datasets from Tunisian doctors.To achieve this goal, we used ANN and XgBoost as learning algorithms and AES as an encryption protocol for sending secure data to medical personnel for diagnostic purposes. The results show an accuracy of 80% with an execution time of 1.54 s.",health
10.1109/IEMBS.2010.5627760,to_check,2010 Annual International Conference of the IEEE Engineering in Medicine and Biology,IEEE,2010-09-04 00:00:00,ieeexplore,Activity-based Process Mining for Clinical Pathways Computer aided design,https://ieeexplore.ieee.org/document/5627760/,"Current trends in health management improvement demand the standardization of care protocols to achieve better quality and efficiency. The use of Clinical Pathways is an emerging solution for that problem. However, current Clinical Pathways are big manuals written in natural language and highly affected by human subjectivity. These problems make the deployment and dissemination of them extremely difficult in real practice environments. In this work, a complete computer based architecture to help the representation and execution of Clinical Pathways is suggested. Furthermore, the difficulties inherent to the design of formal Clinical Pathways in this way requires new specific design tools to help making the system useful. Process Mining techniques can help to automatically infer processes definition from execution samples. Yet, the classical Process Mining paradigm is not totally compatible with the Clinical Pathways paradigm. In this paper, a pattern recognition algorithm based in an evolution of the Process Mining classical paradigm is presented and evaluated as a solution to this situation. The proposed algorithm is able to infer Clinical Pathways from execution logs to support the design of Clinical Pathways.",health
10.1109/ICPHM.2019.8819421,to_check,2019 IEEE International Conference on Prognostics and Health Management (ICPHM),IEEE,2019-06-20 00:00:00,ieeexplore,Application of Deep Learning for Fault Diagnostic in Induction Machine’s Bearings,https://ieeexplore.ieee.org/document/8819421/,"Recent developments in sensor technologies and advances in communication systems have resulted in deployment of a large number of sensors for collecting condition monitoring (CM) data in order to monitor health condition of a manufac-tring/industrial system. Efficient utilization of sensory data leads to highly accurate results in system fault diagnostics/prognostics. The exponential growth of CM data poses significant analytical challenges, due to their high variety, high dimensionality and high velocity rendering conventional health monitoring tools impractical. In this regard, the paper proposes a deep learning-based framework for fault diagnosis of an induction machine's bearing based on real data set provided by Case Western Reserve University bearing data center. In particular, we focus on deep bidirectional long short-term memory (BiD-LSTM) networks fed with raw signals for fault diagnosis to address drawbacks of conventional machine learning (ML) solutions such as support vector machines. A numerical example is provided to illustrate the complete procedure of the proposed framework, which shows the great potentials of the BiD-LSTM for detection of different types of the bearing fault with high accuracy. The effectiveness of the proposed model is demonstrated through a comparison with a recently developed deep neural network (DNN) that considers temporal coherence for the same data set. The results indicate that the proposed framework provides considerably improved performance in comparison to its counterparts.",health
10.1109/ICBAIE52039.2021.9389950,to_check,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",IEEE,2021-03-28 00:00:00,ieeexplore,Design of Cloud Computing Platform Based Accurate Measurement for Structure Monitoring Using Fiber Bragg Grating Sensors,https://ieeexplore.ieee.org/document/9389950/,"The efficient integration of distributed fiber Bragg grating (FBG) sensors and cloud computing platform is used to achieve accurate measurement and evaluation of physical quantities, which solves the problems of traditional fiber Bragg grating sensing technology for health structure monitoring system, such as the cost and space constraints, it is difficult to deploy enough servers to deal with data collection, transmission and storage in real time. The cloud platform using fiber Bragg grating sensors adopts the structure of erbium-doped fiber cascaded Bragg grating, reasonably configures the FBG demodulator acquisition and analysis software, deploys the health monitoring system in the cloud, constructs the cloud platform of high-efficiency health monitoring optical fiber sensor network, improves the scalability of the system, flexibly deploys applications and services, and ensures the security and reliability of various real-time monitoring data and professional data. It can meet the needs of some specific or wide application fields for the automation technology, structural mechanics, computer technology, Internet architecture, cloud deployment and interdisciplinary practical comprehensive valuable application research.",health
10.1109/IJCNN.2013.6707048,to_check,The 2013 International Joint Conference on Neural Networks (IJCNN),IEEE,2013-08-09 00:00:00,ieeexplore,Epidemiological dynamics modeling by fusion of soft computing techniques,https://ieeexplore.ieee.org/document/6707048/,"Infectious disease prevention and control are important in improving, promoting and protecting the health of communities. Epidemiological data analysis plays a crucial role in disease prevention and control. Conventional methods such as moving average or autoregressive analysis normally require the assumption of stationarity, which is often violated in epidemiologic time series. This paper proposes the fusion of neural networks, fuzzy systems and genetic algorithms, with the aim to strengthen the modeling power for epidemiological dynamics. We deploy an additive fuzzy system into a neural network architecture in order to incorporate recurrent nodes to enable the fuzzy system to handle temporal data. The genetic algorithm is employed to optimize the fuzzy rule structure before supervised training is applied to adjust parameters. As epidemiological time series exhibit complex behavior and possibly cyclic patterns, the addition of recurrent nodes to the fuzzy system improves the modeling capability. The proposed model dominates the benchmark feedforward neural network and adaptive neuro-fuzzy inference system model regarding modeling performance. Through real applications for epidemiologic time series modeling, the fusion of soft computing techniques offer accurate forecasts that have considerable meaning in planning infectious disease-control activities.",health
10.1109/ICIT.2015.7125235,to_check,2015 IEEE International Conference on Industrial Technology (ICIT),IEEE,2015-03-19 00:00:00,ieeexplore,Improving accuracy of long-term prognostics of PEMFC stack to estimate remaining useful life,https://ieeexplore.ieee.org/document/7125235/,"Proton Exchange Membrane Fuel cells (PEMFC) are energy systems that facilitate electrochemical reactions to create electrical energy from chemical energy of hydrogen. PEMFC are promising source of renewable energy that can operate on low temperature and have the advantages of high power density and low pollutant emissions. However, PEMFC technology is still in the developing phase, and its large-scale industrial deployment requires increasing the life span of fuel cells and decreasing their exploitation costs. In this context, Prognostics and Health Management of fuel cells is an emerging field, which aims at identifying degradation at early stages and estimating the Remaining Useful Life (RUL) for life cycle management. Indeed, due to prognostics capability, the accurate estimates of RUL enables safe operation of the equipment and timely decisions to prolong its life span. This paper contributes data-driven prognostics of PEMFC by an ensemble of constraint based Summation Wavelet-Extreme Learning Machine (SW-ELM) algorithm to improve accuracy and robustness of long-term prognostics. The SW-ELM is used for ensemble modeling due to its enhanced applicability for real applications as compared to conventional data-driven algorithms. The proposed prognostics model is validated on run-to-failure data of PEMFC stack, which had the life span of 1750 hours. The results confirm capability of the prognostics model to achieve accurate RUL estimates.",health
10.1109/QRS51102.2020.00018,to_check,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)",IEEE,2020-12-14 00:00:00,ieeexplore,PHM Technology for Memory Anomalies in Cloud Computing for IaaS,https://ieeexplore.ieee.org/document/9282796/,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier.",health
10.1109/BDS/HPSC/IDS18.2018.00045,to_check,"2018 IEEE 4th International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing, (HPSC) and IEEE International Conference on Intelligent Data and Security (IDS)",IEEE,2018-05-05 00:00:00,ieeexplore,Real-Time Intelligent Air Quality Evaluation on a Resource-Constrained Embedded Platform,https://ieeexplore.ieee.org/document/8552303/,"Indoor air quality has a major impact on health and comfort of building occupants. Poor air quality may reduce productivity in offices and impair students learning in classes. In order to provide localized air pollution data and tailor it for individual, wearable air quality sensor is a promising solution. Furthermore, crowd sensing has emerged as an Internet-of-Things (IoT) solution, which is economical, scalable and easy to deploy and re-deploy as it uses the power of crowd data collection. The goal of our proposed system is to monitor indoor air quality through a crowd sensing system that will use a set of sensors to measure air quality, monitor the concentration of pollutants continuously, and make recommendations in real time for improved air quality. In this paper artificial network is developed to perform real-time indoor air quality control. Utilizing created neural network embedded into a smart controller comfort level of air quality parameters such as temperature, CO_2 air concentration and humidity could be estimated after every measurement and used for adapting air conditioning systems to adjust air quality. Neural network data preparation and training process are discussed along with deployment of trained network on a smart controller.",health
10.1109/eScience.2019.00014,to_check,2019 15th International Conference on eScience (eScience),IEEE,2019-09-27 00:00:00,ieeexplore,SATVAM: Toward an IoT Cyber-Infrastructure for Low-Cost Urban Air Quality Monitoring,https://ieeexplore.ieee.org/document/9041703/,"Air pollution is a public health emergency in large cities. The availability of commodity sensors and the advent of Internet of Things (IoT) enable the deployment of a city-wide network of 1000's of low-cost real-time air quality monitors to help manage this challenge. This needs to be supported by an IoT cyber-infrastructure for reliable and scalable data acquisition from the edge to the Cloud. The low accuracy of such sensors also motivates the need for data-driven calibration models that can accurately predict the science variables from the raw sensor signals. Here, we offer our experiences with designing and deploying such an IoT software platform and calibration models, and validate it through a pilot field deployment at two mega-cities, Delhi and Mumbai. Our edge data service is able to even-out the differential bandwidths from the sensing devices and to the Cloud repository, and recover from transient failures. Our analytical models reduce the errors of the sensors from a best-case of 63% using the factory baseline to as low as 21%, and substantially advances the state-of-the-art in this domain.",health
10.1109/IOLTS.2015.7229845,to_check,2015 IEEE 21st International On-Line Testing Symposium (IOLTS),IEEE,2015-07-08 00:00:00,ieeexplore,Self-awareness and self-learning for resiliency in real-time systems,https://ieeexplore.ieee.org/document/7229845/,"While the notion of self-awareness has a long history in biology, psychology, medicine, engineering and (more recently) computing, we are seeing the emerging need for self-awareness in the context of complex Systems-on-Chip that must address the often conflicting requirements of performance, resiliency, energy, cost, etc. in the face of highly dynamic operational behaviors coupled with process, environment, and workload variabilities. Unlike traditional Systems-on-Chip (SoCs), self-aware SoCs must deploy an intelligent co-design of the control, communication, and computing infrastructure that interacts with the physical environment in real-time in order to modify the systems behavior so as to adaptively achieve desired objectives and Quality-of-Service (QoS). Self-aware SoCs require a combination of ubiquitous sensing and actuation, health-monitoring, and self-learning to enable the SoCs adaptation over time and space. This special session targets self-learning and self-awareness in two domains. The first one is a self-learning runtime reliability prediction approach by reusing Design-for-Test (DfT) infrastructure. The other one discusses real-time systems and applications to wireless communication, signal processing and control.",health
10.1109/PERCOMW.2017.7917620,to_check,2017 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),IEEE,2017-03-17 00:00:00,ieeexplore,Toward real-time in-home activity recognition using indoor positioning sensor and power meters,https://ieeexplore.ieee.org/document/7917620/,"Automatic recognition of activities of daily living (ADL) can be applied to realize services to support user life such as elderly monitoring, energy-saving home appliance control, and health support. In particular, “real-time” ADL recognition is essential to realize such a service that the system needs to know the user's current activity. There are many studies on ADL recognition. However, none of these studies address all of the following problems: (1) privacy intrusion due to the utilization of high privacy-invasive devices such as cameras and microphones; (2) limited number of recognizable activities; (3) low recognition accuracy; (4) high deployment and maintenance costs due to many sensors used; and (5) long recognition time. In our prior work, we proposed a system which solves the problems (1)- (4) to some extent by using user's position data and power consumption data of home electric appliances. In this paper, aiming to solve all the above problems including (5), we propose a new system by extending our prior work. To realize “real-time” ADL recognition while keeping good recognition accuracy, we developed new power meters with higher sensing frequency and introduced new techniques such as adding new features, selecting the best subset of the features, and selecting the best training dataset used for machine learning. We collected the sensor data in our smart home facility for 11 days, and applied the proposed method to these sensor data. As a result, the proposed method achieved accuracy of 79.393% in recognizing 10 types of ADLs.",health
10.1109/ICCP.2016.7737117,to_check,2016 IEEE 12th International Conference on Intelligent Computer Communication and Processing (ICCP),IEEE,2016-09-10 00:00:00,ieeexplore,Towards application of non-invasive environmental sensors for risks and activity detection,https://ieeexplore.ieee.org/document/7737117/,"One of the main goals of Ambient Assisted Living (AAL) is to provide supportive environment for the elderly or disabled. Such environments are not feasible without correctly identifying states and activities of the persons receiving the care. They rely on the interaction and processing of data originating from many components and objects in the surrounding. In order to collect the data, various sensors are used to monitor the environment, as well as the person's health parameters. One of the main concerns in AAL is preservation of user's privacy. In this paper we address that by proposing a non-intrusive approach for data collection and identification of daily activity and risks. We describe the wiring of such system based on cheap non-intrusive sensors, deployment in a real environment, the protocols for data fusion and processing, and explain how machine learning could be employed for detecting risks and activities. The main contribution of this paper is development of non-intrusive sensor kits that can be easily deployed in real-life environments and are capable of collecting data that can reliable detect activities and risk.",health
10.1109/ACCESS.2021.3051583,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Data-Driven Condition Monitoring of Mining Mobile Machinery in Non-Stationary Operations Using Wireless Accelerometer Sensor Modules,https://ieeexplore.ieee.org/document/9324826/,"This paper presents the development of an easy-to-deploy and smart monitoring IoT system that utilizes vibration measurement devices to assess real-time condition of bulldozers, power shovels and backhoes, in non-stationary operations in the mining industry. According to operating experience data and the type of mining machine, total loss failure rates per machine fleet can reach up to 30%. Vibration analysis techniques are commonly used for condition monitoring and early detection of unforeseen failures to generate predictive maintenance plans for heavy machinery. However, this maintenance strategy is intensively used only for stationary machines and/or mobile machinery in stationary operations. Today, there is a lack of proper solutions to detect and prevent critical failures for non-stationary machinery. This paper shows a cost-effective solution proposal for implementing a vibration sensor network with wireless communication and machine learning data-driven capabilities for condition monitoring of non-stationary heavy machinery in mining operations. During the machine operation, 3-axis accelerations were measured using two sensors deployed across the machine. The machine accelerations (amplitudes and frequencies) are measured in two different frequency spectrums to improve each sensing location's time resolution. Multiple machine learning algorithms use this machine data to assess conditions according to manufacturer recommendations and operational benchmarks Proposed data-driven machine learning models classify the machine condition in states according to the ISO 2372 standards for vibration severity: Good, Acceptable, Unsatisfactory, or Unacceptable. After performing field tests with bulldozers and backhoes from different manufacturers, the machine learning algorithms are able to classify machine health status with an accuracy between 85% - 95%. Moreover, the system allows early detection of “Unacceptable” states between 120 to 170 hours prior to critical failure. These results demonstrate that the proposed system will collect relevant data to generate predictive maintenance plans and avoid unplanned downtimes.",health
10.1109/ACCESS.2021.3101647,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Gas Path Fault Diagnosis of Gas Turbine Engine Based on Knowledge Data-Driven Artificial Intelligence Algorithm,https://ieeexplore.ieee.org/document/9502707/,"As the core power for the aviation industry, shipbuilding industry, and power station industry, it is essential to ensure that the gas turbines operate safely, reliably, greenly and efficiently. Learn from the advantages and disadvantages of the thermodynamic model based and data-driven artificial intelligence based gas-path diagnosis methods, a newfangled gas turbine gas-path diagnosis approach on the basis of knowledge data-driven artificial intelligence is proposed. That is a hybrid method of deep learning and gas path analysis. First, gas turbine thermodynamic model of the object to be diagnosed is constructed by adaptation modeling strategy. And the engine thermodynamic model is taken as the basal model to simulate various gas path faults. Secondly, a large number of knowledge data corresponding to component health parameters and gas turbine boundary condition parameters &amp; gas-path measurable parameters are simulated by setting different component health parameter values and different boundary conditions based on this basal model. And next, define the vector composed of the boundary condition parameters &amp; the gas path measurable parameters in the knowledge database as the input vector, and the component health parameter vector as the output vector, and a deep learning model for regression modeling of this knowledge database is designed. At last, along with the gas turbine engine runs, the trained model outputs component health parameters in real time after trained deep learning model is deployed to the corresponding gas turbine power plant. The simulation experiment results show that, accurate and quantified health parameters of each gas path component can be obtained by the proposed method in this paper, and the overall root mean square error does not exceed 0.033%, and the maximum relative error does not exceed 0.36%, which illustrates the proposed method has great application potential.",health
10.1109/WF-IoT.2019.8767231,to_check,2019 IEEE 5th World Forum on Internet of Things (WF-IoT),IEEE,2019-04-18 00:00:00,ieeexplore,Efficient Deployment of Predictive Analytics in Edge Gateways: Fall Detection Scenario,https://ieeexplore.ieee.org/document/8767231/,"Ambient Assisted Living (AAL) represents the most promising Internet of Things (IoT) application due to its relevance in the elders healthcare and improvement of their quality of life. Recently, the AAL IoT ecosystem has been enriched with promising technologies such as edge computing, which has demonstrated to be the best approach to overcome the demanding requirements of AAL and healthcare services by providing a reduction of the amount of data to transfer to the cloud, an improvement of the response time, and quality of experience. Also, the deployment of Artificial Intelligence (AI) technologies at the edge provides intelligence to improve the decision making timely. However, this approach has been scarcely studied in AAL scenarios and the few proposals based on deploying machine learning models at the edge lack efficiency, security, mechanisms of resource management, service management, and deployment, as well as a real and experimental AAL scenario. For these reasons, this paper proposes an innovative edge gateway architecture to support the deployment of deep learning (DL) models in AAL and healthcare scenarios efficiently. To do so, we have added a predictive analytics module to deploy the models. Since AI technologies demand more resources, a container-based virtualization technology is employed on the edge gateway to manage the limited resources, and provide security and lifecycle services management. The edge gateway performance was evaluated deploying a DL-based fall detection application on it. As a result, our approach improves the inference time compared to that based on the cloud in 34 seconds and to similar approaches in 8 seconds.",health
10.1109/ICACITE51222.2021.9404731,to_check,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),IEEE,2021-03-05 00:00:00,ieeexplore,Face Mask Detection System using CNN,https://ieeexplore.ieee.org/document/9404731/,"Amid current pandemic, Covid-19 has made us realize the importance of Face Masks and we need to understand the crucial effects of not wearing one, now more than ever. Right now, there are no face mask detectors installed at the crowded places. But we believe that it is of utmost importance that at transportation junctions, densely populated residential area, markets, educational institutions and healthcare areas, it is now very important to set up face mask detectors to ensure the safety of the public. In this paper we have tried to build a two phased face mask detector which will be easy to deploy at the mentioned outlets. With the help of Computer Vision, it is now possible to detect and implement this on large scale. We have used CNN/ MobileNet V2 architecture for the implementation of our model. The implementation is done in Python, and the python script implementation will train our face mask detector on our selected dataset using TensorFlow and Keras. We have added more robust features and trained our model on various variations, we made sure to have large varied and augmented dataset so that the model is able to clearly identify and detection the face masks in real time videos. The trained model was tested on both real-time videos and static pictures and in both the cases the accuracy was more than the other designed models.",health
10.1109/ICCE-Berlin.2018.8576251,to_check,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),IEEE,2018-09-05 00:00:00,ieeexplore,CNN Inference: Dynamic and Predictive Quantization,https://ieeexplore.ieee.org/document/8576251/,"Deep Learning techniques like Convolutional Neural Networks (CNN) are the de-facto method for image classification with broad usage spanning across automotive, industrial, medicine, robotics etc. Efficient implementation of CNN inference on embedded device requires a quantization method, which minimizes the accuracy loss, ability to generalize across deployment scenarios as well as real-time processing. Existing literature doesn't address all these three requirements simultaneously. In this paper, we propose a novel quantization algorithm to overcome above mentioned challenges. The proposed solution dynamically selects the scale for quantizing activations and uses Kalman filter to predict quantization scale to reduce accuracy loss. The proposed solution exploits the range statistics from previous inference processes to estimate quantization scale, enabling real-time solution. The proposed solution is implemented on TI's TDA family of embedded automotive processors. The proposed solution is running real time semantic segmentation on TDA2x processor within 0.1% accuracy loss compared floating point algorithm. The solution performs well across multiple deployment scenarios (e.g. rain, snow, night etc) demonstrating generalization capability of the solution.",health
10.1109/SMARTCOMP52413.2021.00027,to_check,2021 IEEE International Conference on Smart Computing (SMARTCOMP),IEEE,2021-08-27 00:00:00,ieeexplore,ARIS: A Real Time Edge Computed Accident Risk Inference System,https://ieeexplore.ieee.org/document/9556252/,"To deploy an intelligent transport system in urban environment, an effective and real-time accident risk prediction method is required that can help maintain road safety, provide adequate level of medical assistance and transport in case of an emergency. Reducing traffic accidents is an important problem for increasing public safety, so accident analysis and prediction have been a subject of extensive research in recent time. Even if a traffic hazard occurs, a readily deployable structure with an accurate prediction of accident can contribute to better management of rescue resources. But the significant shortcomings of current studies are the use of small-scale datasets with minimal scope, being based on extensive data sets, and not being applicable for real-time purposes. To overcome these challenges, we propose ARIS: a system for real-time traffic accident prediction built on a traffic accident dataset named ‘US-Accidents’ which covers 49 states of United States, collected from February 2016 to June 2020. Our approach is based on a deep neural network model that utilizes a variety of data characteristics, such as time-sensitive weather data, textual information, and discerning factors. We have tested ARIS against multiple baselines through a comprehensive series of experiments across several major cities of USA, and we have noticed significant improvement during inference especially in detecting accident classes. Additionally, to make our model edge-implementable we have compressed our model using a joint technique of magnitude-based weight pruning and model quantization. We have also demonstrated the inference results along with power consumption profiling after deploying the model on a resource constrained environment that consists of Intel Neural Compute Stick 2 (NCS2) with Raspberry Pi 4B (RPi4). Our investigation and observations indicate major improvements to predict unusual traffic accident event even after model compression and deployment. We have managed to reduce the model size and inference time by ≈ 6x, and ≈ 70 % respectively with insignificant drop in performance. Furthermore, to better understand the importance of each individual type of variables used in our analysis, we have showcased a comprehensive ablation study.",health
10.1109/TENCON.2019.8929612,to_check,TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON),IEEE,2019-10-20 00:00:00,ieeexplore,Lung Nodule Detection from low dose CT scan using Optimization on Intel Xeon and Core processors with Intel Distribution of OpenVINO Toolkit,https://ieeexplore.ieee.org/document/8929612/,"With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance.",health
10.1109/TASE.2018.2846795,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2019-04-01 00:00:00,ieeexplore,Nonparametric Activity Recognition System in Smart Homes Based on Heterogeneous Sensor Data,https://ieeexplore.ieee.org/document/8413102/,"Throughout the course of life, there is time when we live independently in our house without anyone to look after each other. In order to support these people and ensure their safety with limited medical resources and human labors, it is important to constantly monitor one's activity of daily living (ADL). Therefore, we propose an activity recognition (AR) system for people living independently in smart homes to achieve the concept of “aging in place.” The AR model adopted by the proposed system is powerful to recognize meaningful ADL by integrating heterogeneous data from both ambient and on-body sensors. Moreover, this proposed system adopts a nonparametric approach, which requires much fewer efforts from humans. The average AR precision and recall rates of this proposed system are up to 98.7% and 99.0%, which indicates its feasibility of deployment in a real-life home environment for monitoring users' ADL with promising performance and thus helps realize “aging in place.” Note to Practitioners-With the advance of Internet of Things technologies, it is convenient to collect data about ADL in a smart home environment via ambient sensors and on-body sensors. This proposed system integrates data from these two heterogeneous sensors and discovers potential activities automatically without user labeling or parameter setting. By reducing the above-mentioned user efforts, it is more suitable for users and thus greatly helps realize the concept of “aging in place.”",health
10.1109/ICCCBDA51879.2021.9442522,to_check,2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),IEEE,2021-04-26 00:00:00,ieeexplore,A FPGA Deployment System Based on Convolutional Neural Network for Rolling Bearing Diagnosis,https://ieeexplore.ieee.org/document/9442522/,"Real-time fault diagnosis of rolling bearing is a challenging issue for industry. Although artificial intelligence-based technologies could be well used for fault diagnosis of rolling bearing, the factories may not take into account the deployment of diagnosis algorithms. To tackle the issue, this work proposes a flexible deployment system of diagnosis algorithm for rolling bearing, where the required Convolutional Neural Network (CNN) model is deployed on the Field Programmable Gate Array (FPGA) to identify the working conditions of rolling bearing using vibration signals. The experimental results show that the deployed system performs accurately and efficiently on the test set, while a real-time prediction of FPGA could be guaranteed, indicating its potential as a powerful auxiliary system of rotating machinery.",health
10.1109/JIOT.2020.2994200,to_check,IEEE Internet of Things Journal,IEEE,2020-10-01 00:00:00,ieeexplore,Optimization of Edge-PLC-Based Fault Diagnosis With Random Forest in Industrial Internet of Things,https://ieeexplore.ieee.org/document/9091605/,"Facing globalized competition, there have been increasing requirements for safety and efficiency in smart factories, where the industrial Internet of Things can enable the monitoring of equipment's status and the detecting of faults before they go critical. Regarding cloud computing, data-driven methods running at clouds are adopted to train the model with a large amount of raw data at the beginning, then end machines upload their real-time readings to the cloud center for processing. However, this incurs considerable computational costs and may sometimes bear a severe delay. In this article, we consider a hierarchical structure where edge-PLCs are employed to gather sensed data locally and reduce communication costs. Since a single fault may be related to multiple influencing features, we want to first minimize the number of features that need to determine a fault, then try to find out the minimal set of edge-PLCs which can cover all key features so as to save the deployment cost. We propose a random-forest-based method to handle the features selection problem, and then the selection of edge-PLCs by solving the set coverage problem. Through the simulation on real data trace, we compare our method with other artificial-intelligence-based methods, such as the logistics regression model and its extensions. The results prove the efficiency and performance of the proposed method, which reaches or even exceeds the accuracy of methods using the full set of data.",health
10.1109/ISBI.2019.8759511,to_check,2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),IEEE,2019-04-11 00:00:00,ieeexplore,Real-Time Informative Laryngoscopic Frame Classification with Pre-Trained Convolutional Neural Networks,https://ieeexplore.ieee.org/document/8759511/,"Visual exploration of the larynx represents a relevant technique for the early diagnosis of laryngeal disorders. However, visualizing an endoscopy for finding abnormalities is a time-consuming process, and for this reason much research has been dedicated to the automatic analysis of endoscopic video data. In this work we address the particular task of discriminating among informative laryngoscopic frames and those that carry insufficient diagnostic information. In the latter case, the goal is also to determine the reason for this lack of information. To this end, we analyze the possibility of training three different state-of-the-art Convolutional Neural Networks, but initializing their weights from configurations that have been previously optimized for solving natural image classification problems. Our findings show that the simplest of these three architectures not only is the most accurate (outperforming previously proposed techniques), but also the fastest and most efficient, with the lowest inference time and minimal memory requirements, enabling real-time application and deployment in portable devices.",health
10.1109/JIOT.2019.2900093,to_check,IEEE Internet of Things Journal,IEEE,2019-06-01 00:00:00,ieeexplore,A REM-Enabled Diagnostic Framework in Cellular-Based IoT Networks,https://ieeexplore.ieee.org/document/8643780/,"With the flourishing growth of Internet-of-Things (IoT) applications, cellular-assisted IoT networks are promising to support the ever-increasing wireless traffic demands generated by various kinds of IoT devices. As one of the emerging technologies in the next-generation cellular systems, small cells, or so-called “femtocells” in indoor environments, are expected to be densely deployed for the ubiquitous coverage and capacity enhancement in a cost-effective and energy-efficient way. However, unplanned femtocell deployment and complex interior layouts of buildings may lead to severe coverage and capacity problems and even possible negative impacts on the dense deployment. In this paper, we aim at developing a data-driven indoor diagnostic framework for fault detection in cellular-assisted IoT networks. The framework utilizes machine learning techniques to analyze crowd-sourced measurements uploaded from diverse end devices. Then, some well-trained prediction models are used to construct a global view of a radio environment, namely, radio environment map (REM), for diagnosis and management purposes. Moreover, REMs enable operators to detect existing coverage and capacity problems at any interested location. To verify the feasibility of indoor diagnoses, we collect a real trace data from an indoor testbed and then conduct a series of experiments to evaluate the performance of the machine-learning algorithms in terms of the accuracy, the time complexity, and the sensitivity to data volumes. The experimental results provide an insightful guideline to the indoor deployment of small cells in cellular-based IoT networks.",health
10.1007/s12652-021-03275-w,to_check,Journal of Ambient Intelligence and Humanized Computing,Springer,2021-05-04 00:00:00,springer,An effective arrhythmia classification via ECG signal subsampling and mutual information based subbands statistical features selection,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-03275-w,"The deployment of wireless health wearables is increasing in the framework of mobile health monitoring. The power and processing efficiencies with data compression are key aspects. To this end, an efficient automated arrhythmia recognition method is devised. The aim of this work is to contribute to the realisation of modern wireless electrocardiogram (ECG) gadgets. The proposed system utilizes an intelligent combination of subsampling, denoising and wavelet transform based subbands decomposition. Onward, the subband’s statistical features are extracted and mutual information (MI) based dimension reduction is attained for an effective realization of the ECG wearable processing chain. The amount of information to be processed is reduced in a real-time manner by using subsampling. It brings a remarkable reduction in the proposed system's computational complexity compared to the fixed-rate counterparts. MI based features selection improves the classification performance in terms of precision and processing delay. Moreover, it enhances the compression gain and aptitudes an effective diminishing in the transmission activity. Experimental results show that the designed method attains a 4 times computational gain while assuring an appropriate quality of signal reconstruction. A 7.2-fold compression gain compared to conventional counterparts is also attained. The best classification accuracies of 97% and 99% are secured respectively for cases of 5-class and 4-class arrhythmia datasets. It shows that the suggested method realizes an effective recognition of arrhythmia.",health
10.1007/978-3-030-58740-6_2,to_check,Introduction to Nursing Informatics,Springer,2021-01-01 00:00:00,springer,Connecting Health Immersion of Digital into eHealth,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58740-6_2,"Globally, in most countries the deployment and integration of national eHealth programmes is at an advanced stage. Digital progression in society is recognised as inevitable. Consequently, there is a growing awareness amongst nurse leaders for prioritisation of strategies relating to digital leadership and use of informatics competencies within the profession. As artificial intelligence, machine learning and robotics progress at an accelerated rate in society, thought leaders and policy makers turn their attention to consider how such technology can support healthy behaviors and health care delivery. Considerable efficiencies have been realised by deployment of digital in specific business domains such as fast food outlets, the hospitality and transport industries. Automation of passenger check in for example through air travel security has realised better use of staff time in processing of routine tasks and functions. The natural question is can such efficiencies be realised in the domain of healthcare? Large-scale deployment of digital within the health care domain lags somewhat behind other industries. Health administrations engaged with strategic business cases are deliberating on how the digital transformation can assist with planned service improvements from a macro, meso, and micro systems perspective. A key enabler in targeted service planning is the need to advance shared patient centric integrated care through interdisciplinary care service delivery. In this chapter we consider from a practical perspective the seismic impact of digital health, and how it will influence contemporary nursing practice.",health
10.1016/j.envsci.2021.06.011,to_check,Environmental Science and Policy,scopus,2021-10-01,sciencedirect,A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong,https://api.elsevier.com/content/abstract/scopus_id/85111282536,"All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.",health
10.1016/j.renene.2021.04.040,to_check,Renewable Energy,scopus,2021-08-01,sciencedirect,Damage identification of wind turbine blades with deep convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85105896900,"Online early detection of surface damages on blades is critical for the safety of wind turbines, which could avoid catastrophic failures, minimize downtime, and enhance the reliability of the system. Monitoring the health status of blades is attracting more and more attention including on-site cameras and mobile cameras by drones and crawling robots. To deploy fast and efficient damage detection methods from image data, this work presents a hierarchical identification framework for wind turbine blades, which consists of a Haar-AdaBoost step for region proposal and a convolutional neural network (CNN) classifier for damage detection and fault diagnosis. Case studies are carried out on real data set collected from an eastern China wind farm. Results show that (i) the proposed framework can detect and identify the blade damages and outperforms other schemes include SVM and VGG16 models, (ii) sensitive analysis is conducted to validate the robustness of proposed method under limited data conditions, (iii) the proposed scheme is faster than one-step CNN method that directly classifying raw data.",health
10.1016/j.apenergy.2020.116049,to_check,Applied Energy,scopus,2021-02-01,sciencedirect,Adaptive prognostics in a controlled energy conversion process based on long- and short-term predictors,https://api.elsevier.com/content/abstract/scopus_id/85097470918,"The pulp and paper industry is a fundamental sector of the economy of many countries. However, this sector requires real collaboration and initiatives from stakeholders to reduce its significant consumption of energy and emission of greenhouse gases. Heat exchangers are examples of equipment in pulp mills that are subjected to undesirable and complex phenomena such as evolution of fouling over time, which leads to inefficiency in terms of energy consumption and unplanned shutdowns, resulting in ineffective maintenance strategies and production costs. Therefore, there is a clear need to develop an accurate predictive maintenance tool that helps mill operators avoid such situations. It is necessary for that tool to effectively track the fouling evolution level and, based on it, deploy a reliable prognostics approach to estimate more accurately the time-to-clean of this equipment. This study presents a new hybrid prognostics approach for fouling prediction in heat exchangers. The proposed approach relies on the fusion of information of different prediction horizons to estimate the time-to-clean. Employing long short-term memory, it allows adaptation of long-term predictions by accurate short-term predictions using multiple non-linear auto-regressive exogenous models. This fusion not only captures the changes in degradation trend over time, but also ensures a good accuracy of prognostics results in both the short- and long-term horizons for planning maintenance actions. The effectiveness of the proposed approach was successfully proven on real industrial data collected from a pulp mill heat exchanger located in Canada.",health
10.1016/j.ohx.2020.e00131,to_check,HardwareX,scopus,2020-10-01,sciencedirect,Partially RepRapable automated open source bag valve mask-based ventilator,https://api.elsevier.com/content/abstract/scopus_id/85089470505,"This study describes the development of a simple and easy-to-build portable automated bag valve mask (BVM) compression system, which, during acute shortages and supply chain disruptions can serve as a temporary emergency ventilator. The resuscitation system is based on the Arduino controller with a real-time operating system installed on a largely RepRap 3-D printable parametric component-based structure. The cost of the materials for the system is under $170, which makes it affordable for replication by makers around the world. The device provides a controlled breathing mode with tidal volumes from 100 to 800 mL, breathing rates from 5 to 40 breaths/minute, and inspiratory-to-expiratory ratio from 1:1 to 1:4. The system is designed for reliability and scalability of measurement circuits through the use of the serial peripheral interface and has the ability to connect additional hardware due to the object-oriented algorithmic approach. Experimental results after testing on an artificial lung for peak inspiratory pressure (PIP), respiratory rate (RR), positive end-expiratory pressure (PEEP), tidal volume, proximal pressure, and lung pressure demonstrate repeatability and accuracy exceeding human capabilities in BVM-based manual ventilation. Future work is necessary to further develop and test the system to make it acceptable for deployment outside of emergencies such as with COVID-19 pandemic in clinical environments, however, the nature of the design is such that desired features are relatively easy to add using protocols and parametric design files provided.",health
10.1016/j.iot.2020.100185,to_check,Internet of Things (Netherlands),scopus,2020-09-01,sciencedirect,Highly-efficient fog-based deep learning AAL fall detection system,https://api.elsevier.com/content/abstract/scopus_id/85086362688,"Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement.",health
10.1109/MED.2017.7984310,to_check,2017 25th Mediterranean Conference on Control and Automation (MED),IEEE,2017-07-06 00:00:00,ieeexplore,Cloud computing for big data analytics in the Process Control Industry,https://ieeexplore.ieee.org/document/7984310/,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",industry
10.1109/BDCloud.2018.00136,to_check,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",IEEE,2018-12-13 00:00:00,ieeexplore,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",https://ieeexplore.ieee.org/document/8672266/,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",industry
10.1109/TCPMT.2020.3047089,to_check,"IEEE Transactions on Components, Packaging and Manufacturing Technology",IEEE,2021-02-01 00:00:00,ieeexplore,Automatic Industry PCB Board DIP Process Defect Detection System Based on Deep Ensemble Self-Adaption Method,https://ieeexplore.ieee.org/document/9306873/,"A deep ensemble convolutional neural network (CNN) model to inspect printed circuit board (PCB) board dual in-line package (DIP) soldering defects with Hybrid-YOLOv2 (YOLOv2 as a foreground detector and ResNet-101 as a classifier) and Faster RCNN with ResNet-101 and Feature Pyramid Network (FPN) (FRRF) achieved a detection rate of 97.45% and a false alarm rate (FAR) of 20%-30% in the previous study [34]. However, applying the method to other production lines, environmental variations, such as lighting, orientations of the sample feeds, and mechanical deviations, led to the degradation in detection performance. This article proposes an effective self-adaption method that collects “exception data” like the samples with which the Artificial Intelligent (AI) model made mistakes from the automated optical inspection inference edge to the training server, retraining with exceptions on the server and deploying back to the edge. The proposed defect detection system has been verified with real tests that achieved a detection rate of 99.99% with an FAR 20%-30% and less than 15 s of inspection time on a resolution $7296 \times 6000$ PCB image. The proposed system has proven capable of shortening inspection and repair time for online operators, where a 33% efficiency boost from the three production lines of the collaborated factory has been reported [6]. The contribution of the proposed retraining mechanism is threefold: 1) because the retraining process directly learns from the exceptions, the model can quickly adapt to the characteristic of each production line, leading to a fast and reliable mass deployment; 2) the proposed retraining mechanism is a necessary self-service for conventional users as it incrementally improves the detection performance without professional guidance or fine-tuning; and 3) the semiautomatic exception data collection method helps to reduce the time-consuming manual labeling during the retraining process.",industry
10.1109/CloudCom.2019.00037,to_check,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),IEEE,2019-12-13 00:00:00,ieeexplore,APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge Devices,https://ieeexplore.ieee.org/document/8968863/,"Recently Edge Computing paradigm has gained significant popularity both in industry and academia. With its increased usage in real-life scenarios, security, privacy and integrity of data in such environments have become critical. Malicious deletion of mission-critical data due to ransomware, trojans and viruses has been a huge menace and recovering such lost data is an active field of research. As most of Edge computing devices have compute and storage limitations, difficult constraints arise in providing an optimal scheme for data protection. These devices mostly use Linux/Unix based operating systems. Hence, this work focuses on extending the Ext4 file system to APEX (Adaptive Ext4): a file system based on novel on-the-fly learning model that provides an Adaptive Recover-ability Aware file allocation platform for efficient post-deletion data recovery and therefore maintaining data integrity. Our recovery model and its lightweight implementation allow significant improvement in recover-ability of lost data with lower compute, space, time, and cost overheads compared to other methods. We demonstrate the effectiveness of APEX through a case study of overwriting surveillance videos by CryPy malware on Raspberry-Pi based Edge deployment and show 678% and 32% higher recovery than Ext4 and current state-of-the-art File Systems. We also evaluate the overhead characteristics and experimentally show that they are lower than other related works.",industry
10.1109/ICCC.2018.00010,to_check,2018 IEEE International Conference on Cognitive Computing (ICCC),IEEE,2018-07-07 00:00:00,ieeexplore,An Edge Based Smart Parking Solution Using Camera Networks and Deep Learning,https://ieeexplore.ieee.org/document/8457691/,"The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systems-including license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.",industry
10.1109/ISSE46696.2019.8984462,to_check,2019 International Symposium on Systems Engineering (ISSE),IEEE,2019-10-03 00:00:00,ieeexplore,An IoT Reconfigurable SoC Platform for Computer Vision Applications,https://ieeexplore.ieee.org/document/8984462/,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",industry
10.1109/IGSC48788.2019.8957164,to_check,2019 Tenth International Green and Sustainable Computing Conference (IGSC),IEEE,2019-10-24 00:00:00,ieeexplore,IoT/CPS Ecosystem for Efficient Electricity Consumption : Invited Paper,https://ieeexplore.ieee.org/document/8957164/,"Modern society relies on smart systems like internet of things (IoT) and cyber physical systems (CPS) to monitor and control physical processes. The widespread deployment of IoT and CPSs result in fast growth of sensor data as physical processes are constantly monitored by billions of IP-enabled sensors (44 zettabytes by 2020). Hence, fog nodes are deployed to make network edge rich in computing resources to enable real-time data analytics using artificial intelligence/machine learning (AI/ML) for Big data generated from IoT and CPSs. This paper proposes IoT/CPS ecosystem for smart grid (SG) utilizing industry 4.0 concept to manage and control the loads using an intelligent predictive controller based on artificial neural network (ANN). The ANN is trained to predict the loads in certain districts based on previous smart meter readings installed at consumers and substations. This is a novel approach which integrates IoT/CPSs ecosystem into electric power system to deliver energy to consumers with high efficiency, reduce the cost, optimize the energy consumption, improve the reliability and enable real-time monitoring of power consumption.",industry
10.23919/DATE.2019.8714959,to_check,"2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)",IEEE,2019-03-29 00:00:00,ieeexplore,Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,https://ieeexplore.ieee.org/document/8714959/,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",industry
10.1109/RTSS.2018.00029,to_check,2018 IEEE Real-Time Systems Symposium (RTSS),IEEE,2018-12-14 00:00:00,ieeexplore,Work-in-Progress: Making Machine Learning Real-Time Predictable,https://ieeexplore.ieee.org/document/8603205/,"Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The new trend is to utilize embedded edge devices, as they boast higher computational power and larger memories than before, to perform ML tasks that had previously been limited to cloud-hosted deployments. In this work, we assess the real-time predictability and consider data privacy concerns by comparing traditional cloud services with edge-based ones for certain data analytics tasks. We identify the subset of ML problems appropriate for edge devices by investigating if they result in real-time predictable services for a set of widely used ML libraries. We specifically enhance the Caffe library to make it more suitable for real-time predictability. We then deploy ML models with high accuracy scores on an embedded system, exposing it to industry sensor data from the field, to demonstrates its efficacy and suitability for real-time processing.",industry
10.1109/TPWRS.2019.2922333,to_check,IEEE Transactions on Power Systems,IEEE,2019-11-01 00:00:00,ieeexplore,A Data-Driven Framework for Assessing Cold Load Pick-Up Demand in Service Restoration,https://ieeexplore.ieee.org/document/8735925/,"Cold load pick-up (CLPU) has been a critical concern to utilities. Researchers and industry practitioners have underlined the impact of CLPU on distribution system design and service restoration. The recent large-scale deployment of smart meters has provided the industry with a huge amount of data that are highly granular, both temporally and spatially. In this paper, a data-driven framework is proposed for assessing CLPU demand of residential customers using smart meter data. The proposed framework consists of two interconnected layers: 1) At the feeder level, a nonlinear autoregression model is applied to estimate the diversified demand during the system restoration and calculate the CLPU demand ratio. 2) At the customer level, Gaussian mixture models and probabilistic reasoning are used to quantify the CLPU demand increase. The proposed methodology has been verified using real smart meter data and outage cases.",industry
10.1109/ACCESS.2020.2974035,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Survey on Behavioral Pattern Mining From Sensor Data in Internet of Things,https://ieeexplore.ieee.org/document/8999541/,"The deployment of large-scale wireless sensor networks (WSNs) for the Internet of Things (IoT) applications is increasing day-by-day, especially with the emergence of smart city services. The sensor data streams generated from these applications are largely dynamic, heterogeneous, and often geographically distributed over large areas. For high-value use in business, industry and services, these data streams must be mined to extract insightful knowledge, such as about monitoring (e.g., discovering certain behaviors over a deployed area) or network diagnostics (e.g., predicting faulty sensor nodes). However, due to the inherent constraints of sensor networks and application requirements, traditional data mining techniques cannot be directly used to mine IoT data streams efficiently and accurately in real-time. In the last decade, a number of works have been reported in the literature proposing behavioral pattern mining algorithms for sensor networks. This paper presents the technical challenges that need to be considered for mining sensor data. It then provides a thorough review of the mining techniques proposed in the recent literature to mine behavioral patterns from sensor data in IoT, and their characteristics and differences are highlighted and compared. We also propose a behavioral pattern mining framework for IoT and discuss possible future research directions in this area.",industry
10.1109/ECAI52376.2021.9515185,to_check,"2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2021-07-03 00:00:00,ieeexplore,Automated ceramic plate defect detection using ScaledYOLOv4-large,https://ieeexplore.ieee.org/document/9515185/,"Automated visual inspection has become a popular topic of research in the last couple of decades, as computation power available grew exponentially. Judging by the fact that visual inspection is a critical task for the quality of the products, it would be highly recommended that people only supervise the system. This paper proposes a low cost, rapid development defect detection system based on the Scaled-YOLOv4 object detection model. The original model achieves almost state of the art detection mAP on the COCO dataset with a mAP(mean average precision) of 56.0 for the largest variant, named YOLOv4-P7. Our version is derived from the ScaledYOLOv4-P5 model and is trained on ceramic plate defects and achieves 87.4 mAP at a intersection of union of 0.5, while comfortably processing a frame in 20ms on a consumer RTX3070 GPU. Thus, the real time constraint for the manufacturing system is fulfilled. Hence, the critical aspects of the development process are the: quick development process, fast training, rapid deployment on the factory floor, quick validation and feedback, using images acquired in the lab - not on the factory floor for first iteration and overall low cost of the automated inspection system.",industry
10.1109/RO-MAN50785.2021.9515431,to_check,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),IEEE,2021-08-12 00:00:00,ieeexplore,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,https://ieeexplore.ieee.org/document/9515431/,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",industry
10.1109/ICMLA.2015.183,to_check,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),IEEE,2015-12-11 00:00:00,ieeexplore,Comparative Evaluation of Top-N Recommenders in e-Commerce: An Industrial Perspective,https://ieeexplore.ieee.org/document/7424455/,"We experiment on two real e-commerce datasets and survey more than 30 popular e-commerce platforms to reveal what methods work best for product recommendations in industrial settings. Despite recent academic advances in the field, we observe that simple methods such as best-seller lists dominate deployed recommendation engines in e-commerce. We find our empirical findings to be well-aligned with those of the survey, where in both cases simple personalized recommenders achieve higher ranking than more advanced techniques. We also compare the traditional random evaluation protocol to our proposed chronological sampling method, which can be used for determining the optimal time-span of the training history for optimizing the performance of algorithms. This performance is also affected by a proper hyperparameter tuning, for which we propose golden section search as a fast alternative to other optimization techniques.",industry
10.1109/AIKE.2018.00042,to_check,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),IEEE,2018-09-28 00:00:00,ieeexplore,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,https://ieeexplore.ieee.org/document/8527474/,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",industry
10.1109/ICAICA52286.2021.9497973,to_check,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),IEEE,2021-06-30 00:00:00,ieeexplore,Industrial Internet Security Protection Based on an Industrial Firewall,https://ieeexplore.ieee.org/document/9497973/,"A crucial step in the development of a security system for the industrial Internet is the implementation of an industrial firewall as the first line of defense for the multi-layer defense-in-depth system and an important safeguard for industrial network security. In the design, development, deployment, application, and maintenance of industrial firewalls, the firewall performance and architecture are vital aspects. This thesis focuses on the analysis and discussion of the requirements and abilities of an industrial firewall in terms of adaptability, network isolation, industrial communication protocol identification, filtering and analysis, real-time performance and reliability, and self-protection.",industry
10.1109/SoCPaR.2011.6089156,to_check,2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR),IEEE,2011-10-16 00:00:00,ieeexplore,QoS-oriented Service Management in clouds for large scale industrial activity recognition,https://ieeexplore.ieee.org/document/6089156/,"Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments.",industry
10.1109/RTSI50628.2021.9597339,to_check,2021 IEEE 6th International Forum on Research and Technology for Society and Industry (RTSI),IEEE,2021-09-09 00:00:00,ieeexplore,Towards Graph Machine Learning for Smart Grid Knowledge Graphs in Industrial Scenarios,https://ieeexplore.ieee.org/document/9597339/,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG.",industry
10.1109/AIKE.2018.00037,to_check,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),IEEE,2018-09-28 00:00:00,ieeexplore,"Towards Low-Cost, Real-Time, Distributed Signal and Data Processing for Artificial Intelligence Applications at Edges of Large Industrial and Internet Networks",https://ieeexplore.ieee.org/document/8527469/,"Digital Signal Processors (DSP) are vital system components in industrial Artificial Intelligence (AI) applications. In this paper, FIR filters that could be used for industrial AI applications are designed from the Spline Biorthogonal 1.5 (SB1.5) mother wavelet using a real-time, low-cost, generic industrial IoT (IIoT) hardware: the C28x DSP and low-level, Embedded C, system software. Our contribution in this paper is the first reported application of the C28x for SB1.5 wavelet construction. The significance of this approach is to be able to repurpose low-cost, readily available hardware for distributed AI applications. Our approach is different from the state of the art, in which specialized hardware are always manufactured for implementing AI applications at large network edges. Our approach supports low-cost and fast single-stage FIR implementation suitable for use in real-time, distributed AI application at network edges, since in our case, successive recursion of FIR filters leading to a full implementation of Pyramid Algorithm is not implemented. The designed FIR filter is evaluated and found capable of both low-pass and high pass filtering operations. Results of this paper indicate that the C28x real-time DSP, which exists in many IoT devices, could have improved scalability by being deployed for other important AI and IoT network edge analytic applications, different from its present uses.",industry
10.1109/FDL53530.2021.9568376,to_check,2021 Forum on specification & Design Languages (FDL),IEEE,2021-09-10 00:00:00,ieeexplore,A Container-based Design Methodology for Robotic Applications on Kubernetes Edge-Cloud architectures,https://ieeexplore.ieee.org/document/9568376/,"Programming modern Robots' missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub.",industry
10.1109/AIAM48774.2019.00157,to_check,2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM),IEEE,2019-10-18 00:00:00,ieeexplore,A Digital Twin-Based Approach for Quality Control and Optimization of Complex Product Assembly,https://ieeexplore.ieee.org/document/8950866/,"To address the problems caused by low ability of quality analysis and decision-making in the process of complex product assembly, in this paper, we proposed a digital twin-based approach for quality control and optimization of complex product assembly, by providing a digital twin system to realize the timely and precisely interactive mapping between the physical world and digital world. Specifically, a quality control and optimization mechanism is presented, which provides the theoretical support to the realization of the digital twin-based approach. A data-driven quality control model is introduced to solve the optimization problem by considering the panoramic assembly quality data. A digital twin system for complex product assembly is elaborated by providing detailed deployment and implementation procedures, which includes (1) building of the digital entity of an assembly line, (2) real-time online sensing in multi-source heterogeneous environment, (3) real-time simulation of equipment and assembly process, (4) realization of the intelligent production scheduling under uncertainty conditions, and (5) dynamical adjustment of the assembly process. Finally, the paper presents the validation results considering the practical applications of the proposed approach in real industrial fields.",industry
10.1109/ICRAE50850.2020.9310899,to_check,2020 5th International Conference on Robotics and Automation Engineering (ICRAE),IEEE,2020-11-22 00:00:00,ieeexplore,A ROS Based Automatic Control Implementation for Precision Landing on Slow Moving Platforms Using a Cooperative Fleet of Rotary-Wing UAVs,https://ieeexplore.ieee.org/document/9310899/,"In this paper we present an industrial implementation of an efficient method to solve the problem of the automatic precision landing for rotary-wing UAVs, ready to be used inside a cooperative fleet of drones. The realized software module and tests are part of a large industrial R&amp;D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&amp;C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. Preparatory to the presented results, it was the identification of a non-linear mathematical model as well as the realization of a robust PID-based control system capable of controlling a single drone of the fleet. A discrete-time Kalman filter was integrated and tested to estimate the possible displacement of the landing points, in order to improve the control law through predictive connotations in case of slow moving tags. The presented approach is featured by the balance between computational efficiency and versatility, in particular in the discovering stage of multiple and different AprilTag during the landing phase. The still under test software module uses the Open Source Robotic Operating System (ROS) libraries for both the acquisition of the data necessary to the control laws, and for the execution of the computer vision algorithms implemented for the precision landing. After analyses and simulations campaigns in a synthetic environment and multiple hardware in the loop (HIL) stress tests, the final prototype algorithm was deployed on a commercial-off-the-shelf mini-class UAV, demonstrating landing capacity on a fixed target with an error of less than ten centimeters; moreover, with slow-moving tags, appreciable tracking abilities emerged on sufficiently smooth trajectories. A special interface with the HIL flight controller was then integrated, with the capability of using its telemetry data for distributing them to all the members of the cooperative fleet, making it possible to access the real-time estimate of the states of each single drone, and making each one of them aware of the selected landing areas of the others, by navigation sensors data fusion with a five meters GPS precision.",industry
10.1109/IECON.2013.6699377,to_check,IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society,IEEE,2013-11-13 00:00:00,ieeexplore,Fuel Cells prognostics using echo state network,https://ieeexplore.ieee.org/document/6699377/,"One remaining technological bottleneck to develop industrial Fuel Cell (FC) applications resides in the system limited useful lifetime. Consequently, it is important to develop failure diagnostic and prognostic tools enabling the optimization of the FC. Among all the existing prognostics approaches, datamining methods such as artificial neural networks aim at estimating the process' behavior without huge knowledge about the underlying physical phenomena. Nevertheless, this kind of approach needs huge learning dataset. Also, the deployment of such an approach can be long (trial and error method), which represents a real problem for industrial applications where real-time complying algorithms must be developed. According to this, the aim of this paper is to study the application of a reservoir computing tool (the Echo State Network) as a prognostics system enabling the estimation of the Remaining Useful Life of a Proton Exchange Membrane Fuel Cell. Developments emphasize on the prediction of the mean voltage cells of a degrading FC. Accuracy and time consumption of the approach are studied, as well as sensitivity of several parameters of the ESN. Results appear to be very promising.",industry
10.1109/ICCCE50029.2021.9467162,to_check,2021 8th International Conference on Computer and Communication Engineering (ICCCE),IEEE,2021-06-23 00:00:00,ieeexplore,ICS Cyber Attack Detection with Ensemble Machine Learning and DPI using Cyber-kit Datasets,https://ieeexplore.ieee.org/document/9467162/,"Digitization has pioneered to drive exceptional changes across all industries in the advancement of analytics, automation, and Artificial Intelligence (AI) and Machine Learning (ML). However, new business requirements associated with the efficiency benefits of digitalization are forcing increased connectivity between IT and OT networks, thereby increasing the attack surface and hence the cyber risk. Cyber threats are on the rise and securing industrial networks are challenging with the shortage of human resource in OT field, with more inclination to IT/OT convergence and the attackers deploy various hi-tech methods to intrude the control systems nowadays. We have developed an innovative real-time ICS cyber test kit to obtain the OT industrial network traffic data with various industrial attack vectors. In this paper, we have introduced the industrial datasets generated from ICS test kit, which incorporate the cyber-physical system of industrial operations. These datasets with a normal baseline along with different industrial hacking scenarios are analyzed for research purposes. Metadata is obtained from Deep packet inspection (DPI) of flow properties of network packets. DPI analysis provides more visibility into the contents of OT traffic based on communication protocols. The advancement in technology has led to the utilization of machine learning/artificial intelligence capability in IDS ICS SCADA. The industrial datasets are pre-processed, profiled and the abnormality is analyzed with DPI. The processed metadata is normalized for the easiness of algorithm analysis and modelled with machine learning-based latest deep learning ensemble LSTM algorithms for anomaly detection. The deep learning approach has been used nowadays for enhanced OT IDS performances.",industry
10.1109/AVSS.2018.8639339,to_check,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),IEEE,2018-11-30 00:00:00,ieeexplore,Scene Adaptation for Semantic Segmentation using Adversarial Learning,https://ieeexplore.ieee.org/document/8639339/,"Semantic Segmentation algorithms based on the deep learning paradigm have reached outstanding performances. However, in order to achieve good results in a new domain, it is generally demanded to fine-tune a pre-trained deep architecture using new labeled data coming from the target application domain. The fine-tuning procedure is also required when the domain application settings change, e. g., when a camera is moved, or a new camera is installed. This implies the collection and pixel-wise la-beling of images to be used for training, which slows down the deployment of semantic segmentation systems in real industrial scenarios and increases the industrial costs. Taking into account the aforementioned issues, in this paper we propose an approach based on Adversarial Learning to perform scene adaptation for semantic segmentation. We frame scene adaptation as the task of predicting semantic segmentation masks for images belonging to a Target Scene Context given labeled images coming from a Source Scene Context and unlabeled images coming from the Target Scene Context. Experiments highlight that the proposed method achieves promising performances both when the two scenes contain similar content (i.e., they are related to two different points of view of the same scene) and when the observed scenes contain unrelated content (i.e., they account to completely different scenes).",industry
10.1109/ICCPEIC.2017.8290335,to_check,"2017 International Conference on Computation of Power, Energy Information and Commuincation (ICCPEIC)",IEEE,2017-03-23 00:00:00,ieeexplore,Smart personalized learning system for energy management in buildings,https://ieeexplore.ieee.org/document/8290335/,"Integration of energy management systems into existing buildings brings in several challenges and financial constraints. Some of the challenges in the existing smart building solutions are that they require large-scale deployment of sensors, high rate of data collection, real-time data analysis in short span of time, and lack of knowledge about the energy usage with respect to the behavior of individuals and groups. This work proposes an affordable wearable device system as an alternative for large-scale deployment of sensors in industrial buildings. For effective energy management in the buildings, a personalized behavior analysis has been done in machine learning and neural networks algorithm and integrated with the proposed system. The complete system is implemented and tested extensively. The results show that the proposed system could provide 85% user comfort and 23% energy savings.",industry
10.1109/TASE.2020.3032075,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2021-10-01 00:00:00,ieeexplore,A Virtual Mechanism Approach for Exploiting Functional Redundancy in Finishing Operations,https://ieeexplore.ieee.org/document/9246671/,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <italic>Note to Practitioners</italic>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",industry
10.1109/LRA.2020.2969927,to_check,IEEE Robotics and Automation Letters,IEEE,2020-04-01 00:00:00,ieeexplore,Augmented LiDAR Simulator for Autonomous Driving,https://ieeexplore.ieee.org/document/8972449/,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this letter, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., vehicles, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG (Computer Graphics) models and game engines, our augmented simulator bypasses the requirement to create high-fidelity background CAD (Computer Aided Design) models. Instead, we can deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background points cloud, based on which annotated point cloud can be automatically generated. This “scan-and-simulate” capability makes our approach scalable and practical, ready for large-scale industrial applications. In this letter, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95% accuracy.",industry
10.1109/JSTARS.2015.2442584,to_check,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,IEEE,2016-05-01 00:00:00,ieeexplore,Estimation of Seismic Vulnerability Levels of Urban Structures With Multisensor Remote Sensing,https://ieeexplore.ieee.org/document/7150321/,"The ongoing global transformation of human habitats from rural villages to ever growing urban agglomerations induces unprecedented seismic risks in earthquake prone regions. To mitigate affiliated perils requires the seismic assessment of built environments. Numerous studies emphasize that remote sensing can play a valuable role in supporting the extraction of relevant features for preevent vulnerability analysis. However, the majority of approaches operate on building level. This induces the deployment of very high spatial resolution remote sensing data, which hampers, nowadays, utilization capabilities for larger areas due to data costs and processing requirements. In this paper, we alter the spatial scale of analysis and propose concepts and methods to estimate the seismic vulnerability level of homogeneous urban structures. A procedure is designed, which comprises four main steps dedicated to: 1) delineation of urban structures by means of a tailored unsupervised data segmentation procedure with scale optimization; 2) characterization of urban structures by a joint exploitation of multisensor data; 3) selection of most feasible features under consideration of in situ vulnerability information; and 4) estimation of seismic vulnerability levels of urban structures within a supervised learning framework. We render the prediction problem in three ways to address operational requirements that can evolve in real-life situations. 1) To discriminate two or more classes based on labeled samples of all classes present in the data under investigation, we use the framework of soft margin support vector machines (C-SVM). 2) To consider situations, where solely labeled samples are available for the class(es) of interest and not for all classes present in the data, we deploy ensembles of ν-one-class SVM (ν-OC-SVM). and 3) To fit data with a higher statistical level of measurement (interval or ratio scale), we utilize a support vector regression (SVR) approach to estimate a regression function from the training samples. Experimental results are obtained for the earthquake-prone mega city Istanbul, Turkey. We use multispectral data from the RapidEye constellation, elevation measurements from the TanDEM-X mission, and spatiotemporal analyses based on data from the Landsat archive to characterize the urban environment. In addition, different in situ data sets are incorporated for Istanbul's district Zeytinburnu and the residual settlement area of Istanbul. When estimating damage grades for Zeytinburnu with SVR, best models are characterized by mean absolute percentage errors less than 11%, and fairly strong goodness of fit (R &gt; 0.75). When aiming to identify different types of urban structures for the remaining settlement area of Istanbul (i.e., urban structures determined by large industrial/commercial buildings and tall detached residential buildings, which can be considered here as highly and slightly vulnerable, respectively), results obtained with C-SVM show a distinctive increase of accuracy compared to results obtained with ensembles of ν-OC-SVM. The latter were not able to exceed moderate agreements, with κ statistics slightly above 0.45. Instead, C-SVM allowed obtaining ν statistics expressing substantial and even excellent agreements (κ &gt; 0.6 up to κ &gt; 0.8). Overall, analyzes provide very promising empirical evidence, which confirms the potential of remote sensing to support seismic vulnerability assessment.",industry
10.1109/ACCESS.2019.2916938,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,NASM: Nonlinearly Attentive Similarity Model for Recommendation System via Locally Attentive Embedding,https://ieeexplore.ieee.org/document/8715403/,"Recommendation system, as a core service for many customer-oriented online services, is employed to predict the personalized rating of users on their potentially preferable items. In modern industrial settings, an item-based collaborative filtering (item-based CF) method has been long popular owing to its excellent interpretability and high efficiency in the real-time personalized recommendation. In this model, the current target item is recommended according to the interacted similarity from the user's profile, which implies that the key of item-based CF is in the estimation of historical item similarity. Early studies usually utilize statistical measures including cosine similarity and Pearson correlation coefficient to estimate similarity with low accuracy caused by lacking optimization tailed. Recently, there are some learning-based models attempting to learn item similarity by optimizing a recommendation-aware loss function. However, these efforts are mainly concentrated on the application of the shallow linear model, and relative works that deploy some deep learning components for item-based CF are scarce. In this paper, we propose a nonlinearly attentive similarity model (NASM) for item-based CF via locally attentive embedding by introducing local attention and novel nonlinear attention to capture local and global item information, simultaneously. The NASM is based on a neural attentive item similarity (NAIS) model and further achieves significantly superior performance. The experimental results demonstrate that the NASM achieves more competitive recommendation performance in terms of hit ration (HR) and the normalized discounted cumulative gain (NDGC) in comparison with other state-of-the-art recommendation models.",industry
http://arxiv.org/abs/2107.13252v1,to_check,arxiv,arxiv,2021-07-28 10:28:05+00:00,arxiv,"Multi Agent System for Machine Learning Under Uncertainty in Cyber
  Physical Manufacturing System",http://arxiv.org/abs/2107.13252v1,"Recent advancements in predictive machine learning has led to its application
in various use cases in manufacturing. Most research focused on maximising
predictive accuracy without addressing the uncertainty associated with it.
While accuracy is important, focusing primarily on it poses an overfitting
danger, exposing manufacturers to risk, ultimately hindering the adoption of
these techniques. In this paper, we determine the sources of uncertainty in
machine learning and establish the success criteria of a machine learning
system to function well under uncertainty in a cyber-physical manufacturing
system (CPMS) scenario. Then, we propose a multi-agent system architecture
which leverages probabilistic machine learning as a means of achieving such
criteria. We propose possible scenarios for which our proposed architecture is
useful and discuss future work. Experimentally, we implement Bayesian Neural
Networks for multi-tasks classification on a public dataset for the real-time
condition monitoring of a hydraulic system and demonstrate the usefulness of
the system by evaluating the probability of a prediction being accurate given
its uncertainty. We deploy these models using our proposed agent-based
framework and integrate web visualisation to demonstrate its real-time
feasibility.",industry
http://arxiv.org/abs/1811.07315v1,to_check,arxiv,arxiv,2018-11-18 11:28:24+00:00,arxiv,"Learning to infer: RL-based search for DNN primitive selection on
  Heterogeneous Embedded Systems",http://arxiv.org/abs/1811.07315v1,"Deep Learning is increasingly being adopted by industry for computer vision
applications running on embedded devices. While Convolutional Neural Networks'
accuracy has achieved a mature and remarkable state, inference latency and
throughput are a major concern especially when targeting low-cost and low-power
embedded platforms. CNNs' inference latency may become a bottleneck for Deep
Learning adoption by industry, as it is a crucial specification for many
real-time processes. Furthermore, deployment of CNNs across heterogeneous
platforms presents major compatibility issues due to vendor-specific technology
and acceleration libraries. In this work, we present QS-DNN, a fully automatic
search based on Reinforcement Learning which, combined with an inference engine
optimizer, efficiently explores through the design space and empirically finds
the optimal combinations of libraries and primitives to speed up the inference
of CNNs on heterogeneous embedded devices. We show that, an optimized
combination can achieve 45x speedup in inference latency on CPU compared to a
dependency-free baseline and 2x on average on GPGPU compared to the best vendor
library. Further, we demonstrate that, the quality of results and time
""to-solution"" is much better than with Random Search and achieves up to 15x
better results for a short-time search.",industry
http://arxiv.org/abs/2107.00401v1,to_check,arxiv,arxiv,2021-07-01 12:20:48+00:00,arxiv,"CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous
  Cars on the Loihi Neuromorphic Research Processor",http://arxiv.org/abs/2107.00401v1,"Autonomous Driving (AD) related features provide new forms of mobility that
are also beneficial for other kind of intelligent and autonomous systems like
robots, smart transportation, and smart industries. For these applications, the
decisions need to be made fast and in real-time. Moreover, in the quest for
electric mobility, this task must follow low power policy, without affecting
much the autonomy of the mean of transport or the robot. These two challenges
can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed
on a specialized neuromorphic hardware, SNNs can achieve high performance with
low latency and low power consumption. In this paper, we use an SNN connected
to an event-based camera for facing one of the key problems for AD, i.e., the
classification between cars and other objects. To consume less power than
traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The
experiments are made following an offline supervised learning rule, followed by
mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our
best experiment achieves an accuracy on offline implementation of 86%, that
drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware
implementation has maximum 0.72 ms of latency for every sample, and consumes
only 310 mW. To the best of our knowledge, this work is the first
implementation of an event-based car classifier on a Neuromorphic Chip.",industry
http://arxiv.org/abs/2004.05898v1,to_check,arxiv,arxiv,2020-04-10 14:26:00+00:00,arxiv,Exposing Hardware Building Blocks to Machine Learning Frameworks,http://arxiv.org/abs/2004.05898v1,"There are a plethora of applications that demand high throughput and low
latency algorithms leveraging machine learning methods. This need for real time
processing can be seen in industries ranging from developing neural network
based pre-distortors for enhanced mobile broadband to designing FPGA-based
triggers in major scientific efforts by CERN for particle physics. In this
thesis, we explore how niche domains can benefit vastly if we look at neurons
as a unique boolean function of the form $f:B^{I} \rightarrow B^{O}$, where $B
= \{0,1\}$. We focus on how to design topologies that complement such a view of
neurons, how to automate such a strategy of neural network design, and
inference of such networks on Xilinx FPGAs. Major hardware borne constraints
arise when designing topologies that view neurons as unique boolean functions.
Fundamentally, realizing such topologies on hardware asserts a strict limit on
the 'fan-in' bits of a neuron due to the doubling of permutations possible with
every increment in input bit-length. We address this limit by exploring
different methods of implementing sparsity and explore activation quantization.
Further, we develop a library that supports training a neural network with
custom sparsity and quantization. This library also supports conversion of
trained Sparse Quantized networks from PyTorch to VERILOG code which is then
synthesized using Vivado, all of which is part of the LogicNet tool-flow. To
aid faster prototyping, we also support calculation of the worst-case hardware
cost of any given topology. We hope that our insights into the behavior of
extremely sparse quantized neural networks are of use to the research community
and by extension allow people to use the LogicNet design flow to deploy highly
efficient neural networks.",industry
http://arxiv.org/abs/2009.09926v1,to_check,arxiv,arxiv,2020-09-17 02:36:52+00:00,arxiv,"Cross-Modal Alignment with Mixture Experts Neural Network for
  Intral-City Retail Recommendation",http://arxiv.org/abs/2009.09926v1,"In this paper, we introduce Cross-modal Alignment with mixture experts Neural
Network (CameNN) recommendation model for intral-city retail industry, which
aims to provide fresh foods and groceries retailing within 5 hours delivery
service arising for the outbreak of Coronavirus disease (COVID-19) pandemic
around the world. We propose CameNN, which is a multi-task model with three
tasks including Image to Text Alignment (ITA) task, Text to Image Alignment
(TIA) task and CVR prediction task. We use pre-trained BERT to generate the
text embedding and pre-trained InceptionV4 to generate image patch embedding
(each image is split into small patches with the same pixels and treat each
patch as an image token). Softmax gating networks follow to learn the weight of
each transformer expert output and choose only a subset of experts conditioned
on the input. Then transformer encoder is applied as the share-bottom layer to
learn all input features' shared interaction. Next, mixture of transformer
experts (MoE) layer is implemented to model different aspects of tasks. At top
of the MoE layer, we deploy a transformer layer for each task as task tower to
learn task-specific information. On the real word intra-city dataset,
experiments demonstrate CameNN outperform baselines and achieve significant
improvements on the image and text representation. In practice, we applied
CameNN on CVR prediction in our intra-city recommender system which is one of
the leading intra-city platforms operated in China.",industry
http://arxiv.org/abs/1903.09477v4,to_check,arxiv,arxiv,2019-03-22 12:46:34+00:00,arxiv,"Facilitating Rapid Prototyping in the OODIDA Data Analytics Platform via
  Active-Code Replacement",http://arxiv.org/abs/1903.09477v4,"OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for
distributed real-time analytics, targeting fleets of reference vehicles in the
automotive industry. Its users are data analysts. The bulk of the data
analytics tasks are performed by clients (on-board), while a central cloud
server performs supplementary tasks (off-board). OODIDA can be automatically
packaged and deployed, which necessitates restarting parts of the system, or
all of it. As this is potentially disruptive, we added the ability to execute
user-defined Python modules on clients as well as the server. These modules can
be replaced without restarting any part of the system; they can even be
replaced between iterations of an ongoing assignment. This feature is referred
to as active-code replacement. It facilitates use cases such as iterative A/B
testing of machine learning algorithms or modifying experimental algorithms
on-the-fly. Consistency of results is achieved by majority vote, which prevents
tainted state. Active-code replacement can be done in less than a second in an
idealized setting whereas a standard deployment takes many orders of magnitude
more time. The main contribution of this paper is the description of a
relatively straightforward approach to active-code replacement that is very
user-friendly. It enables a data analyst to quickly execute custom code on the
cloud server as well as on client devices. Sensible safeguards and design
decisions ensure that this feature can be used by non-specialists who are not
familiar with the implementation of OODIDA in general or this feature in
particular. As a consequence of adding the active-code replacement feature,
OODIDA is now very well-suited for rapid prototyping.",industry
http://arxiv.org/abs/1904.09035v2,to_check,arxiv,arxiv,2019-03-21 02:55:14+00:00,arxiv,"Evolving Deep Neural Networks by Multi-objective Particle Swarm
  Optimization for Image Classification",http://arxiv.org/abs/1904.09035v2,"In recent years, convolutional neural networks (CNNs) have become deeper in
order to achieve better classification accuracy in image classification.
However, it is difficult to deploy the state-of-the-art deep CNNs for
industrial use due to the difficulty of manually fine-tuning the
hyperparameters and the trade-off between classification accuracy and
computational cost. This paper proposes a novel multi-objective optimization
method for evolving state-of-the-art deep CNNs in real-life applications, which
automatically evolves the non-dominant solutions at the Pareto front. Three
major contributions are made: Firstly, a new encoding strategy is designed to
encode one of the best state-of-the-art CNNs; With the classification accuracy
and the number of floating point operations as the two objectives, a
multi-objective particle swarm optimization method is developed to evolve the
non-dominant solutions; Last but not least, a new infrastructure is designed to
boost the experiments by concurrently running the experiments on multiple GPUs
across multiple machines, and a Python library is developed and released to
manage the infrastructure. The experimental results demonstrate that the
non-dominant solutions found by the proposed algorithm form a clear Pareto
front, and the proposed infrastructure is able to almost linearly reduce the
running time.",industry
http://arxiv.org/abs/1811.07112v2,to_check,arxiv,arxiv,2018-11-17 07:09:13+00:00,arxiv,Augmented LiDAR Simulator for Autonomous Driving,http://arxiv.org/abs/1811.07112v2,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads
is a critical task. Deep-learning based methods using annotated LiDAR data have
been the most widely adopted approach for this. Unfortunately, annotating 3D
point cloud is a very challenging, time- and money-consuming task. In this
paper, we propose a novel LiDAR simulator that augments real point cloud with
synthetic obstacles (e.g., cars, pedestrians, and other movable objects).
Unlike previous simulators that entirely rely on CG models and game engines,
our augmented simulator bypasses the requirement to create high-fidelity
background CAD models. Instead, we can simply deploy a vehicle with a LiDAR
scanner to sweep the street of interests to obtain the background point cloud,
based on which annotated point cloud can be automatically generated. This
unique ""scan-and-simulate"" capability makes our approach scalable and
practical, ready for large-scale industrial applications. In this paper, we
describe our simulator in detail, in particular the placement of obstacles that
is critical for performance enhancement. We show that detectors with our
simulated LiDAR point cloud alone can perform comparably (within two percentage
points) with these trained with real data. Mixing real and simulated data can
achieve over 95% accuracy.",industry
10.1016/j.jmsy.2021.04.005,to_check,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,LearningADD: Machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",industry
10.1016/j.adhoc.2019.102047,to_check,Ad Hoc Networks,scopus,2020-03-01,sciencedirect,An intelligent Edge-IoT platform for monitoring livestock and crops in a dairy farming scenario,https://api.elsevier.com/content/abstract/scopus_id/85076174369,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud.",industry
10.1016/j.psep.2019.05.016,to_check,Process Safety and Environmental Protection,scopus,2019-07-01,sciencedirect,An intelligent fire detection approach through cameras based on computer vision methods,https://api.elsevier.com/content/abstract/scopus_id/85065893982,"Fire that is one of the most serious accidents in petroleum and chemical factories, may lead to considerable production losses, equipment damages and casualties. Traditional fire detection was done by operators through video cameras in petroleum and chemical facilities. However, it is an unrealistic job for the operator in a large chemical facility to find out the fire in time because there may be hundreds of video cameras installed and the operator may have multiple tasks during his/her shift. With the rapid development of computer vision, intelligent fire detection has received extensive attention from academia and industry. In this paper, we present a novel intelligent fire detection approach through video cameras for preventing fire hazards from going out of control in chemical factories and other high-fire-risk industries. The approach includes three steps: motion detection, fire detection and region classification. At first, moving objects are detected through cameras by a background subtraction method. Then the frame with moving objects is determined by a fire detection model which can output fire regions and their locations. Since false fire regions (some objects similar with fire) may be generated, a region classification model is used to identify whether it is a fire region or not. Once fire appears in any camera, the approach can detect it and output the coordinates of the fire region. Simultaneously, instant messages will be immediately sent to safety supervisors as a fire alarm. The approach can meet the needs of real-time fire detection on the precision and the speed. Its industrial deployment will help detect fire at the very early stage, facilitate the emergency management and therefore significantly contribute to loss prevention.",industry
10.1016/j.jpowsour.2016.05.092,to_check,Journal of Power Sources,scopus,2016-08-30,sciencedirect,Prognostics of Proton Exchange Membrane Fuel Cells stack using an ensemble of constraints based connectionist networks,https://api.elsevier.com/content/abstract/scopus_id/84975104897,"Proton Exchange Membrane Fuel Cell (PEMFC) is considered the most versatile among available fuel cell technologies, which qualify for diverse applications. However, the large-scale industrial deployment of PEMFCs is limited due to their short life span and high exploitation costs. Therefore, ensuring fuel cell service for a long duration is of vital importance, which has led to Prognostics and Health Management of fuel cells. More precisely, prognostics of PEMFC is major area of focus nowadays, which aims at identifying degradation of PEMFC stack at early stages and estimating its Remaining Useful Life (RUL) for life cycle management. This paper presents a data-driven approach for prognostics of PEMFC stack using an ensemble of constraint based Summation Wavelet- Extreme Learning Machine (SW-ELM) models. This development aim at improving the robustness and applicability of prognostics of PEMFC for an online application, with limited learning data. The proposed approach is applied to real data from two different PEMFC stacks and compared with ensembles of well known connectionist algorithms. The results comparison on long-term prognostics of both PEMFC stacks validates our proposition.",industry
10.1016/j.mejo.2013.12.006,to_check,Microelectronics Journal,scopus,2014-03-01,sciencedirect,Enhancing confidence in indirect analog/RF testing against the lack of correlation between regular parameters and indirect measurements,https://api.elsevier.com/content/abstract/scopus_id/84897670549,"The greedy specification testing remains mandatory for analog and radio frequency (RF) integrated circuits because of the accuracy of the sorting based on these measurements. Unfortunately, to be implemented, this kind of testing method often incurs very high costs (expensive instruments, long test time…). A common approach, in the literature, is the so-called indirect/alternate test strategy. This strategy consists in deriving targeted specifications from low-cost Indirect Measurements (IMs). During the industrial test phase, the estimation of regular specifications using IMs is based on a correlation model that has been built previously, during a training phase. Despite the substantial test cost reduction offered by this strategy, its deployment in industry is limited, mainly because of a lack of confidence in the accuracy of estimations made by the correlation model. A solution to increase the confidence in the estimation of specifications using the indirect approach is to implement redundancy in the prediction phase. In this paper, we demonstrate that the redundancy implementation brings more than identifying rare misjudged circuits from a high-correlated model. Indeed redundancy massively increases the accuracy despite of the lack of accurate models that have been assumed in previous implementations of redundant indirect testing. This approach is illustrated on a real case study for which we have experimental measurements on a set of 10,000 devices.",industry
10.1016/j.advengsoft.2013.09.003,to_check,Advances in Engineering Software,scopus,2014-01-01,sciencedirect,Software architecture knowledge for intelligent light maintenance,https://api.elsevier.com/content/abstract/scopus_id/84885359031,"The maintenance management plays an important role in the monitoring of business activities. It ensures a certain level of services in industrial systems by improving the ability to function in accordance with prescribed procedures. This has a decisive impact on the performance of these systems in terms of operational efficiency, reliability and associated intervention costs. To support the maintenance processes of a wide range of industrial services, a knowledge-based component is useful to perform the intelligent monitoring. In this context we propose a generic model for supporting and generating industrial lights maintenance processes. The modeled intelligent approach involves information structuring and knowledge sharing in the industrial setting and the implementation of specialized maintenance management software in the target information system. As a first step we defined computerized procedures from the conceptual structure of industrial data to ensure their interoperability and effective use of information and communication technologies in the software dedicated to the management of maintenance (E-candela). The second step is the implementation of this software architecture with specification of business rules, especially by organizing taxonomical information of the lighting systems, and applying intelligence-based operations and analysis to capitalize knowledge from maintenance experiences. Finally, the third step is the deployment of the software with contextual adaptation of the user interface to allow the management of operations, editions of the balance sheets and real-time location obtained through geolocation data. In practice, these computational intelligence-based modes of reasoning involve an engineering framework that facilitates the continuous improvement of a comprehensive maintenance regime.",industry
10.1109/GLOBECOM42002.2020.9348202,to_check,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,IEEE,2020-12-11 00:00:00,ieeexplore,Image Download and Rate Allocation of Internet-of-Things Analytics at Gateways in Smart Cities,https://ieeexplore.ieee.org/document/9348202/,"Internet-of-Things (IoT) devices are connected to the Internet through a gateway, which can host IoT analytics encapsulated in containers to convert raw sensor data into more condensed processed data. In this paper, we study two research problems to maximize the overall Quality-of-Service (QoS) level of all IoT analytics that run on both data center servers and gateways. The first problem is to select additional IoT analytics to deploy on a gateway to save upload bandwidth due to transmitting raw sensor data. The second problem is to allocate the residue upload bandwidth among all IoT analytics to maximize the overall QoS level. We propose several algorithms to solve these two research problems. We have implemented real testbeds to evaluate our proposed system and algorithms. Our experiment results reveal that the proposed algorithms: (i) capitalize the download bandwidth and storage space of the gateway for saving the upload bandwidth consumption and (ii) achieve high QoS levels without overloading the network and gateway.",smart cities
10.1109/ACCESS.2019.2943013,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Intelligent Data Delivery Approach for Smart Cities Using Road Side Units,https://ieeexplore.ieee.org/document/8846044/,"Smart city progress from classical homogenous technologies with limited facility to heterogeneous interconnected network with immense capabilities. Furthermore, there is a good concern in expanding the scope of application in the smart city. The primary objective of the smart city is to achieve optimization and reinforce the Quality of Service (QoS) of applications by cleverer usage of urban resources. The QoS in the network is measured using several factors like end-end delay, energy consumption, packet loss and throughput. Several pitfalls are experienced in the existing routing innovation. In this proposal, a new technology-based routing structure is proposed. Road Side Units (RSU) will allow the planners to deploy the application without unfamiliar tools for data process and gathering. Data forwarding, acquisition and diffusion are simplified by RSU. K-Nearest Neighbor is used for finding the nearest neighbor nodes and it is optimized using Whale optimization Algorithm (WOA). The evaluation outcomes prove that the intended routing plot provides much spectacle than existing protocols for real time applications.",smart cities
10.1109/DSAA49011.2020.00072,to_check,2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA),IEEE,2020-10-09 00:00:00,ieeexplore,On-Board Unit Big Data: Short-term Traffic Forecasting in Urban Transportation Networks,https://ieeexplore.ieee.org/document/9260026/,"Short-term traffic prediction in transportation net-work is a hot topic for next generation smart cities. Despite several research efforts, there is still a lack of consensus about the most effective way to predict traffic network-wide. Also, the majority of existing studies rely on either small data sets or limited portions of the transportation network. This paper presents the results of a forecasting study related to on-board units data of heavy-goods vehicles in the Brussels-Capital Region. The analysis, enabled by deployment of a big data infrastructure, concerns the short horizon (one hour ahead) forecasting of trucks flow (vehicles/hour) and mean speed (km/hour) over Brussels-Capital Region's network. Both parametric and non-parametric (notably machine learning) models are designed and assessed along with different strategies (simple and linear forecasts combination, individual model selection). The paper shows the potential of large amounts of real-time data obtained from moving sensors where forecasting techniques are applied network-wide. In particular, simple combination schemes (notably the simple median combination forecasts) appear to outperform the other methods in terms of prediction accuracy.",smart cities
10.1109/ACCESS.2020.2984127,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Optimal Placement of Electric Vehicle Charging Stations in the Active Distribution Network,https://ieeexplore.ieee.org/document/9050479/,"Electrification of the transportation sector can play a vital role in reshaping smart cities. With an increasing number of electric vehicles (EVs) on the road, deployment of well-planned and efficient charging infrastructure is highly desirable. Unlike level 1 and level 2 charging stations, level 3 chargers are super-fast in charging EVs. However, their installation at every possible site is not techno-economically justifiable because level 3 chargers may cause violation of critical system parameters due to their high power consumption. In this paper, we demonstrate an optimized combination of all three types of EV chargers for efficiently managing the EV load while minimizing installation cost, losses, and distribution transformer loading. Effects of photovoltaic (PV) generation are also incorporated in the analysis. Due to the uncertain nature of vehicle users, EV load is modeled as a stochastic process. Particle swarm optimization (PSO) is used to solve the constrained nonlinear stochastic problem. MATLAB and OpenDSS are used to simulate the model. The proposed idea is validated on the real distribution system of the National University of Sciences and Technology (NUST) Pakistan. Results show that an optimized combination of chargers placed at judicious locations can greatly reduce cost from $3.55 million to $1.99 million, daily losses from 787kWh to 286kWh and distribution transformer congestion from 58% to 22% when compared to scenario of optimized placement of level 3 chargers for 20% penetration level in commercial feeders. In residential feeder, these statistics are improved from $2.52 to $0.81 million, from 2167kWh to 398kWh and from 106% to 14%, respectively. It is also realized that the integration of PV improves voltage profile and reduces the negative impact of EV load. Our optimization model can work for commercial areas such as offices, university campuses, and industries as well as residential colonies.",smart cities
10.1109/ACCESS.2020.3046999,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,A Framework for the Synergistic Integration of Fully Autonomous Ground Vehicles With Smart City,https://ieeexplore.ieee.org/document/9305204/,"Most of the vehicle manufacturers aim to deploy level-5 fully autonomous ground vehicles (FAGVs) on city roads in 2021 by leveraging extensive existing knowledge about sensors, actuators, telematics and Artificial Intelligence (AI) gained from the level-3 and level-4 autonomy. FAGVs by executing non-trivial sequences of events with decimetre-level accuracy live in Smart City (SC) and their integration with all the SC components and domains using real-time data analytics is urgent to establish better swarm intelligent systems and a safer and optimised harmonious smart environment enabling cooperative FAGVs-SC automation systems. The challenges of urbanisation, if unmet urgently, would entail severe economic and environmental impacts. The integration of FAGVs with SC helps improve the sustainability of a city and the functional and efficient deployment of hand over wheels on robotized city roads with behaviour coordination. SC can enable the exploitation of the full potential of FAGVs with embedded centralised systems within SC with highly distributed systems in a concept of Automation of Everything (AoE). This article proposes a synergistic integrated FAGV-SC holistic framework - FAGVinSCF in which all the components of SC and FAGVs involving recent and impending technological advancements are moulded to make the transformation from today's driving society to future's next-generation driverless society smoother and truly make self-driving technology a harmonious part of our cities with sustainable urban development. Based on FAGVinSCF, a simulation platform is built both to model the varying penetration levels of FAGV into mixed traffic and to perform the optimal self-driving behaviours of FAGV swarms. The results show that FAGVinSCF improves the urban traffic flow significantly without huge changes to the traffic infrastructure. With this framework, the concept of Cooperative Intelligent Transportation Systems (C-ITS) is transformed into the concept of Automated ITS (A-ITS). Cities currently designed for cars can turn into cities developed for citizens using FAGVinSCF enabling more sustainable cities.",smart cities
10.1109/JIOT.2019.2900751,to_check,IEEE Internet of Things Journal,IEEE,2019-10-01 00:00:00,ieeexplore,"Real-Time Fine-Grained Air Quality Sensing Networks in Smart City: Design, Implementation, and Optimization",https://ieeexplore.ieee.org/document/8648185/,"Driven by the increasingly serious air pollution problem, the monitoring of air quality has gained much attention in both theoretical studies and practical implementations. In this paper, we present the architecture, implementation, and optimization of our own air quality sensing system, which provides real-time and fine-grained air quality map of the monitored area. As the major component, the optimization problem of our system is studied in detail. Our objective is to minimize the average joint error of the established real-time air quality map, which involves data inference for the unmeasured data values. A deep Q -learning solution has been proposed for the power control problem to reasonably plan the sensing tasks of the power-limited sensing devices online. A genetic algorithm has been designed for the location selection problem to efficiently find the suitable locations to deploy limited number of sensing devices. The performance of the proposed solutions are evaluated by simulations, showing a significant performance gain when adopting both strategies.",smart cities
10.1109/ITSC.2018.8569391,to_check,2018 21st International Conference on Intelligent Transportation Systems (ITSC),IEEE,2018-11-07 00:00:00,ieeexplore,Leveraging the Channel as a Sensor: Real-time Vehicle Classification Using Multidimensional Radio-fingerprinting,https://ieeexplore.ieee.org/document/8569391/,"Upcoming Intelligent Transportation Systems (ITSs) will transform roads from static resources to dynamic Cyber Physical Systems (CPSs) in order to satisfy the requirements of future vehicular traffic in smart city environments. Up-to-date information serves as the basis for changing street directions as well as guiding individual vehicles to a fitting parking slot. In this context, not only abstract indicators like traffic flow and density are required, but also data about mobility parameters and class information of individual vehicles. Consequently, accurate and reliable systems that are capable of providing these kinds of information in real-time are highly demanded. In this paper, we present a system for the classification of vehicles based on their radio-fingerprints which applies cutting-edge machine learning models and can be non-intrusively installed into the existing road infrastructure in an ad-hoc manner. In contrast to other approaches, it is able to provide accurate classification results without causing privacy-violations or being vulnerable to challenging weather conditions. Moreover, it is a promising candidate for large-scale city deployments due to its cost-efficient installation and maintenance properties. The proposed system is evaluated in a comprehensive field evaluation campaign within an experimental live deployment on a German highway, where it is able to achieve a binary classification success ratio of more than 99% and an overall accuracy of 89.15% for a fine-grained classification task with nine different classes.",smart cities
10.1109/DCOSS.2019.00103,to_check,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),IEEE,2019-05-31 00:00:00,ieeexplore,Emerging Urban Challenge: RPAS/UAVs in Cities,https://ieeexplore.ieee.org/document/8804735/,"An emerging urban challenge will be the proliferation of Remotely Piloted Aircraft Systems (RPAS) or drones, as their usage grows and drones fill the urban skies. Urban airspace will include many more Uninhabited Aerial Vehicles (UAVs), and related accidents and mishaps will also increase. Applications of UAVs in urban environments include photography and film-making, security monitoring, real estate, construction, property and infrastructure inspections, leisure, public safety (fires, natural disasters, investigations), traffic, and much more. Emerging UAV applications include last-mile drone delivery services. This paper discusses urban scenarios where drones are more ubiquitous. Monitoring and related safety of these UAVs will be increasingly important. Though multi-modal purpose-built drone tracking and monitoring systems will be the most effective solution for detection and tracking of these RPAS, during the transition to more regular drone use in urban areas and in everyday urban applications, a more rapid-deployment and agile detection system is needed that does not require installation of hardware. In this paper, sensory substitution is presented as an approach to use ambient and pre-existing microphones and AI techniques to detect the presence of RPAS. Preliminary results show promise for this agile IoT method as a key solution to this emerging urban challenge.",smart cities
10.1109/TITS.2018.2836141,to_check,IEEE Transactions on Intelligent Transportation Systems,IEEE,2019-03-01 00:00:00,ieeexplore,Impact of Data Loss for Prediction of Traffic Flow on an Urban Road Using Neural Networks,https://ieeexplore.ieee.org/document/8370052/,"The deployment of intelligent transport systems requires efficient means of assessing the traffic situation. This involves gathering real traffic data from the road network and predicting the evolution of traffic parameters, in many cases based on incomplete or false data from vehicle detectors. Traffic flows in the network follow spatiotemporal patterns and this characteristic is used to suppress the impact of missing or erroneous data. The application of multilayer perceptrons and deep learning networks using autoencoders for the prediction task is evaluated. Prediction sensitivity to false data is estimated using traffic data from an urban traffic network.",smart cities
10.1109/HPCC/SmartCity/DSS.2019.00082,to_check,2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),IEEE,2019-08-12 00:00:00,ieeexplore,A Learning-Based Vehicle-Trajectory Generation Method for Vehicular Networking,https://ieeexplore.ieee.org/document/8855534/,"With the rapid development of mobile applications, networking technologies have been constantly evolved to offer a more convenient way of sharing information and online-communication anytime and anywhere. Vehicular networks have the potential to become one of the important carriers of future mobile networks. The performance of current vehicular networks has been widely evaluated through simulation experiments due to the high cost and impracticality of other experimental approaches. The most paramount factors of vehicle networks are the authenticity of simulative evaluation, where the mobility of the vehicles is the first significant feature (i.e., the nodes of the vehicular network) that must be properly considered. However, generating the corresponding real mobility datasets has always been a big challenge although it is vital to the simulations of vehicular networks. Therefore, in this paper, we propose a learning-based generation method that can be used to build the vehicle-trajectory data for variety of vehicle densities. Firstly, with analyzing the road bayonet data, we obtain the hidden pattern between road traffic and time. Secondly, we deploy Vissim (a well-known traffic simulator) to generate the experimental data by considering the urban functional areas for the origins of vehicles. The generated experimental data are learned by Extreme Learning Machine (ELM), and the weight matrix of the parameters is obtained, which presents the impact of the experimental parameters on the simulation results. We prove the effectiveness of our method by comparing the generated vehicle-trajectory datasets with the vehicle density predicted by the weight matrix and the realistic traffic flow model.",smart cities
10.1109/ACCESS.2020.3033771,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,An Accelerated Edge Cloud System for Energy Data Stream Processing Based on Adaptive Incremental Deep Learning Scheme,https://ieeexplore.ieee.org/document/9239387/,"As smart metering technology evolves, power suppliers can make low-cost, low-risk estimation of customer-side power consumption by analyzing energy demand data collected in real-time. With advances network infrastructure, smart sensors, and various monitoring technologies, a standardized energy metering infrastructure, called advanced metering infrastructure (AMI), has been introduced and deployed to urban households to allow them to develop efficient power generation plans. Compared to traditional stochastic approaches for time-series data analysis, deep-learning methods have shown superior accuracy on many prediction applications. Because smart meters and infrastructure monitors produce a series of measurements over time, a large amount of data is accumulated, creating a large data stream, which takes much time from data generation to deployment of deep-learning model training. In this article, we propose an accelerated computing system that considers time-variant properties for accurate prediction of energy demand by processing the AMI stream data. The proposed system is a real-time training/inference system that deploys AMI data over a distributed edge cloud. It comprises two core components: an adaptive incremental learning solver and deep-learning acceleration with FPGA-GPU resource scheduling. An adaptive incremental learning scheme adjusts the batch/epoch in training iteration to reduce the time delay of the latest trained model, while trying to prevent biased-training due to the sub-optimal optimizer of incremental learning. In addition, a resource scheduling scheme manages various accelerator resources for accelerated deep-learning processing while minimizing the computational cost. The experimental results demonstrated that our method achieved good performance for adaptive batch size and epoch for incremental learning while guaranteeing a low inference error, a high model score, and queue stability with cost efficient processing.",smart cities
10.1109/TITS.2019.2934574,to_check,IEEE Transactions on Intelligent Transportation Systems,IEEE,2020-10-01 00:00:00,ieeexplore,An Efficient Algorithm for Detection of Vacant Spaces in Delimited and Non-Delimited Parking Lots,https://ieeexplore.ieee.org/document/8809089/,"Smart parking management systems that provide drivers with real-time information about availability and location of parking spaces are high among the wish list of urban dwellers. This paper proposes a machine learning approach for automatic detection of vacant lots in delimited parking spaces where the boundaries of the individual parking lots are predefined, which is then extended to a more general scenario of non-delimited parking spaces. A bag of features (bof) representation for the lots is used with a single svm classifier to discriminate between occupied and vacant lots. For non-delimited spaces, a customized background subtraction algorithm is used to generate proposal regions where vehicles can possibly be found, which are then verified using a bof plus svm approach. When tested on public datasets with images of real parking spaces, the proposed method shows robustness against large intra-class variabilities of vehicles and wide variations in vehicle pose and scale in each parking lot. Compared to state-of-the-art methods, the major advantages of the proposed approach are that it requires fewer images to train the classifier, accepts non-rectangular, variable sized images as input and can also be applied to non-delimited parking spaces, making it suitable for practical deployment.",smart cities
10.1109/ICIMIA48430.2020.9074933,to_check,2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),IEEE,2020-03-07 00:00:00,ieeexplore,A Hybrid Framework for Expediting Emergency Vehicle Movement on Indian Roads,https://ieeexplore.ieee.org/document/9074933/,"Unhindered and smooth movement of emergency vehicles within a city is a crucial aspect of any intelligent transport system. It is common to observe emergency vehicles such as ambulances and fire engines obstructed by traffic snarls on Indian roads, especially in the proximity of busy intersections. Existing literature primarily advocates the deployment of RFID technology to terminate the round-robin sequence of the signal system and switch the signal to green in the required direction. However, this technology has proven to be susceptible to electromagnetic interferences and also the economic feasibility is questionable. This paper proposes a model that employs real time image processing and object detection using a convolutional neural network (CNN) architecture called SSD Mobilenet. Unlike a few other architectures, SSD Mobilenet requires very limited computation, hence enabling swift detection. Furthermore, an acoustic signal (sound) processing (pitch detection) algorithm is employed to detect the sirens of emergency vehicles to nullify the potential false positives (e.g. an ambulance in a non-emergency scenario) that creep into object detection using image processing. Both algorithms work in unison, bolstering the accuracy of detection. Upon detection, the signal instantly switches to green, facilitating the expedited movement of emergency vehicles, even in high traffic conditions.",smart cities
10.1109/CVPRW53098.2021.00465,to_check,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),IEEE,2021-06-25 00:00:00,ieeexplore,Tiny-PIRATE: A Tiny model with Parallelized Intelligence for Real-time Analysis as a Traffic countEr,https://ieeexplore.ieee.org/document/9522996/,"Due to the rapid growth in the number of vehicles over the last decade, there has been a dramatic increase in demand for highway capacity analysis. Vehicle counting, in particular, has become a key element of vision-based intelligent traffic systems deployed across metropolitan areas. Most methods solved the vehicle counting problem under the assumption of state-of-the-art computing systems. However, large-scale deployment of such systems for multi-camera processing is very inefficient. With the recent advancement of cost-efficient Internet-of-Things (IoT) devices alongside machine learning methods developed specifically for such devices, solving the vehicle counting problem for real-time traffic analysis on IoT edge devices, and thereby facilitating its large-scale deployment have become highly favorable. In this paper, we propose a framework of vehicle counting designed specifically for IoT edge computers which follows the detection-tracking-counting (DTC) model. The proposed solution aims at addressing the multimodality of contextual dynamics in traffic scenes with a small detector model, a robust tracker and a counting process that accurately estimate both a vehicle's motion of interest and its exit time from observation areas. Experimental results on AI City 2021 Track-1 Dataset showed that ours outperformed related methods with promising results regarding both accuracy and execution speed.",smart cities
10.1109/TNSE.2019.2901833,to_check,IEEE Transactions on Network Science and Engineering,IEEE,2020-03-01 00:00:00,ieeexplore,Battery Maintenance of Pedelec Sharing System: Big Data Based Usage Prediction and Replenishment Scheduling,https://ieeexplore.ieee.org/document/8653324/,"Pedelecs are an alternative of traditional share bikes by applying the battery-powered motor to assist pedaling and accordingly extend the riding coverage. The large scale deployment of pedelecs, however, requires a careful design of maintenance system to replace the batteries regularly that can be costly. This paper investigates the maintenance of a city-wide pedelec system by developing an offline solution in two steps. First, we develop an optimal and efficient hybrid prediction model which predicts the usage demand of pedelecs in every 48 h on a scale of millions of pedelecs. Our proposal predicts the future usage increment of pedelecs by combining a local predictor, a global predictor, and an inflection predictor, which captures both the short-term and long-term factors affecting the pedelec usage. Second, based on the developed predictor and results of big data analytics, an optimal path planning scheme for the replenishment of pedelec batteries is developed. As compared to other schemes, our scheme can save 40% of the maintenance cost. To verify our proposal, extensive real-data driven simulations are performed which show that the accuracy of the prediction process is high enough than each traditional method and our proposal solves the maintenance problem efficiently.",smart cities
10.1109/ICC.2018.8422231,to_check,2018 IEEE International Conference on Communications (ICC),IEEE,2018-05-24 00:00:00,ieeexplore,Anticipatory Mobility Management by Big Data Analytics for Ultra-Low Latency Mobile Networking,https://ieeexplore.ieee.org/document/8422231/,"Massive deployment of autonomous vehicles, un- manned aerial vehicles, and robots, brings in a new technology challenge to establish ultra-low end-to-end latency mobile networking to enable holistic computing mechanisms. With the aid of open-loop wireless communication and proactive network association in vehicle-centric heterogeneous network architecture, anticipatory mobility management relying on inference and learning from big vehicular data plays a key role to facilitate such a new technological paradigm. Anticipatory mobility management aims to predict APs to be connected in the next time instant and in a real-time manner, such that ultra-low latency downlink open-loop communication can be realized with proactive network association. In this paper, we successfully respond this technology challenge using big data analytics with location-based learning and inference tech- niques, to achieve satisfactory performance of predicting APs. Real vehicular movement data have been used to verify that the proposed prediction methods are effective for the purpose of anticipatory mobility management and thus ultra-low latency mobile networking.",smart cities
10.1109/TMC.2013.14,to_check,IEEE Transactions on Mobile Computing,IEEE,2014-03-01 00:00:00,ieeexplore,SmartDC: Mobility Prediction-Based Adaptive Duty Cycling for Everyday Location Monitoring,https://ieeexplore.ieee.org/document/6412671/,"Monitoring a user's mobility during daily life is an essential requirement in providing advanced mobile services. While extensive attempts have been made to monitor user mobility, previous work has rarely addressed issues with predictions of temporal behavior in real deployment. In this paper, we introduce SmartDC, a mobility prediction-based adaptive duty cycling scheme to provide contextual information about a user's mobility: time-resolved places and paths. Unlike previous approaches that focused on minimizing energy consumption for tracking raw coordinates, we propose efficient techniques to maximize the accuracy of monitoring meaningful places with a given energy constraint. SmartDC comprises unsupervised mobility learner, mobility predictor, and Markov decision process-based adaptive duty cycling. SmartDC estimates the regularity of individual mobility and predicts residence time at places to determine efficient sensing schedules. Our experiment results show that SmartDC consumes 81 percent less energy than the periodic sensing schemes, and 87 percent less energy than a scheme employing context-aware sensing, yet it still correctly monitors 90 percent of a user's location changes within a 160-second delay.",smart cities
10.1109/CSCN.2017.8088624,to_check,2017 IEEE Conference on Standards for Communications and Networking (CSCN),IEEE,2017-09-20 00:00:00,ieeexplore,Big data and machine learning driven handover management and forecasting,https://ieeexplore.ieee.org/document/8088624/,"Handover (HO), as a key aspect of mobility management, plays an important role in improving network quality and mobility performance in mobile networks. Especially, in 5G networks, heterogeneous networks (HetNets) deployment of macro cells and small cells, and the deployment of ultra-dense networks (UDNs) make HO management become more challenging. Besides, the understanding of HO behavior in a cell is quite limited in existing studies, thus the forecasting HO for an individual cell is complicated, even impossible. This challenge led the authors to propose a practical process for managing and forecasting HO for a huge number of cells, based on machinelearning (ML) algorithms and big data. Moreover, based on HO forecasting, the authors also propose an approach to detect any abnormal HO in cells. The performance of the proposed approaches was evaluated by applying it to a real dataset that collected HO KPI of more than 6000 cells of a real network during the years, 2016 and 2017. The results show that the study was successful in identifying, separating HO behavior, forecasting the future number of HO attempts, and detecting abnormal HO behaviors of cells.",smart cities
10.1109/CCGRID.2019.00048,to_check,"2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",IEEE,2019-05-17 00:00:00,ieeexplore,Multivariate LSTM-Based Location-Aware Workload Prediction for Edge Data Centers,https://ieeexplore.ieee.org/document/8752650/,"Mobile Edge Clouds (MECs) is a promising computing platform to overcome challenges for the success of bandwidth-hungry, latency-critical applications by distributing computing and storage capacity in the edge of the network as Edge Data Centers (EDCs) within the close vicinity of end-users. Due to the heterogeneous distributed resource capacity in EDCs, the application deployment flexibility coupled with the user mobility, MECs bring significant challenges to control resource allocation and provisioning. In order to develop a self-managed system for MECs which efficiently decides how much and when to activate scaling, where to place and migrate services, it is crucial to predict its workload characteristics, including variations over time and locality. To this end, we present a novel location-aware workload predictor for EDCs. Our approach leverages the correlation among workloads of EDCs in a close physical distance and applies multivariate Long Short-Term Memory network to achieve on-line workload predictions for each EDC. The experiments with two real mobility traces show that our proposed approach can achieve better prediction accuracy than a state-of-the art location-unaware method (up to 44%) and a location-aware method (up to 17%). Further, through an intensive performance measurement using various input shaking methods, we substantiate that the proposed approach achieves a reliable and consistent performance.",smart cities
10.1109/CAMAD.2018.8515001,to_check,2018 IEEE 23rd International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),IEEE,2018-09-19 00:00:00,ieeexplore,Uncertainty Management for Wearable IoT Wristband Sensors Using Laplacian-Based Matrix Completion,https://ieeexplore.ieee.org/document/8515001/,"Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems.",smart cities
10.1109/IntelCIS.2015.7397237,to_check,2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS),IEEE,2015-12-14 00:00:00,ieeexplore,A knowledge-in-the-loop approach to integrated safety&amp;security for cooperative system-of-systems,https://ieeexplore.ieee.org/document/7397237/,"A system-of-systems (SoS) is inherently open in configuration and evolutionary in lifecycle. For the next generation of cooperative cyber-physical system-of-systems, safety and security constitute two key issues of public concern that affect the deployment and acceptance. In engineering, the openness and evolutionary nature also entail radical paradigm shifts. This paper presents one novel approach to the development of qualified cyber-physical system-of-systems, with Cooperative Intelligent Transport Systems (C-ITS) as one target. The approach, referred to as knowledge-in-the-loop, aims to allow a synergy of well-managed lifecycles, formal quality assurance, and smart system features. One research goal is to enable an evolutionary development with continuous and traceable flows of system rationale from design-time to post-deployment time and back, supporting automated knowledge inference and enrichment. Another research goal is to develop a formal approach to risk-aware dynamic treatment of safety and security as a whole in the context of system-of-systems. Key base technologies include: (1) EAST-ADL for the consolidation of system-wide concerns and for the creation of an ontology for advanced run-time decisions, (2) Learning Based-Testing for run-time and post-deployment model inference, safety monitoring and testing, (3) Provable Isolation for run-time attack detection and enforcement of security in real-time operating systems.",smart cities
10.1109/ITST.2011.6060117,to_check,2011 11th International Conference on ITS Telecommunications,IEEE,2011-08-25 00:00:00,ieeexplore,A systemic and cooperative approach towards an integrated infomobility system at regional scale,https://ieeexplore.ieee.org/document/6060117/,"With the term infomobility we refer to the broad range of Intelligent Transport Systems (ITS) information services. Despite the increasing efforts on development of infomobility services, it is recognized that they still show significant limitations in providing end users with reliable and seamless information. In this paper we argue that a systemic approach is needed to foster the growth of novel solutions for the delivery of integrated realtime and always-on infomobility services, to be seamlessly provided across different transport operators, modes and geographical areas. The proposed approach - conceived and implemented within the ongoing SIMob research project in Tuscany Region (Italy) - relies on the definition of an “Open Working Environment”, i.e. a socio-technical environment whose objective is to facilitate the co-operation of stakeholders to foster the growth of an integrated infomobility system at regional scale. Its main constituents are: a Cooperative Network of stakeholders, including regional and national subjects from Public Administrations, Enterprises and Research Bodies; an Architectural Framework, providing the reference technological baseline enabling the rapid configuration and deployment of different technological capabilities.",smart cities
10.1109/INFOCOM.2018.8486321,to_check,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,IEEE,2018-04-19 00:00:00,ieeexplore,Real-time Video Quality of Experience Monitoring for HTTPS and QUIC,https://ieeexplore.ieee.org/document/8486321/,"The widespread deployment of end-to-end encryption protocols such as HTTPS and QUIC has reduced the visibility for operators into traffic on their networks. Network operators need the visibility to monitor and mitigate Quality of Experience (QoE) impairments in popular applications such as video streaming. To address this problem, we propose a machine learning based approach to monitor QoE metrics for encrypted video traffic. We leverage network and transport layer information as features to train machine learning classifiers for inferring video QoE metrics such as startup delay and rebuffering events. Using our proposed approach, network operators can detect and react to encrypted video QoE impairments in real-time. We evaluate our approach for YouTube adaptive video streams using HTTPS and QUIC. The experimental evaluations show that our approach achieves up to 90% classification accuracy for HTTPS and up to 85 % classification accuracy for QUIC.",smart cities
10.1109/JSAC.2021.3064698,to_check,IEEE Journal on Selected Areas in Communications,IEEE,2021-09-01 00:00:00,ieeexplore,Autonomous and Energy Efficient Lightpath Operation Based on Digital Subcarrier Multiplexing,https://ieeexplore.ieee.org/document/9373418/,"The massive deployment of 5G and beyond will require high capacity and low latency connectivity services, so network operators will have either to overprovision capacity in their transport networks or to upgrade the optical network controllers to make decisions nearly in real time; both solutions entail high capital and operational expenditures. A different approach could be to move the decision making toward the nodes and subsystems, so they can adapt dynamically the capacity to the actual needs and thus reduce operational costs in terms of energy consumption. To achieve this, several technological challenges need to be addressed. In this paper, we focus on the autonomous operation of Digital Subcarrier Multiplexing (DSCM) systems, which enable the transmission of multiple and independent subcarriers (SC). Herein, we present several solutions enabling the autonomous DSCM operation, including: i) SC quality of transmission estimation; ii) autonomous SC operation at the transmitter side and blind SC configuration recognition at the receiver side; and iii) intent-based capacity management implemented through Reinforcement Learning. We provide useful guidelines for the application of autonomous SC management supported by the extensive results presented.",smart cities
10.1007/978-981-33-6195-9_3,to_check,Nature-Inspired Computing for Smart Application Design,Springer,2021-01-01 00:00:00,springer,Environmental Sound Classification Using Neural Network and Deep Learning,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-33-6195-9_3,"In this chapter, a method to classify environmental noise in the traffic of urban cities in real time using artificial neural network is discussed. The objective is to design an accurate model for classification of environment sounds present in the city. In fact, the background noise is random in nature and it becomes important to classify them in-order to identify their sources which may use to create awareness among the people. In recent past, neural networks and deep learning have been gaining popularity in signal processing and classification. Therefore, in this chapter, the features of input audio signal is extracted and used for training the neural network using deep learning algorithm for classifying environmental noise. The trained neural network is used to identify different classes of sounds present in the recorded signal. The proposed method focuses on improving model’s prediction accuracy by the latest deep learning algorithm and techniques, for a diverse variety of sounds and a provide an estimation of the source of the sound. The simulation results show that the Bayesian neural network provides better performance over other networks. Further, this project aims to deploy the network model on an android platform so that it would be compatible on most of the smart phones and benefit wide number of people.",smart cities
http://arxiv.org/abs/1807.00464v1,to_check,arxiv,arxiv,2018-07-02 05:17:15+00:00,arxiv,"Leveraging the Channel as a Sensor: Real-time Vehicle Classification
  Using Multidimensional Radio-fingerprinting",http://arxiv.org/abs/1807.00464v1,"Upcoming Intelligent Transportation Systems (ITSs) will transform roads from
static resources to dynamic Cyber Physical Systems (CPSs) in order to satisfy
the requirements of future vehicular traffic in smart city environments.
Up-to-date information serves as the basis for changing street directions as
well as guiding individual vehicles to a fitting parking slot. In this context,
not only abstract indicators like traffic flow and density are required, but
also data about mobility parameters and class information of individual
vehicles. Consequently, accurate and reliable systems that are capable of
providing these kinds of information in real-time are highly demanded. In this
paper, we present a system for classifying vehicles based on their
radio-fingerprints which applies cutting-edge machine learning models and can
be non-intrusively installed into the existing road infrastructure in an ad-hoc
manner. In contrast to other approaches, it is able to provide accurate
classification results without causing privacy-violations or being vulnerable
to challenging weather conditions. Moreover, it is a promising candidate for
large-scale city deployments due to its cost-efficient installation and
maintenance properties. The proposed system is evaluated in a comprehensive
field evaluation campaign within an experimental live deployment on a German
highway, where it is able to achieve a binary classification success ratio of
more than 99% and an overall accuracy of 89.15% for a fine-grained
classification task with nine different classes.",smart cities
http://arxiv.org/abs/2106.15021v1,to_check,arxiv,arxiv,2021-06-21 11:23:12+00:00,arxiv,"How to Reach Real-Time AI on Consumer Devices? Solutions for
  Programmable and Custom Architectures",http://arxiv.org/abs/2106.15021v1,"The unprecedented performance of deep neural networks (DNNs) has led to large
strides in various Artificial Intelligence (AI) inference tasks, such as object
and speech recognition. Nevertheless, deploying such AI models across commodity
devices faces significant challenges: large computational cost, multiple
performance objectives, hardware heterogeneity and a common need for high
accuracy, together pose critical problems to the deployment of DNNs across the
various embedded and mobile devices in the wild. As such, we have yet to
witness the mainstream usage of state-of-the-art deep learning algorithms
across consumer devices. In this paper, we provide preliminary answers to this
potentially game-changing question by presenting an array of design techniques
for efficient AI systems. We start by examining the major roadblocks when
targeting both programmable processors and custom accelerators. Then, we
present diverse methods for achieving real-time performance following a
cross-stack approach. These span model-, system- and hardware-level techniques,
and their combination. Our findings provide illustrative examples of AI systems
that do not overburden mobile hardware, while also indicating how they can
improve inference accuracy. Moreover, we showcase how custom ASIC- and
FPGA-based accelerators can be an enabling factor for next-generation AI
applications, such as multi-DNN systems. Collectively, these results highlight
the critical need for further exploration as to how the various cross-stack
solutions can be best combined in order to bring the latest advances in deep
learning close to users, in a robust and efficient manner.",smart cities
http://arxiv.org/abs/1911.03878v2,to_check,arxiv,arxiv,2019-11-10 08:59:21+00:00,arxiv,"An Overview of Data-Importance Aware Radio Resource Management for Edge
  Machine Learning",http://arxiv.org/abs/1911.03878v2,"The 5G network connecting billions of Internet-of-Things (IoT) devices will
make it possible to harvest an enormous amount of real-time mobile data.
Furthermore, the 5G virtualization architecture will enable cloud computing at
the (network) edge. The availability of both rich data and computation power at
the edge has motivated Internet companies to deploy artificial intelligence
(AI) there, creating the hot area of edge-AI. Edge learning, the theme of this
project, concerns training edge-AI models, which endow on IoT devices
intelligence for responding to real-time events. However, the transmission of
high-dimensional data from many edge devices to servers can result in excessive
communication latency, creating a bottleneck for edge learning. Traditional
wireless techniques deigned for only radio access are ineffective in tackling
the challenge. Attempts to overcome the communication bottleneck has led to the
development of a new class of techniques for intelligent radio resource
management (RRM), called data-importance aware RRM. Their designs feature the
interplay of active machine learning and wireless communication. Specifically,
the metrics that measure data importance in active learning (e.g.,
classification uncertainty and data diversity) are applied to RRM for efficient
acquisition of distributed data in wireless networks to train AI models at
servers. This article aims at providing an introduction to the emerging area of
importance-aware RRM. To this end, we will introduce the design principles,
survey recent advancements in the area, discuss some design examples, and
suggest some promising research opportunities.",smart cities
http://arxiv.org/abs/2007.07122v2,to_check,arxiv,arxiv,2020-07-14 15:42:55+00:00,arxiv,"Energy-Efficient Resource Management for Federated Edge Learning with
  CPU-GPU Heterogeneous Computing",http://arxiv.org/abs/2007.07122v2,"Edge machine learning involves the deployment of learning algorithms at the
network edge to leverage massive distributed data and computation resources to
train artificial intelligence (AI) models. Among others, the framework of
federated edge learning (FEEL) is popular for its data-privacy preservation.
FEEL coordinates global model training at an edge server and local model
training at edge devices that are connected by wireless links. This work
contributes to the energy-efficient implementation of FEEL in wireless networks
by designing joint computation-and-communication resource management
($\text{C}^2$RM). The design targets the state-of-the-art heterogeneous mobile
architecture where parallel computing using both a CPU and a GPU, called
heterogeneous computing, can significantly improve both the performance and
energy efficiency. To minimize the sum energy consumption of devices, we
propose a novel $\text{C}^2$RM framework featuring multi-dimensional control
including bandwidth allocation, CPU-GPU workload partitioning and speed scaling
at each device, and $\text{C}^2$ time division for each link. The key component
of the framework is a set of equilibriums in energy rates with respect to
different control variables that are proved to exist among devices or between
processing units at each device. The results are applied to designing efficient
algorithms for computing the optimal $\text{C}^2$RM policies faster than the
standard optimization tools. Based on the equilibriums, we further design
energy-efficient schemes for device scheduling and greedy spectrum sharing that
scavenges ""spectrum holes"" resulting from heterogeneous $\text{C}^2$ time
divisions among devices. Using a real dataset, experiments are conducted to
demonstrate the effectiveness of $\text{C}^2$RM on improving the energy
efficiency of a FEEL system.",smart cities
http://arxiv.org/abs/1901.02144v1,to_check,arxiv,arxiv,2019-01-08 03:32:31+00:00,arxiv,"Guidelines and Benchmarks for Deployment of Deep Learning Models on
  Smartphones as Real-Time Apps",http://arxiv.org/abs/1901.02144v1,"Deep learning solutions are being increasingly used in mobile applications.
Although there are many open-source software tools for the development of deep
learning solutions, there are no guidelines in one place in a unified manner
for using these tools towards real-time deployment of these solutions on
smartphones. From the variety of available deep learning tools, the most suited
ones are used in this paper to enable real-time deployment of deep learning
inference networks on smartphones. A uniform flow of implementation is devised
for both Android and iOS smartphones. The advantage of using multi-threading to
achieve or improve real-time throughputs is also showcased. A benchmarking
framework consisting of accuracy, CPU/GPU consumption and real-time throughput
is considered for validation purposes. The developed deployment approach allows
deep learning models to be turned into real-time smartphone apps with ease
based on publicly available deep learning and smartphone software tools. This
approach is applied to six popular or representative convolutional neural
network models and the validation results based on the benchmarking metrics are
reported.",smart cities
http://arxiv.org/abs/1805.04902v2,to_check,arxiv,arxiv,2018-05-13 15:55:33+00:00,arxiv,LMNet: Real-time Multiclass Object Detection on CPU using 3D LiDAR,http://arxiv.org/abs/1805.04902v2,"This paper describes an optimized single-stage deep convolutional neural
network to detect objects in urban environments, using nothing more than point
cloud data. This feature enables our method to work regardless the time of the
day and the lighting conditions.The proposed network structure employs dilated
convolutions to gradually increase the perceptive field as depth increases,
this helps to reduce the computation time by about 30%. The network input
consists of five perspective representations of the unorganized point cloud
data. The network outputs an objectness map and the bounding box offset values
for each point. Our experiments showed that using reflection, range, and the
position on each of the three axes helped to improve the location and
orientation of the output bounding box. We carried out quantitative evaluations
with the help of the KITTI dataset evaluation server. It achieved the fastest
processing speed among the other contenders, making it suitable for real-time
applications. We implemented and tested it on a real vehicle with a Velodyne
HDL-64 mounted on top of it. We achieved execution times as fast as 50 FPS
using desktop GPUs, and up to 10 FPS on a single Intel Core i5 CPU. The deploy
implementation is open-sourced and it can be found as a feature branch inside
the autonomous driving framework Autoware. Code is available at:
https://github.com/CPFL/Autoware/tree/feature/cnn_lidar_detection",smart cities
http://arxiv.org/abs/2008.05255v1,to_check,arxiv,arxiv,2020-08-12 12:03:27+00:00,arxiv,"Identity-Aware Attribute Recognition via Real-Time Distributed Inference
  in Mobile Edge Clouds",http://arxiv.org/abs/2008.05255v1,"With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods.",smart cities
http://arxiv.org/abs/1906.10910v2,to_check,arxiv,arxiv,2019-06-26 08:37:44+00:00,arxiv,"Creating A Neural Pedagogical Agent by Jointly Learning to Review and
  Assess",http://arxiv.org/abs/1906.10910v2,"Machine learning plays an increasing role in intelligent tutoring systems as
both the amount of data available and specialization among students grow.
Nowadays, these systems are frequently deployed on mobile applications. Users
on such mobile education platforms are dynamic, frequently being added,
accessing the application with varying levels of focus, and changing while
using the service. The education material itself, on the other hand, is often
static and is an exhaustible resource whose use in tasks such as problem
recommendation must be optimized. The ability to update user models with
respect to educational material in real-time is thus essential; however,
existing approaches require time-consuming re-training of user features
whenever new data is added. In this paper, we introduce a neural pedagogical
agent for real-time user modeling in the task of predicting user response
correctness, a central task for mobile education applications. Our model,
inspired by work in natural language processing on sequence modeling and
machine translation, updates user features in real-time via bidirectional
recurrent neural networks with an attention mechanism over embedded
question-response pairs. We experiment on the mobile education application
SantaTOEIC, which has 559k users, 66M response data points as well as a set of
10k study problems each expert-annotated with topic tags and gathered since
2016. Our model outperforms existing approaches over several metrics in
predicting user response correctness, notably out-performing other methods on
new users without large question-response histories. Additionally, our
attention mechanism and annotated tag set allow us to create an interpretable
education platform, with a smart review system that addresses the
aforementioned issue of varied user attention and problem exhaustion.",smart cities
http://arxiv.org/abs/2109.02270v1,to_check,arxiv,arxiv,2021-09-06 07:38:24+00:00,arxiv,"Encryption and Real Time Decryption for protecting Machine Learning
  models in Android Applications",http://arxiv.org/abs/2109.02270v1,"With the Increasing use of Machine Learning in Android applications, more
research and efforts are being put into developing better-performing machine
learning algorithms with a vast amount of data. Along with machine learning for
mobile phones, the threat of extraction of trained machine learning models from
application packages (APK) through reverse engineering exists. Currently, there
are ways to protect models in mobile applications such as name obfuscation,
cloud deployment, last layer isolation. Still, they offer less security, and
their implementation requires more effort. This paper gives an algorithm to
protect trained machine learning models inside android applications with high
security and low efforts to implement it. The algorithm ensures security by
encrypting the model and real-time decrypting it with 256-bit Advanced
Encryption Standard (AES) inside the running application. It works efficiently
with big model files without interrupting the User interface (UI) Thread. As
compared to other methods, it is fast, more secure, and involves fewer efforts.
This algorithm provides the developers and researchers a way to secure their
actions and making the results available to all without any concern.",smart cities
http://arxiv.org/abs/2101.07996v1,to_check,arxiv,arxiv,2021-01-20 06:47:41+00:00,arxiv,SplitSR: An End-to-End Approach to Super-Resolution on Mobile Devices,http://arxiv.org/abs/2101.07996v1,"Super-resolution (SR) is a coveted image processing technique for mobile apps
ranging from the basic camera apps to mobile health. Existing SR algorithms
rely on deep learning models with significant memory requirements, so they have
yet to be deployed on mobile devices and instead operate in the cloud to
achieve feasible inference time. This shortcoming prevents existing SR methods
from being used in applications that require near real-time latency. In this
work, we demonstrate state-of-the-art latency and accuracy for on-device
super-resolution using a novel hybrid architecture called SplitSR and a novel
lightweight residual block called SplitSRBlock. The SplitSRBlock supports
channel-splitting, allowing the residual blocks to retain spatial information
while reducing the computation in the channel dimension. SplitSR has a hybrid
design consisting of standard convolutional blocks and lightweight residual
blocks, allowing people to tune SplitSR for their computational budget. We
evaluate our system on a low-end ARM CPU, demonstrating both higher accuracy
and up to 5 times faster inference than previous approaches. We then deploy our
model onto a smartphone in an app called ZoomSR to demonstrate the first-ever
instance of on-device, deep learning-based SR. We conducted a user study with
15 participants to have them assess the perceived quality of images that were
post-processed by SplitSR. Relative to bilinear interpolation -- the existing
standard for on-device SR -- participants showed a statistically significant
preference when looking at both images (Z=-9.270, p<0.01) and text (Z=-6.486,
p<0.01).",smart cities
http://arxiv.org/abs/1902.00159v1,to_check,arxiv,arxiv,2019-02-01 03:24:26+00:00,arxiv,Compressing GANs using Knowledge Distillation,http://arxiv.org/abs/1902.00159v1,"Generative Adversarial Networks (GANs) have been used in several machine
learning tasks such as domain transfer, super resolution, and synthetic data
generation. State-of-the-art GANs often use tens of millions of parameters,
making them expensive to deploy for applications in low SWAP (size, weight, and
power) hardware, such as mobile devices, and for applications with real time
capabilities. There has been no work found to reduce the number of parameters
used in GANs. Therefore, we propose a method to compress GANs using knowledge
distillation techniques, in which a smaller ""student"" GAN learns to mimic a
larger ""teacher"" GAN. We show that the distillation methods used on MNIST,
CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1,
58:1, and 87:1, respectively, while retaining the quality of the generated
image. From our experiments, we observe a qualitative limit for GAN's
compression. Moreover, we observe that, with a fixed parameter budget,
compressed GANs outperform GANs trained using standard training methods. We
conjecture that this is partially owing to the optimization landscape of
over-parameterized GANs which allows efficient training using alternating
gradient descent. Thus, training an over-parameterized GAN followed by our
proposed compression scheme provides a high quality generative model with a
small number of parameters.",smart cities
http://arxiv.org/abs/2102.05449v3,to_check,arxiv,arxiv,2021-02-10 14:12:10+00:00,arxiv,"Adaptive Processor Frequency Adjustment for Mobile Edge Computing with
  Intermittent Energy Supply",http://arxiv.org/abs/2102.05449v3,"With astonishing speed, bandwidth, and scale, Mobile Edge Computing (MEC) has
played an increasingly important role in the next generation of connectivity
and service delivery. Yet, along with the massive deployment of MEC servers,
the ensuing energy issue is now on an increasingly urgent agenda. In the
current context, the large scale deployment of renewable-energy-supplied MEC
servers is perhaps the most promising solution for the incoming energy issue.
Nonetheless, as a result of the intermittent nature of their power sources,
these special design MEC server must be more cautious about their energy usage,
in a bid to maintain their service sustainability as well as service standard.
Targeting optimization on a single-server MEC scenario, we in this paper
propose NAFA, an adaptive processor frequency adjustment solution, to enable an
effective plan of the server's energy usage. By learning from the historical
data revealing request arrival and energy harvest pattern, the deep
reinforcement learning-based solution is capable of making intelligent
schedules on the server's processor frequency, so as to strike a good balance
between service sustainability and service quality. The superior performance of
NAFA is substantiated by real-data-based experiments, wherein NAFA demonstrates
up to 20% increase in average request acceptance ratio and up to 50% reduction
in average request processing time.",smart cities
10.1016/j.tra.2021.01.020,to_check,Transportation Research Part A: Policy and Practice,scopus,2021-03-01,sciencedirect,"Spatio-temporal analysis of on-demand transit: A case study of Belleville, Canada",https://api.elsevier.com/content/abstract/scopus_id/85100659326,"The rapid increase in the cyber-physical nature of transportation, availability of GPS data, mobile applications, and effective communication technologies have led to the emergence of On-Demand Transit (ODT) systems. In September 2018, the City of Belleville in Canada started an on-demand public transit pilot project, where the late-night fixed-route (RT 11) was substituted with the ODT providing a real-time ride-hailing service. We present an in-depth analysis of the spatio-temporal demand and supply, level of service, and origin and destination patterns of Belleville ODT users, based on the data collected from September 2018 till May 2019. The independent and combined effects of the demographic characteristics (population density, working-age, and median income) on the ODT trip production and attraction levels were studied using GIS and the K-means machine learning clustering algorithm. The results indicate that ODT trips demand is highest for 11:00 pm–11:45 pm during the weekdays and 8:00 pm–8:30 pm during the weekends. We expect this to be the result of users returning home from work or shopping. Results showed that 39% of the trips were found to have a waiting time of smaller than 15 min, while 28% of trips had a waiting time of 15–30 min. The dissemination areas with higher population density, lower median income, or higher working-age percentages tend to have higher ODT trip attraction levels, except for the dissemination areas that have highly attractive places like commercial areas. For the sustainable deployment of ODT services, we recommend (a) proactively relocating the empty ODT vehicles near the neighbourhoods with high level of activity, (b) dynamically updating the fleet size and location based on the anticipated changes in the spatio-temporal demand, and (c) using medium occupancy vehicles, like vans or minibuses to ensure high level of service.",smart cities
10.1016/j.comnet.2020.107573,to_check,Computer Networks,scopus,2020-12-09,sciencedirect,AI-enabled mobile multimedia service instance placement scheme in mobile edge computing,https://api.elsevier.com/content/abstract/scopus_id/85091771160,"Leveraging cloud infrastructure to the mobile edge computing helps the mobile users to get real time multimedia services in Fifth Generation (5G) network system. To ensure higher Quality-of-Experience (QoE), faster migration of mobile multimedia service instances is required to cope up with user mobility. By deploying the mobile multimedia service instances proactively in multiple edge nodes (ENs) helps the users to get higher QoE. However, excessive deployment of service replicas might increase the cost of the overall network. To establish trade-off between these two conflicting objectives, we have formulated the problem as a Multi-objective Integer Linear Programming (MILP) by integrating the users’ path prediction model. This problem is proven to be an NP-hard one for large networks, thus we develop an artificial intelligence (AI) based meta-heuristic Binary Particle Swarm Optimization (BPSO) algorithm to achieve near-optimal solution within polynomial time. The performance analysis results show the significant performance improvement in terms of QoE and user satisfaction as compared to other state-of-the-art works.",smart cities
10.1016/j.jnca.2020.102692,to_check,Journal of Network and Computer Applications,scopus,2020-08-15,sciencedirect,MARIO: A spatio-temporal data mining framework on Google Cloud to explore mobility dynamics from taxi trajectories,https://api.elsevier.com/content/abstract/scopus_id/85084749934,"With the major advances in location acquisition techniques, deployment of GPS enabled devices and increasing number of mobile users, substantial amount of location traces are generated from different geographical regions. It provides unprecedented opportunities to analyze and derive valuable insights of urban dynamics, specifically, time-dependent mobility patterns and region-specific travel demands. This work proposes an end-to-end mobility association rule mining framework called MARIO, conducive to extract urban mobility dynamics through analysing large taxi trip traces of a city. The MARIO framework consists of (i) generating mobility-dynamics network by spatio-temporal analysis of taxi-trips, (ii) finding travel demand variations in different functional regions of the urban area, (iii) extracting mobility association rules and (iv) predicting travel demands and traffic dynamics using extracted associative rules. The proposed MARIO framework is implemented in Google Cloud Platform and an extensive set of experiments using real GPS trace dataset of NYC Green and Yellow Taxi trace, Roma Taxi Dataset and San Francisco Taxi Dataset have been carried out to demonstrate the effectiveness of the framework. The performance of the proposed approach is significantly better than the baseline methods in predicting travel demands (with the reduction of average MAPE value and execution time by 50%).",smart cities
10.1016/j.future.2017.08.009,to_check,Future Generation Computer Systems,scopus,2019-03-01,sciencedirect,AGRA: AI-augmented geographic routing approach for IoT-based incident-supporting applications,https://api.elsevier.com/content/abstract/scopus_id/85028541738,"Applications that cater to the needs of disaster incident response generate large amount of data and demand large computational resource access. Such datasets are usually collected in real-time at the incident scenes using different Internet of Things (IoT) devices. Hierarchical clouds, i.e., core and edge clouds, can help these applications’ real-time data orchestration challenges as well as with their IoT operations scalability, reliability and stability by overcoming infrastructure limitations at the ad-hoc wireless network edge. Routing is a crucial infrastructure management orchestration mechanism for such systems. Current geographic routing or greedy forwarding approaches designed for early wireless ad-hoc networks lack efficient solutions for disaster incident-supporting applications, given the high-speed and low-latency data delivery that edge cloud gateways impose. In this paper, we present a novel Artificial Intelligent (AI)-augmented geographic routing approach, that relies on an area knowledge obtained from the satellite imagery (available at the edge cloud) by applying deep learning. In particular, we propose a stateless greedy forwarding that uses such an environment learning to proactively avoid the local minimum problem by diverting traffic with an algorithm that emulates electrostatic repulsive forces. In our theoretical analysis, we show that our Greedy Forwarding achieves in the worst case a 
                        3
                        .
                        291
                      path stretch approximation bound with respect to the shortest path, without assuming presence of symmetrical links or unit disk graphs. We evaluate our approach with both numerical and event-driven simulations, and we establish the practicality of our approach in a real incident-supporting hierarchical cloud deployment to demonstrate improvement of application level throughput due to a reduced path stretch under severe node failures and high mobility challenges of disaster response scenarios.",smart cities
10.1109/CloudCom.2016.0060,to_check,2016 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),IEEE,2016-12-15 00:00:00,ieeexplore,Animation Rendering on Multimedia Fog Computing Platforms,https://ieeexplore.ieee.org/document/7830701/,"Modern distributed multimedia applications are resource-hungry, and they often leverage on-demand cloud services to reduce their expenses. Existing cloud services deploy many servers in a few data centers, which consume a lot of electricity to power up and cold down, and thus are expensive and environmentally unfriendly. In this paper, we present a multimedia fog computing platform that utilizes resources from public crowds, edge networks, and data centers to serve distributed multimedia applications at lower costs. We use animation rendering as a case study, and identify several challenges for optimizing it on our multimedia fog computing platform. Among these challenges, we focus on the problem of predicting the completion time of each rendering job. We propose an efficient algorithm based on state-of-the-art machine learning algorithms. We also fine-tune the algorithm using multi-fold cross-validation for higher prediction accuracy. With real datasets, we conduct trace-driven simulations to quantify the performance of our prediction algorithm and that of the whole platform. The simulation results show that our proposed algorithm outperforms a state-of the-art statistical model in several aspects: completed job ratio by 20%, makespan by 2 times, and normalized deviation by 30 times, on average. Moreover, the overall performance of the platform with our proposed algorithm is fairly close to that with an Oracle of the actual job completion time: a small factor of 1.48 in terms of makespan is observed.",multimedia
10.1109/FPL.2010.22,to_check,2010 International Conference on Field Programmable Logic and Applications,IEEE,2010-09-02 00:00:00,ieeexplore,Real-Time Classification of Multimedia Traffic Using FPGA,https://ieeexplore.ieee.org/document/5694221/,"Real-time classification of Internet traffic according to application types is vital for network management and surveillance. Identifying emerging applications based on well-known port numbers is no longer reliable. While deep packet inspection (DPI) solutions can be accurate, they require constant updates of signatures and become infeasible for encrypted payload especially in multimedia applications (e.g. Skype). Statistical approaches based on machine learning have thus been considered more promising and robust to encryption, privacy, protocol obfuscation, etc. However, the computation complexity of traffic classification using those statistical solutions is high, which prevents them being deployed in systems that need to manage Internet traffic in real time. This paper proposes a FPGA-based parallel architecture to accelerate the statistical identification of multimedia applications while maintaining high classification accuracy. Specifically, we base our design on the k-Nearest Neighbors (k-NN) algorithm which has been shown to be one of the most accurate machine learning algorithms for Internet traffic classification. To enable high-rate data streaming for real-time classification, we adopt the locality sensitive hashing (LSH) for approximate k-NN. The LSH scheme is carefully designed to achieve high accuracy while being efficient for implementation on FPGA. Processing components in the architecture are optimized to realize high throughput. Extensive experiments and FPGA implementation results show that our design can achieve high accuracy above 99% for classifying three main categories of multimedia applications from Internet traffic while sustaining 80 Gbps throughput for minimum size (40 bytes) packets.",multimedia
10.1109/NOF.2017.8251223,to_check,2017 8th International Conference on the Network of the Future (NOF),IEEE,2017-11-24 00:00:00,ieeexplore,Prediction of active UE number with Bayesian neural networks for self-organizing LTE networks,https://ieeexplore.ieee.org/document/8251223/,"Internet-empowered electronic gadgets and content rich multimedia applications have expanded exponentially in recent years. As a consequence, heterogeneous network structures introduced with Long Term Evolution (LTE) Advanced have increasingly gaining momentum in order to handle with data explosion. On the other hand, the deployment of new network equipment is resulting in increasing both capital and operating expenditures. These deployments are done under the consideration of the busy hour periods which the network experiences the highest amount of traffic. However, these periods refer to only a couple of hours over a 24-hour period. In relation to this, accurate prediction of active user equipment (UE) number is significant for efficient network operations and results in decreasing energy consumption. In this paper, we investigate a Bayesian technique to design an optimal feed-forward neural network for shortterm predictor executed at the network management entity and providing proactivity to Energy Saving, a Self-Organizing Network function. We first demonstrate prediction results of active UE number collected from real LTE network. Then, we evaluate the prediction accuracy of the Bayesian neural network as comparing with low complex naive prediction method, Holt- Winter's exponential smoothing method, a deterministic feedforward neural network without Bayesian regularization term.",multimedia
10.1109/VR.2019.8798053,to_check,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),IEEE,2019-03-27 00:00:00,ieeexplore,MagicHand: Interact with IoT Devices in Augmented Reality Environment,https://ieeexplore.ieee.org/document/8798053/,"We present an Augmented Reality (AR) visualization and interaction tool for users to control Internet of Things (IoT) devices with hand gestures. Today, smart IoT devices are becoming increasingly ubiquitous with diverse forms and functions, yet most user controls over them are still limited to mobile devices and web interfaces. Recently, AR has been developed rapidly, and provided immersive solutions to enhance user experience of applications in many fields. Its capability to create immersive interactions allows AR to improve the way smart devices are controlled via more direct visual feedback. In this paper, we create a functional prototype of one such system, enabling seamless interactions with sound and lighting systems through the use of augmented hand-controlled interaction panels. To interpret users' intentions, we implement a standard 2D convolution neural network (CNN) for real-time hand gesture recognition and deploy it within our system. Our prototype is also equipped with a simple but effective object detector which can identify target devices within a proper range by analyzing geometric features. We evaluate the performance of our system qualitatively and quantitatively and demonstrate it on two smart devices.",multimedia
10.1109/IWCMC.2017.7986319,to_check,2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC),IEEE,2017-06-30 00:00:00,ieeexplore,In-vehicle cooperative driver information systems,https://ieeexplore.ieee.org/document/7986319/,"Critical traffic problems such as accidents and traffic congestion require the development of new transportation systems. Research in perceptual and human factors assessment is needed for relevant and correct display of this information for maximal road traffic safety as well as optimal driver comfort. One of the solutions to prevent accidents is to provide information on the surrounding environment of the driver. The development and deployment of cooperative vehicular safety systems undeniably require a combination of dedicated wireless communications, computer vision, and AR technologies as the building blocks of cooperative safety systems. Augmented Reality Head-Up Display (AR-HUD) can facilitate a new form of dialogue between the vehicle and the driver; and enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads. In this paper, we propose a fast deep-learning-based object detection approaches for identifying and recognizing road obstacles types, as well as interpreting and predicting complex traffic situations. A single Convolutional Neural Network (CNN) predicts region of interest and class probabilities directly from full images in one evaluation. We also investigated potential costs and benefits of using dynamic conformal AR cues in improving driving safety. A new AR-HUD approach to create real-time interactive traffic animations was introduced in terms of types of obstacle, rules for placement and visibility, and projection of these on an in-vehicle display.",multimedia
10.1109/AIAM50918.2020.00072,to_check,2020 2nd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),IEEE,2020-10-17 00:00:00,ieeexplore,Application of High Precision 3D Geological Modeling in Horizontal Well Development of Tight Gas Reservoir,https://ieeexplore.ieee.org/document/9426034/,"The southeastern part of Sulige gas field is a typical tight sandstone gas reservoir with frequent channel bifurcations and intersections. The contact mode of single sand body is mainly multi-layer contact formed by lateral accretion. It has been proved by practice that the production of single well can be increased by tackling key problems in horizontal wells, and the development with less wells and more production can be realized. In recent years, based on the geological knowledge base, the combination of well and seismic, and the increase of horizontal well data and other geological constraints as much as possible, the formation of multi-disciplinary integration of high-precision three-dimensional geological model, the realization of sand body internal configuration step-by-step description, fine anatomy of sand body spatial superposition relationship and distribution characteristics, fine guidance of horizontal well deployment and LWD guidance. Through lithofacies and attribute model, the distribution law of gas reservoir is determined; combined with the calculation model of geological reserves abundance, the advantageous position of reservoir development is optimized, and the factors such as well pattern and well type are integrated to deploy high-quality horizontal well location; based on the identification of high-precision sand body and effective sand body, the design trajectory of horizontal well is optimized to effectively reduce the large-scale up-down adjustment in the drilling process of horizontal section Through comprehensive analysis of the model, fine adjustment and real-time steering are carried out while drilling performance of horizontal wells, and the drilling encounter rate of horizontal wells is increased by 5% - 10%. The application of the new technology of 3D geological modeling can realize the well location deployment with quality and quantity basis, and improve drilling speed and quality and benefit. Under the background of poor reservoir quality, the development effect of horizontal well is ensured.",multimedia
10.1109/BigComp48618.2020.00-21,to_check,2020 IEEE International Conference on Big Data and Smart Computing (BigComp),IEEE,2020-02-22 00:00:00,ieeexplore,Benchmarking Jetson Platform for 3D Point-Cloud and Hyper-Spectral Image Classification,https://ieeexplore.ieee.org/document/9070378/,"Modern innovations of embedded system platforms (hardware accelerations) play a vital role in revolutionizing deep learning into practical scenarios, transforming human efforts into an automated intelligent system such as autonomous driving, robotics, IoT (Internet-of-Things) and many other useful applications. NVIDIA Jetson platform provides promising performance in terms of energy efficiency, favorable accuracy, and throughput for running deep learning algorithms. In this paper, we present benchmarking of Jetson platforms (Nano, TX1, and Xavier) by evaluating its performance based on computationally expensive deep learning algorithms. Previously, most of the benchmark results were based on 2-D images with conventional deep learning models for image processing. However, the implementation of many other complex data types at Jetson platform has remained a challenge. We also showed the practical impact of optimizing the algorithm vs improving the hardware accelerations by deploying a diverse range of dense and intensive deep learning architectures at all three aforementioned Jetson platforms, to make a better comparison of performance. In this regard, we have used two entirely different data-types, namely (i) ModelNet-40(Princeton-3D point-cloud) data-set along with PointNet deep learning architecture for classification of 3D point-cloud, and (ii) hyperspectral images (HSI) datasets (KSC and Pavia) alongside stacked autoencoders(SAE) to classify HSI correspondingly. This will broaden the scope of edge-devices to handle 3-D and HSI data whilst real-time classification will be processed at edge-server under the umbrella of edge-computing. The selection of (i) was made to exploit GPU heavily as the code uses TensorFlowgpu whereas (ii) was chosen to challenge the CPU cores of each platform as the code is based on Theano and may suffer from under-utilizing the GPU cores. We have presented the detailed evaluation exclusively in term of performance indices as inference time, the maximum number of concurrent processes, resource utilization per process and efficiency",multimedia
10.1109/PerCom.2014.6813937,to_check,2014 IEEE International Conference on Pervasive Computing and Communications (PerCom),IEEE,2014-03-28 00:00:00,ieeexplore,"Kintense: A robust, accurate, real-time and evolving system for detecting aggressive actions from streaming 3D skeleton data",https://ieeexplore.ieee.org/document/6813937/,"Kintense is a robust, accurate, real-time, and evolving system for detecting aggressive actions such as hitting, kicking, pushing, and throwing from streaming 3D skeleton joint coordinates obtained from Kinect sensors. Kintense uses a combination of: (1) an array of supervised learners to recognize a predefined set of aggressive actions, (2) an unsupervised learner to discover new aggressive actions or refine existing actions, and (3) human feedback to reduce false alarms and to label potential aggressive actions. This paper describes the design and implementation of Kintense and provides empirical evidence that the system is 11% - 16% more accurate and 10% - 54% more robust to changes in distance, body orientation, speed, and person when compared to standard techniques such as dynamic time warping (DTW) and posture based gesture recognizers. We deploy Kintense in two multi-person households and demonstrate how it evolves to discover and learn unseen actions, achieves up to 90% accuracy, runs in real-time, and reduces false alarms with up to 13 times fewer user interactions than a typical system.",multimedia
10.1109/ICME46284.2020.9102769,to_check,2020 IEEE International Conference on Multimedia and Expo (ICME),IEEE,2020-07-10 00:00:00,ieeexplore,Lightningnet: Fast and Accurate Semantic Segmentation for Autonomous Driving Based on 3D LIDAR Point Cloud,https://ieeexplore.ieee.org/document/9102769/,"Semantic segmentation is A series of earlier works have demonstrated that 3D LiDAR point cloud segmentation is a promising approach. However, these methods usually rely on pretrained models. And their requiring improvements in either speed or accuracy prevent them to be applied to mobile platforms. To address these problems, a smaller Convolutional Neural Network (CNN) architecture, namely, LightningNet is presented. Furthermore, we propose a lightweight pipeline with Feature Refinement Modules to boost the performance of real-time 3D LiDAR point cloud segmentation. Our model is trained on spherical images projected from LiDAR point clouds on KITTI [1] dataset. Experiments show accuracy improvements of 4.1-8.2% in small objects and 4% in terms of mIoU over the state-of-the-art of spherical-image based method with 111 fps on a single 1080Ti GPU and 14fps on a Jetson TX2, which enables deployment for real-time semantic segmentation in autonomous driving. We achieve superior performance on another large dataset called SemanticKITTI (illustrated in Figure 1) both in speed and accuracy also.",multimedia
10.1109/ACCESS.2019.2940997,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,One-Shot Learning Hand Gesture Recognition Based on Lightweight 3D Convolutional Neural Networks for Portable Applications on Mobile Systems,https://ieeexplore.ieee.org/document/8835909/,"Though deep convolutional neural networks (CNNs) have made great breakthroughs in the field of vision-based gesture recognition, however it is challenging to deploy these high-performance networks to resource-constrained mobile platforms and acquire large numbers of labeled samples for deep training of CNNs. Furthermore, there are some application scenarios with only a few samples or even a single one for a new gesture class so that the recognition method based on CNNs cannot achieve satisfactory classification performance. In this paper, a well-designed lightweight network based on I3D with spatial-temporal separable 3D convolutions and Fire module is proposed as an effective tool for the extraction of discriminative features. Then some effective capacity by deep training of large samples from related categories can be transferred and utilized to enhance the learning ability of the proposed network instead of training from scratch. In this way, the implementation of one-shot learning hand gesture recognition (OSLHGR) is carried out by a rational decision with distance measure. Moreover, a kind of mechanism of discrimination evolution with innovation of new sample and voting integration based on multi-classifiers is established to improve the learning and classification performance of the proposed method. Finally, a series of experiments and tests on the IsoGD and Jester datasets are conducted to demonstrate the effectiveness of our improved lightweight I3D. Meanwhile, a specific dataset of gestures with variant angles and directions, BSG 2.0, and the ChaLearn gesture dataset (CGD) are used for the test of OSLHGR. The results on different experiment platforms verify and validate the performance advantages of satisfied classification and real-time response speed.",multimedia
10.1109/IS.2012.6335201,to_check,2012 6th IEEE International Conference Intelligent Systems,IEEE,2012-09-08 00:00:00,ieeexplore,An experimental system for real-time interaction between humans and hybrid AI agents,https://ieeexplore.ieee.org/document/6335201/,"There is an emerging belief among AI researchers that intelligence is the product of specialised subsystems collaborating in a type of network of the mind. Several prototype systems have been developed as part of the World-Wide-Mind project [6] that facilitate the on-line deployment of hierarchically structured, multi-author, AI agents that we call minds. These agents function in on-line user defined environments that we call worlds. In this paper we describe the latest prototype system developed as part of the World-Wide-Mind project, the XAI (Experimental Artificial Intelligence) Server. Unlike its predecessors the XAI Server is a multi-agent system (MAS) allowing minds to run concurrently in the same on-line environment, facilitating inter-mind communication, cooperation and competition. The XAI Server also adds real-time interaction between humans and AI agents. Along with the potential for gauging the effectiveness of AI algorithms this raises issues relating to timing and knowledge representation that previous prototypes had no need to address. The XAI Server is loosely coupled with a scheduled threading model that makes it far more scalable than its predecessors. Capable of communicating with a standard Web browser the XAI Server moves the responsibility for content generation from the server to the client via a proprietary user interface language. We demonstrate how this system works with a sample ChatWorld and four example minds. Finally we discuss our future plans for enhancing the system to facilitate 2D and 3D world authoring.",multimedia
10.1109/CBMI50038.2021.9461921,to_check,2021 International Conference on Content-Based Multimedia Indexing (CBMI),IEEE,2021-06-30 00:00:00,ieeexplore,Crowd Violence Detection from Video Footage,https://ieeexplore.ieee.org/document/9461921/,"Surveillance systems currently deploy a variety of devices that can capture visual content (such as CCTV, body-worn cameras, and smartphone cameras), thus rendering the monitoring of video footage obtained from multiple such devices a complex task. This becomes especially challenging when monitoring social events that involve large crowds, particularly when there is a risk of crowd violence. This paper presents and demonstrates a crowd violence detection system that can process, analyze, and alert potential stakeholders, when violence-related content is identified in crowd-based video footage. Based on deep neural networks, the proposed end-to-end framework utilizes a 3D Convolutional Neural Network (CNN) to deal with the (near) real-time analysis of video streams and video files for crowd violence detection. The framework is trained, evaluated, and demonstrated using the Violent Flows dataset, a dataset related to crowd violence that is widely used for research. The presented framework is provided as a standalone application for desktop environments and can analyze both video streams and video files.",multimedia
10.1109/ICRA40945.2020.9197031,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,FADNet: A Fast and Accurate Network for Disparity Estimation,https://ieeexplore.ieee.org/document/9197031/,"Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy in stereo matching than traditional hand-crafted feature based methods. On one hand, however, the designed DNNs require significant memory and computation resources to accurately predict the disparity, especially for those 3D convolution based networks, which makes it difficult for deployment in real-time applications. On the other hand, existing computation-efficient networks lack expression capability in large-scale datasets so that they cannot make an accurate prediction in many scenarios. To this end, we propose an efficient and accurate deep network for disparity estimation named FADNet with three main features: 1) It exploits efficient 2D based correlation layers with stacked blocks to preserve fast computation; 2) It combines the residual structures to make the deeper model easier to learn; 3) It contains multi-scale predictions so as to exploit a multi-scale weight scheduling training technique to improve the accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on two popular datasets, Scene Flow and KITTI 2015. Experimental results show that FADNet achieves state-of-the-art prediction accuracy, and runs at a significant order of magnitude faster speed than existing 3D models. The codes of FADNet are available at https://github.com/HKBU-HPML/FADNet.",multimedia
10.1109/WoWMoM51794.2021.00053,to_check,"2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)",IEEE,2021-06-11 00:00:00,ieeexplore,"Similarity Measures for Location-Dependent MMIMO, 5G Base Stations On/Off Switching Using Radio Environment Map",https://ieeexplore.ieee.org/document/9469498/,"The Massive Multiple-Input Multiple-Output (MMIMO) technique together with Heterogeneous Network (Het-Net) deployment enables high throughput of 5G and beyond networks. However, a high number of antennas and a high number of Base Stations (BSs) can result in significant power consumption. Previous studies have shown that the energy efficiency (EE) of such a network can be effectively increased by turning off some BSs depending on User Equipments (UEs) positions. Such mapping is obtained by using Reinforcement Learning. Its results are stored in a so-called Radio Environment Map (REM). However, in a real network, the number of UEs' positions patterns would go to infinity. This paper aims to determine how to match the current set of UEs' positions to the most similar pattern, i.e., providing the same optimal active BSs set, saved in REM. We compare several state-of-the-art distance metrics using a computer simulator: an accurate 3D-Ray-Tracing model of the radio channel and an advanced system-level simulator of MMIMO Het-Net. The results have shown that the so-called Sum of Minimums Distance provides the best matching between REM data and UEs' positions, enabling up to 56% EE improvement over the scenario without EE optimization.",multimedia
10.1109/ICCV.2019.00718,to_check,2019 IEEE/CVF International Conference on Computer Vision (ICCV),IEEE,2019-11-02 00:00:00,ieeexplore,TSM: Temporal Shift Module for Efficient Video Understanding,https://ieeexplore.ieee.org/document/9008827/,"The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github. com/mit-han-lab/temporal-shift-module.",multimedia
10.1109/ACCESS.2021.3100708,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,ELEGANT: Security of Critical Infrastructures With Digital Twins,https://ieeexplore.ieee.org/document/9499077/,"The past years have witnessed an increasing interest and concern regarding the development of security monitoring and management mechanisms for Critical Infrastructures, due to their vital role in ensuring the availability of many essential services. This task is not easy due to the specific characteristics of such systems, and the natural resistance of Critical Infrastructures operators against actions implying downtime. Digital Twins, as accurate virtual models of physical objects or processes, can provide a faithful environment for security analysis or evaluation of potential mitigation strategies to be deployed in face of specific situations. Nonetheless, their on-premises deployment can be expensive, implying a significant CAPEX whose return will depend on the ability to plan and deploy a suitable support infrastructure, as well as implementing efficient and scalable data collection and processing mechanisms capable of taking advantage of the acquired resources. This paper presents an off-premises approach to design and deploy Digital Twins to secure critical infrastructures, developed in the scope of the ELEGANT project. Such Digital Twins are built using real-time, high fidelity replicas of Programming Logic Controllers, coupled with scalable and efficient data collection processes, supporting the development and validation of Machine Learning models to mitigate security threats like Denial of Service attacks. The validation approach of ELEGANT, which leveraged from the capabilities of the Fed4Fire federated testbeds evaluated the feasibility of using cloudified Digital Twins, thus converting a significant part of the projected CAPEX for the in-premises model into on-demand, pay-as-you-go OPEX, eventually paving the way for the establishment of a DTaaS (Digital Twin as a Service) paradigm. The achieved results demonstrate that the data pipelines providing support for the ELEGANT Digital Twins have low impact in terms of resource usage in Denial of Service and Distributed Denial of Service attack scenarios, when higher volumes of data are generated.",multimedia
10.1109/ICFEC51620.2021.00018,to_check,2021 IEEE 5th International Conference on Fog and Edge Computing (ICFEC),IEEE,2021-05-13 00:00:00,ieeexplore,A Privacy Preserving System for AI-assisted Video Analytics,https://ieeexplore.ieee.org/document/9458893/,"The emerging Edge computing paradigm facilitates the deployment of distributed AI-applications and hardware, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits for parties in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, the widespread deployment of such mechanism in public areas are a growing cause of privacy and security concerns. Data protection strategies need to be appropriately designed and correctly implemented in order to mitigate the associated risks. Most existing approaches focus on privacy and security related operations of the video stream itself or protecting its transmission. In this paper, we propose a privacy preserving system for AI-assisted video analytics, that extracts relevant information from video data and governs the secure access to that information. The system ensures that applications leveraging extracted data have no access to the video stream. An attribute-based authorization scheme allows applications to only query a predefined subset of extracted data. We demonstrate the feasibility of our approach by evaluating an application motivated by the recent COVID-19 pandemic, deployed on typical edge computing infrastructure.",multimedia
10.1109/BigData.2017.8258195,to_check,2017 IEEE International Conference on Big Data (Big Data),IEEE,2017-12-14 00:00:00,ieeexplore,A study of a video analysis framework using Kafka and spark streaming,https://ieeexplore.ieee.org/document/8258195/,"As the use of various sensors and cloud computing technologies has spread, many life-log analysis applications for safety services for the elderly and children have been developed. However, it is difficult to perform real-time large data processing in clouds due to the computational complexity of the analysis because efficient deployment schemes of streaming computing components over cloud resources have not been well-investigated. In this study, we propose a video analysis framework that collects videos from multiple cameras and analyzes them using Apache Kafka and Apache Spark Streaming. We first investigate the data transfer performance of Apache Kafka and examine efficient cluster configuration and parameter settings. We then apply this configuration to the proposed framework and measure the data analysis throughput. The experimental results show that the overall throughput varies depending on the number of broker nodes that store data, the number of topic partitions of data, and the number of nodes that conduct analysis processing. In addition, it is confirmed that the number of cores is needed to consider for the efficient cluster configuration, and that the network bandwidth between the nodes becomes a bottleneck as the amount of data and the number of components increase.",multimedia
10.1109/ICISC47916.2020.9171212,to_check,2020 Fourth International Conference on Inventive Systems and Control (ICISC),IEEE,2020-01-10 00:00:00,ieeexplore,Anomaly Detection in Videos for Video Surveillance Applications using Neural Networks,https://ieeexplore.ieee.org/document/9171212/,"Security is always a main concern in every domain, due to a rise in crime rate in the crowded event or suspicious lonely areas. Abnormal detection and monitoring have major applications of computer vision to tackle various problems. Due to growing demand in the protection of safety, security and personal properties, the needs and deployment of video surveillance systems can recognize and interpret the scene and anomaly events play a vital role in intelligence monitoring. Anomaly detection is a technique used to distinguish various patterns and identify unusual patterns with a minimal period, this pattern is called outliers. Surveillance videos can capture a variety of realistic anomalies. Anomaly detection in video surveillance involves breaking down the whole process into three layers, which are video labelers, image processing, and activity detection. Hence, anomaly detection in videos for video surveillance application gives assured results in regards to real-time scenarios. In this paper, we anomaly was detected in images and videos with an accuracy of 98.5 %.",multimedia
10.1109/WCNC.2019.8885803,to_check,2019 IEEE Wireless Communications and Networking Conference (WCNC),IEEE,2019-04-18 00:00:00,ieeexplore,Edge-assisted Adaptive Video Streaming with Deep Learning in Mobile Edge Networks,https://ieeexplore.ieee.org/document/8885803/,"Most HTTP Adaptive Streaming (HAS) video content delivery solutions today are governed by purely client-based logics. For lack of coordination among clients and awareness of the dynamic Radio Access Network (RAN) conditions, these approaches may lead to suboptimal user experience and underutilization of network resources. Recently, Mobile Edge Computing (MEC) has been studied as a new networking paradigm to play a part in the adaptive video streaming process with lower end-to-end latency and better network awareness. In this paper, we present an edge-assisted adaptive video streaming scheme with deep Q-learning techniques and bandwidth sharing policies, which performs video adaptation according to the real-time radio and network information collected at the network edge. The adaptation scheme is implemented as an edge application hosted on our own deployed MEC server in a real LTE network testbed for experimental evaluation against three popular client-based solutions, namely Buffer-Based Adaptation (BBA), Rate-Based Adaptation (RBA) and adaptation performed by dash.js with its default adaptation logic. To the best of our knowledge, it is one of the few deep Q-learning adaptive video streaming solutions that have been deployed in the MEC framework at network edge in practice. Experiment results show that our proposed scheme outperforms the other three client-based solutions with higher Quality of Experience (QoE) and fairness in the evaluated network environments.",multimedia
10.1145/3326285.3329051,to_check,2019 IEEE/ACM 27th International Symposium on Quality of Service (IWQoS),IEEE,2019-06-25 00:00:00,ieeexplore,LEAP: Learning-Based Smart Edge with Caching and Prefetching for Adaptive Video Streaming,https://ieeexplore.ieee.org/document/9068647/,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4% and reduces video rebuffering by 42.7%, which leads to at least 15.9% improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP.",multimedia
10.1109/ICRA.2019.8793915,to_check,2019 International Conference on Robotics and Automation (ICRA),IEEE,2019-05-24 00:00:00,ieeexplore,Learning to Capture a Film-Look Video with a Camera Drone,https://ieeexplore.ieee.org/document/8793915/,"The development of intelligent drones has simplified aerial filming and provided smarter assistant tools for users to capture a film-look footage. Existing methods of autonomous aerial filming either specify predefined camera movements for a drone to capture a footage, or employ heuristic approaches for camera motion planning. However, both predefined movements and heuristically planned motions are hardly able to provide cinematic footages for various dynamic scenarios. In this paper, we propose a data-driven learning-based approach, which can imitate a professional cameraman's intention for capturing a film-look aerial footage of a single subject in real-time. We model the decision-making process of the cameraman with two steps: 1) we train a network to predict the future image composition and camera position, and 2) our system then generates control commands to achieve the desired shot framing. At the system level, we deploy our algorithm on the limited resources of a drone and demonstrate the feasibility of running automatic filming onboard in real-time. Our experiments show how our data-driven planning approach achieves film-look footages and successfully mimics the work of a professional cameraman.",multimedia
10.1109/JSEN.2021.3054940,to_check,IEEE Sensors Journal,IEEE,2021-04-01 00:00:00,ieeexplore,Dual Camera-Based Supervised Foreground Detection for Low-End Video Surveillance Systems,https://ieeexplore.ieee.org/document/9336687/,"Deep learning-based algorithms showed promising prospects in the computer vision domain. However, their deployment in real-time systems is challenging due to their computational complexity, high-end hardware prerequisites, and the amount of annotated data for training. This paper proposes an efficient foreground detection (EFDNet) algorithm based on deep spatial features extracted from an RGB input image using VGG-16 convolutional neural networks (CNN). The VGG-16 CNN is modified by concatenated residual (CR) blocks to learn better global contextual features and recover lost feature information due to several convolution operations. Anew upsampling network is designed using bilinear interpolation sandwiched between 3 × 3 convolutions to upsample and refine feature maps for pixel-wise prediction. This helps to propagate loss errors from the upsampling network during backpropagation. The experiments showed the effectiveness of the EFDNet in outperforming top-ranked foreground detection algorithms. EFDNet trains faster on low-end hardware and demonstrated promising results with a minimum of 50 training frames with binary ground-truth.",multimedia
10.1109/TNSM.2019.2929511,to_check,IEEE Transactions on Network and Service Management,IEEE,2019-09-01 00:00:00,ieeexplore,iTeleScope: Softwarized Network Middle-Box for Real-Time Video Telemetry and Classification,https://ieeexplore.ieee.org/document/8765778/,"Video continues to dominate network traffic, yet operators today have poor visibility into the number, duration, and resolutions of the video streams traversing their domain. Current monitoring approaches are inaccurate, expensive, or unscalable, as they rely on statistical sampling, middle-box hardware, or packet inspection software. We present iTelescope, the first intelligent, inexpensive, and scalable softwarized network middle-box solution for identifying and classifying video flows in realtime. Our solution is novel in combining dynamic flow rules with telemetry and machine learning, and is built on commodity OpenFlow switches and open-source software. We develop a fully functional system, train it in the lab using multiple machine learning algorithms, and validate its performance to show over 95% accuracy in identifying and classifying video streams from many providers, including YouTube and Netflix. Lastly, we conduct tests to demonstrate its scalability to tens of thousands of concurrent streams, and deploy it live on a campus network serving several hundred real users. Our traffic monitoring system gives unprecedented fine-grained real-time visibility of video streaming performance to operators of enterprise and carrier networks at very low cost.",multimedia
10.1109/ICIoT48696.2020.9089557,to_check,"2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)",IEEE,2020-02-05 00:00:00,ieeexplore,A Scene-to-Speech Mobile based Application: Multiple Trained Models Approach,https://ieeexplore.ieee.org/document/9089557/,"The concept of Scene-to-Speech (STS) is to recognize elements in a captured image or a video clip to speak loudly an informative textual content that describes the scene. The contemporary progression in convolution neural network (CNN) allows us to attain object recognition procedures, in real-time, on mobile handled devices. Considerable number of applications has been developed to perform object recognition in scenes and say loudly their relevant descriptive messages. However, the employment of multiple trained deep learning (DL) models is not fully supported. In our previous work, a mobile application that can capture images and can recognize the objects contained in them was developed. It constructs descriptive sentences and speak them in Arabic and English languages. The notion of employing multi-trained DL models was used but no experimentation was conducted. In this article, we extend our previous work to perform required assessments while using multiple trained DL models. The main aim is to show that the deployment of multiple models approach can reduce the complexity of having one large compound model, and can enhance the prediction time. For this reason, we examine the prediction accuracy for single DL model-based recognition and multiple DL model-based recognition scenarios. The assessments results showed significant improvement in the prediction accuracy and in the prediction time. In the other hand, from the end user aspect, the application is designed primarily for visually impaired people to assist them in understanding their surroundings. In this context, we conduct a usability study to evaluate the usability of the proposed application with normal people and with visually impaired people. In fact, participants showed large interest in using the mobile application daily.",multimedia
10.1109/INDICON47234.2019.9029001,to_check,2019 IEEE 16th India Council International Conference (INDICON),IEEE,2019-12-15 00:00:00,ieeexplore,An End-to-End Real-Time Face Identification and Attendance System using Convolutional Neural Networks,https://ieeexplore.ieee.org/document/9029001/,"Carrying out the attendance process in any academic organization is a very significant task. However, the manual attendance process is very tedious and time- consuming. Hence, an automatic face attendance system using CCTV camera may be helpful by reducing the manpower and it also makes the attendance process faultless. There are some automated systems available commercially, but most of them deploy near frontal faces and processes them one by one, which is again a prolonged task. Some deep learning-based face attendance approaches have been proposed in the literature and improving the efficiency of the face attendance is still under research. In this paper, we propose an end-to-end face identification and attendance approach using Convolutional Neural Networks (CNN), which processes the CCTV footage or a video of the class and mark the attendance of the entire class in a single shot. One of the main advantages of the proposed solution is its robustness against usual challenges like occlusion (partially visible/covered faces), orientation, alignment and luminescence of the classroom. The proposed method obtained a real-time accuracy of 96.02% which is better than that of the existing end-to-end face attendance systems.",multimedia
10.1109/ICCES45898.2019.9002551,to_check,2019 International Conference on Communication and Electronics Systems (ICCES),IEEE,2019-07-19 00:00:00,ieeexplore,Comparative Analysis of Texture Features and Deep Learning Method for Real-time Indoor Object Recognition,https://ieeexplore.ieee.org/document/9002551/,"Object recognition and classification are considered as major tasks in the field of computer vision. They are well suited for applications such as a real-time system for people counting, object recognition system for people with visual impairments, surveillance systems, etc. The deployment of computer vision, machine learning, and deep learning algorithms enable to recognize the objects from an image or video frame. This paper proposes a real-time system for indoor object recognition. Moreover, the proposed work mainly focuses on analyzing the performance of various texture features, machine learning classifiers and deep learning methodologies to recognize the objects in indoor areas. The proposed methodology is validated in a publically available indoor object dataset “MCindoor20000”. The dataset consists of three categories of objects including doors, stairs, and sign. Our developed deep learning model using transfer learning approach yielded 100 % accuracy and texture features such as LPQ and BSIF have yielded an accuracy of more than 98% with SVM and KNN classifiers.",multimedia
10.1109/CVPR.2019.00346,to_check,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2019-06-20 00:00:00,ieeexplore,DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion,https://ieeexplore.ieee.org/document/8953386/,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",multimedia
10.1109/CloudNet.2018.8549558,to_check,2018 IEEE 7th International Conference on Cloud Networking (CloudNet),IEEE,2018-10-24 00:00:00,ieeexplore,Hi-Clust: Unsupervised Analysis of Cloud Latency Measurements Through Hierarchical Clustering,https://ieeexplore.ieee.org/document/8549558/,"Latency is nowadays one of the most relevant network and service performance metrics reflecting end-user experience. With the wide adoption and deployment of delay-sensitive applications in the Cloud (e.g., gaming, interactive video conferencing, corporate services, etc.), monitoring and analysis of Cloud service latency is becoming increasingly relevant for Cloud service providers, tenants and even users. Traditional network monitoring approaches based on time-series analysis and thresholding are capable of raising alarms when anomalous events arise, but are not applicable to detect correlations among multiple monitored dimensions, necessary to provide an adequate interpretation of an anomaly. In this paper we present Hi-Clust, an unsupervised-based approach for analyzing and interpreting anomalies in multi-dimensional network data, through the application of hierarchical clustering techniques. While Hi-Clust is applicable to the analysis of different types of nested or hierarchically structured data, we particularly focus on the analysis of Cloud service latency, using active measurements collected from geographically distributed vantage points. We implement and benchmark multiple density-based clustering approaches for Hi-Clust over four weeks of real multidimensional Cloud service latency measurements. Using the most robust underlying clustering algorithm from the benchmark, we show how to automatically extract and interpret anomalous Cloud service behavior with Hi-Clust. In addition, we show the advantages of Hi-Clust over traditional threshold-based approaches for detecting and interpreting anomalous behavior, through practical examples over the collected measurements.",multimedia
10.1109/CVPR.2019.00437,to_check,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2019-06-20 00:00:00,ieeexplore,Learning to Film From Professional Human Motion Videos,https://ieeexplore.ieee.org/document/8953663/,"We investigate the problem of 6 degrees of freedom (DOF) camera planning for filming professional human motion videos using a camera drone. Existing methods either plan motions for only a pan-tilt-zoom (PTZ) camera, or adopt ad-hoc solutions without carefully considering the impact of video contents and previous camera motions on the future camera motions. As a result, they can hardly achieve satisfactory results in our drone cinematography task. In this study, we propose a learning-based framework which incorporates the video contents and previous camera motions to predict the future camera motions that enable the capture of professional videos. Specifically, the inputs of our framework are video contents which are represented using subject-related feature based on 2D skeleton and scene-related features extracted from background RGB images, and camera motions which are represented using optical flows. The correlation between the inputs and output future camera motions are learned via a sequence-to-sequence convolutional long short-term memory (Seq2Seq ConvLSTM) network from a large set of video clips. We deploy our approach to a real drone cinematography system by first predicting the future camera motions, and then converting them to the drone's control commands via an odometer. Our experimental results on extensive datasets and showcases exhibit significant improvements in our approach over conventional baselines and our approach can successfully mimic the footage of a professional cameraman.",multimedia
10.1109/ICNC.2011.6022169,to_check,2011 Seventh International Conference on Natural Computation,IEEE,2011-07-28 00:00:00,ieeexplore,Near range pedestrian collision detection using bio-inspired visual neural networks,https://ieeexplore.ieee.org/document/6022169/,"New vehicular safety standards require the development of pedestrian collision detection systems that can trigger the deployment of active impact alleviation measures from the vehicle prior to a collision. In this paper, we propose a new vision-based system for near-range pedestrian collision detection. The low-level system uses a bio-inspired visual neural network, which emulates the visual system of the locust, to detect visual cues relevant to objects in front of a moving car. At a higher level, the system employs a neural-network classifier to identify dangerous pedestrian positions, triggering an alarm signal. The system was tuned via simulation and tested using recorded video sequences of real vehicle impacts. The experiment results demonstrate that the system is able to discriminate between pedestrians in dangerous and safe positions, triggering alarms accordingly.",multimedia
10.23919/APNOMS50412.2020.9237059,to_check,2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS),IEEE,2020-09-25 00:00:00,ieeexplore,Prequalification of VDSL2 copper customers for G.fast services with artificial intelligence technology,https://ieeexplore.ieee.org/document/9237059/,"In recent years, due to customers have higher requirements for 4K/BK video and high-speed internet, telecom operators have begun to deploy FTTH network, but found that it is generally difficult to deploy fiber to the home, so G.fast technology has been favored by most telecom operators around the world and have begun to actively deploy. For the most widely deploy VDSL2 line with maximum rate that can only provide 100M internet service, a intelligent and accurate G.fast 300M high speed service prequalification technology, is a major research topic for telecom operators to promote 300M high-speed internet service. This paper proposes to use AI machine learning to estimate the G.fast line rate by using VDSL2 line attenuation to meet the real-site provision needs of telecommunications operators.",multimedia
10.1109/INFCOMW.2016.7562053,to_check,2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),IEEE,2016-04-14 00:00:00,ieeexplore,Resource provisioning and profit maximization for transcoding in Information Centric Networking,https://ieeexplore.ieee.org/document/7562053/,"Adaptive bitrate streaming (ABR) has been widely adopted to support video streaming services over heterogeneous devices and varying network conditions. With ABR, each video content is transcoded into multiple representations in different bitrates and resolutions. However, video transcoding is computing intensive, which requires the transcoding service providers to deploy a large number of servers for transcoding the video contents published by the content producers. As such, a natural question for the transcoding service provider is how to provision the computing resource for transcoding the video contents while maximizing service profit. To address this problem, we design a cloud video transcoding system by taking the advantage of cloud computing technology to elastically allocate computing resource. We propose a method for jointly considering the task scheduling and resource provisioning problem in two timescales, and formulate the service profit maximization as a two-timescale stochastic optimization problem. We derive some approximate policies for the task scheduling and resource provisioning. Based on our proposed methods, we implement our open source cloud video transcoding system Morph and evaluate its performance in a real environment. The experiment results demonstrate that our proposed method can reduce the resource consumption and achieve a higher profit compared with the baseline schemes.",multimedia
10.1109/INFOCOM41043.2020.9155467,to_check,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,IEEE,2020-07-09 00:00:00,ieeexplore,Rldish: Edge-Assisted QoE Optimization of HTTP Live Streaming with Reinforcement Learning,https://ieeexplore.ieee.org/document/9155467/,"Recent years have seen a rapidly increasing traffic demand for HTTP-based high-quality live video streaming. The surging traffic demand, as well as the real-time property of live videos, make it challenging for content delivery networks (CDNs) to guarantee the Quality-of-Experiences (QoE) of viewers. The initial video segment (IVS) of live streaming plays an important role in the QoE of live viewers, particularly when users require fast join time and smooth view experience. State-of-the-art research on this regard estimates network throughput for each viewer and thus may incur a large overhead that offsets the benefit. To tackle the problem, we propose Rldish, a scheme deployed at the edge CDN server, to dynamically select a suitable IVS for new live viewers based on Reinforcement Learning (RL). Rldish is transparent to both the client and the streaming server. It collects the real-time QoE observations from the edge without any client-side assistance, then uses these QoE observations as real-time rewards in RL. We deploy Rldish as a virtualized network function (VNF) in a real HTTP cache server, and evaluate its performance using streaming servers distributed over the world. Our experiments show that Rldish improves the state- of-the-art IVS selection scheme w.r.t. the average QoE of live viewers by up to 22%.",multimedia
10.1109/PCCC.2018.8710767,to_check,2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC),IEEE,2018-11-19 00:00:00,ieeexplore,The Frame Latency of Personalized Livestreaming Can Be Significantly Slowed Down by WiFi,https://ieeexplore.ieee.org/document/8710767/,"The popular personalized livestreaming (PL) in China, arguably the largest PL market in the world, is more monetized than PL in US and hence demands much lower interactive latencies to ensure a good quality of user experience. However, our pilot experiment shows that the video frame latency, dominant component of PL's interactive latency, can be significantly slowed down by WiFi, the primary Internet access method for PL. Understanding and further improving the frame latency over WiFi, however, have difficulties in 1) measuring end-to-end latency; 2) parsing encrypted PL's traffic and 3) modeling complex relationships between WiFi radio factors and the latency. To tackle these challenges, we design and prototype Latency Doctor (LTDr), a practical system which aims to model and optimize PL's video frame latency over WiFi. We deploy LTDr in our campus and obtain several key observations based on 13.9M video frames extracted from 12K individual views on three leading PLs in China. We observe that 40% frame latencies over WiFi hop are more than 30ms, and channel utilization should be less than 64% for low latency. Then we build a predictive model based on the dataset using the machine learning methodologies. Two real cases show that the median frame latencies are decreased by LTDr from 130ms to 22ms, and 50ms to 12ms respectively over WiFi networks.",multimedia
10.1109/TNSM.2021.3085097,to_check,IEEE Transactions on Network and Service Management,IEEE,2021-09-01 00:00:00,ieeexplore,SETA++: Real-Time Scalable Encrypted Traffic Analytics in Multi-Gbps Networks,https://ieeexplore.ieee.org/document/9444314/,"The security and privacy of the end-users are a few of the most important components of a communication network. Though end-to-end encryption (e.g., TLS/SSL) fulfils this requirement, it makes inspecting network traffic with legacy solutions such as Deep Packet Inspection difficult. Recent Machine Learning techniques have shown outstanding performance in encrypted traffic classification. Nevertheless, such approaches require efficient flow sampling at real enterprise-scale networks due to the sheer volume of transferred data. Through this paper, we propose a holistic architecture to extract flow information of encrypted data at multi Gbps line rate using sampling and sketching mechanisms, enabling network operators to estimate flow size distribution accurately and understand the behavior of VPN-obfuscated traffic. Using over 6000 video traffic traces, under three main evaluation scenarios based on trace duration and starting time point, we show that it is possible to achieve 99% accuracy for service provider classification and over 90% accuracy for content classification for a given service provider in the best case. We also deploy our solution at an operational enterprise-scale network leveraging kernel bypassing to demonstrate its capability to efficiently sample live traffic for analytics.",multimedia
10.1109/TCYB.2019.2912205,to_check,IEEE Transactions on Cybernetics,IEEE,2021-04-01 00:00:00,ieeexplore,Memristive Quantized Neural Networks: A Novel Approach to Accelerate Deep Learning On-Chip,https://ieeexplore.ieee.org/document/8705375/,"Existing deep neural networks (DNNs) are computationally expensive and memory intensive, which hinder their further deployment in novel nanoscale devices and applications with lower memory resources or strict latency requirements. In this paper, a novel approach to accelerate on-chip learning systems using memristive quantized neural networks (M-QNNs) is presented. A real problem of multilevel memristive synaptic weights due to device-to-device (D2D) and cycle-to-cycle (C2C) variations is considered. Different levels of Gaussian noise are added to the memristive model during each adjustment. Another method of using memristors with binary states to build M-QNNs is presented, which suffers from fewer D2D and C2C variations compared with using multilevel memristors. Furthermore, methods of solving the sneak path issues in the memristive crossbar arrays are proposed. The M-QNN approach is evaluated on two image classification datasets, that is, ten-digit number and handwritten images of mixed National Institute of Standards and Technology (MNIST). In addition, input images with different levels of zero-mean Gaussian noise are tested to verify the robustness of the proposed method. Another highlight of the proposed method is that it can significantly reduce computational time and memory during the process of image recognition.",multimedia
10.1109/ICRA48506.2021.9561941,to_check,2021 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2021-06-05 00:00:00,ieeexplore,A Robot Walks into a Bar: Automatic Robot Joke Success Assessment,https://ieeexplore.ieee.org/document/9561941/,"Effective social robots should leverage humor’s unique ability to improve relationship connections and dispel stress, but current robots possess limited (if any) humorous abilities. In this paper, we aim to supplement one aspect of autonomous robots by giving robotic systems the ability to ""read the room"" to assess how their humorous statements are received by nearby people in real time. Using a dataset of the audio of crowd responses to a robotic comedian over multiple performances (first presented in past work), we establish human-labeled joke success ground truths and compare individual human rater accuracy against the outputs of lightweight Machine Learning (ML) approaches that are easy to deploy in real-time joke assessment. Our results indicate that all three ML approaches (naïve Bayes, support vector machines, and single-hidden-layer feedforward neural networks) performed significantly better than the baseline approach used in our past work. In particular, support vector machines and neural network approaches are comparable to a human rater in the task of assessing if a joke failed or not in certain cases. The products of this work will inform self-assessment techniques for robots and help social robotics researchers test their own assessment methods on realistic data from human crowds.",multimedia
10.1109/ICASSP.2018.8462273,to_check,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2018-04-20 00:00:00,ieeexplore,Improving Accuracy of Nonparametric Transfer Learning Via Vector Segmentation,https://ieeexplore.ieee.org/document/8462273/,"Transfer learning using deep neural networks as feature extractors has become increasingly popular over the past few years. It allows to obtain state-of-the-art accuracy on datasets too small to train a deep neural network on its own, and it provides cutting edge descriptors that, combined with nonparametric learning methods, allow rapid and flexible deployment of performing solutions in computationally restricted settings. In this paper, we are interested in showing that the features extracted using deep neural networks have specific properties which can be used to improve accuracy of downstream nonparametric learning methods. Namely, we demonstrate that for some distributions where information is embedded in a few coordinates, segmenting feature vectors can lead to better accuracy. We show how this model can be applied to real datasets by performing experiments using three mainstream deep neural network feature extractors and four databases, in vision and audio.",multimedia
10.1109/UEMCON.2018.8796576,to_check,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",IEEE,2018-11-10 00:00:00,ieeexplore,Non-Intrusive Activity Detection and Prediction in Smart Residential Spaces,https://ieeexplore.ieee.org/document/8796576/,"Non-intrusive human activity detection and prediction is an important and challenging problem in smart and pervasive spaces. The advantages of such a design are the reduced dependency on the users and fewer security/privacy concerns. However, these also make it difficult to effectively and accurately understand the activities in real-time. In residential spaces, this can be even a bigger challenge due to nonuniform space boundaries and multiple people sharing the space. In this paper, we present a system, that consists of both hardware and software components, capable of detecting and predicting human activities in a smart residential environment. Our system deploys a finite state machine-based activity detection with 96% accuracy in real-time. Afterwards, we use several machine learning methods to create an effective activity prediction framework. We demonstrate that we can achieve up to 98.5% activity prediction accuracy with 4ms delay, making it a perfect real-time system example. Since our smart and pervasive space implementation does not use any intrusive sensor or data acquisition unit (such as wearables, camera, or audio sources), we reduce the dependency to the user and potential security/privacy issues.",multimedia
10.1109/HPCA.2019.00028,to_check,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),IEEE,2019-02-20 00:00:00,ieeexplore,E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs,https://ieeexplore.ieee.org/document/8675229/,"Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related applications which require efficient and real-time implementations. The two major types are Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. It is a challenging task to have real-time, efficient, and accurate hardware RNN implementations because of the high sensitivity to imprecision accumulation and the requirement of special activation function implementations. Recently two works have focused on FPGA implementation of inference phase of LSTM RNNs with model compression. First, ESE uses a weight pruning based compressed RNN model but suffers from irregular network structure after pruning. The second work C-LSTM mitigates the irregular network limitation by incorporating block-circulant matrices for weight matrix representation in RNNs, thereby achieving simultaneous model compression and acceleration. A key limitation of the prior works is the lack of a systematic design optimization framework of RNN model and hardware implementations, especially when the block size (or compression ratio) should be jointly optimized with RNN type, layer size, etc. In this paper, we adopt the block-circulant matrixbased framework, and present the Efficient RNN (E-RNN) framework for FPGA implementations of the Automatic Speech Recognition (ASR) application. The overall goal is to improve performance/energy efficiency under accuracy requirement. We use the alternating direction method of multipliers (ADMM) technique for more accurate block-circulant training, and present two design explorations providing guidance on block size and reducing RNN training trials. Based on the two observations, we decompose E-RNN in two phases: Phase I on determining RNN model to reduce computation and storage subject to accuracy requirement, and Phase II on hardware implementations given RNN model, including processing element design/optimization, quantization, activation implementation, etc. 1 Experimental results on actual FPGA deployments show that E-RNN achieves a maximum energy efficiency improvement of 37.4× compared with ESE, and more than 2× compared with C-LSTM, under the same accuracy.",multimedia
10.1109/ASAP52443.2021.00022,to_check,"2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)",IEEE,2021-07-09 00:00:00,ieeexplore,How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures,https://ieeexplore.ieee.org/document/9516652/,"The unprecedented performance of deep neural networks (DNNs) has led to large strides in various Artificial Intelligence (AI) inference tasks, such as object and speech recognition. Nevertheless, deploying such AI models across commodity devices faces significant challenges: large computational cost, multiple performance objectives, hardware heterogeneity and a common need for high accuracy, together pose critical problems to the deployment of DNNs across the various embedded and mobile devices in the wild. As such, we have yet to witness the mainstream usage of state-of-the-art deep learning algorithms across consumer devices. In this paper, we provide preliminary answers to this potentially game-changing question by presenting an array of design techniques for efficient AI systems. We start by examining the major roadblocks when targeting both programmable processors and custom accelerators. Then, we present diverse methods for achieving real-time performance following a cross-stack approach. These span model-, system- and hardware-level techniques, and their combination. Our findings provide illustrative examples of AI systems that do not overburden mobile hardware, while also indicating how they can improve inference accuracy. Moreover, we showcase how custom ASIC- and FPGA-based accelerators can be an enabling factor for next-generation AI applications, such as multi-DNN systems. Collectively, these results highlight the critical need for further exploration as to how the various cross-stack solutions can be best combined in order to bring the latest advances in deep learning close to users, in a robust and efficient manner.",multimedia
10.1109/ICRAIE51050.2020.9358310,to_check,2020 5th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE),IEEE,2020-12-03 00:00:00,ieeexplore,Development of a Neural Network Library for Resource Constrained Speech Synthesis,https://ieeexplore.ieee.org/document/9358310/,"Machine learning frameworks, like Tensorflow and PyTorch, use GPU hardware acceleration to deliver the needed performance. Since GPUs require a lot of power (and space) to operate, typical use cases involve high-performance servers, with the final deployment available as a cloud service. To address limitations of this approach, AI Accelerators have been proposed. In this context, we have designed and implemented a library of neural network algorithms, to efficiently run on “edge devices”, with AI Accelerators. Moreover, a unified interface has been provided, to allow easy experimentation with various neural networks applied to the same dataset. Here, let us stress that we do not propose new algorithms, but port known ones to, resource restricted, edge devices. The context is provided by a speech synthesis application for edge devices that is deployed on an NVIDIA Jetson Nano. This application is to be used by social robots for real-time off-cloud text-to-speech processing.",multimedia
10.1109/PERCOMW.2016.7457072,to_check,2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops),IEEE,2016-03-18 00:00:00,ieeexplore,Demonstrating HighFiveLive: A mobile application for recognizing symbolic gestures,https://ieeexplore.ieee.org/document/7457072/,"Gestures are an important part of our social fabric, aiding our speech (and sometimes standing in for it) in face-to-face communications. In this demonstration, we present HighFiveLive, a smartwatch application that uses data from an onboard accelerometer to detect and classify fine-grained symbolic gestures, which convey semantic meaning and have social significance. We apply logistic regression to learn a representative model of 9 symbolic gestures as performed by 32 users; in an offline evaluation study, the model accurately classifies 92% of recognized gestures. Building on this work, we deploy the learned model in our HighFiveLive mobile application, which detects and classifies symbolic gestures in real-time as they are performed. HighFiveLive is implemented as an Android application with connection to a wrist-worn accelerometer stream (such as a Microsoft Band) as well as a standalone Apple Watch application.",multimedia
10.1007/978-3-030-87202-1_64,to_check,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,Springer,2021-01-01 00:00:00,springer,SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based Augmented Reality for Surgical Guidance,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87202-1_64,"We present SurgeonAssist-Net: a lightweight framework making action-and-workflow-driven virtual assistance, for a set of predefined surgical tasks, accessible to commercially available optical see-through head-mounted displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic surgical workflow, our implementation competes with state-of-the-art approaches in prediction accuracy for automated task recognition, and yet requires $$7.4\times $$ 7.4 × fewer parameters, $$10.2\times $$ 10.2 × fewer floating point operations per second (FLOPS), is $$7.0\times $$ 7.0 × faster for inference on a CPU, and is capable of near real-time performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use of an efficient convolutional neural network (CNN) backbone to extract discriminative features from image data, and a low-parameter recurrent neural network (RNN) architecture to learn long-term temporal dependencies. To demonstrate the feasibility of our approach for inference on the HoloLens 2 we created a sample dataset that included video of several surgical tasks recorded from a user-centric point-of-view. After training, we deployed our model and cataloged its performance in an online simulated surgical scenario for the prediction of the current surgical task. The utility of our approach is explored in the discussion of several relevant clinical use-cases. Our code is publicly available at https://github.com/doughtmw/surgeon-assist-net .",multimedia
10.1007/978-3-030-80285-1_50,to_check,Advances in Neuroergonomics and Cognitive Engineering,Springer,2021-01-01 00:00:00,springer,Advanced Cyber and Physical Situation Awareness in Urban Smart Spaces,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80285-1_50,"The ever-growing adoption of big data technologies, smart sensing, data science and artificial intelligence is enabling the development of new intelligent urban spaces with real-time monitoring and advanced cyber-physical situational awareness capabilities. In the S4AllCities international research project, the advancement of cyber-physical situational awareness will be experimented for achieving safer smart city spaces in Europe and beyond. The deployment of digital twins will lead to understanding real-time situation awareness and risks of potential physical and/or cyber-attacks on urban critical infrastructure specifically. The critical extraction of knowledge using digital twins, which ingest, process and fuse observation data and information, prior to machine reasoning is performed in S4AllCities. In this paper, a cyber behavior detection module, which identifies unusualness in cyber traffic networks is described. Also, a physical behaviour detection module is introduced. The two modules function within the so-called Malicious Attacks Information Detection System (MAIDS) digital twin.",multimedia
10.1007/s11265-020-01562-x,to_check,Journal of Signal Processing Systems,Springer,2020-11-01 00:00:00,springer,eBrainII: a 3 kW Realtime Custom 3D DRAM Integrated ASIC Implementation of a Biologically Plausible Model of a Human Scale Cortex,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-020-01562-x,"The Artificial Neural Networks (ANNs), like CNN/DNN and LSTM, are not biologically plausible. Despite their initial success, they cannot attain the cognitive capabilities enabled by the dynamic hierarchical associative memory systems of biological brains. The biologically plausible spiking brain models, e.g., cortex, basal ganglia, and amygdala, have a greater potential to achieve biological brain like cognitive capabilities. Bayesian Confidence Propagation Neural Network (BCPNN) is a biologically plausible spiking model of the cortex. A human-scale model of BCPNN in real-time requires 162 TFlop/s, 50 TBs of synaptic weight storage to be accessed with a bandwidth of 200 TBs. The spiking bandwidth is relatively modest at 250 GBs/s. A hand-optimized implementation of rodent scale BCPNN has been done on Tesla K80 GPUs require 3 kWs, we extrapolate from that a human scale network will require 3 MWs. These power numbers rule out such implementations for field deployment as cognition engines in embedded systems. The key innovation that this paper reports is that it is feasible and affordable to implement real-time BCPNN as a custom tiled application-specific integrated circuit (ASIC) in 28 nm technology with custom 3D DRAM - eBrainII - that consumes 3 kW for human scale and 12 watts for rodent scale. Such implementations eminently fulfill the demands for field deployment.",multimedia
http://arxiv.org/abs/1907.06968v1,to_check,arxiv,arxiv,2019-07-16 12:50:42+00:00,arxiv,"A Unified Deep Framework for Joint 3D Pose Estimation and Action
  Recognition from a Single RGB Camera",http://arxiv.org/abs/1907.06968v1,"We present a deep learning-based multitask framework for joint 3D human pose
estimation and action recognition from RGB video sequences. Our approach
proceeds along two stages. In the first, we run a real-time 2D pose detector to
determine the precise pixel location of important keypoints of the body. A
two-stream neural network is then designed and trained to map detected 2D
keypoints into 3D poses. In the second, we deploy the Efficient Neural
Architecture Search (ENAS) algorithm to find an optimal network architecture
that is used for modeling the spatio-temporal evolution of the estimated 3D
poses via an image-based intermediate representation and performing action
recognition. Experiments on Human3.6M, MSR Action3D and SBU Kinect Interaction
datasets verify the effectiveness of the proposed method on the targeted tasks.
Moreover, we show that our method requires a low computational budget for
training and inference.",multimedia
http://arxiv.org/abs/2104.05828v1,to_check,arxiv,arxiv,2021-04-12 21:30:53+00:00,arxiv,"Evidence-based Prescriptive Analytics, CAUSAL Digital Twin and a
  Learning Estimation Algorithm",http://arxiv.org/abs/2104.05828v1,"Evidence-based Prescriptive Analytics (EbPA) is necessary to determine
optimal operational set-points that will improve business productivity. EbPA
results from what-if analysis and counterfactual experimentation on CAUSAL
Digital Twins (CDTs) that quantify cause-effect relationships in the DYNAMICS
of a system of connected assets. We describe the basics of Causality and Causal
Graphs and develop a Learning Causal Digital Twin (LCDT) solution; our
algorithm uses a simple recurrent neural network with some innovative
modifications incorporating Causal Graph simulation. Since LCDT is a learning
digital twin where parameters are learned online in real-time with minimal
pre-configuration, the work of deploying digital twins will be significantly
simplified. A proof-of-principle of LCDT was conducted using real vibration
data from a system of bearings; results of causal factor estimation, what-if
analysis study and counterfactual experiment are very encouraging.",multimedia
http://arxiv.org/abs/1812.07106v1,to_check,arxiv,arxiv,2018-12-12 22:22:16+00:00,arxiv,"E-RNN: Design Optimization for Efficient Recurrent Neural Networks in
  FPGAs",http://arxiv.org/abs/1812.07106v1,"Recurrent Neural Networks (RNNs) are becoming increasingly important for time
series-related applications which require efficient and real-time
implementations. The two major types are Long Short-Term Memory (LSTM) and
Gated Recurrent Unit (GRU) networks. It is a challenging task to have
real-time, efficient, and accurate hardware RNN implementations because of the
high sensitivity to imprecision accumulation and the requirement of special
activation function implementations.
  A key limitation of the prior works is the lack of a systematic design
optimization framework of RNN model and hardware implementations, especially
when the block size (or compression ratio) should be jointly optimized with RNN
type, layer size, etc. In this paper, we adopt the block-circulant matrix-based
framework, and present the Efficient RNN (E-RNN) framework for FPGA
implementations of the Automatic Speech Recognition (ASR) application. The
overall goal is to improve performance/energy efficiency under accuracy
requirement. We use the alternating direction method of multipliers (ADMM)
technique for more accurate block-circulant training, and present two design
explorations providing guidance on block size and reducing RNN training trials.
Based on the two observations, we decompose E-RNN in two phases: Phase I on
determining RNN model to reduce computation and storage subject to accuracy
requirement, and Phase II on hardware implementations given RNN model,
including processing element design/optimization, quantization, activation
implementation, etc. Experimental results on actual FPGA deployments show that
E-RNN achieves a maximum energy efficiency improvement of 37.4$\times$ compared
with ESE, and more than 2$\times$ compared with C-LSTM, under the same
accuracy.",multimedia
http://arxiv.org/abs/2111.08973v1,to_check,arxiv,arxiv,2021-11-17 08:30:18+00:00,arxiv,Generating Unrestricted 3D Adversarial Point Clouds,http://arxiv.org/abs/2111.08973v1,"Utilizing 3D point cloud data has become an urgent need for the deployment of
artificial intelligence in many areas like facial recognition and self-driving.
However, deep learning for 3D point clouds is still vulnerable to adversarial
attacks, e.g., iterative attacks, point transformation attacks, and generative
attacks. These attacks need to restrict perturbations of adversarial examples
within a strict bound, leading to the unrealistic adversarial 3D point clouds.
In this paper, we propose an Adversarial Graph-Convolutional Generative
Adversarial Network (AdvGCGAN) to generate visually realistic adversarial 3D
point clouds from scratch. Specifically, we use a graph convolutional generator
and a discriminator with an auxiliary classifier to generate realistic point
clouds, which learn the latent distribution from the real 3D data. The
unrestricted adversarial attack loss is incorporated in the special adversarial
training of GAN, which enables the generator to generate the adversarial
examples to spoof the target network. Compared with the existing state-of-art
attack methods, the experiment results demonstrate the effectiveness of our
unrestricted adversarial attack methods with a higher attack success rate and
visual quality. Additionally, the proposed AdvGCGAN can achieve better
performance against defense models and better transferability than existing
attack methods with strong camouflage.",multimedia
http://arxiv.org/abs/2104.02306v1,to_check,arxiv,arxiv,2021-04-06 06:04:57+00:00,arxiv,Binary Neural Network for Speaker Verification,http://arxiv.org/abs/2104.02306v1,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",multimedia
http://arxiv.org/abs/2105.03668v2,to_check,arxiv,arxiv,2021-05-08 10:42:28+00:00,arxiv,"Real-time prediction of probabilistic crack growth with a reduced-order
  digital twin of a helicopter component",http://arxiv.org/abs/2105.03668v2,"To deploy the airframe Digital Twin or to conduct probabilistic evaluations
of the remaining life of a structural component, a (near) real-time crack
growth simulation method is critical. In this paper, a reduced-order simulation
approach is developed to achieve this goal by leveraging two methods. On one
hand, the SGBEM super element - FEM coupling method is combined with parametric
modeling to generate the database of computed Stress Intensity Factors for
cracks with various sizes/shapes in a complex structural component, by which
hundreds of samples are automatically simulated within a day. On the other
hand, machine learning methods are applied to establish the relation between
crack sizes/shapes and crack front SIFs. By combining the reduced-order
computational model with load inputs and fatigue growth laws, a real time
prediction of probabilistic crack growth in complex structures with minimum
computational burden is realized. In an example of a round-robin helicopter
component, even though the fatigue crack growth is simulated cycle by cycle,
the simulation is faster than real-time (as compared to the physical test). The
proposed approach is a key simulation technology towards realizing the Digital
Twin of complex structures, which further requires fusion of model predictions
with flight/inspection/monitoring data.",multimedia
http://arxiv.org/abs/2104.14236v1,to_check,arxiv,arxiv,2021-04-29 09:57:47+00:00,arxiv,Learning Multi-Attention Context Graph for Group-Based Re-Identification,http://arxiv.org/abs/2104.14236v1,"Learning to re-identify or retrieve a group of people across non-overlapped
camera systems has important applications in video surveillance. However, most
existing methods focus on (single) person re-identification (re-id), ignoring
the fact that people often walk in groups in real scenarios. In this work, we
take a step further and consider employing context information for identifying
groups of people, i.e., group re-id. We propose a novel unified framework based
on graph neural networks to simultaneously address the group-based re-id tasks,
i.e., group re-id and group-aware person re-id. Specifically, we construct a
context graph with group members as its nodes to exploit dependencies among
different people. A multi-level attention mechanism is developed to formulate
both intra-group and inter-group context, with an additional self-attention
module for robust graph-level representations by attentively aggregating
node-level features. The proposed model can be directly generalized to tackle
group-aware person re-id using node-level representations. Meanwhile, to
facilitate the deployment of deep learning models on these tasks, we build a
new group re-id dataset that contains more than 3.8K images with 1.5K annotated
groups, an order of magnitude larger than existing group re-id datasets.
Extensive experiments on the novel dataset as well as three existing datasets
clearly demonstrate the effectiveness of the proposed framework for both
group-based re-id tasks. The code is available at
https://github.com/daodaofr/group_reid.",multimedia
http://arxiv.org/abs/2107.06397v1,to_check,arxiv,arxiv,2021-07-13 21:12:34+00:00,arxiv,"SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based
  Augmented Reality for Surgical Guidance",http://arxiv.org/abs/2107.06397v1,"We present SurgeonAssist-Net: a lightweight framework making
action-and-workflow-driven virtual assistance, for a set of predefined surgical
tasks, accessible to commercially available optical see-through head-mounted
displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic
surgical workflow, our implementation competes with state-of-the-art approaches
in prediction accuracy for automated task recognition, and yet requires 7.4x
fewer parameters, 10.2x fewer floating point operations per second (FLOPS), is
7.0x faster for inference on a CPU, and is capable of near real-time
performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use
of an efficient convolutional neural network (CNN) backbone to extract
discriminative features from image data, and a low-parameter recurrent neural
network (RNN) architecture to learn long-term temporal dependencies. To
demonstrate the feasibility of our approach for inference on the HoloLens 2 we
created a sample dataset that included video of several surgical tasks recorded
from a user-centric point-of-view. After training, we deployed our model and
cataloged its performance in an online simulated surgical scenario for the
prediction of the current surgical task. The utility of our approach is
explored in the discussion of several relevant clinical use-cases. Our code is
publicly available at https://github.com/doughtmw/surgeon-assist-net.",multimedia
http://arxiv.org/abs/1912.10609v1,to_check,arxiv,arxiv,2019-12-23 03:50:52+00:00,arxiv,One-Shot Imitation Filming of Human Motion Videos,http://arxiv.org/abs/1912.10609v1,"Imitation learning has been applied to mimic the operation of a human
cameraman in several autonomous cinematography systems. To imitate different
filming styles, existing methods train multiple models, where each model
handles a particular style and requires a significant number of training
samples. As a result, existing methods can hardly generalize to unseen styles.
In this paper, we propose a framework, which can imitate a filming style by
""seeing"" only a single demonstration video of the same style, i.e., one-shot
imitation filming. This is done by two key enabling techniques: 1) feature
extraction of the filming style from the demo video, and 2) filming style
transfer from the demo video to the new situation. We implement the approach
with deep neural network and deploy it to a 6 degrees of freedom (DOF) real
drone cinematography system by first predicting the future camera motions, and
then converting them to the drone's control commands via an odometer. Our
experimental results on extensive datasets and showcases exhibit significant
improvements in our approach over conventional baselines and our approach can
successfully mimic the footage with an unseen style.",multimedia
http://arxiv.org/abs/1710.08637v1,to_check,arxiv,arxiv,2017-10-24 07:46:57+00:00,arxiv,"Improving Accuracy of Nonparametric Transfer Learning via Vector
  Segmentation",http://arxiv.org/abs/1710.08637v1,"Transfer learning using deep neural networks as feature extractors has become
increasingly popular over the past few years. It allows to obtain
state-of-the-art accuracy on datasets too small to train a deep neural network
on its own, and it provides cutting edge descriptors that, combined with
nonparametric learning methods, allow rapid and flexible deployment of
performing solutions in computationally restricted settings. In this paper, we
are interested in showing that the features extracted using deep neural
networks have specific properties which can be used to improve accuracy of
downstream nonparametric learning methods. Namely, we demonstrate that for some
distributions where information is embedded in a few coordinates, segmenting
feature vectors can lead to better accuracy. We show how this model can be
applied to real datasets by performing experiments using three mainstream deep
neural network feature extractors and four databases, in vision and audio.",multimedia
http://arxiv.org/abs/1804.09914v1,to_check,arxiv,arxiv,2018-04-26 07:02:07+00:00,arxiv,"iTeleScope: Intelligent Video Telemetry and Classification in Real-Time
  using Software Defined Networking",http://arxiv.org/abs/1804.09914v1,"Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost.",multimedia
http://arxiv.org/abs/2109.07165v1,to_check,arxiv,arxiv,2021-09-15 09:00:56+00:00,arxiv,3D Annotation Of Arbitrary Objects In The Wild,http://arxiv.org/abs/2109.07165v1,"Recent years have produced a variety of learning based methods in the context
of computer vision and robotics. Most of the recently proposed methods are
based on deep learning, which require very large amounts of data compared to
traditional methods. The performance of the deep learning methods are largely
dependent on the data distribution they were trained on, and it is important to
use data from the robot's actual operating domain during training. Therefore,
it is not possible to rely on pre-built, generic datasets when deploying robots
in real environments, creating a need for efficient data collection and
annotation in the specific operating conditions the robots will operate in. The
challenge is then: how do we reduce the cost of obtaining such datasets to a
point where we can easily deploy our robots in new conditions, environments and
to support new sensors? As an answer to this question, we propose a data
annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry.
The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel
annotations of arbitrary objects without needing accurate 3D models of the
objects prior to data collection and annotation. Our results showcase almost
90% Intersection-over-Union (IoU) agreement on both semantic segmentation and
2D bounding box detection across a variety of objects and scenes, while
speeding up the annotation process by several orders of magnitude compared to
traditional manual annotation.",multimedia
http://arxiv.org/abs/2105.10389v3,to_check,arxiv,arxiv,2021-05-21 15:03:29+00:00,arxiv,Learning Visible Connectivity Dynamics for Cloth Smoothing,http://arxiv.org/abs/2105.10389v3,"Robotic manipulation of cloth remains challenging for robotics due to the
complex dynamics of the cloth, lack of a low-dimensional state representation,
and self-occlusions. In contrast to previous model-based approaches that learn
a pixel-based dynamics model or a compressed latent vector dynamics, we propose
to learn a particle-based dynamics model from a partial point cloud
observation. To overcome the challenges of partial observability, we infer
which visible points are connected on the underlying cloth mesh. We then learn
a dynamics model over this visible connectivity graph. Compared to previous
learning-based approaches, our model poses strong inductive bias with its
particle based representation for learning the underlying cloth physics; it is
invariant to visual features; and the predictions can be more easily
visualized. We show that our method greatly outperforms previous
state-of-the-art model-based and model-free reinforcement learning methods in
simulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we
deploy the model trained in simulation on a Franka arm and show that the model
can successfully smooth different types of cloth from crumpled configurations.
Videos can be found on our project website.",multimedia
http://arxiv.org/abs/2003.10758v1,to_check,arxiv,arxiv,2020-03-24 10:27:11+00:00,arxiv,FADNet: A Fast and Accurate Network for Disparity Estimation,http://arxiv.org/abs/2003.10758v1,"Deep neural networks (DNNs) have achieved great success in the area of
computer vision. The disparity estimation problem tends to be addressed by DNNs
which achieve much better prediction accuracy in stereo matching than
traditional hand-crafted feature based methods. On one hand, however, the
designed DNNs require significant memory and computation resources to
accurately predict the disparity, especially for those 3D convolution based
networks, which makes it difficult for deployment in real-time applications. On
the other hand, existing computation-efficient networks lack expression
capability in large-scale datasets so that they cannot make an accurate
prediction in many scenarios. To this end, we propose an efficient and accurate
deep network for disparity estimation named FADNet with three main features: 1)
It exploits efficient 2D based correlation layers with stacked blocks to
preserve fast computation; 2) It combines the residual structures to make the
deeper model easier to learn; 3) It contains multi-scale predictions so as to
exploit a multi-scale weight scheduling training technique to improve the
accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on
two popular datasets, Scene Flow and KITTI 2015. Experimental results show that
FADNet achieves state-of-the-art prediction accuracy, and runs at a significant
order of magnitude faster speed than existing 3D models. The codes of FADNet
are available at https://github.com/HKBU-HPML/FADNet.",multimedia
http://arxiv.org/abs/2105.01948v1,to_check,arxiv,arxiv,2021-05-05 09:36:30+00:00,arxiv,"Similarity Measures for Location-Dependent MMIMO, 5G Base Stations
  On/Off Switching Using Radio Environment Map",http://arxiv.org/abs/2105.01948v1,"The Massive Multiple-Input Multiple-Output (MMIMO) technique together with
Heterogeneous Network (Het-Net) deployment enables high throughput of 5G and
beyond networks. However, a high number of antennas and a high number of Base
Stations (BSs) can result in significant power consumption. Previous studies
have shown that the energy efficiency (EE) of such a network can be effectively
increased by turning off some BSs depending on User Equipments (UEs) positions.
Such mapping is obtained by using Reinforcement Learning. Its results are
stored in a so-called Radio Environment Map (REM). However, in a real network,
the number of UEs' positions patterns would go to infinity. This paper aims to
determine how to match the current set of UEs' positions to the most similar
pattern, i.e., providing the same optimal active BSs set, saved in REM. We
compare several state-of-the-art distance metrics using a computer simulator:
an accurate 3D-Ray-Tracing model of the radio channel and an advanced
system-level simulator of MMIMO Het-Net. The results have shown that the
so-called Sum of Minimums Distance provides the best matching between REM data
and UEs' positions, enabling up to 56% EE improvement over the scenario without
EE optimization.",multimedia
10.1016/j.physa.2019.123151,to_check,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,Early warning system: From face recognition by surveillance cameras to social media analysis to detecting suspicious people,https://api.elsevier.com/content/abstract/scopus_id/85074532417,"Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the person’s network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",multimedia
10.1109/DSAA.2019.00070,to_check,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),IEEE,2019-10-08 00:00:00,ieeexplore,"Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform",https://ieeexplore.ieee.org/document/8964147/,"With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing.",science
10.1109/BigData.2018.8621926,to_check,2018 IEEE International Conference on Big Data (Big Data),IEEE,2018-12-13 00:00:00,ieeexplore,Harnessing the Nature of Spam in Scalable Online Social Spam Detection,https://ieeexplore.ieee.org/document/8621926/,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management.",science
10.1109/ACCESS.2021.3124386,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,A Multiple Pheromone Communication System for Swarm Intelligence,https://ieeexplore.ieee.org/document/9594791/,"Pheromones are chemical substances essential for communication among social insects. In the application of swarm intelligence to real micro mobile robots, the deployment of a single virtual pheromone has emerged recently as a powerful real-time method for indirect communication. However, these studies usually exploit only one kind of pheromones in their task, neglecting the crucial fact that in the world of real insects, multiple pheromones play important roles in shaping stigmergic behaviors such as foraging or nest building. To explore the multiple pheromones mechanism which enable robots to solve complex collective tasks efficiently, we introduce an artificial multiple pheromone system (ColCOS<inline-formula> <tex-math notation=""LaTeX"">$\Phi $ </tex-math></inline-formula>) to support swarm intelligence research by enabling multiple robots to deploy and react to multiple pheromones simultaneously. The proposed system ColCOS<inline-formula> <tex-math notation=""LaTeX"">$\Phi $ </tex-math></inline-formula> uses optical signals to emulate different evaporating chemical substances i.e. pheromones. These emulated pheromones are represented by trails displayed on a wide LCD display screen positioned horizontally, on which multiple miniature robots can move freely. The color sensors beneath the robots can detect and identify lingering “pheromones” on the screen. Meanwhile, the release of any pheromone from each robot is enabled by monitoring its positional information over time with an overhead camera. No other communication methods apart from virtual pheromones are employed in this system. Two case studies have been carried out which have verified the feasibility and effectiveness of the proposed system in achieving complex swarm tasks as empowered by multiple pheromones. This novel platform is a timely and powerful tool for research into swarm intelligence.",science
10.1109/ITherm51669.2021.9503247,to_check,2021 20th IEEE Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (iTherm),IEEE,2021-06-04 00:00:00,ieeexplore,Deep Learning to Enhance Transient Thermal Performance and Real-Time Control of an Energy Storage (TES) Platform,https://ieeexplore.ieee.org/document/9503247/,"Phase Change Materials (PCMs) are used for thermal energy storage (TES) due to their high latent heat capacity. While inorganic PCMs have the ability to store large amounts of thermal energy (as they possess higher values of latent heat capacity) they need significant supercooling to initiate solidification - which compromises their operational reliability (compared to organic PCMs). `Cold Finger Technique'/ CFT (wherein a portion of the PCM is maintained in the solid-state as the un-melted PCM serves to initiate nucleation for subsequent solidification of the rest of the PCM) enables higher operational reliability at the expense of a marginal loss of storage capacity (from the portion of the PCM that remains un-melted). In order to apply CFT, one possible approach is to target a desired melt fraction in the early stages of operation for avoiding catastrophic failure (i.e., avoiding 100% melting). The objective is to reach as close to 100% melting as possible - without completely melting the whole mass of PCM - using a reliable and predictable method. Hence, at any instant, a reliable way is desired to predict the time remaining to reach a particular melt fraction of the PCM (during the melting cycle).In this study we determine the efficacy of employing Artificial Intelligence (AI) based techniques, especially Deep Learning, for predicting the time remaining to reach a target melt fraction at any instant during the melting cycle. The time history of the PCM melt fraction and temperature transients at multiple locations within the TES platform can thus be correlated by employing an Artificial Neural Network (ANN) to achieve this goal. The aim of this study is to develop and deploy ANN based prediction tools that can enable real-time predictions (e.g., when the melt cycle should be stopped and freezing cycle should be initiated) apriori based on instantaneous values of temperature measured during operation of a TES platform. The data for training and evaluating the efficacy of the ANN model predictions is obtained from PCM melting experiments performed in this study using PureTemp29. Initially a solid mass of PCM is melted (using a nichrome wire heater mounted at the bottom) in a graduated cylinder with three thermocouples recording the temperatures at distinct locations along the height of the cylinder. An automated digital data acquisition (DAQ) system is used to record the transient temperature profiles from these thermocouples. The height of the liquid meniscus (melted PCM) is recorded using a digital image acquisition apparatus (and also monitored periodically from the measuring cylinder containing the PCM) until the PCM is completely melted. The results show that the predictions of the ANN model are most accurate in the later stages of the melting cycle. The error decreases as the melt fraction approaches the target melt. For a target melt of 90%, the error in predictions in the last 1800 seconds of the melting cycle is in the range 150-350 seconds.",science
10.1186/s13007-021-00744-3,to_check,Plant Methods,BioMed Central,2021-04-26 00:00:00,springer,PredCRG: A computational method for recognition of plant circadian genes by employing support vector machine with Laplace kernel,https://www.biomedcentral.com/openurl?doi=10.1186/s13007-021-00744-3,"Background Circadian rhythms regulate several physiological and developmental processes of plants. Hence, the identification of genes with the underlying circadian rhythmic features is pivotal. Though computational methods have been developed for the identification of circadian genes, all these methods are based on gene expression datasets. In other words, we failed to search any sequence-based model, and that motivated us to deploy the present computational method to identify the proteins encoded by the circadian genes. Results Support vector machine (SVM) with seven kernels, i.e., linear, polynomial, radial, sigmoid, hyperbolic, Bessel and Laplace was utilized for prediction by employing compositional, transitional and physico-chemical features. Higher accuracy of 62.48% was achieved with the Laplace kernel, following the fivefold cross- validation approach. The developed model further secured 62.96% accuracy with an independent dataset. The SVM also outperformed other state-of-art machine learning algorithms, i.e., Random Forest, Bagging, AdaBoost, XGBoost and LASSO. We also performed proteome-wide identification of circadian proteins in two cereal crops namely, Oryza sativa and Sorghum bicolor , followed by the functional annotation of the predicted circadian proteins with Gene Ontology (GO) terms. Conclusions To the best of our knowledge, this is the first computational method to identify the circadian genes with the sequence data. Based on the proposed method, we have developed an R-package PredCRG ( https://cran.r-project.org/web/packages/PredCRG/index.html ) for the scientific community for proteome-wide identification of circadian genes. The present study supplements the existing computational methods as well as wet-lab experiments for the recognition of circadian genes.",science
10.1007/s10817-015-9330-8,to_check,Journal of Automated Reasoning,Springer,2015-10-01 00:00:00,springer,MizAR 40 for Mizar 40,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10817-015-9330-8,"As a present to Mizar on its 40th anniversary, we develop an AI/ATP system that in 30 seconds of real time on a 14-CPU machine automatically proves 40 % of the theorems in the latest official version of the Mizar Mathematical Library ( MML ). This is a considerable improvement over previous performance of large-theory AI/ATP methods measured on the whole MML . To achieve that, a large suite of AI/ATP methods is employed and further developed. We implement the most useful methods efficiently, to scale them to the 150000 formulas in MML . This reduces the training times over the corpus to 1–3 seconds, allowing a simple practical deployment of the methods in the online automated reasoning service for the Mizar users ( Miz AR $\mathbb {A}\mathbb {R}$ ).",science
http://arxiv.org/abs/2108.03713v1,to_check,arxiv,arxiv,2021-08-08 19:12:04+00:00,arxiv,"On the Difficulty of Generalizing Reinforcement Learning Framework for
  Combinatorial Optimization",http://arxiv.org/abs/2108.03713v1,"Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP.",science
http://arxiv.org/abs/2004.05953v1,to_check,arxiv,arxiv,2020-04-13 14:09:21+00:00,arxiv,"Software-Defined Network for End-to-end Networked Science at the
  Exascale",http://arxiv.org/abs/2004.05953v1,"Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence.",science
http://arxiv.org/abs/2108.12430v1,to_check,arxiv,arxiv,2021-08-27 18:00:00+00:00,arxiv,"Hardware-accelerated Inference for Real-Time Gravitational-Wave
  Astronomy",http://arxiv.org/abs/2108.12430v1,"The field of transient astronomy has seen a revolution with the first
gravitational-wave detections and the arrival of multi-messenger observations
they enabled. Transformed by the first detection of binary black hole and
binary neutron star mergers, computational demands in gravitational-wave
astronomy are expected to grow by at least a factor of two over the next five
years as the global network of kilometer-scale interferometers are brought to
design sensitivity. With the increase in detector sensitivity, real-time
delivery of gravitational-wave alerts will become increasingly important as an
enabler of multi-messenger followup. In this work, we report a novel
implementation and deployment of deep learning inference for real-time
gravitational-wave data denoising and astrophysical source identification. This
is accomplished using a generic Inference-as-a-Service model that is capable of
adapting to the future needs of gravitational-wave data analysis. Our
implementation allows seamless incorporation of hardware accelerators and also
enables the use of commercial or private (dedicated) as-a-service computing.
Based on our results, we propose a paradigm shift in low-latency and offline
computing in gravitational-wave astronomy. Such a shift can address key
challenges in peak-usage, scalability and reliability, and provide a data
analysis platform particularly optimized for deep learning applications. The
achieved sub-millisecond scale latency will also be relevant for any machine
learning-based real-time control systems that may be invoked in the operation
of near-future and next generation ground-based laser interferometers, as well
as the front-end collection, distribution and processing of data from such
instruments.",science
http://arxiv.org/abs/2010.14000v2,to_check,arxiv,arxiv,2020-10-27 02:19:40+00:00,arxiv,"Graph-based Reinforcement Learning for Active Learning in Real Time: An
  Application in Modeling River Networks",http://arxiv.org/abs/2010.14000v2,"Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.",science
http://arxiv.org/abs/2004.00104v1,to_check,arxiv,arxiv,2020-03-31 20:58:14+00:00,arxiv,"Improvement of electronic Governance and mobile Governance in
  Multilingual Countries with Digital Etymology using Sanskrit Grammar",http://arxiv.org/abs/2004.00104v1,"With huge improvement of digital connectivity (Wifi,3G,4G) and digital
devices access to internet has reached in the remotest corners now a days.
Rural people can easily access web or apps from PDAs, laptops, smartphones etc.
This is an opportunity of the Government to reach to the citizen in large
number, get their feedback, associate them in policy decision with e governance
without deploying huge man, material or resourses. But the Government of
multilingual countries face a lot of problem in successful implementation of
Government to Citizen (G2C) and Citizen to Government (C2G) governance as the
rural people tend and prefer to interact in their native languages. Presenting
equal experience over web or app to different language group of speakers is a
real challenge. In this research we have sorted out the problems faced by Indo
Aryan speaking netizens which is in general also applicable to any language
family groups or subgroups. Then we have tried to give probable solutions using
Etymology. Etymology is used to correlate the words using their ROOT forms. In
5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how
a word is changed according to person,tense,gender,number etc. Later this book
was followed in Western countries also to derive their grammar of comparatively
new languages. We have trained our system for automatic root extraction from
the surface level or morphed form of words using Panian Gramatical rules. We
have tested our system over 10000 bengali Verbs and extracted the root form
with 98% accuracy. We are now working to extend the program to successfully
lemmatize any words of any language and correlate them by applying those rule
sets in Artificial Neural Network.",science
http://arxiv.org/abs/1908.04172v2,to_check,arxiv,arxiv,2019-08-12 14:30:13+00:00,arxiv,"nGraph-HE2: A High-Throughput Framework for Neural Network Inference on
  Encrypted Data",http://arxiv.org/abs/1908.04172v2,"In previous work, Boemer et al. introduced nGraph-HE, an extension to the
Intel nGraph deep learning (DL) compiler, that enables data scientists to
deploy models with popular frameworks such as TensorFlow and PyTorch with
minimal code changes. However, the class of supported models was limited to
relatively shallow networks with polynomial activations. Here, we introduce
nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on
standard, pre-trained models using their native activation functions and number
fields (typically real numbers). The proposed framework leverages the CKKS
scheme, whose support for real numbers is friendly to data science, and a
client-aided model using a two-party approach to compute activation functions.
  We first present CKKS-specific optimizations, enabling a 3x-88x runtime
speedup for scalar encoding, and doubling the throughput through a novel use of
CKKS plaintext packing into complex numbers. Second, we optimize
ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime
speedup. Third, we exploit two graph-level optimizations: lazy rescaling and
depth-aware encoding, which allow us to significantly improve performance.
  Together, these optimizations enable state-of-the-art throughput of 1,998
images/s on the CryptoNets network. Using the client-aided model, we also
present homomorphic evaluation of (to our knowledge) the largest network to
date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with
60.4\percent/82.7\percent\ top-1/top-5 accuracy and an amortized runtime of 381
ms/image.",science
http://arxiv.org/abs/1707.05223v1,to_check,arxiv,arxiv,2017-07-17 15:18:36+00:00,arxiv,A transient search using combined human and machine classifications,http://arxiv.org/abs/1707.05223v1,"Large modern surveys require efficient review of data in order to find
transient sources such as supernovae, and to distinguish such sources from
artefacts and noise. Much effort has been put into the development of automatic
algorithms, but surveys still rely on human review of targets. This paper
presents an integrated system for the identification of supernovae in data from
Pan-STARRS1, combining classifications from volunteers participating in a
citizen science project with those from a convolutional neural network. The
unique aspect of this work is the deployment, in combination, of both human and
machine classifications for near real-time discovery in an astronomical
project. We show that the combination of the two methods outperforms either one
used individually. This result has important implications for the future
development of transient searches, especially in the era of LSST and other
large-throughput surveys.",science
10.1016/j.future.2020.04.018,to_check,Future Generation Computer Systems,scopus,2020-09-01,sciencedirect,Software-Defined Network for End-to-end Networked Science at the Exascale,https://api.elsevier.com/content/abstract/scopus_id/85083299223,"Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence.",science
10.1016/j.renene.2019.09.092,to_check,Renewable Energy,scopus,2020-03-01,sciencedirect,Wind turbine fatigue reduction based on economic-tracking NMPC with direct ANN fatigue estimation,https://api.elsevier.com/content/abstract/scopus_id/85072713179,"The aim of this work is to deploy an advanced Nonlinear Model Predictive Control (NMPC) approach for reducing the tower fatigue of a wind turbine (WT) tower while guaranteeing efficient energy extraction from the wind. To achieve this, different Artificial Neural Network (ANN) architectures are trained and tested in order to estimate the tower fatigue as a surrogate of the traditional Rainflow Counting (RFC) method. The ANNs receive data stemming from the tower top oscillation velocity and the previous fatigue state to directly estimate the fatigue progression. The results are compared to select the most convenient architecture for control implementation. Once an ANN is selected, an economic-tracking NMPC (etNMPC) solution to reduce the fatigue of the WT tower is deployed in real-time. The closed-loop results are then compared to a baseline controller from a renowned WT simulation tool and a classic etNMPC implementation with indirect fatigue minimisation to demonstrate the improvement achieved with the proposed strategy. Finally, conclusions regarding computational cost and real-time deployment capabilities are discussed, as well as future lines of research.",science
10.1016/j.cie.2019.06.040,to_check,Computers and Industrial Engineering,scopus,2019-09-01,sciencedirect,"Bernard, an energy intelligent system for raising residential users awareness",https://api.elsevier.com/content/abstract/scopus_id/85067600850,"Energy efficiency is still a hot topic today. Coming roughly the 25% of the energy consumption in EU from the residential sector, very few cheap and simple tools to promote energy efficiency in home users have been developed. The purpose of this paper is to present Bernard, a concept proof designed for filling this gap. This aims that householders become aware of their energy habits and have useful information that help them to redirect their consumption pattern. To achieve these goals, Bernard offers, through a mobile application, the home energy consumption monitoring in real time, the energy price forecast for the next hour and the appliances which are switched on, among others. Furthermore, it is important to highlight that the system has been designed with the premises of being cheap, non-intrusive, reliable and easily scalable, in order that utilities can gradually deploy and provide it to their customers, gaining at the same time valuable information for decision making and improving its corporate social image. Therefore, the adopted solution is based on a real time streaming data architecture suitable for handling huge volumes of data and applying predictive techniques on a cloud-computing environment. The paper provides a detailed description of the system and experimental results evaluating the performance of the predictive modules built. As case study, REFIT and REDD datasets were used.",science
10.1109/ICRA.2019.8793510,to_check,2019 International Conference on Robotics and Automation (ICRA),IEEE,2019-05-24 00:00:00,ieeexplore,Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs,https://ieeexplore.ieee.org/document/8793510/,"The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.",robotics
10.1109/AIIoT52608.2021.9454183,to_check,2021 IEEE World AI IoT Congress (AIIoT),IEEE,2021-05-13 00:00:00,ieeexplore,Image Classification with Knowledge-Based Systems on the Edge for Real-Time Danger Avoidance in Robots,https://ieeexplore.ieee.org/document/9454183/,"Mobile robots are increasingly common in society and are increasingly being used for complex and high-stakes tasks such as search and rescue. The growing requirements for these robots demonstrate a need for systems which can review and react in real time to environmental hazards, which will allow robots to handle environments that are both dynamic and dangerous. We propose and test a system which allows mobile robots to reclassify environmental objects during operation in conjunction with an edge system. We train an image classification model with 99 percent accuracy and deploy it in conjunction with an edge server and JSON-based ruleset to allow robots to react to and avoid hazards.",robotics
10.1109/RoSE52553.2021.00011,to_check,2021 IEEE/ACM 3rd International Workshop on Robotics Software Engineering (RoSE),IEEE,2021-06-02 00:00:00,ieeexplore,A Modeling Tool for Reconfigurable Skills in ROS,https://ieeexplore.ieee.org/document/9474550/,"Known attempts to build autonomous robots rely on complex control architectures, often implemented with the Robot Operating System platform (ROS). The implementation of adaptable architectures is very often ad hoc, quickly gets cumbersome and expensive. Reusable solutions that support complex, runtime reasoning for robot adaptation have been seen in the adoption of ontologies. While the usage of ontologies significantly increases system reuse and maintainability, it requires additional effort from the application developers to translate requirements into formal rules that can be used by an ontological reasoner. In this paper, we present a design tool that facilitates the specification of reconfigurable robot skills. Based on the specified skills, we generate corresponding runtime models for self-adaptation that can be directly deployed to a running robot that uses a reasoning approach based on ontologies. We demonstrate the applicability of the tool in a real robot performing a patrolling mission at a university campus.",robotics
10.1109/IROS.2018.8593518,to_check,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2018-10-05 00:00:00,ieeexplore,Simultaneous End-User Programming of Goals and Actions for Robotic Shelf Organization,https://ieeexplore.ieee.org/document/8593518/,"Arrangement of items on shelves in stores or warehouses is a tedious, repetitive task that can be feasible for robots to perform. The diversity of products that are available in stores and the different setups and preferences of each store makes pre-programming a robot for this task extremely challenging. Instead, our work argues for enabling end-users to customize the robot to their specific objects and setup at deployment time by programming it themselves. To that end, this paper contributes (i) a task representation for shelf arrangements based on a large dataset of grocery store shelf images, (ii) a method for inferring goal configurations from user inputs including demonstrations and direct parameter specifications, and (iii) a system implementation of the proposed approach that allows simultaneously learning task goals and actions. We evaluate our goal inference approach with ten different teaching strategies that combine alternative user inputs in different ways on the large dataset of grocery configurations, as well as with real human teachers through an online user study (N=32). We evaluate our full system implemented on a Fetch mobile manipulator on eight benchmark tasks that demonstrate end-to-end programming and execution of shelf arrangement tasks.",robotics
10.1109/LRA.2020.2998414,to_check,IEEE Robotics and Automation Letters,IEEE,2020-07-01 00:00:00,ieeexplore,RILaaS: Robot Inference and Learning as a Service,https://ieeexplore.ieee.org/document/9103220/,"Programming robots is complicated due to the lack of `plug-and-play' modules for skill acquisition. Virtualizing deployment of deep learning models can facilitate large-scale use/re-use of off-the-shelf functional behaviors. Deploying deep learning models on robots entails real-time, accurate and reliable inference service under varying query load. This letter introduces a novel Robot-Inference-and-Learning-as-a-Service (RILaaS) platform for low-latency and secure inference serving of deep models that can be deployed on robots. Unique features of RILaaS include: 1) low-latency and reliable serving with gRPC under dynamic loads by distributing queries over multiple servers on Edge and Cloud, 2) SSH based authentication coupled with SSL/TLS based encryption for security and privacy of the data, and 3) front-end REST API for sharing, monitoring and visualizing performance metrics of the available models. We report experiments to evaluate the RILaaS platform under varying loads of batch size, number of robots, and various model placement hosts on Cloud, Edge, and Fog for providing benchmark applications of object recognition and grasp planning as a service. We address the complexity of load balancing with a reinforcement learning algorithm that optimizes simulated profiles of networked robots; outperforming several baselines including round robin, least connections, and least model time with 68.30% and 14.04% decrease in round-trip latency time across models compared to the worst and the next best baseline respectively. Details and updates are available at: https://sites.google.com/view/rilaas.",robotics
10.1109/ROBIO.2017.8324803,to_check,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2017-12-08 00:00:00,ieeexplore,Classification-lock tracking approach applied on person following robot,https://ieeexplore.ieee.org/document/8324803/,"The task of following a person in the real complex environment by camera still keeps at risk even the visual tracking technologies have been well studied in the last decade. Currently, most approaches only utilize single-shot initialization in the first frame and update their tracking models according to the result of the last frame. However, it leads to an uncorrected target selection once the inner appearance changes, i.e., a feature-rich object is moved out of the human. In this paper, we reveal a classification-lock tracking framework and apply our approach on a mobile platform. A pairwise cluster tracker is used to locate the person. A positive &amp; negative classifier is utilized to verify the tracker's result and update tracking model. In addition, a pre-trained CPU optimized neural network is employed to lock the tracking result to only be human. In the experiment, we deploy the common challenges of visual tracking both on the static scene and a real-following task. Furthermore, our approach is compared with other state-of-art approaches on common datasets. Results prove the tracking quality of our approach in both the static and the dynamic scenes. Our approach achieves the best average score on the common dataset.",robotics
10.1109/ICRA40945.2020.9196540,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,https://ieeexplore.ieee.org/document/9196540/,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",robotics
10.1109/LRA.2018.2798286,to_check,IEEE Robotics and Automation Letters,IEEE,2018-04-01 00:00:00,ieeexplore,Real-Time 3-D Shape Instantiation From Single Fluoroscopy Projection for Fenestrated Stent Graft Deployment,https://ieeexplore.ieee.org/document/8269290/,"Robot-assisted deployment of fenestrated stent grafts in fenestrated endovascular aortic repair (FEVAR) requires accurate geometrical alignment. Currently, this process is guided by two-dimensional (2-D) fluoroscopy, which is insufficiently informative and error prone. In this letter, a real-time framework is proposed to instantiate the 3-D shape of a fenestrated stent graft by utilizing only a single low-dose 2-D fluoroscopic image. First, markers were placed on the fenestrated stent graft. Second, the 3-D pose of each stent segment was instantiated by the robust perspective-n-point method. Third, the 3-D shape of the whole stent graft was instantiated via graft gap interpolation. Focal UNet was proposed to segment the markers from 2-D fluoroscopic images to achieve semiautomatic marker detection. The proposed framework was validated on five patient-specific 3-D printed aortic aneurysm phantoms and three stent grafts with new marker placements, showing an average distance error of 1-3 mm and an average angular error of 4°. Shape instantiation codes are available online.",robotics
10.1109/TDSC.2019.2903049,to_check,IEEE Transactions on Dependable and Secure Computing,IEEE,2021-04-01 00:00:00,ieeexplore,Real-Time Error Detection in Nonlinear Control Systems Using Machine Learning Assisted State-Space Encoding,https://ieeexplore.ieee.org/document/8658148/,"Successful deployment of autonomous systems in a wide range of societal applications depends on error-free operation of the underlying signal processing and control functions. Real-time error detection in nonlinear systems has mostly relied on redundancy at the component or algorithmic level causing expensive area and power overheads. This paper describes a real-time error detection methodology for nonlinear control systems for detecting sensor and actuator degradations as well as malfunctions due to soft errors in the execution of the control algorithm on a digital processor. Our approach is based on creation of a redundant check state in such a way that its value can be computed from the current states of the system as well as from a history of prior observable state values and inputs (via machine learning algorithms). By checking for consistency between the two, errors are detected with low latency. The method is demonstrated on two test case simulations - an inverted pendulum balancing problem and a sliding mode controller driven brake-by-wire (BBW) system. In addition, hardware results from error injection experiments in an ARM core representation on an FPGA and artificial sensor degradations on a self-balancing robot prove the practical feasibility of implementation.",robotics
http://arxiv.org/abs/1809.06716v1,to_check,arxiv,arxiv,2018-09-16 07:58:09+00:00,arxiv,A Fog Robotic System for Dynamic Visual Servoing,http://arxiv.org/abs/1809.06716v1,"Cloud Robotics is a paradigm where distributed robots are connected to cloud
services via networks to access unlimited computation power, at the cost of
network communication. However, due to limitations such as network latency and
variability, it is difficult to control dynamic, human compliant service robots
directly from the cloud. In this work, by leveraging asynchronous protocol with
a heartbeat signal, we combine cloud robotics with a smart edge device to build
a Fog Robotic system. We use the system to enable robust teleoperation of a
dynamic self-balancing robot from the cloud. We first use the system to pick up
boxes from static locations, a task commonly performed in warehouse logistics.
To make cloud teleoperation more efficient, we deploy image based visual
servoing (IBVS) to perform box pickups automatically. Visual feedbacks,
including apriltag recognition and tracking, are performed in the cloud to
emulate a Fog Robotic object recognition system for IBVS. We demonstrate the
feasibility of real-time dynamic automation system using this cloud-edge
hybrid, which opens up possibilities of deploying dynamic robotic control with
deep-learning recognition systems in Fog Robotics. Finally, we show that Fog
Robotics enables the self-balancing service robot to pick up a box
automatically from a person under unstructured environments.",robotics
10.1016/j.neunet.2021.01.028,to_check,Neural Networks,scopus,2021-07-01,sciencedirect,Biomimetic FPGA-based spatial navigation model with grid cells and place cells,https://api.elsevier.com/content/abstract/scopus_id/85101901812,"The mammalian spatial navigation system is characterized by an initial divergence of internal representations, with disparate classes of neurons responding to distinct features including location, speed, borders and head direction; an ensuing convergence finally enables navigation and path integration. Here, we report the algorithmic and hardware implementation of biomimetic neural structures encompassing a feed-forward trimodular, multi-layer architecture representing grid-cell, place-cell and decoding modules for navigation. The grid-cell module comprised of neurons that fired in a grid-like pattern, and was built of distinct layers that constituted the dorsoventral span of the medial entorhinal cortex. Each layer was built as an independent continuous attractor network with distinct grid-field spatial scales. The place-cell module comprised of neurons that fired at one or few spatial locations, organized into different clusters based on convergent modular inputs from different grid-cell layers, replicating the gradient in place-field size along the hippocampal dorso-ventral axis. The decoding module, a two-layer neural network that constitutes the convergence of the divergent representations in preceding modules, received inputs from the place-cell module and provided specific coordinates of the navigating object. After vital design optimizations involving all modules, we implemented the tri-modular structure on Zynq Ultrascale+ field-programmable gate array silicon chip, and demonstrated its capacity in precisely estimating the navigational trajectory with minimal overall resource consumption involving a mere 2.92% Look Up Table utilization. Our implementation of a biomimetic, digital spatial navigation system is stable, reliable, reconfigurable, real-time with execution time of about 32 s for 100k input samples (in contrast to 40 minutes on Intel Core i7-7700 CPU with 8 cores clocking at 3.60 GHz) and thus can be deployed for autonomous-robotic navigation without requiring additional sensors.",robotics
10.1016/j.ress.2019.106700,to_check,Reliability Engineering and System Safety,scopus,2020-03-01,sciencedirect,Optimizing inspection routes in pipeline networks,https://api.elsevier.com/content/abstract/scopus_id/85073997788,"Maintaining an aging network is a challenge for many water utilities due to limited budgets and uncertainty surrounding the physical condition of buried pipeline assets. The deployment of robotic inspections provides high quality data, but these platforms have limited use due to cost and operational constraints. To facilitate cost-efficient inspections, operators need to identify high-risk assets while accounting for the effectiveness of the tools at hand. This paper addresses inspection planning with the goal of finding an optimal route while considering tool limitations. An exact integer programming formulation is presented where only three factors are used to characterize tool constraints. Two classes of solution methods are explored: 1) tree based searches, and 2) integer programming. This paper demonstrates how each method can be used to identify optimal paths within a real water distribution system. Empirical trials suggest that tree-based search methods are the most efficient when the path limit is short, but do not scale well when the path length increases. In contrast, integer-programming methods are more effective for longer path lengths but have scalability issues for large network sizes. Data preprocessing, where the input network size is reduced, can provide large computation time reductions while returning near-optimal solutions.",robotics
10.1109/ACCESS.2020.2970728,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,LoPECS: A Low-Power Edge Computing System for Real-Time Autonomous Driving Services,https://ieeexplore.ieee.org/document/8977507/,"To simultaneously enable multiple autonomous driving services on affordable embedded systems, we designed and implemented LoPECS, a Low-Power Edge Computing System for real-time autonomous robots and vehicles services. The contributions of this paper are three-fold: first, we developed a Heterogeneity-Aware Runtime Layer to fully utilize vehicle's heterogeneous computing resources to fulfill the real-time requirement of autonomous driving applications; second, we developed a vehicle-edge Coordinator to dynamically offload vehicle tasks to edge cloudlet to further optimize user experience in the way of prolonged battery life; third, we successfully integrated these components into LoPECS system and implemented it on Nvidia Jetson TX1. To the best of our knowledge, this is the first complete edge computing system in a production autonomous vehicle. Our implementation on Nvidia Jetson demonstrated that it could successfully support multiple autonomous driving services with only 11 W of power consumption, and hence proves the effectiveness of the proposed LoPECS system.",autonomous vehicle
10.1109/ICAMIMIA47173.2019.9223365,to_check,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",IEEE,2019-10-10 00:00:00,ieeexplore,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,https://ieeexplore.ieee.org/document/9223365/,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",autonomous vehicle
10.1109/ANDESCON50619.2020.9272196,to_check,2020 IEEE ANDESCON,IEEE,2020-10-16 00:00:00,ieeexplore,Multipurpose unmanned system: an efficient solution to increase the capabilities of the UAVs,https://ieeexplore.ieee.org/document/9272196/,"The results of this research propose the implementation of a system that significantly increases the capacity of unmanned vehicles, turning them into multifunctional vehicles. The system has a logistics dispatch module and a video analytics module. The first module allows the delivery of medical, food, smoke, disinfectant, etc. The module is practical, safe and economical, features that denote the feasibility of immediate implementation in unmanned vehicles of any rank and / or classification. Note that the implementation of the dispatch module does not require additional radio frequency systems. The second module includes a video analysis process in real time, an aspect that constitutes a significant contribution to the proposed solution, since it allows obtaining important information during the flight; it also reduces the risk in air operations and simultaneously increases the efficiency of themselves. Note that video analytics optimizes resources and avoids jeopardizing the lives of aircraft pilots and crews who traditionally should carry out these activities. In times of pandemic, this innovation avoids direct contact with an infected population and can guarantee the sanitary conditions required in certain circumstances. The solution increases the capabilities of unmanned vehicles and makes them useful tools in various scenarios, whether caused by natural or man-made disasters. Our proposal is very flexible, reliable, and scalable and can be adapted to various models and makes of unmanned vehicles. The system has been implemented on fixed-wing and rotary-wing unmanned vehicles, showing satisfactory results.",autonomous vehicle
10.1016/j.bdr.2021.100241,to_check,Big Data Research,scopus,2021-07-15,sciencedirect,“Brains” for Robots: Application of the Mivar Expert Systems for Implementation of Autonomous Intelligent Robots,https://api.elsevier.com/content/abstract/scopus_id/85107957013,"Recently the contemporary robotic systems can manipulate different objects and make decisions in a range of situations due to significant advances in innovation technologies and artificial intelligence. The new expert technologies can handle millions of instructions on computers and smartphones, which allow them to be used as a tool to create “decision-making systems” for autonomous robots. The goal of this paper was to create a dynamic algorithm of robot actions that can be used in the decision module has been considered. It is proposed to use Mivar expert systems of a new generation for high-level control. The experiment results showed that Mivar decision-making systems can control groups of small robots and even an unmanned autonomous car in real time. The algorithms created in the Mivar environment can be very flexible, and their build-up depends only on engineering approaches. In addition to traditional low-level robot control systems, a Mivar decision-making system has been implemented, which can be considered as universal “Brains” for autonomous intelligent robots and now knowledge bases can be created and various robots can be trained for practical tasks.",autonomous vehicle
10.1016/j.net.2018.12.020,to_check,Nuclear Engineering and Technology,scopus,2019-06-01,sciencedirect,Numerical evaluation of gamma radiation monitoring,https://api.elsevier.com/content/abstract/scopus_id/85059358720,"Airborne Gamma Ray Spectrometry (AGRS) with its important applications such as gathering radiation information of ground surface, geochemistry measuring of the abundance of Potassium, Thorium and Uranium in outer earth layer, environmental and nuclear site surveillance has a key role in the field of nuclear science and human life. The Broyden–Fletcher–Goldfarb–Shanno (BFGS), with its advanced numerical unconstrained nonlinear optimization in collaboration with Artificial Neural Networks (ANNs) provides a noteworthy opportunity for modern AGRS. In this study a new AGRS system empowered by ANN-BFGS has been proposed and evaluated on available empirical AGRS data. To that effect different architectures of adaptive ANN-BFGS were implemented for a sort of published experimental AGRS outputs. The selected approach among of various training methods, with its low iteration cost and non-diagonal scaling allocation is a new powerful algorithm for AGRS data due to its inherent stochastic properties. Experiments were performed by different architectures and trainings, the selected scheme achieved the smallest number of epochs, the minimum Mean Square Error (MSE) and the maximum performance in compare with different types of optimization strategies and algorithms. The proposed method is capable to be implemented on a cost effective and minimum electronic equipment to present its real-time process, which will let it to be used on board a light Unmanned Aerial Vehicle (UAV). The advanced adaptation properties and models of neural network, the training of stochastic process and its implementation on DSP outstands an affordable, reliable and low cost AGRS design. The main outcome of the study shows this method increases the quality of curvature information of AGRS data while cost of the algorithm is reduced in each iteration so the proposed ANN-BFGS is a trustworthy appropriate model for Gamma-ray data reconstruction and analysis based on advanced novel artificial intelligence systems.",autonomous vehicle
10.1016/j.knosys.2018.04.015,to_check,Knowledge-Based Systems,scopus,2018-08-01,sciencedirect,Teaching a vehicle to autonomously drift: A data-based approach using Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/85045849458,"This paper presents a novel approach to teach a vehicle how to drift, in a similar manner that professional drivers do. Specifically, a hybrid structure formed by a Model Predictive Controller and feedforward Neural Networks is employed for this purpose. The novelty of this work lies in a) the adoption of a data-based approach to achieve autonomous drifting along a wide range of road radii and body slip angles, and b) in the implementation of a road terrain classifier to adjust the system actuation depending on the current friction characteristics. The presented drift control system is implemented in a multi-actuated ground vehicle equipped with active front steering and in-wheel electric motors and trained to drift by a real test driver using a driver-in-the-loop setup. Its performance is verified in the simulation environment IPG-CarMaker through different open loop and path following drifting manoeuvres.",autonomous vehicle
10.1016/j.compind.2018.03.014,to_check,Computers in Industry,scopus,2018-06-01,sciencedirect,Real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (MEC-ELM),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",autonomous vehicle
10.1109/CNNA.2006.341615,to_check,2006 10th International Workshop on Cellular Neural Networks and Their Applications,IEEE,2006-08-30 00:00:00,ieeexplore,A CNN Implementation of the Horn &amp; Schunck Motion Estimation Method,https://ieeexplore.ieee.org/document/4145855/,"In this paper the parallel implementation of the Horn and Schunck motion estimation method in image sequences is presented, by using cellular neural networks (CNN). One of the drawbacks of the classical motion estimation algorithms is the computational time. The goal of the CNN implementation of the Horn &amp; Schunck method is to increase the efficiency of the well-known classical implementation of this method, which is one of the most used algorithms among the motion estimation techniques. The aim is to obtain a smaller computation time and to include such an algorithm in motion compensation algorithms implemented using CNN, in order to obtain homogeneous algorithms for real-time applications in artificial vision or medical imaging",health
10.1109/DISCOVER50404.2020.9278059,to_check,"2020 IEEE International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics (DISCOVER)",IEEE,2020-10-31 00:00:00,ieeexplore,A Real-Time IoT Based Arrhythmia Classifier Using Convolutional Neural Networks,https://ieeexplore.ieee.org/document/9278059/,"With one in every four deaths that occur every year being due to a heart-related ailment, it is of utmost importance to study the symptoms, features, and cures for heart diseases so that timely action can be taken to detect and prevent fatalities. Arrhythmia is a type of heart ailment where the heart rate is irregular. It is caused due to erratic behavior of the electrical impulses that control the heartbeat. Detection and classification of arrhythmia are conventionally done manually by expert cardiologists through meticulous analysis of the electrocardiogram (ECG) waveform. Automatic and real-time ECG detection and analysis has gained importance in recent years especially due to the accelerating pace of advances in medical technologies. Therefore, the proposed system takes in data from an ECG sensor module (AD8232) and provides it as input to a trained Convolutional Neural Network (CNN) model in real-time. This model is capable of detecting various types of arrhythmias with accuracy greater than 90%. The classification results are then presented to the user through an interactive mobile application. A caretaker is also a part of the system, who is notified if in case the user's condition turns critical. Although a number of arrhythmia classification systems are implemented, the ease of access by the user and the interactiveness is highly limited. The implementation presented in this paper aims at providing an engaging user experience without compromising the performance and accuracy of measurements and predictions.",health
10.1109/TNS.2017.2706061,to_check,IEEE Transactions on Nuclear Science,IEEE,2017-06-01 00:00:00,ieeexplore,A Hardware Implementation of a Brain Inspired Filter for Image Processing,https://ieeexplore.ieee.org/document/7931608/,"A cognitive image processing implementation for pattern-matching execution is proposed in this paper. It is based on the learning process of the human vision as an edge-enhancing filter for medical images. We set up an experiment to test its impact on the performance of decision-making algorithm working on brain magnetic resonance data. The execution times of similar filters can become unpractical on real 3-D or higher dimensional data, if implemented on a CPU. An innovative and high-performance embedded system for real-time pattern matching was developed. The design uses field-programmable gate arrays and the powerful associative memory chip (an ASIC) to achieve real-time performance and requires a training phase and a data acquisition phase. It is a very compact implementation that improves execution time ×1000 for the training phase and ×100 for the data acquisition phase for 2-D black and white images compared to a last generation i7 CPU. The implementation of this edge-enhancing filter is expected to positively impact on medical devices for real-time diagnosis (e.g., diagnostic ultrasound) and for image processing steps in medical image analysis where computing power is a limiting factor.",health
10.1109/STCR51658.2021.9588936,to_check,"2021 Smart Technologies, Communication and Robotics (STCR)",IEEE,2021-10-10 00:00:00,ieeexplore,Detection of Tuberculosis in Radiographs using Deep Learning-based Ensemble Methods,https://ieeexplore.ieee.org/document/9588936/,"Diagnosis of Tuberculosis has witnessed a substantial improvement owing to the infusion of technology in the form of deep learning classification. The transfer learning techniques implemented to date have delivered acceptable results only for academic research. For real-time implementation, the model needs to be generalized and trained on a larger dataset with diverse features. The results can be improved by using combinations of pre-trained models. This paper presents the implementation of Ensemble methods using Stacked Generalization and Weighted Ensemble (Sum of probabilities and Voting Majority) methods using an amalgamation of VGGNet, DenseNet and EfficientNet models, which yield higher accuracies of 95.19% and a significant reduction in Type I and Type II errors. The data-centric approach has been followed to enhance the data quality and volume while the hyperparameters were deemed consistent in the models. A verified and validated approach of model development and data improvisation has resulted in improved performance of the TB classification.",health
10.1109/ICARCV.2014.7064599,to_check,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),IEEE,2014-12-12 00:00:00,ieeexplore,Distributed signature analysis of induction motors using Artificial Neural Networks,https://ieeexplore.ieee.org/document/7064599/,Motor current signature analysis is a modern approach to fault diagnose and classification for induction motors. Many studies reported successful implementation of MCSA in laboratory situations whereas the method was not so successful in real industrial situation due to propagation of neighbor faults and unwanted noise signals. This paper investigate the correlation between different observations of events in order to provide a more accurate estimation of behavior of electrical motors at a given site. An analytical framework has been implemented to correlate and classify independent fault observations and diagnose the type and identify the origin of fault symptoms. The fault diagnosis algorithm has two layers. Initially outputs of all sensors are processed to generate fault indicators. These fault indicators then are to be classified using an Artificial Neural Network. A typical industrial site is taken as a case study and simulated to evaluate the concept of distributed fault analysis.,health
10.1016/j.micpro.2019.102906,to_check,Microprocessors and Microsystems,scopus,2020-02-01,sciencedirect,Area and power efficient pipelined hybrid merged adders for customized deep learning framework for FPGA implementation,https://api.elsevier.com/content/abstract/scopus_id/85073599282,"With the rapid growth of deep learning and neural network algorithms, various fields such as communication, Industrial automation, computer vision system and medical applications have seen the drastic improvements in recent years. However, deep learning and neural network models are increasing day by day, while model parameters are used for representing the models. Although the existing models use efficient GPU for accommodating these models, their implementation in the dedicated embedded devices needs more optimization which remains a real challenge for researchers. Thus paper, carries an investigation of deep learning frameworks, more particularly as review of adders implemented in the deep learning framework. A new pipelined hybrid merged adders (PHMAC) optimized for FPGA architecture which has more efficient in terms of area and power is presented. The proposed adders represent the integration of the principle of carry select and carry look ahead principle of adders in which LUT is re-used for the different inputs which consume less power and provide effective area utilization. The proposed adders were investigated on different FPGA architectures in which the power and area were analyzed. Comparison of the proposed adders with the other adders such as carry select adders (CSA), carry look ahead adder (CLA), Carry skip adders and Koggle Stone adders has been made and results have proved to be highly vital into a 50% reduction in the area, power and 45% when compared with above mentioned traditional adders.",health
10.1016/j.ijleo.2019.04.034,to_check,Optik,scopus,2019-05-01,sciencedirect,Concealed object segmentation in terahertz imaging via adversarial learning,https://api.elsevier.com/content/abstract/scopus_id/85064317470,"Terahertz imaging (frequency between 0.1 to 10 THz) is a modern technique for public security check. Due to poor imaging quality, traditional machine vision methods often fail to detect concealed weapons in Terahertz samples, while modern instance segmentation approaches have complex multiple-stage concatenation and often hunger for massive and accurate training data. In this work, we realize a novel Conditional Generative Adversarial Nets (CGANs), named as Mask-CGANs to segment weapons in such a challenging imaging quality. The Mask-Generator network employs a “selected-connection U-Net” to restrain false alarms and speed up training convergence. The loss function takes reconstruction errors and sparse priors into consideration to preserve precise segmentation. Such a learning architecture works well with a small training dataset. Experiments show that the proposed model outperforms CGANs (more than 16–32% in Recall, Precision and Accuracy) and Mask-RCNN (more than 3–6%). Moreover, its testing speed (69.7 FPS) is fast enough to be implemented in a real-time security check system, which is 44 times faster than Mask-RCNN. In the experiments for mammographic mass segmentation on INBreast dataset, the Dice index of the proposed method is 91.29, surpasses the-state-of-the-art medical issue segmentation methods. The full implementation (based on TensorFlow) is available at https://github.com/JXPanzz/THz).",health
10.1016/j.jbi.2019.103138,to_check,Journal of Biomedical Informatics,scopus,2019-04-01,sciencedirect,Distributed learning from multiple EHR databases: Contextual embedding models for medical events,https://api.elsevier.com/content/abstract/scopus_id/85062392033,"Electronic health record (EHR) data provide promising opportunities to explore personalized treatment regimes and to make clinical predictions. Compared with regular clinical data, EHR data are known for their irregularity and complexity. In addition, analyzing EHR data involves privacy issues and sharing such data is often infeasible among multiple research sites due to regulatory and other hurdles. A recently published work uses contextual embedding models and successfully builds one predictive model for more than seventy common diagnoses. Despite of the high predictive power, the model cannot be generalized to other institutions without sharing data. In this work, a novel method is proposed to learn from multiple databases and build predictive models based on Distributed Noise Contrastive Estimation (Distributed NCE). We use differential privacy to safeguard the intermediary information sharing. The numerical study with a real dataset demonstrates that the proposed method not only can build predictive models in a distributed manner with privacy protection, but also preserve model structure well and achieve comparable prediction accuracy. The proposed methods have been implemented as a stand-alone Python library and the implementation is available on Github (https://github.com/ziyili20/DistributedLearningPredictor) with installation instructions and use-cases.",health
10.1016/j.artmed.2015.09.006,to_check,Artificial Intelligence in Medicine,scopus,2018-11-01,sciencedirect,Pediatric decision support using adapted Arden Syntax,https://api.elsevier.com/content/abstract/scopus_id/84964998606,"Background
                  Pediatric guidelines based care is often overlooked because of the constraints of a typical office visit and the sheer number of guidelines that may exist for a patient's visit. In response to this problem, in 2004 we developed a pediatric computer based clinical decision support system using Arden Syntax medical logic modules (MLM).
               
                  Methods
                  The Child Health Improvement through Computer Automation system (CHICA) screens patient families in the waiting room and alerts the physician in the exam room. Here we describe adaptation of Arden Syntax to support production and consumption of patient specific tailored documents for every clinical encounter in CHICA and describe the experiments that demonstrate the effectiveness of this system.
               
                  Results
                  As of this writing CHICA has served over 44,000 patients at 7 pediatric clinics in our healthcare system in the last decade and its MLMs have been fired 6182,700 times in “produce” and 5334,021 times in “consume” mode. It has run continuously for over 10 years and has been used by 755 physicians, residents, fellows, nurse practitioners, nurses and clinical staff. There are 429 MLMs implemented in CHICA, using the Arden Syntax standard. Studies of CHICA's effectiveness include several published randomized controlled trials.
               
                  Conclusions
                  Our results show that the Arden Syntax standard provided us with an effective way to represent pediatric guidelines for use in routine care. We only required minor modifications to the standard to support our clinical workflow. Additionally, Arden Syntax implementation in CHICA facilitated the study of many pediatric guidelines in real clinical environments.",health
10.1109/AIMS52415.2021.9466068,to_check,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),IEEE,2021-04-30 00:00:00,ieeexplore,Implementation of Cloud Based Action Recognition Backend Platform,https://ieeexplore.ieee.org/document/9466068/,"The Internet of Things (IoT) growth are rapidly in various fields such as industry 4.0, smart cities, and smart homes. Implementation of IoT for electronic assistance had been researched to increase the longevity of human life. However, not all IoT implementation as human life assistance provides action recognition monitoring on multiple elderly people, provide information such as real-time action monitoring, and real-time streaming in a mobile application. Therefore, this research intends to create a system that can receive and provide information on each elderly people who registered. The Action Recognition Backend Platform will be working as cloud computing to receive and manage input data from Edge Computing Action Recognition. This platform integrated Deep Learning, Data Analytics, Big Data Warehouse that implemented Extract, Transform, and Load (ETL) methods, communication services with MQTT, and Kafka Streaming Processor. The test result showed that the edge computing action recognition got better model accuracy performance from our last model [1], which can predict with 50,7% accuracy in 0.5 confidence threshold. Moreover, the backend platform had been successfully implemented a simple IoT paradigm and got an average delivery time of MQTT communication at 204ms, for streaming data process took an average delay of 680ms.",industry
10.1109/YAC.2018.8406429,to_check,2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC),IEEE,2018-05-20 00:00:00,ieeexplore,Wheel classification using convolutional neural networks,https://ieeexplore.ieee.org/document/8406429/,"With the fast development of automobile wheel industry, many methods of wheel classification have been proposed. The appearance of wheel changes a lot during the process of production, but few of these conventional methods is designed to handle the many challenges of wheel classification for different appearances. This paper studies on wheel classification for different appearances during the process of production, and proposes a wheel classification method using convolutional neural network. Firstly, we implement circle detection, gray value analysis and size normalization on wheel images and collect ten common types of wheel to build a dataset. Based on the features of the wheel images, a convolutional neural network is proposed. Then data augmentation is implemented on the dataset and local response normalization layer is added in the convolutional neural network. The experiment results show that the presented method has a better performance both in classification accuracy and real-time requirement than conventional methods and achieves 95.6% accuracy on wheel classification in the dataset.",industry
10.1109/INFOCT.2018.8356831,to_check,2018 International Conference on Information and Computer Technologies (ICICT),IEEE,2018-03-25 00:00:00,ieeexplore,Fault class prediction in unsupervised learning using model-based clustering approach,https://ieeexplore.ieee.org/document/8356831/,"Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",industry
10.1109/ICIEAM.2019.8742984,to_check,"2019 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",IEEE,2019-03-29 00:00:00,ieeexplore,Objects Geometry Comparative Analysis Method for Industrial Robot Vision System,https://ieeexplore.ieee.org/document/8742984/,"At present, in computer vision systems, neural networks are used to process information received by the system from cameras. The recognition of all objects on the image is an extremely resource-intensive task, the solution of which consumes most of the computing power. For that reason, systems based on neural networks cannot be fully utilized for real-time systems due to limited computing resources. To build real-time computer vision systems, the authors suggested using the contour comparison method. The method allows to supervise the geometry of objects, conduct presorting and screen out defective parts, thereby the pressure on neural networks will reduce. The method is implemented in the Java. The created software performs image processing and objects search on it, that are the most similar to the template. The results of the experiment showed that the desired object is correctly determined on a noisy image and the proposed method can be used to solve the problem of pattern recognition in technical vision systems.",industry
10.1109/ICIT.2006.372319,to_check,2006 IEEE International Conference on Industrial Technology,IEEE,2006-12-17 00:00:00,ieeexplore,Real Time Classifier For Industrial Wireless Sensor Network Using Neural Networks with Wavelet Preprocessors,https://ieeexplore.ieee.org/document/4237641/,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",industry
10.1109/RTSS.2009.14,to_check,2009 30th IEEE Real-Time Systems Symposium,IEEE,2009-12-04 00:00:00,ieeexplore,Component-Based Abstraction Refinement for Timed Controller Synthesis,https://ieeexplore.ieee.org/document/5368182/,"We present a novel technique for synthesizing controllers for distributed real-time environments with safety requirements. Our approach is an abstraction refinement extension to the on-the-fly algorithm by Cassez et al. from 2005. Based on partial compositions of some environment components, each refinement cycle constructs a sound abstraction that can be used to obtain under- and over-approximations of all valid controller implementations. This enables (1) early termination if an implementation does not exist in the over-approximation, or, if one does exist in the under-approximation, and (2) pruning of irrelevant moves in subsequent refinement cycles. In our refinement loop, the precision of the abstractions incrementally increases and converges to all specification-critical components. We implemented our approach in a prototype synthesis tool and evaluated it on an industrial benchmark. In comparison with the timed game solver UPPAAL-Tiga, our technique outperforms the nonincremental approach by an order of magnitude.",industry
10.1109/ISIE.2010.5636556,to_check,2010 IEEE International Symposium on Industrial Electronics,IEEE,2010-07-07 00:00:00,ieeexplore,Real-time evaluation of power quality using FPGA based measurement system,https://ieeexplore.ieee.org/document/5636556/,"Real-time evaluation of power quality is a desired feature in research and industrial projects, especially when embedded systems are employed and/or studied. Fast Fourier Transforms FFT is commonly used to evaluate the harmonic content of electric signals. Artificial Neural Networks (ANN) are also employed for harmonics estimation with short processing time and low implementation complexity. Commercial power quality measurement systems are available and offer good performance, communication and storage capabilities, and other special features, however in most of them the real-time information is not available or it is offered with important communication delays. This paper presents the implementation of a measurement system using Xilinx FPGA target and the Adaptive Linear Neuron (ADALINE) algorithm for real-time evaluation of power quality. Experimental results show that the implemented system can be employed for power quality monitoring and embedded control applications.",industry
10.1016/j.livsci.2021.104700,to_check,Livestock Science,scopus,2021-11-01,sciencedirect,A review of deep learning algorithms for computer vision systems in livestock,https://api.elsevier.com/content/abstract/scopus_id/85118744270,"In livestock operations, systematically monitoring animal body weight, biometric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves massive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phenotypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and the respective deep learning algorithms implemented in Animal Science studies. Then, we reviewed the published research studies in Animal Science which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an improved predictive ability in livestock applications and a faster inference. However, only a few articles fully described the deep learning algorithms and their implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missing. We summarized peer-reviewed articles by computer vision tasks (image classification, object detection, and object segmentation), deep learning algorithms, animal species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions.",industry
10.1016/j.cja.2020.09.011,to_check,Chinese Journal of Aeronautics,scopus,2021-06-01,sciencedirect,Framework and development of data-driven physics based model with application in dimensional accuracy prediction in pocket milling,https://api.elsevier.com/content/abstract/scopus_id/85097765922,"In the manufacturing of thin wall components for aerospace industry, apart from the side wall contour error, the Remaining Bottom Thickness Error (RBTE) for the thin-wall pocket component (e.g. rocket shell) is of the same importance but overlooked in current research. If the RBTE reduces by 30%, the weight reduction of the entire component will reach up to tens of kilograms while improving the dynamic balance performance of the large component. Current RBTE control requires the off-process measurement of limited discrete points on the component bottom to provide the reference value for compensation. This leads to incompleteness in the remaining bottom thickness control and redundant measurement in manufacturing. In this paper, the framework of data-driven physics based model is proposed and developed for the real-time prediction of critical quality for large components, which enables accurate prediction and compensation of RBTE value for the thin wall components. The physics based model considers the primary root cause, in terms of tool deflection and clamping stiffness induced Axial Material Removal Thickness (AMRT) variation, for the RBTE formation. And to incorporate the dynamic and inherent coupling of the complicated manufacturing system, the multi-feature fusion and machine learning algorithm, i.e. kernel Principal Component Analysis (kPCA) and kernel Support Vector Regression (kSVR), are incorporated with the physics based model. Therefore, the proposed data-driven physics based model combines both process mechanism and the system disturbance to achieve better prediction accuracy. The final verification experiment is implemented to validate the effectiveness of the proposed method for dimensional accuracy prediction in pocket milling, and the prediction accuracy of AMRT achieves 0.014 mm and 0.019 mm for straight and corner milling, respectively.",industry
10.1016/j.aei.2020.101136,to_check,Advanced Engineering Informatics,scopus,2020-10-01,sciencedirect,Ensemble deep learning based semi-supervised soft sensor modeling method and its application on quality prediction for coal preparation process,https://api.elsevier.com/content/abstract/scopus_id/85087393963,"Coal preparation is the most effective and economical technique to reduce impurities and improve the product quality for run-of-mine coal. The timely and accurate prediction for key quality characteristics of separated coal plays a significant role in condition monitoring and production control. However, these quality characteristics are usually difficult to directly measure online in industrial practices Although some computation intelligence based soft sensor modeling methods have been developed and reported in existing research for these quality variables estimation, some problems still exist, i.e., manual feature extraction, considerable unlabeled data, temporal dynamic behavior in data, which will influence the accuracy and efficiency for established soft sensor model. To address above-mentioned problem and develop an more excellent quality prediction model for coal preparation process, a novel deep learning based semi-supervised soft sensor modeling approach is proposed which combining the advantage of unsupervised deep learning technique (i.e., Stacked Auto-Encoder (SAE)) with the advantage of supervised deep bidirectional recurrent learner (i.e., Bidirectional Long Short-Term Memory (BLSTM)). More specifically, the unsupervised SAE networks are implemented to learn the representative features hidden in all available input data (labeled and unlabeled samples) and store them as context vector. Then, partial context vector with corresponding labels and the quality variable measure value at previous time are concatenated to form a new merged input feature vector. After that, the temporal and dynamic features are further extracted from the new merged input feature vector via BLSTM networks. Subsequently, the fully connected layers (FCs) are exploited to learn the higher-level features from the last hidden layer of the BLSTM. Lastly, the learned output features by FCs are fed into a supervised liner regression layer to predict the coal quality metrics. Meanwhile, to avoid over-fitting, some regularization techniques are utilized and discussed in proposed network. The application in ash content estimation for a real dense medium coal preparation process and some comparison experiment result demonstrate that the effectiveness and priority of proposed soft sensor modeling approach.",industry
10.1016/j.talanta.2019.120664,to_check,Talanta,scopus,2020-04-01,sciencedirect,Modelling of bioprocess non-linear fluorescence data for at-line prediction of etanercept based on artificial neural networks optimized by response surface methodology,https://api.elsevier.com/content/abstract/scopus_id/85076829838,"In the last years, regulatory agencies in biopharmaceutical industry have promoted the design and implementation of Process Analytical Technology (PAT), which aims to develop rapid and high-throughput strategies for real-time monitoring of bioprocesses key variables, in order to improve their quality control lines. In this context, spectroscopic techniques for data generation in combination with chemometrics represent alternative analytical methods for on-line critical process variables prediction. In this work, a novel multivariate calibration strategy for the at-line prediction of etanercept, a recombinant protein produced in a mammalian cells-based perfusion process, is presented. For data generation, samples from etanercept processes were daily obtained, from which fluorescence excitation-emission matrices were generated in the spectral ranges of 225.0 and 495.0 nm and 250.0 and 599.5 nm for excitation and emission modes, respectively. These data were correlated with etanercept concentration in supernatant (measured by an off-line HPLC-based reference univariate technique) by implementing different chemometric strategies, in order to build predictive models. Partial least squares (PLS) regression evidenced a non-linear relation between signal and concentration when observing actual vs. predicted concentrations. Hence, a non-parametric approach was implemented, based on a multilayer perceptron artificial neural network (MLP). The MLP topology was optimized by means of the response surface methodology. The prediction performance of MLP model was superior to PLS, since the first is able to cope with non-linearity in calibration models, reaching percentage mean relative error in predictions of about 7.0% (against 12.6% for PLS). This strategy represents a fast and inexpensive approach for etanercept monitoring, which conforms the principles of PAT.",industry
10.1016/j.future.2019.04.014,to_check,Future Generation Computer Systems,scopus,2019-10-01,sciencedirect,TIDE: Time-relevant deep reinforcement learning for routing optimization,https://api.elsevier.com/content/abstract/scopus_id/85065443852,"Routing optimization has been researched in network design for a long time, and plenty of optimization schemes have been proposed from both academia and industry. However, such schemes are either too complicated in applications or far from the optimal performance. In recent years, with the development of Software-defined Networking (SDN) and Artificial Intelligence (AI), AI-based methods of routing strategy are being considered. In this paper, we propose TIDE, an intelligent network control architecture based on deep reinforcement learning that can dynamically optimize routing strategies in an SDN network without human experience. TIDE is implemented and validated on a real network environment. Experiment result shows that TIDE can adjust the routing strategy dynamically according to the network condition and can improve the overall network transmitting delay by about 9% compared with traditional algorithms.",industry
10.1016/j.ifacol.2019.08.225,to_check,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Curriculum change for graduate-level control engineering education at the Universidad Pontificia Bolivariana,https://api.elsevier.com/content/abstract/scopus_id/85076258553,"This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.",industry
10.1109/EuCAP.2016.7481737,to_check,2016 10th European Conference on Antennas and Propagation (EuCAP),IEEE,2016-04-15 00:00:00,ieeexplore,Integrating composite urban furniture into ray-tracing simulator for 5G small cells and outdoor device-to-device communications,https://ieeexplore.ieee.org/document/7481737/,"Small cells and device-to-device (D2D) communications will play a vital role in the realization of the next generation of wireless mobile communication systems (IMT-2020 or 5G). Both of them involve transmitters and receivers less than 10 m high inducing a more complex urban environment. This paper presents a framework of integrating composite urban furniture, e.g., traffic signs, traffic lights, etc., into ray-tracing tools. The framework starts with the theoretical modeling for radar cross section of furniture components and validation of full-wave analysis simulation in the far field. Then, in order to locally fulfill the far field condition in the small cells or D2D scenarios, the furniture is divided into small segments so that the models in the far field are still applicable. Finally, the decomposed furniture is implemented in a ray-tracing tool and validated by measurements in real scenarios. Under this framework, researchers can improve the ray-tracing tools with more essential elements of urban environment. Last but not least, this paper provides a case study to demonstrate the implementation of the framework, and the results show that the traffic signs indeed influence the vehicle-to-vehicle communications, which is one of the most frequently occurring applications of outdoor D2D systems.",smart cities
10.1109/IJCNN.2014.6889658,to_check,2014 International Joint Conference on Neural Networks (IJCNN),IEEE,2014-07-11 00:00:00,ieeexplore,Long-term learning behavior in a recurrent neural network for sound recognition,https://ieeexplore.ieee.org/document/6889658/,"In this paper, the long-term learning properties of an artificial neural network model, designed for sound recognition and computational auditory scene analysis in general, are investigated. The model is designed to run for long periods of time (weeks to months) on low-cost hardware, used in a noise monitoring network, and builds upon previous work by the same authors. It consists of three neural layers, connected to each other by feedforward and feedback excitatory connections. It is shown that the different mechanisms that drive auditory attention emerge naturally from the way in which neural activation and intra-layer inhibitory connections are implemented in the model. Training of the artificial neural network is done following the Hebb principle, dictating that ""Cells that fire together, wire together"", with some important modifications, compared to standard Hebbian learning. As the model is designed to be on-line for extended periods of time, also learning mechanisms need to be adapted to this. The learning needs to be strongly attention- and saliency-driven, in order not to waste available memory space for sounds that are of no interest to the human listener. The model also implements plasticity, in order to deal with new or changing input over time, without catastrophically forgetting what it already learned. On top of that, it is shown that also the implementation of short-term memory plays an important role in the long-term learning properties of the model. The above properties are investigated and demonstrated by training on real urban sound recordings.",smart cities
10.1109/HASE.2012.33,to_check,2012 IEEE 14th International Symposium on High-Assurance Systems Engineering,IEEE,2012-10-27 00:00:00,ieeexplore,An Autonomic Reliability Improvement System for Cyber-Physical Systems,https://ieeexplore.ieee.org/document/6375637/,"System reliability is a fundamental requirement of cyber-physical systems. Unreliable systems can lead to disruption of service, financial cost and even loss of human life. Typical cyber-physical systems are designed to process large amounts of data, employ software as a system component, run online continuously and retain an operator-in-the-loop because of human judgment and accountability requirements for safety-critical systems. This paper describes a data-centric runtime monitoring system named ARIS (Autonomic Reliability Improvement System) for improving the reliability of these types of cyber-physical systems. ARIS employs automated online evaluation, working in parallel with the cyber-physical system to continuously conduct automated evaluation at multiple stages in the system workflow and provide real-time feedback for reliability improvement. This approach enables effective evaluation of data from cyber-physical systems. For example, abnormal input and output data can be detected and flagged through data quality analysis. As a result, alerts can be sent to the operator-in-the-loop, who can then take actions and make changes to the system based on these alerts in order to achieve minimal system downtime and higher system reliability. We have implemented ARIS in a large commercial building cyber-physical system in New York City, and our experiment has shown that it is effective and efficient in improving building system reliability.",smart cities
10.1109/TENSYMP52854.2021.9550904,to_check,2021 IEEE Region 10 Symposium (TENSYMP),IEEE,2021-08-25 00:00:00,ieeexplore,Deep Learning based Smart Parking for a Metropolitan Area,https://ieeexplore.ieee.org/document/9550904/,"In this study, we have introduced a method for utilizing the maximum parking space available for a metropolitan city. This will result in much lesser traffic congestion due to street-side parking. Furthermore, it will also decrease the hassle drivers face when they have to leave their vehicles on the side of the road to do other activities. The method introduces a Deep Learning based system where parking spaces are detected using Data Capturing Units (DCU). These DCUs feed data into our database which can be accessed by the users from our mobile application. The users can book parking spaces accordingly. All these data are saved in real-time and can be accessed through the mobile application. A vehicle classification system has also been designed that achieves an accuracy of 77% from multiple vehicle classes. Furthermore, a number plate recognition system has been used for the identification and safety protocols of the vehicles in parking sites. The number plate identification system is very precise and achieves an accuracy of over 90% for each digit. To the best of our knowledge, no other system of this kind has been implemented for the city of Dhaka before this. On top of that, successful implementation in a hectic city like Dhaka implies that it can be applied anywhere in the world. We believe this system can have a huge impact in reducing traffic congestions and can save an endless measure of time and money for citizens in a metropolitan area.",smart cities
10.1109/VLSID51830.2021.00035,to_check,2021 34th International Conference on VLSI Design and 2021 20th International Conference on Embedded Systems (VLSID),IEEE,2021-02-24 00:00:00,ieeexplore,Binary neural network based real time emotion detection on an edge computing device to detect passenger anomaly,https://ieeexplore.ieee.org/document/9407373/,"Passenger safety in public transportation especially while riding in the form of shared cabs, and taxis are often ignored, and not much preventive protocols are devised. In the connected mobility world, emotion recognition from facial expressions is a possibility, however a faster processing and edge computing device to derive anomaly state inferences will be apt for further notifying about the safety of the passenger. FPGA implementation is a viable approach to not only implement in the embedded system automotive electronics, but also accelerate the inference results, hence making it as an ideal real time candidate for passenger anomaly state identification. For the same, a real time emotion detection system using facial features was implemented on FPGA. A Binary Neural Network (BNN) feeded by Local Binary Pattern (LBP) output was designed towards the development of an improved and faster emotion recognition system. LBP is configured as a preprocessing step to extract facial features that is passed on to the BNN layer for successful inference. The preprocessing method utilizes Viola-Jones (VJ) algorithm to extract facial data while removing other background information from the image. The LBP-BNN network is modelled using Facial Expression 2013 (FER-2013) data set for training. The custom hardware accelerator or the overlay is synthesized and the designed IP is implemented on FPGA for the inference. Inference is done using the trained model on FPGA to enable faster classified results. Emotion detection using facial expressions is classified to six states namely: angry, disgust, fear, happy, sad, and surprise. The LBP-BNN network is implemented in FPGA, to realize a real time facial emotion recognition by capturing the image of a person from a web camera interfaced to the FPGA acting as edge computing inference device, with acceptable accuracy. The image processing based emotion detection design is highly suitable for other applications including tracking of emotions for movement disorder patients in hospitals.",smart cities
10.1109/NEUREL.2014.7011506,to_check,12th Symposium on Neural Network Applications in Electrical Engineering (NEUREL),IEEE,2014-11-27 00:00:00,ieeexplore,Integration of signal prediction service in Service Oriented Architecture,https://ieeexplore.ieee.org/document/7011506/,"Smart transducers based on the IEEE 1451 standard provide platform for development of distributed measurement and control systems. Integration of IEEE 1451 services based on Service Oriented Architecture (SOA) enables simple implementation, interoperability, scalability of distributed system components, accessible in the form of network services. Addressing the transport latency and packet loss in distributed systems, we propose the concept of algorithms' and processes' implementation in the form of dislocated network services. Introducing the signal data paths, time-stamped data and time synchronization mechanism, we provide the support for real-time system characteristics. Each component on the data path is able to analyze the packets of time-stamped information, extract an adequate sample set, and apply this set to the input of the implemented real-time algorithm or actuator. Proper management of the time-stamped information along data path enables real-time actions despite the network propagation delay or other systems' latencies. In this paper, the presented architecture is used for integration of the signal prediction service in smart transducers network based on SOA. Purpose of the service is to predict the values of arbitrary signals in order to compensate network delay and packet loss. Two realizations of the signal prediction service, based on neural networks are given: feed-forward and feedback predictive network. The system containing embedded IEEE 1451 smart transducer module for relative humidity measurement and signal prediction service on the general purpose platform is used for the experimental verification of the proposed concept.",smart cities
10.1109/ICCITECHN.2017.8281786,to_check,2017 20th International Conference of Computer and Information Technology (ICCIT),IEEE,2017-12-24 00:00:00,ieeexplore,A feature based method for real time vehicle detection and classification from on-road videos,https://ieeexplore.ieee.org/document/8281786/,"Vision Based vehicle detection and classification has become an active area of research for intelligent transportation system. But this task is very difficult and challenging due to the dynamic condition of roads. Many solutions have been given by the researchers to overcome these challenges. Some of them are giving good performance but computationally highly expensive and fail in some circumstances. In the proposed method, a feature based cost effective detection and classification method is proposed that is suitable for real time applications, provide satisfactory accuracy and computationally cheap. The proposed method uses haar-like features of the images and AdaBoost classifier for detection which provides a very fast detection rate with high accuracy. To reduce inconsiderable false positive rate generated by this method, we propose to use two virtual detection lines (VDL) that reduces the false positive rate. In order to predict the class of a vehicle, a feature based method is proposed. HOG, SIFT, SURF all are well represented feature for image classification. The existing feature based vehicle classification methods lack accuracy because of using those features inefficiently. In order to reduce those lacking, we propose to use bag of visual words (BOVW) model for classification. BOVW model also needs a lower computation time and resources. As the proposed method aims to be implemented in real time, we propose to use SURF feature for BOVW which is faster to compute and well described for recognizing an object. The BOVW then used for identifying the vehicle's class by multi class SVM classifier. Error correcting output code (ECOC) framework is used to achieve multi class prediction with SVM. Extensive experiments have been carried out on different traffic data of varying environments to evaluate the detection and classification performance of the proposed method. Experiment results demonstrate that the proposed method achieves a significant improvement in classification of heterogeneous vehicles in terms of accuracy with a considerable execution time as compared to other methods.",smart cities
http://arxiv.org/abs/1910.05765v1,to_check,arxiv,arxiv,2019-10-13 15:01:56+00:00,arxiv,"Real-Time and Embedded Deep Learning on FPGA for RF Signal
  Classification",http://arxiv.org/abs/1910.05765v1,"We designed and implemented a deep learning based RF signal classifier on the
Field Programmable Gate Array (FPGA) of an embedded software-defined radio
platform, DeepRadio, that classifies the signals received through the RF front
end to different modulation types in real time and with low power. This
classifier implementation successfully captures complex characteristics of
wireless signals to serve critical applications in wireless security and
communications systems such as identifying spoofing signals in signal
authentication systems, detecting target emitters and jammers in electronic
warfare (EW) applications, discriminating primary and secondary users in
cognitive radio networks, interference hunting, and adaptive modulation.
Empowered by low-power and low-latency embedded computing, the deep neural
network runs directly on the FPGA fabric of DeepRadio, while maintaining
classifier accuracy close to the software performance. We evaluated the
performance when another SDR (USRP) transmits signals with different modulation
types at different power levels and DeepRadio receives the signals and
classifies them in real time on its FPGA. A smartphone with a mobile app is
connected to DeepRadio to initiate the experiment and visualize the
classification results. With real radio transmissions over the air, we show
that the classifier implemented on DeepRadio achieves high accuracy with low
latency (microsecond per sample) and low energy consumption (microJoule per
sample), and this performance is not matched by other embedded platforms such
as embedded graphics processing unit (GPU).",smart cities
http://arxiv.org/abs/2108.12118v1,to_check,arxiv,arxiv,2021-08-27 04:58:45+00:00,arxiv,"Densely-Populated Traffic Detection using YOLOv5 and Non-Maximum
  Suppression Ensembling",http://arxiv.org/abs/2108.12118v1,"Vehicular object detection is the heart of any intelligent traffic system. It
is essential for urban traffic management. R-CNN, Fast R-CNN, Faster R-CNN and
YOLO were some of the earlier state-of-the-art models. Region based CNN methods
have the problem of higher inference time which makes it unrealistic to use the
model in real-time. YOLO on the other hand struggles to detect small objects
that appear in groups. In this paper, we propose a method that can locate and
classify vehicular objects from a given densely crowded image using YOLOv5. The
shortcoming of YOLO was solved my ensembling 4 different models. Our proposed
model performs well on images taken from both top view and side view of the
street in both day and night. The performance of our proposed model was
measured on Dhaka AI dataset which contains densely crowded vehicular images.
Our experiment shows that our model achieved mAP@0.5 of 0.458 with inference
time of 0.75 sec which outperforms other state-of-the-art models on
performance. Hence, the model can be implemented in the street for real-time
traffic detection which can be used for traffic control and data collection.",smart cities
http://arxiv.org/abs/2007.11404v1,to_check,arxiv,arxiv,2020-07-21 07:11:27+00:00,arxiv,"A Hybrid Neuromorphic Object Tracking and Classification Framework for
  Real-time Systems",http://arxiv.org/abs/2007.11404v1,"Deep learning inference that needs to largely take place on the 'edge' is a
highly computational and memory intensive workload, making it intractable for
low-power, embedded platforms such as mobile nodes and remote security
applications. To address this challenge, this paper proposes a real-time,
hybrid neuromorphic framework for object tracking and classification using
event-based cameras that possess properties such as low-power consumption (5-14
mW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches
of using event-by-event processing, this work uses a mixed frame and event
approach to get energy savings with high performance. Using a frame-based
region proposal method based on the density of foreground events, a
hardware-friendly object tracking scheme is implemented using the apparent
object velocity while tackling occlusion scenarios. The object track input is
converted back to spikes for TrueNorth classification via the energy-efficient
deep network (EEDN) pipeline. Using originally collected datasets, we train the
TrueNorth model on the hardware track outputs, instead of using ground truth
object locations as commonly done, and demonstrate the ability of our system to
handle practical surveillance scenarios. As an optional paradigm, to exploit
the low latency and asynchronous nature of neuromorphic vision sensors (NVS),
we also propose a continuous-time tracker with C++ implementation where each
event is processed individually. Thereby, we extensively compare the proposed
methodologies to state-of-the-art event-based and frame-based methods for
object tracking and classification, and demonstrate the use case of our
neuromorphic approach for real-time and embedded applications without
sacrificing performance. Finally, we also showcase the efficacy of the proposed
system to a standard RGB camera setup when evaluated over several hours of
traffic recordings.",smart cities
10.1016/j.compenvurbsys.2014.11.005,to_check,"Computers, Environment and Urban Systems",scopus,2015-03-01,sciencedirect,A location aware system for integrated management of Rhynchophorus ferrugineus in urban systems,https://api.elsevier.com/content/abstract/scopus_id/84949118217,"The red palm weevil (RPW), Rhynchophorus ferrugineus (Coleoptera: Curculionidae), an invasive species for many countries, is one of the most dangerous pests of ornamental palms in urban landscapes. When infested by the RPW, all untreated palms typically die. RPW monitoring is difficult, and when the pest is detected, management to save the infested palm becomes intricate. For this reason, an efficient and integrated RPW monitoring, such as a location-aware system (LAS), combined with the scientific knowledge of experts on the palm’s physiology and the RPW biological cycle must be designed, developed and implemented. In the current study, an innovative integrated RPW management in urban landscapes under real conditions is presented. Based on the LAS, this study investigates the effectiveness of monitoring RPW infestations in palms, such as the Phoenix canariensis, under field conditions at the Pedion Areos park, which is located in central Athens, the capital of the Hellenic Republic. The goal of this study is to address the specific needs of RPW management and control by facilitating the treatment process adopted by experts combined with an appropriate spatial decision support system (SDSS), which accounts for the RPW population dynamics and spatio-temporal characteristics of the infested areas. The process of estimating the infestation risk is based on a ten-point scale classification, which is incorporated into the LAS. The results from the aforementioned moderate-scaled field experiment conducted for evaluation purposes showed that the LAS is an innovative, simple, and easy to use integrated platform that can be used for early detection, rapid monitoring and assessment of RPW in urban landscapes. Finally, the results from this four-year experiment showed that the diameter and height of the trunk of canary palms play a significant role in the susceptibility to RPW infestations and the recovery of infested canary palms; the results also demonstrated that control of RPW in urban landscapes is possible.",smart cities
10.1109/IEMBS.1997.757650,to_check,Proceedings of the 19th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 'Magnificent Milestones and Emerging Opportunities in Medical Engineering' (Cat. No.97CH36136),IEEE,1997-11-02 00:00:00,ieeexplore,Two dimensional interleaved differential k-space sampling for fluoroscopic triggering of contrast-enhanced three dimensional MR angiography,https://ieeexplore.ieee.org/document/757650/,"The purpose of this work is to demonstrate how 2D differential k-space sampling can be implemented with interleaving of the view (phase encoding) order, providing smoother temporal behavior and no spatial resolution penalty when compared to standard sequential k-space ordering. Implementation in a clinical setting provides a method for real-time bolus tracking and timing for contrast-enhanced 3D MR angiography.",multimedia
10.1109/ICASSP.1996.550796,to_check,"1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings",IEEE,1996-05-09 00:00:00,ieeexplore,A probabilistic decision-based neural network for locating deformable objects and its applications to surveillance system and video browsing,https://ieeexplore.ieee.org/document/550796/,"Detection of a (deformable) pattern or object is an important machine learning and computer vision problem. The task involves finding specific (but locally deformable) patterns in images, such as human faces and eyes/mouths. There are many important commercial applications. This paper presents a decision-based neural network for finding such patterns with specific applications to detecting human faces and locating eyes in the faces. The system built upon the proposal has been demonstrated to be applicable under reasonable variations of orientation and/or lighting, and with the possibility of eye glasses. This method has been shown to be very robust against a large variation of face features and eye shapes. The algorithm takes only 200 ms on a SUN Sparc20 workstation to find human faces in an image with 320/spl times/240 pixels. For a facial image with 320/spl times/240 pixels, the algorithm takes 500 ms to locate two eyes on a SUN Sparc20 workstation. Furthermore, the algorithm can be easily implemented via specialised hardware for real time performance. We have applied this technique to two applications (surveillance system, video browsing) and this paper provides experimental results. Although we have only shown its successful implementation on face detection and eye localization, the proposed technique is meant for more general applications of detection of any (locally deformable) object.",multimedia
10.1109/ICCES.2017.8275360,to_check,2017 12th International Conference on Computer Engineering and Systems (ICCES),IEEE,2017-12-20 00:00:00,ieeexplore,High performance computing model for action localization in video,https://ieeexplore.ieee.org/document/8275360/,"Last decade, the field of High Performance Computing (HPC) has been rapidly developing. This advances in computational power and hardware resources motivated real-time video application. One of the most challenging application is “action detection” inside video. Beside its tremendous complication, it requires an intensive computation power for video analysis. This work extends our previous work in this problem by addressing both multi-core CPUs and Graphical Processing Unit (GPU) for implementing the video action localization system (previous work). In this paper, multiple (HPC) architectures are implemented to speedup real-time video streaming analytics. The experiments are conducted on the previously implemented recognition system which exploits Deep Learning models to localize an action inside a video. The results illustrates that CUDA implementation accomplished 8x speedup while CPU accomplished 5.2x speedup. The experiments were done on Sports-IM dataset with 487 action.",multimedia
10.1109/ACCESS.2021.3077499,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,A Surveillance Video Real-Time Analysis System Based on Edge-Cloud and FL-YOLO Cooperation in Coal Mine,https://ieeexplore.ieee.org/document/9422766/,"Video monitoring is an important means to ensure production safety in coal mine. However, the currently intelligent video surveillance is difficult to respond in real-time due to the latency of cloud computing. In this paper, a cloud-edge cooperation framework is proposed, which integrates cloud computing and edge computing in a coordinated manner. The cloud computing is used to process non-real-time and global tasks, while the edge computing is responsible for handling local monitoring video in real-time. In order to realize cloud-edge data interaction and online optimization for edge models, the heterogeneous converged network is built. In addition, an object detection model FL-YOLO composed of depthwise separable convolution and down-sampling inverted residual block is proposed, which realizes real-time video analysis at the edge. Finally, this paper discusses the complexity of FL-YOLO by its computational cost and model size. The experiment results show that the model size of FL-YOLO is 16.1MB, which is very light, and it achieves 36.7 FPS on NVIDIA Jetson TX1 and an AP of 76.7% on Multi-scene pedestrian dataset. Comparing with mainstream object detection models, FL-YOLO completes faster detection speed and higher accuracy, and it has lower calculation complexity and smaller model scale. Furthermore, the AP on Single-scene pedestrian dataset of FL-YOLO is improved to 80.7% by cloud-edge cooperation. K-Fold method is also used to further compared the performance of FL-YOLO and other models. Moreover, system test is implemented on coal mine, which validates the actual engineering effect of the proposed cloud-edge cooperation framework.",multimedia
10.1109/FCCM.2008.40,to_check,2008 16th International Symposium on Field-Programmable Custom Computing Machines,IEEE,2008-04-15 00:00:00,ieeexplore,A Hardware Efficient Support Vector Machine Architecture for FPGA,https://ieeexplore.ieee.org/document/4724927/,"In real-time video mining applications it is desirable to extract information about human subjects, such as gender, ethnicity, and age, from grayscale frontal face images. Many algorithms have been developed in the machine learning, statistical data mining, and pattern classification communities that perform such tasks with remarkable accuracy. Many of these algorithms, however, when implemented in software, suffer poor frame rates due to the amount and complexity of the computation involved. This paper presents an FPGA friendly implementation of a Gaussian Radial Basis SVM well suited to classification of grayscale images. We identify a novel optimization of the SVM formulation that dramatically reduces the computational inefficiency of the algorithm. The implementation achieves 88.6% detection accuracy in gender classification which is to the same degree of accuracy of software implementations using the same classification mechanism.",multimedia
10.1109/CISE.2009.5366152,to_check,2009 International Conference on Computational Intelligence and Software Engineering,IEEE,2009-12-13 00:00:00,ieeexplore,A Novel Multi-User Face Detection under Infrared Illumination by Real Adaboost,https://ieeexplore.ieee.org/document/5366152/,"Location and tracking the human faces is one of the critical technologies in free stereoscopic display system. But because of illumination variation or facial expression, it is difficult to detect human faces accurately and fast. In this paper, an infrared face detection based on real Adaboost algorithm and Cascade structure is implemented. With active infrared illumination, the problem caused by variation of illumination is almost solved, which makes the detection system more robust. Meanwhile, the combination of real Adaboost and Cascade structure pays more attention to the human faces which is more difficult to identify, making the detection quicker a lot. In the detection of video sequence, all human faces can be detected, and misdetection rarely appears. The average processing time on a windows XP, PIV 2.4 GHz PC is about 40 ms for a 640*480-pixel image. So the improved detection is real-time. In addition, experiment proves that the improved detection behaves better when there is variation of facial expression or a little degree leaning of human face. Obviously, the improved detection is more efficient and could be used widely.",multimedia
10.1109/CarpathianCC.2018.8399649,to_check,2018 19th International Carpathian Control Conference (ICCC),IEEE,2018-05-31 00:00:00,ieeexplore,Application of bit-serial arithmetic units for FPGA implementation of convolutional neural networks,https://ieeexplore.ieee.org/document/8399649/,"Convolutional Neural Networks (CNN) are commonly used in machine vision applications, for example in Cyber-Physical Systems (CPS), where real-time processing of incoming video feeds is a typical task. Therefore, execution (inference) of CNNs on the incoming stream of pictures (video feeds) must be done with strict application specific deadlines using the available systems resources. The paper is going to present a bit-serial implementation of CNN for inference type applications for FPGAs. The implementation utilizes the fact that the CNN is fully defined, i.e., all layers, coefficients, activation functions, etc. are known in design time for the CNN to be implemented. Therefore, the coefficients of the CNN can be embedded into the bit-serial multipliers and distributed arithmetic can be used significantly reducing the FPGA resources used by the implementation. In addition, the developed implementation uses pipelining to increase performance. The current implementation of the CNN convolutional layer is generated using Python from a high level description of the CNN, i.e., the Python code reads the description, and generates Verilog code, then the resulting Verilog code can be used to synthesize the specified CNN on the target FPGA by standard FPGA design tools. The final paper also demonstrate the implementation on a Xilinx Zynq Z-7020 FPGA.",multimedia
10.1109/FOCI.2007.372162,to_check,2007 IEEE Symposium on Foundations of Computational Intelligence,IEEE,2007-04-05 00:00:00,ieeexplore,Artificial immune systems based novelty detection with CNN-UM,https://ieeexplore.ieee.org/document/4233900/,"In this paper, we show that the earlier presented immune response inspired algorithmic framework in the work of Gy. Cserey et al. (2006, 2004) for spatial-temporal target detection applications using CNN technology by T. Roska and L.O. Chua (1993, 2002) and T. Roska (2002) can be implemented on the latest CNN-UM chip (Acel6k) by A. Rodriguez-Vazquez (2004) and Bi-i system by A. Zarandy and C. Rekcezky (2005). The implementation of the algorithm is real-time and able to detect novelty events in image flows reliably, running 10000 templates/s with video-frame (25 frame/s) speed and on image size of 128 times 128. Besides that some results of the implementation of this AIS model and its application for natural image flows are shown, the realized adaptation and mutation methods are also introduced.",multimedia
10.1109/IRIA53009.2021.9588707,to_check,2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA),IEEE,2021-09-22 00:00:00,ieeexplore,Automatic License Plate Recognition System Using SSD,https://ieeexplore.ieee.org/document/9588707/,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95%.",multimedia
10.1109/SAI.2017.8252178,to_check,2017 Computing Conference,IEEE,2017-07-20 00:00:00,ieeexplore,GWVT: A GPU maritime vessel tracker based on the wisard weightless neural network,https://ieeexplore.ieee.org/document/8252178/,"Maritime surveillance systems increase the security of ports and ships. The video tracking is an important and challenging component of a surveillance system. Difficulties can arise due to weather conditions, target trajectory and appearance, occlusions, lighting conditions and noise. The tracker locates the vessel frame by frame in real time. This paper proposes the GPU WiSARD Vessel Tracker GWVT, a maritime vessel tracker that uses the WiSARD weightless neural network and implemented on a GPU. The GPU parallel processing feature allows the tracking algorithm to be executed very fast. CUDA easy the implementation of the parallel WiSARD tracker because the network discriminators fit well in the thread and block layout. The implementation of a WiSARD based vessel tracker on a GPU is innovative on literature. The GWVT realizes the vessel tracking at only 1 millisecond allowing other tracking techniques to be executed in parallel to rise the performance. Discounting the kernel function call time from GWVT average tracking time, GWVT becomes 13,95 faster than the CPU tracker version.",multimedia
10.1109/CONIELECOMP.2014.6808580,to_check,"2014 International Conference on Electronics, Communications and Computers (CONIELECOMP)",IEEE,2014-02-28 00:00:00,ieeexplore,Implementation of an embedded system on a TS7800 board for robot control,https://ieeexplore.ieee.org/document/6808580/,"Growing Functional Modules (GFM) learning based controllers need to be experimented on real robots. In 2009, looking to develop a flexible and generic embedded interface for such robots, we decided to use a TS-7800 single board computer (SBC) with a Debian Linux operating system. Despite the many advantages of this board, implementing the embedded system has been a complex task. This paper describes the implementation of protocols through the TS-7800 different ports (RS232, TCP/IP, USB, analog and digital pins) as well as the connection of external boards (TS-ADC24, TS-DIO64, SSC-32 and LCD display). This implementation was required to connect a large range of actuators, sensors and other peripherals. Furthermore, the architecture of the embedded system is exposed in detail, including topics such as the XML configuration file that specifies the peripherals connected to the SBC, the concept of virtual sensors, the implementation of parallelism and the embedded system interface launcher. Technical aspects such as the optimization of video capture and processing are detailed because their execution required specific compilers versions, EABI emulation and extra libraries (openCV libjpg and libpngand libv4l). The final embedded system was implemented in a humanoid robot and connected to the GFM controller in charge of developing its equilibrium subsystem.",multimedia
10.1109/ICOIACT46704.2019.8938457,to_check,2019 International Conference on Information and Communications Technology (ICOIACT),IEEE,2019-07-25 00:00:00,ieeexplore,Strawberry Ripeness Classification System Based On Skin Tone Color using Multi-Class Support Vector Machine,https://ieeexplore.ieee.org/document/8938457/,"This research aims to build an automatic sorting system for strawberry ripeness into three categories: unripe, partially ripe, and ripe. Manual fruit sorting has many weaknesses and limitations. One of the disadvantages is human error in the sorting process. Therefore, the implementation of artificial intelligence as replacement of human worker can mitigate the problem. Fruit ripeness is identified based on color characteristic, which is the Red, Green, Blue (RGB) value of the object. Multi-Class Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel function is implemented to classify the ripeness. The data was taken using the Logitech C920 web camera which then divided into training and testing data video. In this research, prototype of strawberries sorting is built with real-time video which is never been considered in previous researches. Training data consists of 70 unripe strawberries, 70 partially ripe strawberries, and 70 ripe strawberries. Meanwhile testing data comprises of 30 unripe strawberries, 30 partially ripe strawberries, and 30 ripe strawberries. The result shows that the strawberry ripeness classification system using Multi-class SVM with RBF kernel function produces up to 85.64% accuracy, where the parameters are C = 7 and gamma (γ) = 10<sup>-2</sup>.",multimedia
10.1109/CNNA.2002.1035109,to_check,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,IEEE,2002-07-24 00:00:00,ieeexplore,Test-bed board for 16/spl times/64 stereo vision CNN chip,https://ieeexplore.ieee.org/document/1035109/,"The implementation of an artificial vision algorithm in real time is really attractive in such an application as the field of environment sensing. The SVCNN (stereo vision cellular neural network) chip is an analogue circuit able to compute in real time the Disparity Map from a couple of images by using a stereo visual system algorithm. A ""test-bed"" board for the 16/spl times/64 SVCNN chip is presented in this paper. This board is composed of an analogue processing core implemented by two 16/spl times/64 SVCNN chips together with a digital high performance pre-processing unit and a video grabbing section.",multimedia
10.1109/TCSVT.2015.2473255,to_check,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,2016-09-01 00:00:00,ieeexplore,Algorithm and VLSI Architecture of Edge-Directed Image Upscaling for 4k Display System,https://ieeexplore.ieee.org/document/7225135/,"High-quality and cost-efficient image upscaling design is very important for many real-time video processing applications, especially when the display panel resolution reaches ultrahigh definition. Compared with New Edge-Directed Interpolation (NEDI) based implicit edge directional upscaling, explicit methods require less computational resource and more easily reach real-time performance, especially when the required image definition and upscaling ratio are very high. Nevertheless, the investigation of applications of explicit methods in video processing systems remains largely missing arguably because it is commonly believed that explicit edge-directed interpolation tends to introduce unexpected artifacts because of inaccurate detection and hence its image quality is relatively poor. This paper proposes an explicit edge-directed adaptive interpolation method that leverages more sophisticated edge detection and orientation estimation algorithms to avoid misinterpolation, thereby providing similar or even better image quality than those with implicit methods. Targeting the real-time 4K video display system, the proposed edge-directed image upscaling algorithm is further implemented with an efficient very-large-scale integration (VLSI) architecture. The experimental results demonstrate that the proposed interpolation algorithm outperforms previous explicit and implicit edge-directed methods in both objective and subjective tests. The presented VLSI implementation further demonstrates that the maximum output video sequence of the proposed interpolation method can reach 4k × 2k@60 Hz with a reasonable hardware cost.",multimedia
10.1109/TCSI.2017.2757036,to_check,IEEE Transactions on Circuits and Systems I: Regular Papers,IEEE,2018-04-01 00:00:00,ieeexplore,An Architecture to Accelerate Convolution in Deep Neural Networks,https://ieeexplore.ieee.org/document/8070363/,"In the past few years, the demand for real-time hardware implementations of deep neural networks (DNNs), especially convolutional neural networks (CNNs), has dramatically increased, thanks to their excellent performance on a wide range of recognition and classification tasks. When considering real-time action recognition and video/image classification systems, latency is of paramount importance. Therefore, applications strive to maximize the accuracy while keeping the latency under a given application-specific maximum: in most cases, this threshold cannot exceed a few hundred milliseconds. Until now, the research on DNNs has mainly focused on achieving a better classification or recognition accuracy, whereas very few works in literature take in account the computational complexity of the model. In this paper, we propose an efficient computational method, which is inspired by a computational core of fully connected neural networks, to process convolutional layers of state-of-the-art deep CNNs within strict latency requirements. To this end, we implemented our method customized for VGG and VGG-based networks which have shown state-of-the-art performance on different classification/recognition data sets. The implementation results in 65-nm CMOS technology show that the proposed accelerator can process convolutional layers of VGGNet up to 9.5 times faster than state-of-the-art accelerators reported to-date while occupying 3.5 mm<sup>2</sup>.",multimedia
10.1109/ACCESS.2018.2839729,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,An Uphill Safety Controller With Deep Learning-Based Ramp Detection for Intelligent Wheelchairs,https://ieeexplore.ieee.org/document/8364538/,"In a society with aging population, the demand for electric wheelchairs is growing with the advancement of automation. However, many accidents have occurred due to the misjudgment of the slope angle and wheelchair speed while the wheelchair is traveling on ramps. This research employs the light electronic assistance pal compact motor package to reduce the weight and size of conventional electric wheelchairs. The modular design of proposed uphill controller and ramp detection functions allows users to easily select and incorporate only the functions they need. This paper proposes a ramp detection model implemented using the deep learning algorithm with CNN-4 structure to analyze depth image data. The model's recognition time of each video frame is 11 times faster than that of the AlexNet and GoogleNet. The uphill safety controller is designed as an adaptive network-based fuzzy inference system with Q-learning. The safe speed is automatically calculated according to the angle obtained from slope classification and revised in real-time during the slope driving to prevent the user from moving towards the dangerous ramp or rolling back due to inadequate speed. The accuracy of ramp detection is further increased by 5% to 97.1% due to assistance from the voting system processing and the gyroscope output data. The 5° ramp experiment of our uphill controller with ramp classification takes 20 s to complete the slope driving which is 23% faster than the controller without ramp detection. The energy consumption is also one half less than the experiment without uphill detection.",multimedia
10.1109/ICOSP.1998.770261,to_check,ICSP '98. 1998 Fourth International Conference on Signal Processing (Cat. No.98TH8344),IEEE,1998-10-16 00:00:00,ieeexplore,High-speed neural network based classifier for real-time application,https://ieeexplore.ieee.org/document/770261/,"This paper describes how to implement a partially connected neural network by a Giga-Ops Spectrum G800 FPGA (field programmable gate arrays)-based custom computer which consists of up to 32 Xilinx XC4010 logic chips. From the training data, a decision tree is generated by the classifier program C4.5. The tree is then used to initialise the neural network to a nearly optimum configuration. This initialised partially connected neural network is then trained by training data. The trained neural network is then implemented by our custom computer system. This implementation requires fewer connections and can provide a very-high-speed classifier for many real-time image recognition applications.",multimedia
10.1109/ICSMC.1998.728162,to_check,"SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)",IEEE,1998-10-14 00:00:00,ieeexplore,Implementing neural network in custom computers,https://ieeexplore.ieee.org/document/728162/,"This paper describes the implementation of a partially connected neural network using FPGAs (field programmable gate arrays) based custom computers. Starting from the training data, a decision tree is generated using the classifier program C4.5. The tree is then used to initialise the architecture of the neural network to a nearly optimum configuration. This initialised partially connected network is then trained using training data. The trained neural network is then implemented by fine-grain Xilinx XC6200 series FPGAs. This implementation requires fewer connections and can provide a very high speed classification for many real-time image recognition applications.",multimedia
10.1109/DeSE51703.2020.9450744,to_check,2020 13th International Conference on Developments in eSystems Engineering (DeSE),IEEE,2020-12-17 00:00:00,ieeexplore,Machine Vision Intelligent Travel Aid for the Visually Impaired (ITAVI) in Developing Countries<sup>*</sup>,https://ieeexplore.ieee.org/document/9450744/,"The visually impaired have little or no effective visual sensory input and have to rely on external assistance for navigation. Several electronic travel aids have been developed to aid independent navigation of the visually impaired, however they are not without limitations and dependence on third-parties. This paper describes the design and implementation of an Intelligent Travel Aid for the Visually Impaired, it combines the detection and recognition of objects in real-time with audio feedback to provide aid to the visually impaired users. This assistive device uses machine vision for object recognition detection, a camera for capturing object images respectively and a speaker all collectively form the core of the system. The system notifies users of obstacles and objects via synthesized speech. Using a quantized MobileNet based Single Shot multibox object detection model pre-trained on the Common Objects in Context dataset, the device was able to detect objects/obstacles, as well as determine the relative position and approximate distance. The device, when tested, was found to achieve real time performance of up to 70.56 frames per second for detections. Audio feedback was also achieved using the eSpeak Text to Speech engine to provide real time voice instructions to the user. All algorithms were implemented using Python language. The device is user friendly, allowing the visually impaired to enjoy easier navigation. However, other features such as extra object classes as well as language variety could be added in order to boost the robustness of the device.",multimedia
10.1109/SiPS.2016.48,to_check,2016 IEEE International Workshop on Signal Processing Systems (SiPS),IEEE,2016-10-28 00:00:00,ieeexplore,FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks,https://ieeexplore.ieee.org/document/7780102/,"In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs), one is a speech-tocharacter RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N-best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time.",multimedia
10.1109/IJCNN.1999.835990,to_check,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),IEEE,1999-07-16 00:00:00,ieeexplore,Fuzzy speech recognition,https://ieeexplore.ieee.org/document/835990/,"Speech recognition is a major topic in speech signal processing. Many algorithms based on results of speech analysis, among which dynamic time warping) and hidden Markov models are the most important, have been advanced. However, these algorithms generally turn out to be too complicated to be implemented in real time systems. The proposed algorithm in this paper, which uses a fuzzy logic recognition approach based on the power distribution pattern of a segment of a speech, allows the implementation of real-time speech recognition.",multimedia
10.1109/ICNN.1997.616140,to_check,Proceedings of International Conference on Neural Networks (ICNN'97),IEEE,1997-06-12 00:00:00,ieeexplore,An analog VLSI front-end for auditory signal analysis,https://ieeexplore.ieee.org/document/616140/,"Several researchers have found that signal processing of speech based on the principles of the human auditory system leads to noise-robust speech recognition systems. However it is impractical to implement such signal processing algorithms on a general-purpose digital computer, due to the enormous computational complexity of these algorithms. Therefore, we have implemented an auditory-signal processing system using low-power real-time analog and mixed-mode circuits. This implementation attempts to minimize the potential device-mismatch limitations, that can affect the performance of the final speech recognition system. The analog VLSI chip will serve as the front-end of a speech-recognition system. The chip architecture is inspired by biological auditory models common to humans and primate vertebrates. We include experimental results for a 1.2 /spl mu/m CMOS prototype. We also include speech recognition results obtained from software simulations of the hardware on the TI-DIGITS database, in which we have used linear discriminant analysis to reduce the feature dimension, and to interface the auditory-features to hidden Markov models.",multimedia
10.1109/ICNLSP.2018.8374383,to_check,2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP),IEEE,2018-04-26 00:00:00,ieeexplore,Spoken Arabic Algerian dialect identification,https://ieeexplore.ieee.org/document/8374383/,"Dialect identification is a challenging task and this becomes more complicated when dealing with under-resourced dialects. In this paper, we propose a system based on prosodic speech information, namely intonation and rhythm for identification of Intra-country dialects. The speech features are extracted after a coarse-grained consonant/vowel segmentation. Dialect models are built using both Deep Neural Networks (DNNs) and SVM. The hyper-parameters for the DNNs topology are tuned using a genetic algorithm. Our framework is implemented and evaluated on KALAM'DZ, a Web-based corpus dedicated to Algerian Arabic Dialectal varieties, with more than 42 h encompassing the four major Algerian subdialects: Hilali, Su-laymite, Ma'qilian, and Algiers-blanks. The results show that the DNNs implementation of Algerian Arabic Dialect IDentification system (a2did) reaches the same results when compared to SVM modeling. In addition, we concluded that a contrastive baseline acoustic-based classification system can serve as a complementary system to our a2did. The overall results reveal the suitability of our prosody-based a2did for speaker-independent dialect identification when utterances size are short. A requirement for real-time applications.",multimedia
10.1007/s12204-021-2348-7,to_check,Journal of Shanghai Jiaotong University (Science),Springer,2021-10-01 00:00:00,springer,Intelligent Analysis of Abnormal Vehicle Behavior Based on a Digital Twin,http://link.springer.com/openurl/pdf?id=doi:10.1007/s12204-021-2348-7,"Analyzing a vehicle’s abnormal behavior in surveillance videos is a challenging field, mainly due to the wide variety of anomaly cases and the complexity of surveillance videos. In this study, a novel intelligent vehicle behavior analysis framework based on a digital twin is proposed. First, detecting vehicles based on deep learning is implemented, and Kalman filtering and feature matching are used to track vehicles. Subsequently, the tracked vehicle is mapped to a digital-twin virtual scene developed in the Unity game engine, and each vehicle’s behavior is tested according to the customized detection conditions set up in the scene. The stored behavior data can be used to reconstruct the scene again in Unity for a secondary analysis. The experimental results using real videos from traffic cameras illustrate that the detection rate of the proposed framework is close to that of the state-of-the-art abnormal event detection systems. In addition, the implementation and analysis process show the usability, generalization, and effectiveness of the proposed framework.",multimedia
10.1007/978-3-030-89820-5_4,to_check,Advances in Soft Computing,Springer,2021-01-01 00:00:00,springer,Improving a Conversational Speech Recognition System Using Phonetic and Neural Transcript Correction,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89820-5_4,"This article describes the successful implementation of a conversational speech recognition system applied to telephonic sales performed by an autonomous agent. Our implementation uses a post-processing corrector based on phonetic representations of text and subsequent neural network classifier. The classifier assesses the proposed correction’s relevance to reduce the errors in the transcript sent to a downstream Natural Language Understanding engine. The experiments were carried on correcting transcripts from real audios of orders placed by customers of a large bottling company. We measured the Word Error Rate of the corrected transcripts against human-annotated ground-truth to verify the improvement produced by the system. To evaluate the corrections’ impact on the entities detected by the Natural Language Understanding engine, we used Jaccard distance, Precision, Recall, and $$F_1$$ F 1 . Results show that the implemented system and architecture enhance the transcript relative Word Error Rate on a 39% and Jaccard distance on 13% in comparison to the Automatic Speech Recognition baseline, making them suitable for real-time telephonic sales systems implementation.",multimedia
10.1007/978-3-030-05198-3_13,to_check,Emerging Technologies for Developing Countries,Springer,2019-01-01 00:00:00,springer,Awale Game: Application Programming Interface and Augmented Reality Interface,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-05198-3_13,"Awale game is one of the famous board games from Africa with many variants and is now played worldwide in various forms. In this paper, we propose an open-source Application Programming Interface ( API ) for developers to allow an easy implementation of the various variants of Awale as well as artificial intelligence based players. The API is available online at https://github.com/Machine-Intelligence-For-You/Awale . Based on this API , we propose a PC Awale game, a mobile Awale game, and an Augmented Reality Game. The Awale API , PC game, and mobile game are implemented in the programming language Java while the game in Augmented Reality is realized with the C# programming language, Unity 3D game engine and the Vuforia Augmented Reality SDK . The various tests carried out show that the API and the different games are totally functional. This API was also used for the first edition of MAIC, an Artificial Intelligence contest https://mify-ai.com/maic2017/ .",multimedia
DOItmp_0558_031242,to_check,Web Technologies Research and Development - APWeb 2005,Springer,2005-01-01 00:00:00,springer,A Real-Time Multimedia Data Transmission Rate Control Using a Neural Network Prediction Algorithm,http://link.springer.com/openurl/pdf?id=doi:DOItmp_0558_031242,"Both the real-time transmission and the amount of valid transmitted data are important factors in real-time multimedia transmission over the Internet. They are mainly affected by the channel bandwidth, delay time and packet loss. In this paper, we propose a predictive rate control system for data transmission, which is designed to improve the number of valid transmitted packets for the transmission of real-time multimedia through the Internet. A real-time multimedia transmission system was implemented using a TCP-Friendly algorithm, in order to obtain the measurement data needed for the proposed system. Neural network modeling was performed using the collected data, which consisted of the round-trip time delay and packet loss rate. The experiment results show that the algorithm proposed in this study increases the number of valid packets compare to the TCP-Friendly algorithm.",multimedia
10.1007/978-3-540-31849-1_98,to_check,Web Technologies Research and Development - APWeb 2005,Springer,2005-01-01 00:00:00,springer,A Real-Time Multimedia Data Transmission Rate Control Using a Neural Network Prediction Algorithm,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-31849-1_98,"Both the real-time transmission and the amount of valid transmitted data are important factors in real-time multimedia transmission over the Internet. They are mainly affected by the channel bandwidth, delay time and packet loss. In this paper, we propose a predictive rate control system for data transmission, which is designed to improve the number of valid transmitted packets for the transmission of real-time multimedia through the Internet. A real-time multimedia transmission system was implemented using a TCP-Friendly algorithm, in order to obtain the measurement data needed for the proposed system. Neural network modeling was performed using the collected data, which consisted of the round-trip time delay and packet loss rate. The experiment results show that the algorithm proposed in this study increases the number of valid packets compare to the TCP-Friendly algorithm.",multimedia
http://arxiv.org/abs/2008.00376v1,to_check,arxiv,arxiv,2020-08-02 01:18:18+00:00,arxiv,"Velocity Regulation of 3D Bipedal Walking Robots with Uncertain Dynamics
  Through Adaptive Neural Network Controller",http://arxiv.org/abs/2008.00376v1,"This paper presents a neural-network based adaptive feedback control
structure to regulate the velocity of 3D bipedal robots under dynamics
uncertainties. Existing Hybrid Zero Dynamics (HZD)-based controllers regulate
velocity through the implementation of heuristic regulators that do not
consider model and environmental uncertainties, which may significantly affect
the tracking performance of the controllers. In this paper, we address the
uncertainties in the robot dynamics from the perspective of the reduced
dimensional representation of virtual constraints and propose the integration
of an adaptive neural network-based controller to regulate the robot velocity
in the presence of model parameter uncertainties. The proposed approach yields
improved tracking performance under dynamics uncertainties. The shallow
adaptive neural network used in this paper does not require training a priori
and has the potential to be implemented on the real-time robotic controller. A
comparative simulation study of a 3D Cassie robot is presented to illustrate
the performance of the proposed approach under various scenarios.",multimedia
10.1109/IJCNN.2017.7966447,to_check,2017 International Joint Conference on Neural Networks (IJCNN),IEEE,2017-05-19 00:00:00,ieeexplore,A software-equivalent SNN hardware using RRAM-array for asynchronous real-time learning,https://ieeexplore.ieee.org/document/7966447/,"Spiking Neural Network (SNN) naturally inspires hardware implementation as it is based on biology. For learning, spike time dependent plasticity (STDP) may be implemented using an energy efficient waveform superposition on memristor based synapse. However, system level implementation has three challenges. First, a classic dilemma is that recognition requires current reading for short voltage-spikes which is disturbed by large voltage-waveforms that are simultaneously applied on the same memristor for real-time learning i.e. the simultaneous read-write dilemma. Second, the hardware needs to exactly replicate software implementation for easy adaptation of algorithm to hardware. Third, the devices used in hardware simulations must be realistic. In this paper, we present an approach to address the above concerns. First, the learning and recognition occurs in separate arrays simultaneously in real-time, asynchronously - avoiding non-biomimetic clocking based complex signal management. Second, we show that the hardware emulates software at every stage by comparison of SPICE (circuit-simulator) with MATLAB<sup>®</sup> (mathematical SNN algorithm implementation in software) implementations. As an example, the hardware shows 97.5% accuracy in classification which is equivalent to software for a Fisher's Iris dataset. Third, the STDP is implemented using a model of synaptic device implemented using HfO<sub>2</sub> memristor. We show that an increasingly realistic memristor model slightly reduces the hardware performance (85%), which highlights the need to engineer RRAM characteristics specifically for SNN.",science
10.1109/IJCNN.2008.4633828,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,Wafer-scale integration of analog neural networks,https://ieeexplore.ieee.org/document/4633828/,"This paper introduces a novel design of an artificial neural network tailored for wafer-scale integration. The presented VLSI implementation includes continuous-time analog neurons with up to 16 k inputs. A novel interconnection and routing scheme allows the mapping of a multitude of network models derived from biology on the VLSI neural network while maintaining a high resource usage. A single 20 cm wafer contains about 60 million synapses. The implemented neurons are highly accelerated compared to biological real time. The power consumption of the dense interconnection network providing the necessary communication bandwidth is a critical aspect of the system integration. A novel asynchronous low-voltage signaling scheme is presented that makes the wafer-scale approach feasible by limiting the total power consumption while simultaneously providing a flexible, programmable network topology.",science
10.1109/IJCNN.2003.1223995,to_check,"Proceedings of the International Joint Conference on Neural Networks, 2003.",IEEE,2003-07-24 00:00:00,ieeexplore,Investigating models of social development using a humanoid robot,https://ieeexplore.ieee.org/document/1223995/,"Human social dynamics rely upon the ability to correctly attribute beliefs, goals, and percepts to other people. The set of abilities that allow an individual to infer these hidden mental states based on observed actions and behavior has been called a ""theory of mind"". Drawing from the models of Baron-Cohen (1995) and Leslie (1994), a novel architecture called embodied theory of mind was developed to link high-level cognitive skills to the low-level perceptual abilities of a humanoid robot. The implemented system determines visual saliency based on inherent object attributes, high-level task constraints, and the attentional states of others. Objects of interest are tracked in real-time to produce motion trajectories which are analyzed by a set of naive physical laws designed to discriminate animate from inanimate movement. Animate objects can be the source of attentional states (detected by finding faces and head orientation) as well as intentional states (determined by motion trajectories between objects). Individual components are evaluated by comparisons to human performance on similar tasks, and the complete system is evaluated in the context of a basic social learning mechanism that allows the robot to mimic observed movements.",science
10.1109/SACI.2007.375494,to_check,2007 4th International Symposium on Applied Computational Intelligence and Informatics,IEEE,2007-05-18 00:00:00,ieeexplore,FPGA Parallel Implementation of CMAC Type Neural Network with on Chip Learning,https://ieeexplore.ieee.org/document/4262496/,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",robotics
10.1109/OPTIM.2008.4602496,to_check,2008 11th International Conference on Optimization of Electrical and Electronic Equipment,IEEE,2008-05-24 00:00:00,ieeexplore,Neural control based on RBF network implemented on FPGA,https://ieeexplore.ieee.org/document/4602496/,"The RBF radial basis function network is intended especially for hardware implementation and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the RBF type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",robotics
10.1109/SSCI.2018.8628809,to_check,2018 IEEE Symposium Series on Computational Intelligence (SSCI),IEEE,2018-11-21 00:00:00,ieeexplore,Bidirectional Fuzzy Brain Emotional Learning Control for Aerial Robots,https://ieeexplore.ieee.org/document/8628809/,This paper proposes a Bidirectional Fuzzy Brain Emotional Learning (BFBEL) control system to control Aerial Robots. The proposed controller is based on the emotional and logical processing of the brain. The proposed control system merges fuzzy inference and a bidirectional brain emotional learning algorithm. The Bidirectional Fuzzy Brain Emotional Learning (BFBEL) control can learn from scratch and adapt rapidly in real-time to control the system without much prior information. The proposed controller is tested against simulations of both a 1-Degree-Of-Freedom (DOF) flapping wing and a 6DOF flapping wing model and successfully implemented on a 1DOF flapping wing experiment which showcases the learning and adaptation capability in a real-time environment.,robotics
10.1109/CIG.2011.6032027,to_check,2011 IEEE Conference on Computational Intelligence and Games (CIG'11),IEEE,2011-09-03 00:00:00,ieeexplore,A neuronal global workspace for human-like control of a computer game character,https://ieeexplore.ieee.org/document/6032027/,"This paper describes a system that uses a global workspace architecture implemented in spiking neurons to control an avatar within the Unreal Tournament 2004 (UT2004) computer game. This system is designed to display human-like behaviour within UT2004, which provides a good environment for comparing human and embodied AI behaviour without the cost and difficulty of full humanoid robots. Using a biologically-inspired approach, the architecture is loosely based on theories about the high level control circuits in the brain, and it is the first neural implementation of a global workspace that is embodied in a dynamic real time environment. At its current stage of development the system can navigate through UT2004 and shoot opponents. We are currently completing the implementation and testing in preparation for the human-like bot competition at CIG 2011 in September.",robotics
10.1109/IROS.2003.1250667,to_check,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),IEEE,2003-10-31 00:00:00,ieeexplore,A robot that reinforcement-learns to identify and memorize important previous observations,https://ieeexplore.ieee.org/document/1250667/,"It is difficult to apply traditional reinforcement learning algorithms to robots, due to problems with large and continuous domains, partial observability, and limited numbers of learning experiences. This paper deals with these problems by combining: (1) reinforcement learning with memory, implemented using an LSTM recurrent neural network whose inputs are discrete events extracted from raw inputs; (2) online exploration and offline policy learning. An experiment with a real robot demonstrates the methodology's feasibility.",robotics
10.1109/CCDC.2019.8832952,to_check,2019 Chinese Control And Decision Conference (CCDC),IEEE,2019-06-05 00:00:00,ieeexplore,Digital Implementation of the Spiking Neural Network and Its Digit Recognition,https://ieeexplore.ieee.org/document/8832952/,"Motivated by biological principles of neural systems, spiking neural network (SNN) shows a tremendous potential in solving pattern recognition and cognitive tasks in recent years. In this study, a biologically inspired SNN composed of three layers is implemented on a reconfigurable FPGA with high computational efficiency and low hardware cost. The proposed SNN is consists of spiking neurons simulated by leaky-integrate-and-fire neuron model. In addition, spiking-time-dependent-plasticity based on event-driven is utilized to train the constructed network. The real-time hardware realization of the proposed SNN demonstrates powerful and efficient learning scheme. Results on different datasets shows that the proposed SNN implementation has the merit of capability of coping with pattern recognition tasks. Furthermore, the proposed implementation with remarkable performance could be applied and embed in bio-inspired neuromorphic platform such as robots for recognition tasks and on-line applications.",robotics
10.1109/TCIAIG.2012.2228483,to_check,IEEE Transactions on Computational Intelligence and AI in Games,IEEE,2013-03-01 00:00:00,ieeexplore,A Neurally Controlled Computer Game Avatar With Humanlike Behavior,https://ieeexplore.ieee.org/document/6357232/,"This paper describes the NeuroBot system, which uses a global workspace architecture, implemented in spiking neurons, to control an avatar within the Unreal Tournament 2004 (UT2004) computer game. This system is designed to display humanlike behavior within UT2004, which provides a good environment for comparing human and embodied AI behavior without the cost and difficulty of full humanoid robots. Using a biologically inspired approach, the architecture is loosely based on theories about the high-level control circuits in the brain, and it is the first neural implementation of a global workspace that has been embodied in a complex dynamic real-time environment. NeuroBot's humanlike behavior was tested by competing in the 2011 BotPrize competition, in which human judges play UT2004 and rate the humanness of other avatars that are controlled by a human or a bot. NeuroBot came a close second, achieving a humanness rating of 36%, while the most human human reached 67%. We also developed a humanness metric that combines a number of statistical measures of an avatar's behavior into a single number. In our experiments with this metric, NeuroBot was rated as 33% human, and the most human human achieved 73%.",robotics
10.1109/RAMECH.2011.6070484,to_check,"2011 IEEE 5th International Conference on Robotics, Automation and Mechatronics (RAM)",IEEE,2011-09-19 00:00:00,ieeexplore,A Q-learning based Cartesian model reference compliance controller implementation for a humanoid robot arm,https://ieeexplore.ieee.org/document/6070484/,This paper presents the implementation (real time and simulation) of a model-free Q-learning based discrete model reference compliance controller for a humanoid robot arm. The Reinforcement learning (RL) scheme uses a recently developed Q-learning scheme to develop an optimal policy on-line. The RL Cartesian (x and y) tracking controller with model reference compliance was implemented using two links (shoulder flexion and elbow flexion joints) of the right arm of the humanoid Bristol-Elumotion-Robotic-Torso II (BERT II) torso.,robotics
10.1109/ICE2T.2017.8215992,to_check,2017 International Conference on Engineering Technology and Technopreneurship (ICE2T),IEEE,2017-09-20 00:00:00,ieeexplore,Elevator button and floor number recognition through hybrid image classification approach for navigation of service robot in buildings,https://ieeexplore.ieee.org/document/8215992/,"To successfully move a robot into the building, the elevator button and elevator floor number detection and recognition can play an important role. It can help a robot move in the building, just as it also can help a visually impaired person who wants to move another floor in the building. Due to vision-based approach, the difference in lighting condition and the complex background are the main obstacles in this research. A hybrid image classification model is presented in this research to overcome all these difficulties. This hybrid model is the combination of histogram of oriented gradients and bag of words models, which later reduces the dimension of image features by using the feature selection algorithm. An artificial neural network has been implemented to get the experimental result by training and testing. In order to get training performance, 1000 training image samples have been used and additional 1000 image samples also been used to get the testing performance. The experimental results of this research indicate that this proposed framework is important for real-time implementation to implement the elevator button and elevator floor number recognition framework.",robotics
10.1109/ICCAE.2009.52,to_check,2009 International Conference on Computer and Automation Engineering,IEEE,2009-03-10 00:00:00,ieeexplore,Environmental Recognition Using RAM-Network Based Type-2 Fuzzy Neural for Navigation of Mobile Robot,https://ieeexplore.ieee.org/document/4804536/,"Reactive autonomous mobile robot navigating in real time environment is one of the most important requirements. Most of the systems have some common drawbacks such as, large computation, expensive equipment, hard implementation, and the complexity of the system. The work presented in this paper deals with a type-2 fuzzy-neural controller using RAM-based network to make navigation decisions. The proposed architecture can be implemented easily with low cost range sensor and low cost microprocessor. To minimize the execution time, we used a look-up table and that output stored into the robot RAM memory and becomes the current controller that drives the robot. This functionality is demonstrated on a mobile robot using a simple, 8 bit microcontroller with 512 bytes of RAM. The experiment results show that source code is efficient, works well, and the robot was able to successfully avoid obstacle in real time.",robotics
10.1109/IROS.2005.1545188,to_check,2005 IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2005-08-06 00:00:00,ieeexplore,Interactive evolution of human-robot communication in real world,https://ieeexplore.ieee.org/document/1545188/,"This paper describes how to implement interactive evolutionary computation (IEC) into a human-robot communication system. IEC is an evolutionary computation (EC) in which the fitness function is performed by human assessors. We used IEC to configure the human-robot communication system. We have already simulated IEC's application. In this paper, we implemented IEC into a real robot. Since this experiment leads considerable burdens on both the robot and experimental subjects, we propose the human-machine hybrid evaluation (HMHE) to increase the diversity within the genetic pool without increasing the number of interactions. We used a communication robot, WAMOEBA-3 (Waseda artificial mind on emotion base), which is appropriate for this experiment. In the experiment, human assessors interacted with WAMOEBA-3 in various ways. The fitness values increased gradually, and assessors felt the robot learnt the motions they desired. Therefore, it was confirmed that the IEC is most suitable as the communication learning system.",robotics
10.1109/ISIC.1994.367848,to_check,Proceedings of 1994 9th IEEE International Symposium on Intelligent Control,IEEE,1994-08-18 00:00:00,ieeexplore,Fuzzy neural network implementation of self tuning PID control systems,https://ieeexplore.ieee.org/document/367848/,"The fuzzy cognitive map (FCM) is a powerful universal method for representation of knowledge in various domains. The fuzzy inference engine can be implemented in the form of a network of FCMs. FCM implementation of the inference engine provides a suitable mechanism for expert control systems and information engineers to embed acquired human expertise, which is often imprecise, vague, or incomplete. The exploitation of an online learning algorithm empowers the fuzzy inference engine with the ability to modify its incomplete or possibly inconsistent knowledge base resulting in continuous improvement of the embedded knowledge. The fact that learning is an inherent feature of neural networks has inspired several researchers with the idea of using neural networks to implement fuzzy inference engines capable of learning. This paper presents a method for neural network FCM implementation of the fuzzy inference engine using the fuzzy columnar neural network architecture (FCNA). In this method the available human expertise is mapped first into an initial set of weights for the neurons. A new learning algorithm is then used to enhance the embedded knowledge in the neural network as a result of real time experience. The fuzzy inference engine (the neural network FCM) is used in computer simulations to control the speed of an underwater autonomous mobile robot. Results and computer simulation experiments are presented along with an evaluation of the new approach.&lt;<ETX>&gt;</ETX>",robotics
10.1109/IROS.2018.8594070,to_check,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2018-10-05 00:00:00,ieeexplore,HARK-Bird-Box: A Portable Real-time Bird Song Scene Analysis System,https://ieeexplore.ieee.org/document/8594070/,"This paper addresses real-time bird song scene analysis. Observation of animal behavior such as communication of wild birds would be aided by a portable device implementing a real-time system that can localize sound sources, measure their timing, classify their sources, and visualize these factors of sources. The difficulty of such a system is an integration of these functions considering the real-time requirement. To realize such a system, we propose a cascaded approach, cascading sound source detection, localization, separation, feature extraction, classification, and visualization for bird song analysis. Our system is constructed by combining an open source software for robot audition called HARK and a deep learning library to implement a bird song classifier based on a convolutional neural network (CNN). Considering portability, we implemented this system on a single-board computer, Jetson TX2, with a microphone array and developed a prototype device for bird song scene analysis. A preliminary experiment confirms a computational time for the whole system to realize a real-time system. Also, an additional experiment with a bird song dataset revealed a trade-off relationship between classification accuracy and time consuming and the effectiveness of our classifier.",robotics
10.1109/LARS-SBR.2016.49,to_check,2016 XIII Latin American Robotics Symposium and IV Brazilian Robotics Symposium (LARS/SBR),IEEE,2016-10-12 00:00:00,ieeexplore,Integration of People Detection and Simultaneous Localization and Mapping Systems for an Autonomous Robotic Platform,https://ieeexplore.ieee.org/document/7783535/,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.",robotics
10.1109/CIMSA.2009.5069917,to_check,2009 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,IEEE,2009-05-13 00:00:00,ieeexplore,Motion planning in unknown environment using an interval fuzzy type-2 and neural network classifier,https://ieeexplore.ieee.org/document/5069917/,"This paper describes environmental recognition and motion control using weightless neural network classifier and interval type-2 fuzzy logic controller. The weightless neural network classifies geometric feature such as U-shape, corridor and left or right corner using ultrasonic sensors. The neural network utilizes previous sensor data and analyzes the situation of the current environment. The behavior of mobile robot is implemented by means of fuzzy control rules. Based on the performance criteria the quality of controller is evaluated to make navigation decisions. This functionality is demonstrated on a mobile robot using modular platform and containing several microcontrollers implies the implementation of a robust architecture. The proposed architecture implemented using low cost range sensor and low cost microprocessor. The experiment results show that the mobile robot can recognize the current environment and was able to successfully avoid obstacle in real time.",robotics
10.1109/IROS.1990.262374,to_check,"EEE International Workshop on Intelligent Robots and Systems, Towards a New Frontier of Applications",IEEE,1990-07-06 00:00:00,ieeexplore,"Single leg walking with integrated perception, planning and control",https://ieeexplore.ieee.org/document/262374/,"Describes an integrated system capable of walking over rugged terrain using a single leg suspended below a carriage that rolls along rails. To walk, the system uses a laser scanner to find a foothold, positions the leg above the foothold, contacts the terrain with the foot, and applies force enough to advance the carriage along the rails. Walking both forward and backward, the system has traversed hundreds of meters of rugged terrain including obstacles too tall to step over, trenches too deep to step in, closely spaced rocks, and sand hills. The implemented system consists of a number of task-specific processes (two for planning, two for perception, one for real-time control) and a central control process that directs the flow of communication between processes. Implementing this integrated system is a significant step toward the goal of the CMU Planetary Rover project: to prototype a autonomous six-legged robot for planetary exploration.&lt;<ETX>&gt;</ETX>",robotics
10.1007/978-3-319-46687-3_31,to_check,Neural Information Processing,Springer,2016-01-01 00:00:00,springer,Learning Visually Guided Risk-Aware Reaching on a Robot Controlled by a GPU Spiking Neural Network,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-46687-3_31,"Risk-aware control is a new type of robust nonlinear stochastic controller in which state variables are represented by time-varying probability densities and the desired trajectory is replaced by a cost function that specifies both the goals of movement and the potential risks associated with deviations. Efficient implementation is possible using the theory of Stochastic Dynamic Operators (SDO), because for most physical systems the SDO operators are near-diagonal and can thus be implemented using distributed computation. I show such an implementation using 4.3 million spiking neurons simulated in real-time on a GPU. I demonstrate successful control of a commercial desktop robot for a visually-guided reaching task, and I show that the operators can be learned during repetitive practice using a recursive learning rule.",robotics
10.1016/j.neucom.2019.04.037,to_check,Neurocomputing,scopus,2019-08-04,sciencedirect,LEGION-based image segmentation by means of spiking neural networks using normalized synaptic weights implemented on a compact scalable neuromorphic architecture,https://api.elsevier.com/content/abstract/scopus_id/85065418438,"LEGION (Locally Excitatory, Globally Inhibitory Oscillator Network) topology has demonstrated good capabilities in scene segmentation applications. However, the implementation of LEGION algorithm requires machines with high performance to process a set of complex differential equations limiting its use in practical real-time applications. Recently, several authors have proposed alternative methods based on spiking neural networks (SNN) to create oscillatory neural networks with low computational complexity and highly feasible to be implemented on digital hardware to perform adaptive segmentation of images. Nevertheless, existing SNN with LEGION configuration focus on the membrane model leaving aside the behavior of the synapses although they play an important role in the synchronization of several segments by self-adapting their weights. In this work, we propose a SNN-LEGION configuration along with normalized weight of the synapses to self-adapt the SNN network to synchronize several segments of any size and shape at the same time. The proposed SNN-LEGION method involves a global inhibitor, which is in charge of performing the segmentation process between different objects with different sizes and shapes on time. To validate the proposal, the SNN-LEGION method is implemented on an optimized scalable neuromorphic architecture. Our preliminary results demonstrate that the proposed normalization process of the synaptic weights along with the SNN-LEGION configuration keep the capacity of the LEGION network to separate the segments on time, which can be useful in video processing applications such as vision processing systems for mobile robots, offering lower computational complexity and area consumption compared with previously reported solutions.",robotics
10.1016/j.neucom.2017.03.028,to_check,Neurocomputing,scopus,2017-06-28,sciencedirect,A real-time FPGA implementation of a biologically inspired central pattern generator network,https://api.elsevier.com/content/abstract/scopus_id/85016031943,"Central pattern generators (CPGs) functioning as biological neuronal circuits are responsible for generating rhythmic patterns to control locomotion. In this paper, a biologically inspired CPG composed of two reciprocally inhibitory neurons was implemented on a reconfigurable FPGA with real-time computational speed and considerably low hardware cost. High-accuracy neural circuit implementation can be computationally expensive, especially for a high-dimensional conductance-based neuron model. Thus, we aimed to present an efficient multiplier-less hardware implementation method for the investigation of real-time hardware CPG (hCPG) networks. In order to simplify the hardware implementation, a modified neuron model without nonlinear parts was given to decrease the complexity of the original model. A simple CPG network involving two chemical coupled neurons was realized which represented the pyloric dilator (PD) and lateral pyloric (LP) neurons in the crustacean pyloric CPG. The implementation results of the hCPG network showed that rhythmic behaviors were successfully reproduced and the resource consumption was dramatically reduced by using our multiplier-less implementation method. The presented FPGA-based implementation of hCPG network with remarkable performance set a prototype for the realization of other large-scale CPG networks and could be applied in bio-inspired robotics and motion rehabilitation for locomotion control.",robotics
10.1016/j.neunet.2013.04.005,to_check,Neural Networks,scopus,2013-09-01,sciencedirect,FPGA implementation of a configurable neuromorphic CPG-based locomotion controller,https://api.elsevier.com/content/abstract/scopus_id/84880792738,"Neuromorphic engineering is a discipline devoted to the design and development of computational hardware that mimics the characteristics and capabilities of neuro-biological systems. In recent years, neuromorphic hardware systems have been implemented using a hybrid approach incorporating digital hardware so as to provide flexibility and scalability at the cost of power efficiency and some biological realism. This paper proposes an FPGA-based neuromorphic-like embedded system on a chip to generate locomotion patterns of periodic rhythmic movements inspired by Central Pattern Generators (CPGs). The proposed implementation follows a top-down approach where modularity and hierarchy are two desirable features. The locomotion controller is based on CPG models to produce rhythmic locomotion patterns or gaits for legged robots such as quadrupeds and hexapods. The architecture is configurable and scalable for robots with either different morphologies or different degrees of freedom (DOFs). Experiments performed on a real robot are presented and discussed. The obtained results demonstrate that the CPG-based controller provides the necessary flexibility to generate different rhythmic patterns at run-time suitable for adaptable locomotion.",robotics
10.1016/j.cogsys.2009.12.003,to_check,Cognitive Systems Research,scopus,2010-09-01,sciencedirect,Cognitive concepts in autonomous soccer playing robots,https://api.elsevier.com/content/abstract/scopus_id/77952550318,"Computational concepts of cognition, their implementation in complex autonomous systems, and their empirical evaluation are key techniques to understand and validate concepts of cognition and intelligence. In this paper we want to describe computational concepts of cognition that were successfully implemented in the domain of soccer playing robots and show the interactions between cognitive concepts, software engineering and real-time application development. Beside a description of the general concepts we will focus on aspects of perception, behavior architecture, and reinforcement learning.",robotics
10.1109/VTCFall.2017.8288318,to_check,2017 IEEE 86th Vehicular Technology Conference (VTC-Fall),IEEE,2017-09-27 00:00:00,ieeexplore,Impact to Longitude Velocity Control of Autonomous Vehicle from Human Driver's Distraction Behavior,https://ieeexplore.ieee.org/document/8288318/,"Driver distraction behaviors are usually blind to autonomous vehicles (AVs), leading to probable late preparation for AVs to take emergency measures. Hence, this paper aims to build a bridge between AV control and driver behavior detection, to assist AVs to predict the potential risk and avoid abnormal drivers carefully like experienced drivers. Our main contributions of this paper consist: i) put forward a practicable system framework integrating driver distraction monitoring, vehicle-to-vehicle communication and AV velocity control; ii) provide a real-time driver distraction monitoring implementation building on convolutional neural network trained offline; iii) propose a method of longitude velocity control of AV considering the risk of driver distraction behavior based on model predictive control strategy. Simulation results validate the effectiveness of our work.",autonomous vehicle
10.1109/EEEIC/ICPSEurope49358.2020.9160705,to_check,2020 IEEE International Conference on Environment and Electrical Engineering and 2020 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),IEEE,2020-06-12 00:00:00,ieeexplore,ASDVC - A Self-Driving Vehicle Controller using Unsupervised Machine Learning,https://ieeexplore.ieee.org/document/9160705/,"ASDVC is a self-driving vehicle controller that uses unsupervised machine learning methods, namely clustering based k-means, hierarchical, Gaussian Matrix Model and self-organizing mapping to optimize the path the vehicle follows from the source to destination. The real-time optimal selection of the unsupervised machine learning based motion control algorithm could provide fast response times of under one microsecond during the lateral, longitudinal and angular motion control of the autonomous vehicle. However, it is shown that a simple selection of one of the machine learning methods may not guarantee the optimality of the results. The successful implementation of ASDVC controller in self-driving vehicles could have a significant contribution towards making mobility more reliable and sustainable for the future vehicular transportation systems.",autonomous vehicle
10.1109/ICCC51557.2021.9454613,to_check,2021 22nd International Carpathian Control Conference (ICCC),IEEE,2021-06-01 00:00:00,ieeexplore,Deep Learning-Based Automated Vehicle Steering,https://ieeexplore.ieee.org/document/9454613/,"Autonomous Vehicle applications are full of open challenges. Despite the advanced technologies, the lack of robust systems still exists due to the high complexity of the surrounded environments. The automated steering is one of the most complex autonomous driving system's application. Model predictive control is the most common control strategy used to implement the automated steering tasks due to its ability to solve an online quadratic optimization problem in the real-time, in addition to its efficiency in handling the constraints of the system's environments. MPC controller is used to drive the vehicle autonomously along the centerline of the road based on two main factors, the lateral deviation and relative yaw angle. Deep learning technology has been widely used in recent years because of the promising performance achieved in different applications and tasks. In this context, we suggested that the implementation of the Deep Neural Network (DNN) will provide a great improvement and it can be more computationally efficient than solving an online quadratic problem (QP), that will naturally lead to reduce the time, the complexity, and the computational loads of implementations. The main aims of this paper are to design a deep learning-based approach for automated vehicle steering based on the behaviour of the traditional MPC controller. In addition, to study the efficiency of the full replacement of the MPC controller by the suggested DNN model. The study is based on performing a comparison between the implementations of both controllers (MPC and DNN model) in terms of the performance and the execution time. The performance indicator is the ability of the controller to drive the decision variables (lateral deviation and yaw angle) to be close to zero in order to drive the vehicle autonomously along the desired path.",autonomous vehicle
10.1109/EMSOFT.2018.8537236,to_check,2018 International Conference on Embedded Software (EMSOFT),IEEE,2018-10-05 00:00:00,ieeexplore,Special Session: Embedded Software for Robotics: Challenges and Future Directions,https://ieeexplore.ieee.org/document/8537236/,"This paper surveys recent challenges and solutions in the design, implementation, and verification of embedded software for robotics. Emphasis is placed on mobile robots, like self-driving cars. In design, it addresses programming support for robotic systems, secure state estimation, and ROS-based monitor generation. In the implementation phase, it describes the synthesis of control software using finite precision arithmetic, real-time platforms and architectures for safety-critical robotics, efficient implementation of neural network based-controllers, and standards for computer vision applications. The issues in verification include verification of neural network-based robotic controllers, and falsification of closed-loop control systems. The paper also describes notable open-source robotic platforms. Along the way, we highlight important research problems for developing the next generation of high-performance, low-resource-usage, correct embedded software.",autonomous vehicle
10.1109/ICM48031.2019.9021904,to_check,2019 31st International Conference on Microelectronics (ICM),IEEE,2019-12-18 00:00:00,ieeexplore,Low power CNN hardware FPGA implementation,https://ieeexplore.ieee.org/document/9021904/,"A convolution Neural Networks (CNN) goes under the wide umbrella of Deep Neural Networks (DNN) whose applications are widely used. For example, the later are used in robotics and different applications of recognition like speech recognition and facial recognition, also nowadays in autonomous cars. Therefore the aim of implementing the CNN is to be used in real time applications. As a result of that, Graphics processing units (GPUs) are used but their worst disadvantage is it's high power consumption which can't be used in daily used equipments. The target of this paper is to solve the power consumption problem by using Field Programmable Array (FPGA) which has low power consumption, and flexible architecture. The implementation architecture of Alex Network, which consists of three fully connected layers and five convolution layers, on FPGA will depend on two main techniques parallelism of resources, and pipelining inside of some layers.",autonomous vehicle
10.1109/ReConFig.2015.7393339,to_check,2015 International Conference on ReConFigurable Computing and FPGAs (ReConFig),IEEE,2015-12-09 00:00:00,ieeexplore,Real-time pedestrian detection on a xilinx zynq using the HOG algorithm,https://ieeexplore.ieee.org/document/7393339/,Advanced driver assistance systems (ADAS) are the key to enable autonomous cars in the near future. One important task for autonomous cars is to detect pedestrians reliably in real-time. The HOG algorithm is one of the best algorithms for this task; however it is very compute intensive. To fulfill the real-time requirements for high resolution images an efficient parallel implementation is necessary. This paper presents an efficient hardware implementation as well as a parallel software implementation of the HOG algorithm for pedestrian detection on a Xilinx Zynq SoC. The hardware implementation achieves a speedup of 2x compared to the parallel software implementation for high resolution images (1920 x 1080). Against state-of-the-art a speedup of 1.32x is achieved. The hardware implementation has a reliable detection rate of 90.2% using a classifier trained by an AdaBoost algorithm and a minor false positive rate of 4 %.,autonomous vehicle
10.1109/SYSOSE.2017.7994953,to_check,2017 12th System of Systems Engineering Conference (SoSE),IEEE,2017-06-21 00:00:00,ieeexplore,Autonomous decision making for a driver-less car,https://ieeexplore.ieee.org/document/7994953/,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome.",autonomous vehicle
10.1109/ICIT.2009.4939663,to_check,2009 IEEE International Conference on Industrial Technology,IEEE,2009-02-13 00:00:00,ieeexplore,Real-time Neural Network based Identification of a Rotary-Wing UAV dynamics for autonomous flight,https://ieeexplore.ieee.org/document/4939663/,"Real time flight implementation of a neural network based black-box identification (NNID) scheme to a rotary wing unmanned aerial vehicle (RUAV) is presented in this paper. The applicability of NNID scheme for real time identification of longitudinal and lateral dynamics of the RUAV is evaluated in flight. To show the efficacy of the method for real time applications, the identification results and error statistics are provided. The challenges involved in terms of hardware implementation, computational time requirements, and real time coding are investigated and reported. Results indicate that NNID is suitable for modeling the dynamics of the RUAV in real time.",autonomous vehicle
10.1109/ROBIO.2018.8665195,to_check,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2018-12-15 00:00:00,ieeexplore,Unsupervised Feature Fusion Combined with Neural Network Applied to UAV Attitude Estimation,https://ieeexplore.ieee.org/document/8665195/,"In the field of an unmanned aerial vehicle (UAV), the navigation algorithm with high precision and easy implementation is a hot topic of research, and the key of UAV control is to obtain accurate and real-time attitude information. In this paper, a feature fusion algorithm based on unsupervised deep autoencoder (DAE) is proposed. It is used for data fusion of multiple sensors. The experimental results show that the unsupervised feature fusion algorithm can effectively improve the accuracy and has the potential to be applied to the data fusion of UAV sensors.",autonomous vehicle
10.1109/ISCAS45731.2020.9181004,to_check,2020 IEEE International Symposium on Circuits and Systems (ISCAS),IEEE,2020-10-14 00:00:00,ieeexplore,Optimized Random Forest Classifier for Drone Pilot Identification,https://ieeexplore.ieee.org/document/9181004/,"Random forest is a powerful machine learning scheme which finds applications in real-time systems such as unmanned aerial vehicles. In such applications not only the classification performance is relevant but also several non-functional requirements including the classification time, the memory usage and the power consumption. This paper proposes a new approach to improve the real-time behavior of a random forest classifier. This is accomplished by reducing the number of evaluated nodes and branches as well as by reducing the branch length in the underlying binary decision trees with numerical split values. A hardware architecture is presented for the improved tree-based classification method. A proof-of-concept implementation on an FPGA platform and some preliminary results show the advantage of this approach compared to related work.",autonomous vehicle
10.1109/MFI.2017.8170439,to_check,2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI),IEEE,2017-11-18 00:00:00,ieeexplore,Wearable gesture control of agile micro quadrotors,https://ieeexplore.ieee.org/document/8170439/,"Quadrotor unmanned aerial vehicles (UAVs) have seen a surge of use in various applications due to its structural simplicity and high maneuverability. However, conventional control methods using joysticks prohibit novices from getting used to maneuvering quadrotors in short time. In this paper, we suggest the use of a wearable device, such as a smart watch, as a new remote-controller for a quadrotor. The user's command is recognized as gestures using the 9-DoF inertial measurement unit (IMU) of a wearable device through a recurrent neural network (RNN) with long short-term memory (LSTM) cells. Our implementation also makes it possible to align the heading of a quadrotor with the heading of the user. Our implementation allows nine different gestures and the trained RNN is used for real-time gesture recognition for controlling a micro quadrotor. The proposed system exploits available sensors in a wearable device and a quadrotor as much as possible to make the gesture-based control intuitive. We have experimentally validated the performance of the proposed system by using a Samsung Gear S smart watch and a Crazyflie Nano Quadcopter.",autonomous vehicle
10.1109/ACCESS.2021.3104738,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Drone Detection Sensor With Continuous 2.4 GHz ISM Band Coverage Based on Cost-Effective SDR Platform,https://ieeexplore.ieee.org/document/9513316/,"The development of Unmanned Aerial Vehicles (UAVs), commonly referred to as drones, has introduced revolutionary changes in many areas over the past few years. However, aside from opening new possibilities, the usage of drones in an irresponsible and dangerous manner leads to many hazardous incidents. This paper presents a drone detection sensor with a continuous 2.400 GHz-2.483 GHz operational frequency range for detection methods based on passive radio frequency imaging techniques. The implementation based on Software Defined Radio (SDR) and Field Programmable Logic Array (FPGA) hardware that overcomes the 40 MHz real-time bandwidth limit of other popular SDRs is presented utilizing low-cost off-the-shelf components. Furthermore, a hardware realization of the signal processing chain for specific detection algorithms is proposed to minimize the throughput between SDR and the companion computer and offload software computations. The device validation is made in a laboratory and real-life scenario and presented in relation to the sensor used in other works. In addition to the increased real-time bandwidth, the measurements show a 9 dB reduction in detection sensitivity compared to the reference receiver, in line with the analog RF front-end specifications. The final analysis demonstrates the proposed device’s relevance as a sensor for obtaining machine learning datasets and as a part of a final anti-drone system.",autonomous vehicle
10.1007/978-3-030-60467-7_7,to_check,Innovation and Research,Springer,2021-01-01 00:00:00,springer,Intelligent and Autonomous Guidance Through a Geometric Model for Conventional Vehicles,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60467-7_7,"Cyber-physical systems (CPS) in the automobile industry are facing major challenges related to the use and validation of these CPS, which entails high costs in the implementation and training tests in the physical world, thus limiting research. Therefore, there is a need to shorten the validation times of these CPS with the use of 3D simulation software. This research article proposes to simulate a CPS in the simulation software Webots, with the aim of emulating the autonomous movement of conventional vehicles by integrating a GPS sensor and a compass sensor which provide information on location and orientation, these data are used for the implementation of a geometric model by vectors, the same one that is developed in a controller that allows to take actions on the vehicles in the simulation software in order to emulate an urban traffic. Finally, a series of configurations have been made to evaluate the geometric model, managing to maintain the default speed of 94.194% with curves greater than 90 degrees. In addition, the validation of this system in a real environment through the instrumentation in land vehicles is drawn as future lines.",autonomous vehicle
10.1007/s10846-020-01274-1,to_check,Journal of Intelligent & Robotic Systems,Springer,2020-12-11 00:00:00,springer,A Dynamically Feasible Fast Replanning Strategy with Deep Reinforcement Learning,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10846-020-01274-1,"In this work, we aim to develop a fast trajectory replanning methodology enabling highly agile aerial vehicles to navigate in cluttered environments. By focusing on reducing complexity and accelerating the replanning problem under strict dynamical constraints, we employ the b-spline theory with local support property for defining the high dimensional agile flight trajectories. We utilize the differential flatness model of an aerial vehicle, allowing us to directly map the desired output trajectory into input states to track a high dimensional trajectory. Dynamically feasible replanning problem is addressed through regenerating the local b-splines with control point reallocation. As the geometric form of the trajectory based on the location of the control points and the knot intervals, the control point reallocation for fast replanning with dynamical constraints is turned into a constrained optimization problem and solved through deep reinforcement learning. The proposed methodology enables generating dynamically feasible local trajectory segments, which are continuous to the existing, hence provides fast local replanning for collision avoidance. The DRL agent is trained with different environmental complexities, and through the batch simulations, it is shown that the proposed methodology allows to solve fast trajectory replanning problem under given or hard dynamical constraints and provide real-time applicability for such collision avoidance applications in agile unmanned aerial vehicles. Hardware implementation tests of the algorithm with the agile trajectory tracker to a small UAV can bee seen in the following video link: https://youtu.be/8IiLQFQ3V0E .",autonomous vehicle
http://arxiv.org/abs/1801.05086v1,to_check,arxiv,arxiv,2018-01-16 01:14:12+00:00,arxiv,Autonomous UAV Navigation Using Reinforcement Learning,http://arxiv.org/abs/1801.05086v1,"Unmanned aerial vehicles (UAV) are commonly used for missions in unknown
environments, where an exact mathematical model of the environment may not be
available. This paper provides a framework for using reinforcement learning to
allow the UAV to navigate successfully in such environments. We conducted our
simulation and real implementation to show how the UAVs can successfully learn
to navigate through an unknown environment. Technical aspects regarding to
applying reinforcement learning algorithm to a UAV system and UAV flight
control were also addressed. This will enable continuing research using a UAV
with learning capabilities in more important applications, such as wildfire
monitoring, or search and rescue missions.",autonomous vehicle
http://arxiv.org/abs/1812.08273v1,to_check,arxiv,arxiv,2018-12-19 22:25:52+00:00,arxiv,Analog Signal Processing Using Stochastic Magnets,http://arxiv.org/abs/1812.08273v1,"We present a low barrier magnet based compact hardware unit for analog
stochastic neurons and demonstrate its use as a building-block for neuromorphic
hardware. By coupling circular magnetic tunnel junctions (MTJs) with a CMOS
based analog buffer, we show that these units can act as leaky-integrate-and
fire (LIF) neurons, a model of biological neural networks particularly suited
for temporal inferencing and pattern recognition. We demonstrate examples of
temporal sequence learning, processing, and prediction tasks in real time, as a
proof of concept demonstration of scalable and adaptive signal-processors.
Efficient non von-Neumann hardware implementation of such processors can open
up a pathway for integration of hardware based cognition in a wide variety of
emerging systems such as IoT, industrial controls, bio- and photo-sensors, and
Unmanned Autonomous Vehicles.",autonomous vehicle
10.1016/j.ejrs.2021.08.007,to_check,Egyptian Journal of Remote Sensing and Space Science,scopus,2021-01-01,sciencedirect,Smart farming for improving agricultural management,https://api.elsevier.com/content/abstract/scopus_id/85114414365,"The food shortage and the population growth are the most challenges facing sustainable development worldwide. Advanced technologies such as artificial intelligence (AI), the Internet of Things (IoT), and the mobile internet can provide realistic solutions to the challenges that are facing the world. Therefore, this work focuses on the new approaches regarding smart farming (SF) from 2019 to 2021, where the work illustrates the data gathering, transmission, storage, analysis, and also, suitable solutions. IoT is one of the essential pillars in smart systems, as it connects sensor devices to perform various basic tasks. The smart irrigation system included those sensors for monitoring water level, irrigation efficiency, climate, etc. Smart irrigation is based on smart controllers and sensors as well as some mathematical relations. In addition, this work illustrated the application of unmanned aerial vehicles (UAV) and robots, where they can be achieved several functions such as harvesting, seedling, weed detection, irrigation, spraying of agricultural pests, livestock applications, etc. real-time using IoT, artificial intelligence (AI), deep learning (DL), machine learning (ML) and wireless communications. Moreover, this work demonstrates the importance of using a 5G mobile network in developing smart systems, as it leads to high-speed data transfer, up to 20 Gbps, and can link a large number of devices per square kilometer. Although the applications of smart farming in developing countries are facing several challenges, this work highlighted some approaches the smart farming. In addition, the implementation of Smart Decision Support Systems (SDSS) in developing countries supports the real-time analysis, mapping of soil characteristics and also helps to make proper decision management. Finally, smart agriculture in developing countries needs more support from governments at the small farms and the private sector.",autonomous vehicle
10.1016/j.comcom.2020.02.009,to_check,Computer Communications,scopus,2020-03-01,sciencedirect,UAV monitoring and forecasting model in intelligent traffic oriented applications,https://api.elsevier.com/content/abstract/scopus_id/85079351564,"Intelligent transportation system is a traffic management system developed with the progress of society and traffic. Its idea is to integrate the real-time operation of people, vehicles, roads and traffic involved in the traffic. The purpose of this paper is to build a safe, reliable and efficient vehicle monitoring and forecasting model for IOT. Based on the Beidou satellite positioning technology and Lora communication technology, aiming at the problem that the deep learning detection method cannot meet the real-time requirements in processing the monitoring video, this paper proposes a method of using multiple single target trackers instead of some yolov3 detection tasks, and puts forward the design idea and specific implementation scheme of the vehicle monitoring and prediction model. The vehicle monitoring and prediction model is used to detect four kinds of targets, namely, small cars, buses, trucks and pedestrians. The multi-target trajectory tracking is used to carry out the traffic statistics of multi vehicle types, the detection of two kinds of abnormal behaviors of traffic targets is low speed and parking, and the capture of pedestrians. The experimental results show that the vehicle monitoring and prediction model has the highest accuracy of location and type recognition for four types of traffic objects, namely, small cars, trucks, buses and pedestrians, reaching 80%.",autonomous vehicle
10.1016/j.procs.2019.09.442,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Unmanned aerial vehicle in the machine learning environment,https://api.elsevier.com/content/abstract/scopus_id/85079097933,"Unmanned Aerial Vehicles and machine learning have started gaining attentions of academic and industrial research. The Unmanned Aerial Vehicles have extended the freedom to operate and monitor the activities from remote locations. This study retrieved and synthesized research on the use of Unmanned Aerial Vehicles along with machine learning and its algorithms in different areas and regions. The objective was to synthesize the scope and importance of machine learning models in enhancing Unmanned Aerial Vehicles capabilities, solutions to problems, and numerous application areas.
                  The machine learning implementation has reduced numbers of challenges to Unmanned Aerial Vehicles besides enhancing the capabilities and opening the door to the different sectors. The Unmanned Aerial Vehicles and machine learning association has resulted in fast and reliable outputs. The combination of Unmanned Aerial Vehicles and machine learning has helped in real time monitoring, data collection and processing, and prediction in the computer/wireless networks, smart cities, military, agriculture, and mining.",autonomous vehicle
10.3182/20120403-3-DE-3010.00041,to_check,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Embedded system for controlling a mini underwater vehicle in autonomous hover mode,https://api.elsevier.com/content/abstract/scopus_id/84866098847,"This work presents the development of a mini underwater vehicle (Triton-PR), the embedded system, and the experiments in real-time for autonomous hover operation. Artificial vision allows the vehicle to obtain the translational position and velocity. The main characteristic of the embedded system is the implementation of low cost devices and materials, besides the number and location of the thrusters was chosen in order to have enough power and generate the rotation and translation movements. The dynamical model of the (Triton-PR) is described by the classic Euler-Lagrange equations, and a PD controller based on saturation functions is proposed for providing autonomous attitude and position of the robot. Finally, the performance of the vehicle is shown in simulation and real-time experimental results.",autonomous vehicle
10.1016/0925-2312(94)00018-N,to_check,Neurocomputing,scopus,1995-01-01,sciencedirect,Fast computation of optimal paths using a parallel Dijkstra algorithm with embedded constraints,https://api.elsevier.com/content/abstract/scopus_id/0029329099,"We have developed a new optimal path algorithm in which the paths are subjected to turning constraints. The restriction which we have incorporated is the next link in the path must not make an angle exceeding 45 ° in magnitude with the preceeding link. This algorithm has a natural implementation as an artificial neural system with either synchronous or asynchronous weight updating, and as an automata executing on a massively parallel array processor. At a given step in the path solution process our path planning artificial neural system keeps track of all constrained optimal paths flowing into the nodes of the network. This new algorithm has applications to any path planning problem where the vehicle traveling the path is subject to a limited turning capability. The ability of the network to solve for constrained paths is illustrated with both a graph theoretic example and a scenario involving an unmanned vehicle that must travel a constrained path through a real terrain area containing artificially generated keep out zones.",autonomous vehicle
10.1109/RWEEK.2018.8473535,to_check,2018 Resilience Week (RWS),IEEE,2018-08-23 00:00:00,ieeexplore,Framework for Data Driven Health Monitoring of Cyber-Physical Systems,https://ieeexplore.ieee.org/document/8473535/,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features.",health
10.1109/AERO.2005.1559665,to_check,2005 IEEE Aerospace Conference,IEEE,2005-03-12 00:00:00,ieeexplore,Health monitoring: new techniques based on vibrations measurements and identification algorithms,https://ieeexplore.ieee.org/document/1559665/,"Purpose of the paper is to present an innovative application inside the nondestructive testing field based on vibrations measurements, developed, at the Department of Aeronautical Engineering of the University of Naples ""Federico II"" (Italy), by the authors during the last three years, and already tested for analysing damages of many structural elements. The aim has been the development of a nondestructive test (NDT) which meet to most of the mandatory requirements for effective health monitoring systems, simultaneously reducing as much as possible the complexity of the data analysis algorithm and of the experimental acquisition instrumentation; these peculiarities may, in fact, not be neglected for an operative implementation of such a system. The proposed new method is based on the acquisition and comparison of frequency response functions (FRFs) of the monitored structure before and after an occurred damage. Structural damages modify the dynamical behaviour of the structure such as mass, stiffened and damping, and consequently the FRFs of the damaged structure in comparison with the FRFs of the sound structure, making possible to identify, to localize and quantify a structural damage. The activities, presented in the paper, mostly focused on a new FRFs processing technique based on the determining of a representative ""damage index"" for identifying and analysing damages both on real scale aeronautical structural components, like large-scale fuselage reinforced panels, and on aeronautical composite panels. Besides it has been carried out a dedicated neural network algorithm aiming at obtaining a ""recognition-based learning""; this kind of learning methodology permits to train the neural network in order to let it recognise only ""positive"" examples discarding as a consequence the ""negative"" ones. Within the structural NDT a ""positive"" example means ""healthy"" state of the analysed structural component and, obviously, a ""negative"" one means a ""damaged"" or perturbed state. With this object in view the neural network has been trained making use of the same FRFs of the healthy structure used for the determining of the damage index, as positive examples. From an architectural point of view magnetostrictive devices have been tested as actuators, and piezoceramic patches as actuators and sensors. Besides it has been used a laser-scanning vibrometer system to validate the behaviour of the piezoceramic patches and define their technical parameters in order to lay the bases for design a light and reliability system. These techniques promise to bring a step forward for the implementation of an automatic ""health monitoring"" system which will be able to identify a structural damage in real time, improving the safety and reducing maintenance costs",health
10.1109/PHM.2017.8079103,to_check,2017 Prognostics and System Health Management Conference (PHM-Harbin),IEEE,2017-07-12 00:00:00,ieeexplore,Keynote speech: FPGA-based machine learning for prognostics and system health management,https://ieeexplore.ieee.org/document/8079103/,"Machine learning has improved to a point where it can achieve near-human accuracy on difficult tasks such as image recognition, speech recognition, and machine translation. However, efficient implementation of machine learning algorithms, particularly for real-time applications remains a challenge. This presentation will first detail the improved Energy, Parallelism, Interface and Customisation (EPIC) opportunities offered by FPGAs over conventional technologies such as microprocessors and graphics processing units (GPUs). These enable systems with smaller footprint, operating at low power and achieving improved functionality. Next, our recent research on FPGA-based implementations of machine learning algorithms will be described. This work includes highspeed and low-latency implementations of kernel adaptive filters, random projections and binarized convolutional neural networks. The talk will conclude with a discussion of applications of this technology to prognostics and system health management.",health
10.1109/ACCESS.2018.2875677,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record,https://ieeexplore.ieee.org/document/8490816/,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings. Despite the significant increase in adoption of EHR systems, these data remain largely unexplored, but present a rich data source for knowledge discovery from patient health histories in tasks, such as understanding disease correlations and predicting health outcomes. However, the heterogeneity, sparsity, noise, and bias in these data present many complex challenges. This complexity makes it difficult to translate potentially relevant information into machine learning algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data, which is personalized for each patient. To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods. Patient2Vec produces a vector space with meaningful structure, and it achieves an area under curve around 0.799, outperforming baseline methods. In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",health
10.1109/ICASSP.2019.8682194,to_check,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2019-05-17 00:00:00,ieeexplore,1-D Convolutional Neural Networks for Signal Processing Applications,https://ieeexplore.ieee.org/document/8682194/,"1D Convolutional Neural Networks (CNNs) have recently become the state-of-the-art technique for crucial signal processing applications such as patient-specific ECG classification, structural health monitoring, anomaly detection in power electronics circuitry and motor-fault detection. This is an expected outcome as there are numerous advantages of using an adaptive and compact 1D CNN instead of a conventional (2D) deep counterparts. First of all, compact 1D CNNs can be efficiently trained with a limited dataset of 1D signals while the 2D deep CNNs, besides requiring 1D to 2D data transformation, usually need datasets with massive size, e.g., in the ""Big Data"" scale in order to prevent the well-known ""overfitting"" problem. 1D CNNs can directly be applied to the raw signal (e.g., current, voltage, vibration, etc.) without requiring any pre- or post-processing such as feature extraction, selection, dimension reduction, denoising, etc. Furthermore, due to the simple and compact configuration of such adaptive 1D CNNs that perform only linear 1D convolutions (scalar multiplications and additions), a real-time and low-cost hardware implementation is feasible. This paper reviews the major signal processing applications of compact 1D CNNs with a brief theoretical background. We will present their state-of-the-art performances and conclude with focusing on some major properties.",health
10.1109/IUCC-CSS.2016.021,to_check,2016 15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS),IEEE,2016-12-16 00:00:00,ieeexplore,A Learning System to Support Social and Empathy Disorders Diagnosis through Affective Avatars,https://ieeexplore.ieee.org/document/7828588/,"Nowadays diagnosis and treatment of cognitive and physical health issues can be empowered through the use of information technologies. However, there is a significant gap between the potential of those technologies and the real application. One example is the use of serious games with health proposals, a trending research area still not implanted in health systems. This paper proposes the use of serious games, particularly an interactive and affective avatar-based application to support the diagnosis and treatment of empathy and socialization issues, in an autonomous way through the implementation of a learning algorithm based on the ground truth obtained from the evaluation with real users, including normotypical users, users with Down syndrome and users with intellectual disability.",health
10.1109/wicom.2011.6040629,to_check,"2011 7th International Conference on Wireless Communications, Networking and Mobile Computing",IEEE,2011-09-25 00:00:00,ieeexplore,Automatic Reminder System of Medical Orders Based on Bluetooth,https://ieeexplore.ieee.org/document/6040629/,"The accurate and real-time implementation of medical orders will be directly related to the patient's health. To improve the accurate and real-time implementation of medical orders, the implementation of medical orders in real time automated reminder system based on wireless communication technology will be introduced in this paper. Through the design of embedded data acquisition, timing light alarm, time display of the automated reminder by touching the screen, the research on the extraction and transformation of the medical orders, design of time reminder device and the development of the information query system and the information management system of medical orders execution. Eventually, we could remind nurses to deal with the nursery content of patients real time and ensure the accuracy of medical advice and real-time implementation by using text messages to remind nurses and adding reminder alarm kit to the drug box.",health
10.23919/CISTI52073.2021.9476616,to_check,2021 16th Iberian Conference on Information Systems and Technologies (CISTI),IEEE,2021-06-26 00:00:00,ieeexplore,Knowledge management applying disruptive technologies as a response to the COVID-19 crisis in public administration,https://ieeexplore.ieee.org/document/9476616/,"The accelerated propagation of Sars-Cov2 and the consequent Covid-19 disease in the world has led the public administrations of different countries to use the advantages provided by disruptive technologies to effectively and efficiently manage the large volume of data generated. This communication presents the main characteristics and advances that distinguish the development of the initiative called Epidempredict for Covid19 in Panama. The project involves the implementation of a cloud platform that facilitates data analysis in a distributed, collaborative and secure form. This platform will enable efficient data ingestion, administration, analysis, visualization and export. The development will be a solution oriented to VUCA (Volatile, Uncertain, Complex and Ambiguous) environments. The implementation integrates tools and resources supported by technologies such as Artificial Intelligence (AI) and Machine Learning (ML) integrating models, predictive algorithms and dashboards that enable decision making based on real and meaningful data. The project proposes the use of a neuro-hybrid model, based on a SIR model with artificial intelligence, combining several algorithms. This initiative aims to support the health authorities of the Panamanian public administration by facilitating decision making and the adoption and implementation of precise strategic actions in the field of health and public welfare.",health
10.1109/NCCC49330.2021.9428873,to_check,2021 National Computing Colleges Conference (NCCC),IEEE,2021-03-28 00:00:00,ieeexplore,Machine Learning-Based Predictive Model for Surgical Site Infections: A Framework,https://ieeexplore.ieee.org/document/9428873/,"Surgical site infections impact hospital readmission rates, length of stay, and patient and hospital expense. The use of computational intelligence methods can help to predict the risk of SSIs and provide an early warning, enabling hospitals to prepare in advance to respond to these infections. The objective of this paper is to present a machine learning-based predictive model for surgical site infections. This paper also reviews the most recent machine learning-based models developed for the prediction of SSIs. When these predictive models are applied correctly and used effectively, they can be helpful for clinical surveillance teams. However, the implementation of these models and related tools requires quality data to be stored in electronic health records, which may not be available in all health information systems. The limitations of clinical data and the absence of labels adding several challenges with the implementation of the predictive models using machine learning; an imbalanced dataset is also a common issue that can influence the performance of the model, thus requiring an improved strategy to address this concern. Recently, the interpretability of predictive models has become important for hospital physicians to translate the models into real practices.",health
10.1109/FPL.2016.7577311,to_check,2016 26th International Conference on Field Programmable Logic and Applications (FPL),IEEE,2016-09-02 00:00:00,ieeexplore,Optimizing hardware design for Human Action Recognition,https://ieeexplore.ieee.org/document/7577311/,"Human action recognition (HAR) is an important topic in computer vision having a wide range of applications: health care, assisted living, surveillance, security, gaming, etc. Despite significant amount of work having been conducted in this area in recent years, the execution speed still limits real-time applications. Moreover, it is highly desirable to have the compute-intensive feature extraction stage done right at the output of the camera to extract and transfer only action feature in multi-camera network setting and hence reduce network bandwidth requirement. In this work, we first evaluate the possibility to perform feature extraction under reduced precision fixed-point arithmetic to ease hardware resource requirements. We compared the Histogram of Oriented Gradient in 3D (HOG3D) feature extraction with state-of-the-art Convolutional Neural Networks (CNNs) methods and shown the later to be 75× slower than the former. Our experiment shows that by re-training the classifier with reduced data precision, the classification performs as well as the original double-precision floating-point. Based on this result, we implement an FPGA-based HAR feature extraction for near camera processing using fixed-point data representation and arithmetic. This implementation, using a single Xilinx Virtex 6 FPGA, achieves about 70× speedup over multicore CPU. Furthermore, a GPU implementation of HAR is introduced with 80× speedup over CPU (on an Nvidia Tesla K20). Last but not least, a power comparison is presented for the three platforms.",health
10.1109/IMTIC.2018.8467234,to_check,2018 5th International Multi-Topic ICT Conference (IMTIC),IEEE,2018-04-27 00:00:00,ieeexplore,Sensors in Smart Homes for Independent Living of the Elderly,https://ieeexplore.ieee.org/document/8467234/,"A rapidly ageing population requires support systems which would enable them to preserve dwellers' independence without compromising on their safety or their quality of life. Smart homes for the elderly have the potential to offer unobtrusive health and wellness monitoring. The aim is to provide a safe, independent living environment which can identify and predict problems by monitoring the activities of daily living (ADLs) of the inhabitants. For this, a system able to handle continuous streams of data is required. Such a system can extract the information by using appropriate classification and learning algorithms and thus allow the remote monitoring of health and wellbeing at a high level. The implementation requires: the use of appropriate sensing technologies, identification of ADLs, data pre-processing techniques and machine learning algorithms. This is challenging due to individual differences: such a system must be able to personalize individual needs. Our contribution was the design and implementation of a platform to smartly monitor health condition of elderly using sensor data from a smart home, through an interactive user interface which is user-friendly and multi-platform. This proof-of-concept used off-line data, with the view to extend to real-time data collection in the future, which could then be used to inform support providers remotely.",health
10.1109/IEMCON.2019.8936183,to_check,"2019 IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",IEEE,2019-10-19 00:00:00,ieeexplore,User Modeling via Anomaly Detection Techniques for User Authentication,https://ieeexplore.ieee.org/document/8936183/,"Anomaly detection is quickly becoming a very significant tool for a variety of applications such as intrusion detection, fraud detection, fault detection, system health monitoring, and event detection in IoT devices. An application that lacks a strong implementation for anomaly detection is user trait modeling. User trait models expose up-to-date representation of the user so that changes in their interests, their learning progress or interactions with the system are noticed and interpreted. The reason behind the lack of adoption in user trait modeling arises from the need for a continuous flow of high-volume data, that is not available in most cases, to achieve high-accuracy detection. This paper provides new insight into the anomaly detection techniques through Big Data utilization. With Big Data characteristics, i.e., volume, variety and velocity, anomaly detection techniques have become more suitable tools for user trait modeling. User traits will be modeled by creating a security user profile for each user. This profile is structured and developed to be a source for a strong real-time user authentication method. An ingenious implementation of three models; k-means, HMM, and auto-encoder neural network has been presented that automatically and accurately build a unique pattern of the users' behavior. The implementation comprises four main steps: prediction of rare user actions, filter security potential actions, build/update a user profile, and generate a real-time (i.e. just in time) set of challenging questions. Real-world scenarios have been given showing the benefits of these challenging questions in building secure knowledge-based user authentication systems.",health
10.1109/WiMOB.2019.8923324,to_check,"2019 International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)",IEEE,2019-10-23 00:00:00,ieeexplore,eHealth Initiatives for The Promotion of Healthy Lifestyle and Allied Implementation Difficulties,https://ieeexplore.ieee.org/document/8923324/,"Research in eHealth has opened a new dimension to improve personal healthcare with the help of information and communication technologies (ICT). eHealth is an `umbrella term' for the use of ICT for health. Remote care-giving technologies (mHealth, Telehealth, Telemedicine) are an extended branch of eHealth initiatives. The concept of health e- Coaching is another promising initiative of eHealth research for real-time personalized lifestyle support. The focus of eHealth initiatives is to deliver high quality, evidence-based, secure, cost- effective, timely care to support people for sustaining a healthy lifestyle. However, the practical implementation of different eHealth initiatives has often been challenging to establish prophesied benefit. Health monitoring and fitness coaching with artificial intelligence will rule the coming decades. The pillars of e-Coaching initiatives are data collection, analysis of data, recommendation (sending the right message to the right people in the right context) and data security. An optimized system for health e-Coaching, management of a huge amount of health data, ensuring data protection are some big challenges to the eHealth researchers. Prediction of human psychology for effective e-Coaching recommendation is another level of difficulty to overcome as human behavior is constantly changing. Different eHealth initiatives for the promotion of healthy lifestyle and its implementation difficulties have been our primary focus of the review in this paper. This paper is a result of our early stage research related to `Health e-Coaching recommendation system generation'. This paper will help young eHealth researchers to have a holistic idea on different eHealth technologies, initiatives till date for the promotion of healthy lifestyle and associated implementation challenges to overcome in the future.",health
10.1109/TVLSI.2016.2633543,to_check,IEEE Transactions on Very Large Scale Integration (VLSI) Systems,IEEE,2017-04-01 00:00:00,ieeexplore,Coordinate Rotation-Based Low Complexity $K$ -Means Clustering Architecture,https://ieeexplore.ieee.org/document/7812776/,"In this brief, we propose a low-complexity architectural implementation of the K-means-based clustering algorithm used widely in mobile health monitoring applications for unsupervised and supervised learning. The iterative nature of the algorithm computing the distance of each data point from a respective centroid for a successful cluster formation until convergence presents a significant challenge to map it onto a low-power architecture. This has been addressed by the use of a 2-D Coordinate Rotation Digital Computer-based low-complexity engine for computing the n-dimensional Euclidean distance involved during clustering. The proposed clustering engine was synthesized using the TSMC 130-nm technology library, and a place and route was performed following which the core area and power were estimated as 0.36 mm<sup>2</sup> and 9.21 mW at 100 MHz, respectively, making the design applicable for low-power real-time operations within a sensor node.",health
10.1109/ITC-Egypt52936.2021.9513888,to_check,2021 International Telecommunications Conference (ITC-Egypt),IEEE,2021-07-15 00:00:00,ieeexplore,A Proposed end to end Telemedicine System based on embedded system and mobile application using CMOS wearable sensors,https://ieeexplore.ieee.org/document/9513888/,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver.",health
10.1109/ICOEI48184.2020.9142946,to_check,2020 4th International Conference on Trends in Electronics and Informatics (ICOEI)(48184),IEEE,2020-06-17 00:00:00,ieeexplore,AI-IoT based Smart Pill Expert System,https://ieeexplore.ieee.org/document/9142946/,"The paper discusses the implementation of a proposed Smart Pill Expert System (SPES) which is based on AI-IoT technology to automate pill dispensing with an effective user interface. The purpose of the proposed SPES is to provide expertise in the real-time diagnosis and thus support every individual and institution that is dependent on medication. Medical Non-Adherence (MNA) is one of the major factors of prolonged recovery, financial troubles, and premature deaths. This product is developed to be used in old age homes, hospices, and home healthcare centers and is capable of catering to the needs of single and multiple users simultaneously. With API and web services, new resources are provided for caregivers (family members, nurses, and doctors) to continuously track and monitor the users. Because of minimal human intervention, SPES has a failure rate of less than 5%.",health
10.1109/ICTAI.2019.00124,to_check,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2019-11-06 00:00:00,ieeexplore,Applying Deep Learning and Wearable Devices for Educational Data Analytics,https://ieeexplore.ieee.org/document/8995238/,"With the popularity of wearable devices, smart watches containing various sensors have been widely adopted for many healthcare applications. Yet there is rarely any research study on the possible uses of smart watches for learning analytics, particularly for analyzing students' learning activities through the physiological and/or movement data collected on their smart watches. This paper considers a pioneering and sophisticated learning analytics platform using fine-tuned deep learning models to predict students' learning activities based on the real-time data, including their heart rates, calories, three-axis accelerometer and gyroscope data, captured on wearable devices and then uploaded onto a cloud server for thorough analyses. To validate on the actual activities conducted by each student, an intelligent mobile application is developed to push instant notifications for students to report their own activities whenever the change of heart rates are deviated significantly from their normal values. Based on students' heart rates and calories, a long-short term memory (LSTM) model is built to classify students' learning states as active or not with an impressive prediction accuracy of 95% whereas another hybrid model combining both the LSTM and convolutional neural networks attains the highest prediction accuracy of 74% to predict students' specific learning activities as based on their physiological and movement data. The prototype implementation clearly demonstrates the feasibility of the proposed framework for learning analytics. More importantly, this work shed lights on various directions including the integration of noise filters to preprocess the collected data for further investigation.",health
10.1109/AICAS51828.2021.9458520,to_check,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),IEEE,2021-06-09 00:00:00,ieeexplore,ECG-TCN: Wearable Cardiac Arrhythmia Detection with a Temporal Convolutional Network,https://ieeexplore.ieee.org/document/9458520/,"Personalized ubiquitous healthcare solutions require energy-efficient wearable platforms that provide an accurate classification of bio-signals while consuming low average power for long-term battery-operated use. Single lead electrocardiogram (ECG) signals provide the ability to detect, classify, and even predict cardiac arrhythmia. In this paper we propose a novel temporal convolutional network (TCN) that achieves high accuracy while still being feasible for wearable platform use. Experimental results on the ECG5000 dataset show that the TCN has a similar accuracy (94.2%) score as the state-of-the-art (SoA) network while achieving an improvement of 16.5% in the balanced accuracy score. This accurate classification is done with 27x fewer parameters and 37x less multiply-accumulate operations. We test our implementation on two publicly available platforms, the STM32L475, which is based on ARM Cortex M4F, and the GreenWaves Technologies GAP8 on the GAPuino board, based on 1+8 RISC-V CV32E40P cores. Measurements show that the GAP8 implementation respects the real-time constraints while consuming 0.10mJ per inference. With 9.91GMAC/s/W, it is 23.0x more energy-efficient and 46.85x faster than an implementation on the ARM Cortex M4F (0.43GMAC/s/W). Overall, we obtain 8.1% higher accuracy while consuming 19.6x less energy and being 35.1x faster compared to a previous SoA embedded implementation.",health
10.1109/ULTSYM.2011.0044,to_check,2011 IEEE International Ultrasonics Symposium,IEEE,2011-10-21 00:00:00,ieeexplore,Noninvasive estimation of dynamic pressures in vitro and in vivo using the subharmonic response from microbubbles,https://ieeexplore.ieee.org/document/6293674/,"In this work, the development of subharmonic emission based noninvasive pressure estimation technique is presented. In vitro, ambient pressures were varied (between 0 and 120 mmHg) in a closed-loop flow system circulating 0.2 ml Sonazoid microbubbles (GE Healthcare, Oslo, Norway) suspended in 750 ml of isotonic diluent and recorded by a Millar pressure catheter as the reference standard. Simultaneously, a SonixRP ultrasound scanner (Ultrasonix Medical Corp., Richmond, BC, Canada) operating in pulse inversion mode (f<sub>transmit</sub>: 2.5 MHz) was used to acquire unprocessed RF data at five different incident acoustic pressures (from 76 kPa to 897 kPa; n=3). The subharmonic data for each pulse was extracted using band-pass filtering with averaging, and subsequently, processed to eliminate noise. The incident acoustic pressure most sensitive to ambient pressure fluctuations was determined; then the ambient pressure was tracked over 20 seconds. Regression analysis compared subharmonic and catheter pressure values. In vivo validation of this technique was performed noninvasively for tracking left ventricular (LV) pressures of two canines using similar post processing as in vitro. The subharmonic signal tracked ambient pressures with r<sup>2</sup> = 0.922 for 20 seconds in vitro. In vivo the subharmonic signal tracked the LV pressures with r<sup>2</sup> &gt;; 0.7P90. Maximum errors in estimating clinically relevant systolic and diastolic pressures ranged from 0.22 to 2.84 mmHg using this subharmonic technique relative to Millar catheter pressures. Clinical validation and real time implementation of this technique may ultimately lead to the first noninvasive cardiac pressure monitoring tool.",health
10.1109/ICKII51822.2021.9574661,to_check,2021 IEEE 4th International Conference on Knowledge Innovation and Invention (ICKII),IEEE,2021-07-25 00:00:00,ieeexplore,Authentication System by using HOG Face Recognition Technique and Web-Based for Medical Dispenser Machine,https://ieeexplore.ieee.org/document/9574661/,"An infectious and contagious disease affects human life significantly. Unexpectedly, monthly check-up of patients is inconvenient to receive a medical supply. Conveniently receiving medical supplies is a solution to hospital patients, eliminating crowded environments, and reducing contamination of the surface area. Therefore, this paper proposed a medical dispenser machine composed of two parts which are Web-based and face recognition in real-time. The medical profession can adjust the quantity of medicine. After the pharmacist arranged medicine, the email is distributed to the patients with the date and time for receiving packages. An automated system with a machine-learning algorithm assists them in receiving medical supply by using face recognition with Histogram Orientated Gradients (HOG) embedded in Raspberry Pi 4B. Face recognition is the ultimate technology to improve and make an impact in people's lives by diminishing contamination by viruses by not touching any surface area. The result of 50 image inputs showed an impressive recognition of authorized individuals with an accuracy of 80.0 %. This work combines hardware set up with PHP Web-based to distribute the medical supply efficiently. Subsequently, the implementation performed a satisfying performance that was suitable for the hospital.",health
10.1109/JSEN.2020.3041668,to_check,IEEE Sensors Journal,IEEE,2021-03-01 00:00:00,ieeexplore,Design of a Pose and Force Controller for a Robotized Ultrasonic Probe Based on Neural Networks and Stochastic Gradient Approximation,https://ieeexplore.ieee.org/document/9274482/,"In medicine and engineering, the implementation of a diagnostic test using an ultrasonic sensor requires suitable contact conditions, and a correct pose to attain the best signal transmission settings. A soft sensor probe provides a good surface adaptation and forces transfer, but it introduces nonlinearities and noisy measurements, making it difficult to control the probe during a real time test by conventional algorithms. In this work, a data driven controller is developed to control force and pose of a soft contact ultrasound sensor. The adaptive controller is based on a fuzzy-rules emulated network structure with the learning algorithm using a stochastic gradient approximation. The proposed control algorithm overcomes the noise environment conditions and nonlinearities of the unknown nonlinear discrete-time system. This was numerically validated and then, experimentally tested with an industrial robotic system using an ultrasonic probe designed in our lab. The results show that the proposed controller performs well under the contact-force regulation and can find the correct contact orientation with a fast convergence.",health
10.1109/ACCESS.2018.2873597,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,Hierarchical Semantic Mapping Using Convolutional Neural Networks for Intelligent Service Robotics,https://ieeexplore.ieee.org/document/8490234/,"The introduction of service robots in the public domain has introduced a paradigm shift in how robots are interacting with people, where robots must learn to autonomously interact with the untrained public instead of being directed by trained personnel. As an example, a hospital service robot is told to deliver medicine to Patient Two in Ward Three. Without awareness of what “Patient Two” or “Ward Three” is, a service robot must systematically explore the environment to perform this task, which requires a long time. The implementation of a Semantic Map allows for robots to perceive the environment similar to people by associating semantic information with spatial information found in geometric maps. Currently, many semantic mapping works provide insufficient or incorrect semantic-metric information to allow a service robot to function dynamically in human-centric environments. This paper proposes a semantic map with a hierarchical semantic organization structure based on a hybrid metric-topological map leveraging convolutional neural networks and spatial room segmentation methods. Our results are validated using multiple simulated and real environments on our lab's custom developed mobile service robot and demonstrate an application of semantic maps by providing only vocal commands. We show that this proposed method provides better capabilities in terms of semantic map labeling and retain multiple levels of semantic information.",health
10.1109/IEEEGCC.2011.5752566,to_check,2011 IEEE GCC Conference and Exhibition (GCC),IEEE,2011-02-22 00:00:00,ieeexplore,Evolutionary multiobjective optimization for medical classification,https://ieeexplore.ieee.org/document/5752566/,"We propose a computational environment based on evolutionary algorithm for medical classification. We use evolutionary multiobjective optimization (EMO) to solve a general medical minimization problem. As an example, we simultaneously minimize three objectives, namely the number of genes responsible for cancer classification while reducing the number of misclassifications in both testing and learning data sets for real patients. Results quality is reported against three genetic operators namely selection, crossover and mutation, each of which offering three different methods. Our implementation gives comparable results to more sophisticated methods, such as NGSAII-like ones, with far less computational efforts.",health
10.1109/CRV.2017.15,to_check,2017 14th Conference on Computer and Robot Vision (CRV),IEEE,2017-05-19 00:00:00,ieeexplore,Leveraging Tree Statistics for Extracting Anatomical Trees from 3D Medical Images,https://ieeexplore.ieee.org/document/8287685/,"Using different priors (e.g. shape and appearance) have proven critical for robust image segmentation of different types of target objects. Many existing methods for extracting trees (e.g. vascular or airway trees) from medical images have leveraged appearance priors (e.g. tubular-ness and bifurcationness) and the knowledge of the cross-sectional geometry (e.g. circles or ellipses) of the tree-forming tubes. In this work, we present the first method for 3D tree extraction from 3D medical images (e.g. CT or MRI) that, in addition to appearance and cross-sectional geometry priors, utilizes prior tree statistics collected from the training data. Our tree extraction method collects and leverages topological tree prior and geometrical statistics, including tree hierarchy, branch angle and length statistics. Our implementation takes the form of a Bayesian tree centerline tracking method combining the aforementioned tree priors with observed image data. We evaluated our method on both synthetic 3D datasets and real clinical CT chest datasets. For synthetic data, our method's key feature of incorporating tree priors resulted in at least 13% increase in correctly detected branches under different noise levels. For real clinical scans, the mean distance from ground truth centerlines to the detected centerlines by our method was improved by 12% when utilizing tree priors. Both experiments validate that, by incorporating tree statistics, our tree extraction method becomes more robust to noise and provides more accurate branch localization.",health
10.1109/AIMS52415.2021.9466061,to_check,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),IEEE,2021-04-30 00:00:00,ieeexplore,3D Control System of Arm Robot Prototype for Skin Cancer Detection,https://ieeexplore.ieee.org/document/9466061/,"Arm robot has a lack of control systems that depend on desired control for assistive medical. Our laboratory robotics &amp; artificial intelligent at Padjadjaran University created skin cancer detection of arm robot with dark flow framework to identify skin cancer in real-time. The implementation of the arm robot was for increasing the accuracy, precision, and stability. The main purpose of this paper was to control an arm robot for skin cancer detection that is capable to scan the whole body skin to localize the skin cancers by driving the manipulator in circular or elliptical skimming. To initiate the communication with the arm robot which used Dynamixel as the actuators, we applied USB2Dynamixel as the communicator. SMPS2Dynamixel was used to supply the power into servo motors. 3D Control system software has designed, and it had some features such as; forward kinematic movement, inverse kinematic movement, and 3D simulation to help user visualize the position of the arm robot. Control software was built in MATLAB GUI environment and 3D simulation adapted Peter Corke Robotics Toolbox.",health
10.1109/NNSP.1999.788166,to_check,Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468),IEEE,1999-08-25 00:00:00,ieeexplore,A comparative study of a hidden Markov model detector for atrial fibrillation,https://ieeexplore.ieee.org/document/788166/,"A comparative study of several atrial fibrillation (AF) detection algorithms was done to determine the algorithm best suited for use in real clinical environments to detect AF in ambulatory ECGs. The algorithms that were investigated for this paper are based on the Hidden Markov Model (HMM), measures of variance, linear predictive coding, and measurement of approximate entropy (AE). Based on the results from the test data set, the HMM algorithm performed best for this application. In general, there is little difference between the performance of the HMM and AE algorithms. However, the implementation of the HMM algorithm is more computationally efficient. Because of the large amount of data that must be analyzed in ambulatory ECG recordings, the computational efficiency must be considered as an issue of practicality. Review of the data illuminated some of the strengths and weaknesses of the various algorithms. Variance measures performed with either high sensitivity or high positive predictivity, but were not able to achieve a desirable operating point that had both acceptable sensitivity and positive predictivity. Although AE and LPC had acceptable sensitivity and positive predictivity, the HMM performed even better than both of these in terms of overall error rate. It would seem that an observational model such as the HMM, fits the data better than parametric models such as AE and LPC. Finally, as the computing power of medical systems increases, more sophisticated algorithms may be exploited in ways that leads to more accurate computerized ECG interpretation.",health
10.1109/FPL.2014.6927450,to_check,2014 24th International Conference on Field Programmable Logic and Applications (FPL),IEEE,2014-09-04 00:00:00,ieeexplore,High-throughput implementation of a million-point sparse Fourier Transform,https://ieeexplore.ieee.org/document/6927450/,"The emergence of data-intensive problems in areas like computational biology, astronomy, medical imaging, etc. has emphasized the need for fast and efficient very large Fourier Transforms. Recent work has shown that we can compute million-point transforms efficiently provided the data is sparse in the frequency domain. Processing input samples at rates approaching 1 GHz would allow real-time processing in several such applications. In this paper, we present a high-throughput FPGA implementation that performs a million-point sparse Fourier Transform on frequency-sparse input data, generating the largest 500 frequency component locations and values every 1.16 milliseconds. This design can process streamed input data at 0.86 Giga samples per second, and does not make any assumptions of the distribution of the frequency components beyond sparsity.",health
10.1109/NSSMIC.2010.5874269,to_check,IEEE Nuclear Science Symposuim & Medical Imaging Conference,IEEE,2010-11-06 00:00:00,ieeexplore,List-mode MLEM image reconstruction from 3D ML position estimates,https://ieeexplore.ieee.org/document/5874269/,"Current thick detectors used in medical imaging allow recording many attributes, such as the 3D location of interaction within the scintillation crystal and the amount of energy deposited. An efficient way of dealing with these data is by storing them in list-mode (LM). To reconstruct the data, maximum-likelihood expectation-maximization (MLEM) is efficiently applied to the list-mode data, resulting in the list-mode maximum-likelihood expectation-maximization (LMMLEM) reconstruction algorithm. In this work, we consider a PET system consisting of two thick detectors facing each other. PMT outputs are collected for each coincidence event and are used to perform 3D maximum-likelihood (ML) position estimation of location of interaction. The mathematical properties of the ML estimation allow accurate modeling of the detector blur and provide a theoretical framework for the subsequent estimation step, namely the LMM-LEM reconstruction. Indeed, a rigorous statistical model for the detector output can be obtained from calibration data and used in the calculation of the conditional probability density functions for the interaction location estimates. Our implementation of the 3D ML position estimation takes advantage of graphics processing unit (GPU) hardware and permits accurate real-time estimates of position of interaction. The LMMLEM algorithm is then applied to the list of position estimates, and the 3D radiotracer distribution is reconstructed on a voxel grid.",health
10.1109/InnoTek.2014.6877371,to_check,2014 IEEE Innovations in Technology Conference,IEEE,2014-05-16 00:00:00,ieeexplore,Preventing recreational boating fatalities and serious injuries with: The ANN (Assistant Naval Navigator) System Enterprise,https://ieeexplore.ieee.org/document/6877371/,"Because of the significant increase in motor vehicle fatalities due to texting while driving, several US States are implementing or drafting laws that will outright ban the use of handheld devices (“I” and cell Phones, pads, business communicators, etc.) while operating a motor vehicle. At the same time, US recreational boaters already are burdened with high numbers of fatal and injurious accidents, and are being encouraged to navigate using Internet-based APPs that require texting. Our analyses of trends in recent US Coast Guard's (USCG's) Boating Statistics<sup>1</sup> seem to support that accidents due to inattention are on the rise. Specifically, owner/operator inattention is causing a higher number of collisions with other recreational boats and these are accompanied by a related rise in fatalities per collision. This observation is in-line with the US Dept. of Transportation findings that texting while driving leads to more violent accidents; therefore, more fatalities/accident. Review of recent years of USCG Boating Statistics (2004 thru 2012, subsequent to the beginning of the fuel crisis) yielded several interesting conclusions. Using data 2012 as a benchmark, there were 4,515 recreational boating accidents resulting in 651 fatalities. In 73% of these fatal accidents, the owner/operators had no USCG approved instruction. Also alarming, in 13% of the 4515 accidents the cause was owner/operator inattention. Add to this, in 1010 accidents of the 4,515, these involved a collision with another recreational boat! (That's over 22% of the total accidents). It's shocking to realize that the number of moms, dads, young men and women and children that are dying and seriously injured per year, while enjoying this recreational activity, was nearly equaled the yearly US troop losses and serious injuries during the Iraq war. Over 10 years ago we began our design effort by assembling a team of experts having experience with on-water Emergency Response, and marine-related technologies. By then, the author and his direct associate had accumulated more than 70 years of successful installation and start-up of computer-based control systems for very large and small processes. The applications included major metals, chemical, plastics, ore-processing, underground coal mining industries, as well as, a medical enterprise application. We envisioned how the vessel navigation process may be adapted to feedforward/feedback control system architecture in real time. Proceeding on this basis, our team met frequently with system integrators, boat and engine manufacturers, Boat Owners Association of the US (BoatUS) Execs, towing station owner/operators and US Coast Guard Certified “A” Captains and pilots. In these meetings, our goal continued to focus on a plan for applying advanced control theory to enhance marine vessel navigation. This plan is now in readiness for productization - a viable, developed and US Patented <sup>2</sup> system with primary missions of life-saving and/serious injury prevention. Think of it! Based on our in-depth studies, we project that implementation of our ANN System will reduce the annual average of 750 recreational boating fatalities by 30% (225 fatalities)! That's the equivalent of 6 City Transit buses filled with Moms, Dads, Children, Men and Women able to go on living their life as though nothing happened - just a very enjoyable boat cruise.",health
10.1109/DCIS.2017.8311636,to_check,2017 32nd Conference on Design of Circuits and Integrated Systems (DCIS),IEEE,2017-11-24 00:00:00,ieeexplore,Random forest training stage acceleration using graphics processing units,https://ieeexplore.ieee.org/document/8311636/,"Graphics Processing Units (GPUs) are platforms very appropriated to accelerate processes with high computational load, like the supervised classification of hyperspectral images. The supervised classifier Random Forest has proved to be a good candidate to classify hyperspectral images and currently constitutes an emerging technology for medical diagnosis. The objective of this paper is focused in the Random Forest training phase acceleration using GPUs, starting from an efficient CPU implementation. For some applications, it is necessary to refine the classification model depending on the new acquired samples. In this paper are presented solutions for two bottlenecks identified in the training stage in order to accelerate the algorithm. The different solutions for the bottlenecks provided in this research study have demonstrated that GPU implementation is a promising technique to generate models in shorter time. With this implementation it is possible to achieve the training process in real-time.",health
10.1109/TBME.2007.909506,to_check,IEEE Transactions on Biomedical Engineering,IEEE,2008-03-01 00:00:00,ieeexplore,A Fully Automatic CAD-CTC System Based on Curvature Analysis for Standard and Low-Dose CT Data,https://ieeexplore.ieee.org/document/4360143/,"Computed tomography colonography (CTC) is a rapidly evolving noninvasive medical investigation that is viewed by radiologists as a potential screening technique for the detection of colorectal polyps. Due to the technical advances in CT system design, the volume of data required to be processed by radiologists has increased significantly, and as a consequence the manual analysis of this information has become an increasingly time consuming process whose results can be affected by inter- and intrauser variability. The aim of this paper is to detail the implementation of a fully integrated CAD-CTC system that is able to robustly identify the clinically significant polyps in the CT data. The CAD-CTC system described in this paper is a multistage implementation whose main system components are: 1) automatic colon segmentation; 2) candidate surface extraction; 3) feature extraction; and 4) classification. Our CAD-CTC system performs at 100% sensitivity for polyps larger than 10 mm, 92% sensitivity for polyps in the range 5 to 10 mm, and 57.14% sensitivity for polyps smaller than 5 mm with an average of 3.38 false positives per dataset. The developed system has been evaluated on synthetic and real patient CT data acquired with standard and low-dose radiation levels.",health
10.1109/JSYST.2019.2952459,to_check,IEEE Systems Journal,IEEE,2020-06-01 00:00:00,ieeexplore,Heterogeneous System-on-Chip-Based Lattice-Boltzmann Visual Simulation System,https://ieeexplore.ieee.org/document/8918263/,"Cerebral aneurysm is a cerebrovascular disorder caused by a weakness in the wall of an artery or vein, which causes a localized dilation or ballooning of the blood vessel. It is life-threatening; hence, an early and accurate diagnosis would be a great aid to medical professionals in making the correct choice of treatment. HemeLB is a massively parallel Lattice-Boltzmann simulation software, which is designed to provide the radiologist with estimates of flow rates, pressures, and shear stresses throughout the relevant vascular structures, intended to eventually permit greater precision in the choice of therapeutic intervention. However, in order to allow surgeries and doctors to view and visualize the results in real-time at medical environments, a cost-efficient, practical platform is needed. In this article, we have developed and evaluated a version of HemeLB on various heterogeneous system-on-chip platforms, allowing users to run HemeLB on a low-cost embedded platform and to visualize the simulation results in real-time. A comprehensive evaluation of implementation on the Zynq SoC and Jetson TX1 embedded graphic processing unit platforms are reported. The achieved results show that the proposed Jetson TX1 implementation outperforms the Zynq implementation by a factor of 19 in terms of site updates per second.",health
10.1147/JRD.2010.2083751,to_check,IBM Journal of Research and Development,IBM,2010-11-01 00:00:00,ieeexplore,High-quality HDR rendering technologies for emerging applications,https://ieeexplore.ieee.org/document/5643226/,"This paper introduces high-dynamic-range (HDR) technologies that enable real-time HDR rendering for emerging imaging applications on both current and upcoming platforms. The technologies introduced in this paper are based on two key methods for real-time transformation of the photographic-quality Reinhard tone-mapping operator on emerging appliances, without compromising its quality. The first allows computing convolutions “selectively,” yielding faster computation than state-of-the-art convolution techniques while requiring a significantly lower memory footprint. The second utilizes machine learning to decrease the number of required convolutions per pixel. Both methods allow for scalable parallel implementation on commodity multicore processors and embedded processors. We also extend this implementation to 3-D and higher dimensions on massively parallel architectures for possible important applications such as medical imaging and multispectral or hyperspectral imaging applications. The inverse tone-mapping operation is equally important for rendering legacy content on new HDR displays. We used the underlying tone-mapping operator to perform inverse tone mapping using the two key methods described above. While existing techniques generally extend the dynamic range from the highlights side, our new operator performs two-sided expansion, yielding enhanced details in shades (and highlights) with generally negligible visible contrast loss. Applications of these technologies include 3-D volume rendering for the medical and seismic industries, video display on HDR television screens, and next-generation digital cameras and smart phones performing on-the-fly tone mapping.",health
10.1109/ACCESS.2020.2963939,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Towards Real-Time Computing of Intraoperative Hyperspectral Imaging for Brain Cancer Detection Using Multi-GPU Platforms,https://ieeexplore.ieee.org/document/8949497/,"Several causes make brain cancer identification a challenging task for neurosurgeons during the surgical procedure. The surgeons' naked eye sometimes is not enough to accurately delineate the brain tumor location and extension due to its diffuse nature that infiltrates in the surrounding healthy tissue. For this reason, a support system that provides accurate cancer delimitation is essential in order to improve the surgery outcomes and hence the patient's quality of life. The brain cancer detection system developed as part of the “HypErspectraL Imaging Cancer Detection” (HELICoiD) European project meets this requirement exploiting a non-invasive technique suitable for medical diagnosis: the hyperspectral imaging (HSI). A crucial constraint that this system has to satisfy is providing a real-time response in order to not prolong the surgery. The large amount of data that characterizes the hyperspectral images, and the complex elaborations performed by the classification system make the High Performance Computing (HPC) systems essential to provide real-time processing. The most efficient implementation developed in this work, which exploits the Graphic Processing Unit (GPU) technology, is able to classify the biggest image of the database (worst case) in less than three seconds, largely satisfying the real-time constraint set to 1 minute for surgical procedures, becoming a potential solution to implement hyperspectral video processing in the near future.",health
10.1109/ICSMC.1991.169903,to_check,"Conference Proceedings 1991 IEEE International Conference on Systems, Man, and Cybernetics",IEEE,1991-10-16 00:00:00,ieeexplore,Fault diagnosis and neural networks,https://ieeexplore.ieee.org/document/169903/,The use of neural networks to implement a model-based fault diagnosis algorithm is discussed. The method resolves the fundamental computational complexity problem which has historically limited the applicability of model-based techniques. This is achieved by using the neural network to implement the equation solver associated with these techniques. The neural network implementation paves the way for real-time operation by transforming the online computation usually associated with model-based fault diagnosis techniques into an offline training process while simultaneously reducing the sensitivity of the algorithm to tolerance effects.&lt;<ETX>&gt;</ETX>,health
10.1109/IVS.2014.6856583,to_check,2014 IEEE Intelligent Vehicles Symposium Proceedings,IEEE,2014-06-11 00:00:00,ieeexplore,Intelligent driving diagnosis based on a fuzzy logic approach in a real environment implementation,https://ieeexplore.ieee.org/document/6856583/,"This paper considers the problem of diagnosing people's driving skills under real driving conditions using GPS data and video records. For this real environment implementation, a brand new intelligent driving diagnosis system based on fuzzy logic was developed. This system seeks to propose an abstraction of expert driving criteria for driving assessment. The analysis takes into account GPS signals such as: position, velocity, accelerations and vehicle yaw angle; because of its relation with drivers' maneuvers. In that sense, this work presents in the first place, the proposed scheme for the intelligent driving diagnosis agent in terms of its own characteristics properties, which explain important considerations about how an intelligent agent must be conceived. Secondly, it attempts to explain the scheme for the implementation of the intelligent driving diagnosis agent based on its fuzzy logic algorithm, which takes into account the analysis of real-time telemetry signals and proposed set of driving diagnosis rules for the intelligent driving diagnosis, based on a quantitative abstraction of some traffic laws and some secure driving techniques. Experimental testing has been performed in driving conditions. All tested drivers performed the driving task on real streets. The testing results show that our intelligent driving diagnosis system allows quantitative qualifications of driving performance with a high degree of reliability.",health
10.1109/PHM-Qingdao46334.2019.8942892,to_check,2019 Prognostics and System Health Management Conference (PHM-Qingdao),IEEE,2019-10-27 00:00:00,ieeexplore,Research of Equipment Fault Diagnosis Based on PHM High Performance Computing Platform,https://ieeexplore.ieee.org/document/8942892/,"Aiming at the problems of poor real-time fault diagnosis and low efficiency in the complex equipment PHM engineering maturity, a fault diagnosis implementation scheme based on PHM high performance computing platform is proposed. The BP neural network algorithm is used as an example to verify. Firstly, the current technical status and urgent needs of the existing PHM operation platform are analyzed. The overall structure and software and hardware optimization configuration of PHM high performance computing platform with FPGA and DSP as the core are expounded. Then, by means of module division, HDL design, functional verification and package testing of the time domain feature extraction method and BP neural network, the implementation of the platform fault diagnosis algorithm is carried out. Finally, combined with the analysis of a certain type of on-board fuel pump fault data, comparative analysis was carried out with the CPU platform operation. The results show that the fault diagnosis implementation proposed in this paper has high real-time performance, low resource consumption and low power consumption, which can provide an important reference for complex equipment PHM engineering applications.",health
10.1109/ACCESS.2019.2901408,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Automatic Verification and Diagnosis of Security Risk Assessments in Business Process Models,https://ieeexplore.ieee.org/document/8651587/,"Organizations execute daily activities to meet their objectives. The performance of these activities can be fundamental for achieving a business objective, but they also imply the assumption of certain security risks that might go against a company's security policies. A risk may be defined as the effects of uncertainty on the achievement of the goals of a company, some of which can be associated with security aspects (e.g., data corruption or data leakage). The execution of the activities can be choreographed using business processes models, in which the risk of the entire business process model derives from a combination of the single activity risks (executed in an isolated manner). In this paper, a risk assessment method is proposed to enable the analysis and evaluation of a set of activities combined in a business process model to ascertain whether the model conforms to the security-risk objectives. To achieve this objective, we use a business process extension with security-risk information to: 1) define an algorithm to verify the level of risk of process models; 2) design an algorithm to diagnose the risk of the activities that fail to conform to the level of risk established in security-risk objectives; and 3) the implementation of a tool that supports the described proposal. In addition, a real case study is presented, and a set of scalability benchmarks of performance analysis is carried out in order to check the usefulness and suitability of automation of the algorithms.",health
10.1109/TIM.2020.3043098,to_check,IEEE Transactions on Instrumentation and Measurement,IEEE,2021-01-01 00:00:00,ieeexplore,SASLN: Signals Augmented Self-Taught Learning Networks for Mechanical Fault Diagnosis Under Small Sample Condition,https://ieeexplore.ieee.org/document/9285302/,"The implementation of condition monitoring and fault diagnosis is of special importance for ensuring wind turbine (WT) operation safely and stably. In practice, however, the fault data of WT are limited, which makes it hard to identify faults of WT accurately using the existing intelligent diagnosis methods. To address this, signals augmented self-taught learning network (SASLN) is proposed for the fault diagnosis of the generator, which is one of the most important parts in WT. In SASLN, fault signal samples are generated by the Wasserstein distance guided generative adversarial networks to expand the limited training data set. The sufficient generated signal samples are used to pretrain the self-taught learning network (SLN) to enhance the generalization ability of SLN. Then, the weights of SLN are fine-tuned using a small number of real signal samples for accurate fault classification. The effectiveness of SASLN is verified by two bearing vibration data sets. The results show that SASLN can achieve fairly high fault classification accuracy using small training samples. Besides, SASLN has good robustness in noisy working environment and can also identify faults even in variable loads and variable rotating speeds cases, which makes it meaningful for decreasing the running costs and improving the maintenance management of WT.",health
10.1109/AISP48273.2020.9073007,to_check,2020 International Conference on Artificial Intelligence and Signal Processing (AISP),IEEE,2020-01-12 00:00:00,ieeexplore,A Real Time Wavelet Filtering for ECG Baseline Wandering Removal,https://ieeexplore.ieee.org/document/9073007/,"Electrocardiogram (ECG) signal analysis plays an unparalleled role in diagnosis of heart diseases. The raw ECG signal usually consists of low-frequency baseline wander (BW) which must be removed for a better diagnosis. This paper presents a real-time implementation of the Discrete Wavelet Transform based filtering approach for BW removal from ECG signal (obtained from MIT-BIH database) using a 16-bit, fixed-point DSP board (TMS320C5515). The results indicate that the wavelet-based filter was effective in removing BW and was efficient for a real-time implementation, when compared with the traditional FIR and IIR filters. These results were compared and validated with the offline implementations using MATLAB.",health
10.1109/AIHAS.1990.93923,to_check,"Proceedings [1990]. AI, Simulation and Planning in High Autonomy Systems",IEEE,1990-03-27 00:00:00,ieeexplore,A hierarchical and modular structure for FMS control and monitoring,https://ieeexplore.ieee.org/document/93923/,"A hierarchical modular structure is presented for real-time control and monitoring of flexible manufacturing systems (FMSs), in which a clear distinction between processing of normal and unusual states is made. It is shown that this approach furthers autonomous diagnosis and recovery, because it makes it possible to integrate two techniques well suited in their own domains: Petri nets for the specification of normal sequences of control and AI techniques for dealing with diagnostic problems. The implementation techniques proposed are similar to rapid prototyping by execution of the specification (token player for Petri nets and inference engine for production rules).&lt;<ETX>&gt;</ETX>",health
10.1109/IMTC.1998.679793,to_check,IMTC/98 Conference Proceedings. IEEE Instrumentation and Measurement Technology Conference. Where Instrumentation is Going (Cat. No.98CH36222),IEEE,1998-05-21 00:00:00,ieeexplore,An adaptive network based fuzzy diagnostic system for linear induction motor drives,https://ieeexplore.ieee.org/document/679793/,"The working conditions identification of technical systems is the challenge of technicians who deal with diagnostic problems. Sometimes, there is not a mathematical model which is able to describe the behaviour of the system or if there is its complexity do not allow a useful implementation to perform an on-line and real-time diagnostic process. In such cases the use of diagnostic techniques based on artificial intelligence is suitable. The aim of this paper is to present an adaptive-network-based fuzzy diagnostic system in which adaptive networks are used to construct as symptoms-faults mapping for linear induction motor drives. Such mapping is carried out by means of learning procedure based on experimental data measured in several normal and faulty working conditions of the system under diagnosis. The proposed diagnostic system has been experimentally validated through plenty of tests.",health
10.1109/MELCON.1994.380954,to_check,Proceedings of MELECON '94. Mediterranean Electrotechnical Conference,IEEE,1994-04-14 00:00:00,ieeexplore,An object-oriented expert system for power system alarm processing and fault identification,https://ieeexplore.ieee.org/document/380954/,"Alarm processing is a traditional feature of energy management systems (EMS) and has not changed significantly over several generations of SCADA design. However recent applications of artificial intelligence have dramatically altered the methods of handling this information. This paper describes two parts of a project carried out at the University of Dundee for Scottish Hydro-Electric plc (HE) on the use of an artificial intelligence system for alarm processing and fault diagnosis. The first part of the project was an overview and comparison study of three real-time object-oriented toolkits: Muse, Kappa and Nexpert Object. The study is based on the capabilities of such toolkits to handle the power system alarm processing integration with external programs and real-time databases, portability, price and execution speed. Some advantages and drawbacks of each toolkit are also pointed out. The second part of the project was the implementation of an object-oriented expert system using the KappaPC toolkit operating on a 486 IBM compatible PC under Microsoft Windows 3.1. The structure of the object-oriented expert system captures the heuristic knowledge used for power system operation. The knowledge-base is automatically updated by the existing SCADA system as the power system status changes. The paper also describes the features of the real-time object-oriented expert system which includes the need for fast deep-level reasoning, easy maintainability of the object-oriented programming and the end user's interface.&lt;<ETX>&gt;</ETX>",health
10.1109/R10-HTC.2018.8629836,to_check,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),IEEE,2018-12-08 00:00:00,ieeexplore,Deep Learning-Based Eye Gaze Controlled Robotic Car,https://ieeexplore.ieee.org/document/8629836/,"In recent years Eye gaze tracking (EGT) has emerged as an attractive alternative to conventional communication modes. Gaze estimation can be effectively used in human-computer interaction, assistive devices for motor-disabled persons, autonomous robot control systems, safe car driving, diagnosis of diseases and even in human sentiment assessment. Implementation in any of these areas however mostly depends on the efficiency of detection algorithm along with usability and robustness of detection process. In this context we have proposed a Convolutional Neural Network (CNN) architecture to estimate the eye gaze direction from detected eyes which outperforms all other state of the art results for Eye-Chimera dataset. The overall accuracies are 90.21% and 99.19% for Eye-Chimera and HPEG datasets respectively. This paper also introduces a new dataset EGDC for which proposed algorithm finds 86.93% accuracy. We have developed a real-time eye gaze controlled robotic car as a prototype for possible implementations of our algorithm.",health
10.1109/IECBES48179.2021.9398816,to_check,2020 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES),IEEE,2021-03-03 00:00:00,ieeexplore,Murine Atherosclerosis Detection Using Machine Learning Under Magnetic Resonance Imaging,https://ieeexplore.ieee.org/document/9398816/,"Over the past few decades, the diagnosis of heart disease, namely atherosclerosis, has been of interest for many researchers. The use of electrocardiogram (ECG) triggered Magnetic Resonance Imaging (MRI) sequences, enables exploration of aortic arches with high-resolution images. Lately, implementation of numerous Machine Learning (ML) techniques improved the accuracy of diagnosing diseases especially cardiovascular ones. In this paper, we proposed a model to diagnose atherosclerosis in mice using features extracted from the electrocardiogram (ECG) and respiratory signals. We defined new variables, after adaptation to the altered nature of electrophysiological signals with superimposed MRI noise. Using the 5-fold cross validation test, we introduced the features into the Random Forest (RF) classifier and obtained an accuracy of 96.96%. Feature selection then took place using the Info Gain (IG) attribute evaluator, which increased our model's accuracy to 97.57%. This presents a first step towards a real-time MRI synchronous cardiovascular finding system.",health
10.1109/INDIN.2014.6945484,to_check,2014 12th IEEE International Conference on Industrial Informatics (INDIN),IEEE,2014-07-30 00:00:00,ieeexplore,Online passive learning of timed automata for cyber-physical production systems,https://ieeexplore.ieee.org/document/6945484/,"Model-based approaches are very often used for diagnosis in production systems. And since the manual creation of behavior models is a tough task, many learning algorithms have been constructed for the automatic model identification. Most of them are tested and evaluated on artificial datasets on personal computers only. However, the implementation on cyber-physical production systems puts additional requirements on learning algorithms, for instance the real-time aspect or the usage of memory space. This paper analyzes the requirements on learning algorithms for cyber-physical production systems and presents an appropriate online learning algorithm, the Online Timed Automaton Learning Algorithm, OTALA. It is the first online passive learning algorithm for timed automata which in addition copes without negative learning examples. An analysis of the algorithm and comparison with offline learning algorithms completes this contribution.",health
10.1109/ICITSI.2016.7858181,to_check,2016 International Conference on Information Technology Systems and Innovation (ICITSI),IEEE,2016-10-27 00:00:00,ieeexplore,[Title page],https://ieeexplore.ieee.org/document/7858181/,"The following topics are dealt with: SDLC SPASI v. 4.0. business process; information extraction; statistics indicator tables; rule generalizations; ontology; conventional learning system; ICT-based learning; job training system; time-series data; RAID; software-based accelerator; virtualization environment; enterprise architecture government organization; TOGAF ADM; SONA; e- library; modified quantitative models for performance measurement system method; business process improvement; district government innovation service case study; government organization; m-government implementation evaluation; trusted Big Data; official statistics study case; data profiling; data quality improvement; secure internet access; copyright protection; color images; transform domain; luminance component; information network architecture; local government; software as a service; expert system; meningitis disease; certainty factor method; digital asset management system; broadcasting organizations; e-portofolio definition; system security requirement identification; electronic payment system; Internet-based long distance education; operational model data governance; requirement engineering; open government information network development; process capability assessment; information security management; information security governance; national cyber physical systems; e-learning readiness; remote control system; serial communications mobile; microcontroller; knowledge sharing; indonesia higher educational institutions; cultural heritage metadata; geo linked open data; NUSANTARA: knowledge management system; adaptive personalized learning system; interactive learning media; RPP ICT; government human capital management; knowledge management tools utilization; knowledge management readiness; analytic hierarchy process; government institutions; usability testing; scrum methodology; assistant information system; automatic arowana raiser; pSPEA2; strength Pareto evolutionary algorithm 2; early diagnosis expert system deficiency; digital forensic; user acceptance; human resource information system; automated plasmodium detection; malaria diagnosis; thin blood smear image; 3D virtual game; MOODLE; SLOODLE; open simulator case study; color-based segmentation; feature detection; ball post; goal post; mobile soccer robot game field; smart farming; real time q-log-based feature normalization; distant speech recognition; Monte Carlo localization; robot operating system; finite element method; 3D DC resistivity modeling; multi GPU; breast cancer lesions; adaptive thresholding; morphological operation; gamification framework; online training; collaborative working system; classification breast cancer ultrasound images; posterior features; three-wheeled omnidirectional robot controller; public services satisfaction; sentiment analysis; color blind test quantification; RGB primary color cluster; ERP modules requirement; micro, small and medium enterprise fashion industry; small culinary enterprises; business system requirement; small craft companies ; power analysis attack; DES and IT value model.",health
10.1109/TBME.2018.2793882,to_check,IEEE Transactions on Biomedical Engineering,IEEE,2018-10-01 00:00:00,ieeexplore,Reducing the Computational Complexity of EEG Source Localization With Cortical Patch Decomposition and Optimal Electrode Selection,https://ieeexplore.ieee.org/document/8259222/,"Objective: Real-time implementation of EEG source localization can be employed in a broad area of applications such as clinical diagnosis of neurologic diseases and brain-computer interface. However, a power-efficient, low-complexity, and real-time implementation of EEG source localization is still challenging due to extensive iterations in the solutions. In this study, two techniques are introduced to reduce the computational burden of the subspace-based MUltiple SIgnal Classification (MUSIC) algorithm. Methods: To shrink the exhaustive search inherent in MUSIC, the cortex is parsed into cortical regions. A novel nomination procedure involving a dictionary learning step will pick a number of regions to be searched for the active sources. In addition, a new electrode selection algorithm based on the Cramer-Rao bound of the errors is introduced to pick the best set of an arbitrary number of electrodes out of the total. Results: The performance of the proposed techniques were evaluated using simulated EEG signal under variation of different parameters such as the number of nominated regions, the signal to noise ratio, and the number of electrodes. The proposed techniques can reduce the computational complexity by up to $90\%$. Furthermore, the proposed techniques were tested on EEG data from an auditory oddball experiment. Conclusion: A good concordance was observed in the comparison of the topographies and the localization errors derived from the proposed technique and regular MUSIC. Significance: Such reduction can be exploited in the real-time, long-run, and mobile monitoring of cortical activity for clinical diagnosis and research purposes.",health
10.1109/ACCESS.2020.2991744,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Simplified Fourier Series Based Transistor Open-Circuit Fault Location Method in Voltage-Source Inverter Fed Induction Motor,https://ieeexplore.ieee.org/document/9084098/,"Transistors in three-phase voltage-source inverter often suffer from open-circuit failures due to the lifting of bonding wires caused by thermic cycling, resulting in performance degradation with ripple torque and current harmonics. Current-spectral-analysis based methods are widely applied to failure diagnosis; however, high calculation consumption and complex implementation limit their application in some real-time occasion. In this paper, a simplified Fourier series method is proposed by the product between reconstructed phase currents and reference signals. Meanwhile, a novel normalized method for DC and fundamental components of simplified Fourier series are proposed to locate twenty-one transistor open-circuit faults. Numerical results show that the proposed Fourier series method coincides with that of Fast Fourier Transform. Experimental results and the comparison with previous methods show high efficiency and merits of its application to transistor open-circuit fault location in the voltage-source inverter.",health
10.1007/s40137-021-00297-3,to_check,Current Surgery Reports,Springer,2021-06-08 00:00:00,springer,The Age of Artificial Intelligence: Use of Digital Technology in Clinical Nutrition,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40137-021-00297-3,"Purpose of review Computing advances over the decades have catalyzed the pervasive integration of digital technology in the medical industry, now followed by similar applications for clinical nutrition. This review discusses the implementation of such technologies for nutrition, ranging from the use of mobile apps and wearable technologies to the development of decision support tools for parenteral nutrition and use of telehealth for remote assessment of nutrition. Recent findings Mobile applications and wearable technologies have provided opportunities for real-time collection of granular nutrition-related data. Machine learning has allowed for more complex analyses of the increasing volume of data collected. The combination of these tools has also translated into practical clinical applications, such as decision support tools, risk prediction, and diet optimization. Summary The state of digital technology for clinical nutrition is still young, although there is much promise for growth and disruption in the future.",health
10.1007/978-981-15-6202-0_37,to_check,Intelligent and Cloud Computing,Springer,2021-01-01 00:00:00,springer,Artificial Intelligence for Smart Healthcare Management: Brief Study,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-6202-0_37,"In recent time, entrepreneurs have started offering smart solutions to monitor healthcare system by using artificial intelligence (AI). It provides a smart alternative to manage the whole healthcare system and helps in accurate prediction and diagnosis of various critical health conditions. Therefore, the advancement and application of AI will change the future healthcare scenario. Even though the AI-based technologies in medical field are advancing rapidly, but in real time, its implementation is yet to be achieved. AI is being used not only in detection of disease but also in allocating professionals, giving home care advises and prescribing medicines. In this article, the importance of AI in the field of healthcare management is shown. Moreover, discussion of several works published in different areas of medicine based on AI during the past few years is carried out briefly. Extending the article, an analysis of the use of AI in health system along with various challenges toward it is done.",health
10.1007/978-981-33-6815-6_7,to_check,Trends of Data Science and Applications,Springer,2021-01-01 00:00:00,springer,Role of Data Analytics in Bio Cyber Physical Systems,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-33-6815-6_7,"Data science has proved its versatility in all dynamics of the field known to mankind, making decision making faster, and accurate over the past two decades. Coupled with IoT devices and their setups, these have been forerunners in terms of data generation and accurate prognosis. According to advisory firm International Data Corporation (IDC), the number of IoT devices is forecasted to reach 41.6 Billion by 2025, and the data generated from these devices is expected to be 79.4 Zettabytes. One broad sector which has emerged as a gold mine for data generation is the Bio Cyber Physical Systems. Bio Cyber Physical Systems are based on the incorporation of computational elements with biological processes of the human body. The following chapter aims to discuss a new design, implementation of a system based on Bio-CPS, focused primarily on health wearable technologies equipped with state-of-the-art sensors, couple their data with machine learning algorithms to detect real-time health complications primarily in a diabetic person and use of long short-term memory (LSTM) for prediction of such health complications.",health
10.1007/978-981-13-0866-6_11,to_check,Internet of Things and Personalized Healthcare Systems,Springer,2019-01-01 00:00:00,springer,Diagnosis of Chest Diseases Using Artificial Neural Networks,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-13-0866-6_11,"The important health problems in the world are mainly caused due to chest diseases. A comparative chest disease diagnosis has been realized in this study by using the following neural networks such as multi-layer, probabilistic, learning vector quantization, constructive fuzzy, focused time delay, and generalized regression neural networks [ 1 ]. The back-propagation algorithm (It is a method used to calculate the error contribution of each neuron after a batch of data has been processed. It is the workhorse of learning in neural networks.) is the most popular algorithm in feed-forward neural network [ 1 ] with a multi-layer system. By adjusting the weights of artificial neural network while moving along the descending gradient direction is applicable to calculate the output error as well as the gradient of the error. The theme is to propose the implementation of back-propagation algorithm to compute and compare the percentage of the output accuracy, which is used for medical diagnosis on various chest diseases (i.e., asthma [ 2 ], tuberculosis [ 1 , 3 ], lung cancer [ 4 ]; it is an iterative procedure that generates pneumonia).",health
http://arxiv.org/abs/2102.01998v1,to_check,arxiv,arxiv,2021-02-03 10:56:58+00:00,arxiv,"Unbox the Black-box for the Medical Explainable AI via Multi-modal and
  Multi-centre Data Fusion: A Mini-Review, Two Showcases and Beyond",http://arxiv.org/abs/2102.01998v1,"Explainable Artificial Intelligence (XAI) is an emerging research topic of
machine learning aimed at unboxing how AI systems' black-box choices are made.
This research field inspects the measures and models involved in
decision-making and seeks solutions to explain them explicitly. Many of the
machine learning algorithms can not manifest how and why a decision has been
cast. This is particularly true of the most popular deep neural network
approaches currently in use. Consequently, our confidence in AI systems can be
hindered by the lack of explainability in these black-box models. The XAI
becomes more and more crucial for deep learning powered applications,
especially for medical and healthcare studies, although in general these deep
neural networks can return an arresting dividend in performance. The
insufficient explainability and transparency in most existing AI systems can be
one of the major reasons that successful implementation and integration of AI
tools into routine clinical practice are uncommon. In this study, we first
surveyed the current progress of XAI and in particular its advances in
healthcare applications. We then introduced our solutions for XAI leveraging
multi-modal and multi-centre data fusion, and subsequently validated in two
showcases following real clinical scenarios. Comprehensive quantitative and
qualitative analyses can prove the efficacy of our proposed XAI solutions, from
which we can envisage successful applications in a broader range of clinical
questions.",health
10.1016/j.eswa.2021.115863,to_check,Expert Systems with Applications,scopus,2021-12-30,sciencedirect,Auto-detection of acoustic emission signals from cracking of concrete structures using convolutional neural networks: Upscaling from specimen,https://api.elsevier.com/content/abstract/scopus_id/85114837702,"Acoustic emission (AE) monitoring has gained significant interest as a promising method for monitoring of changes in structural integrity and durability. Long-term AE monitoring needs to detect and distinguish crack signals from ambient noise (or dummy) signals; however, it is still a daunting task which currently limits field implementation of the AE method. Herein, we explore the feasibility of using convolutional neural network (CNN) models to detect AE crack signals from ambient signals. The trained models are validated both with noise-embedded synthesized signals and with upscaled physical model experiments simulating earthquake loading to a scaled model foundation by using a large-scale shaking table. The 2D CNN model trained the laboratory-synthesized signal sets effectively captured the crack and crack-free signals in all cases including the upscaled physical model experiments. This study presents a simple but robust CNN model for pre-filtering of crack signals and a novel training method for enhanced accuracy, which can be applied for real-time structural health monitoring of concrete-based structures.",health
10.1016/j.compmedimag.2021.101956,to_check,Computerized Medical Imaging and Graphics,scopus,2021-09-01,sciencedirect,Automated three-dimensional vessel reconstruction based on deep segmentation and bi-plane angiographic projections,https://api.elsevier.com/content/abstract/scopus_id/85111016853,"Automated three-dimensional (3D) blood vessel reconstruction to improve vascular diagnosis and therapeutics is a challenging task in which the real-time implementation of automatic segmentation and specific vessel tracking for matching artery sequences is essential. Recently, a deep learning-based segmentation technique has been proposed; however, existing state-of-the-art deep architectures exhibit reduced performance when they are employed using real in-vivo imaging because of serious issues such as low contrast and noise contamination of the X-ray images. To overcome these limitations, we propose a novel methodology composed of the de-haze image enhancement technique as pre-processing and multi-level thresholding as post-processing to be applied to the lightweight multi-resolution U-shaped architecture. Specifically, (1) bi-plane two-dimensional (2D) vessel images were extracted simultaneously using the deep architecture, (2) skeletons of the vessels were computed via a morphology operation, (3) the corresponding skeleton structure between image sequences was matched using the shape-context technique, and (4) the 3D centerline was reconstructed using stereo geometry. The method was validated using both in-vivo and in-vitro models. The results show that the proposed technique could improve the segmentation quality, reduce computation time, and reconstruct the 3D skeleton automatically. The algorithm accurately reconstructed the phantom model and the real mouse vessel in 3D in 2 s. Our proposed technique has the potential to allow therapeutic micro-agent navigation in clinical practice, thereby providing the 3D position and orientation of the vessel.",health
10.1016/j.jstrokecerebrovasdis.2021.105962,to_check,Journal of Stroke and Cerebrovascular Diseases,scopus,2021-09-01,sciencedirect,StrokeWatch: An Instrument for Objective Standardized Real-Time Measurement of Door-to-Needle Times in Acute Ischemic Stroke Treatment,https://api.elsevier.com/content/abstract/scopus_id/85109982805,"Objectives
                  Monitoring critical time intervals in acute ischemic stroke treatment delivers metrics for quality of performance – the door-to-needle time being well-established. To resolve the conflict of self-reporting bias a “StrokeWatch” was designed – an instrument for objective standardized real-time measurement of procedural times.
               
                  Materials and methods
                  An observational, monocentric analysis of patients receiving intravenous thrombolysis for acute ischemic stroke between January 2018 and September 2019 was performed based on an ongoing investigator-initiated, prospective, and blinded endpoint registry. Patient data and treatment intervals before and after introduction of ""StrokeWatch"" were compared.
               
                  Results
                  “StrokeWatch” was designed as a mobile board equipped with three digital stopwatches tracking door-to-needle, door-to-groin, and door-to-recanalization intervals as well as a form for standardized documentation. 118 patients before introduction of “StrokeWatch” (subgroup A) and 53 patients after introduction of “StrokeWatch” (subgroup B) were compared. There were no significant differences in baseline characteristics, procedural times, or clinical outcome. A non-significant increase in patients with door-to-needle intervals of 60 min or faster (93.2 vs 98.1%, p = 0.243) and good functional outcome (mRS d90 ≤ 2, 47.5 vs 58.5%, p = 0.218) as well as a significant increase in reports of delayed arrival of intra-hospital patient transport service (0.8 vs 13.2%, p = 0.001) were observed in subgroup B.
               
                  Conclusions
                  The implementation of StrokeWatch for objective standardized real-time measurement of door-to-needle times is feasible in a real-life setting without negative impact on procedural times or outcome. It helped to reassure a high-quality treatment standard and reveal factors associated with procedural delays.",health
10.1016/j.jaip.2021.03.039,to_check,Journal of Allergy and Clinical Immunology: In Practice,scopus,2021-07-01,sciencedirect,Predicting Severe Asthma Exacerbations in Children: Blueprint for Today and Tomorrow,https://api.elsevier.com/content/abstract/scopus_id/85106217297,"Severe asthma exacerbations are the primary cause of morbidity and mortality in children with asthma. Accurate prediction of children at risk for severe exacerbations, defined as those requiring systemic corticosteroids, emergency department visit, and/or hospitalization, would considerably reduce health care utilization and improve symptoms and quality of life. Substantial progress has been made in identifying high-risk exacerbation-prone children. Known risk factors for exacerbations include demographic characteristics (ie, low income, minority race/ethnicity), poor asthma control, environmental exposures (ie, aeroallergen exposure/sensitization, concomitant viral infection), inflammatory biomarkers, genetic polymorphisms, and markers from other “omic” technologies. The strongest risk factor for a future severe exacerbation remains having had one in the previous year. Combining risk factors into composite scores and use of advanced predictive analytic techniques such as machine learning are recent methods used to achieve stronger prediction of severe exacerbations. However, these methods are limited in prediction efficiency and are currently unable to predict children at risk for impending (within days) severe exacerbations. Thus, we provide a commentary on strategies that have potential to allow for accurate and reliable prediction of children at risk for impending exacerbations. These approaches include implementation of passive, real-time monitoring of impending exacerbation predictors, use of population health strategies, prediction of severe exacerbation responders versus nonresponders to conventional exacerbation management, and considerations for preschool-age children who can be especially high risk. Rigorous prediction and prevention of severe asthma exacerbations is needed to advance asthma management and improve the associated morbidity and mortality.",health
10.1016/j.ijpe.2021.108114,to_check,International Journal of Production Economics,scopus,2021-06-01,sciencedirect,Machine learning-based predictive maintenance: A cost-oriented model for implementation,https://api.elsevier.com/content/abstract/scopus_id/85104317955,"Predictive Maintenance (PdM) is a condition-based maintenance strategy (CBM) that carries out maintenance action when needed, avoiding unnecessary preventive actions or failures. Machine learning (ML), in the form of advanced monitoring and diagnosis technologies, has become increasingly attractive. Implementing ML-based PdM is a difficult and expensive process, especially for those companies which often lack the necessary skills and financial and labour resources. Thus, a cost-oriented analysis is required to define when ML-based PdM is the most suitable maintenance strategy. The implementation of this strategy involves investment costs in IT technologies, in addition to costs incurred from traditional maintenance activities depending of the performance of the ML model classifier; however, no previous research consider both costs in the economic evaluation of PdM.This paper aims to provide a mathematical model where investment costs are included and the ML performance is evaluated in terms of the probability to correctly intercept faults. A error matrix is used to quantify costs due to maintenance actions. Moreover, the mathematical model provides a cost-based quantitative method, based on the Receiver Operating Characteristics (ROC) curve. This optimizes the decision threshold of the ML model classifier, which allows the maintenance costs to be minimized in comparison to traditional decision threshold optimisation methods. Based on the mathematical model, a useful Decision Support System (DSS) that guides PdM implementation is introduced. Finally, the DSS is applied to a real case study to illustrate its applicability.",health
10.1016/j.compeleceng.2021.107036,to_check,Computers and Electrical Engineering,scopus,2021-05-01,sciencedirect,Screening of Glaucoma disease from retinal vessel images using semantic segmentation,https://api.elsevier.com/content/abstract/scopus_id/85101383196,"A timely diagnosis of Glaucoma has crucial importance in preventing blindness. As this disease exists in the immediate vicinity of the optical disk (OD), its precise localization and segmentation are critical in its accurate diagnosis. OD consists of two parts, namely: neuroretinal and optic cup (OC). In the proposed work, the problem of OD and OC segmentation is modeled as a semantic pixel-wise labeling problem, thus bridging the gap between medical image segmentation and semantic segmentation. The proposed method eliminates the need for pre- and post-processing steps. The proposed method is evaluated for the segmentation of OD and OC on Drishti and Rim-one datasets. The offered low computational and resource requirements along with the observed state-of-the-art accuracy of the proposed method support its implementation in the real-time automatic screening of the Glaucoma disease.",health
10.1016/j.ymssp.2020.107398,to_check,Mechanical Systems and Signal Processing,scopus,2021-04-01,sciencedirect,1D convolutional neural networks and applications: A survey,https://api.elsevier.com/content/abstract/scopus_id/85095978325,"During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.",health
10.1016/j.jcv.2021.104757,to_check,Journal of Clinical Virology,scopus,2021-03-01,sciencedirect,"Evaluation of a measles virus multiplex, triple-target real-time RT-PCR in three specimen matrices at a U.S. academic medical center",https://api.elsevier.com/content/abstract/scopus_id/85101360201,"Background
                  Measles virus (MeV) is an important cause of acute febrile illness and pediatric mortality globally, with recent U.S. outbreaks associated with under-vaccination. MeV is highly contagious and timely diagnosis is critical to limit spread. RNA detection is the most sensitive method for acute measles diagnosis; however, MeV nucleic acid amplification assays are not widely available.
               
                  Methods
                  We performed a diagnostic accuracy study of a triple-target, real-time RT-PCR (rRT-PCR) assay for simultaneous detection of MeV N, H, and L genes.
               
                  Results
                  The MeV triple-target rRT-PCR was tested against serial dilutions (7.0−2.0 log10 copies/mL) of five MeV isolates representing circulating genotypes, and detected 98.7% (74/75) of nasopharyngeal (NP) swab dilutions, 100% (75/75) of plasma dilutions, and 85.3% (64/75) of urine dilutions. MeV RNA detection in urine was markedly improved with the addition of a nucleic acid stabilizing agent. A 95% lower limit of detection (LLOD) of < 3.0 log10 copies/mL was established in each specimen matrix. No cross-reactivity with relevant viruses or interfering substances were identified in specificity studies. The MeV triple-target rRT-PCR detected all three gene targets in a clinical NP swab from an individual with confirmed measles infection. Furthermore, pooled testing from 798 influenza A/B/RSV-negative pediatric NP swabs identified two specimens positive for MeV RNA, confirmed by N gene sequencing to represent shedding of the vaccine-type measles virus.
               
                  Conclusions
                  The MeV triple-target rRT-PCR assay showed high analytic sensitivity across circulating MeV genotypes in three clinically-relevant matrices. Implementation of this assay in the clinical laboratory may facilitate timely diagnosis of acute measles infection and implementation of appropriate infection control interventions.",health
10.1016/j.ymssp.2020.107061,to_check,Mechanical Systems and Signal Processing,scopus,2021-01-01,sciencedirect,Recovering compressed images for automatic crack segmentation using generative models,https://api.elsevier.com/content/abstract/scopus_id/85086994715,"In a structural health monitoring (SHM) system that uses digital cameras to monitor cracks of structural surfaces, techniques for reliable and effective data compression are essential to ensure a stable and energy-efficient crack images transmission in wireless devices, e.g., drones and robots with high definition cameras installed. Compressive sensing (CS) is a signal processing technique that allows accurate recovery of a signal from a sampling rate much smaller than the limitation of the Nyquist sampling theorem. Different from the popular approach of simultaneously training encoder and decoder using neural network models, the CS theory ensures a high probability of accurate signal reconstruction based on random measurements that is shorter than the length of the original signal under a sparsity constraint. Such method is particularly useful when measurements are expensive, such as wireless sensing of civil structures, because its hardware implementation allows down sampling of signals during the sensing process. Hence, CS methods can achieve significant energy saving for the sensing devices. However, the strong assumption of the signals being highly sparse in an invertible space is relatively hard to guarantee for many real images, such as image of cracks. In this paper, we present a new approach of CS that replaces the sparsity regularization with a generative model that is able to effectively capture a low dimension representation of targeted images. We develop a recovery framework for automatic crack segmentation of compressed crack images based on this new CS method. We demonstrate the remarkable performance of our method that takes advantage of the strong capability of generative models to capture the necessary features required in the crack segmentation task even the backgrounds of the generated images are not well reconstructed. The superior performance of our recovery framework is illustrated by comparisons to three existing CS algorithms. Furthermore, we show that our framework is potentially extensible to other common problems in automatic crack segmentation, such as defect recovery from motion blurring and occlusion.",health
10.1016/j.exppara.2020.108014,to_check,Experimental Parasitology,scopus,2020-12-01,sciencedirect,Analytical sensitivity of loopamp and quantitative real-time PCR on dried blood spots and their potential role in monitoring human African trypanosomiasis elimination,https://api.elsevier.com/content/abstract/scopus_id/85092023989,"The objective set by WHO to reach elimination of human African trypanosomiasis (HAT) as a public health problem by 2020 is being achieved. The next target is the interruption of gambiense-HAT transmission in humans by 2030. To monitor progress towards this target, in areas where specialized local HAT control capacities will disappear, is a major challenge. Test specimens should be easily collectable and safely transportable such as dried blood spots (DBS). Monitoring tests performed in regional reference centres should be reliable, cheap and allow analysis of large numbers of specimens in a high-throughput format. The aim of this study was to assess the analytical sensitivity of Loopamp, M18S quantitative real-time PCR (M18S qPCR) and TgsGP qPCR as molecular diagnostic tests for the presence of Trypanosoma brucei gambiense in DBS. The sensitivity of the Loopamp test, with a detection limit of 100 trypanosomes/mL, was in the range of parasitaemias commonly observed in HAT patients, while detection limits for M18S and TgsGP qPCR were respectively 1000 and 10,000 trypanosomes/mL. None of the tests was entirely suitable for high-throughput use and further development and implementation of sensitive high-throughput molecular tools for monitoring HAT elimination are needed.",health
10.1016/j.compbiomed.2020.104004,to_check,Computers in Biology and Medicine,scopus,2020-10-01,sciencedirect,Scalable and energy efficient seizure detection based on direct use of compressively-sensed EEG data on an ultra low power multi-core architecture,https://api.elsevier.com/content/abstract/scopus_id/85091921119,"Extracting information from dense multi-channel neural sensors for accurate diagnosis of brain disorders necessitates computationally expensive and advanced signal processing approaches to analyze the massive volume of recorded data. Compressive Sensing (CS) is an efficient method for reducing the computational complexity and power consumption in the resource-constrained multi-site neural systems. However, reconstructing the signal from compressed measurements is computationally intensive, making it unsuitable for real-time applications such as seizure detection. In this paper, a seizure detection algorithm is proposed to overcome these limitations by circumventing the reconstruction phase and directly processing the compressively sampled EEG signals. The Lomb-Scargle Periodogram (LSP) is used to extract the spectral energy features of the compressed data. Performance of the seizure detector using non-linear support vector machine (SVM) classifier, tested on 24 patients of the CHB-MIT data-set for compression ratios (CR) of 1–64x, is 96–93%, 92-87%, 0.95–0.91, and <1 s for sensitivity, accuracy, the area under the curve, and latency, respectively. A power-efficient classification method based on the utilization of dual linear SVM classifiers is proposed. The proposed classification method based on the dual linear SVM classification achieved better classification performance compared to commonly used classifiers, such as K-nearest neighbor, random forest, artificial neural network, and linear SVM, while consuming low power in comparison to non-linear SVM kernels. The hardware-optimized implementation of this algorithm is proposed on a low-power multi-core SoC for near-sensor data analytics: Mr. Wolf. Optimized implementation of this algorithm on Mr. Wolf platform leads to detecting a seizure with an energy budget of 18.4 μJ and 3.9 μJ for a compression ratio of 24x using non-linear SVM classifier and the dual linear SVM based classification method, respectively.",health
10.1016/j.jocn.2020.04.125,to_check,Journal of Clinical Neuroscience,scopus,2020-09-01,sciencedirect,Tele-robotics and artificial-intelligence in stroke care,https://api.elsevier.com/content/abstract/scopus_id/85088965193,"In the last forty years, the field of medicine has experienced dramatic shifts in technology-enhanced surgical procedures – from its initial use in 1985 for neurosurgical biopsies to current implementation of systems such as magnetic-guided catheters for endovascular procedures. Systems such as the Niobe Magnetic Navigation system and CorPath GRX have allowed for utilization of a fully integrated surgical robotic systems for perioperative manipulation, as well as tele-controlled manipulation systems for telemedicine. These robotic systems hold tremendous potential for future implementation in cerebrovascular procedures, but lack of relevant clinical experience and uncharted ethical and legal territory for real-life tele-robotics have stalled their adoption for neurovascular surgery, and might present significant challenges for future development and widespread implementation. Yet, the promise that these technologies hold for dramatically improving the quality and accessibility of cerebrovascular procedures such as thrombectomy for acute stroke, drives the research and development of surgical robotics. These technologies, coupled with artificial intelligence (AI) capabilities such as machine learning, deep-learning, and outcome-based analyses and modifications, have the capability to uncover new dimensions within the realm of cerebrovascular surgery.",health
10.1016/j.micpro.2019.102960,to_check,Microprocessors and Microsystems,scopus,2020-03-01,sciencedirect,A novel hybrid optimized and adaptive reconfigurable framework for the implementation of hybrid bio-inspired classifiers for diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85077060597,"Due to recent advances in IoT (Internet of Things) technologies, availability of reliable data and emergence of machine learning, bio-inspired learning and artificial intelligence, has demonstrated its ability to solve the large complex problems which is not possible before. In particular, machine learning and bio-inspired learning algorithms provides the effective solutions in image processing techniques. However, the implementation of the above-mentioned algorithms in the general CPU requires the intensive usage of bandwidth, area and power which makes the CPU unhealthy of usage and implementation. To overcome this problem, ASIC (application specific integrated circuits), GPU (Graphics Processing Unit) &FPGA (Field Programmable gate arrays) have been employed to improve the performance of the hybrid machine learning (ML) classifiers and deep learning algorithms. FPGA has been recently employed for an effective implementation and to achieve the high performance of the learning algorithms. But integrating the complex learning algorithms in FPGA still remains to be real challenge among the researchers. The paper proposes new reconfigurable architectures for bio- inspired classifiers to diagnosis the medical casualties which can be suitable for the tele health care applications. This paper aim is as follows (i) Design and implementation of Parallel Fusion of FSM and Reconfigurable shared Distributed Arithmetic for Bio-Inspired Classifiers (ii) Development of Accelerator Environment to test the performance of proposed architecture (iii) Performance evaluation of proposed architecture in terms of accuracy of detection in compared with MATLAB simulation iv) Implementation of proposed architectures in different ARtix-7 architectures and determination of power, throughput and area . Moreover, the proposed architecture has been tested with the and compared with the other existing architectures.",health
10.1016/j.imu.2020.100335,to_check,Informatics in Medicine Unlocked,scopus,2020-01-01,sciencedirect,Spark Architecture for deep learning-based dose optimization in medical imaging,https://api.elsevier.com/content/abstract/scopus_id/85084287220,"Background and objectives
                  Deep Learning (DL) and Machine Learning (ML) have brought several breakthroughs to biomedical image analysis by making available more consistent and robust tools for the identification, classification, reconstruction, denoising, quantification, and segmentation of patterns in biomedical images. Recently, some applications of DL and ML in Computed Tomography (CT) scans for low dose optimization were developed. Nowadays, DL algorithms are used in CT to perform replacement of missing data (processing technique) such as low dose to high dose, sparse view to full view, low resolution to high resolution, and limited angle to full angle. Thus, DL comes with a new vision to process biomedical data imagery from CT scan. It becomes important to develop architectures and/or methods based on DL algorithms for minimizing radiation during a CT scan exam thanks to reconstruction and processing techniques.
               
                  Methods
                  This paper describes DL for CT scan low dose optimization, shows examples described in the literature, briefly discusses new methods used in CT scan image processing, and offers conclusions. We based our study on the literature and proposed a pipeline for low dose CT scan image reconstruction. Our proposed pipeline relies on DL and the Spark Framework using MapReduce programming. We discuss our proposed pipeline with those proposed in the literature to conclude the efficiency and importance.
               
                  Results
                  An architecture for low dose optimization using CT imagery is suggested. We used the Spark Framework to design the architecture. The proposed architecture relies on DL, and permits us to develop efficient and appropriate methods to process dose optimization with CT scan imagery. The real implementation of our pipeline for image denoising shows that we can reduce the radiation dose, and use our proposed pipeline to improve the quality of the captured image.
               
                  Conclusion
                  The proposed architecture based on DL is complete and enables faster processing of biomedical CT imagery as compared with prior methods described in the literature.",health
10.1016/j.gie.2019.03.019,to_check,Gastrointestinal Endoscopy,scopus,2019-07-01,sciencedirect,Quality assurance of computer-aided detection and diagnosis in colonoscopy,https://api.elsevier.com/content/abstract/scopus_id/85065917454,"Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field “deep learning,” have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice—polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing.",health
10.1016/j.crad.2019.02.006,to_check,Clinical Radiology,scopus,2019-05-01,sciencedirect,Artificial intelligence in breast imaging,https://api.elsevier.com/content/abstract/scopus_id/85062980487,"This article reviews current limitations and future opportunities for the application of computer-aided detection (CAD) systems and artificial intelligence in breast imaging. Traditional CAD systems in mammography screening have followed a rules-based approach, incorporating domain knowledge into hand-crafted features before using classical machine learning techniques as a classifier. The first commercial CAD system, ImageChecker M1000, relies on computer vision techniques for pattern recognition. Unfortunately, CAD systems have been shown to adversely affect some radiologists' performance and increase recall rates. The Digital Mammography DREAM Challenge was a multidisciplinary collaboration that provided 640,000 mammography images for teams to help decrease false-positive rates in breast cancer screening. Winning solutions leveraged deep learning's (DL) automatic hierarchical feature learning capabilities and used convolutional neural networks. Start-ups Therapixel and Kheiron Medical Technologies are using DL for breast cancer screening. With increasing use of digital breast tomosynthesis, specific artificial intelligence (AI)-CAD systems are emerging to include iCAD's PowerLook Tomo Detection and ScreenPoint Medical's Transpara. Other AI-CAD systems are focusing on breast diagnostic techniques such as ultrasound and magnetic resonance imaging (MRI). There is a gap in the market for contrast-enhanced spectral mammography AI-CAD tools. Clinical implementation of AI-CAD tools requires testing in scenarios mimicking real life to prove its usefulness in the clinical environment. This requires a large and representative dataset for testing and assessment of the reader's interaction with the tools. A cost-effectiveness assessment should be undertaken, with a large feasibility study carried out to ensure there are no unintended consequences. AI-CAD systems should incorporate explainable AI in accordance with the European Union General Data Protection Regulation (GDPR).",health
10.1016/j.therap.2018.12.002,to_check,Therapie,scopus,2019-02-01,sciencedirect,"Early access to health products in France: Major advances of the French “Conseil stratégique des industries de santé” (CSIS) to be implemented (modalities, regulations, funding)",https://api.elsevier.com/content/abstract/scopus_id/85061149651,"In a context of perpetual evolution of treatments, access to therapeutic innovation is a major challenge for patients and the various players involved in the procedures of access to medicines. The revolutions in genomic and personalized medicine, artificial intelligence and biotechnology will transform the medicine of tomorrow and the organization of our health system. It is therefore fundamental that France prepares for these changes and supports the development of its companies in these new areas. The recent “Conseil stratégique des industries de santé” launched by Matignon makes it possible to propose a regulatory arsenal conducive to the implementation and diffusion of therapeutic innovations. In this workshop, we present a number of proposals, our approach having remained pragmatic with a permanent concern to be effective in the short term for the patients and to simplify the procedures as much as possible. This was achieved thanks to the participation in this workshop of most of the players involved (industrial companies, “Agence nationale de sécurité du médicament et des produits de santé”, “Haute Autorité de santé”, “Institut national du cancer”, “Les entreprises du médicament”, hospitals, “Observatoire du médicament, des dispositifs médicaux et de l’innovation thérapeutique”…). The main proposals tend to favor the implementation of clinical trials on our territory, especially the early phases, a wider access to innovations by favoring early access programs and setting up a process called “autorisation temporaire d’utilisation d’extension” (ATUext) that make it possible to prescribe a medicinal product even if the latter has a marketing authorisation in another indication. In addition, we propose a conditional reimbursement that will be available based on preliminary data but will require re-evaluation based on consolidated data from clinical trials and/or real-life data. Finally, in order to better carry out these assessments, with a view to access or care, we propose the establishment of partnership agreements with health agencies/hospitals in order to encourage the emergence of field experts, in order to prioritize an ascending expertise closer to patients’ needs and to real life.",health
10.1016/j.jagp.2019.05.013,to_check,American Journal of Geriatric Psychiatry,scopus,2019-01-01,sciencedirect,A Future Research Agenda for Digital Geriatric Mental Healthcare,https://api.elsevier.com/content/abstract/scopus_id/85067070294,"The proliferation of mobile, online, and remote monitoring technologies in digital geriatric mental health has the potential to lead to the next major breakthrough in mental health treatments. Unlike traditional mental health services, digital geriatric mental health has the benefit of serving a large number of older adults, and in many instances, does not rely on mental health clinics to offer real-time interventions. As technology increasingly becomes essential in the everyday lives of older adults with mental health conditions, these technologies will provide a fundamental service delivery strategy to support older adults’ mental health recovery. Although ample research on digital geriatric mental health is available, fundamental gaps in the scientific literature still exist. To begin to address these gaps, we propose the following recommendations for a future research agenda: 1) additional proof-of-concept studies are needed; 2) integrating engineering principles in methodologically rigorous research may help science keep pace with technology; 3) studies are needed that identify implementation issues; 4) inclusivity of people with a lived experience of a mental health condition can offer valuable perspectives and new insights; and 5) formation of a workgroup specific for digital geriatric mental health to set standards and principles for research and practice. We propose prioritizing the advancement of digital geriatric mental health research in several areas that are of great public health significance, including 1) simultaneous and integrated treatment of physical health and mental health conditions; 2) effectiveness studies that explore diagnostics and treatment of social determinants of health such as “social isolation” and “loneliness;” and 3) tailoring the development and testing of innovative strategies to minority older adult populations.",health
10.1016/j.cmpb.2018.08.005,to_check,Computer Methods and Programs in Biomedicine,scopus,2018-10-01,sciencedirect,Fast unsupervised nuclear segmentation and classification scheme for automatic allred cancer scoring in immunohistochemical breast tissue images,https://api.elsevier.com/content/abstract/scopus_id/85051670704,"Background and objective
                  This paper presents an improved scheme able to perform accurate segmentation and classification of cancer nuclei in immunohistochemical (IHC) breast tissue images in order to provide quantitative evaluation of estrogen or progesterone (ER/PR) receptor status that will assist pathologists in cancer diagnostic process.
               
                  Methods
                  The proposed segmentation method is based on adaptive local thresholding and an enhanced morphological procedure, which are applied to extract all stained nuclei regions and to split overlapping nuclei. In fact, a new segmentation approach is presented here for cell nuclei detection from the IHC image using a modified Laplacian filter and an improved watershed algorithm. Stromal cells are then removed from the segmented image using an adaptive criterion in order to get fast tumor nuclei recognition. Finally, unsupervised classification of cancer nuclei is obtained by the combination of four common color separation techniques for a subsequent Allred cancer scoring.
               
                  Results
                  Experimental results on various IHC tissue images of different cancer affected patients, demonstrate the effectiveness of the proposed scheme when compared to the manual scoring of pathological experts. A statistical analysis is performed on the whole image database between immuno-score of manual and automatic method, and compared with the scores that have reached using other state-of-art segmentation and classification strategies. According to the performance evaluation, we recorded more than 98% for both accuracy of detected nuclei and image cancer scoring over the truths provided by experienced pathologists which shows the best correlation with the expert's score (Pearson's correlation coefficient = 0.993, p-value < 0.005) and the lowest computational total time of 72.3 s/image (±1.9) compared to recent studied methods.
               
                  Conclusions
                  The proposed scheme can be easily applied for any histopathological diagnostic process that needs stained nuclear quantification and cancer grading. Moreover, the reduced processing time and manual interactions of our procedure can facilitate its implementation in a real-time device to construct a fully online evaluation system of IHC tissue images.",health
10.1016/j.ergon.2018.06.005,to_check,International Journal of Industrial Ergonomics,scopus,2018-09-01,sciencedirect,Artificial intelligence models for predicting the performance of hydro-pneumatic suspension struts in large capacity dump trucks,https://api.elsevier.com/content/abstract/scopus_id/85049336711,"Large dump trucks are being matched with large shovels to achieve bulk economic production in surface mining operations. This process results in high impact shovel loading operations (HISLO) and exposes operators to severe levels of whole-body vibrations (WBV). The performance of the hydro-pneumatic suspension struts, responsible for vibration attenuation in large dump trucks, decreases as a truck age. There is a need for a system for monitoring and predicting the performance of the suspension struts in real time. Artificial intelligence (AI) has been applied for modeling and predicting the suspension system performance for light/smaller vehicles. However, no work has been done to implement AI for modeling and predicting the performance of hydro-pneumatic struts in large dump trucks. This paper is a pioneering effort towards developing AI models for solving this problem. These AI models would incorporate the Artificial Neural Networks (ANN), Mamdani Fuzzy Logic (MFL) and a hybrid system, the Hybrid Neural Fuzzy Interference System (HyFIS), for achieving this goal. Experiments were conducted using a 3D virtual simulator for the CAT 793D in MSC.ADMAS. RMS accelerations in the vertical and horizontal directions at the operator seat were recorded as the two main outputs for the suspension system performance. Eighty percent (80%) of the total experimental data was used in training and developing the models and the remaining 20% for testing and validating the developed models. With an R2 and RMSE of 0.98168505 and 0.00852251 for the training phase, respectively, and 0.9660429 and 0.0195620 for the testing phase, HyFIS model showed the best accuracy for predicting the hydro-pneumatic suspension struts performance for dump trucks. This is the first time that AI models have been developed for dump truck suspension system performance prediction. With the implementation of these models in the dump truck, maintenance personnel can monitor the performance of the suspension system in real-time and schedule proper maintenance and/or replacement. Implementation of such a system will improve the workplace safety, operator's health and the overall system efficiency.",health
10.1016/j.bdr.2018.05.001,to_check,Big Data Research,scopus,2018-09-01,sciencedirect,Efficient In-Database Patient Similarity Analysis for Personalized Medical Decision Support Systems,https://api.elsevier.com/content/abstract/scopus_id/85047072435,"Patient similarity analysis is a precondition to apply machine learning technology on medical data. In this sense, patient similarity analysis harnesses the information wealth of electronic medical records (EMRs) to support medical decision making. A pairwise similarity computation can be used as the basis for personalized health prediction. With n patients the amount of 
                        (
                        
                           
                              
                                 n
                              
                           
                           
                              
                                 2
                              
                           
                        
                        )
                      similarity calculations is required. Thus, analyzing patient similarity leads to data explosion when exploiting big data. By increasing the data size the computational burden of this analysis increases. A real-life medical application may exceed the limits of current hardware in a fairly short amount of time. Finding ways to optimize patient similarity analysis and handling this data explosion is the topic of this paper.
                  Current implementations for patient similarity analysis require their users to have knowledge of complex data analysis tools. Moreover, data pre-processing and analysis are performed in synthetic conditions: the data are extracted from the EMR database and then the data preparation and analysis are processed in external tools. After all of this effort the users might not experience a superior performance of the patient similarity analysis. We propose methods to optimize the patient similarity analysis in order to make it scalable to big data. Our method was tested against two real datasets and a low execution time was accomplished. Our result hence benefits a comprehensive medical decision support system. Moreover, our implementation comprises a balance between performance and applicability: the majority of the workload is processed within a database management system to enable a direct implementation on an EMR database.",health
10.1016/j.isatra.2018.05.003,to_check,ISA Transactions,scopus,2018-08-01,sciencedirect,Heart rate monitoring and therapeutic devices: A wavelet transform based approach for the modeling and classification of congestive heart failure,https://api.elsevier.com/content/abstract/scopus_id/85047250258,"Heart rate monitoring and therapeutic devices include real-time sensing capabilities reflecting the state of the heart. Current circuitry can be interpreted as a cardiac electrical signal compression algorithm representing the time signal information into a single event description of the cardiac activity. It is observed that some detection techniques developed for ECG signal detection like artificial neural network, genetic algorithm, Hilbert transform, hidden Markov model are some sophisticated algorithms which provide suitable results but their implementation on a silicon chip is very complicated. Due to less complexity and high performance, wavelet transform based approaches are widely used. In this paper, after a thorough analysis of various wavelet transforms, it is found that Biorthogonal wavelet transform is best suited to detect ECG signal's QRS complex. The main steps involved in ECG detection process consist of de-noising and locating different ECG peaks using adaptive slope prediction thresholding. Furthermore, the significant challenges involved in the wireless transmission of ECG data are data conversion and power consumption. As medical regulatory boards demand a lossless compression technique, lossless compression technique with a high bit compression ratio is highly required. Furthermore, in this work, LZMA based ECG data compression technique is proposed. The proposed methodology achieves the highest signal to noise ratio, and lowest root mean square error. Also, the proposed ECG detection technique is capable of distinguishing accurately between healthy, myocardial infarction, congestive heart failure and coronary artery disease patients with a detection accuracy, sensitivity, specificity, and error of 99.92%, 99.94%, 99.92% and 0.0013, respectively. The use of LZMA data compression of ECG data achieves a high compression ratio of 18.84. The advantages and effectiveness of the proposed algorithm are verified by comparing with the existing methods.",health
10.1016/j.jconrel.2018.04.017,to_check,Journal of Controlled Release,scopus,2018-06-10,sciencedirect,Overcoming safety challenges in CO therapy – Extracorporeal CO delivery under precise feedback control of systemic carboxyhemoglobin levels,https://api.elsevier.com/content/abstract/scopus_id/85046347956,"Carbon monoxide (CO) has demonstrated therapeutic potential in multiple inflammatory conditions including intensive care applications such as organ transplantation or sepsis. Approaches to translate these findings into future therapies, however, have been challenged by multiple hurdles including handling and toxicity issues associated with systemic CO delivery. Here, we describe a membrane-controlled Extracorporeal Carbon Monoxide Release System (ECCORS) for easy implementation into Extracorporeal Membrane Oxygenation (ECMO) setups, which are being used to treat cardiac and respiratory diseases in various intensive care applications. Functionalities of the ECCORS were investigated in a pig model of veno-arterial ECMO. By precisely controlling CO generation and delivery as a function of systemic carboxyhemoglobin levels, the system allows for an immediate onset of therapeutic CO-levels while preventing CO-toxicity. Systemic carboxyhemoglobin levels were profiled in real-time by monitoring exhaled CO levels as well as by pulse oximetry, enabling self-contained and automatic feedback control of CO generation within ECCORS. Machine learning based mathematical modeling was performed to increase the predictive power of this approach, laying foundation for high precision systemic CO delivery concepts of tomorrow.",health
10.1016/j.bbe.2018.08.002,to_check,Biocybernetics and Biomedical Engineering,scopus,2018-01-01,sciencedirect,Fast statistical model-based classification of epileptic EEG signals,https://api.elsevier.com/content/abstract/scopus_id/85052519495,"This paper presents a supervised classification method to accurately detect epileptic brain activity in real-time from electroencephalography (EEG) data. The proposed method has three main strengths: it has low computational cost, making it suitable for real-time implementation in EEG devices; it performs detection separately for each brain rhythm or EEG spectral band, following the current medical practices; and it can be trained with small datasets, which is key in clinical problems where there is limited annotated data available. This is in sharp contrast with modern approaches based on machine learning techniques, which achieve very high sensitivity and specificity but require large training sets with expert annotations that may not be available. The proposed method proceeds by first separating EEG signals into their five brain rhythms by using a wavelet filter bank. Each brain rhythm signal is then mapped to a low-dimensional manifold by using a generalized Gaussian statistical model; this dimensionality reduction step is computationally straightforward and greatly improves supervised classification performance in problems with little training data available. Finally, this is followed by parallel linear classifications on the statistical manifold to detect if the signals exhibit healthy or abnormal brain activity in each spectral band. The good performance of the proposed method is demonstrated with an application to paediatric neurology using 39 EEG recordings from the Children's Hospital Boston database, where it achieves an average sensitivity of 98%, specificity of 88%, and detection latency of 4s, performing similarly to the best approaches from the literature.",health
10.1109/SYSCON.2018.8369547,to_check,2018 Annual IEEE International Systems Conference (SysCon),IEEE,2018-04-26 00:00:00,ieeexplore,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,https://ieeexplore.ieee.org/document/8369547/,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",industry
10.1109/ICRITO48877.2020.9198036,to_check,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",IEEE,2020-06-05 00:00:00,ieeexplore,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,https://ieeexplore.ieee.org/document/9198036/,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",industry
10.1109/GIOTS.2018.8534533,to_check,2018 Global Internet of Things Summit (GIoTS),IEEE,2018-06-07 00:00:00,ieeexplore,A Car as a Semantic Web Thing: Motivation and Demonstration,https://ieeexplore.ieee.org/document/8534533/,"Car signal data is usually hard to access, understand and integrate for non automotive domain experts. In this paper, we use semantic technologies for enriching signal data in the automotive industry and access it through Web of Things interactions. This combination allows the access and integration of car data from the web. We built VSSo, a Vehicle Signal ontology based on SOSA/SSN Observations and Actuations, and generated WoT Actions, Events and Properties, enriched with domain metadata. We mapped VSSo to a Web of Things ontology and we developed a Web of Things protocol binding with LwM2M, and made an implementation in a real car. This implementation resulted in a first working prototype, and a number of future improvements required in order to be compliant with automotive standards.",industry
10.1109/ISIC.2008.4635950,to_check,2008 IEEE International Symposium on Intelligent Control,IEEE,2008-09-05 00:00:00,ieeexplore,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 1: Prototype Design and Development,https://ieeexplore.ieee.org/document/4635950/,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], and initiated the preliminary design of a simple system prototype [6], we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance.",industry
10.1109/ISIC.2008.4635951,to_check,2008 IEEE International Symposium on Intelligent Control,IEEE,2008-09-05 00:00:00,ieeexplore,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 2: Prototype Design Verification,https://ieeexplore.ieee.org/document/4635951/,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system defined its autonomy, communications, and artificial intelligence (AI) requirements, and initiated the preliminary design of a simple system prototype, we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The second-part paper addresses the ICAM system prototype design verification and its logical behavior during sensor faults in the plant.",industry
10.1109/ISIC.2008.4635952,to_check,2008 IEEE International Symposium on Intelligent Control,IEEE,2008-09-05 00:00:00,ieeexplore,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 3: Performance Analysis and System Limitations,https://ieeexplore.ieee.org/document/4635952/,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work (J.H. Taylor and A.F. Sayda, 2005), (A.F. Sayda and J.H. Taylor, 2006), defined its autonomy, communications, and artificial intelligence (AI) requirements and initiated the preliminary design of a simple system prototype (J.H. Taylor and A.F. Sayda, 2008), we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The third-part paper addresses the ICAM system prototype validation in terms of system performance analysis and system behavior during unexpected situations.",industry
10.1109/SSST.1991.138548,to_check,[1991 Proceedings] The Twenty-Third Southeastern Symposium on System Theory,IEEE,1991-03-12 00:00:00,ieeexplore,A neural network based histogramic procedure for fast image segmentation,https://ieeexplore.ieee.org/document/138548/,"The determination of the dimension of a lumber board, the location and extent of surface defects on it, are essential in the construction of a visual inspection station for the lumber industry. The paper presents a neural network based histogramic procedure that performs on the image of a board and can be used to determine the board dimension, the location and extent of surface defects on it, in near real time. The method is based on segmentation of the image based on multiple threshold information derived from a multi-layered neural network. Such a scheme can be applied in general to image analysis and the implementation shows fast processing requiring very little control over the environment. The construction of the network and its training are also discussed.&lt;<ETX>&gt;</ETX>",industry
10.1109/IAS.2005.1518384,to_check,"Fourtieth IAS Annual Meeting. Conference Record of the 2005 Industry Applications Conference, 2005.",IEEE,2005-10-06 00:00:00,ieeexplore,A neural network based optimal wide area control scheme for a power system,https://ieeexplore.ieee.org/document/1518384/,"With deregulation of the power industry, many tie lines between control areas are driven to operate near their maximum capacity, especially those serving heavy load centers. Wide area control systems (WACSs) using wide-area or global signals can provide remote auxiliary control signals to local controllers such as automatic voltage regulators, power system stabilizers, etc to damp out inter-area oscillations. This paper presents the design and the DSP implementation of a nonlinear optimal wide area controller based on adaptive critic designs and neural networks for a power system on the real-time digital simulator (RTDS/spl reg/). The performance of the WACS as a power system stability agent is studied using the Kundur's two area power system example. The WACS provides better damping of power system oscillations under small and large disturbances even with the inclusion of local power system stabilizers.",industry
10.1109/CAIBDA53561.2021.00014,to_check,"2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)",IEEE,2021-05-30 00:00:00,ieeexplore,Applications of Smart Energy Integrated Service Platform in Optimization of Energy Utilization of Customers,https://ieeexplore.ieee.org/document/9545981/,"In the new era, comprehensive energy service has gradually become an important development direction to promote the high-quality development of energy industry and boost the development of real economy. The construction of smart integrated energy service platform is the internal demand for energy service enterprises to transform into integrated energy service providers, and is a solution to provide comprehensive energy support services for power users. The platform is divided into perception layer, acquisition layer, network layer, platform layer, application layer and display layer from bottom to top. Its main functions include energy regulation, energy efficiency analysis, smart operation, energy trading and smart dispatching. The construction of smart energy comprehensive service platform can provide customers with more high-quality, more convenient and more comprehensive energy value-added services, and constantly create more value for customers. Practice has proved that the implementation of smart energy integrated service platform can greatly reduce the monthly energy cost, monthly power load of the park and monthly coal consumption in park and enhance the indexes of monthly active users and customer satisfaction.",industry
10.1109/ITHET.2012.6246060,to_check,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),IEEE,2012-06-23 00:00:00,ieeexplore,Cybernetic Education Centre: Monitoring and control of learner's e-learning study in the field of Cybernetics and Automation by Coloured Petri Nets model,https://ieeexplore.ieee.org/document/6246060/,"This paper presents the Cybernetic Education Centre (CEDUC) as a hybrid e-learning and training centre for higher education of Cybernetics and Automation fields. If we consider Cybernetics we consider basically (1) controlled systems and (2) control systems. In case of controlled systems learner is focused on the process of analyze, identification, design of the mathematical model and simulation of the controlled system. Therefore this paper deals with controlled models in the laboratories of our department (a) real laboratory models, (b) simulation models and (c) virtual models which creates one integrated hybrid architecture what represents one of new ideas in the paper (Fig. 1). Learner of control systems is focused mainly on design of control parameters, design of control algorithms, design of hardware, software and communication architectures of control systems. Overall control system is represented by Distributed Control System (DCS) which serves for learners to verify designed control systems. The verification of the control systems is very important from safety point of view to prepare learners for real production conditions. Second new idea of the paper is implementation of the Coloured Petri Nets as automata to control access to the resources (not only typical study resources but also access to the components of hybrid DCS architecture) as well as to monitor the learner activities during the study. CEDUC is supported by intensive industry-university partnership. Conclusion of the paper summarizes the results of the study process of learners in DCS environment.",industry
10.1109/AIID51893.2021.9456537,to_check,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),IEEE,2021-05-30 00:00:00,ieeexplore,Design and Implementation of Regional Food Distribution Platform Based on Big Data,https://ieeexplore.ieee.org/document/9456537/,"In recent years, the rapid development of big data has made people's daily life very convenient, and at the same time, tasting all kinds of food has become an important activity for people to travel. Due to the vast territory of China, there are many types of cuisines with great differences, it is very important for travelers to understand the special regional cuisines in the area. This project aggregates regional food data based on big data, and provides tourists with efficient, stable and professional data retrieval and analysis services through a visual data interface, and provides intuitive, accurate, and real-time data support for the decision-making of finding characteristic regional food. This thesis first conducted a relevant understanding of big data and the overall situation of regional cuisine, analyzed the distribution of food in the sub-provincial city of Xi'an, explored the research methods and implementation methods of related projects at home and abroad, based on this, summarized the research of this project the goal. At the same time, the focus of this project is to analyze the price, score and popularity of regional food data analysis in my country, and to summarize, proofread and organize the data obtained before into standard and standardized data. Through the classification of basic information, the visual analysis and display of data is realized, and the key data urgently needed by decision makers are extracted from it. To analyze the needs of decision makers, establish corresponding strategies and measures to improve the quality of data services. The establishment of this system provides a useful supplement and improvement to the existing industry data analysis system. The system is mainly divided into six modules, which are data collection, data review, data summary, and visual data display modules. Among them, data collection includes crawling relevant data from the Internet and retrieving key data. The data audit function includes classifying the crawled data and reviewing its effectiveness. Data aggregation includes summarizing the filtered data in an excel table and passing Excel generates a visual chart that you want to know, and at the same time generates a word cloud by associating certain two related items in the data in the excel table. The visual data display module can more clearly show the results of data analysis to users, so that users can make better decisions. The visualization data uses AmChart Flash charts. The implementation of this system adopts Django Python Web framework, the development language chooses Python, the development tool adopted is PyCharm, and the database tool adopts MySQL.",industry
10.1109/EHB50910.2020.9280165,to_check,2020 International Conference on e-Health and Bioengineering (EHB),IEEE,2020-10-30 00:00:00,ieeexplore,Drivers’ Drowsiness Detection and Warning Systems for Critical Infrastructures,https://ieeexplore.ieee.org/document/9280165/,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events.",industry
10.1109/RTC.2012.6418168,to_check,2012 18th IEEE-NPSS Real Time Conference,IEEE,2012-06-15 00:00:00,ieeexplore,Implementation of the disruption predictor APODIS in JET real time network using the MARTe framework,https://ieeexplore.ieee.org/document/6418168/,"The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution.",industry
10.1109/FMEC49853.2020.9144776,to_check,2020 Fifth International Conference on Fog and Mobile Edge Computing (FMEC),IEEE,2020-04-23 00:00:00,ieeexplore,IoT-WLAN Proximity Network for Potentiostats,https://ieeexplore.ieee.org/document/9144776/,"The implementation of potentiostats as portable and communicated devices has reached significant progress to benefit research, industry, and education. The Internet of Things (IoT) is a good opportunity to interconnect devices such as the potentiostats together with electronics, communication technologies, and chemistry into a single system. This work proposes a network for potentiostats using machine-to-machine (M2M) protocols, modifying its functioning mechanism in the broker to check the payload of the message that passes through it and synchronize the sensors depending on its content. Although one sensor can be synchronized directly to another, the broker decides which sensor to pair. This modification was made in the M2M protocol algorithm, both in the Broker and in the Client (sensor). In addition to this, the network uses an interconnection architecture of IoT smart networks of proximity with centralized management. The results of the tests carried out showed that the use of a modified M2M such as the one proposed in the architecture allows synchronization and comparison of the measurements of several sensors in real-time.",industry
10.1109/SMC.2017.8122711,to_check,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",IEEE,2017-10-08 00:00:00,ieeexplore,Knowledge extracted from recurrent deep belief network for real time deterministic control,https://ieeexplore.ieee.org/document/8122711/,"Recently, the market on deep learning including not only software but also hardware is developing rapidly. Big data is collected through IoT devices and the industry world will analyze them to improve their manufacturing process. Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Although deep learning can show the high capability of classification, prediction, and so on, the implementation on GPU devices are required. We may meet the trade-off between the higher precision by deep learning and the higher cost with GPU devices. We can success the knowledge extraction from the trained deep learning with high classification capability. The knowledge that can realize faster inference of pre-trained deep network is extracted as IF-THEN rules from the network signal flow given input data. Some experiment results with benchmark tests for time series data sets showed the effectiveness of our proposed method related to the computational speed.",industry
10.1109/ICPP.2011.40,to_check,2011 International Conference on Parallel Processing,IEEE,2011-09-16 00:00:00,ieeexplore,Location-Aware MapReduce in Virtual Cloud,https://ieeexplore.ieee.org/document/6047196/,"MapReduce is an important programming model for processing and generating large data sets in parallel. It is commonly applied in applications such as web indexing, data mining, machine learning, etc. As an open-source implementation of MapReduce, Hadoop is now widely used in industry. Virtualization, which is easy to configure and economical to use, shows great potential for cloud computing. With the increasing core number in a CPU and involving of virtualization technique, one physical machine can hosts more and more virtual machines, but I/O devices normally do not increase so rapidly. As MapReduce system is often used to running I/O intensive applications, decreasing of data redundancy and load unbalance, which increase I/O interference in virtual cloud, come to be serious problems. This paper builds a model and defines metrics to analyze the data allocation problem in virtual environment theoretically. And we design a location-aware file block allocation strategy that retains compatibility with the native Hadoop. Our model simulation and experiment in real system shows our new strategy can achieve better data redundancy and load balance to reduce I/O interference. Execution time of applications such as RandomWriter, Text Sort and Word Count are reduced by up to 33% and 10% on average.",industry
10.1109/FCCM48280.2020.00067,to_check,2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),IEEE,2020-05-06 00:00:00,ieeexplore,Scalable Full Hardware Logic Architecture for Gradient Boosted Tree Training,https://ieeexplore.ieee.org/document/9114741/,"Gradient Boosted Tree is most effective and standard machine learning algorithm in many fields especially with various type of tabular dataset. Besides, recent industry field and robotics field require high-speed, power efficient and real-time training with enormous data. FPGA is effective device which enable custom domain specific approach to give acceleration as well as power efficiency. We introduce a scalable full hardware implementation of Gradient Boosted Tree training with high performance and flexibility of hyper parameterization. Experimental work shows that our hardware implementation achieved 11-33 times faster than state-of-art GPU acceleration even with small gates and low power FPGA device.",industry
10.1109/TCIAIG.2012.2212194,to_check,IEEE Transactions on Computational Intelligence and AI in Games,IEEE,2012-12-01 00:00:00,ieeexplore,AntBot: Ant Colonies for Video Games,https://ieeexplore.ieee.org/document/6262464/,"The video game industry is an emerging market which continues to expand. From its early beginning, developers have focused mainly on sound and graphical applications, paying less attention to developing game bots or other kinds of nonplayer characters (NPCs). However, recent advances in artificial intelligence offer the possibility of developing game bots which are dynamically adjustable to several difficulty levels as well as variable game environments. Previous works reveal a lack of swarm intelligence approaches to develop these kinds of agents. Considering the potential of particle swarm optimization due to its emerging properties and self-adaptation to dynamic environments, further investigation into this field must be undertaken. This research focuses on developing a generic framework based on swarm intelligence, and in particular on ant colony optimization, such as it allows general implementation of real-time bots that work over dynamic game environments. The framework has been adapted to allow the implementation of intelligent agents for the classical game Ms. Pac-Man. These were trialed at the Ms. Pac-Man competitions held during the 2011 International Congress on Evolutionary Computation.",industry
10.1109/ACCESS.2021.3082934,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Visual Product Tracking System Using Siamese Neural Networks,https://ieeexplore.ieee.org/document/9439511/,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",industry
10.1109/TENCON.2004.1414983,to_check,2004 IEEE Region 10 Conference TENCON 2004.,IEEE,2004-11-24 00:00:00,ieeexplore,Certain studies on sample time for a predictive fuzzy logic controller through real time implementation of phenol-formaldehyde manufacturing,https://ieeexplore.ieee.org/document/1414983/,"In polymer manufacturing industries, the automation and control of chemical process incorporating techniques of fuzzy control neural networks, and expert systems had lead to a more secured and stable operation. A sudden and unpredictable heat is often produced by the nonlinear exothermal reaction when phenol and formaldehyde are mixed together. Therefore, the polymerization process has to be controlled with a high level of precision in order to avoid temperature run-away. This paper proposes a design methodology for a sensor based process control system. The duration of ON and OFF time of certain relays are the parameters to be controlled in order to keep the exothermic reaction under control The universe of discourse for the output of the FLC system is the sample time that assigned to the relays where maximum time for heater or valve can be turned on before the next action is applied This paper discusses a detailed real time implementation of the exothermal process control using Matlab-fuzzy logic toolbox. An enhanced predictive FLC structure is developed and compared to a predictive FLC control structure. The obtained practical results thus ensure that the predictive FLC can be enhanced by modifying the rules and the membership Junctions of the universe of discourse, which is proved to be better in controlling the reaction temperature.",industry
10.1109/AQTR.2018.8402748,to_check,"2018 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",IEEE,2018-05-26 00:00:00,ieeexplore,Time series forecasting for dynamic scheduling of manufacturing processes,https://ieeexplore.ieee.org/document/8402748/,"Manufacturing control systems evolved in the recent decades from pre-programmed rigid systems to adaptable, data driven, cloud based implementations, capable to respond to environment changes and new requirements in real time. A byproduct of this transformation is represented by large amounts of structured and semi-structured information, both historical and real-time data that is made available on various layers of the system. This accumulation of information brings the opportunity to move from the rule based decision making algorithms used traditionally by these control systems towards more intelligent approaches, driven by modern deep learning mechanisms. This paper proposes a time series forecasting model using recursive neural networks (RNN) for operation scheduling and sequencing in a virtual shop floor environment. The time series aspect of the RNN is novel in manufacturing domain, in the sense that the new best prediction produced considers the previous decisions and outcomes. The proposed implementation explains how the RNN can be mapped to the specifics of a manufacturing control system and introduces a bidding mechanism to allow dynamic evaluation of individual forecasts. The pilot implementation, initial experiments on sample data sets and results presented show how using recursive neural networks can optimize resource utilization and energy consumption.",industry
10.1109/JIOT.2019.2940131,to_check,IEEE Internet of Things Journal,IEEE,2019-12-01 00:00:00,ieeexplore,A Two-Stage Transfer Learning-Based Deep Learning Approach for Production Progress Prediction in IoT-Enabled Manufacturing,https://ieeexplore.ieee.org/document/8827506/,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",industry
10.1109/TASE.2020.3044107,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2021-04-01 00:00:00,ieeexplore,An Online Policy for Energy-Efficient State Control of Manufacturing Equipment,https://ieeexplore.ieee.org/document/9308932/,"Machine state control is one of the most promising energy-efficient measures for machining processes. A proper control reduces the energy consumed during idle periods by switching off/on the machines. A critical barrier for practical implementation is related to the knowledge of part arrival process that is affected by uncertainty. The stochastic processes involved in the system are usually assumed to be known. However, real production environments are subject to several sources of randomness that are difficult to model a priori. This work provides an online time-based algorithm that is able to control the machine state. Through a method for the estimation of the stochastic process, the algorithm provides the optimal control parameters based on a collected set of observations. A new policy is formulated to manage the control over time such that changes in the control parameters are applied only under certain conditions. Potential benefits are discussed using realistic numerical cases. Note to Practitioners-This article analyzes the control problem of switching off/on a machine tool for energy saving during machine idle periods. A control policy based on time information is investigated when the machine requires a startup time to resume the service after being switched off. The proposed policy works online while acquiring information from the real system. An algorithm is described for identifying and applying the optimal control parameters. The results of this research will be useful for a practical implementation of a switching policy for energy saving. This implementation requires the estimation of the power adsorbed by the machine in four different states and, therefore, it reduces the implementation effort for practitioners.",industry
10.1109/RO-MAN46459.2019.8956327,to_check,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),IEEE,2019-10-18 00:00:00,ieeexplore,End-User Programming of Low-and High-Level Actions for Robotic Task Planning,https://ieeexplore.ieee.org/document/8956327/,"Programming robots for general purpose applications is extremely challenging due to the great diversity of end-user tasks ranging from manufacturing environments to personal homes. Recent work has focused on enabling end-users to program robots using Programming by Demonstration. However, teaching robots new actions from scratch that can be reused for unseen tasks remains a difficult challenge and is generally left up to robotic experts. We propose iRoPro, an interactive Robot Programming framework that allows end-users to teach robots new actions from scratch and reuse them with a task planner. In this work we provide a system implementation on a two-armed Baxter robot that (i) allows simultaneous teaching of low-and high-level actions by demonstration, (ii) includes a user interface for action creation with condition inference and modification, and (iii) allows creating and solving previously unseen problems using a task planner for the robot to execute in real-time. We evaluate the generalisation power of the system on six benchmark tasks and show how taught actions can be easily reused for complex tasks. We further demonstrate its usability with a user study (N=21), where users completed eight tasks to teach the robot new actions that are reused with a task planner. The study demonstrates that users with any programming level and educational background can easily learn and use the system.",industry
10.1109/ICMNN.1994.593731,to_check,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,IEEE,1994-09-28 00:00:00,ieeexplore,Massively parallel VLSI-implementation of a dedicated neural network for anomaly detection in automated visual quality control,https://ieeexplore.ieee.org/document/593731/,"In this work we will present the VLSI-implementation of a dedicated neural network architecture which we have developed in prior work for anomaly detection in automated visual industrial quality control. The network, denoted as NOVAS performs a filtering of inspection images and highlights defects or anomalies in an isomorphic image representation, allowing the detection and localisation of faults on objects. Training of NOVAS is achieved by simply presenting a set of tolerable objects to the network in a single sweep. NOVAS works with single and with multichannel image representations. The processing principle of NOVAS is closely related to nearest neighbor and hypersphere classifier approaches. We have designed an ASIC for the efficient implementation of the nearest neighbor search. Based on that ASIC we will present an architecture of a modular massively parallel computer suited to meet the real-time constraints of manufacturing processes. Further we will report on the status of a prototype system which is close to completion.",industry
10.1109/DTPI52967.2021.9540189,to_check,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),IEEE,2021-08-15 00:00:00,ieeexplore,A parallel intelligent control system and its industrial application,https://ieeexplore.ieee.org/document/9540189/,"Parallel control refers to the parallel interaction between the actual physical process and the manual calculation process. The ACP method under its theoretical framework includes artificial system, calculation experiment and parallel control. This paper presents a parallel intelligent control system implementation method, parallel control system needs a carrier, including hardware platform and software system, based on this carrier, the system completes artificial intelligence modeling and real-time optimal control. Firstly, the structure of parallel control platform is introduced, which is composed of industrial control computer, server and power supply, The program function of server is the core part of parallel control system;Secondly. The architecture of parallel intelligent control system is given, The artificial system is designed as an object modeling system, Industrial control computer output excitation signal, The server collects the response data and completes the modeling;The calculation experiment is designed as a process of human-computer interaction, which helps to realize control quality judgment and parameter setting;The parallel control is realized by the industrial controller, and the optimal parameters or control algorithm are put into the controller in parallel to realize the real-time control. Finally, an industrial application example is given to prove the effectiveness of this method.",industry
10.1109/ICMA.2019.8816298,to_check,2019 IEEE International Conference on Mechatronics and Automation (ICMA),IEEE,2019-08-07 00:00:00,ieeexplore,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,https://ieeexplore.ieee.org/document/8816298/,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",industry
10.1109/IECON.2016.7793206,to_check,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,IEEE,2016-10-26 00:00:00,ieeexplore,Summer school on intelligent agents in automation: Hands-on educational experience on deploying industrial agents,https://ieeexplore.ieee.org/document/7793206/,"Cyber-physical systems constitutes a framework to develop intelligent, distributed, resilient, collaborative and cooperative systems, promoting the fusion of computational entities and physical devices. Agent technology plays a crucial role to develop this kind of systems by offering a decentralized, distributed, modular, robust and reconfigurable control structure. This paper describes the implementation of a summer school aiming to enhance the participants' knowledge in the field of multi-agent systems applied to industrial environments, being able to gain the necessary theoretical and practical skills to develop real industrial agent based applications. This is accomplished in an international framework where individual knowledge and experiences are shared in a complementary level.",industry
10.1109/IPDPSW.2017.44,to_check,2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),IEEE,2017-06-02 00:00:00,ieeexplore,A pipelined and scalable dataflow implementation of convolutional neural networks on FPGA,https://ieeexplore.ieee.org/document/7965030/,"Convolutional Neural Network (CNN) is a deep learning algorithm extended from Artificial Neural Network (ANN) and widely used for image classification and recognition, thanks to its invariance to distortions. The recent rapid growth of applications based on deep learning algorithms, especially in the context of Big Data analytics, has dramatically improved both industrial and academic research and exploration of optimized implementations of CNNs on accelerators such as GPUs, FPGAs and ASICs, as general purpose processors can hardly meet the ever increasing performance and energy-efficiency requirements. FPGAs in particular are one of the most attractive alternative, as they allow the exploitation of the implicit parallelism of the algorithm and the acceleration of the different layers of a CNN with custom optimizations, while retaining extreme flexibility thanks to their reconfigurability. In this work, we propose a methodology to implement CNNs on FPGAs in a modular, scalable way. This is done by exploiting the dataflow pattern of convolutions, using an approach derived from previous work on the acceleration of Iterative Stencil Loops (ISLs), a computational pattern that shares some characteristics with convolutions. Furthermore, this approach allows the implementation of a high-level pipeline between the different network layers, resulting in an increase of the overall performance when the CNN is employed to process batches of multiple images, as it would happen in real-life scenarios.",industry
10.1109/MetroInd4.0IoT51437.2021.9488447,to_check,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),IEEE,2021-06-09 00:00:00,ieeexplore,IOT data-driven experimental process optimisation for kevlar fiberglass components for aeronautic,https://ieeexplore.ieee.org/document/9488447/,"This paper describes the work carried out during the PROOF experiment (IOT data-driven experimental PROcess Optimization for kevlar Fiberglass components for aeronautic), winner of the second open call of the MIDIH EU project. The main objectives of the experiment are the integration of smart sensing devices with the Energy@Work IoT gateways and the development of cloudified innovative data-driven methodologies and data analytics tools to support process optimization in the production of hybrid composite material parts for the aeronautical sector. Collection of real-time production-data from multiple sensors with several industrial protocols and data transfer to the MIDIH project platform has been performed adopting the IoT gateway developed by Energy@Work, following MIDIH reference architecture for advanced data processing and visualization (e.g., Fiware Orion Context Broker, Apache Flink and Fiware Knowage) by using MQTT protocol. Then, historical and new acquired data has been analysed using advanced clustering techniques and trends, with the purpose to allow a novel CPS-based predictive system on the production process. Machine-Learning algorithms and visualisations (GUI based on Fiware Knowage) in real operating conditions have been used to validate the performance and assess the outcome. Finally, thanks to the implementation of specific optimization rules, able to process data gathered from the sensor network, a framework for distributed processing engine has been exploited by (i) generating tips for energy efficiency and process optimization and (ii) providing different type of alarms based on expected consumptions, resulting in concrete support to production managers for the improvement of the whole production value chain.",industry
10.1109/MoRSE48060.2019.8998694,to_check,"2019 International Conference on Mechatronics, Robotics and Systems Engineering (MoRSE)",IEEE,2019-12-06 00:00:00,ieeexplore,Integration of Blockchains with Management Information Systems,https://ieeexplore.ieee.org/document/8998694/,"In the era of the fourth industrial revolution (In-dustry 4.0), many Management Information Systems (MIS) integrate real-time data collection and use technologies such as big data, machine learning, and cloud computing, to foster a wide range of creative innovations, business improvements, and new business models and processes. However, the integration of blockchain with MIS offers the blockchain trilemma of security, decentralisation and scalability. MIS are usually Web 2.0 client-server applications that include the front end web systems and back end databases; while blockchain systems are Web 3.0 decentralised applications. MIS are usually private systems that a single party controls and manages; while blockchain systems are usually public, and any party can join and participate. This paper clarifies the key concepts and illustrates with figures, the implementation of public, private and consortium blockchains on the Ethereum platform. Ultimately, the paper presents a framework for building a private blockchain system on the public Ethereum blockchain. Then, integrating the Web 2.0 client-server applications that are commonly used in MIS with Web 3.0 decentralised blockchain applications.",industry
10.1109/MSN48538.2019.00085,to_check,2019 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN),IEEE,2019-12-13 00:00:00,ieeexplore,Intelligent Log Analysis System for Massive and Multi-Source Security Logs: MMSLAS Design and Implementation Plan,https://ieeexplore.ieee.org/document/9066044/,"In the Internet of Things and industrial controlnetwork servers, a large number of logs will be formed everymoment. This log information, as an important basis for eventrecording and security auditing, provides important informa-tion for identifying threat sources, identifying threat degreeand judging threat impact. However, the current security loganalysis system usually only standardizes the logs separately, and lacks the correlation analysis of the information fromvarious sources. Thus, this paper presents an intelligent loganalysis system for massive and multi-source security logs-MMSLAS(Massive and Multi-Source Security Log AnalysisSystem). In the log analysis module, the system integratesbusiness rule analysis and behavior analysis and additionallyadopts a machine learning-based analysis method, which fullyexploits the correlation between security logs and realizes thecomprehensive analysis of multi-source security logs. At thesame time, the distributed architecture scheme is also sufficientto cope with the system load caused by a large amount ofdata. The final implementation results show that MMSLAScan quickly locate the improper behavior in the log, and detectthe abnormal requests in advance according to the analysis ofthe behavior trajectory.",industry
10.1109/ISSCS52333.2021.9497411,to_check,"2021 International Symposium on Signals, Circuits and Systems (ISSCS)",IEEE,2021-07-16 00:00:00,ieeexplore,Inverted Pendulum Control with a Robotic Arm using Deep Reinforcement Learning,https://ieeexplore.ieee.org/document/9497411/,"Inverted pendulum control is a benchmark control problem that researchers have used to test the new control strategies over the past 50 years. Deep Reinforcement Learning Algorithm is used recently on the inverted pendulum on a straightforward form. The inverted pendulum had only one degree of freedom and was moving on a plane. This paper demonstrates a successful implementation of a deep reinforcement learning algorithm on an inverted pendulum that rotates freely on a spherical joint with an industrial 6 degrees freedom robot arm. This research used the Deep Reinforcement Learning algorithm in Robot Operating System (ROS) and Gazebo Simulation. Experimental results show that the proposed method achieved promising outputs and reaches the control objectives. We were able to control the inverted pendulum upward for 30 and 20 seconds in two case studies. Two other significant novelties in this research are using an inertial measurement unit (IMU) on the tip of the pendulum, that will facilitate implementation on the real robot for future work and different reward functions in comparing to past publications that enable continuous learning and mastering control in a vertical position",industry
10.1109/IECON.2006.347441,to_check,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,IEEE,2006-11-10 00:00:00,ieeexplore,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,https://ieeexplore.ieee.org/document/4152937/,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",industry
10.1109/ICIEA.2006.257304,to_check,2006 1ST IEEE Conference on Industrial Electronics and Applications,IEEE,2006-05-26 00:00:00,ieeexplore,Performance Studies of Fuzzy Logic Based PI-like Controller Designed for Speed Control of Switched Reluctance Motor,https://ieeexplore.ieee.org/document/4025905/,"Switched reluctance motor (SRM) has gained significant interest in the field of industrial drive. The controller used to drive the machine is conventional PI controller. But the machine characteristics are very much nonlinear. This poses a problem for conventional controller design as regards to maintaining steady performance. There is also a need to adapt to the variable operating conditions. Fuzzy logic based heuristics is prospective since the exact analytical modelling of the system is difficult. PC implementation of the controller offers great flexibility in both design and maintenance phase. This work implements a PI like fuzzy logic controller (FLC) for SRM, which is found to work successfully in real time conditions. The work compares the performance of the FLC with respect to the conventional PI controller",industry
10.1109/ACCESS.2021.3055257,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Adaptive Vision-Based Method for Rotor Dynamic Balance System,https://ieeexplore.ieee.org/document/9337863/,"This article presents a new adaptive vision-based method (AVBM) of performing automatic detection for rotor balancing, and the online implementation proved that the method achieved rapid real-time optimization of system balancing configuration for a rotor dynamic balance machine. The proposed AVBM integrated 3D sensors and dynamic balancing platform using 3D computer vision technique and dynamic balance algorithm to improve the efficiency of rotor dynamic balancing. AVBM applied 3D ToF sensors on active rotor dynamic balance machines to grab 3D point cloud of rotor shaft and balance sprues. By 3D depth data, the background noise can be removed to detect the positions of shaft center, key phasor and balance sprues of rotor automatically. After combining with unbalance vector from dynamic balancing machine, the AVBM system calculated the optimal balance configuration by the vector analysis algorithm. Compares to conventional methods, conventional rotor dynamic balancing process relies on technicians to mount washers on particular balance sprues based on their experience, therefore uncertainty causes productivity decline. Experiments in industrial examples showed that the proposed AVBM required fewer rounds to achieve acceptance, whereas the conventional industrial rotor balancing method performed by operators required more than three rounds in average. Consider the overall dynamic balancing process for the motor, the processing time required for each motor without AVBM was 348.9 seconds, and the daily rotor balancing count of each dynamic balancing machine was 83. The processing time with AVBM was shortened from 348.9 to 283.9 seconds, the daily rotor balancing count had increased from 83 to 101, and the production improvement had reached 22 %. That is, the proposed AVBM can accurately analyze the dynamic balance and greatly reduce the redundant dynamic balance operations. The main advantage of the proposed AVBM over conventional methods is its efficiency, effectiveness and robustness in online optimization of rotor dynamic balance.",industry
10.1109/TIM.2019.2915404,to_check,IEEE Transactions on Instrumentation and Measurement,IEEE,2020-04-01 00:00:00,ieeexplore,An End-to-End Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,https://ieeexplore.ieee.org/document/8709818/,"A complete defect detection task aims to achieve the specific class and precise location of each defect in an image, which makes it still challenging for applying this task in practice. The defect detection is a composite task of classification and location, leading to related methods is often hard to take into account the accuracy of both. The implementation of defect detection depends on a special detection data set that contains expensive manual annotations. In this paper, we proposed a novel defect detection system based on deep learning and focused on a practical industrial application: steel plate defect inspection. In order to achieve strong classification ability, this system employs a baseline convolution neural network (CNN) to generate feature maps at each stage, and then the proposed multilevel feature fusion network (MFN) combines multiple hierarchical features into one feature, which can include more location details of defects. Based on these multilevel features, a region proposal network (RPN) is adopted to generate regions of interest (ROIs). For each ROI, a detector, consisting of a classifier and a bounding box regressor, produces the final detection results. Finally, we set up a defect detection data set NEU-DET for training and evaluating our method. On the NEU-DET, our method achieves 74.8/82.3 mAP with baseline networks ResNet34/50 by using 300 proposals. In addition, by using only 50 proposals, our method can detect at 20 ft/s on a single GPU and reach 92% of the above performance, hence the potential for real-time detection.",industry
10.1109/TVT.2021.3084829,to_check,IEEE Transactions on Vehicular Technology,IEEE,2021-06-01 00:00:00,ieeexplore,Guest EditorialIntroduction to the Special Section on Vehicular Networks in the Era of 6G: End-Edge-Cloud Orchestrated Intelligence,https://ieeexplore.ieee.org/document/9477551/,"The articles in this special section focus on vehicular networks in the era of 6G mobile communication. With the growth of the vehicle population, vehicular networks play a key role in building safe, efficient, and intelligent transport systems and has been attracting a lot of attention from both academic and industrial communities around the world. The rise of autonomous driving technology and the prosperity of mobile applications, e.g., real-time video analytic, image-aided navigation, natural language processing, and etc, have brought tremendous pressure on current vehicular networks, e.g., high bandwidth, ultra-low latency, high reliability, high security, powerful computation capability, and massive connections. It is necessary to continue to develop vehicular networks by combining the latest research intends in other fields to meet quickly rising communication and computation demands. The upcoming 6G technology, which provides Holographic and Artificial Intelligence (AI) enabled communications, together with the increasing implementation of artificial intelligence in mobile devices, will lead to a new research trend to end-edge-cloud orchestrated computing with intelligence. It means that, not only the intelligent communication protocols, but also the intelligent computing resource management and machine learning algorithms among the mobile vehicles, the edge and the cloud, should be redesigned to support the development of vehicular networks.",industry
10.1109/81.747195,to_check,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,IEEE,1999-02-01 00:00:00,ieeexplore,Reaction-diffusion CNN algorithms to generate and control artificial locomotion,https://ieeexplore.ieee.org/document/747195/,"In this paper a physiological-behavioral approach to neural processing is used to realize artificial locomotion in mechatronic devices. The task has been realized by using a particular model of reaction-diffusion cellular neural networks (RD-CNN's) generating autowave fronts as well as Turing patterns. Moreover a programmable hardware cellular neural network structure is presented in order to model, generate, and control in real time some biorobots. The programmable hardware implementation gives the possibility of generating locomotion in real time and also to control the transition among several types of locomotion, with particular attention to hexapodes. The approach proposed allows not only the design of walking robots, but also the ability to build structures able to efficiently solve typical problems in industrial automation, such as online routing of objects moved on conveyor belts.",industry
http://arxiv.org/abs/1901.10281v1,to_check,arxiv,arxiv,2019-01-29 13:43:57+00:00,arxiv,Structural Material Property Tailoring Using Deep Neural Networks,http://arxiv.org/abs/1901.10281v1,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy.",industry
http://arxiv.org/abs/1908.11863v1,to_check,arxiv,arxiv,2019-08-30 17:48:05+00:00,arxiv,Systematic Analysis of Image Generation using GANs,http://arxiv.org/abs/1908.11863v1,"Generative Adversarial Networks have been crucial in the developments made in
unsupervised learning in recent times. Exemplars of image synthesis from text
or other images, these networks have shown remarkable improvements over
conventional methods in terms of performance. Trained on the adversarial
training philosophy, these networks aim to estimate the potential distribution
from the real data and then use this as input to generate the synthetic data.
Based on this fundamental principle, several frameworks can be generated that
are paragon implementations in several real-life applications such as art
synthesis, generation of high resolution outputs and synthesis of images from
human drawn sketches, to name a few. While theoretically GANs present better
results and prove to be an improvement over conventional methods in many
factors, the implementation of these frameworks for dedicated applications
remains a challenge. This study explores and presents a taxonomy of these
frameworks and their use in various image to image synthesis and text to image
synthesis applications. The basic GANs, as well as a variety of different niche
frameworks, are critically analyzed. The advantages of GANs for image
generation over conventional methods as well their disadvantages amongst other
frameworks are presented. The future applications of GANs in industries such as
healthcare, art and entertainment are also discussed.",industry
http://arxiv.org/abs/1609.08018v1,to_check,arxiv,arxiv,2016-09-26 15:15:09+00:00,arxiv,"Small near-Earth asteroids in the Palomar Transient Factory survey: A
  real-time streak-detection system",http://arxiv.org/abs/1609.08018v1,"Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",industry
10.1016/j.ssci.2021.105529,to_check,Safety Science,scopus,2022-02-01,sciencedirect,A novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry
10.1016/j.probengmech.2021.103173,to_check,Probabilistic Engineering Mechanics,scopus,2021-10-01,sciencedirect,Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system,https://api.elsevier.com/content/abstract/scopus_id/85117922944,"The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.",industry
10.1016/j.energy.2021.120700,to_check,Energy,scopus,2021-09-01,sciencedirect,Nonlinear generalized predictive controller based on ensemble of NARX models for industrial gas turbine engine,https://api.elsevier.com/content/abstract/scopus_id/85105736036,"New design and operation of modern gas turbine engines (GTEs) are becoming more and more complex where several limitations and control modes should be fulfilled at the same time to accomplish a safe and ideal performance for the engine. For this purpose, a constrained multi-input multi-output (MIMO) non-linear model predictive controller (NMPC) based on neural network model is designed to fulfill the control requirements of a Siemens SGT-A65 three-spool aero-derivative gas turbine engine (ADGTE) used for power generation. However, the implementation of NMPC in real time has two challenges: Firstly, the design of an accurate non-linear model, which can run many times faster than real time. Secondly, the usage of a rapid and reliable optimization algorithm to solve the optimization problem in real time. To solve these issues, the constrained MIMO NMPC is created based on the generalized predictive control (GPC) algorithm as a result of its clarity, ease of use, and capacity to deal with problems in one algorithm. In addition, seven ensembles of eight multi-input single-output (MISO) non-linear autoregressive network with exogenous inputs (NARX) models are used as a base model for the GPC controller to predict the future process outputs. Estimation of free and forced responses of the GPC based on the neural network (NN) model of the plant each sampling time without performing instantaneous linearization is proposed in this study, which reduces the NMPC optimization problem to a linear optimization problem at each sampling step. In addition, the Hildreth's quadratic programming algorithm is used to solve the quadratic optimization problem within the NMPC controller, which offers ease of use and reliability in real time applications. To demonstrate the performance of the NNGPC controller developed in this study, we have compared the performance of the neural network generalized predictive control (NNGPC) controller to the existing controller of the SGT-A65 engine. The simulation results show that the NNGPC has demonstrated output responses with less oscillatory behavior and smoother control actions to the sudden variation in the electric load disturbance than those observed in the existing min-max controller. However, the min-max controller has faster response than that of the NNGPC controller.",industry
10.1016/j.chemolab.2021.104314,to_check,Chemometrics and Intelligent Laboratory Systems,scopus,2021-06-15,sciencedirect,A scalable approach for the efficient segmentation of hyperspectral images,https://api.elsevier.com/content/abstract/scopus_id/85105360467,"The number of applications of hyperspectral imaging (HSI) is steadily increasing, as technology evolves and cameras become more affordable. However, the volume of data in a hyperspectral image is large (order of Gigabytes) and standard off-the-shelf algorithms for multi-channel image analysis cannot be readily applied, due to the prohibitive computational time and large memory requirements. Therefore, new scalable approaches are required to perform hyperspectral image analysis. In this article we address an efficient methodology for conducting Unsupervised Image Segmentation – one of the basic and most fundamental image analysis operations. In the methodology proposed, unsupervised segmentation is conducted after transforming the spectral and spatial dimensions of the raw hyperspectral image into a more compact representation using multivariate and multiresolution techniques. The clusters identified in the compact image representation are then used to train a discriminative classifier. The classifier is then adapted and transferred for application to the raw image, where it will efficiently label all the original pixels. With the proposed methodology, the computational expensive operations (unsupervised clustering and classifier learning) are minimized, whereas the efficient implementation of the classifier guarantees the analysis at the native resolution. The effectiveness of the proposed methodology was tested on a real case study considering an industrial hyperspectral image capturing the reflectance spectrum for several objects made of different unknown materials. A significant reduction in the computational cost was achieved without compromising the quality of the unsupervised segmentation, demonstrating the potential of the proposed approach.",industry
10.1016/j.fbp.2020.12.009,to_check,Food and Bioproducts Processing,scopus,2021-03-01,sciencedirect,Study of Galactooligosaccharides production from dairy waste by FTIR and chemometrics as Process Analytical Technology,https://api.elsevier.com/content/abstract/scopus_id/85099356128,"Galactooligosaccharides (GOS) production from whey, a relevant by-product of dairy industry, answers to the Circular Economy principle of extending the life cycle of products. Indeed, it allows the reuse of dairy waste to produce prebiotics to be used in functional food preparations. For this purpose, the effective monitoring of GOS production should be performed in real time and by environmentally friendly techniques. Thus, FTIR spectroscopy, combined with different chemometric approaches, has been tested to assess a Process Analytical Technology to follow GOS production from cheese whey. Partial Least Square regression models were reliable for lactose, glucose and galactose determination (Root Mean Square Error of Prediction of 21.9, 11.1 and 12.4 mg mL−1, respectively). Furthermore, Multivariate Curve Resolution – Alternating Least Square models were proposed to describe trends of the reaction components along the process being an interesting alternative to chromatographic determinations. The real time implementation of the proposed approach will provide the dairy industry with a reliable and green Process Analytical Technology for dairy waste reallocation, avoiding sample pre-processing, large use of organic solvents and long times of analysis.",industry
10.1016/j.compind.2020.103329,to_check,Computers in Industry,scopus,2020-12-01,sciencedirect,A Middleware Platform for Intelligent Automation: An Industrial Prototype Implementation,https://api.elsevier.com/content/abstract/scopus_id/85092922057,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms.",industry
10.1016/j.marpol.2020.103829,to_check,Marine Policy,scopus,2020-05-01,sciencedirect,Analyzing gaps in policy: Evaluation of the effectiveness of minimum landing size (MLS) regulations in Turkey,https://api.elsevier.com/content/abstract/scopus_id/85079518573,"The Mediterranean and Black Sea host the most intense overfishing and Turkey has the largest commercial fisheries in them (when both seas considered). However, the state of the Turkish fisheries is in critical condition as both the quality (i.e, in number of caught species, value and sizes of fish) and quantity of fisheries catches have been rapidly declining in recent decades. One pioneer fisheries management initiative thoroughly evaluated here pertains to minimum landing size (MLS) regulations for commercial taxa, with the aim of promoting stock sustainability by ensuring fish reproduce before they are caught. This study examines 29 taxa in relation to MLS by analyzing changes in catch per unit effort trends pre-and post MLS to gauge regulation effectiveness, changes to MLS regulations since implementation, and finally evaluates the Turkish MLS sizes in relation to Turkish maturity sizes, to provide advice for taxa requiring changes. It seems intensive fishing may have reduced the size at maturity for many species in Turkey, as they mature smaller here than the Mediterranean and global averages. Eleven taxa listed in MLS regulations are under the lengths of first maturity (Lmat) sizes in Turkish waters and need to be increased, especially that of bonito, hake, swordfish and bluefish (by 18 cm, 10 cm, 10 cm and 8 cm, respectively), while 16 taxa still require national studies to determine their Lmat sizes in Turkish waters. In conclusion, in Turkey, MLS regulations are completely ineffective due to a lack of monitoring and control for juvenile fish at landing sites, markets and processing plants, along with insufficient penalties for such infractions, yet, there remains plenty of room for improvement. To improve the state of the fisheries, MLS measures could be improved by increasing fines, monitoring and control, making some gear types more selective and use of real-time closures and no fishing zones to protect spawning and nursery habitats.",industry
10.1016/j.promfg.2020.05.123,to_check,,scopus,2020-01-01,sciencedirect,Integrated tool condition monitoring systems and their applications: A comprehensive review,https://api.elsevier.com/content/abstract/scopus_id/85095576577,"In conventional metal cutting, different tool wear modes, and their individual deterioration rates play vital roles in overall production performance. For a given tool (i.e., geometry or materials), many shop floors still follow a standard rule by pre-setting a tool life, which is conservative but not realistic. Premature failure of a tool can cause unexpected machine downtime and material losses, while another tool could serve beyond that pre-set life. As a result, optimized tool life and productivity cannot be achieved. Moreover, nowadays, there is an increased demand of process monitoring and optimization on the unmanned and the semi-automated shop floors.
                  Tool condition monitoring (TCM) systems for process improvement and optimization have been in research for several decades. Both offline and online TCM systems are invented and discussed. A wide range of original publications are reported focusing on different sub-topics, e.g., specific machining process-based TCM methods, measurement or signal acquisition methods, processing methods, and classifiers. With the recent evolution of smart sensors in the era of Industry 4.0, development of online TCM systems received much attention to the researchers. Accordingly, research on some sub-topics also gets motivated into different directions, such as, feasibility of power or current sensors, machine vision technique, and combination of multi-sensors. Thus, from the industrial viewpoint, the current state of implementation of the proposed TCM systems for (near) real-time process monitoring and control needs to be clear. This paper presents the state-of-the-art of the TCM systems covering three major machining operations, discusses their application feasibility in industry environments, and states some current TCMS implementations. Challenges being faced by the industry are concluded, along with direction and suggestions for future researches.",industry
10.1016/j.promfg.2020.04.037,to_check,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Implementing AR/MR - Learning factories as protected learning space to rise the acceptance for mixed and augmented reality devices in production,https://api.elsevier.com/content/abstract/scopus_id/85085498037,"When talking about digitization, changes in the way of working are inevitable: The implementation of intelligent machines or dealing with real-time data lead to new tasks supported by new technology. Also digital technologies such as Augmented and Mixed Reality (AR/MR) are pushing the market and setting new standards in collaboration, prototyping or maintenance. The correct handling of AR/MR devices requires a change in the employees’ behavior; changing working routines are followed by a new skill set and a change in the culture. The acceptance of employees can therefore be regarded as a critical success factor for the implementation of such technologies. Thus, the present paper answers the research question ‘what factors influence the employee’s acceptance of AR and MR data glasses in industry’. On the basis of a comprehensive literature analysis, an implementation workshop was developed and validated in cooperation with an industrial partner. The results were transformed into a workshop within the learning and research factory ‘Smart Production Lab’ to give employees and students the opportunity to train the handling of data glasses in a protected learning space in order to increase the acceptance for the technology.",industry
10.1016/j.vehcom.2019.100198,to_check,Vehicular Communications,scopus,2020-01-01,sciencedirect,In-vehicle network intrusion detection using deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85073150001,"The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.",industry
10.1016/j.ibiod.2019.104744,to_check,International Biodeterioration and Biodegradation,scopus,2019-10-01,sciencedirect,Comparative evaluation of Pseudomonas species in single chamber microbial fuel cell with manganese coated cathode for reactive azo dye removal,https://api.elsevier.com/content/abstract/scopus_id/85069842075,"Microbial fuel cell (MFCs), distinguished by different strains of Pseudomonas species; Pseudomonas aeruginosa (MPEM-MFC I) and Pseudomonas fluorescens (MPEM-MFC II), was analyzed. Results have shown that, over a period of 360 h in the presence of 0.5 mM of model dye, MPEM MFC I produced the maximum power density of 2887 ± 13 μW m−2 (RO-16) and 1906 ± 7 μW m−2 (RB-5) compared with MPEM-MFC II with 1896 ± 15 μW m−2 (RO-16) and 1028 ± 9 μW m−2 (RB-5). Decolorization efficiency of MPEM-MFC I was 98 ± 1.2% (RO-16) and 95 ± 2% (RB-5). Total phenazine production in MPEM-MFC I was 12.3 ± 0.5 μg mL−1 higher than that of 8.9 ± 0.05 μg mL−1 (MPEM-MFC II) and its production have positive influence of electron shuttling that brought out high power output. Addition of phenazine externally reduced the dye degradation. Bioadhesion capability of P. aeruginosa on the anode reduced the internal resistance in MFCs. Thus the implementation of MFC is a most promising technology for the complete decolorization of reactive azo dyes and it has potential economic benefits in real-life industrial application.",industry
10.1016/j.ifacol.2019.11.172,to_check,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Machine learning framework for predictive maintenance in milling,https://api.elsevier.com/content/abstract/scopus_id/85078904429,"In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process.",industry
10.1016/j.scitotenv.2019.02.213,to_check,Science of the Total Environment,scopus,2019-05-20,sciencedirect,Passive sampling of volatile organic compounds in industrial atmospheres: Uptake rate determinations and application,https://api.elsevier.com/content/abstract/scopus_id/85061829807,"This study describes the implementation of a passive sampling-based method followed by thermal desorption gas-chromatography-mass spectrometry (TD-GC–MS) for the monitoring of volatile organic compounds (VOCs) in industrial atmospheres. However, in order to employ passive sampling as a reliable sampling technique, a specific diffusive uptake rate is required for each compound. Accordingly, the aim of the present study was twofold. First, the experimental diffusive uptake rates of the target VOCs were determined under real industrial air conditions using Carbopack X thermal desorption tubes, and active sampling as reference method. The sampling campaigns carried out between October 2017 and May 2018 provided us of experimental diffusive uptake rates between 0.40 mL min−1 and 0.70 mL min−1 and stable over time (RSD % < 8%) for up to 41 VOCs. Secondly, the uptake rates obtained experimentally were applied for the determination of VOCs concentrations at 16 sampling sites in the North Industrial Complex of Tarragona. The results showed i-pentane, n-pentane and the compounds known as BTEX as the most representative ones. Moreover, some sporadic peaks of 1,3-butadiene, acrylonitrile, ethylbenzene and styrene resulting from certain industrial activities were detected.",industry
10.1016/j.matpr.2020.03.363,to_check,Materials Today: Proceedings,scopus,2019-01-01,sciencedirect,Real-time Thermal Error Compensation Strategy for Precision Machine tools,https://api.elsevier.com/content/abstract/scopus_id/85085555603,"Present manufacturing trend is towards producing precision components with better accuracy. Machine errors like geometrical, thermal and process errors affect the component accuracy. Among these errors, thermal error contributes more than 50-60% of the total machining error. This paper mainly focuses on the development of a real-time thermal error compensation module for precision machine tools and talks about effective modeling of thermal errors, development of thermal error compensation model using feed-forward backpropagation neural network and also simplified model using regression analysis technique, algorithm development for real-time compensation and implementation of module onto the open architecture CNC controller. The developed module has been successfully tested on a Diamond Turning Machine (DTM) by machining the precision component and also verified the effectiveness of the module",industry
10.1016/j.procs.2019.09.169,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,IAssistMe - Adaptable assistant for persons with eye disabilities,https://api.elsevier.com/content/abstract/scopus_id/85076257910,"Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers.",industry
10.1016/j.promfg.2019.03.047,to_check,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A Practical Approach of Teaching Digitalization and Safety Strategies in Cyber-Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85065658005,"Digitalization strategies in cyber-physical production systems (CPPS) are one of the key factors of Industry 4.0. The topic not only addresses data preparation, real-time data processing, big data analytics, visualization and machine interface design but also cyber security and safety. Especially, unauthorized access to protected (personal or enterprise) data or unauthorized control of production facilities imply risks when it comes to digitalization. Because of the increased complexity of state-of-the-art technologies, educational institutions need to provide practice-oriented teaching methods in learning factories to help engineers of today understand the impact of those developments.
                  In the light of this fact, this paper presents a practical approach of teaching digitalization strategies in CPPS. Planning, implementing and impacts of digitalization strategies are taught on a use-case with human-robot-collaboration. The objective of the use-case is to realize a real-time obstacle avoidance approach for a collaborative application based on a local positioning system. Here, students not only learn how to model the kinematics of a robot and program a robot but also how to design machine interfaces for real-time data transfer and processing as well as impacts of digitalization on safety and security.
                  The implementation of the use-case is part of the TU Wien teaching portfolio and thus part of its learning factory, where students and apprentices have the possibility to experiment and gain experiences by deliberate error simulations.",industry
10.1016/j.powtec.2018.08.064,to_check,Powder Technology,scopus,2018-11-01,sciencedirect,"Settling velocity of drill cuttings in drilling fluids: A review of experimental, numerical simulations and artificial intelligence studies",https://api.elsevier.com/content/abstract/scopus_id/85052516468,"In this paper, a comprehensive review of experimental, numerical and artificial intelligence studies on the subject of cuttings settling velocity in drilling muds made by researchers over the last seven decades is brought to the fore. In this respect, 91 experimental, 13 numerical simulations and 7 artificial intelligence researches were isolated, reviewed, tabulated and discussed. A comparison of the three methods and the challenges facing each of these methods were also reviewed. The major outcomes of this review include: (1) the unanimity among experimental researchers that mud rheology, particle size and shape and wall effect are major parameters affecting the settling velocity of cuttings in wellbores; (2) the prevalence of cuttings settling velocity experiments done with the mud in static conditions and the wellbore in the vertical configuration; (3) the extensive use of rigid particles of spherical shape to represent drill cuttings due to their usefulness in experimental visualization, particle tracking, and numerical implementation; (4) the existence of an artificial intelligence technique - multi-gene genetic programming (MGGP) which can provide an explicit equation that can help in predicting settling velocity; (5) the limited number of experimental studies factoring in the effect of pipe rotation and well inclination effects on the settling velocity of cuttings and (6) the most applied numerical method for determining settling velocity is the finite element method. Despite these facts, there is need to perform more experiments with real drill cuttings and factor in the effects of conditions such as drillstring rotation and well inclination and use data emanating therefrom to develop explicit models that would include the effects of these. It should be noted however, that the aim of this paper is not to create an encyclopaedia of particle settling velocity research, but to provide to the researcher with a basic, theoretical, experimental and numerical overview of what has so far been achieved in the area of cuttings settling velocity in drilling muds.",industry
10.1016/j.promfg.2018.04.026,to_check,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Design and implementation of a low cost RFID track and trace system in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85052890798,"The factories of the future will make use of actuators, sensors and cyber-physical systems (CPS) to provide an environment in which human beings, machines, and resources will communicate as in a social network. In such a network, communication between various “objects” relay the current state of the physical world. Business decisions are made using the information and it is therefore critical that this information is accurate and in real-time. Information flow is a key enabler of such future factories. Industrial engineers, as designers and improvement agents of such factories of the future, will need to develop better skills in various aspects of data analytics and information communication technologies. This paper describes the development and implementation of a low cost RFID track and trace system (by students) for application in a Learning Factory for teaching undergraduate industrial engineering students key concepts related to Industry 4.0 and “smart factories”. The benefit of this system is not only a demonstrator to be used in the Learning Factory, but also can be used to teach students in a “learning by doing” fashion critical skills related to real time tracking in a manufacturing environment. The system also demonstrates potential low cost implementation of such technologies in SME’s.",industry
10.1016/j.measurement.2015.06.004,to_check,Measurement: Journal of the International Measurement Confederation,scopus,2015-07-06,sciencedirect,A signal pre-processing algorithm designed for the needs of hardware implementation of neural classifiers used in condition monitoring,https://api.elsevier.com/content/abstract/scopus_id/84934767009,"Gearboxes have a significant influence on the durability and reliability of a power transmission system. Currently, extensive research studies are being carried out to increase the reliability of gearboxes working in the energy industry, especially with a focus on planetary gears in wind turbines and bucket wheel excavators. In this paper, a signal pre-processing algorithm designed for condition monitoring of planetary gears working in non-stationary operation is presented. The algorithm is dedicated for hardware implementation on Field Programmable Gate Arrays (FPGAs). The purpose of the algorithm is to estimate the features of a vibration signal that are related to failures, e.g. misalignment and unbalance. These features can serve as the components of an input vector for a neural classifier. The approach proposed here has several important benefits: it is resistant to small speed fluctuations up to 7%, it can be performed in real-time conditions and its implementation does not require many resources of FPGAs.",industry
10.1016/j.ifacol.2015.06.228,to_check,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Multicast dataset synchronization and agent negotiation in distributed manufacturing control systems,https://api.elsevier.com/content/abstract/scopus_id/84953870369,"Multi agent systems represent an elegant approach for the control architecture of manufacturing systems. Distributed control architectures have the potential to achieve greater flexibility by being capable of local decision making based on real time reasoning. One of the main challenges of these distributed architectures is represented by the capability to synchronize the production data across all execution points in a reliable and consistent fashion. In this context, this paper aims to resolve the problems associated with real time production data synchronization in distributed multi-agent control systems by proposing a common dataset synchronized across all agent entities using multicast network communication. On top of this common dataset approach, an agent negotiation mechanism is proposed that addresses the operation sequencing and resource allocation in decentralized operation model. The pilot implementation is using JADE multi agent platform and JGroups for real time data synchronization and NetLogo for abstract representation of the simulation system. Experimental results gathered from the pilot implementation are discussed.",industry
10.1016/j.neucom.2012.04.033,to_check,Neurocomputing,scopus,2013-06-03,sciencedirect,Applying soft computing techniques to optimise a dental milling process,https://api.elsevier.com/content/abstract/scopus_id/84875966713,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study.",industry
10.1016/j.dss.2012.08.006,to_check,Decision Support Systems,scopus,2012-12-01,sciencedirect,Sales forecasting for computer wholesalers: A comparison of multivariate adaptive regression splines and artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/84868667879,"Artificial neural networks (ANNs) have been found to be useful for sales/demand forecasting. However, one of the main shortcomings of ANNs is their inability to identify important forecasting variables. This study uses multivariate adaptive regression splines (MARS), a nonlinear and non-parametric regression methodology, to construct sales forecasting models for computer wholesalers. Through the outstanding variable screening ability of MARS, important sales forecasting variables for computer wholesalers can be obtained to enable them to make better sales management decisions. Two sets of real sales data collected from Taiwanese computer wholesalers are used to evaluate the performance of MARS. The experimental results show that the MARS model outperforms backpropagation neural networks, a support vector machine, a cerebellar model articulation controller neural network, an extreme learning machine, an ARIMA model, a multivariate linear regression model, and four two-stage forecasting schemes across various performance criteria. Moreover, the MARS forecasting results provide useful information about the relationships between the forecasting variables selected and sales amounts through the basis functions, important predictor variables, and the MARS prediction function obtained, and hence they have important implications for the implementation of appropriate sales decisions or strategies.",industry
10.1016/j.neucom.2011.06.027,to_check,Neurocomputing,scopus,2011-11-01,sciencedirect,Neural network based controller for Cr<sup>6+</sup>-Fe<sup>2+</sup> batch reduction process,https://api.elsevier.com/content/abstract/scopus_id/80053311549,An automated pilot plant has been designed and commissioned to carry out online/real-time data acquisition and control for the Cr6+–Fe2+ reduction process. Simulated data from the Cr6+–Fe2+ model derived are validated with online data and laboratory analysis using ICP-AES analysis method. The distinctive trend or patterns exhibited in the ORP profiles for the non-equilibrium model derived have been utilized to train neural network-based controllers for the process. The implementation of this process control is to ensure sufficient Fe2+ solution is dosed into the wastewater sample in order to reduce all Cr6+–Cr3+. The neural network controller has been utilized to compare the capability of set-point tracking with a PID controller in this process. For this process neural network-based controller dosed in less Fe2+ solution compared to the PID controller which hence reduces wastage of chemicals. Industrial Cr6+ wastewater samples obtained from an electro-plating factory has also been tested on the pilot plant using the neural network-based controller to determine its effectiveness to control the reduction process for a real plant. The results indicate the proposed controller is capable of fully reducing the Cr6+–Cr3+ in the batch treatment process with minimal dosage of Fe2+.,industry
10.1109/ACCESS.2020.3010609,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Privacy-Aware Crowd Management System for Smart Cities and Smart Buildings,https://ieeexplore.ieee.org/document/9144582/,"Cities are growing at a dizzying pace and they require improved methods to manage crowded areas. Crowd management stands for the decisions and actions taken to supervise and control densely populated spaces and it involves multiple challenges, from recognition and assessment to application of actions tailored to the current situation. To that end, Wi-Fi-based monitoring systems have emerged as a cost-effective solution for the former one. The key challenge that they impose is the requirement to handle large datasets and provide results in near real-time basis. However, traditional big data and event processing approaches have important shortcomings while dealing with crowd management information. In this paper, we describe a novel system architecture for real-time crowd recognition for smart cities and smart buildings that can be easily replicated. The described system proposes a privacy-aware platform that enables the application of artificial intelligence mechanisms to assess crowds' behavior in buildings employing sensed Wi-Fi traces. Furthermore, the present paper shows the implementation of the system in two buildings, an airport and a market, as well as the results of applying a set of classification algorithms to provide crowd management information.",smart cities
10.1109/ACCESS.2019.2928233,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Towards Disaster Resilient Smart Cities: Can Internet of Things and Big Data Analytics Be the Game Changers?,https://ieeexplore.ieee.org/document/8759905/,"Disasters (natural or man-made) can be lethal to human life, the environment, and infrastructure. The recent advancements in the Internet of Things (IoT) and the evolution in big data analytics (BDA) technologies have provided an open opportunity to develop highly needed disaster resilient smart city environments. In this paper, we propose and discuss the novel reference architecture and philosophy of a disaster resilient smart city (DRSC) through the integration of the IoT and BDA technologies. The proposed architecture offers a generic solution for disaster management activities in smart city incentives. A combination of the Hadoop Ecosystem and Spark are reviewed to develop an efficient DRSC environment that supports both real-time and offline analysis. The implementation model of the environment consists of data harvesting, data aggregation, data pre-processing, and big data analytics and service platform. A variety of datasets (i.e., smart buildings, city pollution, traffic simulator, and twitter) are utilized for the validation and evaluation of the system to detect and generate alerts for a fire in a building, pollution level in the city, emergency evacuation path, and the collection of information about natural disasters (i.e., earthquakes and tsunamis). The evaluation of the system efficiency is measured in terms of processing time and throughput that demonstrates the performance superiority of the proposed architecture. Moreover, the key challenges faced are identified and briefly discussed.",smart cities
10.1109/ICA-ACCA.2018.8609705,to_check,2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA),IEEE,2018-10-19 00:00:00,ieeexplore,A Survey on Intelligent Traffic Lights,https://ieeexplore.ieee.org/document/8609705/,"Transportation of people and goods has a huge impact on the economy of countries. The exponential grows of vehicles in the cities brings traffic congestion consequences like pollution, high combustible consumption and lowers the quality of life of the citizens. The development of Intelligent Transportation Systems is a key component of Smart Cities; Artificial Intelligence (AI) techniques used to build Intelligent Traffic Light Controllers (ITLC). This paper presents a survey of current technologies being use in the development of ITLC; many of them are experimental and tested in simulators, which show that implanting ITLC systems in real scenarios is difficult and expensive. The main objective of this survey is to find related work and algorithms to control traffic in real-time scenarios with AI. Artificial Vision rises as a promising technology for building reliable Intelligent Traffic Lights (ITL) as the technology evolves the error rate is decreasing, and more applications appear such as vehicle counting and classification, identifying vehicle's moving patterns and giving priority for emergency vehicles. Furthermore, ITL implementation discussed, and the algorithms and different solutions compared.",smart cities
10.1109/JSEN.2020.2995779,to_check,IEEE Sensors Journal,IEEE,2001-10-01 20:20:00,ieeexplore,Enabling Real-Time Computation of Psycho-Acoustic Parameters in Acoustic Sensors Using Convolutional Neural Networks,https://ieeexplore.ieee.org/document/9096320/,"Sensor networks have become an extremely useful tool for monitoring and analysing many aspects of our daily lives. Noise pollution levels are very important today, especially in cities where the number of inhabitants and disturbing sounds are constantly increasing. Psycho-acoustic parameters are a fundamental tool for assessing the degree of discomfort produced by different sounds and, combined with wireless acoustic sensor networks (WASNs), could enable, for example, the efficient implementation of acoustic discomfort maps within smart cities. However, the continuous monitoring of psycho-acoustic parameters to create time-dependent discomfort maps requires a high computational demand that prevents real-time computations within the nodes. Moreover, sending audio streams outside of the WASN for their further computation, would require extra communication and computational efforts without warranting a real-time monitoring, with the added problem of violating some privacy laws. As a result, most existing systems for nuisance assessment are usually based on less accurate indicators that require lower computational cost. In this paper, we describe the design and analysis of a deep convolutional neural network (CNN) trained with a big dataset of typical sounds occurrying in a city. The CNN allows to predict the psycho-acoustic parameters considered by the well-known Zwicker's psycho-acoustic nuisance model with great accuracy, directly from the raw recorded audio signal. The proposed CNN-based system has been tested on both desktop computers and typical WASN devices (such as Raspberry Pi), achieving very fast calculation times that allow real-time operation and a continuous monitoring of psycho-acoustic parameters.",smart cities
10.1109/CAST.2016.7914958,to_check,"2016 International Conference on Computing, Analytics and Security Trends (CAST)",IEEE,2016-12-21 00:00:00,ieeexplore,Intelligent traffic signal synchronization using fuzzy logic and Q-learning,https://ieeexplore.ieee.org/document/7914958/,"In the past decade, urban traffic has increased tremendously. As a result, the urban population has to invest more time in traveling. Increased road traffic results in an increased number of road accidents and more consumption of fuel, thus wasting energy. Hence for solving this issue, this paper proposes a traffic signal synchronization system which takes real time traffic signal data as input and with the implementation of multi-agent fuzzy logic, it introduces the design of an intelligent system which would smoothen the overall road traffic of the city. Fuzzy system is capable of handling the various levels of uncertainties found in the input data taken from the traffic signals. Since fuzzy logic system needs expert knowledge for its rule base and the rule base remains unchanged once defined, this paper adds up Q-learning module so that the system learns by itself by updating the set of rule base.",smart cities
10.1109/ICCMC.2019.8819812,to_check,2019 3rd International Conference on Computing Methodologies and Communication (ICCMC),IEEE,2019-03-29 00:00:00,ieeexplore,Locating object - of - interest &amp; preventing Stampede using crowd management,https://ieeexplore.ieee.org/document/8819812/,"Congestion in urban areas is becoming a serious problem in modern society. Large scale events at locations such as movie theatres or sports stadiums, etc which attract thousands of spectators can be expected to cause more inconvenience in future as compared to the present day conditions. Crowd management is important as with the increase in number of individuals a number of problems can occur, like, the risks of people or objects getting lost and stampede in areas of high crowd density. It is essential for people to live safely and securely in such large cities. Real-time analysis and rapid response is necessary for the security and safety. Using Artificial Intelligence we will be able to track people and object of interest almost immediately. In cases of people or costly objects being lost it becomes imperative to find its location in real-time, and this is where this implementation has an upper-hand on all its predecessors. In places of high density of population it is very common for children to get separated from their parents. Even at places where CCTV cameras are installed it is very time seeking to search for a particular object in millions of objects.",smart cities
10.1109/ITSC.2015.196,to_check,2015 IEEE 18th International Conference on Intelligent Transportation Systems,IEEE,2015-09-18 00:00:00,ieeexplore,Assessment of Adaptive Traffic Signal Control Using Hardware in the Loop Simulation,https://ieeexplore.ieee.org/document/7313288/,"Adaptive Traffic Signal Control (ATSC) can potentially mitigate traffic congestion. Research and development in the area of ATSC have produced a number of new systems with promising potential, however the performance of these systems under real-life conditions has been always a concern for practitioners as well as researchers, particularly if and how the new systems would be implementable in the field on controllers with specific capabilities and limitations. Therefore, testing and refining new ATSC systems on actual hardware and under representative traffic conditions prior to field implementation is essential to bridge this gap. In this paper, a hardware in-the-loop simulation (HILS) framework is developed to evaluate MARLIN, as an example of a new self-learning ATSC system. HILS is used for evaluating hardware components running the ATSC software in a simulation environment in which an actual traffic signal controller and an embedded computer are physically connected to a microscopic traffic simulator. Our focus is on the development, implementation of the HILS framework and the evaluation of MARLIN, on an intersection that suffers significant traffic fluctuation and delays - at the City of Burlington, Ontario, Canada. The performance of MARLIN-ATSC is demonstrated with HILS, which consists of a PEEK ATC-1000 traffic controller, an embedded computer running the ATSC system, and Paramics microscopic simulation model. HILS results indicated that MARLIN-ATSC has the potential to reduce the intersection average delay by up to 20% on average compared to the optimized and coordinated actuated signal timing plans.",smart cities
10.1109/ACCESS.2020.2976433,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Short-Term Parking Demand Prediction Method Based on Variable Prediction Interval,https://ieeexplore.ieee.org/document/9016013/,"With the rapid economic development, parking problems have become increasingly prominent due to the city's development model and the emergence of a large number of private cars. Parking management departments around the world focus on intelligent parking system in order to solve parking problems, but most of them are limited to upgrading the parking infrastructure. There is no effective solution from a perspective of the fundamental cause to solve the parking problem. At present, it is generally believed that parking guidance systems can effectively alleviate parking problems and provide drivers and traffic managers with real-time and accurate parking information. As one of the prerequisites of the parking guidance system, the accuracy of the short-term parking demand prediction method determines whether the implementation of the parking guidance scheme can effectively solve the parking problem in a certain area. Based on this, we have studied a short-term parking demand prediction in this paper. First, focusing on the distribution of typical parking arrivals and departures regular pattern, a parking demand prediction model was constructed utilizing the Markov birth and death process, and model parameters were calibrated utilizing curve fitting method and undetermined coefficients method. The simulation environment was set up utilizing the Markov process to verify the accuracy of the availability of parameter estimation method. Secondly, a method for determining the prediction interval based on the parking trend was given for different situations with the parking rush hours and ordinary hours which parking arrival and departure parameters were different. In order to verify the effectiveness of the model in practical applications, parking arrival and departure data from June 17 to June 23, 2019 in Jilin University of Nanling Campus was used to verify the short-term parking demand prediction method proposed in this paper. The results show that the parking demand prediction model proposed in this paper can accurately calibrate model parameters, predict parking demand quickly and effectively and provide theoretical reference and technical support for parking planning and management.",smart cities
10.1109/ColCACI.2019.8781798,to_check,2019 IEEE Colombian Conference on Applications in Computational Intelligence (ColCACI),IEEE,2019-06-07 00:00:00,ieeexplore,Video processing inside embedded devices using SSD-Mobilenet to count mobility actors,https://ieeexplore.ieee.org/document/8781798/,"The actual number of surveillance cameras and the different methods for counting vehicles originate the question: What is the best place to process video flows? This work performs the implementation of a counting system for mobility actors like cars, pedestrians, motorcycles, bicycles, buses, and trucks in the context of an Edge computing application using deep learning. However, the implementation of Deep Neural Networks for Object Detection in low-capacity embedded devices make it difficult to perform tasks that require high processing or must be carried out in real time. To solve this problem this study presents the analysis and implementation of different techniques based on the use of an additional hardware element as is the case of a Vision Processing Unit (VPU) in combination with methods that affect the resolution, bit rate, and time of video processing. For this purpose we consider the Mobilenet-SSD model with two approaches: a pre-trained model with known data sets and a trained model with images from our specific scenarios. The use of SSD-Mobilenet's model generates different results in terms of accuracy and time of video processing in the system. Results show that the use of an embedded device in combination with a VPU and video processing techniques reach 18.62 Frames per Second (FPS). Thus, video processing time is slightly superior (5.63 minutes) for a video of 5 minutes. Recall and precision values of 91% and 97% are reported in the best case (class car) for the vehicle counting system.",smart cities
10.1109/ICMLA.2015.65,to_check,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),IEEE,2015-12-11 00:00:00,ieeexplore,A Neural Network Based Handover Management Strategy for Heterogeneous Networks,https://ieeexplore.ieee.org/document/7424486/,"One of the key challenges for improvement of quality of services (QoS) in Heterogeneous wireless networks is the design of Vertical Handover (VHO) Management strategy. VHO is required to guide the decision for a mobile terminal (MT) to handoff between different types of networks. This is an essential task to cope with various multimedia services QoS settings. In this paper, we present a machine learning scheme based on Neural Network for calls vertical handover in heterogeneous networks. The Neural Network Based Handover Management Scheme (NNBHMS) of this paper aims toward achieving seamless connectivity and Always Best Connected (ABC) call status for group mobility over a set of heterogeneous networks. The proposed scheme evaluates and creates relationships between different decision criteria related to heterogeneous networks conditions, terminal capabilities, application requirements, and user preferences. Afterward, the estimates of each attribute are forwarded to neural network to select the optimal access network. The proposed scheme is applied for vertical handover Management in heterogeneous networks offering both real time services (voice over IP services), and data Services (packet data traffic). Through the implementation of neural networks based machine learning approach, the proposed research scheme allows solving the complexity of the handover decision process resulting from the multitude dimensions of the decision criteria and the dynamicity of many of its components. The performance results evaluated through simulation show that the use of the a neural network based machine learning scheme to carry out the Handover process can enhance the QoS perceived by both types of voice and data service while fulfilling to great extent the user preference.",smart cities
10.1109/TCST.2011.2180386,to_check,IEEE Transactions on Control Systems Technology,IEEE,2013-01-01 00:00:00,ieeexplore,Prediction of Short-Term Traffic Variables Using Intelligent Swarm-Based Neural Networks,https://ieeexplore.ieee.org/document/6126004/,"This brief presents an innovative algorithm integrated with particle swarm optimization and artificial neural networks to develop short-term traffic flow predictors, which are intended to provide traffic flow forecasting information for traffic management in order to reduce traffic congestion and improve mobility of transportation. The proposed algorithm aims to address the issues of development of short-term traffic flow predictors which have not been addressed fully in the current literature namely that: 1) strongly non-linear characteristics are unavoidable in traffic flow data; 2) memory space for implementation of short-term traffic flow predictors is limited; 3) specification of model structures for short-term traffic flow predictors which do not involve trial and error methods based on human expertise; and 4) adaptation to newly-captured, traffic flow data is required. The proposed algorithm was applied to forecast traffic flow conditions on a section of freeway in Western Australia, whose traffic flow information is newly-captured. These results clearly demonstrate the effectiveness of using the proposed algorithm for real-time traffic flow forecasting.",smart cities
10.1109/ICMLC.2008.4620381,to_check,2008 International Conference on Machine Learning and Cybernetics,IEEE,2008-07-15 00:00:00,ieeexplore,Image fusion algorithm in Intelligent Transport System,https://ieeexplore.ieee.org/document/4620381/,"Traffic congestion has been increasing world-wide as a result of increased motorization, urbanization, population growth and changes in population density. Interest in intelligent transport system (ITS) comes from the problems caused by traffic congestion and a synergy of new information technologies for simulation, real-time control and communications networks. Successful implementation of ITS depends upon complete and accurate vehicle information. An image fusion algorithm based on lifting wavelet transform (LWT) is presented in this paper to combine an infrared image and a visible light image into a single composite image. A new image fusion method is proposed in this paper: after lifting wavelet transform, mean gradient are used to determine the coefficients in fusion formula for low frequency component. Local deviation rules are applied to merge the high frequency coefficients. Experimental results have shown that most of the final composite images have better quality than either of the source images. The results indicate that application of this algorithm improves performance of ITS.",smart cities
10.1109/CCA.2003.1223466,to_check,"Proceedings of 2003 IEEE Conference on Control Applications, 2003. CCA 2003.",IEEE,2003-06-25 00:00:00,ieeexplore,Optimal neuro-controller in longitudinal auto-landing of a commercial jet transport,https://ieeexplore.ieee.org/document/1223466/,"In the last three decades, optimality-based auto-landing designs have been considered to the most effective way by many authors. However, it is known that the straight forward solution to the optimal control problem leads to Two Point Boundary Value Problem (TPBVP) (Riccati equation), which is usually too complex in solution, backward in the time, and real-time onboard implementation, or the final time, as a boundary condition, may also not be known precisely. To avoid these problems, first, a suboptimal solution by assuming t/sub f//spl rarr//spl infin/ has been considered and its inapplicability has been discussed. Then an optimal controller for landing phase of a typical commercial aircraft has been designed. Finally, seven neural networks were being trained to learn the costates of the system to estimate the costates in similar scenarios without using the final time value, which usually is needed in solving the optimal control problems.",smart cities
10.1109/TSP.2015.7296288,to_check,2015 38th International Conference on Telecommunications and Signal Processing (TSP),IEEE,2015-07-11 00:00:00,ieeexplore,Adaptive noise suppression in voice communication using a neuro-fuzzy inference system,https://ieeexplore.ieee.org/document/7296288/,"This paper describes the implementation of a combination of techniques of the fuzzy system and artificial intelligence in the application area of non-linear suppression of noise and interference. The structure used is called ANFIS (Adaptive Neuro Fuzzy Inference System). This system finds practical use mainly in audio telephone (mobile) communication in a noisy environment (transport, production halls, sports matches, etc.). Within the experiments carried out, the authors created, based on the ANFIS structure, a comprehensive system for the adaptive suppression of unwanted background interference that occurs in audio communication and which degrades the audio signal. The system designed has been tested on real voice signals. Noise cancellation performance of the algorithms has been compared by means of SSNR (Segmental Signal to Noise Ratio) and DTW (Dynamic Time Warping). Also processing durations of the algorithms are determined for evaluating the possibility of real time implementation. The results imply that a system using ANFIS has better experimental results than conventional systems built on adaptive algorithms of the LMS (Least Mean Squares) and RLS (Recursive Least Squares) families.",smart cities
10.1109/INES.2014.6909343,to_check,IEEE 18th International Conference on Intelligent Engineering Systems INES 2014,IEEE,2014-07-05 00:00:00,ieeexplore,Autonomic monitoring approach based on CEP and ML for logistic of sensitive goods,https://ieeexplore.ieee.org/document/6909343/,"The main objective of this work is to develop a framework for supporting the development of applications for logistic companies that transport perishable goods (food and medicines). Reducing the amount of lost and damaged perishable goods during transportation and storage represents a substantial global challenge, which imply the implementation of cold chain monitoring at all levels of the supply chains. The framework contains several components that enable: (1) the real-time monitoring of goods during transportation; (2) forecast the temperatures of parcels; (3) generate real-time alerts/early warning when the product are not stored according to the acceptance criteria.",smart cities
10.1109/GSCIT.2014.6970119,to_check,2014 Global Summit on Computer & Information Technology (GSCIT),IEEE,2014-06-16 00:00:00,ieeexplore,Implementation of CMT-SCTP in real internet lab setup,https://ieeexplore.ieee.org/document/6970119/,"In this study we implement CMT-SCTP (Concurrent Multipath Transfer extension of the Stream Control Transmission Protocol) in a real lab Internet setup. SCTP is an innovative transport layer protocol, which is reliable and provides services such as multihoming and multistreaming. CMT extension allows simultaneous utilization of multipaths for reaching the higher data transmission rate between two hosts, which is beneficial for the emerging multi-interface devices. The literature on CMT-SCTP optimization shows that, there are several simulation tools used by the researchers for performance evaluation, to validate propositions and to experiment the further extensions of SCTP/CMT-SCTP. Indeed, such tools are convenient and can quickly verify the proposal results. We establish a real lab Internet setup first and then design a number of test scenarios for CMT-SCTP. To verify our implementation the simulations needs to be performed on the test scenarios to set a reference output prior to the lab setup experiments. The experiment results from the successful implementation of CMT-SCTP on the real lab setup agrees to the reference output reasonably.",smart cities
10.1109/ICMLA.2015.209,to_check,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),IEEE,2015-12-11 00:00:00,ieeexplore,Intelligent Bus Stop Identification Using Smartphone Sensors,https://ieeexplore.ieee.org/document/7424444/,"Intelligent transportation systems can be built by developing models that learn from the collected transport data. Data collection and implementation of such systems is often costly, and few countries have support for such systems in their transportation budgets. In places where maintaining currency and accuracy of information is difficult, many problems arise. For instance, in Chennai, India, real time bus transit data is not maintained, there is no proper communication about the bus schedules, bus stops are not regularly updated and inconsistent information about bus stops is observed in the transport authority's website. We are interested in developing models for identifying bus stops from trajectories for situations where accurate and current information is not available and traffic conditions are challenging, such as Chennai, India. We develop a simple yet easily accessible Android mobile application (App) to collect GPS traces of bus routes. We use our App to collect GPS trajectory data from Baltimore, Maryland, a place where there are facilities to access up-to-date information about bus stops. We also collect GPS trajectories from Chennai, India. We then develop a model using machine learning techniques to identify bus stops from the collected trajectories. We experimentally evaluate our model by training it on the Baltimore dataset and testing it on the Chennai dataset, achieving testing accuracy between 85 -- 90%. This is comparable to the accuracy of 95% achieved by both training and testing on the Chennai dataset. This illustrates that our approach is effective in helping maintain an accurate and current transport information system for resource constraint environments.",smart cities
http://arxiv.org/abs/2005.02544v1,to_check,arxiv,arxiv,2020-05-06 00:30:29+00:00,arxiv,"AutoScale: Optimizing Energy Efficiency of End-to-End Edge Inference
  under Stochastic Variance",http://arxiv.org/abs/2005.02544v1,"Deep learning inference is increasingly run at the edge. As the programming
and system stack support becomes mature, it enables acceleration opportunities
within a mobile system, where the system performance envelope is scaled up with
a plethora of programmable co-processors. Thus, intelligent services designed
for mobile users can choose between running inference on the CPU or any of the
co-processors on the mobile system, or exploiting connected systems, such as
the cloud or a nearby, locally connected system. By doing so, the services can
scale out the performance and increase the energy efficiency of edge mobile
systems. This gives rise to a new challenge - deciding when inference should
run where. Such execution scaling decision becomes more complicated with the
stochastic nature of mobile-cloud execution, where signal strength variations
of the wireless networks and resource interference can significantly affect
real-time inference performance and system energy efficiency. To enable
accurate, energy-efficient deep learning inference at the edge, this paper
proposes AutoScale. AutoScale is an adaptive and light-weight execution scaling
engine built upon the custom-designed reinforcement learning algorithm. It
continuously learns and selects the most energy-efficient inference execution
target by taking into account characteristics of neural networks and available
systems in the collaborative cloud-edge execution environment while adapting to
the stochastic runtime variance. Real system implementation and evaluation,
considering realistic execution scenarios, demonstrate an average of 9.8 and
1.6 times energy efficiency improvement for DNN edge inference over the
baseline mobile CPU and cloud offloading, while meeting the real-time
performance and accuracy requirement.",smart cities
http://arxiv.org/abs/2011.06144v1,to_check,arxiv,arxiv,2020-11-12 01:06:17+00:00,arxiv,I-POST: Intelligent Point of Sale and Transaction System,http://arxiv.org/abs/2011.06144v1,"We propose a novel solution for the cashier problem. Current cashier
system/Point of Sale (POS) terminals can be inefficient, cumbersome and
time-consuming for the users. There is a need for a solution dependent on
modern technology and ubiquitous computing resources. We present I-POST
(Intelligent Point of Sale and Transaction) as a software system that uses
smart devices, mobile phone and state of the art machine learning algorithms to
process the user transactions in automated and real time manner. I-POST is an
automated checkout system that allows the user to walk in a store, collect his
items and exit the store. There is no need to stand and wait in a queue. The
system uses object detection and facial recognition algorithm to process the
authentication of the client and the state of the object. At point of exit, the
classifier sends the data to the backend server which execute the payments. The
system uses Convolution Neural Network (CNN) for the image recognition and
processing. CNN is a supervised learning model that has found major application
in pattern recognition problem. The current implementation uses two classifiers
that work intrinsically to authenticate the user and track the items. The model
accuracy for object recognition is 97%, the loss is 9.3%. We expect that such
systems can bring efficiency to the market and has the potential for broad and
diverse applications.",smart cities
http://arxiv.org/abs/1905.11669v1,to_check,arxiv,arxiv,2019-05-28 08:24:58+00:00,arxiv,"CompactNet: Platform-Aware Automatic Optimization for Convolutional
  Neural Networks",http://arxiv.org/abs/1905.11669v1,"Convolutional Neural Network (CNN) based Deep Learning (DL) has achieved
great progress in many real-life applications. Meanwhile, due to the complex
model structures against strict latency and memory restriction, the
implementation of CNN models on the resource-limited platforms is becoming more
challenging. This work proposes a solution, called CompactNet\footnote{Project
URL: \url{https://github.com/CompactNet/CompactNet}}, which automatically
optimizes a pre-trained CNN model on a specific resource-limited platform given
a specific target of inference speedup. Guided by a simulator of the target
platform, CompactNet progressively trims a pre-trained network by removing
certain redundant filters until the target speedup is reached and generates an
optimal platform-specific model while maintaining the accuracy. We evaluate our
work on two platforms of a mobile ARM CPU and a machine learning accelerator
NPU (Cambricon-1A ISA) on a Huawei Mate10 smartphone. For the state-of-the-art
slim CNN model made for the embedded platform, MobileNetV2, CompactNet achieves
up to a 1.8x kernel computation speedup with equal or even higher accuracy for
image classification tasks on the Cifar-10 dataset.",smart cities
http://arxiv.org/abs/2007.12640v1,to_check,arxiv,arxiv,2020-07-24 16:50:41+00:00,arxiv,"Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning
  on Graphs",http://arxiv.org/abs/2007.12640v1,"We consider an autonomous exploration problem in which a range-sensing mobile
robot is tasked with accurately mapping the landmarks in an a priori unknown
environment efficiently in real-time; it must choose sensing actions that both
curb localization uncertainty and achieve information gain. For this problem,
belief space planning methods that forward-simulate robot sensing and
estimation may often fail in real-time implementation, scaling poorly with
increasing size of the state, belief and action spaces. We propose a novel
approach that uses graph neural networks (GNNs) in conjunction with deep
reinforcement learning (DRL), enabling decision-making over graphs containing
exploration information to predict a robot's optimal sensing action in belief
space. The policy, which is trained in different random environments without
human intervention, offers a real-time, scalable decision-making process whose
high-performance exploratory sensing actions yield accurate maps and high rates
of information gain.",smart cities
http://arxiv.org/abs/1812.03078v1,to_check,arxiv,arxiv,2018-12-07 15:57:54+00:00,arxiv,"Evolutionary Games, Complex Networks and Nonlinear Analysis for
  Epileptic Seizures Forecasting",http://arxiv.org/abs/1812.03078v1,"Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.",smart cities
http://arxiv.org/abs/1904.03814v2,to_check,arxiv,arxiv,2019-04-08 03:21:11+00:00,arxiv,Temporal Convolution for Real-time Keyword Spotting on Mobile Devices,http://arxiv.org/abs/1904.03814v2,"Keyword spotting (KWS) plays a critical role in enabling speech-based user
interactions on smart devices. Recent developments in the field of deep
learning have led to wide adoption of convolutional neural networks (CNNs) in
KWS systems due to their exceptional accuracy and robustness. The main
challenge faced by KWS systems is the trade-off between high accuracy and low
latency. Unfortunately, there has been little quantitative analysis of the
actual latency of KWS models on mobile devices. This is especially concerning
since conventional convolution-based KWS approaches are known to require a
large number of operations to attain an adequate level of performance. In this
paper, we propose a temporal convolution for real-time KWS on mobile devices.
Unlike most of the 2D convolution-based KWS approaches that require a deep
architecture to fully capture both low- and high-frequency domains, we exploit
temporal convolutions with a compact ResNet architecture. In Google Speech
Command Dataset, we achieve more than \textbf{385x} speedup on Google Pixel 1
and surpass the accuracy compared to the state-of-the-art model. In addition,
we release the implementation of the proposed and the baseline models including
an end-to-end pipeline for training models and evaluating them on mobile
devices.",smart cities
http://arxiv.org/abs/2001.10632v1,to_check,arxiv,arxiv,2020-01-28 23:13:12+00:00,arxiv,IoT Behavioral Monitoring via Network Traffic Analysis,http://arxiv.org/abs/2001.10632v1,"Smart homes, enterprises, and cities are increasingly being equipped with a
plethora of Internet of Things (IoT), ranging from smart-lights to security
cameras. While IoT networks have the potential to benefit our lives, they
create privacy and security challenges not seen with traditional IT networks.
Due to the lack of visibility, operators of such smart environments are not
often aware of their IoT assets, let alone whether each IoT device is
functioning properly safe from cyber-attacks. This thesis is the culmination of
our efforts to develop techniques to profile the network behavioral pattern of
IoTs, automate IoT classification, deduce their operating context, and detect
anomalous behavior indicative of cyber-attacks.
  We begin this thesis by surveying IoT ecosystem, while reviewing current
approaches to vulnerability assessments, intrusion detection, and behavioral
monitoring. For our first contribution, we collect traffic traces and
characterize the network behavior of IoT devices via attributes from traffic
patterns. We develop a robust machine learning-based inference engine trained
with these attributes and demonstrate real-time classification of 28 IoT
devices with over 99% accuracy. Our second contribution enhances the
classification by reducing the cost of attribute extraction while also
identifying IoT device states. Prototype implementation and evaluation
demonstrate the ability of our supervised machine learning method to detect
behavioral changes for five IoT devices. Our third and final contribution
develops a modularized unsupervised inference engine that dynamically
accommodates the addition of new IoT devices and/or updates to existing ones,
without requiring system-wide retraining of the model. We demonstrate via
experiments that our model can automatically detect attacks and firmware
changes in ten IoT devices with over 94% accuracy.",smart cities
http://arxiv.org/abs/2007.14545v2,to_check,arxiv,arxiv,2020-07-29 01:09:27+00:00,arxiv,"Learning Object-conditioned Exploration using Distributed Soft Actor
  Critic",http://arxiv.org/abs/2007.14545v2,"Object navigation is defined as navigating to an object of a given label in a
complex, unexplored environment. In its general form, this problem poses
several challenges for Robotics: semantic exploration of unknown environments
in search of an object and low-level control. In this work we study
object-guided exploration and low-level control, and present an end-to-end
trained navigation policy achieving a success rate of 0.68 and SPL of 0.58 on
unseen, visually complex scans of real homes. We propose a highly scalable
implementation of an off-policy Reinforcement Learning algorithm, distributed
Soft Actor Critic, which allows the system to utilize 98M experience steps in
24 hours on 8 GPUs. Our system learns to control a differential drive mobile
base in simulation from a stack of high dimensional observations commonly used
on robotic platforms. The learned policy is capable of object-guided
exploratory behaviors and low-level control learned from pure experiences in
realistic environments.",smart cities
http://arxiv.org/abs/1712.02427v1,to_check,arxiv,arxiv,2017-12-06 22:27:36+00:00,arxiv,High performance ultra-low-precision convolutions on mobile devices,http://arxiv.org/abs/1712.02427v1,"Many applications of mobile deep learning, especially real-time computer
vision workloads, are constrained by computation power. This is particularly
true for workloads running on older consumer phones, where a typical device
might be powered by a single- or dual-core ARMv7 CPU. We provide an open-source
implementation and a comprehensive analysis of (to our knowledge) the state of
the art ultra-low-precision (<4 bit precision) implementation of the core
primitives required for modern deep learning workloads on ARMv7 devices, and
demonstrate speedups of 4x-20x over our additional state-of-the-art float32 and
int8 baselines.",smart cities
http://arxiv.org/abs/1702.06329v1,to_check,arxiv,arxiv,2017-02-21 11:07:27+00:00,arxiv,"Towards a Common Implementation of Reinforcement Learning for Multiple
  Robotic Tasks",http://arxiv.org/abs/1702.06329v1,"Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",smart cities
http://arxiv.org/abs/2007.02351v1,to_check,arxiv,arxiv,2020-07-05 14:24:24+00:00,arxiv,Offline Model Guard: Secure and Private ML on Mobile Devices,http://arxiv.org/abs/2007.02351v1,"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time.",smart cities
http://arxiv.org/abs/2002.05509v1,to_check,arxiv,arxiv,2020-02-13 14:22:39+00:00,arxiv,Replacing Mobile Camera ISP with a Single Deep Learning Model,http://arxiv.org/abs/2002.05509v1,"As the popularity of mobile photography is growing constantly, lots of
efforts are being invested now into building complex hand-crafted camera ISP
solutions. In this work, we demonstrate that even the most sophisticated ISP
pipelines can be replaced with a single end-to-end deep learning model trained
without any prior knowledge about the sensor and optics used in a particular
device. For this, we present PyNET, a novel pyramidal CNN architecture designed
for fine-grained image restoration that implicitly learns to perform all ISP
steps such as image demosaicing, denoising, white balancing, color and contrast
correction, demoireing, etc. The model is trained to convert RAW Bayer data
obtained directly from mobile camera sensor into photos captured with a
professional high-end DSLR camera, making the solution independent of any
particular mobile ISP implementation. To validate the proposed approach on the
real data, we collected a large-scale dataset consisting of 10 thousand
full-resolution RAW-RGB image pairs captured in the wild with the Huawei P20
cameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The
experiments demonstrate that the proposed solution can easily get to the level
of the embedded P20's ISP pipeline that, unlike our approach, is combining the
data from two (RGB + B/W) camera sensors. The dataset, pre-trained models and
codes used in this paper are available on the project website.",smart cities
http://arxiv.org/abs/1907.07210v1,to_check,arxiv,arxiv,2019-07-16 18:33:20+00:00,arxiv,Real-time Vision-based Depth Reconstruction with NVidia Jetson,http://arxiv.org/abs/1907.07210v1,"Vision-based depth reconstruction is a challenging problem extensively
studied in computer vision but still lacking universal solution. Reconstructing
depth from single image is particularly valuable to mobile robotics as it can
be embedded to the modern vision-based simultaneous localization and mapping
(vSLAM) methods providing them with the metric information needed to construct
accurate maps in real scale. Typically, depth reconstruction is done nowadays
via fully-convolutional neural networks (FCNNs). In this work we experiment
with several FCNN architectures and introduce a few enhancements aimed at
increasing both the effectiveness and the efficiency of the inference. We
experimentally determine the solution that provides the best
performance/accuracy tradeoff and is able to run on NVidia Jetson with the
framerates exceeding 16FPS for 320 x 240 input. We also evaluate the suggested
models by conducting monocular vSLAM of unknown indoor environment on NVidia
Jetson TX2 in real-time. Open-source implementation of the models and the
inference node for Robot Operating System (ROS) are available at
https://github.com/CnnDepth/tx2_fcnn_node.",smart cities
http://arxiv.org/abs/1707.02880v2,to_check,arxiv,arxiv,2017-07-10 14:34:06+00:00,arxiv,Deep Bilateral Learning for Real-Time Image Enhancement,http://arxiv.org/abs/1707.02880v2,"Performance is a critical challenge in mobile image processing. Given a
reference imaging pipeline, or even human-adjusted pairs of images, we seek to
reproduce the enhancements and enable real-time evaluation. For this, we
introduce a new neural network architecture inspired by bilateral grid
processing and local affine color transforms. Using pairs of input/output
images, we train a convolutional neural network to predict the coefficients of
a locally-affine model in bilateral space. Our architecture learns to make
local, global, and content-dependent decisions to approximate the desired image
transformation. At runtime, the neural network consumes a low-resolution
version of the input image, produces a set of affine transformations in
bilateral space, upsamples those transformations in an edge-preserving fashion
using a new slicing node, and then applies those upsampled transformations to
the full-resolution image. Our algorithm processes high-resolution images on a
smartphone in milliseconds, provides a real-time viewfinder at 1080p
resolution, and matches the quality of state-of-the-art approximation
techniques on a large class of image operators. Unlike previous work, our model
is trained off-line from data and therefore does not require access to the
original operator at runtime. This allows our model to learn complex,
scene-dependent transformations for which no reference implementation is
available, such as the photographic edits of a human retoucher.",smart cities
http://arxiv.org/abs/1512.02972v1,to_check,arxiv,arxiv,2015-12-09 18:08:59+00:00,arxiv,Get More With Less: Near Real-Time Image Clustering on Mobile Phones,http://arxiv.org/abs/1512.02972v1,"Machine learning algorithms, in conjunction with user data, hold the promise
of revolutionizing the way we interact with our phones, and indeed their
widespread adoption in the design of apps bear testimony to this promise.
However, currently, the computationally expensive segments of the learning
pipeline, such as feature extraction and model training, are offloaded to the
cloud, resulting in an over-reliance on the network and under-utilization of
computing resources available on mobile platforms. In this paper, we show that
by combining the computing power distributed over a number of phones, judicious
optimization choices, and contextual information it is possible to execute the
end-to-end pipeline entirely on the phones at the edge of the network,
efficiently. We also show that by harnessing the power of this combination, it
is possible to execute a computationally expensive pipeline at near real-time.
  To demonstrate our approach, we implement an end-to-end image-processing
pipeline -- that includes feature extraction, vocabulary learning,
vectorization, and image clustering -- on a set of mobile phones. Our results
show a 75% improvement over the standard, full pipeline implementation running
on the phones without modification -- reducing the time to one minute under
certain conditions. We believe that this result is a promising indication that
fully distributed, infrastructure-less computing is possible on networks of
mobile phones; enabling a new class of mobile applications that are less
reliant on the cloud.",smart cities
http://arxiv.org/abs/1902.06824v2,to_check,arxiv,arxiv,2019-02-18 22:31:09+00:00,arxiv,"Autonomous Airline Revenue Management: A Deep Reinforcement Learning
  Approach to Seat Inventory Control and Overbooking",http://arxiv.org/abs/1902.06824v2,"Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue.",smart cities
10.1016/j.apenergy.2021.117853,to_check,Applied Energy,scopus,2022-01-01,sciencedirect,Transferable representation modelling for real-time energy management of the plug-in hybrid vehicle based on k-fold fuzzy learning and Gaussian process regression,https://api.elsevier.com/content/abstract/scopus_id/85114985028,"Electric vehicles, including plug-in hybrids, are important for achieving net-zero emission and will dominate road transportation in the future. Energy management, which optimizes the onboard energy usage, is a critical functionality of electric vehicles. It is usually developed following the model-based routine, which is conventionally costly and time-consuming and is hard to meet the increasing market competition in the digital era. To reduce the development workload for the energy management controller, this paper studies an innovative transfer learning routine. A new transferable representation control model is proposed by incorporating two promising artificial intelligence technologies, adaptive neural fuzzy inference system and Gaussian process regression, where the former applies k-fold cross valudation to build a neural fuzzy system for real-time implementation of offline optimization result, and the later connects the neural fuzzy system with a ‘deeper’ architecture to transfer the offline optimization knowledge learnt at source domain to new target domains. By introducing a concept of control utility that evaluates vehicle energy efficiency with a penalty on usage of battery energy, experimental evaluations based on the hardware-in-the-loop testing platform are conducted. Competitive real-time control ultility values (as much as 90% of offline benchmarking results) can be achieved by the proposed control method. They are over 27% higher than that achieved by the neural-network-based model.",smart cities
10.1016/j.eswa.2021.115080,to_check,Expert Systems with Applications,scopus,2021-10-15,sciencedirect,Spectral decision in cognitive radio networks based on deep learning,https://api.elsevier.com/content/abstract/scopus_id/85105355301,"Cognitive radio networks (CRN) have gained great relevance in the efficient use of the radio spectrum, and one of the key aspects of this technology is the spectral decision. The performance of secondary user communication depends largely on the intelligent choice of an appropriate spectral opportunity. The purpose of this research is to propose and assess the performance of a spectral decision model for CRN based on the Deep Learning technique. To achieve this, a classifier was adapted through the feature extraction technique that identifies three levels of traffic (high, medium and low) in a spectral occupation experimental power matrix that models the primary user. The extraction of features is done by Deep Learning and the process of classifying the successful set of features is done by a Support Vector Machine (SVM). These were used along with five evaluation metrics—total handoffs, failed handoffs, bandwidth, delay and throughput—to measure the performance of the proposed spectral decision model based on the Deep Learning technique, and to compare the results with the Multi-Criteria Optimization and Compromise Solution (VIKOR), Technique for Order Preference by Similarity to Ideal Solution (TOPSIS), and Simple Additive Weighting (SAW). This work presents five contributions: incorporation of the real behavior of licensed users, implementation of performance metrics for spectral mobility, proposal of an RGB conversion algorithm based on the threshold level, feedback in the classifier and a methodology based on priorities and scores to establish the channels with the highest availability. The results of this evaluation show that the proposed model has a better performance in the five metrics compared to the other techniques.",smart cities
10.1016/j.watres.2021.117012,to_check,Water Research,scopus,2021-05-15,sciencedirect,Gas-diffusion-electrode based direct electro-stripping system for gaseous ammonia recovery from livestock wastewater,https://api.elsevier.com/content/abstract/scopus_id/85102581788,"Livestock wastewater (LW) typically contains a substantial amount of NH4
                     + that can potentially be recovered and used in fertilizers or chemicals. In an attempt to recover NH4
                     + from LW, a novel electrochemical approach using a gas diffusion electrode (GDE) was developed and its efficacy was demonstrated in this study. The GDE-based electrochemical device, when operated at an air-flow rate of 20 mL/min, was free of back-diffusion flux, which is a fatal drawback of any membrane-based NH4
                     + separation approach. Continuous operation resulted in a nitrogen flux of 890 g N/m2d with synthetic LW and 770 g N/m2d with real LW at a current density of 10 mA/cm2. The electrochemical energy input was 7.42 kWh/kg N with synthetic LW and 9.44 kWh/kg N with real LW. Compared with the traditional stripping method, the GDE-based electrochemical system has a certain potential to be competitive, in terms of energy consumption. For instance, a rough-cost estimate based only on operating costs regarding chemical usage, air blowing, and water pumping revealed that the system consumed 13.44 kWh/kg N, whereas the conventional stripper required 27.6 kWh/kg N. This analysis showed that an electrochemical approach such as our GDE-based method can recover NH3, (particularly in gaseous form) from LW. In addition, with the future development of a smart operation method, as proposed and demonstrated in this study, the cost-effective implementation of a GDE-based method is feasible.",smart cities
10.1016/j.bspc.2019.101701,to_check,Biomedical Signal Processing and Control,scopus,2020-02-01,sciencedirect,Unsupervised automatic online spike sorting using reward-based online clustering,https://api.elsevier.com/content/abstract/scopus_id/85073983688,"Brain-machine interfaces (BMIs) can enable paralyzed people to regain mobility. In these interfaces, some different type of signals can be obtained from the brain, one of which is the action potential waveform (spike). In the case of using spikes, sorting the recorded signals and isolating the effects of the individual neurons can lead to a greater efficiency. Also, because of the nature of BMIs, real-time spike sorting is necessary. In many spike sorting approaches, the main outline consists of the following steps: spike detection, feature extraction, and clustering. In this study, a novel method for clustering is presented. This method is referred to as Reward-Based Online Clustering (RBOC) which is formed based on the reinforcement learning algorithm. The significant property of this proposed technique is its capability for real-time implementation that is required by BMIs. This method can automatically detect the clusters while there is no knowledge about the number of clusters. The performance of the proposed method is demonstrated through both simulation and experimental study. Evaluation with artificially simulated (ground truth) data shows that, on average, the accuracy of categorizing the spikes from the same origins is above 94 percent. Moreover, implementation of the method on the experimental data obtained from the rat brain represents convincing sorting results. It is noteworthy to say that, in most cases, this new method outperforms the results of similar previous works.",smart cities
10.1016/j.ast.2019.04.048,to_check,Aerospace Science and Technology,scopus,2019-07-01,sciencedirect,Real time estimation of impaired aircraft flight envelope using feedforward neural networks,https://api.elsevier.com/content/abstract/scopus_id/85065791672,"Extensive research in recent years has focused on developing flight envelope estimation methods to improve current loss of control prevention and recovery systems. Such methods are practically efficient only if they are able to evaluate in real time the new flight envelope of damaged aircraft based on the altered dynamics. Due to nonlinear dynamics of aircraft, common approaches to estimate the entire flight envelope of high-fidelity models are numerically intensive and their real time implementation is computationally impossible. So current methods are based on reduced complexity models or flight envelopes are determined locally. This paper presents a novel method to estimate the global flight envelope of impaired aircraft in real-time for any unknown failure degree. In the proposed method, first, numerous flight envelopes are evaluated using a high fidelity model at various failure degrees and different flight conditions and prepared as training data. Then multiple feedforward neural networks are trained offline by a Bayesian regularization backpropagation algorithm. Finally, the trained networks are used to estimate flight envelopes in real time. The method is applied to rudder and aileron failure cases of the NASA Generic Transport Model. Results show that the estimated flight envelopes are good approximations of the high fidelity global flight envelopes.",smart cities
10.1016/j.ins.2018.11.028,to_check,Information Sciences,scopus,2019-04-01,sciencedirect,Machine learning based privacy-preserving fair data trading in big data market,https://api.elsevier.com/content/abstract/scopus_id/85056879362,"In the era of big data, the produced and collected data explode due to the emerging technologies and applications that pervade everywhere in our daily lives, including internet of things applications such as smart home, smart city, smart grid, e-commerce applications and social network. Big data market can carry out efficient data trading, which provides a way to share data and further enhances the utility of data. However, to realize effective data trading in big data market, several challenges need to be resolved. The first one is to verify the data availability for a data consumer. The second is privacy of a data provider who is unwilling to reveal his real identity to the data consumer. The third is the payment fairness between a data provider and a data consumer with atomic exchange. In this paper, we address these challenges by proposing a new blockchain-based fair data trading protocol in big data market. The proposed protocol integrates ring signature, double-authentication-preventing signature and similarity learning to guarantee the availability of trading data, privacy of data providers and fairness between data providers and data consumers. We show the proposed protocol achieves the desirable security properties that a secure data trading protocol should have. The implementation results with Solidity smart contract demonstrate the validity of the proposed blockchain-based fair data trading protocol.",smart cities
10.1016/j.matdes.2018.107577,to_check,Materials and Design,scopus,2019-03-05,sciencedirect,Ensemble Kalman filter-based data assimilation for three-dimensional multi-phase-field model: Estimation of anisotropic grain boundary properties,https://api.elsevier.com/content/abstract/scopus_id/85059744257,"Data assimilation (DA) has been used as a machine learning approach to estimate a system's state and the unknown parameters in its numerical model by integrating observed data into model predictions. In this paper, we propose using the DA methodology based on the ensemble Kalman filter (EnKF) to improve the accuracy of microstructure prediction using three-dimensional multi-phase-field (3D-MPF) model and estimate the model parameters simultaneously. To demonstrate the applicability of the DA methodology, we performed numerical experiments in which a priori assumed true parameters related to the grain boundary (GB) energy cusp and GB mobility peak of Σ7 coincidence site lattice GB were estimated from synthetic data of time-evolving polycrystalline microstructure. Four model parameters related to the Σ7 GB properties were successfully estimated by assimilating the synthetic microstructure data to the 3D-MPF model predictions using the EnKF-based DA method. Furthermore, we accurately reproduced the preliminarily assumed true shapes of GB energy cusp and GB mobility peak by using the estimated parameters. The results suggest that implementation of the EnKF-based DA method in the MPF model has great potential for identifying unknown material properties and estimating unmeasurable microstructure evolutions in polycrystalline materials based on real time-series 3D microstructure observation data.",smart cities
10.1016/j.compag.2018.09.037,to_check,Computers and Electronics in Agriculture,scopus,2018-11-01,sciencedirect,A decision support tool to enhance agricultural growth in the Mékrou river basin (West Africa),https://api.elsevier.com/content/abstract/scopus_id/85054181612,"We describe in this paper the implementation of E-Water, an open software Decision Support System (DSS), designed to help local managers assess the Water Energy Food Environment (WEFE) nexus. E-Water aims at providing optimal management solutions to enhance food crop production at river basin level. The DSS was applied in the transboundary Mékrou river basin, shared among Benin, Burkina Faso and Niger. The primary sector for local economy in the region is agriculture, contributing significantly to income generation and job creation. Fostering the productivity of regional agricultural requires the intensification of farming practices, promoting additional inputs (mainly nutrient fertilizers and water irrigation) but, also, a more efficient allocation of cropland.
                  In order to cope with the heterogeneity of data, and the analyses and issues required by the WEFE nexus approach, our DSS integrates the following modules: (1) the EPIC biophysical agricultural model; (2) a simplified regression metamodel, linking crop production with external inputs; (3) a linear programming and a multiobjective genetic algorithm optimization routines for finding efficient agricultural strategies; and (4) a user-friendly interface for input/output analysis and visualization.
                  To test the main features of the DSS, we apply it to various real and hypothetical scenarios in the Mékrou river basin. The results obtained show how food unavailability due to insufficient local production could be reduced by, approximately, one third by enhancing the application and optimal distribution of fertilizers and irrigation. That would also affect the total income of the farming sector, eventually doubling it in the best case scenario. Furthermore, the combination of optimal agricultural strategies and modified optimal cropland allocation across the basin would bring additional moderate increases in food self-sufficiency, and more substantial gains in the total agricultural income.
                  The proposed software framework proves to be effective, enabling decision makers to identify efficient and site-specific agronomic management strategies for nutrients and water. Such practices would augment crop productivity, which, in turn, would allow to cope with increasing future food demands, and find a balanced use of natural resources, also taking other economic sectors—like livestock, urban or energy—into account.",smart cities
10.1016/j.procs.2018.07.138,to_check,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Building An Anomaly Detection Engine (ADE) for IoT Smart Applications,https://api.elsevier.com/content/abstract/scopus_id/85051386119,"Data Analytics is by far the component with more added value in Internet of Things (IoT) networks. One aspect of data analytics is anomaly detection within data points received in some cases in real time that help to conduct predictive maintenance, weather monitoring or cyber security forensics for instance. Although there exists a number of web dashboards that allow IoT users to visualize data in time domain and perform statistical analysis, anomaly detection is often absent else if present not that straightforward, reliable and accurate. The development and implementation of Anomaly Detection Engine (ADE) poses a number of challenges that are in fact addressed in this paper. The research work exposes the multifaceted aspect of IoT networks and applications based on real life use cases and the difficulties engendered in mounting an ADE from both software system engineering and network convergence perspectives. Moreover a comparative description of diverse time series models adopted in anomaly detection is undertaken. It was noticed that there is neither one size fit all solution nor a plug n play alternative and that the unsupervised mode in machine learning as a model for time series analysis is the most versatile and efficient technique for IoT analytics developers.",smart cities
10.1016/j.egypro.2017.03.271,to_check,Energy Procedia,scopus,2017-03-01,sciencedirect,Predicting Large Scale Fine Grain Energy Consumption,https://api.elsevier.com/content/abstract/scopus_id/85017254936,"Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window.",smart cities
10.1016/S2046-0430(16)30168-X,to_check,International Journal of Transportation Science and Technology,scopus,2015-01-01,sciencedirect,Dynamic Travel Time Prediction Models for Buses Using Only GPS Data,https://api.elsevier.com/content/abstract/scopus_id/85039937386,"Providing real-time and accurate travel time information of transit vehicles can be very helpful as it assists passengers in planning their trips to minimize waiting times. The purpose of this research is to develop and compare dynamic travel time prediction models which can provide accurate prediction of bus travel time in order to give real-time information at a given downstream bus stop using only global positioning system (GPS) data. Historical Average (HA), Kalman Filtering (KF) and Artificial Neural Network (ANN) models are considered and developed in this paper. A case has been studied by making use of the three models. Promising results are obtained from the case study, indicating that the models can be used to implement an Advanced Public Transport System. The implementation of this system could assist transit operators in improving the reliability of bus services, thus attracting more travelers to transit vehicles and helping relieve congestion. The performances of the three models were assessed and compared with each other under two criteria: overall prediction accuracy and robustness. It was shown that the ANN outperformed the other two models in both aspects. In conclusion, it is shown that bus travel time information can be reasonably provided using only arrival and departure time information at stops even in the absence of traffic-stream data.",smart cities
10.1016/j.procs.2014.05.151,to_check,Procedia Computer Science,scopus,2014-01-01,sciencedirect,Evaluation of in-vehicle decision support system for emergency evacuation,https://api.elsevier.com/content/abstract/scopus_id/84902825749,"One of the most important issues in Decision Support Systems (DSS) technology is in ensuring their effectiveness and efficiency for future implementations and use. DSS is prominent tool in disaster information system, which allows the authority to provide life safety information directly to the mobile devices of anyone physically located in the evacuation area. After that a personal DSS guides users to a safe point. Due to the large uncertainty in initial conditions and assumptions on underlying process the implementation and evaluation of such DSS are extremely hard, particularly in real environment. We propose a simulation methodology for the evaluation of in-vehicle DSS for emergency evacuation based on transport system and human decision-making modeling.",smart cities
10.1016/j.jhydrol.2013.02.022,to_check,Journal of Hydrology,scopus,2013-01-01,sciencedirect,Runoff forecasting using a Takagi-Sugeno neuro-fuzzy model with online learning,https://api.elsevier.com/content/abstract/scopus_id/84886101097,"A study using local learning Neuro-Fuzzy System (NFS) was undertaken for a rainfall–runoff modeling application. The local learning model was first tested on three different catchments: an outdoor experimental catchment measuring 25m2 (Catchment 1), a small urban catchment 5.6km2 in size (Catchment 2), and a large rural watershed with area of 241.3km2 (Catchment 3). The results obtained from the local learning model were comparable or better than results obtained from physically-based, i.e. Kinematic Wave Model (KWM), Storm Water Management Model (SWMM), and Hydrologiska Byråns Vattenbalansavdelning (HBV) model. The local learning algorithm also required a shorter training time compared to a global learning NFS model. The local learning model was next tested in real-time mode, where the model was continuously adapted when presented with current information in real time. The real-time implementation of the local learning model gave better results, without the need for retraining, when compared to a batch NFS model, where it was found that the batch model had to be retrained periodically in order to achieve similar results.",smart cities
10.1016/j.neunet.2012.02.036,to_check,Neural Networks,scopus,2012-08-01,sciencedirect,Metamodeling and the Critic-based approach to multi-level optimization,https://api.elsevier.com/content/abstract/scopus_id/84861762153,"Large-scale networks with hundreds of thousands of variables and constraints are becoming more and more common in logistics, communications, and distribution domains. Traditionally, the utility functions defined on such networks are optimized using some variation of Linear Programming, such as Mixed Integer Programming (MIP). Despite enormous progress both in hardware (multiprocessor systems and specialized processors) and software (Gurobi) we are reaching the limits of what these tools can handle in real time. Modern logistic problems, for example, call for expanding the problem both vertically (from one day up to several days) and horizontally (combining separate solution stages into an integrated model). The complexity of such integrated models calls for alternative methods of solution, such as Approximate Dynamic Programming (ADP), which provide a further increase in the performance necessary for the daily operation. In this paper, we present the theoretical basis and related experiments for solving the multistage decision problems based on the results obtained for shorter periods, as building blocks for the models and the solution, via Critic-Model-Action cycles, where various types of neural networks are combined with traditional MIP models in a unified optimization system. In this system architecture, fast and simple feed-forward networks are trained to reasonably initialize more complicated recurrent networks, which serve as approximators of the value function (Critic). The combination of interrelated neural networks and optimization modules allows for multiple queries for the same system, providing flexibility and optimizing performance for large-scale real-life problems. A MATLAB implementation of our solution procedure for a realistic set of data and constraints shows promising results, compared to the iterative MIP approach.",smart cities
10.1016/j.eswa.2009.01.049,to_check,Expert Systems with Applications,scopus,2009-09-01,sciencedirect,MamMoeT: An intelligent agent-based communication support platform for multimodal transport,https://api.elsevier.com/content/abstract/scopus_id/67349254167,"In this paper, an intelligent agent-based communication support platform for multimodal transport is developed. The rationale for doing so is found in the potential of such a system to increase cost efficiency, service and safety for different transport-related actors. Although, at present several comparable systems exist, their current implementation is far from successful because of technological and economic obstacles. The new expert communication platform put forward here (called MamMoeT) addresses these two issues by using a software agent-based approach. Software agents are pieces of software representing a single user. They are autonomous, communicative and intelligent. The MamMoeT system developed can be described as a real-time decision support system in which intelligent software agents handle communicative tasks, exchange desired amounts of information among different users using common exchange protocols which act as translators between different systems.",smart cities
10.1016/j.advwatres.2009.01.001,to_check,Advances in Water Resources,scopus,2009-04-01,sciencedirect,Pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models,https://api.elsevier.com/content/abstract/scopus_id/62349136438,"Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",smart cities
10.1016/j.annemergmed.2004.02.037,to_check,Annals of Emergency Medicine,scopus,2004-09-01,sciencedirect,Effects of neural network feedback to physicians on admit/discharge decision for emergency department patients with chest pain,https://api.elsevier.com/content/abstract/scopus_id/4344692494,"Study objective
                  Neural networks can risk-stratify emergency department (ED) patients with potential acute coronary syndromes with a high specificity, potentially facilitating ED discharge of patients to home. We hypothesized that the use of “real-time” neural networks would decrease the admission rate for ED chest pain patients.
               
                  Methods
                  We conducted a before-and-after trial. Consecutive ED patients with chest pain were evaluated before and after implementation of a neural network in an urban university ED. Data included 40 variables used in neural networks for acute myocardial infarction and acute coronary syndrome. Data were obtained in real time, and neural network outputs were provided to the treating physician while patients were in the ED. On hospital discharge, attending physicians received feedback, including neural network output, their initial clinical impression, cardiac test results, and final diagnosis. The main outcome was the actual admit/discharge decision made before versus after the implementation of the neural network.
               
                  Results
                  Before implementation, 4,492 patients were enrolled; after implementation, 432 patients were enrolled. Implementation of the neural network did not decrease the hospital admission rate (before: 62.7% [95% confidence interval (CI) 61.3% to 64.1%] versus after: 66.6% [95% CI 62.2% to 71.0%]). Additionally, the ICU admission rates were not different (11.4% [95% CI 10.5% to 12.3%] versus 9.3% [95% CI 6.6% to 12.0%]). Physician query found that the neural network changed management in only 2 cases (<1%).
               
                  Conclusion
                  The use of real-time neural network feedback did not influence the admission decision for ED patients with chest pain, most likely because the neural network output was delayed until the return of cardiac markers, and the disposition decision had already been made by that time.",smart cities
10.1016/0165-0114(94)00293-G,to_check,Fuzzy Sets and Systems,scopus,1995-06-23,sciencedirect,Hierarchical fuzzy control of multivariable systems,https://api.elsevier.com/content/abstract/scopus_id/0029323823,The paper considers the problem of hierarchical fuzzy control of multivariable systems. Some terms and definitions with regard to this problem are given. A method of hierarchical control is proposed in which the control actions have two components: local and global. The method is used for simulating the behaviour of an urban traffic network and the results are analysed. It is shown that the number of fuzzy relations is significantly reduced and thus real time control implementation is facilitated.,smart cities
10.1109/ICTTA.2008.4529967,to_check,2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications,IEEE,2008-04-11 00:00:00,ieeexplore,Multimedia Learning in Advanced Computer-based Contexts: `Discovering Trier',https://ieeexplore.ieee.org/document/4529967/,"Edutainment is a neologism that expresses the marriage of education and entertainment. In particular edutainment is a form of entertainment designed to educate as well as to amuse and typically seeks to instruct or socialize its audience by embedding lessons in some familiar form of entertainment: television programs, computer and video games, films, music, websites or multimedia software. On the other hand, the design and the implementation of not boring or repetitive edutainment are not trivial or easy tasks. In this paper we propose a new approach for the design of an edutainment. The proposed environment ""translates"" the storyboards of the games in a Dynamic Bayesian Networks that are the extension of Bayesian Networks for modelling times-series data. The Bayesian approach allows a dynamic adaptation of the game to the user's profile and the establishment of several paths in the same game. We furnish some results obtained by the use of a first prototype of our tool in real academic courses.",multimedia
10.1109/SSD.2019.8893218,to_check,"2019 16th International Multi-Conference on Systems, Signals & Devices (SSD)",IEEE,2019-03-24 00:00:00,ieeexplore,Parallel implementation of HEVC encoder on multicore ARM-based platform,https://ieeexplore.ieee.org/document/8893218/,"High Efficiency Video Coding (HEVC) is the new emerging standard released as a successor to H.264/AVC. It aims to improve the encoding performance by saving 50% of the bitrate with the same visual quality. This encoding performance makes it more suitable for high definition video applications and could be the next embedded video codec on the majority of multimedia devices. However, this performance is coupled with tremendous computational complexity which makes it very hard to achieve a real-time video encoding with classic embedded processor architectures. Consequently, multicore technology of programmable processors offers a very promising solution to overcome this complexity. In this context, this paper presents a parallel implementation of the HEVC encoder All-Intra (AI) configuration on Quad-core ARM-based platform running at 1.7GHz. OpenMP is used as parallel programming paradigm exploiting the Frame Level Parallelism technique. Experimental results show that parallel processing using four threads allows saving up to 73% of encoding time and speeds up the encoding process by a factor of 3.77 without any rate distortion in terms of video quality or bitrate.",multimedia
10.1109/ISCAS.2003.1205853,to_check,2003 IEEE International Symposium on Circuits and Systems (ISCAS),IEEE,2003-05-28 00:00:00,ieeexplore,Proceedings of the 2003 IEEE International Symposium on Circuits and Systems (Cat. No.03CH37430),https://ieeexplore.ieee.org/document/1205853/,"The following topics are dealt with: analog circuits and signal processing; amplifiers; analog filters; low-power analog circuits; oscillators and reference circuits; mixing and synthesis; signal conversion; communications; multimedia systems and applications; video transmission; image coding and processing; speech processing and coding; video coding and compression; VLSI architectures; watermarking and encryption; general and nonlinear circuits and systems; blind signal processing; chaotic circuits and systems; graph models and algorithms; power electronics; switching power converters and recurrent neural networks; cellular and high-speed circuits and systems; IC interfaces for sensors/MEMS; variable digital filters; cellular neural networks; EDGE; piecewise linear circuits and systems; biomedical devices, sensors, and related systems; multidimensional circuits and systems; speech and language processing; nonlinear dynamics for coding theory and network traffic; sensor arrays for visual tracking and navigation; security and data hiding; bionics; fault detection and tolerance in distributed networks; frequency response masking techniques; behavioral modeling and simulation of mixed-signal systems; MEMS and applications; digital signal processing; computer aided network design; digital filters; wavelets and multirate signal processing; adaptive signal processing; multidimensional DSP systems; parallel and real-time signal processing; modeling algorithms; analog modeling and simulation; CAD algorithms; biomedical circuits and systems; VLSI systems and applications; neural networks and systems; implantable electronics; memory circuits; VLSI design, Testing and implementation; fault detection and fault tolerance; fuzzy logic circuits; neural learning and intelligent systems.",multimedia
10.1109/CVPR.2017.602,to_check,2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2017-07-26 00:00:00,ieeexplore,3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation from Single Depth Images,https://ieeexplore.ieee.org/document/8100085/,"We propose a simple, yet effective approach for real-time hand pose estimation from single depth images using three-dimensional Convolutional Neural Networks (3D CNNs). Image based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN taking a 3D volumetric representation of the hand depth image as input can capture the 3D spatial structure of the input and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. Experiments show that our proposed 3D CNN based approach outperforms state-of-the-art methods on two challenging hand pose datasets, and is very efficient as our implementation runs at over 215 fps on a standard computer with a single GPU.",multimedia
10.1109/NANO.2013.6720887,to_check,2013 13th IEEE International Conference on Nanotechnology (IEEE-NANO 2013),IEEE,2013-08-08 00:00:00,ieeexplore,A high-throughput and low-cost 3D imaging system for flowing Escherichia coli,https://ieeexplore.ieee.org/document/6720887/,"In this paper, we present a novel design of low-cost, high-throughput hologram-based 3D imaging system towards the development of a detection system for Escherichia coli (E. coli). This focusing-free system can image food samples flowing in a transparent microchannel under illumination of RGB lasers through a pinhole, and yields their 3D structures in real-time using hologram-based reconstruction algorithms. We solve two key technical problems in order to realize the system: 1) implementation of a high-speed (200fps) imaging system for capturing full-frame holograms of cells flowing in microchannels; 2) reconstruction of 3D structure of E. coli by characterizing the holograms under low-cost incoherent light sources such as the incoherent laser diodes. Market-dominating food pathogen detection systems using the polymerase chain reaction (PCR) are very slow (10-20 hours/ml) in addition to their high costs. Moreover, they cannot detect the viable but non-culturable (VBNC) E. coli, which orchestrated the recent massive and deadly food poisoning outbreaks in Europe and North America. The estimated cost of the proposed system is less than US$11,000 and its speed is over 1,000 times faster than the existing methods.",multimedia
10.1109/CVPR42600.2020.00608,to_check,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2020-06-19 00:00:00,ieeexplore,Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled Representation,https://ieeexplore.ieee.org/document/9157707/,"We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance.",multimedia
10.1109/AIHAS.1993.410573,to_check,"1993 4th Annual Conference on AI, Simulation and Planning in High Autonomy Systems",IEEE,1993-09-22 00:00:00,ieeexplore,NPSNET: JANUS-3D providing three-dimensional displays for a two-dimensional combat model,https://ieeexplore.ieee.org/document/410573/,"The integration of the US Army's existing combat modeling tool, JANUS(A), with the real-time three-dimensional graphics display offered by NPSNET is discussed. A scripting tool capable of rendering JANUS(A) scenarios previously executed in the traditional two-dimensional model is discussed. This replay capability allows the gamer/analyst the ability to watch the three-dimensional battle unfold from any position on the battlefield. Also, the implementation of a real-time, networked link from the two-dimensional JANUS model to NPSNET is detailed. This link involves an Ethernet connection from a Sun workstation, which houses the two-dimensional model, to a Silicon Graphics workstation used for rendering the real-time three-dimensional simulation.&lt;<ETX>&gt;</ETX>",multimedia
10.1109/SECON.2017.7925321,to_check,SoutheastCon 2017,IEEE,2017-04-02 00:00:00,ieeexplore,Towards real-time segmentation of 3D point cloud data into local planar regions,https://ieeexplore.ieee.org/document/7925321/,"This article describes an algorithm for efficient segmentation of point cloud data into local planar surface regions. This is a problem of generic interest to researchers in the computer graphics, computer vision, artificial intelligence and robotics community where it plays an important role in applications such as object recognition, mapping, navigation and conversion from point clouds representations to 3D surface models. Prior work on the subject is either computationally burdensome, precluding real time applications such as robotic navigation and mapping, prone to error for noisy measurements commonly found at long range or requires availability of coregistered color imagery. The approach we describe consists of 3 steps: (1) detect a set of candidate planar surfaces, (2) cluster the planar surfaces merging redundant plane models, and (3) segment the point clouds by imposing a Markov Random Field (MRF) on the data and planar models and computing the Maximum A-Posteriori (MAP) of the segmentation labels using Bayesian Belief Propagation (BBP). In contrast to prior work which relies on color information for geometric segmentation, our implementation performs detection, clustering and estimation using only geometric data. Novelty is found in the fast clustering technique and new MRF clique potentials that are heretofore unexplored in the literature. The clustering procedure removes redundant detections of planes in the scene prior to segmentation using BBP optimization of the MRF to improve performance. The MRF clique potentials dynamically change to encourage distinct labels across depth discontinuities. These modifications provide improved segmentations for geometry-only depth images while simultaneously controlling the computational cost. Algorithm parameters are tunable to enable researchers to strike a compromise between segmentation detail and computational performance. Experimental results apply the algorithm to depth images from the NYU depth dataset which indicate that the algorithm can accurately extract large planar surfaces from depth sensor data.",multimedia
10.1109/TC.2016.2621758,to_check,IEEE Transactions on Computers,IEEE,2017-05-01 00:00:00,ieeexplore,Improving 3D DRAM Fault Tolerance Through Weak Cell Aware Error Correction,https://ieeexplore.ieee.org/document/7707388/,"Although the emerging 3D DRAM products can significantly improve the computing system performance, the relatively high cost is one of the most critical issues that prevent their wide real-life adoption. Intuitively, a strong memory fault tolerance can be leveraged to reduce the fabrication cost of DRAM dies, and the total cost will reduce if the fabrication cost saving can off-set the cost overhead of memory fault tolerance. Nevertheless, such a simple concept can be a practically viable option only for 3D DRAM because: (1) The stacked logic die can solely implement memory fault tolerance inside 3D DRAM chips, obviating any changes on the host CPUs and CPU-DRAM interfaces. (2) With the total ownership of both the logic die and DRAM dies inside 3D DRAM chips, DRAM manufacturers can fully exploit the potential to truly minimize the 3D DRAM bit cost. Following this intuition, we developed a 3D DRAM fault tolerance design strategy. It can achieve a very strong tolerance to weak DRAM cells at very small redundancy and latency overhead. The key is to cohesively leverage the detectability of weak cells and runtime configurability of error correction code (ECC) decoding. In addition, this design strategy can gracefully embrace the inaccuracy of weak cell detection (e.g., weak cell miss-detection and false-detection). We carried out thorough mathematical analysis, and the results show that, under the redundancy overhead of 1:8 (same as today's ECC DIMM), this design strategy can tolerate the weak cell rate of as high as 10-4 and 6x10-5 if 100 and 90 percent of all the weak cells are known in prior. Using Micron's hybrid memory cube (HMC) 3D DRAM chips as the test vehicle, we evaluated the implementation cost and the results show that it only consumes less than 0.4 mm2 (45 nm node) on the logic die. Using CPU and DRAM simulators, we further carried out simulations over a variety of computing benchmarks and the results show that this design solution only incurs less than 2 percent performance degradation on average.",multimedia
10.1109/TPDS.2018.2858230,to_check,IEEE Transactions on Parallel and Distributed Systems,IEEE,2019-01-01 00:00:00,ieeexplore,Parana: A Parallel Neural Architecture Considering Thermal Problem of 3D Stacked Memory,https://ieeexplore.ieee.org/document/8416708/,"Recent advances in deep learning (DL) have stimulated increasing interests in neural networks (NN). From the perspective of operation type and network architecture, deep neural networks can be categorized into full convolution-based neural network (ConvNet), recurrent neural network (RNN), and fully-connected neural network (FCNet). Different types of neural networks are usually cascaded and combined as a hybrid neural network (Hybrid-NN) to complete real-life cognitive tasks. Such hybrid-NN implementation is memory-intensive with large number of memory accesses, hence the performance of hybrid-NN is often limited by the insufficient memory bandwidth. A “3D + 2.5D” integration system, which integrates a high-bandwidth 3D stacked DRAM side-by-side with a highly-parallel neural processing unit (NPU) on a silicon interposer, overcomes the bandwidth bottleneck in hybrid-NN acceleration. However, intensive concurrent 3D DRAM accesses produced by the NPU lead to a serious thermal problem in 3D DRAM. In this paper, we propose a neural processor called <italic>Parana</italic> for hybrid-NN acceleration in consideration of thermal problem of 3D DRAM. Parana solves the thermal problem of 3D memory by optimizing both the total number of memory accesses and memory accessing behaviors. For memory accessing behaviors, Parana balances the memory bandwidth by spatial division mapping hybrid-NN onto computing resources, which efficiently avoids that masses of memory accesses are issued in a short time period. To reduce the total number of memory accesses, we design a new NPU architecture and propose a memory-oriented tiling and scheduling mechanism to exploit the maximum utilization of on-chip buffer. Experimental results show that Parana reduces the peak temperature by up to 54.72 <inline-formula> <tex-math notation=""LaTeX"">$^\circ$</tex-math><alternatives><inline-graphic xlink:href=""yin-ieq1-2858230.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/> </alternatives></inline-formula>C and the steady temperature by up to 32.27 <inline-formula><tex-math notation=""LaTeX""> $^\circ$</tex-math><alternatives><inline-graphic xlink:href=""yin-ieq2-2858230.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>C over state-of-the-art accelerators with 3D memory without performance degradation.",multimedia
10.1109/TPAMI.2018.2827052,to_check,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2019-04-01 00:00:00,ieeexplore,Real-Time 3D Hand Pose Estimation with 3D Convolutional Neural Networks,https://ieeexplore.ieee.org/document/8338122/,"In this paper, we present a novel method for real-time 3D hand pose estimation from single depth images using 3D Convolutional Neural Networks (CNNs). Image-based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN-based method, taking a 3D volumetric representation of the hand depth image as input and extracting 3D features from the volumetric input, can capture the 3D spatial structure of the hand and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. To further improve the estimation accuracy, we propose applying the 3D deep network architectures and leveraging the complete hand surface as intermediate supervision for learning 3D hand pose from depth images. Extensive experiments on three challenging datasets demonstrate that our proposed approach outperforms baselines and state-of-the-art methods. A cross-dataset experiment also shows that our method has good generalization ability. Furthermore, our method is fast as our implementation runs at over 91 frames per second on a standard computer with a single GPU.",multimedia
10.1109/VRAIS.1995.512492,to_check,Proceedings Virtual Reality Annual International Symposium '95,IEEE,1995-03-15 00:00:00,ieeexplore,A distributed virtual environment for concurrent engineering,https://ieeexplore.ieee.org/document/512492/,This paper presents a distributed virtual environment that supports collaboration among members of a geographically dispersed multidisciplinary team engaged in concurrent product development. The distributed virtual environment maintains a shared information space that contains product data in a standard ISO STEP compliant format. It supports a user configurable virtual environment and the integration of different CAE applications to support different engineering perspectives. The realistic manipulation of assembly models within the distributed virtual environment is supported by constraint-based 3D manipulation techniques developed at Leeds. The initial implementation of this architecture supports accurate assembly modelling and kinematic simulation for virtual prototypes and runs on a network of SGI Indy workstations over an ATM network.,multimedia
10.1109/ROMAN.1995.531967,to_check,Proceedings 4th IEEE International Workshop on Robot and Human Communication,IEEE,1995-07-07 00:00:00,ieeexplore,A study of real time facial expression detection for virtual space teleconferencing,https://ieeexplore.ieee.org/document/531967/,"A new method for real-time detection of facial expressions from time-sequential images is proposed. The proposed method does not need the tape marks that were pasted to the face for detecting expressions in real-time in the current implementation for the virtual space teleconferencing. In the proposed method, four windows are applied to the four areas in the face image: the left and right eyes, mouth and forehead. Each window is divided into blocks that consist of 8 by 8 pixels. The discrete cosine transform (DCT) is applied to each block, and the feature vector of each window is obtained from taking the summations of the DCT energies in the horizontal, vertical and diagonal directions. By a conversion table, the feature vectors are related to real 3D movements in the face. Experiment show some promising results for accurate expression detection and for the realization of real-time hardware implementation of the proposed method.",multimedia
10.1109/ISCAS.2019.8702353,to_check,2019 IEEE International Symposium on Circuits and Systems (ISCAS),IEEE,2019-05-29 00:00:00,ieeexplore,AR-C3D: Action Recognition Accelerator for Human-Computer Interaction on FPGA,https://ieeexplore.ieee.org/document/8702353/,"In recent years, action recognition has been widely explored and attains significant performance improvement. In this paper, we propose a real-time action recognition specified convolutional 3D (AR-C3D) neural network for human-computer interaction. The CNN structure is optimized to decrease the complexity. Furthermore, Winograd algorithm is adopted to accelerate computation. It achieves 89.9% accuracy in the application which refers to the robot classifies the video captured by itself and would either imitate human's action or give verbal feedback. The Artix-7 FPGA implementation result outperforms previous work in terms of resource utilization and no external storage is consumed. One video can be processed in 6.6ms, and the power consumption is only 2.7W.",multimedia
10.1109/SIBCON50419.2021.9438884,to_check,2021 International Siberian Conference on Control and Communications (SIBCON),IEEE,2021-05-15 00:00:00,ieeexplore,Assessment of Map Construction in vSLAM,https://ieeexplore.ieee.org/document/9438884/,"Vision-based Simultaneous Localization and Mapping (vSLAM) is a challenging task in modern computer vision. vSLAM is particularly important as mobile robotics application. It allows to localize the robot and build the map of unknown environment in 3D in real-time. During research and development of new methods, it needs extensive evaluation on trajectory and map quality compared to known methods. In this work we focus on map quality estimation. We develop the simulated ground-truth data in photo-realistic environment and introduce new metrics in order to estimate map quality. We evaluate neural network based vSLAM methods with our framework in order to show that it fits map quality estimation more than standard approaches. Open-source implementation of our map metrics is available at https://github.com/CnnDepth/slam_comparison.",multimedia
10.1109/MECO.2019.8760001,to_check,2019 8th Mediterranean Conference on Embedded Computing (MECO),IEEE,2019-06-14 00:00:00,ieeexplore,Fiber-based Photonic-FPGA Architecture and In-Fiber Computing,https://ieeexplore.ieee.org/document/8760001/,"Hardware implementation of artificial neural networks facilitates real-time parallel processing of massive data sets. Optical neural networks offer low-volume 3D connectivity together with large bandwidth and minimal heat production in contrast to electronic implementation. In this presentation we will present a conceptual design for in-fiber optical neural network, i.e. a fiber-based realization of a photonic-FPGA. Neurons and synapses are realized in two ways: first as individual silica cores in a multi-core fiber and then within a multi-mode fiber. In the first realization optical signals are transferred transversely between cores by means of optical coupling. Pump driven amplification in erbium-doped cores mimics synaptic interactions. Simulations and experimental validation show classification and learning capabilities. Therefore, devices similar to our proposed multi-core fiber could potentially serve as building blocks for future large-scale small-volume optical artificial neural networks. In the second type of realization we propose the design of an optical artificial neural network-based imaging system that has the ability to self-study image signals from an incoherent light source in different colors. Our design consists of a multi-mode fiber realizing a stochastic neural network. We show that the signals, transmitted through the multi-mode fiber, can be used for image identification purposes and can also be reconstructed using artificial neural networks with a low number of nodes.",multimedia
10.1109/HUMANOIDS.2014.7041373,to_check,2014 IEEE-RAS International Conference on Humanoid Robots,IEEE,2014-11-20 00:00:00,ieeexplore,Footstep planning on uneven terrain with mixed-integer convex optimization,https://ieeexplore.ieee.org/document/7041373/,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",multimedia
10.1109/IEMBS.2006.259498,to_check,2006 International Conference of the IEEE Engineering in Medicine and Biology Society,IEEE,2006-09-03 00:00:00,ieeexplore,Hardware Implementation of Hierarchical Volume Subdivision-based Elastic Registration,https://ieeexplore.ieee.org/document/4462029/,"Real-time, elastic and fully automated 3D image registration is critical to the efficiency and effectiveness of many image-guided diagnostic and treatment procedures relying on multimodality image fusion or serial image comparison. True, real-time performance will make many 3D image registration-based techniques clinically viable. Hierarchical volume subdivision-based image registration techniques are inherently faster than most elastic registration techniques, e.g. free-form deformation (FFD)-based techniques, and are more amenable for achieving real-time performance through hardware acceleration. Our group has previously reported an FPGA-based architecture for accelerating FFD-based image registration. In this article we show how our existing architecture can be adapted to support hierarchical volume subdivision-based image registration. A proof-of-concept implementation of the architecture achieved speedups of 100 for elastic registration against an optimized software implementation on a 3.2 GHz Pentium III Xeon workstation. Due to inherent parallel nature of the hierarchical volume subdivision-based image registration techniques further speedup can be achieved by using several computing modules in parallel",multimedia
10.1109/IAEAC.2018.8577729,to_check,"2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",IEEE,2018-10-14 00:00:00,ieeexplore,Image segmentation based on Blob analysis and quad-tree algorithm,https://ieeexplore.ieee.org/document/8577729/,"Image segmentation is one of the most popular topic in recent research and studies, there are lots of different method to solve this problem. Some existing method works pretty well, and others not. This paper proposed to implement two improved existing method, Split and merge and Blob coloring algorithm, and compared their segmentation results in both 2D and 3D. Meanwhile, to clarify our achievements following scientific method, we have to establish our evaluation function FMI to give a score to tell what level of goodness our implementation could achieve. The first task is implementation of different region growing algorithms. We implement a general split and merge algorithm and the Blob Coloring algorithm that can work both in 2D and in 3D with different homogeneity criteria. For the split and merge algorithm, we will use quad-tree algorithm to split and merge our image based on the homogeneity. For the blob algorithm, we will check the L shape, then merge the pixels with similar homogeneity, and ignore those not. According to the experimental results, We found the split merge algorithm is generally better than the blob algorithm. Although for some case, the blob algorithm can also reach to more than 0.8 FMI score in evaluating, the overall performance is still bad. Split and merge algorithm works well on all the image cases. As we think, we decide to apply our implementation in some real images to achieve some goals. For example, we choose a group of interior design and a group of animals as 3D images. The result coming from our implementation could be used in biomedical field such as cancer detection, as well as in public management such as suspects outline detection. However, there are real problems for us to use region growing techniques to detect boundary of objects with some specific indexes because if the target has obvious difference inside, the algorithm will not treat it as a whole. However, from a visual point of view, we treat it as a complete object.",multimedia
10.1109/NSSMIC.2014.7430955,to_check,2014 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),IEEE,2014-11-15 00:00:00,ieeexplore,Implementation of Self-organizing Map Based Positioning Scheme on FPGA,https://ieeexplore.ieee.org/document/7430955/,"For continuous crystal based PET detector, we develop a Self-organizing Map (SOM) Neural Network Based Positioning Scheme which can achieve 2.07mm average resolution and is feasible for implementation on Field Programmable Gate Array (FPGA). In this paper, we propose the FPGA design of SOM scheme and apply it to our experiment data. Taking advantage of the pipelined and parallel structure, the implementation of this algorithm is able to process 5M events per second with system clock running at 200M Hz. The test results show that the FPGA solution has almost the equal performance with software platform. Considering the potentiality of DOI determination using SOM scheme, it is promising to realize real-time 3D position estimation in the future.",multimedia
10.1109/ROBOT.2010.5509682,to_check,2010 IEEE International Conference on Robotics and Automation,IEEE,2010-05-07 00:00:00,ieeexplore,Indoor scene recognition through object detection,https://ieeexplore.ieee.org/document/5509682/,"Scene recognition is a highly valuable perceptual ability for an indoor mobile robot, however, current approaches for scene recognition present a significant drop in performance for the case of indoor scenes. We believe that this can be explained by the high appearance variability of indoor environments. This stresses the need to include high-level semantic information in the recognition process. In this work we propose a new approach for indoor scene recognition based on a generative probabilistic hierarchical model that uses common objects as an intermediate semantic representation. Under this model, we use object classifiers to associate low-level visual features to objects, and at the same time, we use contextual relations to associate objects to scenes. As a further contribution, we improve the performance of current state-of-the-art category-level object classifiers by including geometrical information obtained from a 3D range sensor that facilitates the implementation of a focus of attention mechanism within a Monte Carlo sampling scheme. We test our approach using real data, showing significant advantages with respect to previous state-of-the-art methods.",multimedia
10.1109/FG47880.2020.00049,to_check,2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020),IEEE,2020-11-20 00:00:00,ieeexplore,ReenactNet: Real-time Full Head Reenactment,https://ieeexplore.ieee.org/document/9320178/,"Video-to-video synthesis is a challenging problem aiming at learning a translation function between a sequence of semantic maps and a photo-realistic video depicting the characteristics of a driving video. We propose a head-to-head system of our own implementation capable of fully transferring the human head 3D pose, facial expressions and eye gaze from a source to a target actor, while preserving the identity of the target actor. Our system produces high-fidelity, temporally-smooth and photo-realistic synthetic videos faithfully transferring the human time-varying head attributes from the source to the target actor. Our proposed implementation: 1) works in real time (~20 fps), 2) runs on a commodity laptop with a webcam as the only input, 3) is interactive, allowing the participant to drive a target person, e.g. a celebrity, politician, etc, instantly by varying their expressions, head pose, and eye gaze, and visualising the synthesised video concurrently.",multimedia
10.1109/ACCESS.2019.2945330,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,A Structural Design and Interaction Algorithm of Smart Microscope Embedded on Virtual and Real Fusion Technologies,https://ieeexplore.ieee.org/document/8856207/,"The microscope is an important teaching tool in secondary school biology. This important equipment needs to be understood and mastered through biological experiments. However, in actual experimental teaching in China, microscope samples are difficult to produce, preserve and share, and the implementation status is not ideal. These conditions seriously affect student enthusiasm and require additional participant labor. Moreover, they hinder teachers' evaluation of experimental learning. Thus, this paper constructs a smart microscope physical interaction kit for secondary school biology experiments. First, the main components of the traditional microscope are replaced by a variety of different sensing elements, which are 3D printed. Then, a novel multichannel information integration strategy based on visual, auditory and tactile information is proposed to manage different channels of information, to understand the user's operational intent and to organize reasonable interactions (including the display of real-time adjustment effects). Then, we propose a navigational interaction paradigm based on multimodal intent understanding, aiming at reminding users of invalid behavior and providing necessary operational guidance for users, thus achieving the purpose of intelligent interaction and intelligent experimental teaching. The experimental results show that the proposed microscope kit, multichannel integration strategy and navigation interaction algorithm not only imbue microscopy experiments with the characteristics of intelligent interaction but also stimulate student enthusiasm and help the evaluation of experimental learning. We find that these capabilities are well received by users.",multimedia
10.1109/TPAMI.2018.2863285,to_check,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2019-08-01 00:00:00,ieeexplore,Deep Supervision with Intermediate Concepts,https://ieeexplore.ieee.org/document/8434117/,"Recent data-driven approaches to scene interpretation predominantly pose inference as an end-to-end black-box mapping, commonly performed by a Convolutional Neural Network (CNN). However, decades of work on perceptual organization in both human and machine vision suggest that there are often intermediate representations that are intrinsic to an inference task, and which provide essential structure to improve generalization. In this work, we explore an approach for injecting prior domain structure into neural network training by supervising hidden layers of a CNN with intermediate concepts that normally are not observed in practice. We formulate a probabilistic framework which formalizes these notions and predicts improved generalization via this deep supervision method. One advantage of this approach is that we are able to train only from synthetic CAD renderings of cluttered scenes, where concept values can be extracted, but apply the results to real images. Our implementation achieves the state-of-the-art performance of 2D/3D keypoint localization and image classification on real image benchmarks including KITTI, PASCALVOC, PASCAL3D+, IKEA, and CIFAR100. We provide additional evidence that our approach outperforms alternative forms of supervision, such as multi-task networks.",multimedia
10.1109/TVCG.2005.46,to_check,IEEE Transactions on Visualization and Computer Graphics,IEEE,2005-05-01 00:00:00,ieeexplore,Hardware-assisted visibility sorting for unstructured volume rendering,https://ieeexplore.ieee.org/document/1407861/,"Harvesting the power of modern graphics hardware to solve the complex problem of real-time rendering of large unstructured meshes is a major research goal in the volume visualization community. While, for regular grids, texture-based techniques are well-suited for current GPUs, the steps necessary for rendering unstructured meshes are not so easily mapped to current hardware. We propose a novel volume rendering technique that simplifies the CPU-based processing and shifts much of the sorting burden to the GPU, where it can be performed more efficiently. Our hardware-assisted visibility sorting algorithm is a hybrid technique that operates in both object-space and image-space. In object-space, the algorithm performs a partial sort of the 3D primitives in preparation for rasterization. The goal of the partial sort is to create a list of primitives that generate fragments in nearly sorted order. In image-space, the fragment stream is incrementally sorted using a fixed-depth sorting network. In our algorithm, the object-space work is performed by the CPU and the fragment-level sorting is done completely on the GPU. A prototype implementation of the algorithm demonstrates that the fragment-level sorting achieves rendering rates of between one and six million tetrahedral cells per second on an ATI Radeon 9800.",multimedia
10.1109/TIE.2012.2183833,to_check,IEEE Transactions on Industrial Electronics,IEEE,2013-01-01 00:00:00,ieeexplore,Real-Time Nonlinear Parameter Estimation Using the Levenberg–Marquardt Algorithm on Field Programmable Gate Arrays,https://ieeexplore.ieee.org/document/6129410/,"The Levenberg-Marquardt (LM) algorithm is a nonlinear parameter learning algorithm that converges accurately and quickly. This paper demonstrates for the first time to our knowledge, a real-time implementation of the LM algorithm on field programmable gate arrays (FPGAs). It was used to train neural networks to solve the eXclusive Or function (XOR), and for 3D-to-2D camera calibration parameter estimation. A Xilinx Virtex-5 ML506 was used to implement the LMA as a hardware-in-the-loop system. The XOR function was approximated in only 13 iterations from zero initial conditions, usually the same function is approximated in thousands of iterations using the error backpropagation algorithm. Also, this type of training not only reduced the number of iterations but also achieved a speed up in excess of 3 ×10<sup>6</sup> when compared to the software implementation. A real-time camera calibration and parameter estimation was performed successfully on FPGAs. Compared to the software implementation the FPGA implementation led to an increase in the mean squared error and standard deviation by only 17.94% and 8.04% respectively. The FPGA increased the calibration speed by a factor of 1.41 × 10<sup>6</sup>. There are a wide range of systems problems solved via nonlinear parameter optimization, this study demonstrated that a hardware solution for systems such as automated optical inspection systems or systems dealing with projective geometry estimation and motion compensation systems in robotic vision systems is possible in real time.",multimedia
10.1109/ACCESS.2021.3082011,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Texture-Generic Deep Shape-From-Template,https://ieeexplore.ieee.org/document/9435325/,"Shape-from-Template (SfT) solves the registration and 3D reconstruction of a deformable 3D object, represented by the template, from a single image. Recently, methods based on deep learning have been able to solve SfT for the wide-baseline case in real-time, clearly surpassing classical methods. However, the main limitation of current methods is the need for fine tuning of the neural models to a specific geometry and appearance represented by the template texture map. We propose the first texture-generic deep learning SfT method which adapts to new texture maps at run-time, without the need for texture specific fine tuning. We achieve this by dividing the problem into a segmentation step and a registration and reconstruction step, both solved with deep learning. We include the template texture map as one of the neural inputs in both steps, training our models to adapt to different ones. We show that our method obtains comparable or better results to previous deep learning models, which are texture specific. It works in challenging imaging conditions, including complex deformations, occlusions, motion blur and poor textures. Our implementation runs in real-time, with a low-cost GPU and CPU.",multimedia
10.1109/CNNA.2010.5430245,to_check,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),IEEE,2010-02-05 00:00:00,ieeexplore,A multi-FPGA distributed embedded system for the emulation of Multi-Layer CNNs in real time video applications,https://ieeexplore.ieee.org/document/5430245/,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.",multimedia
10.1109/CBMS.2019.00041,to_check,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),IEEE,2019-06-07 00:00:00,ieeexplore,Action Recognition in Real Homes using Low Resolution Depth Video Data,https://ieeexplore.ieee.org/document/8787393/,"We report work in progress from interdisciplinary research on Assisted Living Technology in smart homes for older adults with mild cognitive impairments or dementia. We present our field trial, the set-up for collecting and storing data from real homes, and preliminary results on action recognition using low resolution depth video cameras. The data have been collected from seven apartments with one resident each over a period of two weeks. We propose a pre-processing of the depth videos by applying an Infinite Response Filter (IIR) for extracting the movements in the frames prior to classification. In this work we classify four actions: TV interaction (turn it on/ off and switch over), standing up, sitting down, and no movement. Our first results indicate that using the IIR filter for movement information extraction improves accuracy and can be an efficient method for recognizing actions. Our current implementation uses a convolutional long short-term memory (ConvLSTM) neural network, and achieved an average peak accuracy of 86%.",multimedia
10.1109/ICASSP.1995.480079,to_check,"1995 International Conference on Acoustics, Speech, and Signal Processing",IEEE,1995-05-12 00:00:00,ieeexplore,Codebook adaptation algorithm for a scene adaptive video coder,https://ieeexplore.ieee.org/document/480079/,"Proposes a codebook adaptation algorithm for very low bit rate, real-time video coding. Although adaptive codebook design has been studied in the past, its implementation at very low coding rates suitable for the MPEG4 standard remains significantly challenging. The coder uses a standard motion compensated predictor with DCT quantization. It is unique in that it uses a hybrid scalar/vector quantizer to code predictor residuals. Bits are dynamically allocated to minimize distortion in the current frame, and scalar quantized blocks are used to adapt the VQ codebook. A codebook adaptation algorithm is described which uses an ""equidistortion principle"" and a competitive learning algorithm to continuously adapt the codewords. This training algorithm results in an increased use of the more efficient vector quantizer and improved video quality.",multimedia
10.1109/MMSP48831.2020.9287080,to_check,2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP),IEEE,2020-09-24 00:00:00,ieeexplore,DEMI: Deep Video Quality Estimation Model using Perceptual Video Quality Dimensions,https://ieeexplore.ieee.org/document/9287080/,"Existing works in the field of quality assessment focus separately on gaming and non-gaming content. Along with the traditional modeling approaches, deep learning based approaches have been used to develop quality models, due to their high prediction accuracy. In this paper, we present a deep learning based quality estimation model considering both gaming and non-gaming videos. The model is developed in three phases. First, a convolutional neural network (CNN) is trained based on an objective metric which allows the CNN to learn video artifacts such as blurriness and blockiness. Next, the model is fine-tuned based on a small image quality dataset using blockiness and blurriness ratings. Finally, a Random Forest is used to pool frame-level predictions and temporal information of videos in order to predict the overall video quality. The light-weight, low complexity nature of the model makes it suitable for real-time applications considering both gaming and non-gaming content while achieving similar performance to existing state-of-the-art model NDNetGaming. The model implementation for testing is available on GitHub<sup>1</sup>.",multimedia
10.1109/DSMP.2018.8478575,to_check,2018 IEEE Second International Conference on Data Stream Mining & Processing (DSMP),IEEE,2018-08-25 00:00:00,ieeexplore,Development and Implementation of Human Face Alignment and Tracking in Video Streams,https://ieeexplore.ieee.org/document/8478575/,"The paper presents a method that allows detection, alignment and tracking of a human face in a real time in video streams. To detect and to align face on an image a face shape regression approach is used. The developed method uses scanning window, a cascade of ensembles of regression and classification trees, and adaptive boosting. The same trees are used for classification whether the given window contains a face and for regression of a face shape. For face tracking a starting position for face search is taken from the found shape on the previous frame. Conducted analysis of the proposed method implementation gave good performance results but revealed shortcomings and directions for future work. Sensitivity of face detection is 78% and accuracy of face alignment is 95%. The implementation can track faces in real time with a speed of at least 20 frames per second.",multimedia
10.1109/ICMLC.2008.4620899,to_check,2008 International Conference on Machine Learning and Cybernetics,IEEE,2008-07-15 00:00:00,ieeexplore,GPU based video stylization,https://ieeexplore.ieee.org/document/4620899/,"In this paper, we present a GPU based video stylization framework that can artistically stylize video stream in real time. In this framework, firstly, we use a separable implementation of bilateral filter as an adaptive and iterative smoothing operation that selectively simplifies image color, leading to an abstracted look. Secondly, we perform a soft color quantization step on the abstracted video. A significant advantage of the soft color quantization implementation is preserving temporal coherence and reducing computation time as well. Successively, some optional approaches are designed to generate different artistic styles. We evaluate the effectiveness of our stylization framework with the experiment results.",multimedia
10.1109/SPC.2018.8704126,to_check,"2018 IEEE Conference on Systems, Process and Control (ICSPC)",IEEE,2018-12-15 00:00:00,ieeexplore,Localized Object Information from Detected Objects Based on Deep Learning in Video Scene,https://ieeexplore.ieee.org/document/8704126/,"The ultimate goal of computer vision research is to understand a scene semantically from an image or a video. Real-time object detection received significant attention over the past few years. Many challenges remain, especially in the focus of extraction of localized object information for scene representation. In order to have an accurate, intelligent and fast real-time object detection, the implementation of accurate localized information in the machine is inevitable. This research will focus on developing the object localization extractor that can extract the localized object information from the scene for further scene prediction and inference. In particular, (i) our localized extractor can encode significantly high-level features information; (ii) this rich localized information will be used for scene representation and understanding.",multimedia
10.1109/NNSP.2003.1318072,to_check,2003 IEEE XIII Workshop on Neural Networks for Signal Processing (IEEE Cat. No.03TH8718),IEEE,2003-09-19 00:00:00,ieeexplore,Neural network classifiers for automated video surveillance,https://ieeexplore.ieee.org/document/1318072/,"In automated visual surveillance applications, detection of suspicious human behaviors is of great practical importance. However due to random nature of human movements, reliable classification of suspicious human movements can be very difficult. Artificial neural network (ANN) classifiers can perform well however their computational requirements can be very large for real time implementation. In this paper, a data-based modeling neural network such as modified probabilistic neural network (MPNN) is introduced which partitions the decision space nonlinearly in order to achieve reliable classification, however still with acceptable computations. The experiment shows that the compact MPNN attains good classification performance compared to that of other larger conventional neural network based classifiers such as multilayer perceptron (MLP) and self organising map (SOM).",multimedia
10.1109/ICNNSP.2008.4590383,to_check,2008 International Conference on Neural Networks and Signal Processing,IEEE,2008-06-11 00:00:00,ieeexplore,Video object matching based on SIFT algorithm,https://ieeexplore.ieee.org/document/4590383/,"SIFT (scale invariant feature transform) is used to solve visual tracking problem, where the appearances of the tracked object and scene background change during tracking. The implementation of this algorithm has five major stages: scale-space extrema detection; keypoint localization; orientation assignment; keypoint descriptor; keypoint matching. From the beginning frame, object is selected as the template, its SIFT features are computed. Then in the following frames, the SIFT features are computed. Euclidean distance between the object's SIFT features and the frames' SIFT features can be used to compute the accurate position of the matched object. The experimental results on real video sequences demonstrate the effectiveness of this approach and show this algorithm is of higher robustness and real-time performance. It can solve the matching problem with translation, rotation and affine distortion between images. It plays an important role in video object tracking and video object retrieval.",multimedia
10.1109/CNNA.2002.1035038,to_check,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,IEEE,2002-07-24 00:00:00,ieeexplore,Watermarking for the authentication of video on CNN-UM,https://ieeexplore.ieee.org/document/1035038/,"Digital watermarks have been proposed for authentication of both video and still images. In such applications, the watermark is embedded within a host image such that subsequent alteration to the watermarked image can be detected with high probability. In this paper the possibility of implementing real time watermarking on a video stream is presented. In fact the new CNN-UM implementation offers time operation of only microseconds working on 64/spl times/64 images.",multimedia
10.1109/TCSVT.2018.2864321,to_check,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,2019-08-01 00:00:00,ieeexplore,A Real-Time Convolutional Neural Network for Super-Resolution on FPGA With Applications to 4K UHD 60 fps Video Services,https://ieeexplore.ieee.org/document/8429522/,"In this paper, we present a novel hardware-friendly super-resolution (SR) method based on a convolutional neural network (CNN) and its dedicated hardware (HW) on field programmable gate array (FPGA). Although CNN-based SR methods have shown very promising results for SR, their computational complexities are prohibitive for hardware implementation. To the best of our knowledge, we are the first to implement a real-time CNN-based SR HW that upscales 2K full high-definition video to 4K ultra high-definition (UHD) video at 60 frames per second (fps). In our dedicated CNN-based SR HW, low-resolution input frames are processed line-by-line, and the number of convolutional filter parameters is reduced significantly by incorporating depth-wise separable convolutions with a residual connection. Our CNN-based SR HW incorporates a cascade of 1D convolutions having large receptive fields along horizontal lines while keeping vertical receptive fields minimal, which allows us to save required line memory space in achieving comparable SR performance against full 2D convolution operations. For efficient HW implementation, we use a simple and effective quantization method with little peak signal-to-noise ratio (PSNR) degradation. Also, we propose a compression method to efficiently store intermediate feature map data to reduce the number of line memories used in HW. Our HW implementation on the FPGA generates 4K UHD frames of higher PSNR values at 60 fps and shows better visual quality, compared with conventional CNN-based SR methods that are trained and tested in software.",multimedia
10.1109/76.544741,to_check,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,1996-12-01 00:00:00,ieeexplore,A cellular neural network for clustering-based adaptive quantization in subband video compression,https://ieeexplore.ieee.org/document/544741/,"This paper presents a novel cellular connectionist model for the implementation of a clustering-based adaptive quantization in video coding applications. The adaptive quantization has been designed for a wavelet-based video coding system with a desired scene adaptive and signal adaptive quantization. Since the adaptive quantization is accomplished through a maximum a posteriori probability (MAP) estimation-based clustering process, its massive computation of neighborhood constraints makes it difficult for a software-based real-time implementation of video coding applications. The proposed cellular connectionist model aims at designing an architecture for the real-time implementation of the clustering-based adaptive quantization. With a cellular neural network architecture mapping onto the image domain, the powerful Gibbs spatial constraints are realized through interactions among neurons connected with their neighbors. In addition, the computation of coefficient distribution is designed as an external input to each component of a neuron or processing element (PE). We prove that the proposed cellular neural network does converge to the desired steady state with the proposed, update scheme. This model also provides a general architecture for image processing tasks with Gibbs spatial constraint-based computations.",multimedia
10.5594/M00218,to_check,31st SMPTE Advanced Motion Imaging Conference Technical Papers Program,SMPTE,1997-02-07 00:00:00,ieeexplore,525-line Progressive Scan Digital Broadcasting System via Satellite,https://ieeexplore.ieee.org/document/7265931/,"This paper describes the development of 525-line progressive scan (525P) digital broadcasting system, which provides cost effective high picture quality service. — 525P is a type of progressive (non-interlaced) scan format and the scan rate is exactly double that of the conventional 525-line interlaced scan (525I). This format has several advantageous features; better picture quality compared with 525I, lower encoder/decoder implementation cost compared with HDTV, high compatibility with 525I. That's why the 525P format enables cost effective high picture quality services especially adequate for consumer use TV. — The 525P digital broadcasting system prototype has been realized conforming to the digital broadcasting standard via communication satellite (CS) in Japan, which uses MPEG-2 video coding technology. For the implementation of 525P MPEG-2 encoder and decoder, Main Profile at High-1440 Level (MP@H14L) instead of Main Profile at Main Level (MP@ML) was used with a slight modification of MPEG-2 MP@ML encoder/decoder for NTSC/PAL service. This is the first implementation of real-time MPEG-2 encoder and IRD (Integrated Receiver Decoder) for 525P. MPEG-2 multiplexer for interfacing with 525P encoder was developed as well. Using this system, we have carried out several transmission experiments via CS and verified technical feasibility of high quality digital broadcasting service at the 525P video bit-rate of 9Mbps or around.",multimedia
10.1109/ICSMC.2003.1244605,to_check,"SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",IEEE,2003-10-08 00:00:00,ieeexplore,A novel approach for person authentication and content-based tracking in videos using kernel methods and active appearance models,https://ieeexplore.ieee.org/document/1244605/,"A novel integration of methods for person authentication and tracking is proposed for real time security systems. The implementation of the idea for this real time implementation follows a three step procedure-face detection, recognition and content-based tracking. Instead of analyzing continuous videos we sample the frame based on a method derived from Shannon's information theory model. The Face-detector detects multi-viewed faces in a video using feature-based kernel methods in a reduced feature space obtained using ICA. The identified ""face regions"" are then passed on to the face recognition system which is based on Active Appearance Models (AAM). Once the subject is recognized, it can be tracked in the video using kernel based object tracking method. Several space reduction techniques have been used like ICA, PCA and skin-color segmentation.",multimedia
10.1109/I2MTC.2014.6861016,to_check,2014 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) Proceedings,IEEE,2014-05-15 00:00:00,ieeexplore,A single sensor NIR depth camera for gesture control,https://ieeexplore.ieee.org/document/6861016/,"There is a large amount of sustained research activity in the area of Human-Computer Interaction (HCI). Gesture control is one of hottest in this field. Although gesture control research began with terminals attached to computers in combination with a pointing device, the large scale implementation and utilization of gesture control continues to be infeasible today. As machine vision, image processing, and artificial intelligence algorithms are error prone, gestures will never be interpreted in the same way for all instances of the gesture's appearance, especially due to changes of the ambient light. This latter condition led to the use of near infrared (NIR) illumination such that the recorded scene is not affected by light variations. Since the posture of a gesture may require the processing of overlapping features, reliability is greatly improved through the use of images that contain a third dimension. In this paper, a smart and real-time depth camera based on a new depth generation principle is introduced. A monotonic increasing and decreasing function is used to control the frequency and duty-cycle of the NIR illumination pulses. The adjusted light pulses reflect off of the object of interest and are captured as a series of images. A reconfigurable hardware architecture calculates the depth-map of the visible face of the object in real-time from a number of images. The final depth map is then used for gesture detection, tracking and recognition. A series of tests and measurements will explain how the camera builds the depth map and how it can operate in both near and far ranges. Results on controlling video game consoles and Smart TVs using the above camera will be given.",multimedia
10.1109/IGEHT.2017.8094075,to_check,2017 International Conference on Innovations in Green Energy and Healthcare Technologies (IGEHT),IEEE,2017-03-18 00:00:00,ieeexplore,A study on cognitive social data fusion,https://ieeexplore.ieee.org/document/8094075/,"Traditional data mining usually deals with data from a single domain. In the big data era, we are facing a diversity of datasets from different sources in different domains. These datasets consist of multiple modalities, each of which has a different representation, distribution, scale and density. Big data have volumes in range Exabyte's ten to the power of eighteen. A large number of data's are stored in Big Data storage every second. For instance in YouTube for every second a video of size 72 hours are being uploaded. It shows that big data have a big scope in handling of large data. Big data for learning, intelligence, data fusion, social network, mining and so many plays a vital role in it. The big data technologies along with machine learning algorithm have developed lots of advanced development in the field of social mining, network and social Medias. It has also developed so many challenges in data storage, handling, representation, mining, analysing the user behaviours and so many. In Social mining along with text the symbols are also analysed for effective mining of users. This paper does not only introduce high-level principles of each category of methods, but also give examples in which these techniques are used to handle real big data problems. The data storage size can be optimized by using the map reduce algorithm in a effective way for data storage in big data. When the repeateddata's are replaced with its reference then the storage will be optimized. Thus this method implementation will leads to the effective data mining and the analysing of persons behaviour more effectively.",multimedia
10.1109/CMPEUR.1992.218450,to_check,CompEuro 1992 Proceedings Computer Systems and Software Engineering,IEEE,1992-05-08 00:00:00,ieeexplore,A systolic neural network image processing architecture,https://ieeexplore.ieee.org/document/218450/,"The architecture introduced, the neural accelerator board (NAB) system, has been designed to perform a range a machine vision functions, of which one is motion parallax range estimation. The approach used in the NAB system, is to perform real-time calculations on the image data, captured by a moving video camera, using a highly parallel neural network architecture. The NAB system architecture, its physical implementation, and the surrounding application system are described. The implementation of the motion parallax algorithm, as a demonstration of the capabilities of this architecture, is outlined.&lt;<ETX>&gt;</ETX>",multimedia
10.1109/ICPR.2016.7900142,to_check,2016 23rd International Conference on Pattern Recognition (ICPR),IEEE,2016-12-08 00:00:00,ieeexplore,A temporally coherent neural algorithm for artistic style transfer,https://ieeexplore.ieee.org/document/7900142/,"Within the fields of visual effects and animation, humans have historically spent painstaking hours mastering the skill of drawing frame-by-frame animations. One such animation technique that has been widely used is called “rotoscoping” and has allowed uniquely stylized animations to capture the motion of real life action sequences. Automating this arduous process would free animators from performing frame by frame stylization to concentrate on artistic contributions. We introduce a new artificial system based on an existing neural style transfer method which creates artistically stylized animations that simultaneously reproduce both the motion of the original videos that they are derived from and the unique style of a given artistic work. This system utilizes a convolutional neural network framework to extract a hierarchy of image features used for generating images that appear visually similar to a given artistic style while at the same time faithfully preserving temporal content. The use of optical flow allows the combination of style and content to be integrated directly with the apparent motion over frames of a video to produce smooth and visually appealing transitions. This implementation demonstrates how biologically-inspired systems such as convolutional neural networks are rapidly approaching human-level behavior in tasks that were once thought impossible. Further, this research provides unique insights into the way that humans who produce artistically stylized animations perceive temporal information.",multimedia
10.1109/FPT.2018.00061,to_check,2018 International Conference on Field-Programmable Technology (FPT),IEEE,2018-12-14 00:00:00,ieeexplore,An FPGA Realization of OpenPose Based on a Sparse Weight Convolutional Neural Network,https://ieeexplore.ieee.org/document/8742338/,"The OpenPose is a kind of a deep learning based pose estimator which achieved a top accuracy for multiple person pose estimations. Even if using the OpenPose, it is necessary to used high-performance GPU since it requires massive parameters access with high-bandwidth off-chip GDDR5 memories and a higher operation clock frequency. Thus, the power consumption becomes a critical issue to realization. Also, its computation time is slower than the current video standard frame speed (29.97 FPS). In the paper, we introduce a sparse weight CNN to reduce the amount of memory size for weights, which is Then, we offer the indirect memory access architecture to realize the sparse CNN convolutional operation efficiently. Also, to increase throughput further, we applied the six stages of pipeline architecture with a pipeline buffer memory realization. Our implementation satisfied the timing constraint for real-time applications. Since our architecture computed an image with 42.6 msec, the number of frames per second (FPS) was 23.43. We measured the total board power consumption: It was 55 Watt. Thus, the performance per power efficiency was 0.444 (FPS/W). Compared with the NVidia Titan X Pascal architecture GPU, it was 3.49 times faster, it dissipated 3.54 times lower power, and its performance per power efficiency was 13.05 times better. As far as we know, this work is the first FPGA implementation of the OpenPose.",multimedia
10.1109/IECON.2008.4758338,to_check,2008 34th Annual Conference of IEEE Industrial Electronics,IEEE,2008-11-13 00:00:00,ieeexplore,An embedded face recognition system on A VLSI array architecture and its FPGA implementation,https://ieeexplore.ieee.org/document/4758338/,Face recognition is a non-intrusive way of automated person identification. A popular method for face recognition is using eigen faces. Eigen faces are obtained by doing principal component analysis (PCA) on the face database. A neural network that performs PCA is called principal component neural network (PCNN). This paper presents a systolic array design for principal component neural network-based face recognition system. Results of implementation of the design in an FPGA device of Xilinx confirm the suitability of the design for real-time video surveillance and high-speed access control.,multimedia
10.1109/MWSCAS.2012.6291962,to_check,2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS),IEEE,2012-08-08 00:00:00,ieeexplore,Cellular neural networks: Implementation of a segmentation algorithm on a Bio-inspired hardware processor,https://ieeexplore.ieee.org/document/6291962/,"The Cellular Neural/Nonlinear Network (CNN) paradigm has recently led to a Bio-inspired (Bi-i) Cellular Vision system, which represents a computing platform consisting of sensing, array sensing-processing and digital signal processing. This paper illustrates the implementation of a novel CNN-based segmentation algorithm onto the Bi-i system. The experimental results, carried out for a benchmark video sequence, show the feasibility of the approach, which provides a frame rate of about 26 frame/sec. Finally, comparisons with existing CNN-based methods highlight that the proposed implementation represents a good trade-off between real-time requirements and accuracy.",multimedia
10.1109/ICICCS48265.2020.9121163,to_check,2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS),IEEE,2020-05-15 00:00:00,ieeexplore,Convolutional Neural Network based Automated Attendance System by using Facial Recognition Domain,https://ieeexplore.ieee.org/document/9121163/,"This project aims to recognize faces in an image, video, or via live camera using a deep learning-based Convolutional Neural Network model that is fast as well as accurate. Face recognition is a process of identifying faces in an image and has practical applications in a variety of domains, including information security, biometrics, access control, law enforcement, smart cards, and surveillance system. Deep Learning uses numerous layers to discover interpretations of data at different extraction levels. It has improved the landscape for performing research in facial recognition. The state-of-the-art implementation has been bettered by the introduction of deep learning in face recognition and has stimulated success in practical applications. Convolutional neural networks, a kind of deep neural network model has been proven to achieve success in the face recognition domain. For real-time systems, sampling must be done before using CNNs. On the other hand, complete images (all the pixel values) are passed as the input to Convolutional Neural Networks. The following steps: feature selection, feature extracti on, and training are performed in each step. This might lead to the assumption, where convolutional neural network implementation has a chance to get complicated and time-consuming.",multimedia
10.1109/DASIP.2016.7853831,to_check,2016 Conference on Design and Architectures for Signal and Image Processing (DASIP),IEEE,2016-10-14 00:00:00,ieeexplore,Demo: HELICoiD tool demonstrator for real-time brain cancer detection,https://ieeexplore.ieee.org/document/7853831/,"In this paper, a demonstrator of three different elements of the EU FET HELICoiD project is introduced. The goal of this demonstration is to show how the combination of hyperspectral imaging and machine learning can be a potential solution to precise real-time detection of tumor tissues during surgical operations. The HELICoiD setup consists of two hyperspectral cameras, a scanning unit, an illumination system, a data processing system and an EMB01 accelerator platform, which hosts an MPPA-256 manycore chip. All the components are mounted fulfilling restrictions from surgical environments, as shown in the accompanying video recorded at the operating room. An in-vivo human brain hyperspectral image data base, obtained at the University Hospital Doctor Negrin in Las Palmas de Gran Canaria, has been employed as input to different supervised classification algorithms (SVM, RF, NN) and to a spatial-spectral filtering stage (SVM-KNN). The resulting classification maps are shown in this demo. In addition, the implementation of the SVM-KNN classification algorithm on the MPPA EMB01 platform is demonstrated in the live demo.",multimedia
10.1109/ITAIC49862.2020.9339101,to_check,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),IEEE,2020-12-13 00:00:00,ieeexplore,Design of Embedded Real-time Target Tracking System Based on KNN Algorithm,https://ieeexplore.ieee.org/document/9339101/,"With the rapid development of science and technology, more and more high-tech products have been applied to all aspects of life. In the field of security and defense, video monitoring plays an important role. In order to solve the problem of real-time and miniaturization of target tracking system, this paper applies a tracking algorithm based on KNN(k-nearest neighbor) background splitter combined with Kalman filter. When tracking a moving target, it is important to determine the moving target in the initial frame. In this paper, the motion detection method in KNN background segmentation is adopted to automatically initialize the moving target. Kalman filter can predict the motion state of the moving target in the next frame according to the information of the moving target in the current frame, and track the target with the predicted value as the input of CamShift algorithm. The implementation of the tracking algorithm on the embedded raspberry pi platform is beneficial to the miniaturization of smart devices. The experimental results show that the algorithm has better tracking ability, faster operation speed and stronger robustness for moving targets in raspberry pi embedded environment, which improves the performance of the algorithm and gets good effects.",multimedia
10.1109/CNNA.2014.6888621,to_check,2014 14th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA),IEEE,2014-07-31 00:00:00,ieeexplore,Design of a third generation Real-Time Cellular Neural Network emulator,https://ieeexplore.ieee.org/document/6888621/,"In this paper, the features of the next generation Real-Time Cellular Neural Network Processor (RTCNNP-v3) are discussed. The RTCNNP-v2 structure is the only CNN implementation that is reported to be capable of processing full-HD 1080p@60 (1920×1080 resolution at 60 Hz frame rate) video images in real-time, due to its fully-pipelined architecture, however, it has some weaknesses like the inability to divide the processing in spatial domain, record and recall intermediate results to an external memory and has some issues in its internal memory coding. Those shortcomings are to be addressed in the next design of our CNN emulator - RTCNNP-v3, which will increase the range of applications and enable the implementation to match the requirements of the cutting-edge movie production technologies like UHD (4K) and the future FUHD (8K).",multimedia
10.1109/ICMLC.2004.1384539,to_check,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),IEEE,2004-08-29 00:00:00,ieeexplore,Efficient motion estimation using two-phase algorithm,https://ieeexplore.ieee.org/document/1384539/,"This work introduces a fast block-based motion estimation algorithm named two-phase motion estimation algorithm (TPMEA). The idea of the algorithm is to divide motion estimation procedure into two-phase: in the first phase, the sum of pixels in a line (or a column) is used as the 1D vector; In the second phase, a filtering threshold is deduced from the 1D vector matching, as Minkowski inequality has shown, blocks cannot match well if their corresponding 1D vector do not match well. Hence, the expensive 2D block matching need only be performed in a limited scope, which saves a lot of time. Moreover, an efficient implementation to calculate 1D vector is presented to speed up encoder further. From the experiment conducted in the test model of H.264(version:JM61d), the algorithm presented in This work can save approximately 30% of the time compared with the exhaustive search algorithm and achieve the same image quality. It can satisfy the demand for high performance and real-time video encoding.",multimedia
10.1109/DSD.2012.64,to_check,2012 15th Euromicro Conference on Digital System Design,IEEE,2012-09-08 00:00:00,ieeexplore,FPGA-based Neural Network for Nonuniformity Correction on Infrared Focal Plane Arrays,https://ieeexplore.ieee.org/document/6386892/,"Despite recent technological advances which improve their performance and reduce their cost, Focal Plane Arrays for infrared imagers suffer from spatial nonuniformity that renders their output unusable unless a suitable correction method is applied. This paper describes an embedded hardware implementation of Scribner's algorithm for online nonuniformity correction. Our implementation on a Xilinx Spartan XC3S1200E FPGA achieves a throughput of more than 130 frames per second on 320x240-pixel IR video, which greatly exceeds real-time requirements. The power consumption of our system is 329mW, which is two orders of magnitude smaller than a software implementation of the algorithm on a traditional processor, and can be greatly reduced with a custom-VLSI implementation of the architecture.",multimedia
10.1109/IVCNZ51579.2020.9290654,to_check,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),IEEE,2020-11-27 00:00:00,ieeexplore,Fast Portrait Segmentation of the Head and Upper Body,https://ieeexplore.ieee.org/document/9290654/,"Portrait segmentation is the process whereby the head and upper body of a person is separated from the background of an image or video stream. This is difficult to achieve accurately, although good results have been obtained with deep learning methods which cope well with occlusion, pose and illumination changes. These are however, either slow or require a powerful system to operate in real-time. We present a new method of portrait segmentation called FaceSeg which uses fast DBSCAN clustering combined with smart face tracking that can replicate the benefits and accuracy of deep learning methods at a much faster speed. In a direct comparison using a standard testing suite, our method achieved a segmentation speed of 150 fps for a 640x480 video stream with median accuracy and F1 scores of 99.96% and 99.93% respectively on simple backgrounds, with 98.81% and 98.13% on complex backgrounds. The state-of-art deep learning based FastPortrait / Mobile Neural Network method achieved 15 fps with 99.95% accuracy and 99.91% F1 score on simple backgrounds, and 99.01% accuracy and 98.43 F1 score on complex backgrounds. An efficacy-boosted implementation for FaceSeg can achieve 75 fps with 99.23% accuracy and 98.79% F1 score on complex backgrounds.",multimedia
10.1109/QoMEX48832.2020.9123142,to_check,2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX),IEEE,2020-05-28 00:00:00,ieeexplore,How Deep is Your Encoder: An Analysis of Features Descriptors for an Autoencoder-Based Audio-Visual Quality Metric,https://ieeexplore.ieee.org/document/9123142/,The development of audio-visual quality assessment models poses a number of challenges in order to obtain accurate predictions. One of these challenges is the modelling of the complex interaction that audio and visual stimuli have and how this interaction is interpreted by human users. The No-Reference Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with this problem from a machine learning perspective. The metric receives two sets of audio and video features descriptors and produces a low-dimensional set of features used to predict the audio-visual quality. A basic implementation of NAViDAd was able to produce accurate predictions tested with a range of different audio-visual databases. The current work performs an ablation study on the base architecture of the metric. Several modules are removed or re-trained using different configurations to have a better understanding of the metric functionality. The results presented in this study provided important feedback that allows us to understand the real capacity of the metric's architecture and eventually develop a much better audio-visual quality metric.,multimedia
10.1109/PAAP.2011.29,to_check,"2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming",IEEE,2011-12-11 00:00:00,ieeexplore,Implementation of G.729A on Embedded SIMD Processor,https://ieeexplore.ieee.org/document/6128499/,"This paper addresses a real-time implementation of multi-channel, high quality G.729A speech codec based on an embedded SIMD processor, which is used in a SIP Video Phone. A series of strategies are designed for the special characteristics of the processor and the G.729A, including the memory management and SIMD decomposing. The profile shows that the dramatic improvement is achieved. Less than 20% CPU load ensures the video codec's smooth output of the video phone.",multimedia
10.1109/IC4ME247184.2019.9036531,to_check,"2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2)",IEEE,2019-07-12 00:00:00,ieeexplore,Object Detection Based Security System Using Machine learning algorthim and Raspberry Pi,https://ieeexplore.ieee.org/document/9036531/,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally.",multimedia
10.1109/ICCV.1993.378163,to_check,1993 (4th) International Conference on Computer Vision,IEEE,1993-05-14 00:00:00,ieeexplore,Occam algorithms for computing visual motion,https://ieeexplore.ieee.org/document/378163/,"By drawing an analogy with machine learning, the author proposes to define visual motion as a predictor that can accurately predict future frames. Under this new definition, visual motion can be specified by a collection of image patches, each moving in a simple motion. An implementation with rectangular patches determined recursively by a binary decision tree is described. Experimental results on real video sequences verify the algorithm assumptions and show that motion in typical sequences can be accurately described in terms of a few parameters.&lt;<ETX>&gt;</ETX>",multimedia
10.1109/WiCOM.2006.313,to_check,"2006 International Conference on Wireless Communications, Networking and Mobile Computing",IEEE,2006-09-24 00:00:00,ieeexplore,Optimization of an AVS Audio Decoder on DSP,https://ieeexplore.ieee.org/document/4149490/,"The part III of Audio Video Standard of China (AVS) is the first Hi-Fi audio coding standard in China public domain. Through nonlinear quantization and context-dependent bitplane entropy coding (CBC), it offers distinct features of nonlinear quantization noise adaptation and high efficiency fine grain scalability (FGS), but also imposes great challenges on fixed-point implementation with high precision requirement and stringent computational resource restriction. This paper presents tuned optimization strategies for fixed-point arithmetic, buffer manipulation, entropy decoding, and CBC decoding loop reduction to address the challenges. The demonstration real-time implementation on fixed-point DSP TMS320C5509A shows that our design optimization cuts down computational complexity from over 200 MIPs to 60 MIPs while preserving over 15 effective bits precision for 16-bit PCM output",multimedia
10.1109/CNSM.2015.7367349,to_check,2015 11th International Conference on Network and Service Management (CNSM),IEEE,2015-11-13 00:00:00,ieeexplore,Predicting service metrics for cluster-based services using real-time analytics,https://ieeexplore.ieee.org/document/7367349/,"Predicting the performance of cloud services is intrinsically hard. In this work, we pursue an approach based upon statistical learning, whereby the behaviour of a system is learned from observations. Specifically, our testbed implementation collects device statistics from a server cluster and uses a regression method that accurately predicts, in real-time, client-side service metrics for a video streaming service running on the cluster. The method is service-agnostic in the sense that it takes as input operating-systems statistics instead of service-level metrics. We show that feature set reduction significantly improves prediction accuracy in our case, while simultaneously reducing model computation time. We also discuss design and implementation of a real-time analytics engine, which processes streams of device statistics and service metrics from testbed sensors and produces model predictions through online learning.",multimedia
10.1109/CNNA.2005.1543155,to_check,2005 9th International Workshop on Cellular Neural Networks and Their Applications,IEEE,2005-05-30 00:00:00,ieeexplore,The real-time image processing based on DSP,https://ieeexplore.ieee.org/document/1543155/,"At present, wavelet transform is gradually used in the processing of video and image. But the algorithm of 2D wavelet transform is too burdensome and complicated to process huge amount of image data in real-time. To the problem, this paper proposes a novel method to process video and image real-timely based on DSP. At the aspect of wavelet algorithm, the method changes the traditional image convolution mode, which is based on the cross filtering of rows and columns. According to the feature of lifting wavelet transform, 2D image data is firstly linearized in some scanning order to the 1D sequence. Subsequently the 1D sequence is decomposed by lifting wavelet transform. At the other aspect of hardware, the program structure of DSP is optimized to exert the operation power with the in-chip memorizer of DSP. The experiment results show that the implementation meets the real-time requirement of image coding, so is a feasible and efficient DSP solution.",multimedia
10.1109/ICMLC.2010.5580745,to_check,2010 International Conference on Machine Learning and Cybernetics,IEEE,2010-07-14 00:00:00,ieeexplore,The study and implementation of real-time face recognition and tracking system,https://ieeexplore.ieee.org/document/5580745/,"During the past several years, face recognition in video has received significant attention. For the video monitoring and artificial vision, real time face recognition has very important meaning. The current method is still very susceptible to the illumination condition, non-real time and very common to fail to track the target face especially when partly covered or moving fast. In this paper, we propose to use AdaBoost Cascade for face detection and then in order to recognize the candidate faces, they will be analyzed by the hybrid Wavelet and LDA method. After that, Mean shift will be invoked to track the face. The implementation shows that the algorithm has quite good performance in terms of real-time and the tracking procedure is triggered accurately.",multimedia
10.1109/ICMLC.2009.5212738,to_check,2009 International Conference on Machine Learning and Cybernetics,IEEE,2009-07-15 00:00:00,ieeexplore,Time series analysis during the releasing arrow stage,https://ieeexplore.ieee.org/document/5212738/,"In this paper, during the releasing arrow stage, the relationship between the time series trajectory and the archery performance has been studied. With the aid of the high sampling rate 1200 frames/second of video camera, the time series aiming trajectory during the releasing stage can be captured for analysis, especially the releasing string motion. The linear time invariant auto-regressive exogenous (ARX) process is adopted to model the intended aiming trajectory before the releasing arrow stage. Then based on the model the estimated trajectory during the releasing arrow stage is evaluated as the reference, so its difference between the real one is the main focus in the paper. According to these discrepancies some key parameters are defined first, and the implementation of correlation approach can result in some significant relationships which can characterize his particular releasing string motion during this stage. For example, his releasing arrow trajectory always has an upper linear trend along the vertical direction, and the vertical deviation on the target plays more important role than the radial deviation.",multimedia
10.1109/CVPR.2005.348,to_check,2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05),IEEE,2005-06-25 00:00:00,ieeexplore,Tracking multiple colored blobs with a moving camera,https://ieeexplore.ieee.org/document/1467577/,"This paper concerns a method for tracking multiple blobs exhibiting certain color distributions in images acquired by a possibly moving camera. The method encompasses a collection of techniques that enable modeling and detecting the blobs possessing the desired color distribution(s), as well as inferring their temporal association across image sequences. Appropriately colored blobs are detected with a Bayesian classifier, which is bootstrapped with a small set of training data. Then, an online iterative training procedure is employed to refine the classifier using additional training images. Online adaptation of color probabilities is used to enable the classifier to cope with illumination changes. Tracking over time is realized through a novel technique, which can handle multiple colored blobs. Such blobs may move in complex trajectories and occlude each other in the field of view of a possibly moving camera, while their number may vary over time. A prototype implementation of the developed system running on a conventional Pentium IV processor at 2.5 GHz operates on 320/spl times/240 live video in real time (30Hz). It is worth pointing out that currently, the cycle time of the tracker is determined by the maximum acquisition frame rate that is supported by our IEEE 1394 camera, rather than the latency introduced by the computational overhead for tracking blobs.",multimedia
10.1109/JSTSP.2019.2901195,to_check,IEEE Journal of Selected Topics in Signal Processing,IEEE,2019-05-01 00:00:00,ieeexplore,An End-to-End Multimodal Voice Activity Detection Using WaveNet Encoder and Residual Networks,https://ieeexplore.ieee.org/document/8649655/,"Recently, there has been growing use of deep neural networks in many modern speech-based systems such as speaker recognition, speech enhancement, and emotion recognition. Inspired by this success, we propose to address the task of voice activity detection (VAD) by incorporating auditory and visual modalities into an end-to-end deep neural network. We evaluate our proposed system in challenging acoustic environments including high levels of noise and transients, which are common in real-life scenarios. Our multimodal setting includes a speech signal captured by a microphone and a corresponding video signal capturing the speaker's mouth region. Under such difficult conditions, robust features need to be extracted from both modalities in order for the system to accurately distinguish between speech and noise. For this purpose, we utilize a deep residual network, to extract features from the video signal, while for the audio modality, we employ a variant of WaveNet encoder for feature extraction. The features from both modalities are fused using multimodal compact bilinear pooling to form a joint representation of the speech signal. To further encode the temporal information, we feed the fused signal to a long short-term memory network and the system is then trained in an end-to-end supervised fashion. Experimental results demonstrate the improved performance of the proposed end-to-end multimodal architecture compared to unimodal variants for VAD. Upon the publication of this paper, we will make the implementation of our proposed models publicly available at https://github.com/iariav/End-to-End-VAD and https://israelcohen.com.",multimedia
10.1109/TPAMI.1981.4767166,to_check,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,1981-11-01 00:00:00,ieeexplore,Real-Time Adaptive Contrast Enhancement,https://ieeexplore.ieee.org/document/4767166/,"A recursive filter approach is introduced to simplify real-time implementation of an adaptive contrast enhancement scheme for imaging sensors. With this scheme, even scenes possessing large global dynamic ranges (&gt;40 dB) can be accommodated by the limited dynamic range (20 dB) of a display without losing the local contrast essential for image interpretation. This paper describes the recursive filter implementation of the local area contrast enhancement scheme using charge-coupled devices and the resultant real-time hardware capable of processing standard 525 and 875 line TV compatible video (from vidicons, videotape recorders, etc). Several examples from video imagery are included to demonstrate its effectiveness.",multimedia
10.1109/TCSVT.2019.2954474,to_check,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,2020-11-01 00:00:00,ieeexplore,Recent Advances on HEVC Inter-Frame Coding: From Optimization to Implementation and Beyond,https://ieeexplore.ieee.org/document/8906164/,"High Efficiency Video Coding (HEVC) has doubled the video compression ratio with equivalent subjective quality as compared to its predecessor H.264/AVC. The significant coding efficiency improvement is attributed to many new techniques. Inter-frame coding is one of the most powerful yet complicated techniques therein and has posed high computational burden thus main obstacle in HEVC-based real-time applications. Recently, plenty of research has been done to optimize the inter-frame coding, either to reduce the complexity for real-time applications, or to further enhance the encoding efficiency. In this paper, we provide a comprehensive review of the state-of-the-art techniques for HEVC inter-frame coding from three aspects, namely fast inter coding solutions, implementation on different hardware platforms as well as advanced inter coding techniques. More specifically, different algorithms in each aspect are further subdivided into sub-categories and compared in terms of pros, cons, coding efficiency and coding complexity. To the best of our knowledge, this is the first such comprehensive review of the recent advances of the inter-frame coding for HEVC and hopefully it would help the improvement, implementation and applications of HEVC as well as the ongoing development of the next generation video coding standard.",multimedia
10.1109/TCAD.2018.2878162,to_check,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,IEEE,2019-11-01 00:00:00,ieeexplore,System-on-a-Chip (SoC)-Based Hardware Acceleration for an Online Sequential Extreme Learning Machine (OS-ELM),https://ieeexplore.ieee.org/document/8509179/,"Machine learning algorithms such as those for object classification in images, video content analysis, and human action recognition are used to extract meaningful information from data recorded by image sensors and cameras. Among the existing machine learning algorithms for such purposes, extreme learning machines (ELMs) and online sequential ELMs (OS-ELMs) are well known for their computational efficiency and performance when processing large datasets. The latter approach was derived from the ELM approach and optimized for real-time application. However, OS-ELM classifiers are computationally demanding, and the existing state-of-the-art computing platforms are not efficient enough for embedded systems, especially for applications with strict requirements in terms of low power consumption, high throughput, and low latency. This paper presents the implementation of an ELM/OS-ELM in a customized system-on-a-chip field-programmable gate array-based architecture to ensure efficient hardware acceleration. The acceleration process comprises parallel extraction, deep pipelining, and efficient shared memory communication.",multimedia
10.1109/CERMA.2007.4367696,to_check,"Electronics, Robotics and Automotive Mechanics Conference (CERMA 2007)",IEEE,2007-09-28 00:00:00,ieeexplore,Image Recognition Processor based on Morphological Associative Memories,https://ieeexplore.ieee.org/document/4367696/,"In this paper, the description of an image recognition processor based on morphological associative memories (MAM) is presented. The combination of the MAM features with its implementation in a programmable logic device grants to the system high speeds of processing, high immunity to the present noise in the images and great capacity of storage; these features make of our proposal a robust system and ideal to work in applications with autonomous real time systems. The described processor, with aid of a binary process of the image, realizing the filter functions, and the use of morphological heteroassociative memories min demonstrated a high performance in the recognition of images corrupted with diverse types of noises.",multimedia
10.1109/ACCESS.2021.3051625,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Development of Effective Methods for Structural Image Recognition Using the Principles of Data Granulation and Apparatus of Fuzzy Logic,https://ieeexplore.ieee.org/document/9324755/,"The processes of intelligent data processing in computer vision systems have been researched. The problem of structural image recognition is relevant. This is a promising way to assess the degree of similarity of objects. This approach provides the simplicity of construction and the high reliability of decision making. The main problem of an effective description of characteristic features is the distortion of fragments of analyzed objects. The reasons for changing the input data can be the actions of geometric transformations, the influence of background or interference. The elements of false objects with similar characteristics are formed. The problem of ensuring high-quality recognition requires the implementation of effective means of image processing. Methods of statistical modeling, granulation of data and fuzzy sets, detection and comparison of keypoints on the image, classification and clustering of data, and simulation modelling are used in this research. The implementation of the proposed approaches provides the formation of a concise description of features or a vector representation of unique keypoints. The verification of theoretical foundations and evaluation of the effectiveness of the proposed data processing methods for real image bases is performed using the OpenCV library. The applied significance of the work is substantiated according to the criterion of data processing time without reducing the characteristics of reliability and interference immunity. The developed methods allow to increase the structural recognition of images by several times. Perspectives of research may involve identifying the optimal number of keypoints of the base set.",multimedia
10.1109/ICSICT.2018.8565768,to_check,2018 14th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT),IEEE,2018-11-03 00:00:00,ieeexplore,Research on Field Programmable Neuron Array (FPNA) Based on Reusable ANN Neurons,https://ieeexplore.ieee.org/document/8565768/,"An artificial neural network (ANN) is a parallel, distributed processing system consisting of a large number of interconnected neurons. At present, artificial neural networks have been widely used in signal processing, automation, control systems, image recognition and many other fields. Usually, the realization of neural networks is based on software. However, because the software implementation method cannot achieve real-time calculation in many cases, the hardware implementation method can reflect the inherent parallel processing characteristics of neural networks. This paper proposes an ANN neural network field programmable neuron array based on the design of reusable ANN artificial neural network IP core. It not only can save a lot of hardware resources and improve processing speed, but also has a rapid development cycle and good reconfigurability and other advantages.",multimedia
10.1109/JSSC.2016.2617317,to_check,IEEE Journal of Solid-State Circuits,IEEE,2017-01-01 00:00:00,ieeexplore,A 502-GOPS and 0.984-mW Dual-Mode Intelligent ADAS SoC With Real-Time Semiglobal Matching and Intention Prediction for Smart Automotive Black Box System,https://ieeexplore.ieee.org/document/7744546/,"The advanced driver assistance system (ADAS) for adaptive cruise control and collision avoidance is strongly dependent upon the robust image recognition technology such as lane detection, vehicle/pedestrian detection, and traffic sign recognition. However, the conventional ADAS cannot realize more advanced collision evasion in real environments due to the absence of intelligent vehicle/pedestrian behavior analysis. Moreover, accurate distance estimation is essential in ADAS applications and semiglobal matching (SGM) is most widely adopted for high accuracy, but its system-on-chip (SoC) implementation is difficult due to the massive external memory bandwidth. In this paper, an ADAS SoC with behavior analysis with Artificial Intelligence functions and hardware implementation of SGM is proposed. The proposed SoC has dual-mode operations of high-performance operation for intelligent ADAS with real-time SGM in D-Mode (d-mode) and ultralow-power operation for black box system in parking-mode. It features: 1) task-level pipelined SGM processor to reduce external memory bandwidth by 85.8%; 2) region-of-interest generation processor to reduce 86.2% of computation; 3) mixed-mode intention prediction engine for dual-mode intelligence; and 4) dynamic voltage and frequency scaling control to save 36.2% of power in d-mode. The proposed ADAS processor achieves 862 GOPS/W energy efficiency and 31.4GOPS/mm<sup>2</sup> area efficiency, which are 1.53× and 1.75× improvements than the state of the art, with 30 frames/s throughput under 720p stereo inputs.",multimedia
10.1109/ICDIM.2007.369342,to_check,2006 1st International Conference on Digital Information Management,IEEE,2006-12-06 00:00:00,ieeexplore,An Active Rule Based Approach to Audio Steganalysis with a Genetic Algorithm,https://ieeexplore.ieee.org/document/4221879/,"Differentiating anomalous audio document (stego audio) from pure audio document (cover audio) is difficult and tedious. Steganalytic techniques strive to detect whether an audio contains a hidden message or not. This paper presents a genetic algorithm (GA) based approach to audio steganalysis. The basic idea is that, the various audio quality metrics calculated on cover audio signals and on stego audio signals vis-avis their denoised versions, are statistically different. GA is employed to derive a set of classification rules from audio data using these audio quality metrics, and the support-confidence framework is utilized as fitness function to judge the quality of each rule. The generated rules are then used to detect or classify the audio documents in a real-time environment. Unlike most existing GA-based approaches, because of the simple representation of rules and the effective fitness function, the proposed method provides flexibility to generally detect any new steganography technique. The implementation of the GA based audio steganalyzer relies on the choice of these audio quality metrics and the construction of a two-class classifier, which will discriminate between the adulterated and the untouched audio samples. Experimental results show that the proposed technique provides promising detection rates.",multimedia
10.1109/ICASSP.2018.8462116,to_check,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2018-04-20 00:00:00,ieeexplore,"TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation",https://ieeexplore.ieee.org/document/8462116/,"Robust speech processing in multi-talker environments requires effective speech separation. Recent deep learning systems have made significant progress toward solving this problem, yet it remains challenging particularly in real-time, short latency applications. Most methods attempt to construct a mask for each source in time-frequency representation of the mixture signal which is not necessarily an optimal representation for speech separation. In addition, time-frequency decomposition results in inherent problems such as phase/magnitude decoupling and long time window which is required to achieve sufficient frequency resolution. We propose Time-domain Audio Separation Network (TasNet) to overcome these limitations. We directly model the signal in the time-domain using an encoder-decoder framework and perform the source separation on nonnegative encoder outputs. This method removes the frequency decomposition step and reduces the separation problem to estimation of source masks on encoder outputs which is then synthesized by the decoder. Our system outperforms the current state-of-the-art causal and noncausal speech separation algorithms, reduces the computational cost of speech separation, and significantly reduces the minimum required latency of the output. This makes TasNet suitable for applications where low-power, real-time implementation is desirable such as in hearable and telecommunication devices.",multimedia
10.1109/ICASSP.2017.7952602,to_check,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2017-03-09 00:00:00,ieeexplore,A case study of machine learning hardware: Real-time source separation using Markov Random Fields via sampling-based inference,https://ieeexplore.ieee.org/document/7952602/,"We explore sound source separation to isolate human voice from background noise on mobile phones, e.g. talking on your cell phone in an airport. The challenges involved are real-time execution and power constraints. As a solution, we present a novel hardware-based sound source separation implementation capable of real-time streaming performance. The implementation uses a recently introduced Markov Random Field (MRF) inference formulation of foreground/background separation, and targets voice separation on mobile phones with two microphones. We demonstrate a real-time streaming FPGA implementation running at 150 MHz with total of 207 KB RAM. Our implementation achieves a speedup of 20× over a conventional software implementation, achieves an SDR of 6.655 dB with 1.601 ms latency, and exhibits excellent perceived audio quality. A virtual ASIC design shows that this architecture is quite small (less than 10M gates), consumes only 69.977 mW running at 20 MHz (52× less than an ARM Cortex-A9 software reference), and appears amenable to additional optimization for power.",multimedia
10.1109/EuroSP48549.2020.00021,to_check,2020 IEEE European Symposium on Security and Privacy (EuroS&P),IEEE,2020-09-11 00:00:00,ieeexplore,DLA: Dense-Layer-Analysis for Adversarial Example Detection,https://ieeexplore.ieee.org/document/9230412/,"In recent years Deep Neural Networks (DNNs) have achieved remarkable results and even showed superhuman capabilities in a broad range of domains. This led people to trust in DNN classifications even in security-sensitive environments like autonomous driving. Despite their impressive achievements, DNNs are known to be vulnerable to adversarial examples. Such inputs contain small perturbations to intentionally fool the attacked model. In this paper, we present a novel end-to-end framework to detect such attacks without influencing the target model's performance. Inspired by research in neuron-coverage guided testing we show that dense layers of DNNs carry security-sensitive information. With a secondary DNN we analyze the activation patterns of the dense layers during classification run-time, which enables effective and real-time detection of adversarial examples. Our prototype implementation successfully detects adversarial examples in image, natural language, and audio processing. Thereby, we cover a variety of target DNN architectures. In addition to effectively defending against state-of-the-art attacks, our approach generalizes between different sets of adversarial examples. Our experiments indicate that we are able to detect future, yet unknown, attacks. Finally, during white-box adaptive attacks, we show our method cannot be easily bypassed.",multimedia
10.1109/MMRP.2019.00015,to_check,2019 International Workshop on Multilayer Music Representation and Processing (MMRP),IEEE,2019-01-24 00:00:00,ieeexplore,Heretic: Modeling Anthony Braxton's Language Music,https://ieeexplore.ieee.org/document/8665363/,"This article presents a new system for real-time machine listening within human-machine free improvisation. Heretic uses Anthony Braxton's Language Music system as a grammatical model for contextualizing real-time audio feature data within free improvisation. Heretic hears, recognizes, and organizes unseen musical material from a human improviser into a fluid, coherent, and expressive musical language. Systems similar to Heretic often prioritize agnostic approaches to machine listening by avoiding prior musical knowledge in the system's training stage. However, prominent improvisers such as Cecil Taylor, Ornette Coleman, Joe Morris, and Anthony Braxton detail their approaches to improvisation as languages or grammatical systems. These improvisers contextualize the real-time musical materials of their band-mates by applying their formulated grammatical systems to their decision-making processes. Taylor, Coleman, Morris, and Braxton's autonomy and musical creativity are not compromised by using grammatical systems. In regards to human-machine improvisation, Heretic demonstrates that a grammatical approach to machine listening can yield idiosyncratic interactions, full machine autonomy, and novel musical output. This article details a re-imagining of Anthony Braxton's Language Music within the context of machine listening, and an implementation of Language Music within Heretic via SuperCollider's audio feature extraction functionality and Wekinator's multi-layer perceptron neural networks.",multimedia
10.1109/IJCNN.2015.7280619,to_check,2015 International Joint Conference on Neural Networks (IJCNN),IEEE,2015-07-17 00:00:00,ieeexplore,Musical notes classification with neuromorphic auditory system using FPGA and a convolutional spiking network,https://ieeexplore.ieee.org/document/7280619/,"In this paper, we explore the capabilities of a sound classification system that combines both a novel FPGA cochlear model implementation and a bio-inspired technique based on a trained convolutional spiking network. The neuromorphic auditory system that is used in this work produces a form of representation that is analogous to the spike outputs of the biological cochlea. The auditory system has been developed using a set of spike-based processing building blocks in the frequency domain. They form a set of band pass filters in the spike-domain that splits the audio information in 128 frequency channels, 64 for each of two audio sources. Address Event Representation (AER) is used to communicate the auditory system with the convolutional spiking network. A layer of convolutional spiking network is developed and trained on a computer with the ability to detect two kinds of sound: artificial pure tones in the presence of white noise and electronic musical notes. After the training process, the presented system is able to distinguish the different sounds in real-time, even in the presence of white noise.",multimedia
10.1109/IoTDI.2018.00031,to_check,2018 IEEE/ACM Third International Conference on Internet-of-Things Design and Implementation (IoTDI),IEEE,2018-04-20 00:00:00,ieeexplore,PAWS: A Wearable Acoustic System for Pedestrian Safety,https://ieeexplore.ieee.org/document/8366992/,"With the prevalence of smartphones, pedestrians and joggers today often walk or run while listening to music. Since they are deprived of their auditory senses that would have provided important cues to dangers, they are at a much greater risk of being hit by cars or other vehicles. In this paper, we build a wearable system that uses multi-channel audio sensors embedded in a headset to help detect and locate cars from their honks, engine and tire noises, and warn pedestrians of imminent dangers of approaching cars. We demonstrate that using a segmented architecture and implementation consisting of headset-mounted audio sensors, a front-end hardware that performs signal processing and feature extraction, and machine learning based classification on a smartphone, we are able to provide early danger detection in real-time, from up to 60m distance, near 100% precision on the vehicle detection and alert the user with low latency.",multimedia
10.1109/ACCESS.2021.3082565,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Voice Pathology Detection and Classification by Adopting Online Sequential Extreme Learning Machine,https://ieeexplore.ieee.org/document/9438683/,"In the last decade, the implementation of machine learning algorithms in the analysis of voice disorder is paramount in order to provide a non-invasive voice pathology detection by only using audio signal. In spite of that, most recent systems of voice pathology work on a limited acoustic database. In other words, the systems use one vowel, such as /a/, and ignore sentences and other vowels when analyzing the audio signal. Other key issues that should be considered in the systems are accuracy and time consumption of an algorithm. Online Sequential Extreme Learning Machine (OSELM) is one of the machine learning algorithms that can be regarded as a rapid and accurate algorithm in the classification process. Therefore, this paper presents a voice pathology detection and classification system by using OSELM algorithm as a classifier, and Mel-frequency cepstral coefficient (MFCC) as a featured extraction. In this work, the voice samples were taken from the Saarbrücken voice database (SVD). This system involves two parts of the database; the first part includes all voices in SVD with sentences and vowels /a/, /i/, and /u/, which are uttered in high, low, and normal pitches; and the second part utilizes voice samples of the common three types of pathologies (cyst, polyp, and paralysis) based on the vowel /a/ that is produced in normal pitch. The experimental results have shown that OSELM was able to achieve the highest accuracy up to 91.17%, 94% of precision, and 91% of recall. Furthermore, OSELM obtained 87%, 87.55%, and 97.67% for f-measure, G-mean, and specificity, respectively. The proposed system also presents a high ability to achieve detection and classification results in real-time clinical applications.",multimedia
10.1109/ICNN.1995.488985,to_check,Proceedings of ICNN'95 - International Conference on Neural Networks,IEEE,1995-12-01 00:00:00,ieeexplore,A Bayesian neural network chip design for speech recognition system,https://ieeexplore.ieee.org/document/488985/,"The Bayesian neural network (BNN) has been widely used as speech recognition template which combines the merits of the dynamic programming (DP) and hidden Markov model (HMM) methods. However, it is computationally intensive and very costly to implement using DSP component. A single chip implementation of the BNN will drastically reduce the cost and the size of many speech recognition systems. It will also make low cost implementation of real-time speech recognition system possible. In this paper, the implementation of single BNN chip for the real-time speech recognizer is presented. Fabricated in 0.8 /spl mu/m double-metal CMOS technology, the chip contains approximately 13000 transistors which occupy a 3.1/spl times/3.2 mm/sup 2/ area and has been tested to be fully functional at IMS XL-60 tester.",multimedia
10.1109/ISEEE48094.2019.9136152,to_check,2019 6th International Symposium on Electrical and Electronics Engineering (ISEEE),IEEE,2019-10-20 00:00:00,ieeexplore,Compact Isolated Speech Recognition on Raspberry-Pi based on Reaction Diffusion Transform,https://ieeexplore.ieee.org/document/9136152/,"A low complexity solution for speech recognition is proposed and its implementation on a resources constrained platform, namely the Raspberry-Pi is evaluated. In order to achieve good performance with limited resources, both the feature extractor and the classifier are specially designed. A special form of feature extractor, called RDT (reaction-diffusion transform) was optimized and evaluated in conjunction with a specially designed ELM (extreme learning machine) classifier. Optimization of parameters led to very good recognition rates (up to 100%) for a small dictionary of isolated sounds representing vocal commands for automotive applications. The real time factor is sub-unitary, ensuring the realization of real time identification using the proposed method.",multimedia
10.1109/TENCON.1997.647303,to_check,TENCON '97 Brisbane - Australia. Proceedings of IEEE TENCON '97. IEEE Region 10 Annual Conference. Speech and Image Technologies for Computing and Telecommunications (Cat. No.97CH36162),IEEE,1997-12-04 00:00:00,ieeexplore,Implementation of an autoassociative recurrent neural network for speech recognition,https://ieeexplore.ieee.org/document/647303/,"This paper describes an implementation of a small vocabulary isolated word speech recognition system using a recurrent neural network and some of the extensions required for a large vocabulary forms. The network operates in a self-supervised manner by adjusting an internally generated segmentation of the speech input according to the algorithm proposed by Lee et al. (see IEEE Proceedings of the International Conference ASSP, vol.5, p.3319-22, 1995) and employs the recurrent real-time learning rule described by Williams and Zipser (1989).",multimedia
10.1109/VDAT50263.2020.9190415,to_check,2020 24th International Symposium on VLSI Design and Test (VDAT),IEEE,2020-07-25 00:00:00,ieeexplore,DynRP- Non-Intrusive Profiler for Dynamic Reconfigurability,https://ieeexplore.ieee.org/document/9190415/,"Emerging technological areas such as machine learning, speech recognition, computer vision, autonomous robots, AI, bioinformatics involving big data, require implementation in complex heterogeneous accelerator platforms, to be able to handle data explosion with higher efficiency, lower power, and better performance. Dynamic reconfiguration in such platforms can help in run-time optimization to meet the design goals. The required optimal platform configuration can be achieved by a flexible design space exploration and appropriate task partitioning obtained through profiling computation and communication of processes in application code. This paper focuses on profiling, it being the key to the success of obtaining optimal platform configurations. It points to existing profiling techniques, their pros and cons vis-à-vis dynamic reconfigurable architectures, and the challenges in their design for obtaining optimal profiling performance. It further outlines desirable specifications for a profiler to allow dynamic real-time profiling for effective use of dynamic reconfiguration. DynRP, a non-intrusive hardware profiler for dynamic reconfiguration is proposed based on the desirable specifications, followed by its design and implementation details.",multimedia
10.1109/ICPR.2018.8546084,to_check,2018 24th International Conference on Pattern Recognition (ICPR),IEEE,2018-08-24 00:00:00,ieeexplore,Dynamic Learning Rate for Neural Networks: A Fixed-Time Stability Approach,https://ieeexplore.ieee.org/document/8546084/,"Neural Networks (NN) have become important tools that have demonstrated their value solving complex problems regarding pattern recognition, natural language processing, automatic speech recognition, among others. Recently, the number of applications that require running the training process at the front-end in an online manner have increased dramatically. Unfortunately, in state-of-the-art (SoA) methods, this training process is an unbounded function of the initial conditions. Thus, there is no insight on the number of epochs required, making the online training a difficult problem. Speeding up the training process plays a key role in machine learning. In this work, an algorithm for dynamic learning rate is proposed based on recent results from fixed-time stability of continuous-time nonlinear systems, which ensures a convergence time bound to the equilibrium point independently of the initial conditions. We show experimentally that our discrete-time implementation presents promising results, proving that the number of epochs required for the training remains bounded, independently of the initial weights. This constitutes an important feature toward learning systems with real-time constraints. The efficiency of the method proposed is illustrated under different scenarios, including the public database MNIST, which shows that out algorithm outperforms SoA methods in terms of the number of epoch required for the training.",multimedia
10.1109/ICASSP.2014.6855060,to_check,"2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2014-05-09 00:00:00,ieeexplore,X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks,https://ieeexplore.ieee.org/document/6855060/,"Deep neural networks show very good performance in phoneme and speech recognition applications when compared to previously used GMM (Gaussian Mixture Model)-based ones. However, efficient implementation of deep neural networks is difficult because the network size needs to be very large when high recognition accuracy is demanded. In this work, we develop a digital VLSI for phoneme recognition using deep neural networks and assess the design in terms of throughput, chip size, and power consumption. The developed VLSI employs a fixed-point optimization method that only uses +Δ, 0, and -Δ for representing each of the weight. The design employs 1,024 simple processing units in each layer, which however can be scaled easily according to the needed throughput, and the throughput of the architecture varies from 62.5 to 1,000 times of the real-time processing speed.",multimedia
10.1109/TVLSI.2017.2717950,to_check,IEEE Transactions on Very Large Scale Integration (VLSI) Systems,IEEE,2017-10-01 00:00:00,ieeexplore,Accelerating Recurrent Neural Networks: A Memory-Efficient Approach,https://ieeexplore.ieee.org/document/7967731/,"Recurrent neural networks (RNNs) have achieved the state-of-the-art performance on various sequence learning tasks due to their powerful sequence modeling capability. However, RNNs usually require a large number of parameters and high computational complexity. Hence, it is quite challenging to implement complex RNNs on embedded devices with stringent memory and latency requirement. In this paper, we first present a novel hybrid compression method for a widely used RNN variant, long-short term memory (LSTM), to tackle these implementation challenges. By properly using circulant matrices, forward nonlinear function approximation, and efficient quantization schemes with a retrain-based training strategy, the proposed compression method can reduce more than 95% of memory usage with negligible accuracy loss when verified under language modeling and speech recognition tasks. An efficient scalable parallel hardware architecture is then proposed for the compressed LSTM. With an innovative chessboard division method for matrix-vector multiplications, the parallelism of the proposed hardware architecture can be freely chosen under certain latency requirement. Specifically, for the circulant matrix-vector multiplications employed in the compressed LSTM, the circulant matrices are judiciously reorganized to fit in with the chessboard division and minimize the number of memory accesses required for the matrix multiplications. The proposed architecture is modeled using register transfer language (RTL) and synthesized under the TSMC 90-nm CMOS technology. With 518.5-kB on-chip memory, we are able to process a 512×512 compressed LSTM in 1.71 μs, corresponding to 2.46 TOPS on the uncompressed one, at a cost of 30.77-mm<sup>2</sup> chip area. The implementation results demonstrate that the proposed design can achieve significantly high flexibility and area efficiency, which satisfies many real-time applications on embedded devices. It is worth mentioning that the memory-efficient approach of accelerating LSTM developed in this paper is also applicable to other RNN variants.",multimedia
10.1109/TNNLS.2016.2598655,to_check,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2017-11-01 00:00:00,ieeexplore,Online Training of an Opto-Electronic Reservoir Computer Applied to Real-Time Channel Equalization,https://ieeexplore.ieee.org/document/7553521/,"Reservoir computing is a bioinspired computing paradigm for processing time-dependent signals. The performance of its analog implementation is comparable to other state-of-the-art algorithms for tasks such as speech recognition or chaotic time series prediction, but these are often constrained by the offline training methods commonly employed. Here, we investigated the online learning approach by training an optoelectronic reservoir computer using a simple gradient descent algorithm, programmed on a field-programmable gate array chip. Our system was applied to wireless communications, a quickly growing domain with an increasing demand for fast analog devices to equalize the nonlinear distorted channels. We report error rates up to two orders of magnitude lower than previous implementations on this task. We show that our system is particularly well suited for realistic channel equalization by testing it on a drifting and a switching channel and obtaining good performances.",multimedia
10.1109/TCSI.2018.2852260,to_check,IEEE Transactions on Circuits and Systems I: Regular Papers,IEEE,2018-11-01 00:00:00,ieeexplore,Real-Time Embedded Machine Learning for Tensorial Tactile Data Processing,https://ieeexplore.ieee.org/document/8439040/,"Machine learning (ML) has increasingly been recently employed to provide solutions for difficult tasks, such as image and speech recognition, and tactile data processing achieving a near human decision accuracy. However, the efficient hardware implementation of ML algorithms in particular for real time applications is still a challenge. This paper presents the hardware architectures and implementation of a real time ML method based on tensorial kernel approach dealing with multidimensional input tensors. Two different hardware architectures are proposed and assessed. Results demonstrate the feasibility of the proposed implementations for real time classification. The proposed parallel architecture achieves a peak performance of 302 G-ops while consuming 1.14 W for the Virtex-7 XC7VX980T FPGA device overcoming state of the art solutions.",multimedia
10.1109/JSSC.2020.2968800,to_check,IEEE Journal of Solid-State Circuits,IEEE,2020-04-01 00:00:00,ieeexplore,"Vocell: A 65-nm Speech-Triggered Wake-Up SoC for 10-<inline-formula> <tex-math notation=""LaTeX"">$\mu$ </tex-math></inline-formula>W Keyword Spotting and Speaker Verification",https://ieeexplore.ieee.org/document/8978574/,"The use of speech-triggered wake-up interfaces has grown significantly in the last few years for use in ubiquitous and mobile devices. Since these interfaces must always be active, power consumption is one of their primary design metrics. This article presents a complete mixed-signal system-on-chip, capable of directly interfacing to an analog microphone and performing keyword spotting (KWS) and speaker verification (SV), without any need for further external accesses. Through the use of: 1) an integrated single-chip digital-friendly design; b) hardware-aware algorithmic optimization; and c) memoryand power-optimized accelerators, ultra-low power is achieved while maintaining high accuracy for speech recognition tasks. The 65-nm implementation achieves 18.3-μW worst case power consumption or 10.6-μW power for typical real-time scenarios, 10× below state of the art (SoA).",multimedia
10.1109/72.129420,to_check,IEEE Transactions on Neural Networks,IEEE,1992-05-01 00:00:00,ieeexplore,Voiced-speech representation by an analog silicon model of the auditory periphery,https://ieeexplore.ieee.org/document/129420/,"An analog CMOS integration of a model for the auditory periphery is presented. The model consists of middle ear, basilar membrane, and hair cell/synapse modules which are derived from neurophysiological studies. The circuit realization of each module is discussed, and experimental data of each module's response to sinusoidal excitation are given. The nonlinear speech processing capabilities of the system are demonstrated using the voiced syllable mod ba mod . The multichannel output of the silicon model corresponds to the time-varying instantaneous firing rates of auditory nerve fibers that have different characteristic frequencies. These outputs are similar to the physiologically obtained responses. The actual implementation uses subthreshold CMOS technology and analog continuous-time circuits, resulting in a real-time, micropower device with potential applications as a preprocessor of auditory stimuli.&lt;<ETX>&gt;</ETX>",multimedia
10.1109/PDCAT.2011.40,to_check,"2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies",IEEE,2011-10-22 00:00:00,ieeexplore,Fast Estimation of Gaussian Mixture Model Parameters on GPU Using CUDA,https://ieeexplore.ieee.org/document/6118944/,"Gaussian Mixture Models (GMMs) are widely used among scientists e.g. in statistics toolkits and data mining procedures. In order to estimate parameters of a GMM the Maximum Likelihood (ML) training is often utilized, more precisely the Expectation-Maximization (EM) algorithm. Nowadays, a lot of tasks works with huge datasets, what makes the estimation process time consuming (mainly for complex mixture models containing hundreds of components). The paper presents an efficient and robust implementation of the estimation of GMM statistics used in the EM algorithm on GPU using NVIDIA's Compute Unified Device Architecture (CUDA). Also an augmentation of the standard CPU version is proposed utilizing SSE instructions. Time consumptions of presented methods are tested on a large dataset of real speech data from the NIST Speaker Recognition Evaluation (SRE) 2008. Estimation on GPU proves to be more than 400 times faster than the standard CPU version and 130 times faster than the SSE version, thus a huge speed up was achieved without any approximations made in the estimation formulas. Proposed implementation was also compared to other implementations developed by other departments over the world and proved to be the fastest (at least 5 times faster than the best implementation published recently).",multimedia
10.23919/DATE48585.2020.9116560,to_check,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",IEEE,2020-03-13 00:00:00,ieeexplore,Offline Model Guard: Secure and Private ML on Mobile Devices,https://ieeexplore.ieee.org/document/9116560/,"Performing machine learning tasks in mobile applications yields a challenging conflict of interest: highly sensitive client information (e.g., speech data) should remain private while also the intellectual property of service providers (e.g., model parameters) must be protected. Cryptographic techniques offer secure solutions for this, but have an unacceptable overhead and moreover require frequent network interaction.In this work, we design a practically efficient hardware-based solution. Specifically, we build OFFLINE MODEL GUARD (OMG) to enable privacy-preserving machine learning on the predominant mobile computing platform ARM-even in offline scenarios. By leveraging a trusted execution environment for strict hardware-enforced isolation from other system components, OMG guarantees privacy of client data, secrecy of provided models, and integrity of processing algorithms. Our prototype implementation on an ARM HiKey 960 development board performs privacy-preserving keyword recognition using TensorFlow Lite for Microcontrollers in real time.",multimedia
10.1109/WASPAA.2013.6701825,to_check,2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics,IEEE,2013-10-23 00:00:00,ieeexplore,Spectral feature-based nonlinear residual echo suppression,https://ieeexplore.ieee.org/document/6701825/,"We propose a method for nonlinear residual echo suppression that consists of extracting spectral features from the far-end signal, and using an artificial neural network to model the residual echo magnitude spectrum from these features. We compare the modeling accuracy achieved by realizations with different features and network topologies, evaluating the mean squared error of the estimated residual echo magnitude spectrum. We also present a low complexity real-time implementation combining an offline-trained network with online adaptation, and investigate its performance in terms of echo suppression and speech distortion for real mobile phone recordings.",multimedia
10.1109/ICME.2002.1035657,to_check,Proceedings. IEEE International Conference on Multimedia and Expo,IEEE,2002-08-29 00:00:00,ieeexplore,Texture classification based on multiple Gauss mixture vector quantizers,https://ieeexplore.ieee.org/document/1035657/,"We propose a texture classification method using multiple Gauss mixture vector quantizers (GMVQ). We designed a separate model codebook or Gauss mixture for each texture using the generalized Lloyd algorithm with a minimum discrimination information (MDI) distortion based on a training data set. The multi-codebook structure of the GMVQ classifier is an extension to images of the isolated utterance speech recognizer of J.E. Shore and D. Burton (see Proc. Int. Conf. Acoust., Speech, and Sig. Processing, IEEE82Ch.1746-7, p.907-10, 1982). We applied the algorithm to the Brodatz texture database and showed it to be competitive in performance in comparison to other texture classifiers. Its low complexity implementation and real-time operation make the approach suitable for content-based image retrieval.",multimedia
10.1109/TCAD.2020.3012320,to_check,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,IEEE,2020-11-01 00:00:00,ieeexplore,UltraTrail: A Configurable Ultralow-Power TC-ResNet AI Accelerator for Efficient Keyword Spotting,https://ieeexplore.ieee.org/document/9216480/,"Recent advances in machine learning show the superior behavior of temporal convolutional networks (TCNs) and especially their combination with residual networks (TC-ResNet) for intelligent sensor signal processing in comparison to classical CNNs and LSTMs. In this article, we propose UltraTrail, a configurable, ultralow-power TC-ResNet AI accelerator for sensor signal processing and its application to efficient keyword spotting (KWS). Following a strict hardware/model co-design approach, we have derived an optimized low-power hardware architecture for generalized TC-ResNet topologies consisting of a configurable array of processing elements and a distributed memory with dynamic content reallocation. We additionally extend the network with conditional computing to reduce the number of operations during inference and to provide the possibility for power-gating. The final accelerator implementation in Globalfoundries' 22FDX technology achieves a power consumption of 8.2 μW for the task of always-on KWS meeting the real-time requirement of 100 ms per inference with an accuracy of 93% on the Google Speech Command Dataset.",multimedia
10.1007/978-3-319-13650-9_16,to_check,Nature-Inspired Computation and Machine Learning,Springer,2014-01-01 00:00:00,springer,A Hierarchical Reinforcement Learning Based Artificial Intelligence for Non-Player Characters in Video Games,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-319-13650-9_16,"Nowadays, video games conforms a huge industry that is always developing new technology. In particular, artificial intelligence techniques have been used broadly in the well-known non-player characters (NPC) given the opportunity to users to feel video games more real. This paper proposes the usage of the MaxQ-Q hierarchical reinforcement learning algorithm in non-player characters in order to increase the experience of the user in terms of naturalness. A case study of an NPC with the proposed artificial intelligence based algorithm in a first personal shooter video game was developed. Experimental results show that this implementation improves naturalness from the user’s point of view. In addition, the proposed MaxQ-Q based algorithm in NPCs allow to programmers a robust way to give artificial intelligence to them.",multimedia
http://arxiv.org/abs/2102.09360v1,to_check,arxiv,arxiv,2021-02-18 14:12:24+00:00,arxiv,"All-optical spiking neurosynaptic networks with self-learning
  capabilities",http://arxiv.org/abs/2102.09360v1,"Software-implementation, via neural networks, of brain-inspired computing
approaches underlie many important modern-day computational tasks, from image
processing to speech recognition, artificial intelligence and deep learning
applications. Yet, differing from real neural tissue, traditional computing
architectures physically separate the core computing functions of memory and
processing, making fast, efficient and low-energy brain-like computing
difficult to achieve. To overcome such limitations, an attractive and
alternative goal is to design direct hardware mimics of brain neurons and
synapses which, when connected in appropriate networks (or neuromorphic
systems), process information in a way more fundamentally analogous to that of
real brains. Here we present an all-optical approach to achieving such a goal.
Specifically, we demonstrate an all-optical spiking neuron device and connect
it, via an integrated photonics network, to photonic synapses to deliver a
small-scale all-optical neurosynaptic system capable of supervised and
unsupervised learning. Moreover, we exploit wavelength division multiplexing
techniques to implement a scalable circuit architecture for photonic neural
networks, successfully demonstrating pattern recognition directly in the
optical domain using a photonic system comprising 140 elements. Such optical
implementations of neurosynaptic networks promise access to the high speed and
bandwidth inherent to optical systems, which would be very attractive for the
direct processing of telecommunication and visual data in the optical domain.",multimedia
http://arxiv.org/abs/1609.06612v1,to_check,arxiv,arxiv,2016-09-21 15:59:59+00:00,arxiv,Multimedia Communication Quality Assessment Testbeds,http://arxiv.org/abs/1609.06612v1,"We make an intensive use of multimedia frameworks in our research on modeling
the perceived quality estimation in streaming services and real-time
communications. In our preliminary work, we have used the VLC VOD software to
generate reference audiovisual files with various degree of coding and network
degradations. We have successfully built machine learning based models on the
subjective quality dataset we have generated using these files. However,
imperfections in the dataset introduced by the multimedia framework we have
used prevented us from achieving the full potential of these models.
  In order to develop better models, we have re-created our end-to-end
multimedia pipeline using the GStreamer framework for audio and video
streaming. A GStreamer based pipeline proved to be significantly more robust to
network degradations than the VLC VOD framework and allowed us to stream a
video flow at a loss rate up to 5\% packet very easily. GStreamer has also
enabled us to collect the relevant RTCP statistics that proved to be more
accurate than network-deduced information. This dataset is free to the public.
The accuracy of the statistics eventually helped us to generate better
performing perceived quality estimation models.
  In this paper, we present the implementation of these VLC and GStreamer-based
multimedia communication quality assessment testbeds with the references to
their publicly available code bases.",multimedia
http://arxiv.org/abs/1910.12750v1,to_check,arxiv,arxiv,2019-10-28 15:21:48+00:00,arxiv,"Deep-Learning-Based Image Segmentation Integrated with Optical
  Microscopy for Automatically Searching for Two-Dimensional Materials",http://arxiv.org/abs/1910.12750v1,"Deep-learning algorithms enable precise image recognition based on
high-dimensional hierarchical image features. Here, we report the development
and implementation of a deep-learning-based image segmentation algorithm in an
autonomous robotic system to search for two-dimensional (2D) materials. We
trained the neural network based on Mask-RCNN on annotated optical microscope
images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm
is run on a 1024 x 1024 px2 optical microscope images for 200 ms, enabling the
real-time detection of 2D materials. The detection process is robust against
changes in the microscopy conditions, such as illumination and color balance,
which obviates the parameter-tuning process required for conventional
rule-based detection algorithms. Integrating the algorithm with a motorized
optical microscope enables the automated searching and cataloging of 2D
materials. This development will allow researchers to utilize unlimited amounts
of 2D materials simply by exfoliating and running the automated searching
process.",multimedia
http://arxiv.org/abs/2103.07220v1,to_check,arxiv,arxiv,2021-03-12 11:49:51+00:00,arxiv,Real-time Timbre Transfer and Sound Synthesis using DDSP,http://arxiv.org/abs/2103.07220v1,"Neural audio synthesis is an actively researched topic, having yielded a wide
range of techniques that leverages machine learning architectures. Google
Magenta elaborated a novel approach called Differential Digital Signal
Processing (DDSP) that incorporates deep neural networks with preconditioned
digital signal processing techniques, reaching state-of-the-art results
especially in timbre transfer applications. However, most of these techniques,
including the DDSP, are generally not applicable in real-time constraints,
making them ineligible in a musical workflow. In this paper, we present a
real-time implementation of the DDSP library embedded in a virtual synthesizer
as a plug-in that can be used in a Digital Audio Workstation. We focused on
timbre transfer from learned representations of real instruments to arbitrary
sound inputs as well as controlling these models by MIDI. Furthermore, we
developed a GUI for intuitive high-level controls which can be used for
post-processing and manipulating the parameters estimated by the neural
network. We have conducted a user experience test with seven participants
online. The results indicated that our users found the interface appealing,
easy to understand, and worth exploring further. At the same time, we have
identified issues in the timbre transfer quality, in some components we did not
implement, and in installation and distribution of our plugin. The next
iteration of our design will address these issues. Our real-time MATLAB and
JUCE implementations are available at https://github.com/SMC704/juce-ddsp and
https://github.com/SMC704/matlab-ddsp , respectively.",multimedia
http://arxiv.org/abs/1912.11350v1,to_check,arxiv,arxiv,2019-12-22 22:22:55+00:00,arxiv,Atmospheric turbulence removal using convolutional neural network,http://arxiv.org/abs/1912.11350v1,"This paper describes a novel deep learning-based method for mitigating the
effects of atmospheric distortion. We have built an end-to-end supervised
convolutional neural network (CNN) to reconstruct turbulence-corrupted video
sequence. Our framework has been developed on the residual learning concept,
where the spatio-temporal distortions are learnt and predicted. Our experiments
demonstrate that the proposed method can deblur, remove ripple effect and
enhance contrast of the video sequences simultaneously. Our model was trained
and tested with both simulated and real distortions. Experimental results of
the real distortions show that our method outperforms the existing ones by up
to 3.8% in term of the quality of restored images, and it achieves faster speed
than the state-of-the-art methods by up to 23 times with GPU implementation.",multimedia
http://arxiv.org/abs/2011.02833v3,to_check,arxiv,arxiv,2020-11-02 19:08:49+00:00,arxiv,"Digital Twins: State of the Art Theory and Practice, Challenges, and
  Open Research Questions",http://arxiv.org/abs/2011.02833v3,"Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins.",multimedia
http://arxiv.org/abs/2104.08002v1,to_check,arxiv,arxiv,2021-04-16 09:54:30+00:00,arxiv,Efficient and Generic 1D Dilated Convolution Layer for Deep Learning,http://arxiv.org/abs/2104.08002v1,"Convolutional neural networks (CNNs) have found many applications in tasks
involving two-dimensional (2D) data, such as image classification and image
processing. Therefore, 2D convolution layers have been heavily optimized on
CPUs and GPUs. However, in many applications - for example genomics and speech
recognition, the data can be one-dimensional (1D). Such applications can
benefit from optimized 1D convolution layers. In this work, we introduce our
efficient implementation of a generic 1D convolution layer covering a wide
range of parameters. It is optimized for x86 CPU architectures, in particular,
for architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions.
We use the LIBXSMM library's batch-reduce General Matrix Multiplication
(BRGEMM) kernel for FP32 and BFloat16 precision. We demonstrate that our
implementation can achieve up to 80% efficiency on Intel Xeon Cascade Lake and
Cooper Lake CPUs. Additionally, we show the generalization capability of our
BRGEMM based approach by achieving high efficiency across a range of
parameters. We consistently achieve higher efficiency than the 1D convolution
layer with Intel oneDNN library backend for varying input tensor widths, filter
widths, number of channels, filters, and dilation parameters. Finally, we
demonstrate the performance of our optimized 1D convolution layer by utilizing
it in the end-to-end neural network training with real genomics datasets and
achieve up to 6.86x speedup over the oneDNN library-based implementation on
Cascade Lake CPUs. We also demonstrate the scaling with 16 sockets of
Cascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs
using a similar power envelop. In the end-to-end training, we get a speedup of
1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on
Cooper Lake with BFloat16 over eight V100 GPUs with FP32.",multimedia
http://arxiv.org/abs/2006.01804v2,to_check,arxiv,arxiv,2020-06-02 17:39:32+00:00,arxiv,"Practical sensorless aberration estimation for 3D microscopy with deep
  learning",http://arxiv.org/abs/2006.01804v2,"Estimation of optical aberrations from volumetric intensity images is a key
step in sensorless adaptive optics for 3D microscopy. Recent approaches based
on deep learning promise accurate results at fast processing speeds. However,
collecting ground truth microscopy data for training the network is typically
very difficult or even impossible thereby limiting this approach in practice.
Here, we demonstrate that neural networks trained only on simulated data yield
accurate predictions for real experimental images. We validate our approach on
simulated and experimental datasets acquired with two different microscopy
modalities, and also compare the results to non-learned methods. Additionally,
we study the predictability of individual aberrations with respect to their
data requirements and find that the symmetry of the wavefront plays a crucial
role. Finally, we make our implementation freely available as open source
software in Python.",multimedia
http://arxiv.org/abs/2003.11100v1,to_check,arxiv,arxiv,2020-03-24 20:15:12+00:00,arxiv,"How deep is your encoder: an analysis of features descriptors for an
  autoencoder-based audio-visual quality metric",http://arxiv.org/abs/2003.11100v1,"The development of audio-visual quality assessment models poses a number of
challenges in order to obtain accurate predictions. One of these challenges is
the modelling of the complex interaction that audio and visual stimuli have and
how this interaction is interpreted by human users. The No-Reference
Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with
this problem from a machine learning perspective. The metric receives two sets
of audio and video features descriptors and produces a low-dimensional set of
features used to predict the audio-visual quality. A basic implementation of
NAViDAd was able to produce accurate predictions tested with a range of
different audio-visual databases. The current work performs an ablation study
on the base architecture of the metric. Several modules are removed or
re-trained using different configurations to have a better understanding of the
metric functionality. The results presented in this study provided important
feedback that allows us to understand the real capacity of the metric's
architecture and eventually develop a much better audio-visual quality metric.",multimedia
http://arxiv.org/abs/2007.15152v2,to_check,arxiv,arxiv,2020-07-29 23:41:33+00:00,arxiv,"Accelerating Multi-attribute Unsupervised Seismic Facies Analysis With
  RAPIDS",http://arxiv.org/abs/2007.15152v2,"Classification of seismic facies is done by clustering seismic data samples
based on their attributes. Year after year, 3D datasets used by exploration
geophysics increase in size, complexity, and number of attributes, requiring a
continuous rise in the classification performance. In this work, we explore the
use of Graphics Processing Units (GPUs) to perform the classification of
seismic surveys using the well-established Machine Learning (ML) method
k-means. We show that the high-performance distributed implementation of the
k-means algorithm available at the RAPIDS library can be used to classify
facies in large seismic datasets much faster than a classical parallel CPU
implementation (up to 258-fold faster in NVIDIA V100 GPUs), especially for
large seismic blocks. We tested the algorithm with different real seismic
volumes, including Netherlands, Parihaka, and Kahu (from 12GB to 66GB).",multimedia
http://arxiv.org/abs/1711.00541v2,to_check,arxiv,arxiv,2017-11-01 21:19:22+00:00,arxiv,"TasNet: time-domain audio separation network for real-time,
  single-channel speech separation",http://arxiv.org/abs/1711.00541v2,"Robust speech processing in multi-talker environments requires effective
speech separation. Recent deep learning systems have made significant progress
toward solving this problem, yet it remains challenging particularly in
real-time, short latency applications. Most methods attempt to construct a mask
for each source in time-frequency representation of the mixture signal which is
not necessarily an optimal representation for speech separation. In addition,
time-frequency decomposition results in inherent problems such as
phase/magnitude decoupling and long time window which is required to achieve
sufficient frequency resolution. We propose Time-domain Audio Separation
Network (TasNet) to overcome these limitations. We directly model the signal in
the time-domain using an encoder-decoder framework and perform the source
separation on nonnegative encoder outputs. This method removes the frequency
decomposition step and reduces the separation problem to estimation of source
masks on encoder outputs which is then synthesized by the decoder. Our system
outperforms the current state-of-the-art causal and noncausal speech separation
algorithms, reduces the computational cost of speech separation, and
significantly reduces the minimum required latency of the output. This makes
TasNet suitable for applications where low-power, real-time implementation is
desirable such as in hearable and telecommunication devices.",multimedia
http://arxiv.org/abs/1905.03554v1,to_check,arxiv,arxiv,2019-05-09 11:52:10+00:00,arxiv,1D Convolutional Neural Networks and Applications: A Survey,http://arxiv.org/abs/1905.03554v1,"During the last decade, Convolutional Neural Networks (CNNs) have become the
de facto standard for various Computer Vision and Machine Learning operations.
CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating
convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and
millions of parameters have the ability to learn complex objects and patterns
providing that they can be trained on a massive size visual database with
ground-truth labels. With a proper training, this unique ability makes them the
primary tool for various engineering applications for 2D signals such as images
and video frames. Yet, this may not be a viable option in numerous applications
over 1D signals especially when the training data is scarce or
application-specific. To address this issue, 1D CNNs have recently been
proposed and immediately achieved the state-of-the-art performance levels in
several applications such as personalized biomedical data classification and
early diagnosis, structural health monitoring, anomaly detection and
identification in power electronics and motor-fault detection. Another major
advantage is that a real-time and low-cost hardware implementation is feasible
due to the simple and compact configuration of 1D CNNs that perform only 1D
convolutions (scalar multiplications and additions). This paper presents a
comprehensive review of the general architecture and principals of 1D CNNs
along with their major engineering applications, especially focused on the
recent progress in this field. Their state-of-the-art performance is
highlighted concluding with their unique properties. The benchmark datasets and
the principal 1D CNN software used in those applications are also publically
shared in a dedicated website.",multimedia
http://arxiv.org/abs/2103.11052v1,to_check,arxiv,arxiv,2021-03-19 22:48:03+00:00,arxiv,"A first step towards automated species recognition from camera trap
  images of mammals using AI in a European temperate forest",http://arxiv.org/abs/2103.11052v1,"Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices.",multimedia
http://arxiv.org/abs/1209.6393v1,to_check,arxiv,arxiv,2012-09-27 23:03:53+00:00,arxiv,Learning Robust Low-Rank Representations,http://arxiv.org/abs/1209.6393v1,"In this paper we present a comprehensive framework for learning robust
low-rank representations by combining and extending recent ideas for learning
fast sparse coding regressors with structured non-convex optimization
techniques. This approach connects robust principal component analysis (RPCA)
with dictionary learning techniques and allows its approximation via trainable
encoders. We propose an efficient feed-forward architecture derived from an
optimization algorithm designed to exactly solve robust low dimensional
projections. This architecture, in combination with different training
objective functions, allows the regressors to be used as online approximants of
the exact offline RPCA problem or as RPCA-based neural networks. Simple
modifications of these encoders can handle challenging extensions, such as the
inclusion of geometric data transformations. We present several examples with
real data from image, audio, and video processing. When used to approximate
RPCA, our basic implementation shows several orders of magnitude speedup
compared to the exact solvers with almost no performance degradation. We show
the strength of the inclusion of learning to the RPCA approach on a music
source separation application, where the encoders outperform the exact RPCA
algorithms, which are already reported to produce state-of-the-art results on a
benchmark database. Our preliminary implementation on an iPad shows
faster-than-real-time performance with minimal latency.",multimedia
http://arxiv.org/abs/2108.00516v1,to_check,arxiv,arxiv,2021-08-01 18:14:46+00:00,arxiv,"BundleTrack: 6D Pose Tracking for Novel Objects without Instance or
  Category-Level 3D Models",http://arxiv.org/abs/2108.00516v1,"Tracking the 6D pose of objects in video sequences is important for robot
manipulation. Most prior efforts, however, often assume that the target
object's CAD model, at least at a category-level, is available for offline
training or during online template matching. This work proposes BundleTrack, a
general framework for 6D pose tracking of novel objects, which does not depend
upon 3D models, either at the instance or category-level. It leverages the
complementary attributes of recent advances in deep learning for segmentation
and robust feature extraction, as well as memory-augmented pose graph
optimization for spatiotemporal consistency. This enables long-term, low-drift
tracking under various challenging scenarios, including significant occlusions
and object motions. Comprehensive experiments given two public benchmarks
demonstrate that the proposed approach significantly outperforms state-of-art,
category-level 6D tracking or dynamic SLAM methods. When compared against
state-of-art methods that rely on an object instance CAD model, comparable
performance is achieved, despite the proposed method's reduced information
requirements. An efficient implementation in CUDA provides a real-time
performance of 10Hz for the entire framework. Code is available at:
https://github.com/wenbowen123/BundleTrack",multimedia
http://arxiv.org/abs/2106.03167v2,to_check,arxiv,arxiv,2021-06-06 16:16:08+00:00,arxiv,"Mathematical Vocoder Algorithm : Modified Spectral Inversion for
  Efficient Neural Speech Synthesis",http://arxiv.org/abs/2106.03167v2,"In this work, we propose a new mathematical vocoder algorithm(modified
spectral inversion) that generates a waveform from acoustic features without
phase estimation. The main benefit of using our proposed method is that it
excludes the training stage of the neural vocoder from the end-to-end speech
synthesis model. Our implementation can synthesize high fidelity speech at
approximately 20 Mhz on CPU and 59.6MHz on GPU. This is 909 and 2,702 times
faster compared to real-time. Since the proposed methodology is not a
data-driven method, it is applicable to unseen voices and multiple languages
without any additional work. The proposed method is expected to adapt for
researching on neural network models capable of synthesizing speech at the
studio recording level.",multimedia
http://arxiv.org/abs/1903.10883v1,to_check,arxiv,arxiv,2019-03-25 15:40:34+00:00,arxiv,Generalized Feedback Loop for Joint Hand-Object Pose Estimation,http://arxiv.org/abs/1903.10883v1,"We propose an approach to estimating the 3D pose of a hand, possibly handling
an object, given a depth image. We show that we can correct the mistakes made
by a Convolutional Neural Network trained to predict an estimate of the 3D pose
by using a feedback loop. The components of this feedback loop are also Deep
Networks, optimized using training data. This approach can be generalized to a
hand interacting with an object. Therefore, we jointly estimate the 3D pose of
the hand and the 3D pose of the object. Our approach performs en-par with
state-of-the-art methods for 3D hand pose estimation, and outperforms
state-of-the-art methods for joint hand-object pose estimation when using depth
images only. Also, our approach is efficient as our implementation runs in
real-time on a single GPU.",multimedia
http://arxiv.org/abs/2002.10718v2,to_check,arxiv,arxiv,2020-02-25 08:04:31+00:00,arxiv,"Denoising IMU Gyroscopes with Deep Learning for Open-Loop Attitude
  Estimation",http://arxiv.org/abs/2002.10718v2,"This paper proposes a learning method for denoising gyroscopes of Inertial
Measurement Units (IMUs) using ground truth data, and estimating in real time
the orientation (attitude) of a robot in dead reckoning. The obtained algorithm
outperforms the state-of-the-art on the (unseen) test sequences. The obtained
performances are achieved thanks to a well-chosen model, a proper loss function
for orientation increments, and through the identification of key points when
training with high-frequency inertial data. Our approach builds upon a neural
network based on dilated convolutions, without requiring any recurrent neural
network. We demonstrate how efficient our strategy is for 3D attitude
estimation on the EuRoC and TUM-VI datasets. Interestingly, we observe our dead
reckoning algorithm manages to beat top-ranked visual-inertial odometry systems
in terms of attitude estimation although it does not use vision sensors. We
believe this paper offers new perspectives for visual-inertial localization and
constitutes a step toward more efficient learning methods involving IMUs. Our
open-source implementation is available at
https://github.com/mbrossar/denoise-imu-gyro.",multimedia
http://arxiv.org/abs/1911.01921v1,to_check,arxiv,arxiv,2019-11-05 16:31:23+00:00,arxiv,DLA: Dense-Layer-Analysis for Adversarial Example Detection,http://arxiv.org/abs/1911.01921v1,"In recent years Deep Neural Networks (DNNs) have achieved remarkable results
and even showed super-human capabilities in a broad range of domains. This led
people to trust in DNNs' classifications and resulting actions even in
security-sensitive environments like autonomous driving.
  Despite their impressive achievements, DNNs are known to be vulnerable to
adversarial examples. Such inputs contain small perturbations to intentionally
fool the attacked model.
  In this paper, we present a novel end-to-end framework to detect such attacks
during classification without influencing the target model's performance.
Inspired by recent research in neuron-coverage guided testing we show that
dense layers of DNNs carry security-sensitive information. With a secondary DNN
we analyze the activation patterns of the dense layers during classification
runtime, which enables effective and real-time detection of adversarial
examples.
  Our prototype implementation successfully detects adversarial examples in
image, natural language, and audio processing. Thereby, we cover a variety of
target DNNs, including Long Short Term Memory (LSTM) architectures. In
addition, to effectively defend against state-of-the-art attacks, our approach
generalizes between different sets of adversarial examples. Thus, our method
most likely enables us to detect even future, yet unknown attacks. Finally,
during white-box adaptive attacks, we show our method cannot be easily
bypassed.",multimedia
10.1016/j.media.2021.102171,to_check,Medical Image Analysis,scopus,2021-10-01,sciencedirect,Automatic skull defect restoration and cranial implant generation for cranioplasty,https://api.elsevier.com/content/abstract/scopus_id/85111529504,"A fast and fully automatic design of 3D printed patient-specific cranial implants is highly desired in cranioplasty - the process to restore a defect on the skull. We formulate skull defect restoration as a 3D volumetric shape completion task, where a partial skull volume is completed automatically. The difference between the completed skull and the partial skull is the restored defect; in other words, the implant that can be used in cranioplasty. To fulfill the task of volumetric shape completion, a fully data-driven approach is proposed. Supervised skull shape learning is performed on a database containing 167 high-resolution healthy skulls. In these skulls, synthetic defects are injected to create training and evaluation data pairs. We propose a patch-based training scheme tailored for dealing with high-resolution and spatially sparse data, which overcomes the disadvantages of conventional patch-based training methods in high-resolution volumetric shape completion tasks. In particular, the conventional patch-based training is applied to images of high resolution and proves to be effective in tasks such as segmentation. However, we demonstrate the limitations of conventional patch-based training for shape completion tasks, where the overall shape distribution of the target has to be learnt, since it cannot be captured efficiently by a sub-volume cropped from the target. Additionally, the standard dense implementation of a convolutional neural network tends to perform poorly on sparse data, such as the skull, which has a low voxel occupancy rate. Our proposed training scheme encourages a convolutional neural network to learn from the high-resolution and spatially sparse data. In our study, we show that our deep learning models, trained on healthy skulls with synthetic defects, can be transferred directly to craniotomy skulls with real defects of greater irregularity, and the results show promise for clinical use. Project page: https://github.com/Jianningli/MIA.",multimedia
10.1016/j.buildenv.2021.107929,to_check,Building and Environment,scopus,2021-08-01,sciencedirect,MOOSAS – A systematic solution for multiple objective building performance optimization in the early design stage,https://api.elsevier.com/content/abstract/scopus_id/85105785192,"There is great potential for building performance optimization (BPO) in the early design stage, but there is still a lack of methods, algorithms, and tools to support the BPO process in this stage. Through a comprehensive review, this study identified three critical issues that affect the implementation of the BPO process in the early design stage: model integration, real-time performance analysis, and interactive optimization design. This study provides a systematic solution to these three critical issues. In terms of model integration, a feature-based and graph-based 3D building space recognition algorithm is proposed to automatically convert the computer-aided design (CAD model) into a computer-aided engineering model (CAE model). In terms of real-time performance analysis, a simplified physical method, an HPC-accelerated method, and an AI-based method are explored, and a real-time energy modeling module and a real-time daylighting analysis module are developed. In terms of the interactive optimization design, a preference-based multi-objective BPO design algorithm that can consider user preferences is proposed to make full use of the decision-making ability of humans and the computing power of machines and significantly improve the optimization efficiency and result satisfaction. Based on the systematic solution, a multi-objective BPO design software, MOOSAS, is developed. MOOSAS allows real-time performance feedback, dynamic parameter analysis, and interactive optimization, supporting the BPO process in the early design stage. The innovations of this study are as follows: first, this study proposes a systematic solution to the three critical issues of the BPO process, i.e., model integration, real-time performance analysis, and interactive optimization design; second, this study develops a multi-objective BPO design software (MOOSAS) for the early design stage.",multimedia
10.1016/j.wasman.2020.09.032,to_check,Waste Management,scopus,2021-01-01,sciencedirect,Detecting glass and metal in consumer trash bags during waste collection using convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85092313671,"We present a proof-of-concept method to classify the presence of glass and metal in consumer trash bags. With the prevalent utilization of waste collection trucks in municipal solid waste management, the aim of this method is to help pinpoint the locations where waste sorting quality is below accepted standards, making it possible and more efficient to develop tailored procedures that can improve the waste sorting quality in areas with the most urgent needs. Using trash bags containing various amounts of glass and metal, in addition to common waste found in households, we use a combination of sound recording and a beat-frequency oscillation metal detector as inputs to a machine learning algorithm to identify the occurrence of glass and metal in trash bags. A custom-built test rig was developed to mimic a real waste collection truck, which was used to test different sensors and build the datasets. Convolutional neural networks were trained for the classification task, achieving accuracies of up to 98%. These promising results support this method’s potential implementation in real waste collection trucks, enabling location-specific and long-term monitoring of consumer waste sorting quality, which can provide decision support for waste management systems, and research on consumer behavior.",multimedia
10.1109/WCCIT.2013.6618739,to_check,2013 World Congress on Computer and Information Technology (WCCIT),IEEE,2013-06-24 00:00:00,ieeexplore,Design and application of academic frontier-based approach in engineering courses,https://ieeexplore.ieee.org/document/6618739/,"The reform of teaching approach has been the focus of modern opening educational reform and research. The real science thrives on both revolutionary and frontier progress that the textbook can not include. Based on the analysis of the characteristics of several the engineering courses, we build the academic frontier-based approach (AFA) for the purpose of combination of the basic and academic frontier knowledge by the application of the theory of constructivist learning. AFA is a learner-centred instructional approach used to promote active and deep learning by involving learners in learning academic frontier topics in an open and collaborative environment. The design and implementation of the approach enrich the teaching mode and the content of the key curriculums in depth, have an effective role in expansion knowledge, fun learning, and dig the potential of the learners in the group and team.",science
10.1109/JBHI.2018.2869779,to_check,IEEE Journal of Biomedical and Health Informatics,IEEE,2019-07-01 00:00:00,ieeexplore,Classifier Personalization for Activity Recognition Using Wrist Accelerometers,https://ieeexplore.ieee.org/document/8462755/,"Intersubject variability in accelerometer-based activity recognition may significantly affect classification accuracy, limiting a reliable extension of methods to new users. In this paper, we propose an approach for personalizing classification rules to a single person. We demonstrate that the method improves activity detection from wrist-worn accelerometer data on a four-class recognition problem of interest to the exercise science community, where classes are ambulation, cycling, sedentary, and other. We extend a previously published activity classification method based on support vector machines so that it estimates classification uncertainty. Uncertainty is used to drive data label requests from the user, and the resulting label information is used to update the classifier. Two different datasets- one from 33 adults with 26 activity types, and another from 20 youth with 23 activity types-were used to evaluate the method using leave-one-subject-out and leave-one-groupout cross validation. The new method improved overall recognition accuracy up to 11% on average, with some large person-specific improvements (ranging from -2% to +36%). The proposed method is suitable for online implementation supporting real-time recognition systems.",science
10.1109/BMAS.2008.4751244,to_check,2008 IEEE International Behavioral Modeling and Simulation Workshop,IEEE,2008-09-26 00:00:00,ieeexplore,An Implementation of a Biological Neural Model using Analog-Digital Integrated Circuits,https://ieeexplore.ieee.org/document/4751244/,"Given the trends in reconfigurable hardware systems inspired by biology, we present a hardware implementation of a closed-loop neural system. The hardware implementation focuses on modeling the behavior of two-cells, PD-LP system of a Pyloric Network from a lobster's stomach. This two-cell network emulates, in real-time, a digital representation of interacting neurons whose biological behavior is known. We evaluated the circuit design by varying the circuit values to determine the appropriateness and range of operation of the model. Future development of hardware models will be used to evaluate the feasibility of creating a platform of specialized circuits or an FPNA of biological neural characteristics.",science
10.1109/ACCESS.2020.2997921,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Physics-Based Neural-Network Way to Perform Seismic Full Waveform Inversion,https://ieeexplore.ieee.org/document/9102272/,"Seismic full waveform inversion is a common technique that is used in the investigation of subsurface geology. Its classic implementation involves forward modeling of seismic wavefield based on a certain type of wave equation, which reflects the physics nature of subsurface seismic wavefield propagation. However, obtaining a good inversion result using traditional seismic waveform inversion methods usually comes with a high computational cost. Recently, with the emerging popularity of deep learning techniques in various computer vision tasks, deep neural network (DNN) has demonstrated an impressive ability in dealing with complex nonlinear problems, including seismic velocity inversion. Now, extensive efforts have been made in developing a DNN architecture to tackle the problem of seismic velocity inversion, and promising results have been achieved. However, due to the dependence of a labeled dataset, i.e., the barely accessible true velocity model corresponding to real seismic data, the current supervised deep learning inversion framework may suffer from limitations on generalization. One possible solution to mitigate this issue is to impose the governing physics into this kind of purely data-driven method. Thus, following the procedures of traditional seismic full waveform inversion, we propose a seismic waveform inversion network, namely SWINet, based on wave-equation-based forward modeling network cells. By treating the single-shot observation data and its corresponding shot position as training data pairs, the inverted velocity model can be obtained as the trainable network parameters. Moreover, since the proposed seismic waveform inversion method is performed in a neural-network way, its implementation and inversion effect could benefit from some built-in tools in Pytorch, such as automatic differentiation, Adam optimizer and mini-batch strategy, etc. Numerical examples indicate that the SWINet method may possess great potential in resulting a good velocity inversion effect with relatively fast convergence and lower computation cost.",science
10.1109/INFOCOM.2017.8057066,to_check,IEEE INFOCOM 2017 - IEEE Conference on Computer Communications,IEEE,2017-05-04 00:00:00,ieeexplore,SybilSCAR: Sybil detection in online social networks via local rule based propagation,https://ieeexplore.ieee.org/document/8057066/,"Detecting Sybils in online social networks (OSNs) is a fundamental security research problem as adversaries can leverage Sybils to perform various malicious activities. Structure-based methods have been shown to be promising at detecting Sybils. Existing structure-based methods can be classified into two categories: Random Walk (RW)-based methods and Loop Belief Propagation (LBP)-based methods. RW-based methods cannot leverage labeled Sybils and labeled benign users simultaneously, which limits their detection accuracy, and they are not robust to noisy labels. LBP-based methods are not scalable, and they cannot guarantee convergence. In this work, we propose SybilSCAR, a new structure-based method to perform Sybil detection in OSNs. SybilSCAR maintains the advantages of existing methods while overcoming their limitations. Specifically, SybilSCAR is Scalable, Convergent, Accurate, and Robust to label noises. We first propose a framework to unify RW-based and LBP-based methods. Under our framework, these methods can be viewed as iteratively applying a (different) local rule to every user, which propagates label information among a social graph. Second, we design a new local rule, which SybilSCAR iteratively applies to every user to detect Sybils. We compare SybilSCAR with a state-of-the-art RW-based method and a state-of-the-art LBP-based method, using both synthetic Sybils and large-scale social network datasets with real Sybils. Our results demonstrate that SybilSCAR is more accurate and more robust to label noise than the compared state-of-the-art RW-based method, and that SybilSCAR is orders of magnitude more scalable than the state-of-the-art LBP-based method and is guaranteed to converge. To facilitate research on Sybil detection, we have made our implementation of SybilSCAR publicly available on our webpages.",science
10.1109/CCST.2016.7815686,to_check,2016 IEEE International Carnahan Conference on Security Technology (ICCST),IEEE,2016-10-27 00:00:00,ieeexplore,System for monitoring natural disasters using natural language processing in the social network Twitter,https://ieeexplore.ieee.org/document/7815686/,"This paper presents the design and implementation of an automated system that allows monitoring the social network Twitter, making a connection to the API, to filter content according to four categories (volcanic, telluric, fires and climatological) which affect Ecuador because of its geographical location, taking into account that these cannot be easily predicted, and stores all tweets in a database for analysis. The filtering process is performed by using the NLTK tool with which the frequency of a word is determined within a tweet, to be classified later in one of the proposed categories. The results for each category are displayed on a web page that contains real-time statistics of the database. This work provides access to information on natural disasters because they are classified.",science
10.1049/cp:19990285,to_check,"Image Processing And Its Applications, 1999. Seventh International Conference on (Conf. Publ. No. 465)",IET,1999-07-15 00:00:00,ieeexplore,A neural vision based controller for a robot footballer,https://ieeexplore.ieee.org/document/791354/,"Robot football is growing in popularity both as a research topic and as a sporting event. The football setting provides rich interaction possibilities and a ready source of competition in an environment containing both predictable and non-deterministic elements. Successful players must be able to react quickly in real time, exhibit multiple competences and choose between several possibly conflicting goals. Opportunities exist to explore reflexive behaviour, strategic behaviour and even communication and social behaviour in team events. At the same time, artificial neural networks are increasingly being used in robot controllers to explore new biologically-inspired ideas relating to perception, memory and motor control. The research described in this paper attempts to combine these two areas of study to produce a framework for a neurally based and visually guided football-playing controller. A controller architecture is proposed in which a small set of high-level features in the robot's environment are extracted from raw image data by using a feedforward neural network. These feature signals, collectively termed the ""feature bus"", are then available for use by other controller modules. The feature bus signals are sufficiently general and high-level to be used with many different controller strategies, and their low dimensionality compared to the raw visual input makes the implementation of learning controllers more feasible.",science
10.1109/CCWC47524.2020.9031269,to_check,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),IEEE,2020-01-08 00:00:00,ieeexplore,An Automated Framework for Real-time Phishing URL Detection,https://ieeexplore.ieee.org/document/9031269/,"An increasing number of services, including banking and social networking, are being integrated with world wide web in recent years. The crux of this increasing dependence on the internet is the rise of different kinds of cyberattacks on unsuspecting users. One such attack is phishing, which aims at stealing user information via deceptive websites. The primary defense against phishing consists of maintaining a black list of the phishing URLs. However, a black list approach is reactive and cannot defend against new phishing websites. For this reason, a number of research have been done on using machine learning techniques to detect previously unseen phishing URLs. While they show promising results, any such implementation is yet to be seen. This is because 1) little work has been done on developing a complete end-to-end framework for phishing URL detection 2) it is prohibitively slow to detect phishing URLs using machine learning algorithms. In this work we address these two issues by formulating a robust framework for fast and automated detection of phishing URLs. We have validated our framework with a real dataset achieving 87% accuracy in a real-time setup.",science
10.1109/ICCSRE.2019.8807586,to_check,2019 International Conference of Computer Science and Renewable Energies (ICCSRE),IEEE,2019-07-24 00:00:00,ieeexplore,Arab Sign language Recognition with Convolutional Neural Networks,https://ieeexplore.ieee.org/document/8807586/,"The implementation of an automatic recognition system for Arab sign language (ArSL) has a major social and humanitarian impact. With the growth of the deaf-dump community, such a system will help in integrating those people and enjoy a normal life. Like other languages, Arab sign language has many details and diverse characteristics that need a powerful tool to treat it. In this work, we propose a new system based on the convolutional neural networks, fed with a real dataset, this system will recognize automatically numbers and letters of Arab sign language. To validate our system, we have done a comparative study that shows the effectiveness and robustness of our proposed method compared to traditional approaches based on k-nearest neighbors (KNN) and support vector machines (SVM).",science
10.1109/EMCTECH49634.2020.9261512,to_check,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),IEEE,2020-10-22 00:00:00,ieeexplore,Digital Socio-Political Communication and its Transformation in the Technological Evolution of Artificial Intelligence and Neural Network Algorithms,https://ieeexplore.ieee.org/document/9261512/,"The study aims to analyze the specifics of determining the subjects of digital social and political communication in the context of the development of artificial intelligence technologies and neural network algorithms. The work uses a case-study design. As a research methodology, the method of critical analysis of the digital communications practice in the socio-political sphere, as well as discourse analysis of modern scientific research in the field of the development of artificial intelligence and neural network algorithms, are used. It is concluded that the implementation of technological solutions based on artificial intelligence and neural network algorithms into the processes of socio-political communications creates a problem of defining the subject of communication acts in the socio-political sphere. Society may face such communication practices in which hybrid subjectness is realized. In the conditions of hybrid subjectness, both real subjects and programmed neural network algorithms acting as real subjects (but only imitating own subjectivity) interact in common communication space. The originality of the work lies in the formulation of the author's hypothesis about the emergence of the phenomenon of hybrid subjectness in the space of modern socio-political communications and its potential in the aspect of influencing the mass consciousness of citizens.",science
10.1109/I4CS.2015.7294480,to_check,2015 15th International Conference on Innovations for Community Services (I4CS),IEEE,2015-07-10 00:00:00,ieeexplore,Happy hour - improving mood with an emotionally aware application,https://ieeexplore.ieee.org/document/7294480/,"Mobile sensing in Cyber-Physical Systems has been evolving proportionally with smartphones. In fact, we are witnessing a tremendous increase in systems that sense various facets of human beings and their surrounding environments. In particular, the detection of human emotions can lead to emotionally-aware applications that use this information to benefit people's daily lives. This work presents the implementation of a Human-inthe- loop emotionally-aware Cyber-Physical System that attempts to positively impact its user's mood through moderate walking exercise. Data from smartphone sensors, a smartshirt's electrocardiogram and weather information from a web API are processed through a machine learning algorithm to infer emotional states. When negative emotions are detected, the application timely suggests walking exercises, while providing real-time information regarding nearby points of interest. This information includes events, background music, attendance, agitation and general mood. In addition, the system also dynamically adapts privacy and networking configurations based on emotions. The sharing of the user's location on social networks and the device's networking interfaces are configured according to user-defined rules in order to reduce frustration and provide a better Quality of Experience.",science
10.1109/KCIC.2018.8628512,to_check,2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC),IEEE,2018-10-30 00:00:00,ieeexplore,Estimating Adaptive Individual Interests and Needs Based on Online Local Variational Inference for a Logistic Regression Mixture Model,https://ieeexplore.ieee.org/document/8628512/,"In real companies engaged in economic activities through transactions involving consumer items, such as retail, distribution, finance, and information materials, supplying an opportunity to customers to choose specialized items is an important factor that can improve customer satisfaction and convenience allowing their diverse and time-dependent needs to be met. However, capturing the specialized needs of customers accurately is a difficult task because their needs depend on time, context, situation, and meaning. Recently, physical computational environments have been developing rapidly, thereby allowing easy implementation to sense a customer's action and deal with it sequentially. In this paper, we propose a personalized method to predict individual interests and demands appropriately. In particular, the system learns the customers' situation, meaning, and action from their action history, and reflects a feedback of the result to predict the next action. To realize this method, we utilize the following two methodologies: the mathematical model of meaning (MMM), which is a semantic associative search technology; and the local variational inference (LVI), which is an approximation of the Bayesian inference. A numerical experiment shows that the proposed method performed better than a typical method.",science
10.1109/IJCNN.1991.155235,to_check,IJCNN-91-Seattle International Joint Conference on Neural Networks,IEEE,1991-07-12 00:00:00,ieeexplore,Experimental demonstration of large-scale holographic optical neural network,https://ieeexplore.ieee.org/document/155235/,"The authors present an experimental implementation of the N/sup 4/ holographic optical neural network and demonstrations of the potential capabilities of large-scale and high-speed pattern recognition applications of this architecture. The extremely large space-bandwidth-product of the holographic materials makes it possible to construct large-scale static holographic optical neural networks (HONNs). A 1024-neuron HONN has been experimentally demonstrated for pattern recognition operations. A 32*32 optical neural network was trained to recognize different airplanes and to give the correct names at the output detector array. The computational time for training the four pairs of patterns took only a few minutes on a 386 microcomputer. It is estimated that with proper selection, over 200 pairs of heteroassociative patterns can be stored in the neural network of 1024 neurons for target recognition using the interpattern association (IPA) neural network model (T. Lu et al., 1990). By recording more holograms on a holographic plate, a 128*128 neural network and eventually a 256*256 neural network can be constructed, thus a high-speed real-time recognition of an enemy's aircraft can be performed.&lt;<ETX>&gt;</ETX>",science
10.1109/IGARSS.2013.6723057,to_check,2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS,IEEE,2013-07-26 00:00:00,ieeexplore,Parallel sparse unmixing of hyperspectral data,https://ieeexplore.ieee.org/document/6723057/,"In this paper, a new parallel method for sparse spectral unmixing of remotely sensed hyperspectral data on commodity graphics processing units (GPUs) is presented. A semi-supervised approach is adopted, which relies on the increasing availability of spectral libraries of materials measured on the ground instead of resorting to endmember extraction methods. This method is based on the spectral unmixing by splitting and augmented Lagrangian (SUNSAL) that estimates the material's abundance fractions. The parallel method is performed in a pixel-by-pixel fashion and its implementation properly exploits the GPU architecture at low level, thus taking full advantage of the computational power of GPUs. Experimental results obtained for simulated and real hyperspectral datasets reveal significant speedup factors, up to 164 times, with regards to optimized serial implementation.",science
10.1109/RCAR47638.2019.9044114,to_check,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),IEEE,2019-08-09 00:00:00,ieeexplore,Research on omnidirectional mobile robot motion control based on integration of traction and steering wheel,https://ieeexplore.ieee.org/document/9044114/,"In order to solve the automatic transportation of heavy materials under the limited working space of production workshops and warehouses, two sets of heavy-duty omnidirectional mobile robot motion control systems with steering wheel drive units were designed. The steering wheel combination drive unit of the “walking + steering” set is used to build the mobile robot chassis, and the mechatronics servo system and mathematical model of multi-motor coordinated motion are constructed. The communication between the controller and the steering wheel combination drive unit is established through the CAN bus. The specific implementation is to capture and analyze the control signal through the controller to obtain the desired motion mode, to obtain the motion of each set of steering wheel unit through the mathematical model, and to realize the desired motion through the synthesis of each set of steering wheel unit motion. It has been verified by experiments that the two sets of steering wheel unit-driven mobile robot control system realizes the zero turning radius, 360-degree omnidirectional movement of the robot and rotation during the movement. It can be used for flexible work in tight spaces.",science
10.1007/s12525-020-00414-7,to_check,Electronic Markets,Springer,2021-06-01 00:00:00,springer,AI-based chatbots in customer service and their effects on user compliance,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12525-020-00414-7,"Communicating with customers through live chat interfaces has become an increasingly popular means to provide real-time customer service in many e-commerce settings. Today, human chat service agents are frequently replaced by conversational software agents or chatbots, which are systems designed to communicate with human users by means of natural language often based on artificial intelligence (AI). Though cost- and time-saving opportunities triggered a widespread implementation of AI-based chatbots, they still frequently fail to meet customer expectations, potentially resulting in users being less inclined to comply with requests made by the chatbot. Drawing on social response and commitment-consistency theory, we empirically examine through a randomized online experiment how verbal anthropomorphic design cues and the foot-in-the-door technique affect user request compliance. Our results demonstrate that both anthropomorphism as well as the need to stay consistent significantly increase the likelihood that users comply with a chatbot’s request for service feedback. Moreover, the results show that social presence mediates the effect of anthropomorphic design cues on user compliance.",science
10.1007/978-3-030-69367-1_1,to_check,Impact and Opportunities of Artificial Intelligence Techniques in the Steel Industry,Springer,2021-01-01 00:00:00,springer,Challenges and Frontiers in Implementing Artificial Intelligence in Process Industry,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-69367-1_1,"The implementation of artificial intelligence faces different challenges of infrastructural, data related, security related and social scope. These aspects are discussed, reflecting on the requirements of introducing such a technology in a broader way. Machine learning, as subfield of artificial intelligence, benefits from advances in Big Data science. One example is the $$\lambda $$ λ -architecture which can be used for the treatment of streamed process data in industrial applications. Digital twins are shown as further tool providing object-oriented data for machine learning applications. Yet, the increasing freedom of data transfer within a plant, as propagated by Industry 4.0, poses new risks for information technology and automation systems: security of those components is one of the big challenges. Here, artificial intelligence can seen as both risk and solution. A last relevant challenge is acceptance among the staff, as artificial intelligence is associated with fears. Counterstrategies for those fears are presented as a proposed guideline for real applications. Finally, current frontiers at process industry are considered and discussed. These include the need for strengthening the use of high-dimensional data availability, increased roll-out of optimisation concepts and rigorous progresses in semantic modelling of processes and process chains, in order to fully exploit the beneficial scope of artificial intelligence in industry.",science
10.1007/978-3-030-86383-8_32,to_check,Artificial Neural Networks and Machine Learning – ICANN 2021,Springer,2021-01-01 00:00:00,springer,Design and Evaluation of Deep Learning Models for Real-Time Credibility Assessment in Twitter,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86383-8_32,"Social media have an enormous impact on modern life but are prone to the dissemination of false information. In several domains, such as crisis management or political communication, it is of utmost importance to detect false and to promote credible information. Although educational measures might help individuals to detect false information, the sheer volume of social big data, which sometimes need to be analysed under time-critical constraints, calls for automated and (near) real-time assessment methods. Hence, this paper reviews existing approaches before designing and evaluating three deep learning models (MLP, RNN, BERT) for real-time credibility assessment using the example of Twitter posts. While our BERT implementation achieved best results with an accuracy of up to 87.07% and an F1 score of 0.8764 when using metadata, text, and user features, MLP and RNN showed lower classification quality but better performance for real-time application. Furthermore, the paper contributes with a novel dataset for credibility assessment.",science
10.1007/978-3-030-81321-5_15,to_check,Fashion Communication,Springer,2021-01-01 00:00:00,springer,"Building a Prosocial Communication Model in the Fashion Sector, Based on Sustainability and Artificial Intelligence, Derived from COVID-19",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-81321-5_15,"Considering communication in fashion from the perspective of sustainability and artificial intelligence allows us to resize its social focus, especially when we contextualize it in the era of COVID-19, which has stopped humanity in a loop, to rethink a new normal that accelerates processes, unthinkable in others historical moments. We see an undeniable trend, where a humanitarian sensitivity towards sustainability in fashion develops, towards the rational and real use of an intelligence in the fashion sector that is no longer artificial but advanced. This research responds to a question that belongs to the world of the social. How to communicate for a society that urgently needs to implement sustainability strategies in the fashion sector, based on artificial intelligence at the service of rescuing the human? In this sense, based on a projective documentary research and a series of in-depth interviews with fashion experts, it allowed us to initiate what aims to lay the foundations of a prosocial communication model in the fashion sector, based on the sustainability of the human being on the planet and, as a corollary, also based on artificial intelligence. The prosocial field is oriented to collaborate and help other people in a positive, productive and social way. Artificial intelligence in fashion, at the service of sustainability and communication, allows us to think with a humanistic approach to communication, where the focus is to understand the human being in a deep way, with an interrelation with artificial intelligence and the increase of sustainability in fashion in different areas of the value chain. This investigation concludes by showing that the implementation of the proposed model of this investigation is plausible.",science
10.1007/s10817-014-9303-3,to_check,Journal of Automated Reasoning,Springer,2014-08-01 00:00:00,springer,Learning-Assisted Automated Reasoning with Flyspeck,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10817-014-9303-3,"The considerable mathematical knowledge encoded by the Flyspeck project is combined with external automated theorem provers (ATPs) and machine-learning premise selection methods trained on the Flyspeck proofs, producing an AI system capable of proving a wide range of mathematical conjectures automatically. The performance of this architecture is evaluated in a bootstrapping scenario emulating the development of Flyspeck from axioms to the last theorem, each time using only the previous theorems and proofs. It is shown that 39 % of the 14185 theorems could be proved in a push-button mode (without any high-level advice and user interaction) in 30 seconds of real time on a fourteen-CPU workstation. The necessary work involves: (i) an implementation of sound translations of the HOL Light logic to ATP formalisms: untyped first-order, polymorphic typed first-order, and typed higher-order, (ii) export of the dependency information from HOL Light and ATP proofs for the machine learners, and (iii) choice of suitable representations and methods for learning from previous proofs, and their integration as advisors with HOL Light. This work is described and discussed here, and an initial analysis of the body of proofs that were found fully automatically is provided.",science
http://arxiv.org/abs/1911.08448v4,to_check,arxiv,arxiv,2019-11-19 18:14:58+00:00,arxiv,Artificial intelligence approach to momentum risk-taking,http://arxiv.org/abs/1911.08448v4,"We propose a mathematical model of momentum risk-taking, which is essentially
real-time risk management focused on short-term volatility of stock markets.
Its implementation, our fully automated momentum equity trading system
presented systematically, proved to be successful in extensive historical and
real-time experiments. Momentum risk-taking is one of the key components of
general decision-making, a challenge for artificial intelligence and machine
learning with deep roots in cognitive science; its variants beyond stock
markets are discussed. We begin with a new algebraic-type theory of news impact
on share-prices, which describes well their power growth, periodicity, and the
market phenomena like price targets and profit-taking. This theory generally
requires Bessel and hypergeometric functions. Its discretization results in
some tables of bids, which are basically expected returns for main investment
horizons, the key in our trading system. The ML procedures we use are similar
to those in neural networking. A preimage of our approach is the new contract
card game provided at the end, a combination of bridge and poker. Relations to
random processes and the fractional Brownian motion are outlined.",science
http://arxiv.org/abs/1805.03045v2,to_check,arxiv,arxiv,2018-05-08 14:15:46+00:00,arxiv,"A new method for unveiling Open Clusters in Gaia: new nearby Open
  Clusters confirmed by DR2",http://arxiv.org/abs/1805.03045v2,"The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",science
http://arxiv.org/abs/2010.07634v3,to_check,arxiv,arxiv,2020-10-15 10:09:09+00:00,arxiv,"Towards Reflectivity profile inversion through Artificial Neural
  Networks",http://arxiv.org/abs/2010.07634v3,"The goal of Specular Neutron and X-ray Reflectometry is to infer materials
Scattering Length Density (SLD) profiles from experimental reflectivity curves.
This paper focuses on investigating an original approach to the ill-posed
non-invertible problem which involves the use of Artificial Neural Networks
(ANN). In particular, the numerical experiments described here deal with large
data sets of simulated reflectivity curves and SLD profiles, and aim to assess
the applicability of Data Science and Machine Learning technology to the
analysis of data generated at neutron scattering large scale facilities. It is
demonstrated that, under certain circumstances, properly trained Deep Neural
Networks are capable of correctly recovering plausible SLD profiles when
presented with never-seen-before simulated reflectivity curves. When the
necessary conditions are met, a proper implementation of the described approach
would offer two main advantages over traditional fitting methods when dealing
with real experiments, namely, 1. sample physical models are described under a
new paradigm: detailed layer-by-layer descriptions (SLDs, thicknesses,
roughnesses) are replaced by parameter free curves $\rho(z)$, allowing a-priori
assumptions to be fed in terms of the sample family to which a given sample
belongs (e.g. ""thin film"", ""lamellar structure"", etc.) 2. the time-to-solution
is shrunk by orders of magnitude, enabling faster batch analyses for large
datasets.",science
http://arxiv.org/abs/2012.14325v1,to_check,arxiv,arxiv,2020-12-22 09:54:04+00:00,arxiv,Digital me ontology and ethics,http://arxiv.org/abs/2012.14325v1,"This paper addresses ontology and ethics of an AI agent called digital me. We
define digital me as autonomous, decision-making, and learning agent,
representing an individual and having practically immortal own life. It is
assumed that digital me is equipped with the big-five personality model,
ensuring that it provides a model of some aspects of a strong AI:
consciousness, free will, and intentionality. As computer-based personality
judgments are more accurate than those made by humans, digital me can judge the
personality of the individual represented by the digital me, other individuals'
personalities, and other digital me-s. We describe seven ontological qualities
of digital me: a) double-layer status of Digital Being versus digital me, b)
digital me versus real me, c) mind-digital me and body-digital me, d) digital
me versus doppelganger (shadow digital me), e) non-human time concept, f)
social quality, g) practical immortality. We argue that with the advancement of
AI's sciences and technologies, there exist two digital me thresholds. The
first threshold defines digital me having some (rudimentarily) form of
consciousness, free will, and intentionality. The second threshold assumes that
digital me is equipped with moral learning capabilities, implying that, in
principle, digital me could develop their own ethics which significantly
differs from human's understanding of ethics. Finally we discuss the
implications of digital me metaethics, normative and applied ethics, the
implementation of the Golden Rule in digital me-s, and we suggest two sets of
normative principles for digital me: consequentialist and duty based digital me
principles.",science
http://arxiv.org/abs/1409.7699v3,to_check,arxiv,arxiv,2014-09-26 20:00:14+00:00,arxiv,"The Overlooked Potential of Generalized Linear Models in Astronomy-II:
  Gamma regression and photometric redshifts",http://arxiv.org/abs/1409.7699v3,"Machine learning techniques offer a precious tool box for use within
astronomy to solve problems involving so-called big data. They provide a means
to make accurate predictions about a particular system without prior knowledge
of the underlying physical processes of the data. In this article, and the
companion papers of this series, we present the set of Generalized Linear
Models (GLMs) as a fast alternative method for tackling general astronomical
problems, including the ones related to the machine learning paradigm. To
demonstrate the applicability of GLMs to inherently positive and continuous
physical observables, we explore their use in estimating the photometric
redshifts of galaxies from their multi-wavelength photometry. Using the gamma
family with a log link function we predict redshifts from the PHoto-z Accuracy
Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from
Data Release 10. We obtain fits that result in catastrophic outlier rates as
low as ~1% for simulated and ~2% for real data. Moreover, we can easily obtain
such levels of precision within a matter of seconds on a normal desktop
computer and with training sets that contain merely thousands of galaxies. Our
software is made publicly available as an user-friendly package developed in
Python, R and via an interactive web application
(https://cosmostatisticsinitiative.shinyapps.io/CosmoPhotoz). This software
allows users to apply a set of GLMs to their own photometric catalogues and
generates publication quality plots with minimum effort from the user. By
facilitating their ease of use to the astronomical community, this paper series
aims to make GLMs widely known and to encourage their implementation in future
large-scale projects, such as the Large Synoptic Survey Telescope.",science
10.1016/j.jmps.2020.104277,to_check,Journal of the Mechanics and Physics of Solids,scopus,2021-02-01,sciencedirect,Thermodynamics-based Artificial Neural Networks for constitutive modeling,https://api.elsevier.com/content/abstract/scopus_id/85097707935,"Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications.
                  Here we propose a new class of data-driven, physics-based, neural networks for constitutive modeling of strain rate independent processes at the material point level, which we define as Thermodynamics-based Artificial Neural Networks (TANNs). The two basic principles of thermodynamics are encoded in the network’s architecture by taking advantage of automatic differentiation to compute the numerical derivatives of a network with respect to its inputs. In this way, derivatives of the free-energy, the dissipation rate and their relation with the stress and internal state variables are hardwired in the architecture of TANNs. Consequently, our approach does not have to identify the underlying pattern of thermodynamic laws during training, reducing the need of large data-sets. Moreover the training is more efficient and robust, and the predictions more accurate. Finally and more important, the predictions remain thermodynamically consistent, even for unseen data. Based on these features, TANNs are a starting point for data-driven, physics-based constitutive modeling with neural networks.
                  We demonstrate the wide applicability of TANNs for modeling elasto-plastic materials, using both hyper- and hypo-plasticity models. Strain hardening and softening are also considered for the hyper-plastic scenario. Detailed comparisons show that the predictions of TANNs outperform those of standard ANNs. Finally, we demonstrate that the implementation of the laws of thermodynamics confers to TANNs high robustness in the presence of noise in the training data, compared to standard approaches.
                  TANNs’ architecture is general, enabling applications to materials with different or more complex behavior, without any modification.",science
10.1016/j.prro.2020.07.003,to_check,Practical Radiation Oncology,scopus,2021-01-01,sciencedirect,Time Analysis of Online Adaptive Magnetic Resonance–Guided Radiation Therapy Workflow According to Anatomical Sites,https://api.elsevier.com/content/abstract/scopus_id/85090017005,"Purpose
                  To document time analysis of detailed workflow steps for the online adaptive magnetic resonance–guided radiation therapy treatments (MRgRT) with the ViewRay MRIdian system and to identify the barriers to and solutions for shorter treatment times.
               
                  Methods and Materials
                  A total of 154 patients were treated with the ViewRay MRIdian system between September 2018 and October 2019. The time process of MRgRT workflow steps of 962 fractions for 166 treatment sites was analyzed in terms of patient and online adaptive treatment (ART) characteristics.
               
                  Results
                  Overall, 774 of 962 fractions were treated with online ART, and 83.2% of adaptive fractions were completed in less than 60 minutes. Sixty-three percent, 50.3%, and 4.2% of fractions were completed in less than 50 minutes, 45 minutes, and 30 minutes, respectively. Eight-point-three percent and 3% of fractions were completed in more than 70 minutes and 80 minutes, respectively. The median time (tmed) for ART workflow steps were as follows: (1) setup tmed: 5.0 minutes, (2) low-resolution scanning tmed: 1 minute, (3) high-resolution scanning tmed: 3 minutes, (4) online contouring tmed: 9 minutes, (5) reoptimization with online quality assurance tmed: 5 minutes, (6) real targeting tmed: 3 minutes, (7) beam delivery with gating tmed: 17 minutes, and (8) net total treatment time tmed: 45 minutes. The shortest and longest tmean rates of net total treatment time were 41.59 minutes and 64.43 minutes for upper-lung-lobe-located thoracic tumors and ultracentrally located thoracic tumors, respectively.
               
                  Conclusions
                  To our knowledge, this is the first broad treatment-time analysis for online ART in the literature. Although treatment times are long due to human- and technology-related limitations, benefits offered by MRgRT might be clinically important. In the future, implementation of artificial intelligence segmentation, an increase in dose rate, and faster multileaf collimator and gantry speeds may lead to achieving shorter MRgRT treatments.",science
10.1016/j.aca.2020.04.007,to_check,Analytica Chimica Acta,scopus,2020-06-01,sciencedirect,Dual-emission CdTe/AgInS<inf>2</inf> photoluminescence probe coupled to neural network data processing for the simultaneous determination of folic acid and iron (II),https://api.elsevier.com/content/abstract/scopus_id/85083073504,"This work focused on the combination of CdTe and AgInS2 quantum dots in a dual-emission nanoprobe for the simultaneous determination of folic acid and Fe(II) in pharmaceutical formulations. The surface chemistry of the used QDs was amended with suitable capping ligands to obtain appropriate reactivity in terms of selectivity and sensitivity towards the target analytes. The implementation of PL-based sensing schemes combining multiple QDs of different nature, excited at the same wavelength and emitting at different ones, allowed to obtain a specific analyte-response profile. The first-order fluorescence data obtained from the whole emission spectra of the CdTe/AgInS2 combined nanoprobe upon interaction with folic acid and Fe(II) were processed by using chemometric tools, namely partial least-squares (PLS) and artificial neural network (ANN). This enabled to circumvent the selectivity issues commonly associated with the use of QDs prone to indiscriminate interaction with multiple species, which impair reliable and accurate quantification in complex matrices samples.
                  ANN demonstrated to be the most efficient chemometric model for the simultaneous determination of both analytes in binary mixtures and pharmaceutical formulations due to the non-linear relationship between analyte concentration and fluorescence data that it could handle. The R2
                     P and SEP% obtained for both analytes quantification in pharmaceutical formulations through ANN modelling ranged from 0.92 to 0.99 and 5.7–9.1%, respectively. The obtained results revealed that the developed approach is able to quantify, with high reliability and accuracy, more than one analyte in complex mixtures and real samples with pharmaceutical interest.",science
10.1016/j.amar.2020.100113,to_check,Analytic Methods in Accident Research,scopus,2020-03-01,sciencedirect,"Big data, traditional data and the tradeoffs between prediction and causality in highway-safety analysis",https://api.elsevier.com/content/abstract/scopus_id/85078666924,"The analysis of highway accident data is largely dominated by traditional statistical methods (standard regression-based approaches), advanced statistical methods (such as models that account for unobserved heterogeneity), and data-driven methods (artificial intelligence, neural networks, machine learning, and so on). These methods have been applied mostly using data from observed crashes, but this can create a problem in uncovering causality since individuals that are inherently riskier than the population as a whole may be over-represented in the data. In addition, when and where individuals choose to drive could affect data analyses that use real-time data since the population of observed drivers could change over time. This issue, the nature of the data, and the implementation target of the analysis imply that analysts must often tradeoff the predictive capability of the resulting analysis and its ability to uncover the underlying causal nature of crash-contributing factors. The selection of the data-analysis method is often made without full consideration of this tradeoff, even though there are potentially important implications for the development of safety countermeasures and policies. This paper provides a discussion of the issues involved in this tradeoff with regard to specific methodological alternatives and presents researchers with a better understanding of the trade-offs often being inherently made in their analysis.",science
10.1016/j.ssci.2019.06.025,to_check,Safety Science,scopus,2019-12-01,sciencedirect,Securing instant messaging based on blockchain with machine learning,https://api.elsevier.com/content/abstract/scopus_id/85067872085,"Instant Messaging (IM) offers real-time communications between two or more participants on Internet. Nowadays, most IMs take place on mobile applications, such as WhatsApp, WeChat, Viber and Facebook Messenger, which have more users than social networks, such as Twitter and Facebook. Among the applications of IMs, online shopping has become a part of our everyday life, primarily those who are busiest. However, transaction disputes are often occurred online shopping. Since most IMs are centralized and message history is not stored in the center, the messaging between users and owners of online shops are not reliable and traceable. In China, online shopping sales have soared from practically zero in 2003 to nearly 600 hundred million dollars last year, and now top those in the United States. It is very crucial to secure the instant messaging in online shopping in China. We present techniques to exploit blockchain and machine learning algorithms to secure instant messaging. Since the cryptography of Chinese national standard is encouraged to adopt in security applications of China, we propose a blockchain-based IM scheme with the Chinese cryptographic bases. First, we design a message authentication model based on SM2 to avoid the counterfeit attack and replay attack. Second, we design a cryptographic hash mode based on SM3 to verify the integrity of message. Third, we design a message encryption model based on SM4 to protect the privacy of users. Besides, we propose a method based on machine learning algorithms to monitor the activity on blockchain to detect anomaly. To prove and verify the blockchain-based IM scheme, a blockchain-based IM system has been designed on Linux platforms. The implementation result shows that it is a practical and secure IM system, which can be applied to a variety of instant messaging applications directly.",science
10.1016/j.procs.2019.01.012,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Combining supervised and unsupervised machine learning algorithms to predict the learners' learning styles,https://api.elsevier.com/content/abstract/scopus_id/85062675875,"The implementation of an efficient adaptive e-learning system requires the construction of an effective student model that represents the student’s characteristics, among those characteristics, there is the learning style that refers to the way in which a student prefers to learn. Knowing learning styles helps adaptive E-learning systems to improve the learning process by providing customized materials to students. In this work, we have proposed an approach to identify the learning style automatically based on the existing learners’ behaviors and using web usage mining techniques and machine learning algorithms. The web usage mining techniques were used to pre-process the log file extracted from the E-learning environment and capture the learners’ sequences. The captured learners’ sequences were given as an input to the K-modes clustering algorithm to group them into 16 learning style combinations based on the Felder and Silverman learning style model. Then the naive Bayes classifier was used to predict the learning style of a student in real time. To perform our approach, we used a real dataset extracted from an e-learning system’s log file, and in order to evaluate the performance of the used classifier, the confusion matrix method was used. The obtained results demonstrate that our approach yields excellent results.",science
10.1016/j.heliyon.2018.e00972,to_check,Heliyon,scopus,2018-11-01,sciencedirect,Modeling the output power of heterogeneous photovoltaic panels based on artificial neural networks using low cost microcontrollers,https://api.elsevier.com/content/abstract/scopus_id/85057181952,"Many implementations of artificial neural networks have been reported in scientific papers. However, few of these implementations allow the direct use of off-line trained networks. Moreover, no implementation reported the use of relatively small network adequate to run on low cost microcontroller. Hence, this work, which presents a small artificial neural network, which models the output power of heterogeneous photovoltaic panel. In addition, the work discuss the hardware implementation that allows such network to run on low cost microcontroller. The hardware implementation has the ability to model heterogeneous photovoltaic panel's output power with very high accuracy and fast response time. Feedforward back propagation has been used because of its high resolution and accurate activation function. Real-time measured parameters can be used as inputs for the developed system. The resulting hardware data is tested with data from real photovoltaic panels; to confirm that it can efficiently implement the models prepared off-line with Matlab. The comparison revealed the robustness of the proposed heterogeneous photovoltaic model system at different conditions. The proposed heterogeneous photovoltaic model system offer a proper and efficient tool that can be used in monitoring photovoltaic panels, such as the ones used in smart-house applications.",science
10.1109/ICIS.2017.7959982,to_check,2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),IEEE,2017-05-26 00:00:00,ieeexplore,Robotics data real-time management based on NoSQL solution,https://ieeexplore.ieee.org/document/7959982/,"In nowadays, robotics database management systems are increasing. These systems ensure good storage of data and with big data analytic, a new approach demands new structures and methods for collecting, recording, and analyzing enterprise data. This paper work deals with the NoSQL databases which are the secret of the continual progression data that new data management solutions have been emerged. They crossed several areas as personalization, profile management, big data in real-time, content management, catalogue, view of customers, mobile applications, internet of things, digital communication and fraud detection. Machine learning, for example, thrives on more data, so smart machines can learn more and faster, the Robotics are our use of case to focus on our Test. The implementation of NoSQL for Robotics wrestle all the data they acquire into usable form because with the ordinary type of Robotics we are facing very big limits to manage and find the exact information in real-time. Our original proposed approach was demonstrated by experimental studies and running example used as a use case.",robotics
10.1109/ICIRCA48905.2020.9182995,to_check,2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA),IEEE,2020-07-17 00:00:00,ieeexplore,An Approach for Digital Farming using Mobile Robot,https://ieeexplore.ieee.org/document/9182995/,"Farming is the backbone of the Indian economy and it has been unchartered territory for a technological solution. As of late developments in Artificial Intelligence technology combined with Robotics has paved the way for an option of digital farming. As a matter of fact, Indian farming has been facing various challenges that include abrupt change in climatic conditions, spoiling of yields, soil nutrient requirement, pests/weed control and so forth. Robotics and Artificial Intelligence (AI) along with the integration of various sensors ensures the possibility of better outcome. In this work the simulation of Mobile robot for the purpose of seed sowing along with its movement has been presented. The implementation comprises of the Motor schema for the navigation of robot and Gale Shapley (GS) algorithm for stable match of seed and yield combination. Such a robotic system combined with AI in real time will form excellent means of farming in terms of yield.",robotics
10.1109/CNNA.2000.876849,to_check,Proceedings of the 2000 6th IEEE International Workshop on Cellular Neural Networks and their Applications (CNNA 2000) (Cat. No.00TH8509),IEEE,2000-05-25 00:00:00,ieeexplore,Design of a dedicated CNN chip for autonomous robot navigation,https://ieeexplore.ieee.org/document/876849/,"Obstacle avoidance is the main issue in autonomous robotics. It requires a three-dimensional effective environment sensing in real time. Among the others, the stereo vision approach to environmental information extraction seems to be very appealing, even if it leads an extremely high computational cost. However, a high performance implementation of this algorithm on a cellular neural network is able to overcome these difficulties. In the paper, the design of a CNN chip well suited for this algorithm is presented. This chip, performing a real time processing of the stereo vision data, will improve the cruising speed of a robotic platform.",robotics
10.1109/SBR-LARS.2012.57,to_check,2012 Brazilian Robotics Symposium and Latin American Robotics Symposium,IEEE,2012-10-19 00:00:00,ieeexplore,Fixed-Point Neural Network Ensembles for Visual Navigation,https://ieeexplore.ieee.org/document/6363361/,"Visual navigation is an important research field in robotics because of the low cost and the high performance that is usually achieved by visual navigation systems. Pixel classification as a road pixel or a non-road pixel is a task that can be well performed by Artificial Neural Networks. In the case of real-time instances of the image classification problem, as when applied to autonomous vehicles navigation, it is interesting to achieve the best possible execution time. Hardware implementations of these systems can achieve fast execution times but the floating-point implementation of Neural Networks are commonly complex and resource intensive. This work presents the implementation and analysis of a fixed-point Neural Network Ensemble for image classification. The system is composed by six fixed-point Neural Networks verified with cross-validation technique, using some proposed voting schemes and analyzed considering the execution time, precision, memory consumption and accuracy for hardware implementation. The results show that the fixed-point implementation is faster, consumes less memory and has an acceptable precision compared to the floating-point implementation. This fact suggests that the fixed point implementation should be used in systems that need a fast execution time. Some questions about ensembles and voting have to be reviewed for fixed-point Neural Network Ensembles.",robotics
10.1109/HUMANOIDS.2017.8246941,to_check,2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids),IEEE,2017-11-17 00:00:00,ieeexplore,Human-robot interaction assessment using dynamic engagement profiles,https://ieeexplore.ieee.org/document/8246941/,"This paper addresses the use of convolutional neural networks for image analysis resulting in an engagement metric that can be used to assess the quality of human robot interactions. We propose a method based on a pretrained convolutional network able to map emotions onto a continuous [0-1] interval, where 0 represents disengaged and 1 fully engaged. The network shows a good accuracy at recognizing the engagement state of humans given positive emotions. A time based analysis of interaction experiments between small humanoid robots and humans provides time series of engagement estimates, which are further used to understand the nature of the interaction as well as the overall mood and interest of the participant during the experiment. The method allows a real-time implementation and supports a quantitative and qualitative assessment of a human robot interaction with respect to a positive engagement and is applicable to humanoid robotics as well as other related contexts.",robotics
10.23919/ACC.1992.4792313,to_check,1992 American Control Conference,IEEE,1992-06-26 00:00:00,ieeexplore,Learning for Skill Acquisition and Refinement: Toward Exploring Everyday Physics,https://ieeexplore.ieee.org/document/4792313/,"The present talk claims that ""robotics"" is not a test bed for AI but should involve a research frontier, which attempts to account for intelligibility of everyday physics underlying human activities such as perception, remembrance, planning, practices, and skill. In addition to traditional AI and neuro-network approaches, more of new domains that can account for any aspect of human intellectual behaviors must be exploited, and also more of new tools that actualize real implementation of intelligence in machines need to be devised. To aim at going on an expedition in this direction, this talk introduces one new domain and another new tool. The former is practice-based learning for skill refinement and the latter is a design tool of signal-based structured information base for skill acquirement.",robotics
10.1109/IROS.1991.174419,to_check,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,IEEE,1991-11-05 00:00:00,ieeexplore,Learning for skill refinement,https://ieeexplore.ieee.org/document/174419/,"It is claimed that 'robotics' is not a test bed for AI but should involve a research frontier relating to the physics underlying human activities such as perception, remembering, planning, practice, and skill. In addition to traditional AI and neural network approaches, other domains that can account for any aspect of human intellectual behavior must be exploited, and tools that actualize real implementation of intelligence in machines need to be devised. A practice-based learning domain for skill refinement and a design tool for a signal-based structured information base for skill acquisition are presented.&lt;<ETX>&gt;</ETX>",robotics
10.1109/ICRA.2016.7487351,to_check,2016 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2016-05-21 00:00:00,ieeexplore,Object discovery and grasp detection with a shared convolutional neural network,https://ieeexplore.ieee.org/document/7487351/,"Grasp an object from a stack of objects in real-time is still a challenge in robotics. This requires the robot to have the ability of both fast object discovery and grasp detection: a target object should be picked out from the stack first and then a proper grasp configuration is applied to grasp the object. In this paper, we propose a shared convolutional neural network (CNN) which can simultaneously implement these two tasks in real-time. The processing speed of the model is about 100 frames per second on a GPU which largely satisfies the requirement. Meanwhile, we also establish a labeled RGBD dataset which contains scenes of stacked objects for robotic grasping. At last, we demonstrate the implementation of our shared CNN model on a real robotic platform and show that the robot can accurately discover a target object from the stack and successfully grasp it.",robotics
10.1109/ICTAACS48474.2019.8988124,to_check,2019 International Conference on Theoretical and Applicative Aspects of Computer Science (ICTAACS),IEEE,2019-12-16 00:00:00,ieeexplore,Online Adversarial Planning in μRTS : A Survey,https://ieeexplore.ieee.org/document/8988124/,"Online planning is an important research area focusing on the problem of real-time decision making, using information extracted from the environment. The aim is to compute, at each decision point, the best decision possible that contributes to the realization of a fixed objective. Relevant application domains include robotics, control engineering and computer games. Real-time strategy (RTS) games pose considerable challenges to artificial intelligence techniques, due to their dynamic, complex and adversarial aspects, where online planning plays a prominent role. They also constitute an ideal research platform and test-bed for online planning. μRTS is an open-source AI research platform that features a minimalistic, yet complete RTS implementation, used by AI researchers for developing and testing intelligent RTS game-playing agents. The unique characteristics of μRTS helped for the emergence of interesting online adversarial planning techniques, dealing with multiple levels of abstraction. This paper presents the major μRTS online planning approaches to date, categorized by the degree of abstraction, in fully and partially observable environments.",robotics
10.1109/IJCNN.2014.6889837,to_check,2014 International Joint Conference on Neural Networks (IJCNN),IEEE,2014-07-11 00:00:00,ieeexplore,Optimising the overall power usage on the SpiNNaker neuromimetic platform,https://ieeexplore.ieee.org/document/6889837/,"Simulations of biological tissue have been extensively used to replicate phenomena observed by in-vivo and in-vitro experiments as an alternative methodology for explaining how computations could take place in a brain region. Additional benefits of simulated neural networks over in-vivo experiments include greater observability, experimental control and reproducibility. General-purpose supercomputers provide the computational power and parallelism required to implement highly complex neural models, but this comes at the expense of high power requirements and communication overheads. Moreover, there are certain cases where real-time simulation performance is a desirable feature, for example in the field of cognitive robotics where embodied agents need to interact with their environment through biologically inspired asynchronous sensors. The SpiNNaker neuromimetic platform is a scalable architecture that has been designed to enable energy-efficient, large-scale simulations of spiking neurons in biological realtime. This work is based on a recent study which revealed that while they are generally energy efficient, SpiNNaker chips dissipate significant amount of power whilst in the idle state. In this paper we perform a systematic investigation into the overall energy consumption of a SpiNNaker system and propose a number of optimised suspend modes in order to reduce this. The proposed implementation is 60% more energy efficient in the idle state, 50% in the uploading and 52% in the downloading phases, while the power dissipation of the whole simulation is reduced by 52%. For demonstration purposes, we run a neural network simulation comprising thousands of neurons and millions of complex synapses on a 48-chip SpiNNaker board, generating millions of synaptic events per second.",robotics
10.1109/ECMR.2019.8870936,to_check,2019 European Conference on Mobile Robots (ECMR),IEEE,2019-09-06 00:00:00,ieeexplore,Real-time Vision-based Depth Reconstruction with NVidia Jetson,https://ieeexplore.ieee.org/document/8870936/,"Vision-based depth reconstruction is a challenging problem extensively studied in computer vision but still lacking universal solution. Reconstructing depth from single image is particularly valuable to mobile robotics as it can be embedded to the modern vision-based simultaneous localization and mapping (vSLAM) methods providing them with the metric information needed to construct accurate maps in real scale. Typically, depth reconstruction is done nowadays via fully-convolutional neural networks (FCNNs). In this work we experiment with several FCNN architectures and introduce a few enhancements aimed at increasing both the effectiveness and the efficiency of the inference. We experimentally determine the solution that provides the best performance/accuracy tradeoff and is able to run on NVidia Jetson with the framerates exceeding 16FPS for 320 × 240 input. We also evaluate the suggested models by conducting monocular vSLAM of unknown indoor environment on NVidia Jetson TX2 in real-time. Open-source implementation of the models and the inference node for Robot Operating System (ROS) are available at https://github.com/CnnDepth/tx2_fcnn_node.",robotics
10.1109/iCREATE.2014.6828372,to_check,2014 International Conference on Robotics and Emerging Allied Technologies in Engineering (iCREATE),IEEE,2014-04-24 00:00:00,ieeexplore,A comparison of various robotic control architectures for autonomous navigation of mobile robots,https://ieeexplore.ieee.org/document/6828372/,"For mobile robots, the most fundamental and pressing issue is that of autonomous navigation. Successful navigation of mobile robots is closely dependent on four vitals i.e. perception, localization, cognition and motion control. Implementation of each of these vital blocks requires consideration of at least one of the two well-known control architectures Deliberative Navigation Control and Reactive Navigation Control or a combination of the two, also known as a Hybrid Navigation Control. This paper compares each of these control architectures on the basis of their flexibility, ease of implementation, reactivity, robustness, efficiency and many other architecture specifications. The paper concludes with suggesting the schema that seems to be the best of each of these control schemes, on the basis of the analysis made, in order to cope with unknown and dynamic navigation problems encountered in real life scenarios.",robotics
10.1109/CADCG.2009.5246869,to_check,2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics,IEEE,2009-08-21 00:00:00,ieeexplore,A study on autonomous animated robots: Anibots,https://ieeexplore.ieee.org/document/5246869/,"In this paper, we demonstrate a design of autonomous virtual creatures (called animated robots: Anibots in this paper) and develop a design tool for animated robots. An animated robot can behave autonomously by using its own sensors and controllers on three-dimensional physically modeled environment. The developed tool can enable us to execute the simulation of Anibots on physical environment at any time during the modeling process. In order to simulate more realistic world, an approximate fluid environment model with low computational costs is presented. It is shown that a combinatorial use of neural network implementation for controllers and the genetic algorithm (GA) or the particle swarm optimization (PSO) is effective for emerging more realistic autonomous behaviours of animated robots.",robotics
10.1109/SmartWorld.2018.00106,to_check,"2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",IEEE,2018-10-12 00:00:00,ieeexplore,Real-Time Data Processing Architecture for Multi-Robots Based on Differential Federated Learning,https://ieeexplore.ieee.org/document/8560084/,"The emergency of ubiquitous intelligence in various things has become the ultimate cornerstone in building a smart interconnection of the physical world and the human world, which also caters to the idea of Internet of Things (IoT). Nowadays, robots as a new type of ubiquitous IoT devices have gained much attention. With the increasing number of distributed multi-robots, such smart environment generates unprecedented amounts of data. Robotic applications are faced with challenges of such big data: the serious real-time assurance and data privacy. Therefore, in order to obtain the big data values via knowledge sharing under the premise of ensuring the real-time data processing and data privacy, we propose a real-time data processing architecture for multi-robots based on the differential federated learning, called RT-robots architecture. A global shared model with differential privacy protection is trained on the cloud iteratively and distributed to multiple edge robots in each round, and the robotic tasks are processed locally in real time. Our implementation and experiments demonstrate that our architecture can be applied on multiple robotic recognition tasks, balance the trade-off between the performance and privacy.",robotics
10.1109/TAMD.2010.2086453,to_check,IEEE Transactions on Autonomous Mental Development,IEEE,2010-12-01 00:00:00,ieeexplore,Multilevel Darwinist Brain (MDB): Artificial Evolution in a Cognitive Architecture for Real Robots,https://ieeexplore.ieee.org/document/5599851/,"The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide autonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main original contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high computational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning capabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with minimal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic operation of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the validity of this evolutionary approach.",robotics
10.1109/ICCITECHN.2016.7860248,to_check,2016 19th International Conference on Computer and Information Technology (ICCIT),IEEE,2016-12-20 00:00:00,ieeexplore,A support vector machine approach for real time vision based human robot interaction,https://ieeexplore.ieee.org/document/7860248/,"Today humanoid robots are being exhibited to redact various task as a personal assistant of a human. To be an assistant, a robot needs to interact with human as a human. For this reason robot needs to understand the human gender, facial expression, facial gesture in real time. Ribo - A humanoid robot build in RoboSUST lab which has the ability to communicate in Bangla with the people speaking in Bengali. In this article the authors show the implementation of theoretical knowledge of the recognition of real time facial expression, detection of human gender and yes / no from facial gesture in Ribo. Real time facial expression and gender detection can be performed using Support Vector Machine (SVM). A prepared dataset containing the facial landmarks leveled as five different expression: sad, angry, smile, surprise and normal, is given to SVM to construct a classifier. For the prediction of any expression, facial images are taken in real time and provided the facial landmarks data to SVM. Local Binary Pattern(LBP) algorithm is used for extracting features from face images. These features leveled as male and female are responsible to build the classifier. The face gesture for detecting `yes/no' is performed by tracking the movement of face in a certain time. After those implementations the principal results will make a framework that will be used in Ribo to recognize human facial expression, facial gesture movement and detect human gender.",robotics
10.1109/IJCNN.2015.7280807,to_check,2015 International Joint Conference on Neural Networks (IJCNN),IEEE,2015-07-17 00:00:00,ieeexplore,Applying the canonical distributed Embodied Evolution algorithm in a collective indoor navigation task,https://ieeexplore.ieee.org/document/7280807/,"The automatic design of control systems for multi-robot teams that operate in real time is not affordable with traditional evolutionary algorithms mainly due to the huge computational requirements they imply. Embodied Evolution (EE) is an evolutionary paradigm that aims to address this problem through the embodiment of the individuals that make up the population in the physical robots. The interest for this type of evolutionary approach has been increasing steadily, leading to different algorithms and variations adapted to solve very specific practical cases. In a previous work, the authors started the implementation of a standard canonical EE algorithm that captures the more general principles of this paradigm and that can be applied to any distributed optimization problem. This canonical algorithm has been characterized already over a set of theoretical fitness landscapes corresponding to representative examples of the basic casuistry found in collective tasks. The current paper goes one step ahead in this research line, and the canonical algorithm is applied here in a collective navigation task in which a fleet of Micro Aerial Vehicles (MAVs) has to gather red rocks in an indoor scenario. The objective is to confirm that the characterization conclusions are generalizable to a practical case and to show that the canonical algorithm can be configured to operate as a specific algorithm easily.",robotics
10.1109/AMS.2017.22,to_check,2017 Asia Modelling Symposium (AMS),IEEE,2017-12-06 00:00:00,ieeexplore,Autonomous Rover Navigation Using GPS Based Path Planning,https://ieeexplore.ieee.org/document/8424312/,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino).",robotics
10.1109/IJCNN.2008.4633875,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,Bio-inspired stochastic chance-constrained multi-robot task allocation using WSN,https://ieeexplore.ieee.org/document/4633875/,"The multi-robot task allocation (MRTA) especially in unknown complex environment is one of the fundamental problems, a mostly important object in research of multi-robot. The MRTA problem is initially formulated as a chance-constrained optimization problem. Monte Carlo simulation is used to verify the accuracy of the solution provided by the algorithm. Ant colony optimization (ACO) algorithm based on bionic swarm intelligence was used. A hybrid intelligent algorithm combined Monte Carlo simulation and neural network is used for solving stochastic chance constrained models of MRTA. A practical implementation with real WSN and real mobile robots were carried out. In environment the successful implementation of tasks without collision validates the efficiency, stability and accuracy of the proposed algorithm. The convergence curve shows that as iterative generation grows, the utility increases and finally reaches a stable and optimal value. Results show that using sensor information fusion can greatly improve the efficiency. The algorithm is proved better than tradition algorithms without WSN for MRTA in real time.",robotics
10.1109/ICRA.2016.7487617,to_check,2016 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2016-05-21 00:00:00,ieeexplore,Decentralized multi-agent exploration with online-learning of Gaussian processes,https://ieeexplore.ieee.org/document/7487617/,"Exploration is a crucial problem in safety of life applications, such as search and rescue missions. Gaussian processes constitute an interesting underlying data model that leverages the spatial correlations of the process to be explored to reduce the required sampling of data. Furthermore, multi-agent approaches offer well known advantages for exploration. Previous decentralized multi-agent exploration algorithms that use Gaussian processes as underlying data model, have only been validated through simulations. However, the implementation of an exploration algorithm brings difficulties that were not tackle yet. In this work, we propose an exploration algorithm that deals with the following challenges: (i) which information to transmit to achieve multi-agent coordination; (ii) how to implement a light-weight collision avoidance; (iii) how to learn the data's model without prior information. We validate our algorithm with two experiments employing real robots. First, we explore the magnetic field intensity with a ground-based robot. Second, two quadcopters equipped with an ultrasound sensor explore a terrain profile. We show that our algorithm outperforms a meander and a random trajectory, as well as we are able to learn the data's model online while exploring.",robotics
10.1109/IROS.2008.4651150,to_check,2008 IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2008-09-26 00:00:00,ieeexplore,High-dimensional underactuated motion planning via task space control,https://ieeexplore.ieee.org/document/4651150/,"Kinodynamic planning algorithms have the potential to find feasible control trajectories which accomplish a task even in very nonlinear or constrained dynamical systems. Underactuation represents a particular form of a dynamic constraint, inherently present in many machines of interest (e.g., walking robots), and necessitates planning for long-term control solutions. A major limitation in motion planning techniques, especially for real-time implementation, is that they are only practical for relatively low degree-of-freedom problems. Here we present a model-based dimensionality reduction technique based on an extension of partial feedback linearization control into a task-space framework. This allows one to plan motions for a complex underactuated robot directly in a low-dimensional task-space, and to resolve redundancy with lower-priority tasks. We illustrate the potential of this approach with an extremely simple motion planning system which solves the swing-up problem for multi-link underactuated pendula, and discuss extensions to the control of walking.",robotics
10.1109/CEC.2003.1299606,to_check,"The 2003 Congress on Evolutionary Computation, 2003. CEC '03.",IEEE,2003-12-12 00:00:00,ieeexplore,Implementation of an immuno-genetic network on a real Khepera II robot,https://ieeexplore.ieee.org/document/1299606/,"The design of autonomous navigation systems for mobile robots, with simultaneous objectives to be satisfied such as garbage collection with integrity maintenance, requires refined coordination mechanisms to deal with modules of elementary behaviour. This paper shows the implementation on a real Khepera II robot of an immuno-genetic network for autonomous navigation that combines an evolutionary algorithm with a continuous immune network model. The proposed immuno-genetic system has the immune network implementing a dynamic process of decision-making, and the evolutionary algorithm defining the network structure. To be able to evaluate the controllers (immune networks) on the evolutionary process, a virtual environment was used for computer simulation, based on the characteristics of the navigation problem. The immune networks obtained by evolution were then analyzed and tested on new situations, presenting coordination capability in simple and more complex tasks. Some preliminary experiments on a real Khepera II robot demonstrate the feasibility of the evolved immune networks.",robotics
10.1109/IJCNN.2017.7965938,to_check,2017 International Joint Conference on Neural Networks (IJCNN),IEEE,2017-05-19 00:00:00,ieeexplore,Modeling direction selective visual neural network with ON and OFF pathways for extracting motion cues from cluttered background,https://ieeexplore.ieee.org/document/7965938/,"The nature endows animals robust vision systems for extracting and recognizing different motion cues, detecting predators, chasing preys/mates in dynamic and cluttered environments. Direction selective neurons (DSNs), with preference to certain orientation visual stimulus, have been found in both vertebrates and invertebrates for decades. In this paper, with respect to recent biological research progress in motion-detecting circuitry, we propose a novel way to model DSNs for recognizing movements on four cardinal directions. It is based on an architecture of ON and OFF visual pathways underlies a theory of splitting motion signals into parallel channels, encoding brightness increments and decrements separately. To enhance the edge selectivity and speed response to moving objects, we put forth a bio-plausible spatial-temporal network structure with multiple connections of same polarity ON/OFF cells. Each pair-wised combination is filtered with dynamic delay depending on sampling distance. The proposed vision system was challenged against image streams from both synthetic and cluttered real physical scenarios. The results demonstrated three major contributions: first, the neural network fulfilled the characteristics of a postulated physiological map of conveying visual information through different neuropile layers; second, the DSNs model can extract useful directional motion cues from cluttered background robustly and timely, which hits at potential of quick implementation in vision-based micro mobile robots; moreover, it also represents better speed response compared to a state-of-the-art elementary motion detector.",robotics
10.1109/AIMS.2014.23,to_check,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",IEEE,2014-11-20 00:00:00,ieeexplore,Online Tool for Benchmarking of Simulated Intervention Autonomous Underwater Vehicles: Evaluating Position Controllers in Changing Underwater Currents,https://ieeexplore.ieee.org/document/7102468/,"Benchmarking is nowadays an issue on robotic research platforms, due to the fact that it is not easy to reproduce previous experiments and knowing in detail in which real conditions other algorithms have been performed. Having a web-based tool to configure and execute benchmarks opens the door to new opportunities as the design of virtual tele-laboratories that permit the implementation of new algorithms using specific and detailed constraints. This is fundamental for designing benchmarks that allow the experiments to be made in a more scientific manner, taking into account that these experiments should be able to be reproduced again by other people under the same circumstances. In the context of underwater interventions with semi-autonomous robots, the situation gets even more interesting, specially those performed on real sea scenarios, which are expensive, and difficult to perform and reproduce. This paper presents the recent advances in the online configuration tool for benchmarking, a tool that is continuously being improved in our laboratory. Our last contribution focuses on evaluating position controllers for changing underwater currents and the possibility for the user to upload its own controllers to the benchmarking tool to get online performance results.",robotics
10.1109/SII.2010.5708353,to_check,2010 IEEE/SICE International Symposium on System Integration,IEEE,2010-12-22 00:00:00,ieeexplore,Realization and analysis of giant-swing motion using Q-Learning,https://ieeexplore.ieee.org/document/5708353/,"Many research papers have reported studies on sports robots that realize giant-swing motion. However, almost all these robots were controlled using trajectory planning methods, and few robots realized giant-swing motion by learning. Consequently, in this study, we attempted to construct a humanoid robot that realizes giant-swing motion by Q-learning, a reinforcement learning technique. The significant aspect of our study is that few robotic models were constructed beforehand; the robot learns giant-swing motion only by interaction with the environment during simulations. Our implementation faced several problems such as imperfect perception of the velocity state and robot posture issues caused by using only the arm angle. However, our real robot realized giant-swing motion by averaging the Q value and by using rewards - the absolute angle of the foot angle and the angular velocity of the arm angle-in the simulated learning data; the sampling time was 250 ms. Furthermore, the feasibility of generalization of learning for realizing selective motion in the forward and backward rotational directions was investigated; it was revealed that the generalization of learning is feasible as long as it does not interfere with the robot's motions.",robotics
10.1109/3477.499796,to_check,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",IEEE,1996-06-01 00:00:00,ieeexplore,Hidden state and reinforcement learning with instance-based state identification,https://ieeexplore.ieee.org/document/499796/,"Real robots with real sensors are not omniscient. When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the robot suffers from the hidden state problem. State identification techniques use history information to uncover hidden state. Some previous approaches to encoding history include: finite state machines, recurrent neural networks and genetic programming with indexed memory. A chief disadvantage of all these techniques is their long training time. This paper presents instance-based state identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ""memory-based"") learning to history sequences-instead of recording instances in a continuous geometrical space, we record instances in action-percept-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",robotics
10.1109/TNNLS.2014.2354400,to_check,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2015-08-01 00:00:00,ieeexplore,Opportunistic Behavior in Motivated Learning Agents,https://ieeexplore.ieee.org/document/6913540/,"This paper focuses on the novel motivated learning (ML) scheme and opportunistic behavior of an intelligent agent. It extends previously developed ML to opportunistic behavior in a multitask situation. Our paper describes the virtual world implementation of autonomous opportunistic agents learning in a dynamically changing environment, creating abstract goals, and taking advantage of arising opportunities to improve their performance. An opportunistic agent achieves better results than an agent based on ML only. It does so by minimizing the average value of all need signals rather than a dominating need. This paper applies to the design of autonomous embodied systems (robots) learning in real-time how to operate in a complex environment.",robotics
10.1109/TSMCB.2010.2089978,to_check,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",IEEE,2011-06-01 00:00:00,ieeexplore,"Walking Motion Generation, Synthesis, and Control for Biped Robot by Using PGRL, LPI, and Fuzzy Logic",https://ieeexplore.ieee.org/document/5640679/,"This paper proposes the implementation of fuzzy motion control based on reinforcement learning (RL) and Lagrange polynomial interpolation (LPI) for gait synthesis of biped robots. First, the procedure of a walking gait is redefined into three states, and the parameters of this designed walking gait are determined. Then, the machine learning approach applied to adjusting the walking parameters is policy gradient RL (PGRL), which can execute real-time performance and directly modify the policy without calculating the dynamic function. Given a parameterized walking motion designed for biped robots, the PGRL algorithm automatically searches the set of possible parameters and finds the fastest possible walking motion. The reward function mainly considered is first the walking speed, which can be estimated from the vision system. However, the experiment illustrates that there are some stability problems in this kind of learning process. To solve these problems, the desired zero moment point trajectory is added to the reward function. The results show that the robot not only has more stable walking but also increases its walking speed after learning. This is more effective and attractive than manual trial-and-error tuning. LPI, moreover, is employed to transform the existing motions to the motion which has a revised angle determined by the fuzzy motion controller. Then, the biped robot can continuously walk in any desired direction through this fuzzy motion control. Finally, the fuzzy-based gait synthesis control is demonstrated by tasks and point- and line-target tracking. The experiments show the feasibility and effectiveness of gait learning with PGRL and the practicability of the proposed fuzzy motion control scheme.",robotics
10.1109/ICRA.2019.8794187,to_check,2019 International Conference on Robotics and Automation (ICRA),IEEE,2019-05-24 00:00:00,ieeexplore,Deep Learning based Motion Prediction for Exoskeleton Robot Control in Upper Limb Rehabilitation,https://ieeexplore.ieee.org/document/8794187/,"The synchronization of the movement between exoskeleton robot and human arm is crucial for Robot-assisted training (RAT) in upper limb rehabilitation. In this paper, we propose a deep learning based motion prediction model which is applied to our recently developed 8 degrees-of-freedom (DoFs) upper limb rehabilitation exoskeleton, named NTUH-II. The human arm dynamics and surface electromyography (sEMG) can be first measured by two wireless sensors and used as input of deep learning model to predict user's motion. Then, the prediction can be used as desired motion trajectory of the exoskeleton. As a result, the robot arm can follow the movement on either side of the user's arm in real-time. Various experiments have been conducted to verify the performance of the proposed motion prediction model, and the results show that the proposed motion prediction implementation can reduce the mean absolute error and the average delay time of movement between human arm and robot arm.",robotics
10.1109/ISMA.2009.5164850,to_check,2009 6th International Symposium on Mechatronics and its Applications,IEEE,2009-03-26 00:00:00,ieeexplore,Fuzzy motion-based control for a bi-steerable mobile robot navigation,https://ieeexplore.ieee.org/document/5164850/,"This paper presents an implementation of a Fuzzy Motion Controller (FMC) to endow the mobile robot Robucar with capability to achieve the action behavior allowing smooth motion generation with intelligence in real-time. The robot state space (velocity and distances) is modeled in discrete intervals leading to linguistic variables. The fuzzy motion control rules are derived and used in a fuzzy inference mechanism to give the final control command to the robot actuators. Simulation and experimental results show FMC capabilities in generating smooth motions, illustrating then its adaptivity and intelligence.",robotics
10.1109/ICTC49870.2020.9289214,to_check,2020 International Conference on Information and Communication Technology Convergence (ICTC),IEEE,2020-10-23 00:00:00,ieeexplore,Learning Control Policy with Previous Experiences from Robot Simulator,https://ieeexplore.ieee.org/document/9289214/,"Advances in deep reinforcement learning enabled cost-efficient training of control policy of physical robot actions from robot simulators. Learning control policy in a simulated environment is cost-efficient over learning in a real environment. Reward engineering is one of the key components to train efficient control policy. For tasks with long horizons such as navigation and manipulation, a sparse reward is providing limited information. The robot simulator for a physical engine of physical robot manipulation has made it easy for researchers in the field of deep reinforcement learning to simulate complicated robot manipulation environments. In this paper, A robot manipulation simulator and a deep RL framework are utilized for implement a training control policy by utilizing previous experiences. For implementation, Recent innovation Hindsight Experience Replay (HER) algorithms with previous experiences to calculate dense rewards from a sparse reward is leveraged . Proposed implementation showed an approach to investigate the reward engineering method to formulate dense reward in robot manipulator tasks.",robotics
10.1109/ROBOT.1995.525344,to_check,Proceedings of 1995 IEEE International Conference on Robotics and Automation,IEEE,1995-05-27 00:00:00,ieeexplore,Neural network based iterative learning controller for robot manipulators,https://ieeexplore.ieee.org/document/525344/,"An efficient neural network based learning control scheme is proposed to solve the trajectory tracking controI problem of robot manipulators. The proposed approach has four distinctive characteristics: 1) good tracking performance can be achieved during the first learning trial; 2) learning algorithm for adjusting neural network weights is independent of the manipulator dynamic model, thus displays strong robustness to torque disturbances and model parameter uncertainty; 3) no acceleration measurement or estimation is needed; and 4) real-time implementation with a higher sampling rate is readily possible. Simulation results on a 3 degree-of-freedom manipulator are presented to show its validity.",robotics
10.1109/TSMCC.2004.840063,to_check,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",IEEE,2005-11-01 00:00:00,ieeexplore,"""Sticky Hands"": learning and generalization for cooperative physical interactions with a humanoid robot",https://ieeexplore.ieee.org/document/1522534/,"""Sticky Hands"" is a physical game for two people involving gentle contact with the hands. The aim is to develop relaxed and elegant motion together, achieve physical sensitivity-improving reactions, and experience an interaction at an intimate yet comfortable level for spiritual development and physical relaxation. We developed a control system for a humanoid robot allowing it to play Sticky Hands with a human partner. We present a real implementation including a physical system, robot control, and a motion learning algorithm based on a generalizable intelligent system capable itself of generalizing observed trajectories' translation, orientation, scale and velocity to new data, operating with scalable speed and storage efficiency bounds, and coping with contact trajectories that evolve over time. Our robot control is capable of physical cooperation in a force domain, using minimal sensor input. We analyze robot-human interaction and relate characteristics of our motion learning algorithm with recorded motion profiles. We discuss our results in the context of realistic motion generation and present a theoretical discussion of stylistic and affective motion generation based on, and motivating cross-disciplinary research in computer graphics, human motion production and motion perception.",robotics
10.1109/TSMCA.2003.811766,to_check,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",IEEE,2003-07-01 00:00:00,ieeexplore,Autonomous fuzzy parking control of a car-like mobile robot,https://ieeexplore.ieee.org/document/1235979/,"This paper is devoted to design and implement a car-like mobile robot (CLMR) that possesses autonomous garage-parking and parallel-parking capability by using real-time image processing. For fuzzy garage-parking control (FGPC) and fuzzy parallel-parking control (FPPC), feasible reference trajectories are provided for the fuzzy logic controller to maneuver the steering angle of the CLMR. We propose two FGPC methods and two FPPC methods to back-drive or head-in the CLMR to the garage and the parking lot, respectively. Simulation results illustrate the effectiveness of the developed schemes. The overall experimental setup of the parking system developed in this paper is composed of a host computer, a communication module, a CLMR, and a vision system. Finally, the image-based real-time implementation experiments of the CLMR demonstrate the feasibility and effectiveness of the proposed schemes.",robotics
10.1109/ACCESS.2021.3093340,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Ball Motion Control in the Table Tennis Robot System Using Time-Series Deep Reinforcement Learning,https://ieeexplore.ieee.org/document/9467347/,"One of the biggest challenges hindering a table tennis robot to play as well as a professional player is the ball’s accurate motion control, which depends on various factors such as the incoming ball’s position, linear, spin velocity and so forth. Unfortunately, some factors are almost impossible to be directly measured in real practice, such as the ball’s spin velocity, which is difficult to be estimated from vision due to the little texture on the ball’s surface. To perform accurate motion control in table tennis, this study proposes to learn a ball stroke strategy to guarantee desirable “target landing location” and the “over-net height” which are two key indicators to evaluate the quality of a stroke. To overcome the spin velocity challenge, a deep reinforcement learning (DRL) based stroke approach is developed with the spin velocity estimation capability, through which the system can predict the relative spin velocity of the ball and stroke it back accurately by iteratively learning from the robot-environment interactions. To pre-train the DRL-based strategy effectively, this paper develops a virtual table tennis playing environment, through which various simulated data can be collected. For the real table tennis robot implementation, experimental results demonstrate the superior performance of the proposed control strategy compared to that of the traditional aerodynamics-based method with an average landing error around 80mm and the landing-within-table probability higher than 70%.",robotics
10.1109/TCSI.2004.827654,to_check,IEEE Transactions on Circuits and Systems I: Regular Papers,IEEE,2004-05-01 00:00:00,ieeexplore,Reaction-diffusion navigation robot control: from chemical to VLSI analogic processors,https://ieeexplore.ieee.org/document/1296805/,"We introduce a new methodology and experimental implementations for real-time wave-based robot navigation in a complex, dynamically changing environment. The main idea behind the approach is to consider the robot arena as an excitable medium, in which moving objects-obstacles and the target-are represented by sites of autowave generation: the target generates attractive waves, while the obstacles repulsive ones. The moving robot detects traveling and colliding wave fronts and uses the information about dynamics of the autowaves to adapt its direction of collision-free motion toward the target. This approach allows us to achieve a highly adaptive robot behavior and thus an optimal path along which the robot reaches the target while avoiding obstacles. At the computational and experimental levels, we adopt principles of computation in reaction-diffusion (RD) nonlinear active media. Nonlinear media where autowaves are used for information processing purposes can therefore be considered as RD computing devices. In this paper, we design and experiment with three types of RD processors: experimental and computational Belousov-Zhabotinsky chemical processor, computational CNN processor, and experimental RD-CNN very large-scale integration chip-the complex analog and logic computing engine (CACE1k). We demonstrate how to experimentally implement robot navigation using space-time snapshots of active chemical medium and how to overcome low-speed limitation of this ""wetware"" implementation in CNN-based silicon processors.",robotics
10.1109/KCIC.2018.8628468,to_check,2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC),IEEE,2018-10-30 00:00:00,ieeexplore,An Incremental Episodic Memory Framework for Topological Map Building,https://ieeexplore.ieee.org/document/8628468/,"In this paper, an episodic memory learning framework is proposed for categorizing and encoding sensory information that acquired from a robot for environment adaptation and sensorimotor map building. The proposed learning model termed as Incremental Episodic Memory Adaptive Resonance Theory (In-EMART), consists two layers of ART networks which used to detect novel event encountered by the robot and learn the spatio-temporal relationship by creating neurons incrementally. A set of connected episodes forms a sensorimotor map that can be used for path planning and goal navigation autonomously. The experimental results for a mobile robot show that: (i) In-EMART can learn sensory data in real time which is important for robot implementation; (ii) the model solves the perceptual aliasing issue by recalling the connected episode neurons; (iii) compared with previous works, the proposed method further generates a sensorimotor map for connecting episodes together to navigate from one place to another continuously.",robotics
10.1109/IJCNN.2012.6252637,to_check,The 2012 International Joint Conference on Neural Networks (IJCNN),IEEE,2012-06-15 00:00:00,ieeexplore,Building block of a programmable neuromorphic substrate: A digital neurosynaptic core,https://ieeexplore.ieee.org/document/6252637/,"The grand challenge of neuromorphic computation is to develop a flexible brain-inspired architecture capable of a wide array of real-time applications, while striving towards the ultra-low power consumption and compact size of biological neural systems. Toward this end, we fabricated a building block of a modular neuromorphic architecture, a neurosynaptic core. Our implementation consists of 256 integrate-and-fire neurons and a 1,024×256 SRAM crossbar memory for synapses that fits in 4.2mm<sup>2</sup> using a 45nm SOI process and consumes just 45pJ per spike. The core is fully configurable in terms of neuron parameters, axon types, and synapse states and its fully digital implementation achieves one-to-one correspondence with software simulation models. One-to-one correspondence allows us to introduce an abstract neural programming model for our chip, a contract guaranteeing that any application developed in software functions identically in hardware. This contract allows us to rapidly test and map applications from control, machine vision, and classification. To demonstrate, we present four test cases (i) a robot driving in a virtual environment, (ii) the classic game of pong, (iii) visual digit recognition and (iv) an autoassociative memory.",robotics
10.1109/ASCC.2017.8287420,to_check,2017 11th Asian Control Conference (ASCC),IEEE,2017-12-20 00:00:00,ieeexplore,Deep learning for picking point detection in dense cluster,https://ieeexplore.ieee.org/document/8287420/,"This paper considers the problem of picking objects in cluster. This requires the robot to reliably detect the picking point for the known or unseen objects under the environment with occlusion, disorder and a variety of objects. We present a novel pipeline to detect picking point based on deep convolutional neural network (CNN). A two-dimensional picking configuration is proposed, thus an extensive data augmentation strategy is enabled and a labeled dataset is established quickly and easily. At last, we demonstrate the implementation of our method on a real robot and show that our method can accurately detect picking point of unseen objects and achieve a pick success of 91% in cluster bin-picking scenario.",robotics
10.1109/CCTA41146.2020.9206279,to_check,2020 IEEE Conference on Control Technology and Applications (CCTA),IEEE,2020-08-26 00:00:00,ieeexplore,Direct Force Feedback using Gaussian Process based Model Predictive Control,https://ieeexplore.ieee.org/document/9206279/,"Many robotic applications require control of the applied forces or moments. Model predictive control allows for the direct or indirect control of forces, while taking constraints into account. However, challenges arise when the robot environment that affects the force is highly variable, uncertain and difficult to model. Learning supported model predictive control makes it possible to combine the advantages of optimal control, such as the explicit consideration of constraints, with the advantages of machine learning, such as adaptive data-based modeling. In this paper Gaussian processes are used to model the contact forces that are applied in model predictive force control. The Gaussian process learns the static output mapping describing the interaction of the robot with the environment. It is shown that stability guarantees can be derived in a similar way as in classical predictive control. A proof-of-concept experimental implementation of a direct hybrid position force controller for a lightweight robot shows real-time feasibility.",robotics
10.1109/ICSMC.1995.537949,to_check,"1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century",IEEE,1995-10-25 00:00:00,ieeexplore,Dynamic path planning,https://ieeexplore.ieee.org/document/537949/,"Path planning is dynamic when the path is continually recomputed as more information becomes available. A computational framework for dynamic path planning is proposed which has the ability to provide navigational directions during the computation of the plan. Path planning is performed using a potential field approach. We use a specific type of potential function-a harmonic function-which has no local minima. The implementation is parallel and consists of a collection of communicating processes, across a network of SPARC &amp; SGI workstations using a message passing software package called PVM. The computation of the plan is performed independently of the execution of the plan. A hierarchical coarse-to-fine procedure is used to guarantee a correct control strategy at the expense of accuracy. We have successfully navigated a Nomad robot around our lab space with no a priori map in real-time. The result of the described approach is a parallel implementation which permits dynamic path planning using available processor resources.",robotics
10.1109/IROS40897.2019.8967568,to_check,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2019-11-08 00:00:00,ieeexplore,From Pixels to Buildings: End-to-end Probabilistic Deep Networks for Large-scale Semantic Mapping,https://ieeexplore.ieee.org/document/8967568/,"We introduce TopoNets, end-to-end probabilistic deep networks for modeling semantic maps with structure reflecting the topology of large-scale environments. TopoNets build a unified deep network spanning multiple levels of abstraction and spatial scales, from pixels representing geometry of local places to high-level descriptions of semantics of buildings. To this end, TopoNets leverage complex spatial relations expressed in terms of arbitrary, dynamic graphs. We demonstrate how TopoNets can be used to perform end-to-end semantic mapping from partial sensory observations and noisy topological relations discovered by a robot exploring large-scale office spaces. Thanks to their probabilistic nature and generative properties, TopoNets extend the problem of semantic mapping beyond classification. We show that TopoNets successfully perform uncertain reasoning about yet unexplored space and detect novel and incongruent environment configurations unknown to the robot. Our implementation of TopoNets achieves real-time, tractable and exact inference, which makes these new deep models a promising, practical solution to mobile robot spatial understanding at scale.",robotics
10.1109/IROS.1994.407570,to_check,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94),IEEE,1994-09-16 00:00:00,ieeexplore,Generation of optimal configuration for a redundant manipulator with a trained neural network,https://ieeexplore.ieee.org/document/407570/,"Redundant manipulators have more degrees of freedom than what is absolutely necessary for performing a task. The extra degrees of freedom can be used for avoiding obstacles or to optimize certain performance indices like manipulability or task compatibility. Maximizing manipulability keeps the manipulator away from singularities and provides more velocity transmission ratios in all directions. Optimizing task compatibility improves the force/velocity transmission ratios in the specified directions. However, the real time implementation of various optimizing algorithms is difficult because of the need of large computing time. In the present work, robot configurations for an optimum performance index are computed throughout the workspace. These configurations are then used to train a layered feed forward neural network (FFNN). During operation of the robot, the trained neural net outputs optimal configurations in real-time. The neural net captures the gross behaviour of the training data rather than memorizing the individual data, as in a lookup table. Thus its output is smooth and ideally suited for control purposes. We have simulated this approach on a 3-DOF redundant planar manipulator and the results are discussed in this paper.&lt;<ETX>&gt;</ETX>",robotics
10.1109/ETFA.2005.1612680,to_check,2005 IEEE Conference on Emerging Technologies and Factory Automation,IEEE,2005-09-22 00:00:00,ieeexplore,Hyper-redundant robotic micro-grippers with neural control,https://ieeexplore.ieee.org/document/1612680/,"The paper introduces a novel approach for the kinematic coordination of mechanical robot micro-grippers on the basis of neural networks. Conventional robot systems use specialized grippers for specific tasks. For objects with an amorphous structure, variable shape or small dimensions, conventional grippers become unreliable due to several reasons. The present paper presents an approach on the basis of a tentacle shaped micro-gripper with a high number of links and rotational articulated joints. The proposed method for gripping an object is based on wrapping the manipulator's links around the object in order to establish a firm grip. Sensors located near the joints of the micro-manipulator detect the corresponding distances to the object. Since the multi-link manipulator has a high degree of kinematic redundancy (consider up to several hundred links in a chain), the implementation of an effective trajectory control unit is a challenging task, especially if sensor-based real-time coordination is required. In this paper, we use an optimized geometrical path generator in order to teach dynamic neural nets a certain motion behavior, dependent on distance sensor signals. We show that the neural net is able to learn the procedural knowledge for the gripping process with ability of generalization and discuss the results",robotics
10.1109/SSST.1998.660084,to_check,Proceedings of Thirtieth Southeastern Symposium on System Theory,IEEE,1998-03-10 00:00:00,ieeexplore,Implementation of a navigational neural network on a parallel DSP board,https://ieeexplore.ieee.org/document/660084/,"This work presents a neural network architecture that is motivated by the learning and memory characteristics of a part of the brain known as hippocampus, which is important in navigational behavior in humans and animals. Neural networks perform nonlinear transformations on data to yield suitable classification or control actions. In our case, the navigation network takes the distance information as data and maps it to control actions by the mobile robot. Navigation is a very important engineering problem for unknown or hazardous environments to ensure the safety of equipment and human life. Hardware implementation can benefit applications in real time where speed is the major concern. Our objective is to implement such a navigational neural network in parallel so that real time performance can be achieved by using a parallel DSP board system. Supplementary studies are also being carried out on the IBM SP2 supercomputer to understand the design and scaling properties of the parallel algorithm.",robotics
10.1109/IJCNN.1999.832713,to_check,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),IEEE,1999-07-16 00:00:00,ieeexplore,Nonlinear system adaptive trajectory tracking by dynamic neural control,https://ieeexplore.ieee.org/document/832713/,"In this article, new nonlinear control techniques based on dynamic neural networks are presented. The authors discuss the implementation of a modified identification algorithm using dynamic neural networks as well as a control law, based on the neural identifier, which eliminates modeling error effects via sliding mode techniques. Simulation and real time results are presented for systems like an inverted pendulum and a full actuated robot manipulator.",robotics
10.1109/SMC.2016.7844571,to_check,"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",IEEE,2016-10-12 00:00:00,ieeexplore,Robotic attention manager using fuzzy controller with fractal analysis,https://ieeexplore.ieee.org/document/7844571/,"This paper is focused on the application of fractal analysis in the attention management of humanoid robot. We designed a fuzzy controller to combine the face detection, movement detection and the fractal dimension signals to control the head movement of robot Nao. Also, the gaze problem is addressed by the controller. Implementation details are included in the paper, including configuration parameters, which we found optimal according to subjective analysis and possibilities of current hardware. We found the fuzzy controller to be advantageous for implementation of attention manager because of smoothing of the movement of robot when compared to the simple rule based implementation, and also because the fuzzy controller implementation of manager is more clear than a naive if-then heuristics code. We also found the fractal dimension to be useful additional signal for attention management of robot, which can be computed in near real-time on current hardware and static input images.",robotics
10.1109/ASAP.2018.8445099,to_check,"2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)",IEEE,2018-07-12 00:00:00,ieeexplore,Towards Hardware Accelerated Reinforcement Learning for Application-Specific Robotic Control,https://ieeexplore.ieee.org/document/8445099/,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment based on how good the decisions are and tries to find an optimal decision-making policy that maximises its longterm cumulative reward. This paper presents a novel approach which has showon promise in applying accelerated simulation of RL policy training to automating the control of a real robot arm for specific applications. The approach has two steps. First, design space exploration techniques are developed to enhance performance of an FPGA accelerator for RL policy training based on Trust Region Policy Optimisation (TRPO), which results in a 43% speed improvement over a previous FPGA implementation, while achieving 4.65 times speed up against deep learning libraries running on GPU and 19.29 times speed up against CPU. Second, the trained RL policy is transferred to a real robot arm. Our experiments show that the trained arm can successfully reach to and pick up predefined objects, demonstrating the feasibility of our approach.",robotics
10.1109/MED48518.2020.9183337,to_check,2020 28th Mediterranean Conference on Control and Automation (MED),IEEE,2020-09-18 00:00:00,ieeexplore,Unsupervised Learning for Subterranean Junction Recognition Based on 2D Point Cloud,https://ieeexplore.ieee.org/document/9183337/,"This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology.",robotics
10.1109/IROS.2013.6696581,to_check,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2013-11-07 00:00:00,ieeexplore,Unsupervised learning of predictive parts for cross-object grasp transfer,https://ieeexplore.ieee.org/document/6696581/,"We present a principled solution to the problem of transferring grasps across objects. Our approach identifies, through autonomous exploration, the size and shape of object parts that consistently predict the applicability of a grasp across multiple objects. The robot can then use these parts to plan grasps onto novel objects. By contrast to most recent methods, we aim to solve the part-learning problem without the help of a human teacher. The robot collects training data autonomously by exploring different grasps on its own. The core principle of our approach is an intensive encoding of low-level sensorimotor uncertainty with probabilistic models, which allows the robot to generalize the noisy autonomously-generated grasps. Object shape, which is our main cue for predicting grasps, is encoded with surface densities, that model the spatial distribution of points that belong to an object's surface. Grasp parameters are modeled with grasp densities, that correspond to the spatial distribution of object-relative gripper poses that lead to a grasp. The size and shape of grasp-predicting parts are identified by sampling the cross-object correlation of local shape and grasp parameters. We approximate sampling and integrals via Monte Carlo methods to make our computer implementation tractable. We demonstrate the applicability of our method in simulation. A proof of concept on a real robot is also provided.",robotics
10.1109/ROBOT.1991.131999,to_check,Proceedings. 1991 IEEE International Conference on Robotics and Automation,IEEE,1991-04-11 00:00:00,ieeexplore,Visual navigation around curved obstacles,https://ieeexplore.ieee.org/document/131999/,"An approach to path-planning around smooth obstacles that exploits visually derived geometry is proposed. A moving robot can scan the silhouette or apparent contour of an obstacle and estimate a minimum length path. This is done by seeking geodesics which can be extrapolated smoothly, around the obstacle and towards the goal. Preliminary implementation of this idea uses a real-time visual contour tracker running at 16 Hz, with a camera mounted on an Adept robot arm. The camera first dithers to generate visual motion, a safe path is estimated, and the robot steers the camera around the obstacle with a clearance of a few millimeters.&lt;<ETX>&gt;</ETX>",robotics
10.1109/LRA.2020.2965911,to_check,IEEE Robotics and Automation Letters,IEEE,2020-04-01 00:00:00,ieeexplore,Aggressive Perception-Aware Navigation Using Deep Optical Flow Dynamics and PixelMPC,https://ieeexplore.ieee.org/document/8957291/,"Recently, vision-based control has gained traction by leveraging the power of machine learning. In this work, we couple a model predictive control (MPC) framework to a visual pipeline. We introduce deep optical flow (DOF) dynamics, which is a combination of optical flow and robot dynamics. Using the DOF dynamics, MPC explicitly incorporates the predicted movement of relevant pixels into the planned trajectory of a robot. Our implementation of DOF is memory-efficient, data-efficient, and computationally cheap so that it can be computed in real-time for use in an MPC framework. The suggested Pixel Model Predictive Control (PixelMPC) algorithm controls the robot to accomplish a high-speed racing task while maintaining visibility of the important features (gates). This improves the reliability of vision-based estimators for localization and can eventually lead to safe autonomous flight. The proposed algorithm is tested in a photorealistic simulation with a high-speed drone racing task.",robotics
10.1109/70.68083,to_check,IEEE Transactions on Robotics and Automation,IEEE,1991-02-01 00:00:00,ieeexplore,An automatic navigation system for vision guided vehicles using a double heuristic and a finite state machine,https://ieeexplore.ieee.org/document/68083/,"A navigation system for automatic vision-guided vehicles which uses an efficient double heuristic search algorithm for path planning is presented. It is capable of avoiding unknown obstacles and recovering from unidentifiable locations. A linked list representation of the path network database makes the implementation feasible in any high-level language and renders it suitable for real-time application. Extensive simulated experiments have been conducted to verify the validity of the proposed algorithms. The combination of the techniques of robot navigation in unexplored terrain and the global map method proved to be a valid technique for automated guided vehicle (AGV) guidance. A learning mechanism is used in the AGV by updating the path network during navigation. Simulated results supported all the theoretically expected conclusions, since the robot planned its path correctly between the requested nodes and maneuvered its way around the obstacles. Overall, the results were very encouraging.&lt;<ETX>&gt;</ETX>",robotics
10.1007/978-3-030-80624-8_23,to_check,"Advances in Artificial Intelligence, Software and Systems Engineering",Springer,2021-01-01 00:00:00,springer,Artificial Intelligence and Tomorrow’s Education,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80624-8_23,"Nowadays, there is a rapid technological progress around the world that has enabled realities long ago unimaginable. We live in a technological era that represents new possibilities and challenges for society, and for the educational models in each country [ 1 ]. Research on smart education, which has forced the educational community to rethink on new ways of learning and teaching has been developed globally. Due to the advent of artificial intelligence (AI), the educational model for both, teachers and students will change. Nevertheless, to transform educational systems, it is necessary to update and train students, educators, and administrators effectively [ 2 ]. This research aims to describe the possible applications of AI in education from: 1) the automation of administrative tasks; 2) collection and analysis of information [ 3 ] to create smart content; 3) the implementation of virtual assistants in the teaching-learning process; 4) the potential delivery of lectures by humanoid robots with AI.",robotics
10.1007/978-3-030-77070-9_10,to_check,Artificial Intelligence for a Sustainable Industry 4.0,Springer,2021-01-01 00:00:00,springer,Smart and Intelligent Chatbot Assistance for Future Industry 4.0,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-77070-9_10,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers.",robotics
10.1007/978-981-13-1702-6_26,to_check,Image and Graphics Technologies and Applications,Springer,2018-01-01 00:00:00,springer,Fall Detection System Based on Mobile Robot,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-13-1702-6_26,"This paper proposed an accurate fall detection algorithm based on the feature of whole human body. The feature is extracted from convolutional neural network. The implementation of algorithm is integrated into a hardware system based on a visual mobile robot platform. To ensure the robustness and flexibility of algorithm in actual situation, a set of systemic strategies was applied on mobile robot. Finally, sufficient experiments on public dataset were conduct on our algorithm. Moreover, in a real indoor scene, experiment results proved the efficiency and precision of the designed fall detection system.",robotics
10.1007/978-3-030-00308-1_8,to_check,RoboCup 2017: Robot World Cup XXI,Springer,2018-01-01 00:00:00,springer,Toward Real-Time Decentralized Reinforcement Learning Using Finite Support Basis Functions,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-00308-1_8,"This paper addresses the design and implementation of complex Reinforcement Learning (RL) behaviors where multi-dimensional action spaces are involved, as well as the need to execute the behaviors in real-time using robotic platforms with limited computational resources and training times. For this purpose, we propose the use of decentralized RL, in combination with finite support basis functions as alternatives to Gaussian RBF, in order to alleviate the effects of the curse of dimensionality on the action and state spaces respectively, and to reduce the computation time. As testbed, a RL based controller for the in-walk kick in NAO robots, a challenging and critical problem for soccer robotics, is used. The reported experiments show empirically that our solution saves up to 99.94% of execution time and 98.82% of memory consumption during execution, without diminishing performance compared to classical approaches.",robotics
10.1007/s11063-015-9426-5,to_check,Neural Processing Letters,Springer,2016-04-01 00:00:00,springer,Introducing Synaptic Delays in the NEAT Algorithm to Improve Modelling in Cognitive Robotics,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-015-9426-5,"This paper describes and tests an approach to improve the temporal processing capabilities of the neuroevolution of augmenting topologies (NEAT) algorithm. This algorithm is quite popular within the robotics community for the production of trained neural networks without having to determine a priori their size and topology. The main drawback of the traditional NEAT algorithm is that, even though it can implement recurrent synaptic connections, which allow it to perform some time related processing tasks, its capabilities are rather limited, especially when dealing with precise time dependent phenomena. NEAT’s ability to capture the underlying dynamics that correspond to complex time series still has a lot of room for improvement. To address this issue, the paper describes a new implementation of the NEAT algorithm that is able to generate artificial neural networks (ANNs) with trainable time delayed synapses in addition to its previous capacities. We show that this approach, called $$\uptau $$ τ -NEAT improves the behavior of the neural networks obtained when dealing with complex time related processes. Several examples are presented, both dealing with the generation of ANNs that are able to produce complex theoretical signals such as chaotic signals or real data series, as in the case of the monthly number of international airline passengers or monthly $$\hbox {CO}_{2}$$ CO 2 concentrations. In these examples, $$\uptau $$ τ -NEAT clearly improves over the traditional NEAT algorithm in these tasks. A final example of the integration of this approach within a robot cognitive mechanism is also presented, showing the clear improvements it could provide in the modeling required for many cognitive processes.",robotics
10.1016/j.enggeo.2020.105817,to_check,Engineering Geology,scopus,2020-12-05,sciencedirect,"Successful implementations of a real-time and intelligent early warning system for loess landslides on the Heifangtai terrace, China",https://api.elsevier.com/content/abstract/scopus_id/85090411791,"Real-time monitoring and intelligent early warning system are crucial and significant to take mitigation measures and reduce casualties and property losses related to landslides. It is difficult to obtain entire monitoring data in the accelerated deformation phase in a landslide event, and hard to issue early warning information using a traditional monitoring approach with fixed and low sampling frequency. Displacement increments of loess landslides induced by agriculture irrigation on the Heifangtai terrace could be sudden and extremely rapid. Typical landslide types include loess flowslides and loess falls. It is of practical significance to develop a self-adaptive data acquisition monitoring technique and establish a real-time landslide early warning system (LEWS) to meet the needs for risk mitigation of rapid sliding slopes on the Heifangtai terrace. The monitoring technique can wirelessly transmit displacement data and the LEWS was devised using the new artificial intelligence. The LEWS could automatically release the warning information in advance of the event once the early warning parameters exceed default thresholds. In this study, the early warning procedures, real-time monitoring approach, intelligent LEWS, a multiple criteria warning model, warning release and emergency mitigation measures, and performance are introduced in detail. Six loess landslides at Heifangtai and eight landslides in other regions of China have been successfully warned since its implementation in 2012. This study proposed an effective and practical solution for the early warning of loess landslides at Heifangtai. Two typical loess landslides that had successful early warnings at Heifangtai were presented. The successful implementation could serve as a reference for global rapid slope failure cases, considering the complex nature of landslide behaviors and failure mechanisms.",robotics
10.1016/j.eswa.2017.11.011,to_check,Expert Systems with Applications,scopus,2018-06-15,sciencedirect,Towards a common implementation of reinforcement learning for multiple robotic tasks,https://api.elsevier.com/content/abstract/scopus_id/85035079318,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",robotics
10.1016/j.robot.2018.02.010,to_check,Robotics and Autonomous Systems,scopus,2018-06-01,sciencedirect,Visual attention and object naming in humanoid robots using a bio-inspired spiking neural network,https://api.elsevier.com/content/abstract/scopus_id/85044145526,"Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred’ orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors.",robotics
10.1016/j.physa.2017.11.155,to_check,Physica A: Statistical Mechanics and its Applications,scopus,2018-03-15,sciencedirect,Efficient digital implementation of a conductance-based globus pallidus neuron and the dynamics analysis,https://api.elsevier.com/content/abstract/scopus_id/85042234061,"Balance between biological plausibility of dynamical activities and computational efficiency is one of challenging problems in computational neuroscience and neural system engineering. This paper proposes a set of efficient methods for the hardware realization of the conductance-based neuron model with relevant dynamics, targeting reproducing the biological behaviors with low-cost implementation on digital programmable platform, which can be applied in wide range of conductance-based neuron models. Modified GP neuron models for efficient hardware implementation are presented to reproduce reliable pallidal dynamics, which decode the information of basal ganglia and regulate the movement disorder related voluntary activities. Implementation results on a field-programmable gate array (FPGA) demonstrate that the proposed techniques and models can reduce the resource cost significantly and reproduce the biological dynamics accurately. Besides, the biological behaviors with weak network coupling are explored on the proposed platform, and theoretical analysis is also made for the investigation of biological characteristics of the structured pallidal oscillator and network. The implementation techniques provide an essential step towards the large-scale neural network to explore the dynamical mechanisms in real time. Furthermore, the proposed methodology enables the FPGA-based system a powerful platform for the investigation on neurodegenerative diseases and real-time control of bio-inspired neuro-robotics.",robotics
10.1016/j.eswa.2017.03.002,to_check,Expert Systems with Applications,scopus,2017-09-01,sciencedirect,Incremental Q-learning strategy for adaptive PID control of mobile robots,https://api.elsevier.com/content/abstract/scopus_id/85015894497,"Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.",robotics
10.1016/j.neunet.2015.07.017,to_check,Neural Networks,scopus,2015-11-01,sciencedirect,Cost-efficient FPGA implementation of basal ganglia and their Parkinsonian analysis,https://api.elsevier.com/content/abstract/scopus_id/84940021490,"The basal ganglia (BG) comprise multiple subcortical nuclei, which are responsible for cognition and other functions. Developing a brain–machine interface (BMI) demands a suitable solution for the real-time implementation of a portable BG. In this study, we used a digital hardware implementation of a BG network containing 256 modified Izhikevich neurons and 2048 synapses to reliably reproduce the biological characteristics of BG on a single field programmable gate array (FPGA) core. We also highlighted the role of Parkinsonian analysis by considering neural dynamics in the design of the hardware-based architecture. Thus, we developed a multi-precision architecture based on a precise analysis using the FPGA-based platform with fixed-point arithmetic. The proposed embedding BG network can be applied to intelligent agents and neurorobotics, as well as in BMI projects with clinical applications. Although we only characterized the BG network with Izhikevich models, the proposed approach can also be extended to more complex neuron models and other types of functional networks.",robotics
10.1016/j.neucom.2015.04.066,to_check,Neurocomputing,scopus,2015-01-01,sciencedirect,A general CPG network and its implementation on the microcontroller,https://api.elsevier.com/content/abstract/scopus_id/84952631900,"Over the last few years, it has been confirmed by the biologists that Central Pattern Generator (CPG) is the key mechanism of generating adaptive and versatile locomotion in animals. This gives a new inspiration of locomotion control for robots. Although the design of CPG controller using coupled oscillators has been proposed previously, it cannot comprehensively reproduce different rhythmic motion along with smooth transitions to mimic the versatility of animal locomotion. To tackle this problem, we propose a general CPG model emphasizing on its stability analysis, smooth transition and implementation architecture. Global exponential stability of the model is derived by using strict mathematical analysis. Transitions between different oscillating forms are also smooth, and the implementation architecture has low computational cost, thus suitable for microcontrollers. Moreover, all control parameters not only have clear relationships with the physical outputs, but also can be modified online. Both virtual and real robotic fish are developed to verify the effectiveness of our CPG controller together with the proposed implementation architecture, through the experiments of four locomotion gaits and three transitions among them.",robotics
10.1016/j.neunet.2014.10.001,to_check,Neural Networks,scopus,2015-01-01,sciencedirect,Trends in extreme learning machines: A review,https://api.elsevier.com/content/abstract/scopus_id/84908682236,"Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives.",robotics
10.1016/j.engappai.2013.12.003,to_check,Engineering Applications of Artificial Intelligence,scopus,2014-02-01,sciencedirect,Hardware opposition-based PSO applied to mobile robot controllers,https://api.elsevier.com/content/abstract/scopus_id/84892858663,"Adaptation of mobile robot controllers commonly requires the computation of optimal points of operation. Specifically, for miniature mobile robots with serious computational limitations, that are typical of embedded systems, one of the main challenges is the adaptation of efficient computational methods in order to find solutions of complex optimization problems, which demand large execution times. This drawback compels the design of high-performance parallel optimization algorithms which must run over embedded system platforms. This paper describes how adequate hardware implementations of the Particle Swarm Optimization (PSO) algorithm can be useful for real time adaptation of mobile robot controllers. For achieving this, a new architecture is proposed, which is based on an FPGA implementation of the opposition-based learning (OBL) approach applied to the PSO (for short HPOPSO), and which explores the intrinsic parallelism of this algorithm in order to adjust the weights of a neural robot controller in real time according to desired behaviors. The proposed HPOPSO was applied to the learning-from-demonstration problem in which a teacher performs executions of the desired behavior. Effectiveness of the proposed architecture was demonstrated by numerical simulations and the feasibility of the adaptive behavior of the neural robot controller was confirmed for two obstacle avoidance case studies that were preserved when one or more failures on the distance sensors occur. The HPOPSO, which uses the OBL technique, improves the quality of the solutions in comparison with the standard PSO. Comparisons of the adaptation time between hardware and software approaches have demonstrated the suitability of the FPGA implementation of the proposed HPOPSO for attending specific requirements of embedded system applications.",robotics
10.1016/j.robot.2014.08.002,to_check,Robotics and Autonomous Systems,scopus,2014-01-01,sciencedirect,Integrated neural and robotic simulations. Simulation of cerebellar neurobiological substrate for an object-oriented dynamic model abstraction process,https://api.elsevier.com/content/abstract/scopus_id/84908424049,"Experimental studies of the Central Nervous System (CNS) at multiple organization levels aim at understanding how information is represented and processed by the brain’s neurobiological substrate. The information processed within different neural subsystems is neurocomputed using distributed and dynamic patterns of neural activity. These emerging patterns can be hardly understood by merely taking into account individual cell activities. Studying how these patterns are elicited in the CNS under specific behavioral tasks has become a groundbreaking research topic in system neuroscience. This methodology of synthetic behavioral experimentation is also motivated by the concept of embodied neuroscience, according to which the primary goal of the CNS is to solve/facilitate the body–environment interaction.
                  With the aim to bridge the gap between system neuroscience and biological control, this paper presents how the CNS neural structures can be connected/integrated within a body agent; in particular, an efficient neural simulator based on EDLUT (Ros et al., 2006) has been integrated within a simulated robotic environment to facilitate the implementation of object manipulating closed loop experiments (action–perception loop). This kind of experiment allows the study of the neural abstraction process of dynamic models that occurs within our neural structures when manipulating objects.
                  The neural simulator, communication interfaces, and a robot platform have been efficiently integrated enabling real time simulations. The cerebellum is thought to play a crucial role in human-body interaction with a primary function related to motor control which makes it the perfect candidate to start building an embodied nervous system as illustrated in the simulations performed in this work.",robotics
10.1016/j.procs.2013.05.187,to_check,Procedia Computer Science,scopus,2013-01-01,sciencedirect,Comparing support vector machines and artificial neural networks in the recognition of steering angle for driving of mobile robots through paths in plantations,https://api.elsevier.com/content/abstract/scopus_id/84896966222,"The use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route.",robotics
10.1016/j.eswa.2011.04.207,to_check,Expert Systems with Applications,scopus,2011-10-01,sciencedirect,Intelligent control based on wavelet decomposition and neural network for predicting of human trajectories with a novel vision-based robotic,https://api.elsevier.com/content/abstract/scopus_id/79959924555,"In this paper, an intelligent novel vision-based robotic tracking model is developed to predict the performance of human trajectories with a novel vision-based robotic tracking system. The developed model is based on wavelet packet decomposition, entropy and neural network. We represent an implementation of a novel vision-based robotic tracking system based on wavelet decomposition and artificial neural (WD-ANN) which can track desired human trajectory pattern in real environments. The input–output data set of the novel vision-based robotic tracking system were first stored and than these data sets were used to predict the robotic tracking based on WD-ANN. In simulations, performance measures were obtained to compare the predicted and human–robot trajectories like actual values for model validation. In statistical analysis, the RMS value is 0.0729 and the R
                     2 value is 99.76% for the WD-ANN model. This study shows that the values predicted with the WD-ANN can be used to predict human trajectory by vision-based robotic tracking system quite accurately. All simulations have shown that the proposed method is more effective and controls the systems quite successful.",robotics
10.1109/ACCESS.2021.3054124,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Velocity Planning Method Base on Fuzzy Neural Network for Autonomous Vehicle,https://ieeexplore.ieee.org/document/9335025/,"In order to improve the comfort performance and reduce the planning algorithm complexity in autonomous vehicle, an intelligent longitudinal velocity planning method based on fuzzy neural network (FNN) is proposed. With the manual driving experience, fuzzy planning model is established. By utilizing the self-learning function of neural network, fuzzy planning model is modified, which is attempted to establish FNN planning model. The planning method is applied to velocity planning. Three kinds of driving scenes are analyzed, and velocity planning models based on FNN are established accordingly. The simulation and experiment results indicate that acceleration generated by FNN planning model has good smooth property, and it is easy to be tracked by the subsequent control module. Compared with traditional method, the proposed method has certain anti-disturbance ability and self-adaptability. Also, the proposed method is convenient for engineering application, which ensures both the real-time performance and stability of the algorithm.",autonomous vehicle
10.23919/ECC51009.2020.9143756,to_check,2020 European Control Conference (ECC),IEEE,2020-05-15 00:00:00,ieeexplore,Deep Traffic Light Perception with Spatiotemporal Analysis for Autonomous Driving,https://ieeexplore.ieee.org/document/9143756/,"Traffic light perception is crucial for autonomous driving. In this paper, a three-step method for traffic-light detection, recognition and understanding is developed to guide self-driving vehicles to pass crossroads. The first step adopts a state-of-the-art deep neural object detection architecture to detect traffic lights. The second step designs a novel four-channel convolutional neural network to classify traffic lights. The last step develops a spatiotemporal trajectory analysis method to filter out false positives and to guide self-driving vehicles. The proposed method is evaluated on two datasets, and experiment results show that it can efficiently perceive traffic-light states and perform better than baseline considered. Furthermore, real vehicle testings are conducted, which demonstrate the effectiveness of the proposed method.",autonomous vehicle
10.23919/ChiCC.2019.8865311,to_check,2019 Chinese Control Conference (CCC),IEEE,2019-07-30 00:00:00,ieeexplore,Collaborative Multi-Agent Tracking based on Distributed Learning,https://ieeexplore.ieee.org/document/8865311/,"In order to reasonably allocate the Unmanned Aerial Vehicle (UAV) resources and keep the target tracked, we propose a collaborative multi-agent tracking method based on distributed learning. Considering the interactive fusion of information between multiple UAVs, we use the Information Filter (IF) to transform the target state fusion problem into a simple algebraic superposition. Meanwhile, the UAV's movements can be planned in real time so that the UAV formation can obtain more accurate target measurement as much as possible. The simulation experiment of collaborative multi-UAV tracking proves that the distributed multi-UAV system can make reasonable actions and track targets effectively.",autonomous vehicle
10.1109/GNCC42960.2018.9019103,to_check,"2018 IEEE CSAA Guidance, Navigation and Control Conference (CGNCC)",IEEE,2018-08-12 00:00:00,ieeexplore,Research on Drogue Detection Algorithm for Aerial Refueling (IEEE/CSAA GNCC),https://ieeexplore.ieee.org/document/9019103/,"Unmanned aerial vehicle(UAV) has become more and more widely used in military and non-military applications, but endurance has become a major limiting factor in the development of UAV. In order to increase the UAV's endurance, this paper uses a vision guidance method to achieve automatic aerial refueling technology. This paper proposes an automatic refueling technology based on a deep-learning algorithm for detection. The Faster RCNN algorithm can simultaneously recognize the fuel receiver and the Drogue at long-distance and returns the center coordinate value of the receiver. If the pixel area of the identified Drogue is greater than a certain threshold, the central coordinate value of the Drogue is returned. The experiment results show that the average accuracy of the algorithm reaches 67.75%, and the real-time performance is about 5HZ.",autonomous vehicle
10.1109/ICIVC.2018.8492751,to_check,"2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)",IEEE,2018-06-29 00:00:00,ieeexplore,Siamese Network for Object Tracking in Aerial Video,https://ieeexplore.ieee.org/document/8492751/,"In Unmanned Aerial Vehicle (UAV) videos, object tracking remains a challenge, due to its low spatial resolution and poor real-time performance. Recently, methods of deep learning have made great progress in object tracking in computer vision, especially fully-convolutional siamese neural networks (SiamFC). Inspired by it, this paper aims to investigate the use of SiamFC for object tracking in UAV videos. The network is trained on part of a UAV123 dataset and Stanford Drone dataset. First, exemplar image is extracted from the first frame and search regions are extracted in the following frames. Then, a Siamese network is used for tracking objects by calculating the similarity between exemplar image and search region. To evaluate our method, we test on a challenge VIVID dataset. The experiment shows that the proposed method has improvements in accuracy and speed in low spatial resolution UAV videos compared to existing methods.",autonomous vehicle
10.1109/ACCESS.2019.2950053,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Research on Recognition Method of Electrical Components Based on YOLO V3,https://ieeexplore.ieee.org/document/8886369/,"The reliability of electrical components affects the stable operation of the power system. Electrical components inspection has long been important issues in the intelligent power system. The main problems of traditional recognition methods of electrical components are low detection accuracy and poor real-time performance, which are challenging to extract necessary features from the inspection images. This paper proposes a way to detect the electrical components in the Unmanned Aerial Vehicle (UAV) inspection image based on You Only Look Once (YOLO) V3 algorithm. Due to some of the inspection images are not clear, which result in the reduction of the available dataset. On this basis, we adopt Super-Resolution Convolutional Neural Network (SRCNN) to realize super-resolution reconstruction on the blurred image, which achieves the expansion of the dataset. We compare the performance of the proposed method with other popular recognition methods. The results of experiment verify the effectiveness of the proposed method, and the technique reaches high recognition accuracy, good robustness, and strong real-time performance for UAV power inspection system.",autonomous vehicle
10.1109/ICoIAS.2018.8494201,to_check,2018 International Conference on Intelligent Autonomous Systems (ICoIAS),IEEE,2018-03-03 00:00:00,ieeexplore,UAVNet: An Efficient Obstacel Detection Model for UAV with Autonomous Flight,https://ieeexplore.ieee.org/document/8494201/,"Autonomous navigation for large Unmanned Aerial Vehicles(UAVs) is straight-forward to implement, just employ expensive and sophisticated sensors and monitoring devices. On the contrary, usual small quadrotor UAV still have the challenge on obstacle avoidance since this kind of UAV can only carry very light weight sensors such as cameras. Given the above reason, making autonomous navigation over obstacles on small UAV is much more challenging. In this paper, we focus on proposing a novel and memory efficient deep network architecture named UAVNet for small UAV to achieve obstacle detection in the urban environment. Compared with state-of-the-art DNN architecture, UAVNet has only 2.23M parameters(which is half compared with MobileNet) and 141 MFLOPs complexity. Though the parameters are fewer than usual, the accuracy is acceptable, about 80% validated on ImageNet-2102 dataset. To further justify the utility of UAVNet, we also implement the architecture on Nvidia TX2 in real environment using NCTU campus dataset. The experiment shows the proposed UAVNet can detect obstacles to 15 fps, which is a real-time application.",autonomous vehicle
10.1109/ACCESS.2020.2976686,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Large-Scale Synthetic Urban Dataset for Aerial Scene Understanding,https://ieeexplore.ieee.org/document/9015998/,"The geometric extraction and semantic understanding in bird's eye view plays an important role in cyber-physical-social systems (CPSS), because it can help human or intelligent agents (IAs) to perceive larger range of environment. Moreover, due to lack of comprehensive dataset from oblique perspective, fog-end deep learning algorithms for this purpose is still in blank. In this paper, we propose a novel method to generate synthetic large-scale dataset for geometric and semantic urban scene understanding from bird's eye view. There are two main steps involved, one is modeling and the other is rendering, which are processed by CityEngine and UnrealEngine4 respectively. In this way, synthetic aligned multi-model data are obtained efficiently, including spectral images, semantic labels, depth and normal maps. Specifically, terrain elevation, street graph, building style and trees distribution are all randomly generated according realistic situation, a few of handcrafted semantic labels annotated by colors spread throughout the scene, virtual cameras moved according to realistic trajectories of unmanned aerial vehicles (UAVs). For evaluation of practicability of our dataset, we manually labeled tens of aerial images downloaded from internet. And the experiment result show that, in both pure and combined mode, the dataset can improve the performance significantly.",autonomous vehicle
10.1016/j.neucom.2019.07.103,to_check,Neurocomputing,scopus,2020-05-21,sciencedirect,Visual Recognition of traffic police gestures with convolutional pose machine and handcrafted features,https://api.elsevier.com/content/abstract/scopus_id/85074513481,"Autonomous vehicles have become a hot spot of the automotive industry, many cities have claimed that autonomous vehicles should be capable of recognizing gestures used by traffic police. Traditional traffic police gesture recognition methods rely on depth-sensor or wearable-devices, which limits their availability in the domain of the intelligent vehicle. Vision-based methods have fewer requirements for distance, but the modeling process is challenging due to the complexity of the visual scenes. Inspired by the recent success in vision-based pose estimation networks such as Convolutional Pose Machine (CPM), in this paper, we propose a novel vision-based human-machine interface to recognize eight kinds of Chinese traffic police gestures and apply it in the real-time recognition tasks. This method integrates a modified CPM network and two kinds of handcrafted features: Relative Bone Length and Angle with Gravity as spatial domain features, and adopt a Long short-term memory (LSTM) network to extract temporal domain features. To train and validate our method, we create a gestures dataset with two hours of traffic police gesture videos, which has 3354 gesture instances. The experiment results show that the proposed method is capable of recognizing traffic police gestures, and is fast enough for online gesture prediction.",autonomous vehicle
10.1145/2808797.2809307,to_check,2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),IEEE,2015-08-28 00:00:00,ieeexplore,Social restricted Boltzmann Machine: Human behavior prediction in health social networks,https://ieeexplore.ieee.org/document/7403573/,"Modeling and predicting human behaviors, such as the activity level and intensity, is the key to prevent the cascades of obesity, and help spread wellness and healthy behavior in a social network. The user diversity, dynamic behaviors, and hidden social influences make the problem more challenging. In this work, we propose a deep learning model named Social Restricted Boltzmann Machine (SRBM) for human behavior modeling and prediction in health social networks. In the proposed SRBM model, we naturally incorporate self-motivation, implicit and explicit social influences, and environmental events together into three layers which are historical, visible, and hidden layers. The interactions among these behavior determinants are naturally simulated through parameters connecting these layers together. The contrastive divergence and back-propagation algorithms are employed for training the model. A comprehensive experiment on real and synthetic data has shown the great effectiveness of our deep learning model compared with conventional methods.",health
10.1109/iCMLDE.2018.00021,to_check,2018 International Conference on Machine Learning and Data Engineering (iCMLDE),IEEE,2018-12-07 00:00:00,ieeexplore,Using Electronic Health Records and Machine Learning to Make Medical-Related Predictions from Non-Medical Data,https://ieeexplore.ieee.org/document/8614004/,"Objectives: Administrative HIS (Hospital Information System) and EHR (Electronic Health Record) data are characterized by lower privacy sensitivity, thus easier portability and handling, as well as higher information quality. In this paper we test the hypothesis that the application of machine learning techniques on data of this nature can be used to address prediction/forecasting problems in the Health IT domain. The novelty of this approach consists in that medical data (test results, diagnoses, doctors' notes etc.) are not included in the predictors' dataset. Moreover, there is limited need for separation of patient cohorts based on specific health conditions. Methods: We experiment with the prediction of the probability of early readmission at the time of a patient's discharge. We extract real HIS data and perform data processing techniques. We then apply a series of machine learning algorithms (Logistic Regression, Support Vector Machine, Gaussian Naïve Bayes, K-Nearest Neighbors and Deep Multilayer Neural Network) and measure the performance of the emergent models. Results: All applied methods performed well above random guessing, even with minimal hyper-parameter tuning. Conclusions: Given that the experiments provide evidence in favor of the underlying hypothesis, future experimentation on more fine-tuned (thus more robust) models could result in applications suited for productive environments.",health
10.1109/HSI49210.2020.9142632,to_check,2020 13th International Conference on Human System Interaction (HSI),IEEE,2020-06-08 00:00:00,ieeexplore,Classification of Hazardous Chemicals with Raman Spectrum by Convolution Neural Network,https://ieeexplore.ieee.org/document/9142632/,"Dangerous chemicals have always been the hidden danger of social security, how to accurately identify chemicals is very important. In this experiment, the Raman scattering instrument will provide us with the Raman spectrum signal of about 190 chemical substances, each of which has its own characteristics. However, the traditional methods of identifying and classifying chemicals are not only inefficient, but also lack of security. This study proved the feasibility of using neural network to classify chemical substances. For one-dimensional signal, the experiment mainly uses the semi-supervised learning method to establish the 1D-DCNN model and simulate the real noise environment. One-dimensional signal is used as input and then the model is trained to get the model. The experimental results show that the accuracy of toxic and toxic, flammable, corrosive, environment hazard, health hazard, safe, expansive, harmful classification is 99% ± 1%. This shows that the 1D-DCNN model has strong anti-interference and robustness for signals in noise environments. This rapid classification method will provide reference value for the identification of chemical substances.",health
10.1109/ICDE.2017.221,to_check,2017 IEEE 33rd International Conference on Data Engineering (ICDE),IEEE,2017-04-22 00:00:00,ieeexplore,Enabling Real-Time Drug Abuse Detection in Tweets,https://ieeexplore.ieee.org/document/7930118/,"Prescription drug abuse is one of the fastest growing public health problems in the USA. To address this epidemic, a near real-time monitoring strategy, instead of one resorting to a retrospective health records, may improve detecting the prevalence and patterns of abuse of both illegal drugs and prescription medications. In this paper, our primary goals are to demonstrate the possibility of utilizing social media, e.g., Twitter, for automatic monitoring of illegal drug and prescription medication abuse. We use machine learning methods for an automatic classification that can identify tweets that are indicative of drug abuse. We collected tweets associated with well-known illegal and prescription drugs. We manually annotated 300 tweets that are likely to be related to drug abuse. Our experiment compares a set of classification algorithms, and a decision tree classifier J48, and the SVM outperform others for determining whether tweets contain signals of drug abuse. This automatic supervised classification study results illustrate the utility of Twitter in examining patterns of abuse, and show the feasibility of building the drug abuse detection system that can process large volume data from social media sources in a near real-time.",health
10.1109/ICMLA.2016.0023,to_check,2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),IEEE,2016-12-20 00:00:00,ieeexplore,Interaction Network Representations for Human Behavior Prediction,https://ieeexplore.ieee.org/document/7838127/,"Human behavior prediction is critical to studying how healthy behavior can spread through a social network. In this work we present a novel user representation based human behavior prediction model, the User Representation-based Socialized Gaussian Process model (UrSGP). First, we present the Deep Interaction Representation Learning (Deep Interaction) model for learning latent representations of interaction social networks in which each user is characterized by a set of attributes. In particular, we consider social interaction factors and user attribute factors to build a bimodal, fixed representation of each user in the network. Our model aims to capture the evolution of social interactions and user attributes and learn the hidden correlations between them. We then use our latent features for human behavior prediction via the UrSGP model. An empirical experiment conducted on a real health social network demonstrates that our model outperforms baseline approaches for human behavior prediction.",health
10.1109/SeGAH.2019.8882436,to_check,2019 IEEE 7th International Conference on Serious Games and Applications for Health (SeGAH),IEEE,2019-08-07 00:00:00,ieeexplore,Moving Beyond Branching: Evaluating Educational Impact of Procedurally-Generated Virtual Patients,https://ieeexplore.ieee.org/document/8882436/,"Electronic virtual patients (VPs) are interactive screen-based computer simulations of real-life clinical scenarios that are widely used for the purposes of health sciences education. VPs allow for safe and supportive experiences for health sciences students to practice problem solving and diagnostic skills without endangering real patients. Our previous work investigated the feasibility of procedurally generating virtual patient cases that leverage Bayesian network (BN) models learned from EMR data to present clinical scenarios and control outcomes of learners' decisions within the context of a presented VP. In this paper, we describe an experiment that compares a VP case based on a BN model to one created using a traditional narrative-branched VP system across multiple categories. Our results show that the narrative-branched VP case was rated significantly higher than its BN-based counterpart on reflecting the learning objectives, introducing/reinforcing clinical skills, attitudes, and behaviors relevant to sepsis treatment, providing formative feedback for choices/outcomes, and being more effective in teaching the subject matter to novice clinical practitioners. The BN-based VP case was rated significantly higher than the branched-narrative version on representing clinical variations associated with sepsis, on realism, and on learner engagement.",health
10.1109/ISITIA.2015.7219946,to_check,2015 International Seminar on Intelligent Technology and Its Applications (ISITIA),IEEE,2015-05-21 00:00:00,ieeexplore,Multi behavior NPC coordination using fuzzy coordinator and Gaussian distribution,https://ieeexplore.ieee.org/document/7219946/,"Nowadays, Artificial Intelligence (AI) techniques have an important role in modern computer games especially to make NPCs in games more human-like, believable and natural. In a real time warfare game, coordination of NPC troops can create a deeper sense of immersion. Anyhow, multi behavior NPC troops have intelligence for selecting their behavior itself which sometimes does not appropriate to accomplish a team objective. Therefore, in this research we propose to develop smart agent for team coordination to produce better team behavior. Fuzzy coordinator method is employed for team coordination based on smart agent as a leader in attacking scenario. The smart agent coordinates a team strategy with uncoordinated action from the NPC to produce coordinated action for each NPC. Gaussian distribution is used to make variation action of each NPC more natural and unpredictable. The experiment demonstrates the smart agent with fuzzy coordinator who becomes the leader can analyze attack time remaining, health of each NPC and enemy to decide which one has to offense or defense in battle by change decision of each NPC troop to select action that has contribution for team.",health
10.1109/ICET51757.2021.9451149,to_check,2021 IEEE 4th International Conference on Electronics Technology (ICET),IEEE,2021-05-10 00:00:00,ieeexplore,Non-contact Human Fatigue Assessment System Based on Millimeter Wave Radar,https://ieeexplore.ieee.org/document/9451149/,"In order to measure the heart rate and breath rate of human-body and evaluate the fatigue level in real-time, a noncontact human fatigue assessment system based on millimeter wave radar AWR1642 is proposed in this paper to assess the health condition of human conveniently. It has the characteristics of high classification accuracy and high accuracy. The function of system mainly includes getting accurate heart rate and breath rate by AWR1642, establishing data set, extracting features, annotating data and predicting fatigue level by Particle Swarm optimization Back Propagation (PSO-BP) neural network model. The experiment results show that, heart rate got by AWR1642 had an error less than 5% compared with heart rate got by standard medical oximeter. The system can be used for the parameters measurement of vital sign. On the other hand, the PSO-BP neural network model built with the variance of heart rate and breath rate is difficult to cause over fitting. The system has a good practicability can be used for the prediction of fatigue level and it's accuracy can reach 93.74%.",health
10.1109/BDACS53596.2021.00011,to_check,2021 International Conference on Big Data Analysis and Computer Science (BDACS),IEEE,2021-06-27 00:00:00,ieeexplore,Prevention and control of COVID-19 based on Spark and spatial big data,https://ieeexplore.ieee.org/document/9516492/,"The novel coronavirus pneumonia is a major public health emergency with fast transmission rate, wide infection range and great difficulty in prevention and control, which poses challenges to the urban governance system and governance capacity. At this moment, it is particularly significant to get the track of people’s movements. And at the same time, trajectory, as a typical spatio-temporal data, has been more and more used in subject researches such as road change detection, travel pattern exploration and urban hotspot analysis in recent years. In this paper, based on Spark and GeoSpark technology, real-time monitoring of the whereabouts of the community, schools and other personnel is carried out, in order to generate action tracks. At the same time, the deep learning algorithm is used to classify and warn the danger level of the trajectory of the people who are about to go in or go out of the residential district, schools, etc. It provides strong support for the public security, health and epidemic command and other government departments to achieve scientific prevention and control, intelligent prevention and control. The results show that spark can achieve high throughput and fault-tolerant real-time stream data processing. Geospark processes large-scale spatial data on the basis of spark, and can create point, line, surface and other spatial data structures based on longitude and latitude information. At the same time, the semi supervised learning model based on recurrent neural network is used to classify and early warn the danger level of personnel trajectories. The experiment randomly selected 2000 users from districts and schools in Chengdu, and divided the experimental data set into training set and verification set in the proportion of 8:2. The best performance of the trained model is 96.2%.",health
10.1109/ACCESS.2019.2925468,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,A Neural-Network-Based Method for RUL Prediction and SOH Monitoring of Lithium-Ion Battery,https://ieeexplore.ieee.org/document/8747502/,"The prognostic and health management (PHM) of lithium-ion batteries has received increasing attention in recent years. The remaining useful life (RUL) prediction and state of health (SOH) monitoring are two important parts in PHM of the lithium-ion battery. Nowadays, the development of signal processing technology and neural network technology introduces new data-driven methods to RUL prediction and SOH monitoring of the lithium-ion battery. This paper presents a neural-network-based method that combines long short-term memory (LSTM) network with particle swarm optimization and attention mechanism for RUL prediction and SOH monitoring of the lithium-ion battery. Before predicting RUL of the lithium-ion battery, the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) is utilized for the raw data denoising, which can improve the accuracy of prediction. A real-life cycle dataset of lithium-ion batteries from NASA is used to evaluate the proposed method, and the experiment results show that when compared with traditional methods, the proposed method has higher accuracy.",health
10.1109/ACCESS.2021.3116974,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,An Efficient Prediction Method for Coronary Heart Disease Risk Based on Two Deep Neural Networks Trained on Well-Ordered Training Datasets,https://ieeexplore.ieee.org/document/9555589/,"This study proposes an efficient prediction method for coronary heart disease risk based on two deep neural networks trained on well-ordered training datasets. Most real datasets include an irregular subset with higher variance than most data, and predictive models do not learn well from these datasets. While most existing prediction models learned from the whole or randomly sampled training datasets, our suggested method draws up training datasets by separating regular and highly biased subsets to build accurate prediction models. We use a two-step approach to prepare the training dataset: (1) divide the initial training dataset into two groups, commonly distributed and highly biased using Principal Component Analysis, (2) enrich the highly biased group by Variational Autoencoders. Then, two deep neural network classifiers learn from the isolated training groups separately. The well-organized training groups enable a chance to build more accurate prediction models. When predicting the risk of coronary heart disease from the given input, only one appropriate model is selected based on the reconstruction error on the Principal Component Analysis model. Dataset used in this study was collected from the Korean National Health and Nutritional Examination Survey. We have conducted two types of experiments on the dataset. The first one proved how Principal Component Analysis and Variational Autoencoder models of the proposed method improves the performance of a single deep neural network. The second experiment compared the proposed method with existing machine learning algorithms, including Naïve Bayes, Random Forest, K-Nearest Neighbor, Decision Tree, Support Vector Machine, and Adaptive Boosting. The experimental results show that the proposed method outperformed conventional machine learning algorithms by giving the accuracy of 0.892, specificity of 0.840, precision of 0.911, recall of 0.920, f-measure of 0.915, and AUC of 0.882.",health
10.1109/ACCESS.2020.3005981,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Anti-Motion Interference Wearable Device for Monitoring Blood Oxygen Saturation Based on Sliding Window Algorithm,https://ieeexplore.ieee.org/document/9129723/,"Wearable multi-physiological parameter monitoring technology can realize non-intrusive, non-invasive daily health monitoring of the human body. It has the characteristics of convenient operation, long-term continuous work, display of results, abnormal physiological condition alarm and wireless data transmission. First, an improved sliding window confidence propagation algorithm is proposed, which reduces the inefficient iterative process in the decoding process by correlation coefficients and reduces the running time of the algorithm by more than 1.5 times; designing a channel noise estimation method based on sliding window confidence propagation algorithm, and The source coding is similar. In order to estimate the local statistical parameters of the channel noise, the confidence propagation, the decoder uses the local partial probability of the adjacent accompanying child nodes to estimate the noise parameters of the channel noise. Secondly, the design and research of the blood oxygen saturation monitor, including filtering out high-frequency glitch noise, low-frequency baseline drift noise, and mixing and sudden motion interference noise, also achieved rapid and accurate extraction of pulse wave feature points, and calculated the blood oxygen value and pulse rate value. Compared with the traditional oximeter, the filtering algorithm and pulse wave feature point extraction algorithm on the oximeter software have less calculation amount and higher real-time performance. Finally, the relevant test experiment of the blood oxygen saturation monitor proves that the blood oximeter has good stability, accuracy and sensitivity. The practical use of anti-motion interference wearable devices not only shows that the wearable PPG sensor in this paper can stably obtain high-quality PPG signals, but also reflects its many applications in the field of real-time blood.",health
10.1109/ACCESS.2018.2858856,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,Remaining Useful Life Prediction for Lithium-Ion Battery: A Deep Learning Approach,https://ieeexplore.ieee.org/document/8418374/,"Accurate prediction of remaining useful life (RUL) of lithium-ion battery plays an increasingly crucial role in the intelligent battery health management systems. The advances in deep learning introduce new data-driven approaches to this problem. This paper proposes an integrated deep learning approach for RUL prediction of lithium-ion battery by integrating autoencoder with deep neural network (DNN). First, we present a multi-dimensional feature extraction method with autoencoder model to represent battery health degradation. Then, the RUL prediction model-based DNN is trained for multi-battery remaining cycle life estimation. The proposed approach is applied to the real data set of lithium-ion battery cycle life from NASA, and the experiment results show that the proposed approach can improve the accuracy of RUL prediction.",health
10.1109/JBHI.2013.2292576,to_check,IEEE Journal of Biomedical and Health Informatics,IEEE,2014-09-01 00:00:00,ieeexplore,System Light-Loading Technology for mHealth: Manifold-Learning-Based Medical Data Cleansing and Clinical Trials in WE-CARE Project,https://ieeexplore.ieee.org/document/6674050/,"Cardiovascular disease (CVD) is a major issue to public health. It contributes 41% to the Chinese death rate each year. This huge loss encouraged us to develop a Wearable Efficient teleCARdiology systEm (WE-CARE) for early warning and prevention of CVD risks in real time. WE-CARE is expected to work 24/7 online for mobile health (mHealth) applications. Unfortunately, this purpose is often disrupted in system experiments and clinical trials, even if related enabling technologies work properly. This phenomenon is rooted in the overload issue of complex Electrocardiogram (ECG) data in terms of system integration. In this study, our main objective is to get a system light-loading technology to enable mHealth with a benchmarked ECG anomaly recognition rate. To achieve this objective, we propose an approach to purify clinical features from ECG raw data based on manifold learning, called the Manifold-based ECG-feature Purification algorithm. Our clinical trials verify that our proposal can detect anomalies with a recognition rate of up to 94% which is highly valuable in daily public health-risk alert applications based on clinical criteria. Most importantly, the experiment results demonstrate that the WE-CARE system enabled by our proposal can enhance system reliability by at least two times and reduce false negative rates to 0.76%, and extend the battery life by 40.54%, in the system integration level.",health
10.1109/ACCESS.2017.2694446,to_check,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,Disease Prediction by Machine Learning Over Big Data From Healthcare Communities,https://ieeexplore.ieee.org/document/7912315/,"With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.",health
10.1109/ICCA.2007.4376646,to_check,2007 IEEE International Conference on Control and Automation,IEEE,2007-06-01 00:00:00,ieeexplore,Establishing Common Pattern of Traditional Chinese Medicine Fingerprint by SVM,https://ieeexplore.ieee.org/document/4376646/,"Establishing the common pattern of the traditional Chinese medicine fingerprint signal is an important problem for the evaluation in herbal medicine field. This study deals with the application of support vector machine regression combined with discrete wavelet transform in creating the common pattern of fingerprint signal. The original signal was first decomposed to different scales by discrete wavelet transform constructed by lifting scheme, and then the decomposed signal components were approximated by different kernel function of support vector machine regression. The real-life samples is used for the experiment to train and test the support vector machine regression in achieving the common pattern, the result shows that the approach proposed in this study is a useful method in establishing the common pattern of fingerprint signal.",health
10.1109/GCIS.2012.60,to_check,2012 Third Global Congress on Intelligent Systems,IEEE,2012-11-08 00:00:00,ieeexplore,A Natural Hand Gesture System for Intelligent Human-Computer Interaction and Medical Assistance,https://ieeexplore.ieee.org/document/6449559/,"This paper presents a novel hand gesture system for intelligent human-computer interaction (HCI) and its applications in medical assistance, e.g. intelligent wheelchair control. The hand gesture vocabulary in the system consists of five key hand postures and three compound states, and its design strategy covers the minimal hand motions, distraction detection and user-friendly design. The experiment results show that the designed lexicon is intuitive, ergonomic, and easy to be remembered and performed. The system is tested in both of the indoor and outdoor environments and shows the robustness to lighting change and users' errors. The proposed intelligent HCI system can run in real-time and offers a natural and efficient interface for people with disability in their limbs to communicate with robots.",health
10.1109/ICIECS.2009.5366912,to_check,2009 International Conference on Information Engineering and Computer Science,IEEE,2009-12-20 00:00:00,ieeexplore,Development of Electric Control High Power Medical-Use X-Ray Generator,https://ieeexplore.ieee.org/document/5366912/,A continuous operation with intermittent loading high voltage high frequency digital electric control generator is designed based on switching power supply technology and serial resonant inverter technology. Tube ARC suppressor is placed in order to limit arcing and a neural network PED control approach is established in order to improve the filament controller's properties. Tube stator current frequency spectrum is being used to estimate the X-ray tube motor's speed. Embedded real-time operating system-ARM7 is applied to the realization of the electric control generator. Simulation results testify that these methods are feasible and experiment results show that the X-ray power source satisfies the requirements of security inspection equipment. These methods have been widely used in the X-ray generator products.,health
10.1109/ACCESS.2020.2990167,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Medical Remote Monitoring of Multiple Physiological Parameters Based on Wireless Embedded Internet,https://ieeexplore.ieee.org/document/9078100/,"In view of the current situation, that physiological parameter monitoring systems can only achieve local monitoring, and the multi-physiological parameter monitors are large, expensive, and disadvantageous to remote monitoring. This paper combines embedded and mobile communication technologies to develop a new type of multi-physiological parameter medical monitoring system with remote data transmission function. First, through the analysis of embedded system principles, an embedded computer system based on ARM is designed. Secondly, the human-computer interaction interface, data acquisition, and analysis module are designed. Finally, by connecting to the Internet network to communicate with the medical center server, the remote transmission of local detection data and the issuance of alarm signals when dangerous situations occur are realized. The system can collect and display multiple physiological parameters such as heart rate, blood pressure, blood oxygen saturation, and body temperature in real time. The simulation experiment results show that the system's monitoring function and remote data transmission function meet the design requirements, can quickly and accurately find out-of-standard data, and perform remote alarm. The system is small, easy to expand, stable in data transmission, high in reliability, convenient for remote monitoring and data sharing, and is an ideal monitoring device for hospitals and community medical centers.",health
10.1109/BIBM49941.2020.9313262,to_check,2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),IEEE,2020-12-19 00:00:00,ieeexplore,"Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours",https://ieeexplore.ieee.org/document/9313262/,"One usage of medical ultrasound imaging is to visualize and characterize human tongue shape and motion during a real-time speech to study healthy or impaired speech production. Due to the low-contrast characteristic and noisy nature of ultrasound images, it might require expertise for non-expert users to recognize tongue gestures in applications such as visual training of a second language. Moreover, quantitative analysis of tongue motion needs the tongue dorsum contour to be extracted, tracked, and visualized. Manual tongue contour extraction is a cumbersome, subjective, and error-prone task. Furthermore, it is not a feasible solution for real-time applications. The growth of deep learning has been vigorously exploited in various computer vision tasks, including ultrasound tongue contour tracking. In the current methods, the process of tongue contour extraction comprises two steps of image segmentation and post-processing. This paper presents a new novel approach of automatic and real-time tongue contour tracking using deep neural networks. In the proposed method, instead of the two-step procedure, landmarks of the tongue surface are tracked. This novel idea enables researchers in this filed to benefits from available previously annotated databases to achieve high accuracy results. Our experiment disclosed the outstanding performances of the proposed technique in terms of generalization, performance, and accuracy.",health
10.1109/CBMS.2007.94,to_check,Twentieth IEEE International Symposium on Computer-Based Medical Systems (CBMS'07),IEEE,2007-06-22 00:00:00,ieeexplore,Rotation Forest and Random Oracles: Two Classifier Ensemble Methods,https://ieeexplore.ieee.org/document/4262617/,"Classification methods are widely used in computer-based medical systems. Often, the accuracy of a classifier can be improved using a classifier ensemble, the combination of several classifiers. Two classifiers ensembles and their results on several medical data sets will be presented: Rotation Forest (Rodriguez, Kuncheva and Alonso) and Random Oracles (Kuncheva and Rodriguez). Rotation Forest is a method for generating classifier ensembles based on feature extraction. To create the training data for a base classifier, the feature set is randomly split into K subsets (K is a parameter of the algorithm) and Principal Component Analysis (PCA) is applied to each subset. All principal components are retained in order to preserve the variability information in the data. Thus, K axis rotations take place to form the new features for a base classifier. The idea of the rotation approach is to encourage simultaneously individual accuracy and diversity within the ensemble. Diversity is promoted through the feature extraction for each base classifier. Decision trees were chosen here because they are sensitive to rotation of the feature axes, hence the name ""forest."" Accuracy is sought by keeping all principal components and also using the whole data set to train each base classifier. Comparisons with various standard ensemble methods (Bagging, AdaBoost, and Random Forest) will be reported. Diversity-error diagrams reveal that Rotation Forest ensembles construct individual classifiers which are more accurate than these in AdaBoost and Random Forest and more diverse than these in Bagging, sometimes more accurate as well. A random oracle classifier is a mini-ensemble formed by a pair of classifiers and a fixed, randomly created oracle that selects between them. The random oracle can be thought of as a random discriminant function which splits the data into two subsets with no regard of any class labels or cluster structure. Two random oracles has been considered: linear and spherical. A random oracle classifier can be used as the base classifier of any ensemble method. It is argued that this approach encourages extra diversity in the ensemble while allowing for high accuracy of the individual ensemble members. Experiments with several data sets from UCI and 11 ensemble models will be reported. Each ensemble model will be examined with and without the oracle. The results will show that all ensemble methods benefited from the new approach, most markedly so random subspace and bagging. A further experiment with seven real medical data sets will demonstrate the validity of these findings outside the UCI data collection. When using Naive Bayes Classifiers as base classifiers, the experiments show that ensembles based solely upon the spherical oracle (and no other ensemble heuristic) outrank Bagging, Wagging, Random Subspaces, AdaBoost.Ml, MultiBoost and Decorate. Moreover, all these ensemble methods are better with any of the two random oracles than their standard versions without the oracles.",health
10.1109/ACCESS.2020.2980025,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Automatic Segmentation of Multiple Structures in Knee Arthroscopy Using Deep Learning,https://ieeexplore.ieee.org/document/9032130/,"Minimally invasive surgery (MIS) is among the preferred procedures for treating a number of ailments as patients benefit from fast recovery and reduced blood loss. The trade-off is that surgeons lose direct visual contact with the surgical site and have limited intra-operative imaging techniques for real-time feedback. Computer vision methods as well as segmentation and tracking of the tissues and tools in the video frames, are increasingly being adopted to MIS to alleviate such limitations. So far, most of the advances in MIS have been focused on laparoscopic applications, with scarce literature on knee arthroscopy. Here for the first time, we propose a new method for the automatic segmentation of multiple tissue structures for knee arthroscopy. The training data of 3868 images were collected from 4 cadaver experiments, 5 knees, and manually contoured by two clinicians into four classes: Femur, Anterior Cruciate Ligament (ACL), Tibia, and Meniscus. Our approach adapts the U-net and the U-net++ architectures for this segmentation task. Using the cross-validation experiment, the mean Dice similarity coefficients for Femur, Tibia, ACL, and Meniscus are 0.78, 0.50, 0.41, 0.43 using the U-net and 0.79, 0.50, 0.51, 0.48 using the U-net++. While the reported segmentation method is of great applicability in terms of contextual awareness for the surgical team, it can also be used for medical robotic applications such as SLAM and depth mapping.",health
10.1109/JSEN.2021.3115105,to_check,IEEE Sensors Journal,IEEE,2001-11-01 20:21:00,ieeexplore,IMU-Based Estimation of Lower Limb Motion Trajectory With Graph Convolution Network,https://ieeexplore.ieee.org/document/9547298/,"In recent years, the motion capture system has received a lot of attention because of its wide application, such as movie animation, sport and medical applications. In the field of rehabilitation, motion capture systems are often used to collect the motion information of the patient when he/she is performing rehabilitation tasks. It can be used to quantify the patient’s rehabilitation effectiveness and provide the physician with more objective data as a reference. Using inertial sensors is one of motion capture methods. However, the major challenge to reconstruct accurate human posture with sensor measurements are signal noise, bias and inaccurate gyroscope estimation during long-term wearing. Based on the reason mentioned above, this paper proposes a graph convolutional network (GCN)-based deep learning architecture that uses effective information collected by inertial sensors to predict a sequence of position of each joint of a human’s lower limbs in the 3D space throughout the entire motion. Such motion prediction result based on the deep learning model is to reduce the joint’s tracking error relying only on direct calculations based on measurements of inertial sensors. In the final experiment, the lower limb motion trajectory during walking is used to verify that the method proposed in this work can actually outperform the traditional ones as just mentioned by achieving 2.9 mm drop in mean per joint position error (MPJPE) in 3.6M dataset, resulting in 12.2 mm in MPJPE, and mitigate the drift in real scenario. In our future work, we plan to verify the effectiveness of the proposed model and system for rehabilitation through clinical studies.",health
10.1109/ICCE-TW46550.2019.8991863,to_check,2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW),IEEE,2019-05-22 00:00:00,ieeexplore,A Smart Real-Time Monitoring System for Fault-Diagnosis of Ball-Bearing,https://ieeexplore.ieee.org/document/8991863/,"In this paper, a smart real-time monitoring system is developed, which attempts to produce key features for different kinds of sensing targets via applying chaos mapping strategy. Traditional way for machine learning is to extract and select those features from original signals, which requires domain knowledge related to the sensing targets, and many of statistics approaches are necessary for further selecting high-correlated features. In this paper, a smart machine with feature-production is developed, where those measured signals are mapped into chaotic domain. Through properly parameters adjusting and autonomous optimization of feature values, different and obvious key-features can be obtained for each of distinct states, which are clear as well as unique to their original signals properties. Finally, simple criteria can be further defined for states classifications. The classical ball-bearing system with four distinct states is illustrated for investigations. The experiment results reveal the proposed strategy is effective and feasible, where the high accuracy rate for states classifications can be achieved.",health
10.1109/SOSE.2018.00022,to_check,2018 IEEE Symposium on Service-Oriented System Engineering (SOSE),IEEE,2018-03-29 00:00:00,ieeexplore,An Ensemble Signature-Based Approach for Performance Diagnosis in Big Data Platform,https://ieeexplore.ieee.org/document/8359155/,"The big data platform always suffers from performance problems due to internal impairments (e.g. software bugs) and external impairments (e.g. resource hog). And the situation is exacerbated by the properties of velocity, variety and volume (3Vs) of big data. To recovery the system from performance anomaly, the first step is to find the root causes. In this paper, we propose a novel signature-based performance diagnosis approach to rapidly pinpoint the root causes of performance problems in big data platforms. The performance diagnosis is formalized as a pattern recognition problem. We leverage Maximum Information Criterion (MIC) to express the invariant relationships amongst the performance metrics in the normal state. Each performance problem occurred in the big data platform is signified by a unique binary vector named signature, which consists of a set of violations of MIC invariants. The signatures of multiple performance problems form a signature database. If the Key Performance Indicator (KPI) of the big data application exhibits model drift, our approach can identify the real culprits by retrieving the root causes which have similar signatures to the current performance problem. Moreover, considering the diversity of big data applications, we establish an ensemble approach to treat each application separately. The experiment evaluations in a controlled big data platform show that our approach can pinpoint the real culprits of performance problems in an average 84% precision and 87% recall when one fault occurs, which is better than several state-of-the-art approaches.",health
10.1109/WCICA.2010.5554755,to_check,2010 8th World Congress on Intelligent Control and Automation,IEEE,2010-07-09 00:00:00,ieeexplore,Information fusion diagnosis research based on fiber CAN communication in vehicle,https://ieeexplore.ieee.org/document/5554755/,"In order to transmit high-speed and large-capacity data with powerful ability of anti-electromagnetic interference in modern automobile, data communication and fault diagnosis platform based on fiber CAN communication is built up, from which every sensor in accessory nodes achieves at gathering fault signal and processing fault real time. Combine neural network with evidence theory organically, process the elementary diagnosis output of neural network as the base belief assignment value of evidence theory, and fuse with evidence theory to obtain the final diagnosis result. Fault diagnosis experiment on automatic transmission shows the proposed fusing diagnosis method can fully make use of redundancy and complement information of fault data, the veracity and reliability of fusing diagnosis result with neural network and evidence theory is much better than that of only neural network.",health
10.1109/CISE.2009.5363473,to_check,2009 International Conference on Computational Intelligence and Software Engineering,IEEE,2009-12-13 00:00:00,ieeexplore,Model Bootstrapping for Auto-Diagnosis of Enterprise Systems,https://ieeexplore.ieee.org/document/5363473/,"Models for fault diagnosis can help reduce the time taken to accurately identify faults, but the complexity of modern enterprise systems means that the process of manually model-building is itself very time-consuming. We study here the relevance of bootstrapping a diagnostic model that can then be manually refined and augmented by domain experts. We present an approach to model construction, developed by analyzing log traces from a real data center. We compare the automatically-bootstrapped model against a manually-constructed reference model for the same problem set in order to measure what amount of the model can be automatically built. An experiment with an Oracle enterprise system shows that approximately 15% of the model, diagnosing 30% of the related issues, can be automatically built.",health
10.1109/ICICIP.2016.7885912,to_check,2016 Seventh International Conference on Intelligent Control and Information Processing (ICICIP),IEEE,2016-12-04 00:00:00,ieeexplore,Support Vector Machine-recursive feature elimination for the diagnosis of Parkinson disease based on speech analysis,https://ieeexplore.ieee.org/document/7885912/,"Parkinson disease has become a serious problem in the old people. There is no precise method to diagnosis Parkinson disease now. Considering the significance and difficulty of recognizing the Parkinson disease, the measurement of samples' voices is regard as one of the best non-invasive ways to find the real patient. Support Vector Machine is one of the most effective tools to classify in machine learning, and it has been applied successfully in many areas. In this paper, we implement the SVM-recursive feature elimination which has not been used before for selecting the subset including the most important features for classification from the original features. We also implement SVM with PCA for selecting the principle components for diagnosis PD set with 22 features in order to compare. At last, we discuss the relationship between SVM-RFE and SVM with PCA specially in the experiment. The experiment illustrates that the SVM-RFE has the better performance than other methods in general.",health
10.1109/IJCNN.2000.861460,to_check,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,IEEE,2000-07-27 00:00:00,ieeexplore,Using neural networks for fault diagnosis,https://ieeexplore.ieee.org/document/861460/,"A universal fault instance model, which aims to solve problems existing in the present technology of fault diagnosis, such as the lack of universality, the difficulty in the use of real time systems and the dilemma of stability and plasticity, is proposed. An experiment demonstrates that the FANNC used can successfully settle the problems mentioned above by its effective incremental ability and processing new input patterns via one round learning.",health
10.1109/ACCESS.2020.3020906,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Fault Diagnosis Method Based on Improved Adaptive Filtering and Joint Distribution Adaptation,https://ieeexplore.ieee.org/document/9184065/,"In the real environment of industrial equipment, the vibration signals of essential components show deviations due to the fault and noise. Notably, the noise in the signal will interfere with the diagnosis process of the signal and reduce the accuracy of fault diagnosis. Based on the above problem, adaptive filtering (AF) is used as an excellent method to attenuate noise without specifying the noise type. However, how to define the most appropriate length and type of morphological filter element is the most inherent problem which needs to be solved first. This paper proposed a cooperative diagnosis method of rolling bearings vibration signal based on improved adaptive filtering and joint distribution adaptation (JDA). First, the kurtosis under different element types and lengths is calculated as an index. The structural element corresponding to maximum kurtosis is selected as the most suitable morphological filter element because the different morphological filter elements reflect the effect of feature extraction. Then, JDA aims to improve both the marginal distribution and the conditional distribution to solve the chaotic distribution of time-domain features under variable working conditions. Finally, the improved least squares support vector machine (LSSVM) verified the effectiveness and improvement of the proposed method under bearing acceleration signal. At the same time, the comparative experiment proved that the proposed method not only directly corrects the most appropriate elements greatly optimizes the feature structure, but also enhances the accuracy of fault diagnosis.",health
10.1109/ACCESS.2020.2986356,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Data Fusion Generative Adversarial Network for Multi-Class Imbalanced Fault Diagnosis of Rotating Machinery,https://ieeexplore.ieee.org/document/9058665/,"For the fault diagnosis problems of rotating machinery in the real industrial practice, measurement data with imbalanced class distributions negatively affect the diagnostic performance of most conventional machine learning classification algorithms since equal cost weights are assigned to different fault classes. Meanwhile, the widely used traditional data generation methods for the imbalanced data problem are limited by data dependencies over time continuity. To fill this research gap, this paper develops a new diagnostic framework based on the adversarial neural networks (GAN) and multi-sensor data fusion technique to generate new synthetic data for data compensation purpose. Two different practice modes are designed based on this framework according to the position logic of the data fusion, namely a Pre-fusion GAN mode and a Post-fusion GAN mode. More concretely, without data pre-processing, the designed generator generates synthetic data to puzzle the discriminator and the synthetic data that out-trick the discriminator can be used to compensate the minor class. To avoid data dependency and to ensure the generality of the proposed framework, the network modelling are trained with a more practical approach where the training and test data are obtained under different rotating speeds. Two imbalanced data sets on the rotating machinery, one benchmark public rolling bearing data set and another gear box data set acquired in our lab, are used to validate the proposed method. The performance is examined through a wide range of data imbalanced ratios (as high as 30:1), and compared with other state-of-the-art methods. The experiment results conclude that the proposed Pre-fusion GAN and Post-fusion GAN frameworks both have good performance on the imbalanced fault diagnosis of rotating machinery.",health
10.1109/ICAICTA.2014.7005957,to_check,"2014 International Conference of Advanced Informatics: Concept, Theory and Application (ICAICTA)",IEEE,2014-08-21 00:00:00,ieeexplore,A comparison for handling imbalanced datasets,https://ieeexplore.ieee.org/document/7005957/,"In various real case, imbalanced datasets problems are inevitable, such as in metal detecting security or diagnosis of disease. With the limitations of existing learning algorithms when faced with imbalanced datasets, the prediction error is caused by the dominance of the majority against the minority class. Various techniques have been made to address the above circumstances. This paper compares those techniques of handling imbalanced datasets with resample and ensembles. From a different standpoint, this paper examines how much influence the number of instances, number of attributes, the attributes data types, the number of the target class, and missing attribute values affect the classification results with performance analysis using f-measure. An experiment has resulted that the criteria regarding the number of attributes, attribute data types, and the number of the target class do not affect the classification results. While the missing attribute with values have an affect classification result. For better high F-measure, the experiment shows that the best performer is combination of SMOTE 5000/0 and AdaBoostMl.",health
10.1109/ICECE51571.2020.9393134,to_check,2020 11th International Conference on Electrical and Computer Engineering (ICECE),IEEE,2020-12-19 00:00:00,ieeexplore,Automated Recognition of Rice Grain Diseases Using Deep Learning,https://ieeexplore.ieee.org/document/9393134/,"To detect and classify various diseases of plants, the images of leaves are the main wellspring of information. Misidentifying various diseases in agricultural crops can lead to significant economic loss and environmental impacts. Rice is the most predominant food crop in Bangladesh. It possesses around 75% of the grossed harvest area of the country. Early diagnosis of rice grain diseases can save tonnes of agricultural products every year. The traditional manual observation of the crop disease is time-consuming and less accurate. For addressing this issue, researchers have utilized different high-performing Convolutional Neural Networks (CNNs) that perform well in the disease classification tasks. But these networks have very complex architecture and not very much suitable for real-life applications. Moreover, there is a limitation of devices and stable connectivity in agricultural areas. So there requires a lightweight efficiency for applying the automated disease detection system. In this paper, we present a custom memory-efficient Convolutional Neural Network (CNN) namely, RiceNet to automatically detect rice grain diseases. An extensive experiment showed the effectiveness of the proposed custom network (RiceNet) which provides a classification accuracy of 93.75%. We also apply the transfer learning approach through various pre-trained lightweight structures and achieved 97.94% accuracy through EfficientNetB0.",health
10.1109/ICMLC.2009.5212719,to_check,2009 International Conference on Machine Learning and Cybernetics,IEEE,2009-07-15 00:00:00,ieeexplore,The design of a distributed virtual laboratory,https://ieeexplore.ieee.org/document/5212719/,"Being a key aspect in teaching and learning activities, experiment teaching plays an important role to improve students' capability of analyzing and solving problems. In the current network education, due to the lack of effective environment to carry out experiment, it is difficult to build and develop students' capability all round. Our paper presents a design schema of constructing the distributed virtual experiment environment for various subjects. This distributed virtual experiment environment not only can support students to carry out virtual experiments in both independent and collaborative way under network environment, but also make real-time diagnosis and guidance of the experimental processes and results.",health
10.1109/INOCON50539.2020.9298269,to_check,2020 IEEE International Conference for Innovation in Technology (INOCON),IEEE,2020-11-08 00:00:00,ieeexplore,Transfer Learning based Convolutional Neural Network Model for Classification of Mango Leaves Infected by Anthracnose,https://ieeexplore.ieee.org/document/9298269/,"Anthracnose is a plant disease caused by fungi that develop saucer shaped acervulus spore on leaves sunken, it lesions leaves and sooty mold mango fruit leaves. The Anthracnose disease affect the quality of mango fruit and productivity. Hence, it is essential to develop espousing model to detect lesion areas on Mango leaves, identify level of infection, diagnosis of the Anthracnose disease. The deep learning techniques are known for their performance and Convolutional Neural Networks(CNNs) are widely accepted model for pattern identification and image categorization. Therefore, this paper propose a modified version of VGGNet model called V2IncepNet that integrate best feature of VGGNet and Inception module. The VGGNet module extract basic feature while inception module perform extraction of high-dimensional features and classification of images. This paper use various features such as leaf color, venation, petiole condition, tip shape, tip conditions, leaf shape, leaf margin, dark spots on leaf blade and on midrib, margin of burns on leaf, leaf blade, midrib, and petiole. In our data set there are 2268 color images of Mango leaves, which include 1198 on-field self-captured real-time color images and 1070 color Mango leaves images downloaded from Plantvillage. The experiment result envisage that the proposed model can classify Anthracnose disease infection level on Mango leaves with accuracy not less than 92%. The proposed model is simple yet efficient.",health
10.1109/ACCESS.2020.3023970,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Deep Residual Network for Identifying Bearing Fault Location and Fault Severity Concurrently,https://ieeexplore.ieee.org/document/9195861/,"Fault diagnosis is composed of two tasks, i.e., fault location detection and fault severity identification, which are both significant to equipment maintenance. The former can indicate where the defective component lies in, and the latter provides evidence on the residual life of the component. However, traditional fault diagnosis methods, like the time-based methods, frequency-based methods and time-frequency-based methods, can only achieve one goal every time. They are not able to produce highly representative features for dealing with above-mentioned two tasks simultaneously. In addition, there is a huge increase in the amount of monitoring data of equipment. There is urgent need for handling this massive data, obtaining highly discriminative features, and further producing accurate diagnosis results in the field of fault diagnosis. Aimed at these problems, a deep residual network based on multi-task learning is proposed, taking detection of fault location and judgment of fault severity into account simultaneously. This network is fed with two kinds of diagnostic information, which is helpful to mine the potential links between two tasks of fault diagnosis and generate very representative features. Moreover, based on maximizing activation value, a visualization method of role of deep neural network is proposed. It can break in the traditional way of using deep neural network as black box. A real bearing experiment validates that the proposed method is reliable and effective in bearing fault diagnosis.",health
10.1109/ACCESS.2021.3067360,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Muscle Activation Visualization System Using Adaptive Assessment and Forces-EMG Mapping,https://ieeexplore.ieee.org/document/9381904/,"VR-based serious games are obtained without details about real-time guide and feedback in the rehabilitation after stroke, leading to undesirable recovery outcomes. This study investigated the feasibility of real-time visualization in muscle state feedback by sEMG. Then we explored the application in movement guide and diagnosis. We provided a force-sEMG mapping approach based on body weight to visualize the implicit bioinformation. And 10 healthy subjects participated in an experiment that the K-means cluster algorithm and the support vector regression are employed to filter the unexpected data and adjust the visualization parameter for each subject dynamically. The verification experiment demonstrates that force and sEMG can be mapped as a data pair by support vector regression and normalized by the low-cost calibration in 6-8 times reparative actions. We define the predictive accuracy as the ratio of the predicted to the practical tasks. And mean absolute value is the most suitable index to compete for most data scales which can provide a predictive accuracy of 89.84% ± 5.28% in biceps and 84.26% ± 6.44% in triceps. We present the motivation improvement for patients and supervision in online and offline application scenarios for therapists. This system can be applied in precision and telemedicine to improve the efficacy of rehabilitation by objective data and intuitive expression through visualization.",health
10.1007/s00521-021-06070-y,to_check,Neural Computing and Applications,Springer,2021-08-21 00:00:00,springer,Application of CT coronary flow reserve fraction based on deep learning in coronary artery diagnosis of coronary heart disease complicated with diabetes mellitus,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06070-y,"Coronary heart disease is a heart disease caused by coronary atherosclerosis, which seriously endangers human life and health. More and more studies have shown that diabetes is one of the main pathogenic factors of coronary heart disease and has an important relationship with coronary heart disease. At present, the mainstream treatment of coronary heart disease complicated with diabetes mellitus is the use of coronary angiography, which is the gold standard of treatment at present. However, it still has a certain risk, and most of the postoperative complications, the improvement method is to use FFR evaluation standard. At present, there are few special researches on this aspect. Therefore, this paper proposes the application research of FFR based on deep learning in the diagnosis of coronary heart disease complicated with diabetes mellitus. Through the core theoretical research on coronary heart disease complicated with diabetes mellitus and FFR, this paper analyzes that the existing coronary angiography is still in the development stage, which has a positive effect on the treatment of coronary heart disease and diabetes mellitus. On this basis, combined with FFR can play a better therapeutic effect. The second part is the establishment method of the related comparative experiment. This experiment adopts real coronary heart disease complicated with diabetes cases, through the way of random grouping, each group of 41 people, one group was FFR_CT group, the other was FFR_QCA group, and in order to ensure the experimental effect, a unified evaluation index is established. In the third part, the comparative analysis of the experimental methods of angina pectoris was carried out. Through the analysis of experimental data, it is shown that the safety, postoperative complications control and comprehensive treatment effect of this method are significantly improved compared with the traditional methods.",health
10.1007/s00521-021-05897-9,to_check,Neural Computing and Applications,Springer,2021-03-20 00:00:00,springer,Traditional Chinese medicine entity relation extraction based on CNN with segment attention,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-05897-9,"Extracting medical entity relations from Traditional Chinese Medicine (TCM) related article is crucial to connect domain knowledge between TCM with modern medicine. Herb accounts for the majority of Traditional Chinese Medicine, so our work mainly focuses on herb. The problem would be effectively solved by extracting herb-related entity relations from PubMed literature. In order to realize the entity relation mining, we propose a novel deep-learning model with improved layers without manual feature engineering. We design a new segment attention mechanism based on Convolutional Neural Network, which enables extracting local semantic features through word embedding. Then we classify the relations by connecting different embedding features. We first test this method on the Chemical-Induced Disease task and the experiment show better result comparing to other state-of-the-art deep learning methods. Further, we apply this method to a herbal-related data set (Herbal-Disease and Herbal Chemistry, HD-HC) constructed from PubMed to explore entity relation classification. The experiment shows superior results than other baseline methods.",health
10.1007/s42452-021-04427-5,to_check,SN Applied Sciences,Nature,2021-03-09 00:00:00,springer,Recurrent neural networks with long term temporal dependencies in machine tool wear diagnosis and prognosis,https://www.nature.com/articles/s42452-021-04427-5,"Data-driven approaches for machine tool wear diagnosis and prognosis are gaining attention in the past few years. The goal of our study is to advance the adaptability, flexibility, prediction performance, and prediction horizon for online monitoring and prediction. This paper proposes the use of a recent deep learning method, based on Gated Recurrent Neural Network architecture, including Long Short Term Memory (LSTM), which try to captures long-term dependencies than regular Recurrent Neural Network method for modeling sequential data, and also the mechanism to realize the online diagnosis and prognosis and remaining useful life (RUL) prediction with indirect measurement collected during the manufacturing process. Existing models are usually tool-specific and can hardly be generalized to other scenarios such as for different tools or operating environments. Different from current methods, the proposed model requires no prior knowledge about the system and thus can be generalized to different scenarios and machine tools. With inherent memory units, the proposed model can also capture long-term dependencies while learning from sequential data such as those collected by condition monitoring sensors, which means it can be accommodated to machine tools with varying life and increase the prediction performance. To prove the validity of the proposed approach, we conducted multiple experiments on a milling machine cutting tool and applied the model for online diagnosis and RUL prediction. Without loss of generality, we incorporate a system transition function and system observation function into the neural net and trained it with signal data from a minimally intrusive vibration sensor. The experiment results showed that our LSTM-based model achieved the best overall accuracy among other methods, with a minimal Mean Square Error (MSE) for tool wear prediction and RUL prediction respectively.",health
10.1007/s11227-020-03152-x,to_check,The Journal of Supercomputing,Springer,2020-11-01 00:00:00,springer,Two-stage classification of tuberculosis culture diagnosis using convolutional neural network with transfer learning,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-020-03152-x,"Tuberculosis (TB) has been one of top 10 leading causes of death. A computer-aided diagnosis system to accelerate TB diagnosis is crucial. In this paper, we apply convolutional neural network and deep learning to classify the images of TB culture test—the gold standard of TB diagnostic test. Since the dataset is small and imbalanced, a transfer learning approach is applied. Moreover, as the recall of non-negative class is an important metric for this application, we propose a two-stage classification method to boost the results. The experiment results on a real dataset of TB culture test (1727 samples with 16,503 images from Tao-Yuan General Hospital, Taiwan) show that the proposed method can achieve 99% precision and 98% recall on the non-negative class.",health
10.1007/s12065-020-00359-y,to_check,Evolutionary Intelligence,Springer,2020-02-15 00:00:00,springer,Neural network with adaptive evolutionary learning and cascaded support vector machine for fault localization and diagnosis in power distribution system,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12065-020-00359-y,"Fault diagnosis and classification in electric power system is necessary to maintain a protected operation of power system. The classification of this signal is complex due to the large dataset, computational complexity and limited real time performance. This paper focuses on the detection and classification of electric power transmission using neural network with adaptive evolutionary learning and cascade support vector machine (SVM) with wavelet descriptors of the signal to overcome such limitations. Initially the wavelet decomposed fault signals are extracted from the simulated signals. The received signal consists of normal signals and fault signals such as transient, sag and swells signals respectively. The wavelet descriptors of different datasets are applied to the cascade SVM for better classification. This real experiment of this paper shows that this cascade SVM provides good generalization and much fast speed compared with traditional SVMs.",health
10.1016/j.talanta.2021.123030,to_check,Talanta,scopus,2022-02-01,sciencedirect,A turn-on NIR fluorescence sensor for gossypol based on Yb-based metal-organic framework,https://api.elsevier.com/content/abstract/scopus_id/85118877273,"The development of analytical method for selective and sensitive detection of gossypol (Gsp), an extraction from the cotton plants, is important but still challenging in food safety and medical field. Herein, we reported a turn-on near infrared (NIR) fluorescence detection strategy for Gsp based on a metal-organic framework (MOF), QBA-Yb, which was prepared from 4,4’-(quinolone-5, 8-diyl) benzoate with Yb(NO3)3·5H2O by solvothermal synthesis. The Gsp acted as another “antenna” to sensitize the luminescence of Yb3+, leading to the turn-on NIR emission upon 467 nm excitation. As Gsp concentration increased, the NIR emission at 973 nm enhanced gradually, thus enabling highly sensitive Gsp detection in a turn-on way. The experiment and theoretical calculation results revealed the presence of strong hydrogen bonds between Gsp molecules and the MOF skeleton. The developed QBA-Yb probe showed excellent characteristics for detection of Gsp molecules, accompanied by wide linear range (5–160 μg/mL), low detection limit (0.65 μg/mL) and short response time (within 10 min). We have further demonstrated that the QBA-Yb probe was successfully applied for the determination of Gsp in real samples of cottonseeds.",health
10.1016/j.cmpb.2021.106379,to_check,Computer Methods and Programs in Biomedicine,scopus,2021-10-01,sciencedirect,MFB-LANN: A lightweight and updatable myocardial infarction diagnosis system based on convolutional neural networks and active learning,https://api.elsevier.com/content/abstract/scopus_id/85114670974,"Background and objectives: 12 leads electrocardiogram (ECG) are widely used to diagnose myocardial infarction (MI). Generally, the symptoms of MI can be reflected by waveforms in the heartbeat, and the contribution of different ECG leads to different types of MI is different. Therefore, it is significant to use the heartbeat waveform features and the lead relationship features for multi-category MI diagnosis. Moreover, the challenge of individual differences and lightweight algorithms also need to be further resolved and explored in the ECG automatic diagnosis system.
                  
                     Methods: This paper presents a lightweight MI diagnosis system named multi-feature-branch lead attention neural network (MFB-LANN) via 12 leads ECG signals. It is designed based on the characteristics of the ECG lead. Specifically, 12 independent feature branches correspond to different leads, and each branch contains different convolutional layers to extract features in the heartbeat, then a novel attention module is developed named lead attention mechanism (LAM) to assign different weights to each feature branch. Finally all the weighted feature branches are fused for classification. Furthermore, to overcome individual differences, patient-specific scheme and active learning (AL) are used to train and update the model iteratively.
                  
                     Results: Experimental results based on Physikalisch-Technische Bundesanstalt (PTB) database shows that the MFB-LANN achieved satisfactory results with accuracy of 99.63% based on 5-fold cross validation under the intra-patient scheme. The patient-specific experiment yielded an average accuracy of 96.99% compared to the state-of-the-art. By contrast, the model achieved acceptable results on the hybrid database (PTB and PTB-XL), especially achieving 94.19% accuracy after the update. Moreover, the system can complete the update process and real-time diagnosis on the ARM Cortex-A72 platform.
                  
                     Conclusions: Experiments show that the proposed method for MI diagnosis has more obvious advantages compared to other recent methods, and it has great potential to be applied to the mobile medical field.",health
10.1016/j.media.2021.102167,to_check,Medical Image Analysis,scopus,2021-10-01,sciencedirect,Computer-aided diagnosis tool for cervical cancer screening with weakly supervised localization and detection of abnormalities using adaptable and explainable classifier,https://api.elsevier.com/content/abstract/scopus_id/85111506720,"While pap test is the most common diagnosis methods for cervical cancer, their results are highly dependent on the ability of the cytotechnicians to detect abnormal cells on the smears using brightfield microscopy. In this paper, we propose an explainable region classifier in whole slide images that could be used by cyto-pathologists to handle efficiently these big images (100,000x100,000 pixels). We create a dataset that simulates pap smears regions and uses a loss, we call classification under regression constraint, to train an efficient region classifier (about 66.8% accuracy on severity classification, 95.2% accuracy on normal/abnormal classification and 0.870 KAPPA score). We explain how we benefit from this loss to obtain a model focused on sensitivity and, then, we show that it can be used to perform weakly supervised localization (accuracy of 80.4%) of the cell that is mostly responsible for the malignancy of regions of whole slide images. We extend our method to perform a more general detection of abnormal cells (66.1% accuracy) and ensure that at least one abnormal cell will be detected if malignancy is present. Finally, we experiment our solution on a small real clinical slide dataset, highlighting the relevance of our proposed solution, adapting it to be as easily integrated in a pathology laboratory workflow as possible, and extending it to make a slide-level prediction.",health
10.1016/j.cmpb.2021.106281,to_check,Computer Methods and Programs in Biomedicine,scopus,2021-09-01,sciencedirect,xECGNet: Fine-tuning attention map within convolutional neural network to improve detection and explainability of concurrent cardiac arrhythmias,https://api.elsevier.com/content/abstract/scopus_id/85111343607,"Background and objectiveDetecting abnormal patterns within an electrocardiogram (ECG) is crucial for diagnosing cardiovascular diseases. We start from two unresolved problems in applying deep-learning-based ECG classification models to clinical practice: first, although multiple cardiac arrhythmia (CA) types may co-occur in real life, the majority of previous detection methods have focused on one-to-one relationships between ECG and CA type, and second, it has been difficult to explain how neural-network-based CA classifiers make decisions. We hypothesize that fine-tuning attention maps with regard to all possible combinations of ground-truth (GT) labels will improve both the detection and interpretability of co-occurring CAs.
                  
                     Methods To test our hypothesis, we propose an end-to-end convolutional neural network (CNN), xECGNet, that fine-tunes the attention map to resemble the averaged response maps of GT labels. Fine-tuning is achieved by adding to the objective function a regularization loss between the attention map and the reference (averaged) map. Performance is assessed by F1 score and subset accuracy.
                  
                     Results The main experiment demonstrates that fine-tuning alone significantly improves a model’s multilabel subset accuracy from 75.8% to 84.5% when compared with the baseline model. Also, xECGNet shows the highest F1 score of 0.812 and yields a more explainable map that encompasses multiple CA types, when compared to other baseline methods.
                  
                     Conclusions xECGNet has implications in that it tackles the two obstacles for the clinical application of CNN-based CA detection models with a simple solution of adding one additional term to the objective function.",health
10.1016/j.micpro.2020.103737,to_check,Microprocessors and Microsystems,scopus,2021-03-01,sciencedirect,HPAC-sbox- a novel implementation of predictive learning classifier and adaptive chaotic s-box for counterfeiting sidechannel attacks in an IOT networks,https://api.elsevier.com/content/abstract/scopus_id/85099437035,"Today, embedded systems are augmented with the Internet of things and more with the artificial intelligence to make world even connected with aliens. With an IoT networks are getting its insight since it deals with large number of data information, security has considered to be more important and needs to be a diagnosis for every minute. To enhance the security in the network, a mathematically secure algorithms were formulated and runs on the cryptographic embedded chips to counterfeit the risks which are caused by the different attacks such as side channel attacks (SCA) on the networks. Even though many cryptographic encryption algorithms such as AES, DES, RC4 algorithms were gaining its importance, fixed encryption keys, non-intelligent detection of attacks, cognitive countermeasures are some of the real-time challenges in an existing system of encryption. Following the limitations of existing systems, this research article focuses on design of new AES with HPAC-SBOX (Hybrid Prediction and Adaptive Chaos) which integrates powerful predictive learning algorithms and adaptive chaotic logistic S-Box. The following contributions of this research articles are: a) Preparation of Data Sets from the Power consumption traces captured from Multi Core Embedded boards while running the Advanced Encryption Systems(AES) on it b) Implementation of High Speed and High Accurate Prediction learning machines for the prediction of side-channel attacks c) Design of Adaptive Chaotic S-Box using 3-Dlogistic Hyperbolic maps for attacked bits. To evaluate the proposed architecture, experimentation in carried out in an IoT networks and various performance parameters were calculated and analyzed. The results show that the proposed architecture outperforms the other existing algorithms in terms of prediction and performance.",health
10.1016/j.matpr.2020.11.261,to_check,Materials Today: Proceedings,scopus,2021-01-01,sciencedirect,Analysis and control of the motor vibration using arduino and machine learning model,https://api.elsevier.com/content/abstract/scopus_id/85107352700,"The motor vibrations provide information for diagnosing and predicting errors through signal processing. Motor vibrations provide information for diagnosing and predicting errors through data signal. In this article, we introduce an IoT-based detecting scheme for recording the vibration of an induction motor. A three different sensor such as vibration, Micro-Electro-Mechanical Systems (MEMS) and temperature sensor is used to gather the data from the motor vibration. We conducted an experiment by analysing the motor vibration by using Arduino, when the motor vibration is going to abnormal at that time Arduino send the signal to relay by cutting the supply to the motor. However, the motor running on proper condition with proper temperature, the controller is continuously monitoring and forward the data to storage system. The results can presentation on the mobile phone. The experiments were performed in the steady-state condition and the measured vibration signals were analysed using the Discrete Fourier Transform (DFT). Depend on the outcomes of the proposed model, which greatly monitor and early diagnosis the motor vibration, and performance was analysed by using the Machine Learning model of Decision Tree (DT). The controller can forward the vibration data the cloud with a maximum delay of about 0.9 sec. the stored data is retrieved by using the train the DT model to analysis the performance classification accuracy. This article introduces a new method for implementing real-time vibration measurement and analysis tools based on MATLAB.",health
10.1016/j.bspc.2020.102178,to_check,Biomedical Signal Processing and Control,scopus,2021-01-01,sciencedirect,Towards effective classification of brain hemorrhagic and ischemic stroke using CNN,https://api.elsevier.com/content/abstract/scopus_id/85089894411,"Brain stroke is one of the most leading causes of worldwide death and requires proper medical treatment. Therefore, in this paper, our aim is to classify brain computed tomography (CT) scan images into hemorrhagic stroke, ischemic stroke and normal. Our newly proposed convolutional neural network (CNN) model utilizes image fusion and CNN approaches. Initially, some preprocessing operations have been employed by using multi-focus image fusion in order to improve the quality of CT images. Further, preprocessed images are fed into the newly proposed 13 layers CNN architecture for stroke classification. The robustness of our CNN method has been checked by conducting two experiments on two different datasets. In the first experiment, CT image dataset is partitioned into 20% testing and 80% training sets, while in the second experiment, 10 fold cross-validation of the image dataset has been performed. The classification accuracy obtained by our method on dataset 1 in the first experiment is 98.33% and in the second experiment, it is 98.77%, while in dataset 2 accuracy obtained in experiment 1 and 2 is 92.22% and 93.33% respectively. All the experiments have been conducted on the real CT image dataset which we have been collected from Himalayan Institute of Medical Sciences (HIMS), Dehradun, India. The results obtained by the proposed method have also been compared with AlexNet and ResNet50 where results show improvement over these CNN architectures.",health
10.1016/S2215-0366(20)30285-6,to_check,The Lancet Psychiatry,scopus,2020-11-01,sciencedirect,Brain-based mediation of non-conscious reduction of phobic avoidance in young women during functional MRI: a randomised controlled experiment,https://api.elsevier.com/content/abstract/scopus_id/85092479184,"Background
                  Exposure therapy is the treatment of choice for anxiety disorders but requires people to confront feared situations and can be distressing. We tested the hypothesis that exposure without conscious awareness would reduce fear in participants with specific phobia by harnessing the neural circuitry supporting the automatic extinction of fear.
               
                  Methods
                  In this single-centre, randomised controlled experiment, we recruited women aged 18–29 years from an ethnically diverse, community-based population in northeastern USA, between Sept 1, 2013, and Aug 1, 2016. Eligible participants classified as having phobia met the DSM-5 criteria for specific phobia but not for any other disorder, had scores in the top 10% of respondents to the Fear of Spiders Questionnaire, and exhibited impairing avoidance of a live tarantula. Eligible controls met no criteria for any disorder, were in the bottom 30% of questionnaire respondents, and displayed no avoidance of the tarantula. The randomisation schedule was generated with the open source Research Randomizer Tool. A research assistant randomly assigned participants to the active intervention of very brief exposure (VBE)—the repeated presentation of masked phobic stimuli (ie, spiders)—or the control intervention which used masked flowers (VBF). VBE and VBF were given code numbers to prevent staff from knowing which intervention they were administering. During a 10 min functional MRI (fMRI) task, each participant was exposed to 16 blocks of ten masked target stimuli (spiders or flowers), alternating with 16 blocks of ten masked neutral stimuli. A few minutes after fMRI, participants with spider phobia approached the tarantula again so we could measure changes in phobic behaviour. The primary outcome was real-time changes in brain activity measured by fMRI. All analyses were done by intention to treat.
               
                  Results
                  We recruited 82 women, of whom 42 had spider phobia and 40 were controls. VBE generated stronger neural activity in participants with spider phobia than in controls, particularly in regions supporting emotion, emotion regulation, and attention systems, such as the inferior frontal cortex (Cohen's d 0·95, 95% CI 0·93–0·98, Bayesian posterior probability 99·5%) and the caudate nucleus (1·16, 1·14–1·18, 100·0%). In participants with phobia, VBE also generated stronger activity in these regions than did VBF (eg, dorsal anterior cingulate cortex Cohen's d 0·80, 95% CI 0·78–0·80, Bayesian posterior probability 98·5%; caudate nucleus 1·0, 0·98–1·02, 99·5%). VBE reduced avoidance of the live tarantula in participants with phobia. Regions supporting fear extinction (including ventral medial prefrontal cortex) and emotional salience processing mediated this effect. No adverse events occurred.
               
                  Interpretation
                  VBE reduced fear non-consciously in participants with spider phobia by recruiting brain regions supporting automatic fear extinction, emotion regulation, and top-down attentional processing. Future studies should explore the use of VBE in other fear-based disorders.
               
                  Funding
                  National Institutes of Mental Health and Brain & Behavior Research Foundation.",health
10.1016/j.heliyon.2020.e05243,to_check,Heliyon,scopus,2020-10-01,sciencedirect,"Fast, easy, cheap, robust and safe method of analysis of Sudan dyes in chilli pepper powder",https://api.elsevier.com/content/abstract/scopus_id/85092504210,"Illicit use of Sudan dyes, a group of harmful and carcinogenic azo dyes, in the food industry has taken a surge in various parts of the world, especially in Africa. Their use in food as additives pose a dire health risk to consumers and have been banned by various food regulatory bodies worldwide. To help increase surveillance, various methods have been proposed for their analysis in literature. This study also sought to experiment and propose an alternative method for quick, easy, cheap, robust and ecologically safe analysis of Sudan dyes in chilli pepper powder and similar matrices. The optimized method used a 6.0 mL mixture of acetone:acetonitrile (1:5 v/v) solvent in a modified QuEChERs method for extraction of Sudan dyes I-IV. The simultaneous analysis of the dyes were achieved on Shimadzu prominence UFLC 20AD coupled with SPD 20AX UV detector operated at dual wavelength of 500 and 480 nm. A total of twenty four (24) chilli pepper powder samples from eight different vendors on the Ghana market were analysed using the optimized method. Quantitation of analytes were done using the external standard calibration method with determination coefficient, R2 > 0.9999. The limit of detection (LOD) and limit of quantitation (LOQ) of the method were 0.02–0.04 mg/kg and 0.05–0.13 mg/kg respectively. A good recovery range between 85.3 – 121.2% were obtained for a spike level of 1.0 mg/kg in real samples. ANOVA analysis at 95% CL showed statistically no significant difference (p > 0.05) in the recoveries between samples and also between the individual compounds. The method experimented and proposed in this study is fast, easy, cheap, robust and ecologically safe, presenting an alternative method for routine analysis for increased rate of surveillance against the illicit use of Sudan dyes as food additives.",health
10.1016/j.neucom.2020.04.075,to_check,Neurocomputing,scopus,2020-09-24,sciencedirect,Cost sensitive active learning using bidirectional gated recurrent neural networks for imbalanced fault diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85085615458,"Most existing fault diagnosis methods may fail in the following three scenarios: (1) serial correlations exist in the process data; (2) fault data are much less than normal data; and (3) it is impractical to obtain enough labeled data. In this paper, a novel form of the bidirectional gated recurrent unit (BGRU) is developed to underpin effective and efficient fault diagnosis using cost sensitive active learning. Specifically, BGRU is devised to consider the dynamic behavior of a complex process. In the training phase of BGRU, the idea of weighting each training example is proposed to reduce the effect of class imbalance. Besides, in order to explore the unlabeled data, cost sensitive active learning is utilized to select the candidate instances. The effectiveness of the proposed method is evaluated on the Tennessee Eastman (TE) dataset and a real plasma etching process dataset. The experiment results show that the proposed cost senstive active learning bidirectional gated recurrent unit (CSALBGRU) method achieves better performance in both binary fault diagnosis and multi-class fault diagnosis.",health
10.1016/j.microc.2020.105038,to_check,Microchemical Journal,scopus,2020-09-01,sciencedirect,A smartphone-based rapid quantitative detection platform for lateral flow strip of human chorionic gonadotropin with optimized image algorithm,https://api.elsevier.com/content/abstract/scopus_id/85085341749,"Colloidal gold immunochromatographic test strip has been widely used as a rapid, simple and low-cost correct detection technology. However, its detection is often qualitative or semi-quantitative, which limits its clinical application to some extent. Herein, a portable test strip quantitative detection device based on smartphone to detect human chorionic gonadotropin (HCG) is developed. In experiment, a colloidal gold HCG detection strip based on antigen antibody immune response is constructed, and the quantitative results of three different image processing methods on the same strip detection are compared, including the threshold processing algorithm based on location information, the RGB color component extraction algorithm and the grayscale projection value processing algorithm, the results show that the last algorithm can realize the best recognition of the region of interest of strip. The mobile phone application software (App) based on this design shows that the detection limit of constructed colloidal gold HCG strip is 3 ng/mL with a linear range of 6–300 ng/mL. The detection result of real urine sample is consistent with the spiked concentration (R2 = 0.988), indicating that the concentration of HCG can be accurately measured in urine with this method, presenting the potential for instant diagnosis.",health
10.1016/j.jenvp.2020.101406,to_check,Journal of Environmental Psychology,scopus,2020-04-01,sciencedirect,Attention restoration during environmental exposure via alpha-theta oscillations and synchronization,https://api.elsevier.com/content/abstract/scopus_id/85080995062,"Evidence has revealed that exposure to a restorative environment can have health and cognitive benefits, but the mechanism remains unclear. This study was designed to explore the neural mechanism of environmental restorative experiences. We conducted an experiment by randomly exposing thirty-two participants to either 20 min of a restorative (wooded garden) or a nonrestorative (traffic island) environment. Participants’ real-time electroencephalogram (EEG) signals were monitored via a 14-channel mobile device during the exposure. They also completed a series of psychological assessments of affective and cognitive functioning, as well as the perceived restorativeness of the environment, before and after the exposure.
                  The results revealed stronger and more efficient alpha-theta synchronization (functional connectivity) during the restorative experience, as well as stronger alpha-theta oscillations in the occipital lobes. Regression analysis revealed that perceived coherence was associated with the efficiency of the alpha-theta synchronization network (alpha: coef. = 2.02, 95% CI [0.68, 3.36], p = 0.020, R2 = 23.97%; theta: coef. = 2.41, 95% CI [1.30, 3.52], p < 0.001, R2 = 39.54%); fatigue recovery was associated with alpha-theta oscillations in the occipital lobes (alpha: coef. = -0.58, 95% CI [-0.88, −0.28], p = 0.001, R2 = 33.62%; theta: coef. = -0.50, 95% CI [-0.73, −0.26], p < 0.001, R2 = 37.74%); and the degree of fatigue recovery further improved attention-related cognitive performance (coef. = 0.22, 95% CI [0.03, 0.41], p = 0.027; R2 = 15.28%). Based on the abovementioned evidence, we proposed that the perceived coherence of the restorative environment may induce fatigue recovery and, hence, attention restoration via alpha-theta oscillations and synchronization. The increased alpha-theta oscillations in the occipital lobes suppress visual processing, allowing the human brain to reorganize itself via alpha-theta synchronization.",health
10.1016/j.cortex.2019.11.021,to_check,Cortex,scopus,2020-04-01,sciencedirect,Response patterns in the developing social brain are organized by social and emotion features and disrupted in children diagnosed with autism spectrum disorder,https://api.elsevier.com/content/abstract/scopus_id/85077919917,"Adults and children recruit a specific network of brain regions when engaged in “Theory of Mind” (ToM) reasoning. Recently, fMRI studies of adults have used multivariate analyses to provide a deeper characterization of responses in these regions. These analyses characterize representational distinctions within the social domain, rather than comparing responses across preferred (social) and non-preferred stimuli. Here, we conducted opportunistic multivariate analyses in two previously collected datasets (Experiment 1: n = 20 5–11 year old children and n = 37 adults; Experiment 2: n = 76 neurotypical and n = 29 5–12 year old children diagnosed with Autism Spectrum Disorder (ASD)) in order to characterize the structure of representations in the developing social brain, and in order to discover if this structure is disrupted in ASD. Children listened to stories that described characters' mental states (Mental), non-mentalistic social information (Social), and causal events in the environment (Physical), while undergoing fMRI. We measured the extent to which neural responses in ToM brain regions were organized according to two ToM-relevant models: 1) a condition model, which reflected the experimenter-generated condition labels, and 2) a data-driven emotion model, which organized stimuli according to their emotion content. We additionally constructed two control models based on linguistic and narrative features of the stories. In both experiments, the two ToM-relevant models outperformed the control models. The fit of the condition model increased with age in neurotypical children. Moreover, the fit of the condition model to neural response patterns was reduced in the RTPJ in children diagnosed with ASD. These results provide a first glimpse into the conceptual structure of information in ToM brain regions in childhood, and suggest that there are real, stable features that predict responses in these regions in children. Multivariate analyses are a promising approach for sensitively measuring conceptual and neural developmental change and individual differences in ToM.",health
10.1016/j.compeleceng.2019.106530,to_check,Computers and Electrical Engineering,scopus,2020-01-01,sciencedirect,Arm fracture detection in X-rays based on improved deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85075720723,"In this paper, a novel deep learning method is proposed and applied to fracture detection in arm bone X-rays. The main improvements include three aspects. First, a new backbone network is established based on feature pyramid architecture to gain more fractural information. Second, an image preprocessing procedure including opening operation and pixel value transformation is developed to enhance the contrast of original images. Third, the receptive field adjustment containing anchor scale reduction and tiny RoIs expansion is exploited to find more fractures. In the experiments, nearly 4000 arm fracture X-ray radiographs collected from MURA dataset are annotated by experienced radiologists. The experiment results show that the proposed deep learning method achieves the state-of-the-art AP in arm fracture detection and it has strong potential application in real clinical environments.",health
10.1016/j.asoc.2019.105890,to_check,Applied Soft Computing Journal,scopus,2020-01-01,sciencedirect,A new convolutional neural network model for peripapillary atrophy area segmentation from retinal fundus images,https://api.elsevier.com/content/abstract/scopus_id/85074508955,"Peripapillary atrophy (PPA) is a clinical finding, which reflects the atrophy of retina layer and retinal pigment epithelium. The size of PPA area is a useful medical indicator, as it is highly associated with many diseases such as glaucoma and myopia. Therefore, separating the PPA area from retinal images, which is called PPA area segmentation, is very important. It is a challenging task, because PPA areas are irregular and non-uniform, and their contours are blurry and change gradually. To solve these issues, we transform the PPA area segmentation task into a task of segmenting another two areas with relatively regular and uniform shapes, and then propose a novel multi-task fully convolutional network (MFCN) model to jointly extract them from retinal images. Meanwhile, we take edge continuity of the target area into consideration. To evaluate the performance of the proposed model, we conduct experiments on images with PPA areas labelled by experts and achieve an average precision of 0.8928, outperforming the state-of-the-art models. To demonstrate the application of PPA segmentation in medical research, we apply PPA related features based on the segmented PPA area on differentiating glaucomatous and physiologic large cup cases. Experiment conducted on real datasets confirms the effectiveness of using these features for glaucoma diagnosis.",health
10.1016/j.bios.2019.111549,to_check,Biosensors and Bioelectronics,scopus,2019-10-01,sciencedirect,Efficient electron-mediated electrochemical biosensor of gold wire for the rapid detection of C-reactive protein: A predictive strategy for heart failure,https://api.elsevier.com/content/abstract/scopus_id/85071785022,"C-reactive protein (CRP) is considered a promising biomarker for the rapid and high-throughput real-time monitoring of cardiovascular disease and inflammation in unprocessed clinical samples. Implementation of this monitoring would enable various transformative biomedical applications. We have fabricated a highly specific sensor chip to detect CRP with a detection limit of 2.25 fg/mL. The protein was immobilized on top of a gold (Au) wire/polycarbonate (PC) substrate using 1-ethyl-3-(3-dimethylamino-propyl) carbodiimide hydrochloride/N-hydroxy succinimide-activated 3-mercaptoproponic acid (MPA) as a self-assembled monolayer agent and bovine serum albumin (BSA) as a blocking agent. In contrast to the bare PC substrate, the CRP/BSA/anti-CRP/MPA/Au substrate exhibited a considerably high electrochemical signal toward CRP. The influence of the experimental parameters on CRP detection was assessed via various analysis methods, and these parameters were then optimized. The linear dynamic range of the CRP was 5–220 fg/mL for voltammetric and impedance analysis. Morever, the strategy exhibited high selectivity against various potential interfering species and was capable of directly probing trace amounts of the target CRP in human serum with excellent selectivity. The analytical assay based on the CRP/BSA/anti-CRP/MPA/Au substrate could be exploited as a potentially useful tool for detecting CRP in clinical samples.",health
10.1016/j.chemolab.2019.07.006,to_check,Chemometrics and Intelligent Laboratory Systems,scopus,2019-08-15,sciencedirect,Optical detection of contamination event in water distribution system using online Bayesian method with UV–Vis spectrometry,https://api.elsevier.com/content/abstract/scopus_id/85069656799,"The detection of contamination events in water distribution systems remains a major concern to public health. However, much of the contaminant detection methods are supervised learning which cannot adapt to the complex environment in practical applications. In this work, a contaminant detection method using ultraviolet–visible spectroscopy technique was developed to achieve a goal of real-time detection on online acquisition of absorbance spectra. This method combined the advantages of probability distribution, message-passing algorithm and Bayesian theory. Message-passing algorithm was used to achieve a goal of online detection on noisy UV–Vis spectra signals. The proposed Bayesian algorithm organized the message passing schedule and helped to extract sequential patterns for event classification, which can avoid extreme conclusions. In addition, parameters were set up based on reasonable prior, by which the detection model was dynamically updated. Pilot scale experiment was conducted for long-term online monitoring of the water distribution system. And the experiment results showed improved performances in its ability to detect contamination events with higher probabilities, compared to previous studies.",health
10.1016/j.jenvman.2019.04.026,to_check,Journal of Environmental Management,scopus,2019-07-01,sciencedirect,Validated predictive modelling of sulfonamide and beta-lactam resistance genes in landfill leachates,https://api.elsevier.com/content/abstract/scopus_id/85064155267,"The spread of antimicrobial resistance via landfill leachates jeopardizes millions of people's health, which can be exacerbated due to the unclear quantitative relationships between leachate characteristics and occurrences of antibiotic resistance genes (ARGs). Here, in parallel with sampling raw leachates from a real landfill, we constructed a lab-scale landfill and collected its leachates for 260 days. All leachate samples were analyzed for the abundance of integrons, sulfonamide resistance (sulR; sul1 and sul2) and beta-lactams resistance (blaR; bla
                     OXA, bla
                     CTX-M, and bla
                     TEM) genes. The enrichment of sulR subtypes was closely associated with the integrons' prevalence during the landfilling process (0.65–0.75 log10(copies/mL)), which can be explained by the multiple linear regression that contained intl1, pH, and nitrogen compounds as variables. The predicted abundance of sulR genes (6.06 ± 0.6 log10(copies/mL)) was statistically the same as the observed value in raw leachates (P = 0.73). The abundance of blaR genes decreased from 5.0 to 2.5 log10(copies/mL) during the experiment (P < 0.001); and a locally weighted regression of blaR genes with integrons, COD and total nitrogen accurately predicted blaR genes abundance in raw leachate (Bootstrap = 10,000, P = 0.67). The partial least squares path modelling (PLS-PM) showed that variations of blaR genes in the lab and raw leachates shared an identical pattern (PLS-PM, Bootstrap = 10,000, P > 0.05), which was influenced by integrons and environmental factors with the coefficients of −0.11 and 0.39, respectively. We believe the validated models are highly useful tools to streamline the strategies for monitoring and prediction of ARGs.",health
10.3168/jds.2018-14616,to_check,Journal of Dairy Science,scopus,2019-04-01,sciencedirect,Effect of repeated intravenous lipopolysaccharide infusions on systemic inflammatory response and endometrium gene expression in Holstein heifers,https://api.elsevier.com/content/abstract/scopus_id/85061058344,"This study aimed to evaluate the effect of repeated intravenous lipopolysaccharide (LPS) infusions in nonlactating heifers on (1) the systemic proinflammatory state as measured by biomarkers in blood and plasma, and (2) endometrial gene expression of candidate transcripts on d 15 of gestation. Our hypothesis was that target transcripts related to a major functional group would be negatively modified in the preimplantation endometrium by the LPS treatments. In the first experiment (n = 13), a systemic proinflammatory state [defined as increased plasma concentrations of tumor necrosis factor (TNF)-α and haptoglobin for 2 wk] was established using 2 different sequential LPS infusion protocols. In the second experiment, heifers (n = 22; 11 mo of age) had their time of ovulation synchronized by a modified Ovsynch protocol and were enrolled in 1 of 2 treatments: control (CON; n = 11), which received sterile saline solution i.v., and LPS treatment (LPS; n = 11), submitted to repeated i.v. LPS injections (0.10, 0.25, 0.50, 0.75, 1.00, and 1.25 µg/kg) starting 2 d after artificial insemination (AI; d 0) and then every other day until d 15 after AI. At each LPS injection, rectal temperatures were measured hourly for 6 h. Blood samples were collected from d −1 to d 13 for analyses of progesterone, TNF-α, and haptoglobin in plasma, along with white blood cell (WBC) count and differential analysis. On d 15, endometrium tissue biopsies were taken and kept at −80°C until quantitative real-time PCR analysis of 30 target transcripts related to the immune system, adhesion molecules, and endometrium receptivity. Data were checked for normality and analyzed by repeated-measures ANOVA using PROC UNIVARIATE and PROC MIXED of SAS (SAS Institute Inc., Cary, NC). After each LPS injection, temperature was greater in the first 4 h in the LPS group compared with CON. Both TNF-α and haptoglobin increased in the LPS treatment with a significant treatment by day interaction. Total leukocyte count did not differ between treatments, but the differential count increased for neutrophils, band cells, and monocytes, and decreased for lymphocytes and eosinophils in LPS compared with CON. Progesterone concentrations in plasma did not differ between treatments during the experimental period. Out of 30 target genes analyzed, 3 transcripts were differentially expressed: indoleamine 2,3-dioxygenase (IDO; fold-change = 0.48) and pentraxin-3 (PTX3; fold-change = 0.38) were downregulated, whereas myxovirus-resistance protein (MX1; fold-change = 2.85) was upregulated in the LPS group. Sequential LPS injections were able to induce a prolonged systemic proinflammatory state, but effects on gene expression were limited to transcripts associated with the immune system. These results suggest that a mechanism for subfertility is linked to a proinflammatory state in dairy heifers.",health
10.1016/j.midw.2018.10.004,to_check,Midwifery,scopus,2019-01-01,sciencedirect,Developing and evaluating an online learning tool to improve midwives’ accuracy of visual estimation of blood loss during waterbirth: An experimental study,https://api.elsevier.com/content/abstract/scopus_id/85055294797,"Objective
                  The principal objective was to test the effectiveness of an online learning tool to improve midwives’ accuracy of blood loss estimations in a birthing pool environment. The secondary objective was to assess the acceptability of the online learning tool to the midwives using it.
               
                  Design
                  A one group pre-test, post-test experiment with immediate and six weeks follow-up to test ability together with an online questionnaire to assess perceived usefulness of an online learning tool.
               
                  Setting
                  A large NHS maternity hospital comprising an acute care obstetric unit, a small district unit labour ward, one alongside midwifery-led unit and three freestanding midwifery-led units.
               
                  Participants
                  Volunteer NHS employed midwives who had experience in caring for women labouring and giving birth in water (n = 24).
               
                  Intervention
                  An online learning tool comprising six randomly ordered short video simulations of blood loss in a birthing pool in real time, and a tutorial giving verbal and pictorial guidance on making accurate blood loss estimations in water was developed then piloted. Midwives’ accuracy scores for estimating blood loss in each of the videos were calculated at three timepoints; pre and immediately post the learning component, and six weeks later. The estimated blood loss volume was subtracted from the actual blood loss volume, to give the difference between estimated and real blood loss in millilitres (ml) which was then converted to percentage difference to standardise comparison across the six volumes. The differences between pre- and post-learning for each of the six blood volumes was analysed using a repeated measures ANOVA. Statistical significance was set at p < 0.05. An online questionnaire incorporated questions using Likert scales to gauge confidence and competence and free text. Free text responses were analysed using a modified form of inductive content analysis.
               
                  Findings
                  Twenty-two midwives completed the online learning and immediate post-test, 14 completed a post-test after six weeks, and 15 responded to the online questionnaire. Pre-test results showed under-estimation of all blood loss volumes and particularly for the two largest volumes (1000 and 1100 ml). Across all volumes, accuracy of estimation was significantly improved at post-test 1. Accuracy diminished slightly, but overall improvement remained, at post-test 2. Participants rated the online tool positively and made suggestions for refining it.
               
                  Key conclusions and implications for practice
                  This is the first study measuring the accuracy of midwives’ blood loss estimations in a birthing pool using real-time simulations and testing the effectiveness of an online learning tool to improve this important skill. Our findings indicate a need to develop interventions to improve midwives’ accuracy at visually estimating blood loss in water, and the potential of an online approach. Most women who labour and/or give birth in water do so in midwifery-led settings without immediate access to medical support. Accuracy in blood loss estimations is an essential core skill",health
10.1016/j.neucom.2017.10.044,to_check,Neurocomputing,scopus,2018-03-15,sciencedirect,Mortality prediction for ICU patients combining just-in-time learning and extreme learning machine,https://api.elsevier.com/content/abstract/scopus_id/85032892910,"Mortality prediction for patients in intensive care unit (ICU) is necessary to prioritize resources as well as to help the medical staff to make decisions, and hence more accurate methods for identifying high risk patients are very important for improving clinical care. However, many existing approaches including some scoring systems now being used in the hospital are not good enough since they try to establish a global/average offline model, which may be unsuitable for a specific patient. Thus, a more robust and effective monitoring model adaptable to individual patients is needed. To establish a more personalized model, this study proposes a two-step framework, in which the first step is for clustering and while the second one is for mortality predication. A novel method combining just-in-time learning (JITL) and extreme learning machine (ELM), referred to JITL-ELM, is proposed for mortality prediction, which applies global optimization of variables and neighborhood of appropriate samples to build an accurate patient-specific model. In addition, a simplified JITL-ELM with less key physiological variables is developed. In the experiment, 4000 real clinical records of ICU patients are collected to validate the proposed algorithm, of which the AUC index is 0.8568, which is much better than the existing traditional global/average models, and furthermore the simplified JITL-ELM still performs well.",health
10.1109/ICICTA.2010.569,to_check,2010 International Conference on Intelligent Computation Technology and Automation,IEEE,2010-05-12 00:00:00,ieeexplore,Model of Viability Prediction Based on Neural Network and Data Mining Technique for Forest Industry Enterprise,https://ieeexplore.ieee.org/document/5522660/,"The operating status of a forest industry enterprise is disclosed periodically for viability. As a result, the manager usually only get information about the operating decision. An employer may be in after the formal financial statement has been published. If the employer executives intentionally package financial statements with the purpose of hiding the actual status of the forestry industry enterprise, then manager will have even less chance of obtaining the real financial information. To improve the accuracy of the viability prediction, viability ratios, non-viability ratios, and factor analysis had been used to extract adaptable variables. Moreover, the neural network and data mining technique were used to build the viability prediction model. The empirical experiment with a total of viability and non-viability ratios and projects as the initial samples obtained a satisfactory result, which testifies for the feasibility and validity of our proposed methods for the viability prediction of forestry industry enterprise.",industry
10.1109/ICIII.2008.195,to_check,"2008 International Conference on Information Management, Innovation Management and Industrial Engineering",IEEE,2008-12-21 00:00:00,ieeexplore,BP Neural Network Optimized with PSO Algorithm for Daily Load Forecasting,https://ieeexplore.ieee.org/document/4737732/,"Accurate forecasting of daily electricity load has been one of the most important issues in the electricity industry. In recent few decades, the artificial neural network has been successfully employed to solve this problem because of the powerful capability to generalize the nonlinear relationships between the inputs and the desired outputs, without considering real problem domain expressions. A short-term load forecasting method based on BP neural network which is optimized by particle swarm optimization (PSO) algorithm is presented in this paper. The PSO is used to optimize the initial parameters of the BP neural network, then based on the optimized result, the BP neural network is used for short-term load forecasting. The experiment results show the method in the paper has greater improvement in both accuracy and velocity of convergence for BP neural network. Consequently, the model is practical and effective and provides a alternative for forecasting electricity load.",industry
10.1109/ICFSP.2018.8552045,to_check,2018 4th International Conference on Frontiers of Signal Processing (ICFSP),IEEE,2018-09-27 00:00:00,ieeexplore,Building an Automatic Defect Verification System Using Deep Neural Network for PCB Defect Classification,https://ieeexplore.ieee.org/document/8552045/,"In the PCB industry, automatic optical inspection (AOI) system takes an important role to increase yield rate. However, the false alarm rate of AOI equipment is high. Therefore, the high cost of human visual inspection at verify and repair system (VRS) station is becoming a problem. Therefore, we propose an automatic defect verification system, called Auto-VRS, to decrease the false alarm rate and reduce operator's workload. The proposed system is composed of two subsystems, referred to fast circuit comparison and deep neural network based defect classification. The fast circuit comparison is to find the accurate defect region of interest (ROI). The deep neural network based defect classification is to verify which is real defect or pseudo defect. The experiment results showed that the Auto-VRS can recognition defects well and has the significant reduction in both false alarm rate and escape rate. With the advantage of the Auto-VRS, it can further improve the VRS operator's efficiency and accuracy in the future.",industry
10.1109/ICDM.2011.44,to_check,2011 IEEE 11th International Conference on Data Mining,IEEE,2011-12-14 00:00:00,ieeexplore,Cross Domain Random Walk for Query Intent Pattern Mining from Search Engine Log,https://ieeexplore.ieee.org/document/6137226/,"Understanding search intents of users through their condensed short queries has attracted much attention both in academia and industry. The search intents of users are generally assumed to be associated with various query patterns, such as ""MobileName price"", where ""MobileName"" could be any named entity of mobile phone model and this pattern indicates that the user intends to buy a mobile phone. However, discovering the query intent patterns for general search is challenging mainly due to the difficulty in collecting sufficient training data for learning query patterns across a large number of searchable domains. In this work, we propose Cross Domain Random Walk (CDRW) algorithm, which is semi-supervised, to discover the query intent patterns across different domains from search engine click-through log data. Starting with some manually tagged seed queries in one or more independent domains, CDRW takes the query patterns as bridge and propagates the transition probability across domains to collect the query intent patterns among different domains based on the assumption that ""users who have similar intent in different but similar domains will have high probability to share similar query patterns across domains"". Different from classical random walk algorithms, CDRW walks across different domains to disseminate the shared knowledge in a transfer learning manner. Extensive experiment results on real log data of a commercial search engine well validate the effectiveness and efficiency of the proposed algorithm.",industry
10.1109/GUCON50781.2021.9573994,to_check,"2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)",IEEE,2021-09-26 00:00:00,ieeexplore,Cyber Warfare Threat Categorization on CPS by Dark Web Terrorist,https://ieeexplore.ieee.org/document/9573994/,"The Industrial Internet of Things (IIoT) also referred as Cyber Physical Systems (CPS) as critical elements, expected to play a key role in Industry 4.0 and always been vulnerable to cyber-attacks and vulnerabilities. Terrorists use cyber vulnerability as weapons for mass destruction. The dark web's strong transparency and hard-to-track systems offer a safe haven for criminal activity. On the dark web (DW), there is a wide variety of illicit material that is posted regularly. For supervised training, large-scale web pages are used in traditional DW categorization. However, new study is being hampered by the impossibility of gathering sufficiently illicit DW material and the time spent manually tagging web pages. We suggest a system for accurately classifying criminal activity on the DW in this article. Rather than depending on the vast DW training package, we used authorized regulatory to various types of illicit activity for training Machine Learning (ML) classifiers and get appreciable categorization results. Espionage, Sabotage, Electrical power grid, Propaganda and Economic disruption are the cyber warfare motivations and We choose appropriate data from the open source links for supervised Learning and run a categorization experiment on the illicit material obtained from the actual DW. The results shows that in the experimental setting, using TF-IDF function extraction and a AdaBoost classifier, we were able to achieve an accuracy of 0.942. Our method enables the researchers and System authoritarian agency to verify if their DW corpus includes such illicit activity depending on the applicable rules of the illicit categories they are interested in, allowing them to identify and track possible illicit websites in real time. Because broad training set and expert-supplied seed keywords are not required, this categorization approach offers another option for defining illicit activities on the DW.",industry
10.1109/IMCEC46724.2019.8984019,to_check,"2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",IEEE,2019-10-13 00:00:00,ieeexplore,Deep Learning: Excellent Method at Surface Defect Detection of Industrial Products,https://ieeexplore.ieee.org/document/8984019/,"Surface defect detection of industrial products has always been an important part of the manufacturing industry. At present,there is a high false detection rate and low efficiency problem of traditional image processing algorithms which easy to be disturbed by complex background. Aiming at the above problems, a method for surface defect detection based on deep learning is proposed. YOLOv3 network adopted in this paper has great advantages in small target recognition and location of target in complex background. In addition, the train-set is effectively extended by elastic deformation and thin-plate spline algorithm. The experiment results show that the scratch recognition rate is as high as 95.8%, the over-judgment rate is 5.4%,and the missed rate is 1.3%.The method can identify the surface defects in a short time, and the average detection time does not exceed 0.4s, which can meet the real-time and precision requirements of industrial applications.",industry
10.1109/SCC.2017.51,to_check,2017 IEEE International Conference on Services Computing (SCC),IEEE,2017-06-30 00:00:00,ieeexplore,Deep and Shallow Model for Insurance Churn Prediction Service,https://ieeexplore.ieee.org/document/8035004/,"Churn prediction is very important to the insurance industry. Therefore, there is a big value to investigate how to improve its performance. More importantly, a good model can be used by a common service provider and benefit many companies. State-of-the-art methods either use 1) shallow models such as logistic regression, with sophisticated feature engineering, or 2) deep models that learn features and classification models simultaneously. In terms of performance, shallow models can memorize better while deep models can generalize better but may under-generalize with insufficient data. Therefore, we propose a combined Deep &amp;, Shallow model (DSM) to take the strengths of both memorization and generalization in one model by jointly training shallow models and deep models. The experiment results show that for insurance churn prediction, joint training can significantly improve the performance and the DSM earns better performance than both shallow-only and deep-only models. In our real-life dataset, the DSM performs better than CNN, LSTM, Stochastic Gradient Descent, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Gaussian Naive Bayes, AdaBoost, Random Forest, and Gradient Tree Boosting. In addition, the DSM can also be applied to other prediction services.",industry
10.1109/SNPD.2017.8022721,to_check,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE,2017-06-28 00:00:00,ieeexplore,Issues with conducting controlled on-line experiments for E-Commerce,https://ieeexplore.ieee.org/document/8022721/,"More and more on-line experiments have been done in E-Commerce in order to understand the behavior of users or customers and then apply the data analysis technique to provide business guidance. One of the techniques is A/B testing. However, there is not clear guidance on the sample size in order for us to have valuable, trustable discovery. The purpose of this work is to find out a way to group customers in the data sample in order to achieve an optimal difference between the buckets. Based on the analysis result of real data collected during joining an industry project, we think the problem is complex and the meaningful conclusions have to be drawn with caution from business experiments such as A/B testing, due to the vast variation in the data. Moreover, if we don't allocate enough samples in the treatment group, the experiment could be inconclusive even if the testing lasts for a longer enough time, such as one month.",industry
10.1109/ICCMC51019.2021.9418273,to_check,2021 5th International Conference on Computing Methodologies and Communication (ICCMC),IEEE,2021-04-10 00:00:00,ieeexplore,Systematic Study on Hardware Optimization of 5G Communication,https://ieeexplore.ieee.org/document/9418273/,"Systematic study on hardware optimization of 5G communication is conducted in this paper. With the continuous development of the current era, 5G communication has attracted much attention and has become an important carrier of the future communication industry. Therefore, before launching the specific operations, the most critical thing is to improve the application research level of 5G communication technology as much as possible, and to comprehensively deal with the problems presented in the previous stage in the research process. 5G network architecture realizes the transformation from mobile cellular to general distributed and heterogeneous communication methods. This paper considers the deep learning as the basis for the analysis of efficient modelling. The model is designed on FPGA and the optimization is combined with neural networks. The experiment has shown the 5G communication is enhanced with the combination of computational intelligence.",industry
10.1109/ACCESS.2020.2996214,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Machine Learning Security Framework for Iot Systems,https://ieeexplore.ieee.org/document/9097876/,"Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges.",industry
10.1109/ACCESS.2020.2991474,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,An Adaptive Method for Inspecting Illumination of Color Intensity in Transparent Polyethylene Terephthalate Preforms,https://ieeexplore.ieee.org/document/9082606/,"Machine vision systems are applied in industry to control the quality of production while optimizing efficiency. A machine vision and AI-based inspection of color intensity in transparent Polyethylene Terephthalate (PET) preforms is especially sensitive to backgrounds and lighting, therefore, much attention is given to its illumination conditions. The paper examines the adverse factors affecting the quality of image recognition and presents an adaptive method for reducing the influence of changing illumination conditions in the color inspection process of transparent PET preforms. The method is based on predicting measured color intensity correction parameters according to illumination conditions. To test this adaptive method, a hardware and software system for image capture and processing was developed. This system is capable of inspecting large quantities of preforms in real time using a neural network with a modified gradient descent and momentum algorithm. The experiment showed that correction of the measured color intensity value reduced the standard deviation caused by variable and uneven illumination by 61.51%, demonstrating that machine vision color intensity evaluation is a robust and adaptive solution under illuminated conditions for detecting abnormalities in machine-based PET inspection procedures.",industry
10.1109/TSE.2013.2295827,to_check,IEEE Transactions on Software Engineering,IEEE,2014-04-01 00:00:00,ieeexplore,Effects of Developer Experience on Learning and Applying Unit Test-Driven Development,https://ieeexplore.ieee.org/document/6690135/,"Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.",industry
10.1109/ICST46873.2019.9047714,to_check,2019 13th International Conference on Sensing Technology (ICST),IEEE,2019-12-04 00:00:00,ieeexplore,A Fundamental Experiment on Contact Position Estimation on Vision based Dome-type Soft Tactile Sensor using Ready-made Medium,https://ieeexplore.ieee.org/document/9047714/,"Tactile sensors are critical components in robotics fields. Recently, soft tactile sensor utilizing vision is actively developed for safe human machine interaction. Some researches use novel custom-made medium in order to achieve tactile sensing. Deep learning can recognize pattern from any vision data when it has sufficient dataset, i.e., the system does not require specific pattern embedded hardware for the pattern recognition. To achieve soft tactile sensor's economical application for robot fingers, this paper presents a fundamental experiment on contract position estimation on vision based dome-type soft tactile sensor utilizing ready-made silicon as a medium and convolutional neural network. In order to estimate and classify the contact position, convolutional neural network (CNN) was applied. The modified VGGNet architecture was coded using Tensorflow and Keras. 1000 images were taken to train the modified VGG network; 200 images were taken for each neutral, left, right, lower, upper direction. For each direction, fingertip, pencil, ruler, and table corner were utilized to capture various situations. After checking the results of the test set, the trained model was applied to the embedded board and checked the contact position estimation in real-time. The experiment showed high accuracy on classifying the con-tact position of the vision based dome-type soft tactile sensor in real time. This contact position estimation system will be critical for the finger-typed robots since the system is reasonably small and it will reduce significant amount of manufacturing cost for the safe human machine interaction system. For the future work, we will acquire more image data and apply more advanced network architecture to improve accuracy.",industry
10.1109/SMC.2019.8914044,to_check,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",IEEE,2019-10-09 00:00:00,ieeexplore,Realizing an assembly task through virtual capture,https://ieeexplore.ieee.org/document/8914044/,"Modern manufacturing strategy requires the robotic infrastructure to be able to adapt to new products or to accomplish new tasks quickly. In order to respond to this demand, teaching a robot to realize a task by demonstration has regained popularity in recent years, especially for dual-arm or humanoid robots. One of the main issues using this method is to adapt the captured motion from the human demonstration to the robot's specific kinematics and control. In this paper we present a method where the motion and grasping adaptation is tackled during the capture. We demonstrate the validity of this method with an experiment where a humanoid robot realizes an assembly previously demonstrated by a user wearing a Head Mounted Display (HMD) performing an assembly task in a virtual environment.",industry
10.1109/ASMC49169.2020.9185292,to_check,2020 31st Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC),IEEE,2020-08-26 00:00:00,ieeexplore,Trace Data Analytics with Knowledge Distillation : DM: Big Data Management and Mining,https://ieeexplore.ieee.org/document/9185292/,"In this paper, we propose the “trace data analytics” for classifying fault conditions from multivariate time series sensor signals using well-known deep CNN models. In our approach, multiple sensor signals are converted into two dimensional representations using the proposed conversion methods to optimize the classification performance. Many studies on the prediction of manufacturing results using sensor signals have been conducted in the field of fault detection and classification for display and semiconductor manufacturing processes. It is challenging to apply machine learning to real-life manufacturing problems due to practical limitations, class imbalance and data insufficiency, which also make it difficult to produce a generalized model. To overcome these challenges, we propose using omni-supervised learning but with a new approach to knowledge distillation that ensembles predictions from multiple instantiations of a CNN model of synthetically generated data samples from a deep generative model. Our experiment results show that the fault classification accuracy improves substantially by applying trace data analytics to manufacturing data from display fabrication lines. The results also show that the quality of trained CNN models using the proposed knowledge distillation is maintained steadily and stably.",industry
10.1109/ACCESS.2020.3029868,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Research on Adaptive Job Shop Scheduling Problems Based on Dueling Double DQN,https://ieeexplore.ieee.org/document/9218934/,"Traditional approaches for job shop scheduling problems are ill-suited to deal with complex and changeable production environments due to their limited real-time responsiveness. Based on disjunctive graph dispatching, this work proposes a deep reinforcement learning (DRL) framework, that combines the advantages of real-time response and flexibility of a deep convolutional neural network (CNN) and reinforcement learning (RL), and learns behavior strategies directly according to the input manufacturing states, thus is more appropriate for practical order-oriented manufacturing problems. In this framework, a scheduling process using a disjunction graph is viewed as a multi-stage sequential decision-making problem and a deep CNN is used to approximate the state-action value. The manufacturing states are expressed as multi-channel images and input into the network. Various heuristic rules are used as available actions. By adopting the dueling double Deep Q-network with prioritized replay (DDDQNPR), the RL agent continually interacts with the scheduling environment through trial and error to obtain the best policy of combined actions for each decision step. Static computational experiments are performed on 85 JSSP instances from the well-known OR-Library. The results indicate that the proposed algorithm can obtain optimal solutions for small scale problems, and performs better than any single heuristic rule for large scale problems, with performances comparable to genetic algorithms. To prove the generalization and robustness of our algorithm, the instances with random initial states are used as validation sets during training to select the model with the best generalization ability, and then the performance of the trained policy on scheduling instances with different initial states is tested. The results show that the agent is able to get better solutions adaptively. Meanwhile, some studies on dynamic instances with random processing time are performed and experiment results indicate that out method can achieve comparable performances in dynamic environment in the short run.",industry
10.1109/TII.2020.3038780,to_check,IEEE Transactions on Industrial Informatics,IEEE,2021-10-01 00:00:00,ieeexplore,Toward Secure Data Fusion in Industrial IoT Using Transfer Learning,https://ieeexplore.ieee.org/document/9262056/,"As an emerging technology, the industrial Internet of Things (IIoT) can promote the development of industrial intelligence, improve production efficiency, and reduce manufacturing costs. In IIoT, the improvement and progress of industrial production and applications are inseparable from data fusion, a process that realizes the collection, analysis, and processing of the massive IoT data generated by industrial equipment and applications. IIot demands a real-time, effective, and privacy-preserving data fusion process. However, the existing works need to train different learning models for data analysis, which cannot meet real-time requirements in IIoT. Meanwhile, the lack of defense against internal attacks and the difficulty to balance system performance and privacy protection hinder the effectiveness and privacy protection in the data fusion process. To solve the abovementioned problems, in this article, we propose a new transfer learning-based secure data fusion strategy (TSDF) for IIoT. The proposed TSDF consists of three parts, guidance based deep deterministic policy gradient (GDDPG) algorithm for task classification, transfer learning based GDDPG for grouping of task receivers, and a multiblockchain mechanism for privacy preservation. The experiment results show that TSDF can achieve high system throughput and low latency, providing privacy preservation in data fusion under various IIoT application environments.",industry
10.1109/ETFA.2019.8869274,to_check,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),IEEE,2019-09-13 00:00:00,ieeexplore,Analyzing availability and QoS of service-oriented cloud for industrial IoT applications,https://ieeexplore.ieee.org/document/8869274/,"Internet of Things and cloud services are one of main enablers in fourth industrial revolution. Real-time industrial systems have high availability requirements of 99.9% to 99.999% whereas architectures built on regional cloud services and IoT do not provide similar guarantees or Service Level Agreement. These differences of QoS and SLA availability between Operational Technology and Information Technology has become a main challenge in adoption of Industrial Internet of Things (IIoT) for real-time applications.This work presents an approach to find end-to-end QoS and availability for an IIoT architecture. Device-to-cloud, cloud-to-cloud and inside-cloud experiments have been performed over eight weeks where each experiment have more then four million QoS measurements. Our availability analysis shows that a remote IoT connected to a less busy cloud region gives higher availability as compared to an IoT device inside a busy cloud region. IIoT and regional cloud services provide good QoS with 99% to 99.9% availability for 1sec soft real-time requirements. In 100ms applications, more efforts are required to achieve higher then 95% availability and design industrial SLA. IIoT applications with 10sec latency like machine learning models can get 99.9% availability with cloud. Availability loss due to communication is almost 1% for 100ms applications. These results also provide requirements and future work of industrial edge computing for IIoT on real-time cloud.",industry
10.1109/GLOBECOM42002.2020.9348249,to_check,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,IEEE,2020-12-11 00:00:00,ieeexplore,Communication-Efficient Federated Learning for Anomaly Detection in Industrial Internet of Things,https://ieeexplore.ieee.org/document/9348249/,"With the rapid development of the Industrial Internet of Things (IIoT), various IoT devices and sensors generate massive industrial sensing data. Sensing big data can be analyzed for insights that lead to better decisions and strategic industrial production by using advanced machine learning technologies. However, vulnerable IoT devices are easy to be compromised thus causing IoT devices failures (i.e., anomalies). The anomalies seriously affect the production of industrial products, thereby, it is increasingly important to accurately and timely detect anomalies. To this end, we first introduce a Federated Learning (FL) framework to enable decentralized edge devices to collaboratively train a Deep Anomaly Detection (DAD) model, which can improve its generalization ability. Second, we propose a Convolutional Neural Network-Long Short Term Memory (CNN-LSTM) model to accurately detect anomalies. The CNN-LSTM model uses CNN units to capture fine-grained features and retains the advantages of LSTM unit in predicting time series data. Third, to achieve real-time and lightweight anomaly detection in the proposed framework, a gradient compression mechanism is applied to reduce communication costs and improve communication efficiency. Extensive experiment results based on realworld datasets demonstrate that the proposed framework and mechanism can accurately and timely detect anomalies, and also reduce about 50% communication overhead when compared with traditional schemes.",industry
10.1109/HICSS.1999.772817,to_check,Proceedings of the 32nd Annual Hawaii International Conference on Systems Sciences. 1999. HICSS-32. Abstracts and CD-ROM of Full Papers,IEEE,1999-01-08 00:00:00,ieeexplore,Design of a vision system for identity verification,https://ieeexplore.ieee.org/document/772817/,"The use of biometric data for automated identity verification, is one of the major challenges in many application domains. This is certainly a formidable task which requires the development of a complex system including several concurrent agents operating in real time. In this paper a system for automated identity verification (currently under development within an European research project) encompassing the active vision paradigm is described. In our approach the amount of data to be processed is limited by selecting and analysing only few areas within the face image. The number of pixels for each area are also reduced by applying a space-variant conformal mapping. The devised system does not require to use special hardware. On the other hand, robustness can be enforced by performing the final matching with more than a single image. This may require to adopt a simple, coarse scale, multi-processor architecture. The system is conceived for banking applications but can be ported to a variety of industrial applications. Several experiment's on identity verification, performed on real images, are presented.",industry
10.23919/ChiCC.2019.8866554,to_check,2019 Chinese Control Conference (CCC),IEEE,2019-07-30 00:00:00,ieeexplore,Diffusion welding furnace temperature controller based on Actor-Critic,https://ieeexplore.ieee.org/document/8866554/,"Based on the basic mechanism of reinforcement learning, this paper proposes an adaptive PID control algorithm based on Actor-Critic according to the nonlinear and large delay characteristics of complex industrial processes, and controls the diffusion welding furnace. The simulation experiment of the effect shows that the algorithm has better real-time and robustness than the traditional PID.",industry
10.1109/ANTHOLOGY.2013.6784984,to_check,IEEE Conference Anthology,IEEE,2013-01-08 00:00:00,ieeexplore,Real-time cloud computing for web-based searching system of pattern recognition,https://ieeexplore.ieee.org/document/6784984/,"For cross-platform real-time systems, cloud computing technology is an innovative application of pattern recognition method. This study is the use of associative memory of the way to do the work of pattern recognition; this system is real-time client-server type network pattern recognition system. Remote user can operate through the browser to draw the shape or character of industrial components, and recognition system to the database through Internet search. Cloud storage server contains a database of pattern samples. In the training period, the user can specify any of the pattern to what in real-time. Patterns are recorded in the cloud server database. In the recall period, an innovative database matching methods have been proposed. This method can effectively solve the problem of RNN a false state of the database than on the technology to overcome the problem of capacity constraints RNN. In this new approach, CWBPR system partition database in the cloud server, a pattern record set, and then figure out they were separate sections for each value of W and θ. CWBPR system to deal with each of the last segment of the pattern recognition work. Pattern recognition technology for the network, the paper has two simulation experiments are clearly discussed. The first experiment identified a number of characters; the second experiment is the pattern recognition of industrial components. Finally, the paper also put forward innovative pattern recognition method to the traditional text input search method comparison.",industry
10.1109/ICNN.1995.487735,to_check,Proceedings of ICNN'95 - International Conference on Neural Networks,IEEE,1995-12-01 00:00:00,ieeexplore,Using CMAC neural networks and optimal control,https://ieeexplore.ieee.org/document/487735/,"This paper explores the real-time control of an industrial robotic arm to balance a mass at the end of a pole. The 3D inverted pendulum is a MIMO nonlinear inherently unstable system. The control system uses combined optimal and neural network techniques. To provide stable control, an optimal, linear quadratic regulator controller was developed from the linearized system model. When applied to the robotic system, this controller produced a relatively large limit cycle, due primarily to unmodelled system nonlinearities. The CMAC neural network was then introduced into the controller to implement a technique referred to as prediction feedback. The purpose of this adaptive feedback controller was to learn system nonlinearities, reject any residual noise, and reduce the system limit cycle. When applied to the robotic pole-balancer, the addition of adaptive prediction feedback helped to significantly decrease the magnitude and frequency of oscillation. This experiment is a primary example of how an intelligent controller can be developed by combining the strengths of different control techniques.",industry
10.23919/ChiCC.2019.8865406,to_check,2019 Chinese Control Conference (CCC),IEEE,2019-07-30 00:00:00,ieeexplore,Vision-Based Position/Impedance Control for Robotic Assembly Task,https://ieeexplore.ieee.org/document/8865406/,"In robotic assembly processes, detecting target pose is indispensable for robot manipulation algorithms. Such a tedious work hinders the further application of robot manipulation algorithms to real industrial processes. In this paper, a vision-based position/impedance control framework is proposed. Therein, the objective pose is firstly obtained by a motion capture system, where a kinematic equation is solved to Figure out the target assembly pose. Next the robot arm motion is approximated by an impedance model, in this way, the position/impedance control law of the robot is derived, which drives the robot to perform a compliant assembly manipulation. Finally, a memory stick assembly experiment is conducted on a 6-DOF robot arm equipped with a motion capture system and a force-torque sensor. Experiment shows the efficiency of the vision-based position/impedance hybrid controller in terms of autonomous positioning and compliant assembly. Due to its convenience and generality, the present controller can be expected to apply to more general industrial scenarios.",industry
10.1109/JSYST.2020.2994548,to_check,IEEE Systems Journal,IEEE,2021-06-01 00:00:00,ieeexplore,An Improved Just-in-Time Learning Scheme for Online Fault Detection of Nonlinear Systems,https://ieeexplore.ieee.org/document/9099847/,"Just-in-time learning (JITL) scheme has been employed as an efficient tool of online soft sensor. It requires current measured data with high accuracy. However, in real industrial environments, it is difficult to ensure that no disturbance is added to the measurement. To solve this problem, this article proposes an improved JITL scheme, which employs leverage calculation to trim the weight of variables and abate the affect of disturbances. On this basis, an online modeling and output prediction algorithm is further presented. The experiment on a typical nonlinear system shows the better robustness and accuracy of the presented algorithm in comparison with conventional JITL-based approaches. Moreover, an online fault detection strategy for nonlinear systems is proposed based on JITL and partial least squares (PLS), for the purpose of simplifying the parameter setting and reducing the computational load of conventional fault detection approaches for nonlinear systems. Four statistic indexes are designed, including conventional T<sup>2</sup> and SPE for fault detection, T<sup>2</sup><sub>h</sub> and T<sup>2</sup><sub>t</sub> of two orthogonal decomposition input subspaces for fault birth subspace observation. A numerical nonlinear system and an industrial benchmark of Tennessee Eastman process are employed for fault detection experiments, testifying the effectiveness of the proposed strategy.",industry
10.1109/TASE.2019.2909043,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2020-01-01 00:00:00,ieeexplore,RFID-Driven Energy-Efficient Control Approach of CNC Machine Tools Using Deep Belief Networks,https://ieeexplore.ieee.org/document/8694932/,"Under the consideration of massive energy consumption of machine tools, many approaches have been proposed, and state control method of machine tools has proved its effectiveness. In order to satisfy the demand of real-time production control, a deep learning methodology for energy-efficient control of CNC machine tools is proposed in RFID-enabled ubiquitous environment. First, the energy-efficient control strategies for multiple machine tools are proposed to reduce the carbon emission of the machining process. Then, through evaluating the process progress in the RFID-enabled environment, a deep learning methodology for energy-efficient strategies selection of CNC machine tools using deep belief networks (DBNs) is established to realize the real-time and accurate control of machine tools. Finally, comparisons between the proposed approach and some state-of-the-art ones are given, and the experiment results indicate that the proposed method is effective and efficient for the energy-efficient control problem of machine tools. The proposed method can realize the real-time control of CNC machine tools based on the interaction information in Industrial 4.0. Furthermore, the machine tools will be converted to smart machines, which can complete self-perception and self-adjustment automatically.",industry
10.1109/ACCESS.2020.3043817,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Research on Double Edge Detection Method of Midsole Based on Improved Otsu Method,https://ieeexplore.ieee.org/document/9290011/,"The midsole is an important part of the shoe, but in its industrial production, the surface quality of the midsole currently relies on manual testing. It cost high and cannot meet the needs of industrial online real-time detection. To realize online surface defect detection, extracting the double edges of the midsole resulting from its special structure becomes an indispensable pre-work. This paper proposes a two-step Otsu method (TT-Otsu) to extract double edges of products. This method adopts the improved Otsu method to process the midsole image in two steps. It respectively combines with the Weighted Object Variance method (WOV) and the Neighborhood Valley-Emphasis method (NVE) to calculate the optimal threshold. Then the image is segmented to extract the edges and the misclassification error (ME) is 0.0007. To ensure the accuracy of the edge detection, the neighborhood gradient extreme value discrimination method is used to realize the local self-checking and make appropriate adjustments to the deviated edge. The false positive rate (FPR) and the false negative rate (FNR) of TT-Otsu are approximately equal to 5%. This method can effectively and clearly extract the double edges of the midsole. The precision rate is 95.61%, the average running time is 1.8s. The experiment demonstrates that the proposed method in this paper has good detection performance and good applicability.",industry
10.1109/ACCESS.2020.2980162,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Research on the Feature Selection Approach Based on IFDS and DPSO With Variable Thresholds in Complex Data Environments,https://ieeexplore.ieee.org/document/9032173/,"Neighborhood rough model is widely used in feature selection with high dimension, fuzzy, continuous and discrete attributes, incomplete data and so on, and the application of neighborhood rough model depends on neighborhood threshold. In the application of the model, the point-value neighborhood threshold is not adaptive, which leads to low classification accuracy and high time complexity of the algorithm. In order to solve the above problems, a feature selection approach based on IFDS (Incomplete Fuzzy Hybrid Decision System) and DPSO (Discrete Particle Swarm Optimization Algorithm) with variable thresholds is proposed. Firstly, a neighborhood rough model capable of simultaneously processing fuzzy, hybrid and incomplete data was established. The average reachable distance was introduced to construct the attribute neighborhood threshold set and reduce the interference of noise data on classification accuracy. Secondly, we constructed the DPSO particle fitness function using the feature subset length, the significance of the attribute and the negative domain of the neighborhood, and improved the inertia weight computing method, so as to enhance the feature selection speed and the feature subset quality. Finally, the simulation experiment was performed using the real industrial production data. The experiment effect shows that this method has obvious advantages in improving the classification accuracy, optimizing the search speed and the optimal feature subset quality.",industry
10.1016/j.jmsy.2021.08.009,to_check,Journal of Manufacturing Systems,scopus,2021-10-01,sciencedirect,Machine learning-based real-time monitoring system for smart connected worker to improve energy efficiency,https://api.elsevier.com/content/abstract/scopus_id/85113549791,"Recent advances in machine learning and computer vision brought to light technologies and algorithms that serve as new opportunities for creating intelligent and efficient manufacturing systems. In this study, the real-time monitoring system of manufacturing workflow for the Smart Connected Worker (SCW) is developed for the small and medium-sized manufacturers (SMMs), which integrates state-of-the-art machine learning techniques with the workplace scenarios of advanced manufacturing systems. Specifically, object detection and text recognition models are investigated and adopted to ameliorate the labor-intensive machine state monitoring process, while artificial neural networks are introduced to enable real-time energy disaggregation for further optimization. The developed system achieved efficient supervision and accurate information analysis in real-time for prolonged working conditions, which could effectively reduce the cost related to human labor, as well as provide an affordable solution for SMMs. The competent experiment results also demonstrated the feasibility and effectiveness of integrating machine learning technologies into the realm of advanced manufacturing systems.",industry
10.1016/j.eswa.2021.115019,to_check,Expert Systems with Applications,scopus,2021-09-15,sciencedirect,A hybrid model integrating deep learning with investor sentiment analysis for stock price prediction,https://api.elsevier.com/content/abstract/scopus_id/85104927982,"Whether stock prices are predictable has been the center of debate in academia. In this paper, we propose a hybrid model that combines a deep learning approach with a sentiment analysis model for stock price prediction. We employ a Convolutional Neural Network model for classifying the investors’ hidden sentiments, which are extracted from a major stock forum. We then propose a hybrid research model by applying the Long Short-Term Memory (LSTM) Neural Network approach for analyzing the technical indicators from the stock market and the sentiment analysis results from the first step. Furthermore, this work has conducted real-life experiments from six key industries of three time intervals on the Shanghai Stock Exchange (SSE) to validate the effectiveness and applicability of the proposed model. The experiment results indicate that the proposed model has achieved better performance in classifying investor sentiments than the baseline classifiers, and this hybrid approach performs better in predicting stock prices compared to the single model and the models without sentiment analysis.",industry
10.1016/j.measurement.2020.108554,to_check,Measurement: Journal of the International Measurement Confederation,scopus,2021-03-01,sciencedirect,Tool wear mechanism and prediction in milling TC18 titanium alloy using deep learning,https://api.elsevier.com/content/abstract/scopus_id/85092319229,"Rapid tool wear from milling TC18 (Ti-5Al-5Mo-5V-1Cr-1Fe) leads to increased surface deterioration and manufacturing costs. Here, a real-life tool wear experiment was introduced, and the three stages of tool wear were analyzed in detail according to the tool wear micro-topography and chemical elements. In the initial and normal stage, the tool wear was slow because of the protection of the adhesive titanium layer and dense alumina film. Diffusion wear and oxidation wear occurred until the sever wear stage. Based on the above wear mechanism determination, after acquiring the real time cutting force, the tool wear prediction models were established using a convolutional bi-directional long short-term memory networks (CNN + BILSTM) and a convolutional bi-directional gated recurrent unit (CNN + BIGRU). The results show that the errors of the predicted minimum values are all within 8%, demonstrating that the deep learning method offers a new and promising approach for in monitoring tool wear on-line.",industry
10.1016/j.procs.2021.03.072,to_check,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Crafting adversarial samples for anomaly detectors in industrial control systems,https://api.elsevier.com/content/abstract/scopus_id/85106726514,"The increasing adoption of the Industry 4.0 paradigm encompasses digitally interconnected factories which enables many advantages. However, it is still necessary to dedicate effort towards investigating protection mechanisms against cyberattacks in these scenarios. Despite the power demonstrated by Anomaly Detection-based Intrusion Detection Systems in industrial scenarios, their vulnerabilities to adversarial attacks, especially to evasion attacks, make Machine Learning and Deep Learning models ineffective for real scenarios. These type of attacks craft samples misclassified by the Intrusion Detection System and potentially reach industrial devices, causing potentially damaging impacts to factory workers and industry resources. Adversarial attacks linked to industrial scenarios are currently in early stages of development, hence most of them have the capability to craft samples misclassified by the IDS but not reach industrial devices. In this work, we present a new adversarial attack named Selective and Iterative Gradient Sign Method that overcomes the limitation of the adversarial attacks present in the literature. To complement this work we also detail a study of how the detection rate of an Intrusion Detection System is degraded and the time required by each technique to generate adversarial samples. The experiments were carried out using a dataset named Electra, collected from an Electric Traction Substation, and showed that adversarial attacks evaluated crafted samples misclassified by the IDS. However, only the method we proposed generated samples that can be understood by intermediate network devices and, therefore, reach their destination. Our experiment outputs demonstrate a lower period of time to achieve and craft adversarial samples using out our iterative based process method as opposed to other current iterative methods currently available.",industry
10.1016/bs.adcom.2020.08.001,to_check,Advances in Computers,scopus,2021-01-01,sciencedirect,Demystifying the blockchain technology,https://api.elsevier.com/content/abstract/scopus_id/85091072369,"The blockchain paradigm is being widely touted by many as the innovative and disruptive one capable of bringing in a few exemplary and elegant transformations in the IT space. As business operations and offerings are substantially enabled through the various crucial accomplishments and advancements in the IT field, business executives across the globe are equally keen to experiment with and embrace this new and futuristic technology to reap a slew of business benefits. Interestingly, blockchain has the inherent potential and promise to bring forth a bevy of strategically sound implications for various industry verticals. Cryptocurrency is one of the finest and foremost applications of the blockchain technology. The supply chain domain is exploring this new phenomenon for realizing some crucial advantages. The IoT discipline is another one capable of attaining a number of distinct benefits out of all the trendsetting improvisations being realized in the blockchain space. This chapter is specially crafted to tell what, why, how, and where the indispensable blockchain paradigm is being used toward real digital transformations.",industry
10.1016/j.precisioneng.2020.03.002,to_check,Precision Engineering,scopus,2020-05-01,sciencedirect,An integrated error compensation method based on on-machine measurement for thin web parts machining,https://api.elsevier.com/content/abstract/scopus_id/85081052034,"Thin webs are widely used in the aerospace industry for the advantages of compact structure, light weight and high strength-to-weight ratio. Due to its low rigidity, serious machining error may occur, therefore, Finite Element method and mechanism analysis are usually utilized to modeling its deformation. However, they are very time-consuming and only suitable for elastic deformation error. In this study, an integrated error compensation method is proposed based on on-machine measurement (OMM) inspection and error compensation. The OMM inspection is firstly applied to measure the comprehensive machining errors. The Hampel filtering is then used to eliminate outliers, followed by the triangulation-based cubic interpolation as well as a machine learning algorithm which are used to establish the compensation model. At last, the real time compensation of high-density cutting points is realized by developing the compensation system based on External Machine Zero Point Shift (EMZPS) function of machine tool. Three sets of machining experiment of a typical thin web part are conducted to validate the feasibility and efficiency of the proposed method. Experiment results revealed that after compensation, the comprehensive machining errors were controlled under different machining conditions and 58.1%, 68.4% and 62.6% of the machining error ranges were decreased, respectively. This method demonstrates immense potential for further applications in efficiency and accuracy improvement of thin-walled surface parts.",industry
10.1016/j.jmsy.2020.02.010,to_check,Journal of Manufacturing Systems,scopus,2020-04-01,sciencedirect,Smart augmented reality instructional system for mechanical assembly towards worker-centered intelligent manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85080923298,"Quality and efficiency are crucial indicators of any manufacturing company. Many companies are suffering from a shortage of experienced workers across the production line to perform complex assembly tasks. To reduce time and error in an assembly task, a worker-centered system consisting of multi-modal Augmented Reality (AR) instructions with the support of a deep learning network for tool detection is introduced. The integrated AR is designed to provide on-site instructions including various visual renderings with a fine-tuned Region-based Convolutional Neural Network, which is trained on a synthetic tool dataset. The dataset is generated using CAD models of tools and displayed onto a 2D scene without using real tool images. By experimenting the system to a mechanical assembly of a CNC carving machine, the result of a designed experiment shows that the system helps reduce the time and errors of the given assembly tasks by 33.2 % and 32.4 %, respectively. With the integrated system, an efficient, customizable smart AR instruction system capable of sensing, characterizing requirements, and enhancing worker’s performance has been built and demonstrated.",industry
10.1016/j.jmapro.2019.05.013,to_check,Journal of Manufacturing Processes,scopus,2019-07-01,sciencedirect,Real-time weld geometry prediction based on multi-information using neural network optimized by PCA and GA during thin-plate laser welding,https://api.elsevier.com/content/abstract/scopus_id/85066236249,"Real-time monitoring of the welding quality is quite important during the process of industrial laser manufacturing. In this paper, a multi-information fused neural network, combining welding parameters and morphological features of the molten pool, was proposed to predict geometric features of the weld seam. Firstly, a modified optical fiber laser coaxial monitoring platform was set up to acquire clear images of the molten pool. Then, several morphological characteristics of the molten pool were extracted. By using principal component analysis (PCA) to reduce the redundancy of these features, the welding speed, the laser power and the two PCA components acted on as the four input neurons, while the two output neurons consisted of the weld waist width (WW) and the weld back width (BW) representing weld seam quality. Before training, the genetic algorithm (GA) was adopted to optimize the initialized weights and bias of the neural network due to its globally search ability. The experiment results showed that our proposed model can effectively and steadily predict the geometric features of the weld seam with the mean absolute percentage error (MAPE) less than 1% and the mean square error (MSE) less than 10−3. Time analysis showed that the whole process time of our system containing feature extraction and neural network was less than 90 ms which can meet the time requirements of large-scale real-time thin-plate laser welding application. Our system lays a foundation on the real-time quality monitoring in the process of laser welding thin-plate butt joint.",industry
10.1016/j.jmapro.2018.10.042,to_check,Journal of Manufacturing Processes,scopus,2018-12-01,sciencedirect,Adaptive control for laser welding with filler wire of marine high strength steel with tight butt joints for large structures,https://api.elsevier.com/content/abstract/scopus_id/85056243076,"To large structures, ensuring a uniform joint gap without mismatch over an entire seam remains a challenge. Varying gaps and mismatches significantly affect the welding qualities, especially for sheet metal. In the paper, an adaptive filling model for process parameters based on back propagation neural network (BPNN) combined with genetic algorithm (GA) is used as an adaptive controller to solve the problems. First, a real-time closed loop feedback control from groove information collection, processing to model prediction and control output was established. Then, an adaptive control approach was investigated, a precision 3D laser sensor was used for measuring the size of gap and mismatch, and an adaptive parameters table was designed as an adaptive controller based on the optimal BPNN, an interpolation operation was introduced for an intermediate value. The results of the experiment showed that the welding parameters can be continuously adjusted in real time despite the variation in the gap and mismatch according to the adaptive filling algorithm and the laser sensor; the welds with desirable uniform weld appearance was achieved despite the changing gap, from 0 to 1.0 mm, and mismatch, from 0 to 1.2 mm, meet the requirement of practical industrial application.",industry
10.1016/j.engappai.2018.02.011,to_check,Engineering Applications of Artificial Intelligence,scopus,2018-05-01,sciencedirect,Computational narrative mapping for the acquisition and representation of lessons learned knowledge,https://api.elsevier.com/content/abstract/scopus_id/85043510280,"Lessons learned knowledge is traditionally gained from trial and error or narratives describing past experiences. Learning from narratives is the preferred option to transfer lessons learned knowledge. However, learners with insufficient prior knowledge often experience difficulties in grasping the right information from narratives. This paper introduces an approach that uses narrative maps to represent lessons learned knowledge to help learners understand narratives. Since narrative mapping is a time-consuming, labor-intensive and knowledge-intensive process, the proposed approach is supported by a computational narrative mapping (CNM) method to automate the process. CNM incorporates advanced technologies, such as computational linguistics and artificial intelligence (AI), to identify and extract critical narrative elements from an unstructured, text-based narrative and organize them into a structured narrative map representation. This research uses a case study conducted in the construction industry to evaluate CNM performance in comparison with existing paragraph and concept mapping approaches. Among the results, over 90% of respondents asserted that CNM enhanced their understanding of the lessons learned. CNM’s performance in identifying and extracting narrative elements was evaluated through an experiment using real-life narratives from a reminiscence study. The experiment recorded a precision and recall rate of over 75%.",industry
10.1016/j.jmsy.2017.02.013,to_check,Journal of Manufacturing Systems,scopus,2017-04-01,sciencedirect,Multi-bearing remaining useful life collaborative prediction: A deep learning approach,https://api.elsevier.com/content/abstract/scopus_id/85014511127,"Rolling bearing health analysis and remaining useful life prediction have become an increasingly crucial research area that can promote reliability and efficiency in the modern manufacturing industry. Internet-of-Things and cyber manufacturing techniques make it convenient to collect large volumes of sensor data that can provide powerful support for efficient data analytics such as deep learning. The combination of a massive amount of available data and advanced machine learning models brings new opportunities for bearing remaining useful life prediction. This paper proposes an integrated deep learning approach for multi-bearing remaining useful life collaborative prediction by combining both time domain features and frequency domain features. The method can extract high-quality degradation patterns of rolling bearing from vibration signals. Regarding features extracted from bearing vibration signals, in addition to three conventional time domain features, a novel frequency domain feature is adopted in the proposed method as well. Based on the extracted features, the deep neural network model is introduced to predict the remaining useful life of rolling bearing. We evaluate the performance of the proposed method on a real dataset and compare it with several commonly used shallow prediction methods Numerical experiment results show the effectiveness and superiority of the proposed approach.",industry
10.1016/j.engappai.2016.02.016,to_check,Engineering Applications of Artificial Intelligence,scopus,2016-06-01,sciencedirect,Label consistent semi-supervised non-negative matrix factorization for maintenance activities identification,https://api.elsevier.com/content/abstract/scopus_id/84961626004,"Health prognostic is playing an increasingly essential role in product and system management, for which non-negative matrix factorization (NMF) has been an effective method to model the high dimensional recorded data of the device or system. However, the existing unsupervised and supervised NMF models fail to learn from both labeled and unlabeled data together. Therefore, we propose a label consistent semi-supervised non-negative matrix factorization (LCSSNMF) framework that can simultaneously factorize both labeled and unlabeled data, where the discriminability of label data is preserved. Specifically, it firstly incorporates a class-wise coefficient distance regularization term that makes the coefficients for similar samples or samples with the same label close. Moreover, a label reconstruction regularization term is also presented, as the classification error with coefficient matrix of labeled data is expected as low as possible, which will potentially improve the classification accuracy in maintenance activities identification for industrial remote monitoring and diagnostics. The experiment results on real maintenance activities identification application from PHM 2013 data challenge competition demonstrate that LCSSNMF outperforms the state-of-arts NMF methods and results provided by the competition.",industry
10.1016/j.neucom.2014.03.043,to_check,Neurocomputing,scopus,2014-10-22,sciencedirect,Employing box plots to build high-dimensional manufacturing models for new products in TFT-LCD plants,https://api.elsevier.com/content/abstract/scopus_id/84904317401,"Electronics product life cycles are becoming shorter and shorter because of the severe global competition. In such highly competitive industry, it has become an important strategy to accelerate new products launching to the market to earn more shares. However, the lead times of pilot runs are usually long in new product development (NPD) processes, and reducing pilot runs has thus become one of the key tasks of manufacturing systems. Specifically, since the shorter a test period is the smaller sample size one can obtain, making that to find a small data learning method for a manufacturing system being a new challenge. Facing the problem, this work, based on the box plots and the fuzzy techniques, develops an approach to systematically generate synthetic samples to help stabilize the learning process for the used back-propagation neural network (BPN). A real learning task taken from the Array process of a TFT-LCD manufacturer (a new high-resolution product of 4K2K in 2013) is employed as an example to illustrate the details of the proposed method. The task contains nine inputs and 72 output manufacturing attributes, but only with 20 samples. It is quite difficult for most existing modeling algorithms to deal with such a high dimensional situation when the sample size is small. The experiment results show that the proposed approach can effectively improve the robustness and preciseness of a BPN forecasting model. In addition to the reduction of pilot runs, more process knowledge is obtained in the input–output analysis.",industry
10.1016/j.nima.2012.07.046,to_check,"Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",scopus,2012-11-21,sciencedirect,Development of transportable gamma-ray tomographic system for industrial application,https://api.elsevier.com/content/abstract/scopus_id/84865277796,"This paper introduces a gamma-ray tomographic system which is transportable and can be used for on-line systems such as a pipeline operation. In a previous study, a feasibility study on a gamma-ray tomographic system with a scanning geometry of Electron Beam CT was carried out by Monte Carlo simulation. This paper contains a successive work on a previous study by developing and evaluating a real system. To construct a gamma-ray CT, 137Cs was used as a gamma-ray source and radiation measurement system with 72 channel CsI detectors whose crystal is a 12mm×12mm×20mm rectangular parallelepiped was developed to operate jointly with a motion control system. ML-EM algorithm was used for image reconstruction of experimental data. Using the developed transportable gamma-ray system, laboratory and field experiments were carried out successfully. The field experiment results show that a gamma-ray CT with an Electron Beam CT scanning geometry can be a transportable gantry for objects which are parts of processes.",industry
10.1109/ACCESS.2020.3012995,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Robust Vehicle Detection Scheme for Intelligent Traffic Surveillance Systems in Smart Cities,https://ieeexplore.ieee.org/document/9152951/,"Accurately obtaining road vehicle information is important in intelligent traffic surveillance systems for smart cities. Especially smart vehicle detection is recognized as the critical research issue of intelligent traffic surveillance systems. In this paper, a robust real-time vehicle detection method for the system is proposed. The method combines background subtraction model MOG2(Mixture of Gaussians) with a modified SqueezeNet model (H-SqueezeNet). The MOG2 model is utilized to create scale-insensitive Region of Interest (RoIs) from video frames. H-SqueezeNet is then proposed to accurately identify vehicle category. The effectiveness of the method was verified in CDnet2014 dataset, UA-DETRAC dataset and video data from a traffic intersection in Suzhou, China. The experiment results show that the method can achieves excellent detection accuracy in traffic surveillance systems, and achieve an average detection speed of 39.1 FPS.",smart cities
10.1109/ANCS.2019.8901872,to_check,2019 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS),IEEE,2019-09-25 00:00:00,ieeexplore,A Feature-based Video Transmission Framework for Visual IoT in Fog Computing Systems,https://ieeexplore.ieee.org/document/8901872/,"The rapid development of the internet of things (IoT)promotes research in smart city and Fog computing. The vast volume of real-time visual data produced from the tremendous end devices in IoT is a big challenge for the network to transmit and for the data center to store. The typical case is the huge volume of visual data produced by the surveillance cameras in a smart city. In this paper, we consider the problem of how to allocate the calculation ability of the Fog node to handle the surveillance data to obtain low delay meanwhile maintain the video quality. To solve this challenge, we attempt to reduce the tremendous video data using deep learning models in the computational Fog node and optimize the transmission function for high efficiency. To reduce data, we extract video feature and keep salient zones with high resolution meanwhile leave the unavoidable distortion in less important areas. To obtain the least transmission delay under the dynamic bandwidth in Fog computing, we model the transmission delay function and solve it by Lagrangian dual decomposition. We make experiments on public dataset Cityscapes and 4G/LTE Bandwidth Log to evaluate our method. The experiment results show that our feature-based image processing method obtains around 68.7% higher average SSIM (structural similarity index)than the traditional HEVC in the salient zones, and our solution reduces the system delay by 71.02 % comparing with the plain transmission method. It proves our solution reduces the video transmission latency meanwhile keeps the SSIM of salient areas in the video.",smart cities
10.1109/ICICoS48119.2019.8982448,to_check,2019 3rd International Conference on Informatics and Computational Sciences (ICICoS),IEEE,2019-10-30 00:00:00,ieeexplore,Energy Aware Parking Lot Availability Detection Using YOLO on TX2,https://ieeexplore.ieee.org/document/8982448/,"Finding a parking space is a tedious and time-consuming task in a metropolitan city. Due to this problem, many researchers proposed an automatic parking lot occupancy detection system using a camera with a deep learning method to provide useful information in the smart city system. Since object detection for the parking lot is performed in real-time by utilizing CPU and GPUs while parking detection is working 24 hours a day and 365 days a year, therefore power saving is important to reduce the electricity cost. However, the energy-aware is not considered in most related works. In this paper, we proposed an energy-saving algorithm for parking lot availability detection using YOLO running on the TX2 machine. We experiment using small parking lot prototype and remote control cars. In the experiment, we compare our algorithm with the direct application of original YOLO for parking lot detection, the results show that it reduces power by 97 percent when there is no moving object in the parking lot area and 71 percent when there are moving objects in the parking lot area.",smart cities
10.1109/MASS.2018.00035,to_check,2018 IEEE 15th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),IEEE,2018-10-12 00:00:00,ieeexplore,Deep Learning Based Urban Post-Accidental Congestion Prediction,https://ieeexplore.ieee.org/document/8567557/,"Urban roads tend to cause traffic congestion for a long time after the occurrence of traffic accidents, which greatly affects daily transportations. Therefore, the prediction of the duration of traffic jams caused by traffic accidents can allocate traffic resources more reasonably and effectively, release induced traffic information, avoid secondary congestion, and quickly handle traffic accidents. It is of great significance to the rapid rescue of traffic accidents and to eliminate traffic safety hazards. In response to this hot issue, many scholars have done a lot of researches through numerous models, such as probability distribution and time series, and artificial neural networks. However, these models usually only consider temporal features or are based on shallow networks. Therefore, this work adopts a hybrid deep spatial-temporal residual neural network HD-SP-ResNet to predict the traffic volume and velocity, as well as the road congestion duration after traffic accident, so as to monitor and dispatch real-time traffic, response to the postaccidental congestion in time, in order to reduce the various losses incurred by congestion and improve people's satisfaction with traffic on the road. To verify the effectiveness of the proposed model, we conduct extensive experiments based on the taxi trajectory data and road accident data in Shanghai. The experiment results show that the proposed model can achieve a relatively accurate prediction on traffic volume and velocity, as well as the post-accidental congestion duration.",smart cities
10.1109/ICET51757.2021.9451124,to_check,2021 IEEE 4th International Conference on Electronics Technology (ICET),IEEE,2021-05-10 00:00:00,ieeexplore,Global Urban Detection Based on High-Resolution and Multi-temporal Sentinel-l Big Data,https://ieeexplore.ieee.org/document/9451124/,"In this work, a deep learning framework is proposed to achieve global urban detection from the high-resolution and multitemporal Sentinel-1 image on Google earth engine platform. Relative to the massive amount of SAR image collected, the completeness of training dataset hardly can be ensured when the sampling is limited. Therefore, different from other existing works, the building of training dataset is also taken into consideration of designing the deep learning framework, and some dynamic programming and transfer learning strategies are adopted to improve its classification ability in this cloud-based platform. Taking Mumbai, Beijing and Stockholm as example, experiment results on real data illustrate the feasibility of the proposed method.",smart cities
10.1109/IWISA.2009.5073229,to_check,2009 International Workshop on Intelligent Systems and Applications,IEEE,2009-05-24 00:00:00,ieeexplore,Urban Short-Term Traffic Forecasting Based on Grey Neural Network Combined Model: Macao Experience,https://ieeexplore.ieee.org/document/5073229/,"The paper presents three kinds of grey neural network combined model for short-term prediction of urban traffic parameters, which are parallel grey neural network, series grey neural network, and inlaid grey neural network. They are employed to forecast a real vehicle speed in Barbosa road of Macao with satisfied precision. The experiment shows that the above three kinds of mode are feasible and effective in comparison with single model GM(1,1) and neural network. And actual traffic speed varies smoothly or not will influence significantly the accuracy for forecasting.",smart cities
10.1109/ECTIDAMTNCON51128.2021.9425771,to_check,"2021 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunication Engineering",IEEE,2021-03-06 00:00:00,ieeexplore,A Real-Time Bus Arrival Time Prediction System Based on Spark Framework and Machine Learning Approaches: a case study in Chiang Mai,https://ieeexplore.ieee.org/document/9425771/,"An accurate real-time public transport prediction system occupies an important position in urban development. It includes the accurate prediction of the model and the real-time processing of the fitting data. This paper developed a bus arrival time prediction system based on the Spark framework. The process included data collection, data storage (using HDFS), data preprocessing, and modeling (ARIMAX and SVR). Moreover, we have collected data of 78 days of Chiang Mai bus real-time location and location timestamp. We used these data to construct attributes related to bus prediction. The experiment results show that the SVR model's accuracy is as high as 99.5%, which is 25% higher than that of the ARIMAX model. Therefore, the time series prediction system developed based on the Spark framework with the SVR algorithm can quickly and accurately predict bus arrival time.",smart cities
10.1109/WCICA.2006.1714248,to_check,2006 6th World Congress on Intelligent Control and Automation,IEEE,2006-06-23 00:00:00,ieeexplore,Dynamic Traffic Prediction Based on Traffic Flow Mining,https://ieeexplore.ieee.org/document/1714248/,"ITS technology collects a large of historical traffic flow data that may provide information for the support and improvement of traffic control. Data mining technique is appropriate to analysis the large amount of ITS data to acquire useful traffic pattern. We present a dynamic traffic prediction model, the model deals with traffic flow data to convert them into traffic status. In this paper two data mining techniques, the clustering analysis and the classification analysis, are used to develop the model, and the classification model can be used to predict traffic status in real time. The experiment shows the prediction model can be used efficiently in the dynamic traffic prediction for the urban traffic flow guidance",smart cities
10.1109/ITSC.2016.7795689,to_check,2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC),IEEE,2016-11-04 00:00:00,ieeexplore,Forecasting dynamic public transport Origin-Destination matrices with long-Short term Memory recurrent neural networks,https://ieeexplore.ieee.org/document/7795689/,"A considerable number of studies have been undertaken on using smart card data to analyse urban mobility. Most of these studies aim to identify recurrent passenger habits, reveal mobility patterns, reconstruct and predict passenger flows, etc. Forecasting mobility demand is a central problem for public transport authorities and operators alike. It is the first step to efficient allocation and optimisation of available resources. This paper explores an innovative approach to forecasting dynamic Origin-Destination (OD) matrices in a subway network using long Short-term Memory (LSTM) recurrent neural networks. A comparison with traditional approaches, such as calendar methodology or Vector Autoregression is conducted on a real smart card dataset issued from the public transport network of Rennes Métropole, France. The obtained results show that reliable short-term prediction (over a 15 minutes time horizon) of OD pairs can be achieved with the proposed approach. We also experiment with the effect of taking into account additional data about OD matrices of nearby transport systems (buses in this case) on the prediction accuracy.",smart cities
10.1109/ICIAI.2019.8850814,to_check,2019 1st International Conference on Industrial Artificial Intelligence (IAI),IEEE,2019-07-27 00:00:00,ieeexplore,Prediction And Analysis Of Road Traffic Efficiency Based On DBN-SVR,https://ieeexplore.ieee.org/document/8850814/,"With the development of China's economy, the number of private cars has increased significantly, leading to heavy traffic pressure on the road network and a serious decline in traffic efficiency, which has become a shackle hindering urban development. This paper takes traffic data as the research object, establishes a DBN network model with a top-level predictor, and extracts three influencing factors affecting traffic flow, namely lane number, time ratio of red and green signal lights, and whether to turn left, through learning the past traffic data. On this basis, SVR algorithm is combined with the above traffic impact factors to make short-term prediction of the future traffic flow of the road section. Finally, the prediction results are used to optimize the road efficiency to improve the road network efficiency. In this paper, a simulation experiment is carried out on the real road network data of jinan city, and the experimental analysis proves that the method proposed in this paper can effectively improve traffic congestion and improve traffic efficiency. This method has important theoretical significance and practical value for intelligent road construction.",smart cities
10.1109/ICNC.2008.55,to_check,2008 Fourth International Conference on Natural Computation,IEEE,2008-10-20 00:00:00,ieeexplore,Rough Sets and FCM-Based Neuro-fuzzy Inference System for Traffic Incident Detection,https://ieeexplore.ieee.org/document/4667982/,"Detecting incidents on urban freeway or arterials using loop detector data is quite challenging. Considering the ability of fuzzy clustering for data discretization, that of rough sets theory to reduction of decision system, and that of fuzzy neural networks to nonlinear mapping, a novel hybrid neuron-fuzzy inference method that synergies fuzzy c-means (FCM),rough sets theory, and adaptive neuro-fuzzy inference system for incident detection was proposed. Firstly, the continuous attributes from detector loop were discretized with FCM clustering. Then, attribute reduction were performed based on rough sets theory using generic algorithm, and the key conditions for incident were determined. Lastly, according to the chosen attribute data, the ANFIS(Adaptive-Network-Based Fuzzy Inference System) was designed for detection. The major advantage of this approach is to optimize the overall structure of ANFIS and avoid the ""dimensional disaster"" with rough sets theory to attribute reduction. The efficiency of the new method is also illustrated by means of applying to real traffic data, the result of the experiment demonstrated that the solution was very effective to increase the recognition rate and to reduce the number of false detections.",smart cities
10.23919/JSC.2021.0003,to_check,Journal of Social Computing,TUP,2021-03-01 00:00:00,ieeexplore,Estimating Multiple Socioeconomic Attributes via Home Location—A Case Study in China,https://ieeexplore.ieee.org/document/9355035/,"Inferring people's Socioeconomic Attributes (SEAs), including income, occupation, and education level, is an important problem for both social sciences and many networked applications like targeted advertising and personalized recommendation. Previous works mainly focus on estimating SEAs from peoples' cyberspace behaviors and relationships, such as the content of tweets or the social networks between online users. Besides cyberspace data, alternative data sources about users' physical behavior, like their home location, may offer new insights. More specifically, in this paper, we study how to predict a person's income level, family income level, occupation type, and education level from his/her home location. As a case study, we collect people's home locations and socioeconomic attributes through a survey involving 9 provinces and 85 cities in China. We further enrich home location with the knowledge from real estate websites, government statistics websites, online map services, etc. To learn a shared representation from input features as well as attribute-specific representations for different SEAs, we propose H2SEA, a factorization machine-based multi-task learning method with attention mechanism. Extensive experiment results show that: (1) Home location can clearly improve the estimation accuracy for all SEA prediction tasks (e.g., 80.2% improvement in terms of F1-score in estimating personal income level); (2) The proposed H2SEA model outperforms alternative models for SEA inference in terms of various evaluation metrics, such as Area Under Curve (AUC), F-measure, and specificity; (3) The performance of specific SEA prediction tasks (e.g., personal income) can be further improved if H2SEA only focuses on cities or villages due to urban-rural gap in China; (4) Compared with online crawled housing price data, the area-level average income and Points Of Interest (POI) are more important features for SEA inferences in China.",smart cities
10.1109/SDF.2018.8547068,to_check,"2018 Sensor Data Fusion: Trends, Solutions, Applications (SDF)",IEEE,2018-10-11 00:00:00,ieeexplore,A Capsule Network for Traffic Speed Prediction in Complex Road Networks,https://ieeexplore.ieee.org/document/8547068/,"This paper proposes a deep learning approach for traffic flow prediction in complex road networks. Traffic flow data from induction loop sensors are essentially a time series, which is also spatially related to traffic in different road segments. The spatio-temporal traffic data can be converted into an image where the traffic data are expressed in a 3D space with respect to space and time axes. Although convolutional neural networks (CNNs) have been showing surprising performance in understanding images, they have a major drawback. In the max pooling operation, CNNs are losing important information by locally taking the highest activation values. The inter-relationship in traffic data measured by sparsely located sensors in different time intervals should not be neglected in order to obtain accurate predictions. Thus, we propose a neural network with capsules that replaces max pooling by dynamic routing. This is the first approach that employs the capsule network on a time series forecasting problem, to our best knowledge. Moreover, an experiment on real traffic speed data measured in the Santander city of Spain demonstrates the proposed method outperforms the state-of-the-art method based on a CNN by 13.1% in terms of root mean squared error.",smart cities
10.1109/IECON.2019.8927134,to_check,IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society,IEEE,2019-10-17 00:00:00,ieeexplore,Crowd-parking: A New Idea of Parking Guidance Based on Crowdsourcing of Parking Location Information from Automobiles,https://ieeexplore.ieee.org/document/8927134/,"City-wide parking guidance system (CPGS) is the emerging infrastructure of intelligent transportation systems in China. The most difficulty in CPGS development is the lack of parking lots data. Without these data, the traditional well-designed parking guidance algorithms will fail. Meanwhile, the real-time location information of automobiles is increasing and can be accessed easily thanks to the popularity of smartphone and smart vehicular devices. In this paper, we propose a new idea of guiding automobiles to proper parking lots by using the data collected from the vehicular Global Positioning System (GPS) device. Then we build a spatial-temporal classifier based on convolutional neural network (CNN) with long-short term memory (LSTM) to learn the parking experience from the automobiles uploading the parking locations. Finally, we use the classifier to recommend the proper parking lots nearby to vehicles, considering the related driving context. The experiment shows the proposed method can guide automobiles to the target parking lots accurately even if we do not know the real-time data of the target parking lots. Moreover, the classifier shows the obvious spatial characteristics and preference for the first time when guiding vehicles. This method is a kind of crowdsourcing and encourages automobiles to share their parking experience, to help park easier for themselves eventually. So, we think the method, crowd-parking, is very novel and competitive in the current state of endless business and technique negotiation among parking lots.",smart cities
10.1109/BigDataCongress.2016.72,to_check,2016 IEEE International Congress on Big Data (BigData Congress),IEEE,2016-07-02 00:00:00,ieeexplore,"Don't Fire Me, a Kernel Autoregressive Hybrid Model for Optimal Layoff Plan",https://ieeexplore.ieee.org/document/7584978/,"Job cutting occurs when a modern service enterprise reduces the employing labour cost by firing some staffs. Making an appropriate layoff plan is always quite difficult since a bad job cutting has a serious impact on not only the organization but also the business process executing efficiency. Therefore, in this paper, we address the problem of making an optimal layoff plan with the least influence on the executing of the business process. The key challenge is estimating the process throughput under a layoff plan. We overcome this challenge by two steps: regressing the activity throughput by the stuff number and inferring process throughput by the maximum flow or minimum cut algorithm on the Directed Acyclic Graph of process. In the regressing step, a kernel autoregressive hybrid model is proposed, whose MSE is 30% lower than SVM. After that, an augmenting path based algorithm is introduced to make an optimal layoff plan. To evaluate the accuracy of our model, we conduct an external experiment on a real dataset from the workflow system employed in the government of Hangzhou City in China, which results in 9750969 logs from 2050 activities and 16295 employees in two years.",smart cities
10.1109/SASO.2014.22,to_check,2014 IEEE Eighth International Conference on Self-Adaptive and Self-Organizing Systems,IEEE,2014-09-12 00:00:00,ieeexplore,Estimating p-Values for Deviation Detection,https://ieeexplore.ieee.org/document/7001005/,"Deviation detection is important for self-monitoring systems. To perform deviation detection well requires methods that, given only ""normal"" data from a distribution of unknown parametric form, can produce a reliable statistic for rejecting the null hypothesis, i.e. evidence for devating data. One measure of the strength of this evidence based on the data is the p-value, but few deviation detection methods utilize p-value estimation. We compare three methods that can be used to produce p-values: one class support vector machine (OCSVM), conformal anomaly detection (CAD), and a simple ""most central pattern"" (MCP) algorithm. The SVM and the CAD method should be able to handle a distribution of any shape. The methods are evaluated on synthetic data sets to test and illustrate their strengths and weaknesses, and on data from a real life self-monitoring scenario with a city bus fleet in normal traffic. The OCSVM has a Gaussian kernel for the synthetic data and a Hellinger kernel for the empirical data. The MCP method uses the Mahalanobis metric for the synthetic data and the Hellinger metric for the empirical data. The CAD uses the same metrics as the MCP method and has a k-nearest neighbour (kNN) non-conformity measure for both sets. The conclusion is that all three methods give reasonable, and quite similar, results on the real life data set but that they have clear strengths and weaknesses on the synthetic data sets. The MCP algorithm is quick and accurate when the ""normal"" data distribution is unimodal and symmetric (with the chosen metric) but not otherwise. The OCSVM is a bit cumbersome to use to create (quantized) p-values but is accurate and reliable when the data distribution is multimodal and asymmetric. The CAD is also accurate for multimodal and asymmetric distributions. The experiment on the vehicle data illustrate how algorithms like these can be used in a self-monitoring system that uses a fleet of vehicles to conduct deviation detection without supervision and without prior knowledge about what is being monitored.",smart cities
10.1109/TDSC.2019.2934096,to_check,IEEE Transactions on Dependable and Secure Computing,IEEE,2019-12-01 00:00:00,ieeexplore,F-PAD: Private Attribute Disclosure Risk Estimation in Online Social Networks,https://ieeexplore.ieee.org/document/8895669/,"In online social networks, users always expect to share some information for benefits (e.g., personalized services) while hiding the others for privacy. Unfortunately, the hidden information is likely to be predicted by various powerful inference attacks with the rapid advances in machine learning. Then, what is the risk that a user's private information could be disclosed? What countermeasures can be taken to fight against the privacy violation for the user? To tackle these issues, this article proposes a general Framework for Private Attribute Disclosure estimation (F-PAD) including three steps: 1) private attribute prediction; 2) disclosure model training; 3) disclosure risk estimation. Not like most prior risk estimation studies focusing on one specific attack model and private attribute, F-PAD can estimate disclosure risk for individual users in terms of disclosure probability and risk level within a high confidence given a basket of potential inference attack models; furthermore, F-PAD can adapt to various attributes (e.g., gender, age) and offer countermeasures to help users lower the risk. Extensive experiment studies on two real social network datasets, Facebook and Book-Crossing, have verified the effectiveness of F-PAD in `current city', `gender' and `age' disclosure risk estimation.",smart cities
10.1109/ACCESS.2019.2923459,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Group-Wise Itinerary Planning in Temporary Mobile Social Network,https://ieeexplore.ieee.org/document/8737994/,"Temporary mobile social networks has been used at hotels, concerts, theme parks, and sports arenas, where people form a mobile social group for a short time with a common interest or activity. People confined to such specific places or activities are allowed to join the temporary mobile social networks using their main social network accounts (e.g., Foursquare, Facebook). Users registered for the same business/research conference may have common connections and thus may be willing to travel together in the conference city. Traveling with temporal friends can improve the mobile users' experiences as well as help them save money. Currently, renting cars to travel around becomes very general, and one car usually can contain at least four guests. Therefore, traveling with temporal friends can help those guests save their travel cost, such as renting cost and oil cost. To this end, in this paper, we propose a group-wise itinerary planning framework to improve the mobile users' experiences. The experiment results over real data sets illustrate the effectiveness of our proposed framework.",smart cities
10.1109/ASPDAC.2015.7059003,to_check,The 20th Asia and South Pacific Design Automation Conference,IEEE,2015-01-22 00:00:00,ieeexplore,An accurate and low-cost PM<inf>2.5</inf> estimation method based on Artificial Neural Network,https://ieeexplore.ieee.org/document/7059003/,"PM<sub>2.5</sub> has already been a major pollutant in many cities in China. It is a kind of harmful pollutant which may cause several kinds of lung diseases. However, the existing methods to monitor PM<sub>2.5</sub> with high accuracy are too expensive to popularize. The high cost also limits the further researches about PM<sub>2.5</sub>. This paper implements a method to estimate PM<sub>2.5</sub> with low cost and high accuracy by Artificial Neural Network (ANN) technique using other pollutants and meteorological factors that are easy to be monitored. An Entropy Maximization step is proposed to avoid the over-fitting related to the data distribution of pollutant data. Also, how to choose the input attributes is abstracted to an optimization problem. An iterative greedy algorithm is proposed to solve it, which reduces the cost and increases the estimation accuracy at the same time. The experiment shows that the linear correlation coefficient between the estimated value and real value is 0.9488. Our model can also classify PM<sub>2.5</sub> levels with a high accuracy. Additionally, the trade-off between accuracy and cost is investigated according to the price and error rate of each sensor.",smart cities
10.1109/EMBC.2012.6346320,to_check,2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IEEE,2012-09-01 00:00:00,ieeexplore,Development of a Bayesian neural network to perform obstacle avoidance for an intelligent wheelchair,https://ieeexplore.ieee.org/document/6346320/,"This paper presents an extension of a real-time obstacle avoidance algorithm for our laser-based intelligent wheelchair, to provide independent mobility for people with physical, cognitive, and/or perceptual impairments. The laser range finder URG-04LX mounted on the front of the wheelchair collects immediate environment information, and then the raw laser data are directly used to control the wheelchair in real-time without any modification. The central control role is an obstacle avoidance algorithm which is a neural network trained under supervision of Bayesian framework, to optimize its structure and weight values. The experiment results demonstrated that this new approach provides safety, smoothness for autonomous tasks and significantly improves the performance of the system in difficult tasks such as door passing.",smart cities
10.1109/EMBC.2013.6610332,to_check,2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),IEEE,2013-07-07 00:00:00,ieeexplore,The advancement of an obstacle avoidance bayesian neural network for an intelligent wheelchair,https://ieeexplore.ieee.org/document/6610332/,"In this paper, an advanced obstacle avoidance system is developed for an intelligent wheelchair designed to support people with mobility impairments who also have visual, upper limb, or cognitive impairment. To avoid obstacles, immediate environment information is continuously updated with range data sampled by an on-board laser range finder URG-04LX. Then, the data is transformed to find the relevant information to the navigating process before being presented to a trained obstacle avoidance neural network which is optimized under the supervision of a Bayesian framework to find its structure and weight values. The experiment results showed that this method allows the wheelchair to avoid collisions while simultaneously navigating through an unknown environment in real-time. More importantly, this new approach significantly enhances the performance of the system to pass narrow openings such as door passing.",smart cities
10.1109/DASC.2018.8569796,to_check,2018 IEEE/AIAA 37th Digital Avionics Systems Conference (DASC),IEEE,2018-09-27 00:00:00,ieeexplore,A Model of Fuel Consumption Estimation and Abnormality Detection based on Airplane Flight Data Analysis,https://ieeexplore.ieee.org/document/8569796/,"Due to the importance of fuel consumption to cost reduction of air transport, the major airlines in the world pay close attention to research on fuel saving. How to improve the ability of fuel consumption estimation and abnormality detection of fuel system becomes an important topic of airlines. To enhance the ability of fuel consumption estimation and abnormality detection, this paper analyzes the flight data of fuel system and establishes a model of fuel consumption estimation and abnormality detection, which can upgrade the precision of flight plan's fuel estimation and abnormality detection of fuel system to reduce the fuel consumption cost and find faults or faults once them occur in the fuel system. The fuel estimation and abnormality detection model is closely related to airplane performance, engine performance, flight track and meteorology. Most traditional fuel consumption models are based on energy balance principle, which need to inquire performance graphs. However, it is difficult to obtain the performance data from performance graphs. To deal with this problem, this paper builds an airplane fuel estimation model by training a BP neural network based on the airplane flight data. What's more, using the abnormal data when faults or faults occur in the fuel system, the model can detect the abnormal condition of fuel system and warn the pilots. Because it is not enough accurate to estimate fuel consumption of the entire flight course once, based on flight data, the flight course is divided automatically into five routes by the model in this paper, which is takeoff, climb, cruise and decline. Models of different routes are trained in different BP neural networks to estimate the fuel consumption of each route. Adding all the fuel consumption of different routes, the fuel consumption of entire flight course will be calculated. Considering the factors of meteorology, the speed and acceleration of airplane, including (but not limited to), wind direction, wind speed, tilt, longitudinal acceleration and transverse acceleration, and so on, are taken as inputs when the model is established. These factors make the fuel consumption estimation much closer to the real flight fuel consumption data and abnormal condition be easily detected. The experiment indicates that the model of airplane fuel consumption estimation and abnormality detection which is proposed performs with high precision.",smart cities
10.1109/EURCON.2007.4400280,to_check,"EUROCON 2007 - The International Conference on ""Computer as a Tool""",IEEE,2007-09-12 00:00:00,ieeexplore,Modelling of Intelligent Agents for Energy Distribution Control,https://ieeexplore.ieee.org/document/4400280/,"The purpose of the paper is to present the model of intelligent control of power network, for mechatronics systems such as public electric transport. The paper presents the model and the Menger's algorithm for optimization of servicing time for electric substations. Algorithm is based on artificial intelligence and allows to solve both static and dynamic optimization problems. An intelligent multi-agent system is used for task solution. Elements of bond graphing are used to describe the intelligent control of power networks. The algorithm is workable as regular maintenance as for emergencies. The workability of the algorithm is checked by an abstract numerical example. The model and the algorithm is realized in computer experiment for intelligent agent system. As an object, Riga electric network is selected. All data for computer experiment coincide with the real object data. As an example of electrical energy consumers Riga tram system is selected. Experiment show the possibility of energy saving by using of intelligent agent-based control and coordination for mechatronics system of Riga trams.",smart cities
10.1109/ICIBA50161.2020.9277383,to_check,"2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)",IEEE,2020-11-08 00:00:00,ieeexplore,Study on Vibration Monitoring of Railway Seamless Line,https://ieeexplore.ieee.org/document/9277383/,"With the continuous expansion of the scale of high-speed rail network in China, in order to improve the speed, train load and safety performance, seamless rail technology is widely used in Chinese lines. And seamless rail is to weld 25 meters long rail into several hundred meters long or even thousands of meters long. Because highspeed rail cannot freely expand and contract, the rail is affected by temperature stress, vibration stress, at the same time high speed starting and braking of high speed railway, Large volume of transport is easy to make the rail fatigue and fracture. In order to ensure the safety of people's life and property, in this study, a new real-time monitoring method for rail condition is proposed., using the rail vibration signal analysis of rail damage, through acceleration sensor to collect the rail. vibration signal, then reduce the interference through kalman filter, get the rail close to the real vibration amplitude. under laboratory conditions, build vibration signal test system, use bench drill hammer rail model, use MPU6050 acceleration sensor to collect vibration signal of rail model, pass STM32F103CBT6 to computer keil program, and import the result into MATLAB for filtering and amplitude analysis. The experiment shows that the acceleration sensor is effective in monitoring the amplitude of rail.",smart cities
10.1109/WCICA.2008.4594366,to_check,2008 7th World Congress on Intelligent Control and Automation,IEEE,2008-06-27 00:00:00,ieeexplore,Working points on-line intelligent optimization strategy for dredging slurry pipeline transportation,https://ieeexplore.ieee.org/document/4594366/,"Based on the analysis of the structure and boundary conditions of a dredge slurry pipeline transport system, a novel working points on-line intelligent optimization strategy was introduced, which uses the pipeline systempsilas specific energy consumption as its optimization goal. It can solve not only the problem of high energy consumption and low efficiency of the optimization methods which use output maximum as their goals, but also the problem that offline static optimization methods canpsilat be applied to the slurry pipeline transport systempsilas working points on-line continuous optimization. Based on the real time process data analysis, this method uses the intelligent technology, such as fuzzy decision and data fusion, to optimize the operating pointspsila parameters. This method doesnpsilat rely on the precise description of the characteristics of slurry pipeline transport system and it can be easily applied to the dredging operation, during which the soil and systempsilas characteristics vary from time to time. Field experiments were carried out to verify the novel optimization strategy. Experiment results show that this method can not only improve the dredging output, but also minimize the fluctuation of slurry concentration and velocity, thus the slurry pipeline transport systempsilas specific energy consumption is lower and its efficiency is higher.",smart cities
10.23919/CCC52363.2021.9549607,to_check,2021 40th Chinese Control Conference (CCC),IEEE,2021-07-28 00:00:00,ieeexplore,Design of license plate recognition platform in micro traffic environment,https://ieeexplore.ieee.org/document/9549607/,"In order to meet the experimental and teaching needs, a license plate recognition platform is designed by using deep learning network and micro traffic environment. Firstly, the platform captures the car which cross the boundary through the video monitoring system. Then, the captured images are transmitted to the server through the wireless communication network. And the server will call the faster R-CNN to detect the license plate, transmitting it to the LPRNet for recognition. Finally, the Tkinter Library in Python is used to display the recognition results in real-time. The experiment shows that the platform can play an important role in the research of intelligent transportation system, and has some importance in engineering application.",smart cities
http://arxiv.org/abs/1901.02126v1,to_check,arxiv,arxiv,2019-01-08 02:04:12+00:00,arxiv,"Smart-Edge-CoCaCo: AI-Enabled Smart Edge with Joint Computation,
  Caching, and Communication in Heterogeneous IoT",http://arxiv.org/abs/1901.02126v1,"The development of mobile communication technology, hardware, distributed
computing, and artificial intelligence (AI) technology has promoted the
application of edge computing in the field of heterogeneous Internet of Things
(IoT). In order to overcome the defects of the traditional cloud computing
model in the era of big data. In this article, we first propose a new AIenabled
smart edge with heterogeneous IoT architecture which combines edge computing,
caching, and communication. Then, we propose the Smart-Edge-CoCaCo algorithm.
To minimize total delay and confirm the computation offloading decision,
Smart-Edge-CoCaCo uses joint optimization of the wireless communication model,
the collaborative filter caching model in edge cloud, and the computation
offloading model. Finally, we built an emotion interaction testbed to perform
computational delay experiments in real environments. The experiment results
showed that the computation delay of the Smart-Edge-CoCaCo algorithm is lower
than that of traditional cloud computing model with the increase of computing
task data and the number of concurrent users.",smart cities
http://arxiv.org/abs/1807.10603v2,to_check,arxiv,arxiv,2018-07-23 10:40:22+00:00,arxiv,A Capsule Network for Traffic Speed Prediction in Complex Road Networks,http://arxiv.org/abs/1807.10603v2,"This paper proposes a deep learning approach for traffic flow prediction in
complex road networks. Traffic flow data from induction loop sensors are
essentially a time series, which is also spatially related to traffic in
different road segments. The spatio-temporal traffic data can be converted into
an image where the traffic data are expressed in a 3D space with respect to
space and time axes. Although convolutional neural networks (CNNs) have been
showing surprising performance in understanding images, they have a major
drawback. In the max pooling operation, CNNs are losing important information
by locally taking the highest activation values. The inter-relationship in
traffic data measured by sparsely located sensors in different time intervals
should not be neglected in order to obtain accurate predictions. Thus, we
propose a neural network with capsules that replaces max pooling by dynamic
routing. This is the first approach that employs the capsule network on a time
series forecasting problem, to our best knowledge. Moreover, an experiment on
real traffic speed data measured in the Santander city of Spain demonstrates
the proposed method outperforms the state-of-the-art method based on a CNN by
13.1% in terms of root mean squared error.",smart cities
http://arxiv.org/abs/1911.02673v2,to_check,arxiv,arxiv,2019-11-06 23:14:53+00:00,arxiv,"Towards the Use of Neural Networks for Influenza Prediction at Multiple
  Spatial Resolutions",http://arxiv.org/abs/1911.02673v2,"We introduce the use of a Gated Recurrent Unit (GRU) for influenza prediction
at the state- and city-level in the US, and experiment with the inclusion of
real-time flu-related Internet search data. We find that a GRU has lower
prediction error than current state-of-the-art methods for data-driven
influenza prediction at time horizons of over two weeks. In contrast with other
machine learning approaches, the inclusion of real-time Internet search data
does not improve GRU predictions.",smart cities
http://arxiv.org/abs/1802.04931v1,to_check,arxiv,arxiv,2018-02-14 02:19:13+00:00,arxiv,Energy Spatio-Temporal Pattern Prediction for Electric Vehicle Networks,http://arxiv.org/abs/1802.04931v1,"Information about the spatio-temporal pattern of electricity energy carried
by EVs, instead of EVs themselves, is crucial for EVs to establish more
effective and intelligent interactions with the smart grid. In this paper, we
propose a framework for predicting the amount of the electricity energy stored
by a large number of EVs aggregated within different city-scale regions, based
on spatio-temporal pattern of the electricity energy. The spatial pattern is
modeled via using a neural network based spatial predictor, while the temporal
pattern is captured via using a linear-chain conditional random field (CRF)
based temporal predictor. Two predictors are fed with spatial and temporal
features respectively, which are extracted based on real trajectories data
recorded in Beijing. Furthermore, we combine both predictors to build the
spatio-temporal predictor, by using an optimal combination coefficient which
minimizes the normalized mean square error (NMSE) of the predictions. The
prediction performance is evaluated based on extensive experiments covering
both spatial and temporal predictions, and the improvement achieved by the
combined spatio-temporal predictor. The experiment results show that the NMSE
of the spatio-temporal predictor is maintained below 0.1 for all investigate
regions of Beijing. We further visualize the prediction and discuss the
potential benefits can be brought to smart grid scheduling and EV charging by
utilizing the proposed framework.",smart cities
http://arxiv.org/abs/1909.09931v2,to_check,arxiv,arxiv,2019-09-22 02:56:09+00:00,arxiv,"Volume Preserving Image Segmentation with Entropic Regularization
  Optimal Transport and Its Applications in Deep Learning",http://arxiv.org/abs/1909.09931v2,"Image segmentation with a volume constraint is an important prior for many
real applications. In this work, we present a novel volume preserving image
segmentation algorithm, which is based on the framework of entropic regularized
optimal transport theory. The classical Total Variation (TV) regularizer and
volume preserving are integrated into a regularized optimal transport model,
and the volume and classification constraints can be regarded as two measures
preserving constraints in the optimal transport problem. By studying the dual
problem, we develop a simple and efficient dual algorithm for our model.
Moreover, to be different from many variational based image segmentation
algorithms, the proposed algorithm can be directly unrolled to a new Volume
Preserving and TV regularized softmax (VPTV-softmax) layer for semantic
segmentation in the popular Deep Convolution Neural Network (DCNN). The
experiment results show that our proposed model is very competitive and can
improve the performance of many semantic segmentation nets such as the popular
U-net.",smart cities
http://arxiv.org/abs/1802.02888v1,to_check,arxiv,arxiv,2018-02-05 05:41:11+00:00,arxiv,"Spatial mapping and analysis of aerosols during a forest fire using
  computational mobile microscopy",http://arxiv.org/abs/1802.02888v1,"Forest fires are a major source of particulate matter (PM) air pollution on a
global scale. The composition and impact of PM are typically studied using only
laboratory instruments and extrapolated to real fire events owing to a lack of
analytical techniques suitable for field-settings. To address this and similar
field test challenges, we developed a mobile-microscopy and
machine-learning-based air quality monitoring platform called c-Air, which can
perform air sampling and microscopic analysis of aerosols in an integrated
portable device. We tested its performance for PM sizing and morphological
analysis during a recent forest fire event in La Tuna Canyon Park by spatially
mapping the PM. The result shows that with decreasing distance to the fire
site, the PM concentration increases dramatically, especially for particles
smaller than 2 microns. Image analysis from the c-Air portable device also
shows that the increased PM is comparatively strongly absorbing and asymmetric,
with an aspect ratio of 0.5-0.7. These PM features indicate that a major
portion of the PM may be open-flame-combustion-generated element carbon
soot-type particles. This initial small-scale experiment shows that c-Air has
some potential for forest fire monitoring.",smart cities
10.1016/j.comcom.2021.09.005,to_check,Computer Communications,scopus,2021-12-01,sciencedirect,Improving performance and data transmission security in VANETs,https://api.elsevier.com/content/abstract/scopus_id/85115794144,"This article proposes a new approach to achieve fast and reliable transfer of data and uses machine learning techniques for data processing to improve the performance and data transmission security of the vehicular network. The proposed approach is the combination of 5G cellular network and alternative data transmission channels. The data collection experiment took place within different areas of the city of Berlin over a 3-month time period and involved the use of 5G technologies. The study carried out the analysis and classification of big data with the help of position-based routing protocols and the Support Vector Machine algorithms. The said techniques were employed to detect non-line-of-sight (NLoS) conditions in real time, which ensure the secure transmission of data without the loss or degradation of network performance. The novelty of the work is that it tackles various traffic scenarios (the extent of road congestion can affect the quality of big data transmission) and offers a way to improve big data transfer using the Support Vector Machine technology. The study results show that the proposed approach is effective enough with big data and can be employed to improve the performance of urban VANET networks and data transmission security. The study results can be useful in developing high-performance 5G-VANET applications to improve traffic safety in urban vehicular environments.",smart cities
10.1016/j.compeleceng.2021.107004,to_check,Computers and Electrical Engineering,scopus,2021-03-01,sciencedirect,Vehicle logo detection based on deep convolutional networks,https://api.elsevier.com/content/abstract/scopus_id/85100057639,"For intelligent transportation systems, vehicle logo is an important part in the research of vehicle information recognition. Deep convolutional neural networks (CNNs) have been more reasonable and stronger than artificial selection in feature extraction and expression for targets. In our previous work, we found that the network of feature extraction and the training policy of detection have great effects on the accuracy of vehicle logo. For improving small-scale object detecting precision, we change the training policy. According to our previous work, we propose a lightweight network structure with separable convolution to improve the real-time character for vehicle logo while implement the method in embedded devices. The experiment proves that our model can effectively improve the detection accuracy of vehicle logo. Our training policy is valid method for small-scale objects. The lightweight network can solve the equilibrium problem of detecting precision and speed.",smart cities
10.1016/j.jvcir.2020.102845,to_check,Journal of Visual Communication and Image Representation,scopus,2020-08-01,sciencedirect,Volume preserving image segmentation with entropy regularized optimal transport and its applications in deep learning,https://api.elsevier.com/content/abstract/scopus_id/85088041387,"Image segmentation with a volume constraint is an important prior for many real applications. In this work, we present a novel volume preserving image segmentation algorithm, which is based on the entropy and Total Variation (TV) regularized optimal transport theory. The volume and classification constraints can be regarded as two measures preserving constraints in the optimal transport. By studying the dual problem, we develop a simple but efficient dual algorithm for our model. Moreover, to be different from many variational based image segmentation algorithms, the proposed algorithm can be directly unrolled to a new Volume Preserving and TV regularized softmax (VPTV-softmax) layer for semantic segmentation in the popular Deep Convolution Neural Network (DCNN). The experiment results show that our proposed model is very competitive and can improve the performance of many semantic segmentation networks such as the popular U-net and DeepLabv3+.",smart cities
10.1016/j.scs.2019.101615,to_check,Sustainable Cities and Society,scopus,2019-10-01,sciencedirect,"Thai sentiment analysis with deep learning techniques: A comparative study based on word embedding, POS-tag, and sentic features",https://api.elsevier.com/content/abstract/scopus_id/85067187685,"A smart city connects physical, information technology, social, and business infrastructures together to leverage their collective intelligence. Feedback drives improvements in service, city development, and quality of life in the city. Therefore, sentiment analysis in real-time of opinions expressed in text form by residents in the city is absolutely necessary. Nowadays, machine learning is widely applied to sentiment analysis of decisions in business, especially deep learning. In this experiment, we evaluated and compared the performances of several conventional deep learning models: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Bidirectional LSTM (Bi-LSTM), in sentiment analysis of Thai children tales. In several previous studies, many features have been used in all of the models mentioned, features such as word embedding that helps a model to understand the semantics of each word, POS-tag that helps a model to understand the grammatical function of words, and sentic that helps a model to understand the emotion of words. Some combinations of these features have also been used. The results of this experiment show that the CNN model that used all three features gave the best result of 0.817 F1-score at p < 0.01, which was significantly better than all other models.",smart cities
10.1016/j.phymed.2019.152967,to_check,Phytomedicine,scopus,2019-09-01,sciencedirect,Mechanism-based pharmacokinetics-pharmacodynamics studies of harmine and harmaline on neurotransmitters regulatory effects in healthy rats: Challenge on monoamine oxidase and acetylcholinesterase inhibition,https://api.elsevier.com/content/abstract/scopus_id/85066284236,"Background
                  β-Carboline alkaloid harmine (HAR) and harmaline (HAL) are monoamine oxidase (MAO) and acetylcholinesterase (AChE) inhibitors. However, whether HAR and HAL inhibit MAO or AChE selectively and competitively is unclear.
               
                  Purpose
                  The purpose of this study was to investigate the potential competition inhibition of HAR and HAL on MAO and AChE in brain endothelial cells (RBE4) and in healthy rats to provide a basis for the application of the inhibitors in the treatment of patients with depression and with Parkinson's disease or Alzheimer's disease.
               
                  Study design/methods
                  The transport properties of HAR and HAL by using blood-brain barrier models constructed with RBE4 were systematically investigated. Then, the modulation effects of HAR and HAL on CNS neurotransmitters (NTs) in healthy rat brains were determined by a microdialysis method coupled with LC-MS/MS. The competition inhibition of HAR and HAL on MAO and AChE was evaluated through real time-PCR, Western blot analysis, and molecular docking experiments.
               
                  Results
                  Results showed that HAL and HAR can be detected in the blood and striatum 300 min after intravenous injection (1 mg/kg). Choline (Ch), gamma-aminobutyric acid (GABA), glutamate (Glu), and phenylalanine (Phe) levels in the striatum decreased in a time-dependent manner after the HAL treatment, with average velocities of 1.41, 0.73, 3.86, and 1.10 (ng/ml)/min, respectively. The Ch and GABA levels in the striatum decreased after the HAR treatment, with average velocities of 1.16 and 0.22 ng/ml/min, respectively. The results of the cocktail experiment using the human liver enzyme indicated that the IC50 value of HAL on MAO-A was 0.10 ± 0.08 µm and that of HAR was 0.38 ± 0.21 µm. Their IC50 values on AChE were not obtained. These findings indicated that HAL and HAR selectively acted on MAO in vitro. However, RT-PCR and Western blot analysis results showed that the AChE mRNA and protein expression decreased in a time-dependent manner in RBE4 cells after the HAR and HAL treatments.
               
                  Conclusion
                  NT analysis results showed that HAL and HAR selectively affect AChE in vivo. HAL and HAR may be highly and suitably developed for the treatment of Alzheimer's disease.",smart cities
10.1016/j.jcis.2014.07.024,to_check,Journal of Colloid and Interface Science,scopus,2014-11-15,sciencedirect,Responsive delivery of drug cocktail via mesoporous silica nanolamps,https://api.elsevier.com/content/abstract/scopus_id/84906518894,"After a substantial advancement in single drug nanocarrier, nanomedicine now demands an integration of nanotechnology with combination therapy to achieve synergistic therapeutic effects. In this respect, a smart and multiple drug shuttling nanotheranostic system is developed which transport diverse kinds of anticancer drugs to cancer cells in a controlled and responsive manner respectively. Synthetically, a significantly high dose of hydrophobic camptothecin (CPT) is first loaded into the porous structure of quantum dots (CdS) coupled mesoporous silica nanocomposite. Subsequently, fluorescent doxorubicin (DOX) molecules are exclusively anchored onto the surface of CdS; as a result, the fluorescence of both CdS and DOX is quenched. Upon exposing to mildly acidic conditions, the fluorescence of both species is recovered, such fluorescent “on–off” states provides an added opportunity to real time sense drug release. In-vitro cell experiment reveals an excellent anticancer efficacy of drug cocktail, merely 3μg/ml concentration of multiple drugs loaded nanocarrier reduces the cell viability to 30%. Furthermore, confocal imaging indicates a successful release of both therapeutic entities. We visualize that our newly fabricated multifunctional double drug-carrying nanoparticles can be a valuable addition to next generation of materials that simultaneously deliver cocktail of drugs with imaging functionality.",smart cities
10.1016/j.radmeas.2009.01.002,to_check,Radiation Measurements,scopus,2009-02-01,sciencedirect,"Response of Radon in a seismic calibration explosion, Israel",https://api.elsevier.com/content/abstract/scopus_id/67349101260,"Radon measurements were performed at shallow levels during an in-land 20-ton seismic calibration explosion experiment, simulating a 2.6-M
                     L earthquake, to investigate the influence of the explosive blast and the transitory seismic wave fields on the Radon transport in the country rock, adjacent to the focus of the explosion. The experiment was conducted in a basalt quarry in the northern margin of the Beit Shean valley (Israel). Five gamma-ray sensors were placed, at a depth of about 2m, along a line located 17–150m from the edge of the explosion zone. Measurements commenced 4 days before and continued for 9 days after the explosion with 15min integrations. A 10-s sampling was used in the interval of several hours before and after the explosion itself.
                  Diurnal variations of Radon, reflecting the typical variation pattern of Radon in the shallow environment, were registered before and after the explosion. No significant change in the overall Radon concentration was observed as a consequence of the main explosion as well as three smaller experimental shots (0.5–2tons) in the 2h prior to the calibration blast. The seismological data indicate that the transient excess pressure at the farthest Radon sensor was above 5barm−1 during 0.2–0.4s, and evidently much higher at the nearest sensors, but none of the sensors responded by recording any exceptional change in the Radon concentration. Moreover the hypothesis that additional Radon may emanate from solid grains as a result of the excess local pressure exerted by the blast is also not observed.
                  In contrast to a real earthquake event an explosion experiment has neither eventual preceding nor following geodynamic activity. Therefore the absence of significant Radon anomalies during or after the blast does not contradict assumptions, observations or conclusions as the occurrence of Radon anomalies prior or after an earthquake event due to associated long-term geodynamic processes.",smart cities
10.1016/j.mvr.2004.06.007,to_check,Microvascular Research,scopus,2004-01-01,sciencedirect,VEGF increases paracellular transport without altering the solvent-drag reflection coefficient,https://api.elsevier.com/content/abstract/scopus_id/14244265356,"Vascular endothelial growth factor (VEGF) increases microvascular permeability and has been implicated in the development of numerous pathologies including diabetic retinopathy (DR), hypoxia/ischemia, and tumor biology. The transport pathways by which water and solutes cross the endothelium in response to VEGF, however, are not completely understood. We measured, in real time, bovine retinal endothelial cell (BREC) hydraulic conductivity (Lp), 70 kDa dextran permeability (Pe), and the solvent-drag reflection coefficient (σ) before and after addition of 50 ng/ml VEGF. The diffusional permeability coefficient for dextran (Pd) was measured before pressure gradient application. The sudden application of a 10-cm H2O hydrostatic pressure gradient induced water and solute fluxes that decayed to steady-state values after approximately 2 h. Subsequently, the addition of VEGF significantly increased Lp and Pe by 4.3-fold ± 0.7-fold and 3.0-fold ± 0.3-fold, respectively, after 110 min; however, the reflection coefficient remained approximately constant throughout the experiment (approximately 0.8).
                  These observations suggest that water and dextran utilize common paracellular channels across BREC monolayers. Furthermore, the addition of VEGF increases the number or availability of channels but does not alter the selectivity of the monolayer to 70 kDa dextran.",smart cities
10.1109/ICMLA.2013.79,to_check,2013 12th International Conference on Machine Learning and Applications,IEEE,2013-12-07 00:00:00,ieeexplore,Evolutionary Content Pre-fetching in Mobile Networks,https://ieeexplore.ieee.org/document/6784649/,"Recently, an increasing number of smart phone users are eagerly using the cellular network in extensive data applications. In particular, multimedia downloads generated by Internet-capable smart phones and other portable devices (such as iPad) have been widely recognized as the major source for strains in cellular networks, to a degree where service quality for all users is significantly impacted. Lately, patterns in both the content consumption as well as the Wi-Fi access by the users were alleged to be available. In this paper we introduce a technique to schedule the content for prefetching based on mobile usage patterns. This technique utilizes both a content profile as well as a bandwidth profile to schedule content for prefetching. Users can then use the cached version of the content in order to achieve a better user experience and reduce the peak-to-average ratio in mobile networks, especially during peak hours of the day. An experiment using real users traces was conducted and the results after applying the proposed evolutionary scheduling algorithm show that up to 70 percent of the user content requests can be fulfilled i.e. the content was successfully cached before request.",multimedia
10.1109/MED.2019.8798521,to_check,2019 27th Mediterranean Conference on Control and Automation (MED),IEEE,2019-07-04 00:00:00,ieeexplore,Dynamic Predictive Modeling Approach of User Behavior in Virtual Reality based Application,https://ieeexplore.ieee.org/document/8798521/,"Virtual Reality (VR) is considered to be a powerful modern medium for immersive data visualization and exploration. However, few studies have proposed solutions to complement data visualization in immersive environment considering the user's behavior. This paper addresses dynamic modeling of user behavior approach in an interactive VR based application. In this application, real-time data communication is employed to track accurate location and orientation of head mounted display device worn by the user. In our experiment, we use example of collected data and provide a methodology to predict next movements of the user by using nonlinear autoregressive (NAR) and location in the application by the nonlinear autoregressive neural network with exogenous inputs (NARX). Results suggest both neural networks are suitable for performing prediction which can be used to achieve an improved feeling of presence while reducing required high computational power. Data analysis part of the research is also linked to human behaviors to improve studies which are usually performed by traditional survey techniques.",multimedia
10.1109/QoMEX48832.2020.9123085,to_check,2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX),IEEE,2020-05-28 00:00:00,ieeexplore,Influence of Hand Tracking as a Way of Interaction in Virtual Reality on User Experience,https://ieeexplore.ieee.org/document/9123085/,"With the rising interest in Virtual Reality and the fast development and improvement of available devices, new features of interactions are becoming available. One of them that is becoming very popular is hand tracking, as the idea to replace controllers for interactions in virtual worlds. This experiment aims to compare different interaction types in VR using either controllers or hand tracking. Participants had to play two simple VR games with various types of tasks in those games - grabbing objects or typing numbers. While playing, they were using interactions with different visualizations of hands and controllers. The focus of this study was to investigate user experience of varying interactions (controller vs. hand tracking) for those two simple tasks. Results show that different interaction types statistically significantly influence reported emotions with Self-Assessment Manikin (SAM), where for hand tracking participants were feeling higher valence, but lower arousal and dominance. Additionally, task type of grabbing was reported to be more realistic, and participants experienced a higher presence. Surprisingly, participants rated the interaction type with controllers where both where hands and controllers were visualized as statistically most preferred. Finally, hand tracking for both tasks was rated with the System Usability Scale (SUS) scale, and hand tracking for the task typing was rated as statistically significantly more usable. These results can drive further research and, in the long term, contribute to help selecting the most matching interaction modality for a task.",multimedia
10.1109/ICIEA.2009.5138778,to_check,2009 4th IEEE Conference on Industrial Electronics and Applications,IEEE,2009-05-27 00:00:00,ieeexplore,Application of an adaptive control algorithm with neural network vibration compensation in a 3D laser scanning system,https://ieeexplore.ieee.org/document/5138778/,"Based on generalized least variance and optimal prediction theory, a novel adaptive vibration control algorithm with neural network compensation is developed to restrain periodical micro-vibration in three dimension (3D) scanning system. In this way, the controller can deal with nonlinear plants, which exhibit features such as uncertainties, nonminimum phase behaviour, coupling effects and delete unmodelled dynamics etc. To demonstrate the effectiveness of the developed algorithm, a 3D scanning system designed by us is taken as a real plant, and its vibration model is established, which is consist of linear ARMA model and nonlinear unmodelled dynamic, then, based on the vibration model and proposed algorithm, the simulation experiment was done, the successful results show that the vibration can be compensated effectively so that the 3D scanning precision is assured.",multimedia
10.1109/REM49740.2020.9313886,to_check,2020 21st International Conference on Research and Education in Mechatronics (REM),IEEE,2020-12-11 00:00:00,ieeexplore,Expandable YOLO: 3D Object Detection from RGB-D Images,https://ieeexplore.ieee.org/document/9313886/,"This paper aims at constructing a light-weight object detector that inputs a depth and a color image from a stereo camera. Specifically, by extending the network architecture of YOLOv3 to 3D in the middle, it is possible to output in the depth direction. In addition, Intersection over Union (IoU) in 3D space is introduced to confirm the accuracy of region extraction results. In the field of deep learning, object detectors that use distance information as input are actively studied for utilizing automated driving. However, the conventional detector has a large network structure, and the real-time property is impaired. The effectiveness of the detector constructed as described above is verified using datasets. The experiment verified that the proposed model is able to output 3D bounding boxes and detect people whose body is partly hidden. Further, the processing speed of the model reached 44.35 fps.",multimedia
10.1109/ISBI.2016.7493536,to_check,2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),IEEE,2016-04-16 00:00:00,ieeexplore,Real-time 2D/3D registration via CNN regression,https://ieeexplore.ieee.org/document/7493536/,"In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.",multimedia
10.1109/EIIS.2017.8298584,to_check,2017 First International Conference on Electronics Instrumentation & Information Systems (EIIS),IEEE,2017-06-05 00:00:00,ieeexplore,Real-time 3D object detection in unstructured environments,https://ieeexplore.ieee.org/document/8298584/,"Real-time specific 3D object detection plays an important role in intelligent service robot or intelligent surveillance fields. A novel discriminative learning based method is proposed to detect a specific 3D object in unstructured environments with a single RGB image. This method contains two stages: the candidate extraction stage and the recognition stage. On the candidate extraction stage, we use a simple, fast, and high quality objectness measure Binarized Normed Gradients (BING) to highlight the target candidate regions. On the recognition stage, each candidate region is verified by the designed cascade classifiers in order to be classified into different classes based on multi features including color, shape and texture contents. Our method is evaluated by its performance on our proposed challenging new dataset consisting of 3 objects and is compared in two public challenging dataset with other approaches for single RGB image based 3D object detection. The experiment results show that the proposed method can not only achieve a high precision (97%) at 90% recall, but also can meet the real-time processing requirement of about 22 fps on video sequences.",multimedia
10.1109/ICCSN.2016.7586606,to_check,2016 8th IEEE International Conference on Communication Software and Networks (ICCSN),IEEE,2016-06-06 00:00:00,ieeexplore,Research of 3D face recognition algorithm based on deep learning stacked denoising autoencoder theory,https://ieeexplore.ieee.org/document/7586606/,"This electronic Due to the fact that the 3D face depth data have more information, the 3D face recognition is attracting more and more attention in the machine learning area. Firstly, this paper selects 30 feature points from the 113 feature points of Candide-3 face model to characterize face, which improves the efficiency of recognition algorithm obviously without affecting the recognition accuracy. With the significant advantage of the characterization of essential features by learning a deep nonlinear network, this paper presents a stacked denoising autoencoder algorithm model based on deep learning which improves neural networks model. This algorithm conducts the unsupervised preliminary training of face depth data and the supervised training to fine-tuning the network which is better than neural network's random initialization. The experiment indicates that compared with real face data, the reconstruction face model has a small matching error by using SDAE algorithm and it achieves an excellent face recognition effect.",multimedia
10.1109/IEMBS.2007.4352483,to_check,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IEEE,2007-08-26 00:00:00,ieeexplore,Research on 3D Modeling for Head MRI Image Based on Immune Sphere-Shaped Support Vector Machine,https://ieeexplore.ieee.org/document/4352483/,"In head MRI image sequences, the boundary of each encephalic tissue is highly complicated and irregular. It is a real challenge to traditional 3D modeling algorithms. Support vector machine (SVM) based on statistical learning theory has solid theoretical foundation. sphere-shaped SVM (SSSVM) was originally developed for solving some special classification problems. In this paper, it is extended to image 3D modeling which tries to find the smallest hypersphere enclosing target data in high dimensional space by kernel function. However, selecting parameter is a complicated problem which directly affects modeling accuracy. Immune algorithm (IA), mainly applied to optimization, is used to search optimal parameter for SSSVM. So, immune SSSVM (ISSSVM) is proposed to construct the 3D models for encephalic tissues. As our experiment demonstrates, the models are constructed and reach satisfactory modeling accuracies. Theory and experiment indicate ISSSVM exhibits its great potential in image 3D modeling.",multimedia
10.1109/IJCNN48605.2020.9207590,to_check,2020 International Joint Conference on Neural Networks (IJCNN),IEEE,2020-07-24 00:00:00,ieeexplore,TSCNN: A 3D Convolutional Activity Recognition Network Based on RFID RSSI,https://ieeexplore.ieee.org/document/9207590/,"Human activity recognition has a wide range of applications, especially for the care of elderly people living alone and the monitoring of abnormal behaviors of key personnel. Although conventional video surveillance technology has made many research advances in this field, this technology destroys people's privacy. Activity recognition technology based on RFID avoids damage to people's privacy, and is being widely studied and applied. This paper uses RFID Received Signal Strength Indicator (RSSI) to identify and classify human behaviors. Predecessors employed CNN and LSTM for human activity identification, but there were still some shortcomings: 1) The 2D convolution loses the temporal information of continuous actions and reduces the classification accuracy. 2) LSTM network has a series of training difficulties. 3) No available public dataset for the current mission. To solve these problems, this paper proposes a convolutional neural network called temporal spatial convolutional neural network (TSCNN). Taking the continuous frame sequence as input, the network is designed using 3D convolution to realize realtime activities recognition. The average classification accuracy of our network is 94.6%, 15.6% higher than the state-of-the- art - Tagfree. Our lowest accuracy is 81.8%, and Tagfree is 35.4%. Besides, the ablation experiment proves the necessity of the design in the TSCNN network. Furthermore, we collect more than 60000 RFID signal data and transform them into corresponding pixel maps to form a new dataset. We present and expose the dataset called RF-men.",multimedia
10.1109/ICSE-Companion52605.2021.00052,to_check,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE,2021-05-28 00:00:00,ieeexplore,Testing Object Detection for Autonomous Driving Systems via 3D Reconstruction,https://ieeexplore.ieee.org/document/9402601/,"Object detection is to identify objects from images. In autonomous driving systems, object detection serves as an intermediate module, which is used as the input of autonomous decisions for vehicles. That is, the accuracy of autonomous decisions relies on the object detection. The state-of-the-art object detection modules are designed based on Deep Neural Networks (DNNs). It is difficult to employ white-box testing on DNNs since the output of a single neuron is inexplicable. Existing work conducted metamorphic testing for object detection via image synthesis: the detected object in the original image should be detected in the new synthetic image. However, a synthetic image may not look real from humans' perspective. Even the object detection module fails in detecting such synthetic image, the failure may not reflect the ability of object detection. In this paper, we propose an automatic approach to testing object detection via 3D reconstruction of vehicles in real photos. The 3D reconstruction is developed via vanishing point estimation in photos and heuristic based image insertion. Our approach adds new objects to blank spaces in photos to synthesize images. For example, a new vehicle can be added to a photo of a road and vehicles. In this approach, the output synthetic images are expected to be more natural-looking than randomly synthesizing images. The experiment is conducting on 500 driving photos from the Apollo autonomous driving dataset.",multimedia
10.1109/TMM.2020.2991532,to_check,IEEE Transactions on Multimedia,IEEE,2021-01-01 00:00:00,ieeexplore,3D Pose Estimation Based on Reinforce Learning for 2D Image-Based 3D Model Retrieval,https://ieeexplore.ieee.org/document/9082869/,"In this paper, we propose a novel characteristic view selection model (CVSM) to address the 2D image-based 3D object retrieval problem. This work includes two key contributions: 1) we propose a novel reinforcement learning model to estimate the 3D pose based on a 2D image; and 2) we render the pose-specific model to generate a representative angle view for retrieval applications. First, we define state, policy, action and reward functions to train an agent with the reinforcement learning framework, by which the agent can effectively reduce the computational cost of the characteristic view selection and directly obtain the 3D model pose. Second, to resolve the problem of computing similarity in the cross-domain between the virtual 3D model view and the real query image, we project them into the skeleton domain, and the skeleton information can effectively bridge the gap between the image and 3D model view for cross-media retrieval. To demonstrate the performance of our approach, we compare with some classic 3D pose estimation methods using the popular Pascal3D dataset. To demonstrate the performance of our approach in model retrieval, we collect a new dataset that includes pairs of 2D images and 3D objects, where 3D objects are based on the ModelNet40 dataset and 2D images are based on the ImageNet dataset, and we experiment with our method using the SHREC 2018 and SHREC 2019 databases. The experimental results demonstrate the superiority of our method.",multimedia
10.1109/ACCESS.2020.2993574,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,FashionFit: Analysis of Mapping 3D Pose and Neural Body Fit for Custom Virtual Try-On,https://ieeexplore.ieee.org/document/9091008/,"Visual compatibility and virtual feel are critical metrics for fashion analysis yet are missing in existing fashion designs and platforms. An explicit model is much needed for implanting visual compatibility through fashion image inpainting and virtual try-on. With rapid advancements in the Computer Vision realm, the increase in creating customer experience which leads to the great potential of interest to retailers and customers. The public datasets available are very much fit for generating outfits from Generative Adversarial Networks (GANs) but the custom outfits of the users themselves lead to low accuracy levels. This work is the first step in analyzing and experimenting with the fit of custom outfits and visualizing it to the users on them which creates the great customer experience. The work analyses the need for providing visualization of custom outfits on users in the large corpora of AI in Fashion. The authors propose a novel architecture which facilitates the combining outfits provided by the retailers and visualize it on the users themselves using Neural Body Fit. This work creates a benchmark in disentangling the custom generation of cloth outfits using GANs and virtually trying it on the users to ensure a virtual-photorealistic appearance and results to create a great customer experience by using AI. Extensive experiments show the high accuracy levels on custom outfits generated by GANs but not in customized levels. This experiment creates new state-of-art results by plotting users pose for calculating the lengths of each body-part segment (hand, leg, and so forth), segmentation + NBF for accurate fitting of the cloth outfit. This paper is different from all other competitors in terms of approach for the virtual try-on for creating a new customer experience.",multimedia
10.1109/IROS.1992.601935,to_check,Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,1992-07-10 00:00:00,ieeexplore,"""Arnie P."" - A Robot Golfing System Using Binocular And A Heuristic Feedback Mechanism",https://ieeexplore.ieee.org/document/601935/,"This paper describes a robot vision golfing system. The Automated Robotic Navigational unit with Intelligent Eye and Putter (ARNIE P)<sup>τ</sup>project was initiated to investigate the problems and develop software solutions for robotic tasks that require good hand-eye coordination and an intelligent sensor feedback mechanism. This system has only one frame buffer and no specialized hardware, so quasi-real time 3D tracking is accomplished in software using the Unix Spline facility. The single frame buffer and digitizer, stores and retains the location of the ball from two separate cameras during the time interval between the golf ball initially crossing a trigger scan line and the ball coming to a complete stop. The most novel aspect of this study is that by attempting to build or model a difficult perceptory task such as golf, which requires integrating many complicated computational pieces (binocular stereo vision, robot arm motion, heuristic feedback, learning), it appears to be a good plarform to experiment with artificial intelligence techniques and robotics.",multimedia
10.1109/ROBIO.2015.7419109,to_check,2015 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2015-12-09 00:00:00,ieeexplore,A novel extrinsic calibration method for robotic systems with structured light sensors,https://ieeexplore.ieee.org/document/7419109/,"The extrinsic calibration for robotic systems with structured light sensor is fundamental to accomplish the tasks of 3D measurement, reverse engineering and quality inspection. In this paper, we proposed a cuboid calibration target with nicks on the surfaces which is easy to extract the control points. In the calibration process, both least square method and the nonlinear optimization based on maximum likelihood criterion are utilized. The degenerate configuration is discussed for non-singularity solution. Both simulation and real data are utilized for robustness test of the algorithm and the experiment shows that the proposed method has 0.6 mm RMS errors of control points. Compared to the existed methods, the proposed method has the advantages of small error, simple operation and high flexibility.",multimedia
10.1109/CGNCC.2014.7007211,to_check,"Proceedings of 2014 IEEE Chinese Guidance, Navigation and Control Conference",IEEE,2014-08-10 00:00:00,ieeexplore,Adaptive terrain classification in field environment based on self-supervised learning,https://ieeexplore.ieee.org/document/7007211/,"This paper focuses on terrain classification in field environment and proposes a self-supervised terrain classification method which is based on 3D laser sensor and monocular vision sensor to adapt to changes in terrain environment and external conditions. First of all, extract typical traversable areas and typical obstacle areas by analyzing range data from 3D laser sensor and project these two kinds of areas into image space to label the image data. Then extract visual feature from the corresponding image to train classifier to classify the terrain. The experiment results demonstrate that the proposed method in this paper can obtain high classification accuracy and good real-time performance.",multimedia
10.1109/ISCID.2017.145,to_check,2017 10th International Symposium on Computational Intelligence and Design (ISCID),IEEE,2017-12-10 00:00:00,ieeexplore,Path Planning Method Combining Depth Learning and Sarsa Algorithm,https://ieeexplore.ieee.org/document/8283229/,"When the traditional Sarsa(λ) algorithm is applied to the path planning, there are some problems such as slow learning of environmental knowledge and neglecting a lot of useful information. This paper proposes a method to combine the algorithm of Stacking Denoising AutoEncoders, Extract real-time environmental features by stack denoising sparse autoencoders. While eliminating the impact of environmental noise. The position information is obtained by mapping the SOM neural network, whereby the position information yields the R value. Sarsa(λ) updates the Q value based on the R value and carries out the corresponding path planning. At the same time, SOM neural network mapping can effectively avoid the long time iterative operation and output error of other neural networks. This method makes the algorithm more effective to extract the environmental characteristic information, and makes the path planning more accurate and efficient. The simulation experiment takes the agent path planning in 2D and 3D complex environment as the background, the traditional Sarsa(λ) algorithm is compared with the DSAE-Sarsa(λ) that proposed in this paper. Through the path planning performance, the algorithm convergence and the convergence speed, the reward value situation and so on, verifies the ability and superiority of the algorithm that proposed in this paper.",multimedia
10.1109/ICIP.2019.8803694,to_check,2019 IEEE International Conference on Image Processing (ICIP),IEEE,2019-09-25 00:00:00,ieeexplore,Towards Robust Retrieval for Imperfectly Scanned Point Cloud Objects,https://ieeexplore.ieee.org/document/8803694/,"With the development of 3D model analysis and particularly on classification challenge, the algorithms are getting better and better. Since no large dataset of scanned models is available, evaluating the algorithms in real-life scenarios is not straightforward. For now, these studies rely on ModelNet, a dataset of CAD models. Moreover, no studies considered the robustness to common recording data corruptions like occlusion or noise. In this paper, we present a preliminary study to assess the retrieval performances of point cloud-based algorithms for occluded or noisy objects. The experiment shows very promising results, even in the case of deep learning networks which has been pre-trained with CAD models. Indeed, more than 90% of the objects are retrieved when even only 40% of the query object is visible.",multimedia
10.1109/ICAIE50891.2020.00083,to_check,2020 International Conference on Artificial Intelligence and Education (ICAIE),IEEE,2020-06-28 00:00:00,ieeexplore,Virtualized circuit welding and simulation experiment system based on Unity3D,https://ieeexplore.ieee.org/document/9262558/,"Circuit welding and simulation experiment is a compulsory course for electronic information students in college. To make students more intuitively learn the principles of circuits and exercise their practical ability, we use Unity3D and 3DMax software to develop an immersive 3D virtual experiment system, and give the system's overall structure design and some key technologies. The system serves as an auxiliary teaching and learning tool, simulating a real experimental environment, and providing necessary conditions for students* autonomous experiments.",multimedia
10.1109/TPAMI.2005.22,to_check,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2005-02-01 00:00:00,ieeexplore,A Fourier theory for cast shadows,https://ieeexplore.ieee.org/document/1374876/,"Cast shadows can be significant in many computer vision applications, such as lighting-insensitive recognition and surface reconstruction. Nevertheless, most algorithms neglect them, primarily because they involve nonlocal interactions in nonconvex regions, making formal analysis difficult. However, many real instances map closely to canonical configurations like a wall, a V-groove type structure, or a pitted surface. In particular, we experiment with 3D textures like moss, gravel, and a kitchen sponge, whose surfaces include canonical configurations like V-grooves. This paper takes a first step toward a formal analysis of cast shadows, showing theoretically that many configurations can be mathematically analyzed using convolutions and Fourier basis functions. Our analysis exposes the mathematical convolution structure of cast shadows and shows strong connections to recent signal-processing frameworks for reflection and illumination.",multimedia
10.1109/CASE49439.2021.9551440,to_check,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),IEEE,2021-08-27 00:00:00,ieeexplore,Integration of Digital Twin and Machine Learning for Geometric Feature Online Inspection System,https://ieeexplore.ieee.org/document/9551440/,"The effective control of the welding quality can ensure the overall performance of the product. For the geometric feature online inspection system of welding process, this paper used digital twin technology to realize the mapping integration between physical and virtual workshop. The paper also proposed a control chart pattern recognition method based on distance mode profile of time series and convolutional neural network, along with an ensemble learning model for the decision of fixture adjustment. The experiment results show that the integration of digital twin and machine learning provides a feasible way for real-time monitoring and accurate control of welding quality.",multimedia
10.1109/AIKE48582.2020.00041,to_check,2020 IEEE Third International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),IEEE,2020-12-13 00:00:00,ieeexplore,Multi-Agent Pathfinding with Hierarchical Evolutionary Hueristic A,https://ieeexplore.ieee.org/document/9355477/,"Multiagent pathfinding (MAPF) problem is an important topic to various domains including video games, robotics, logistics, and crowd simulation. The goal of a pathfinding algorithm for MAPF is to find the shortest possible path for each agent without collisions with other agents. Search is among the most fundamental techniques for problem solving, and A* is the best known heuristic search algorithm. While A* guarantees to find the shortest path using a heuristic function, it cannot handle the large scale and many uncertainties in MAPF. The main challenge of MAPF is the scalability. The problem complexity grows exponentially as both the size of environments and the number of autonomous agents increase, which becomes more difficult for A* to compute results in real time under the constraints of memory and computing resources. To overcome the challenges in MAPF, distributed approaches are introduced to reduce the computational time and complexity. Contrast to centralized approaches, which use a single controller to determine every move of all agents, distributed approaches allow each agent to search for its own solution. Distributed MAPF algorithms need to refine solutions for all agents that are collision-free. The algorithm should lead agents to take another path, or standby on the same node at the moment, to avoid conflicts between any two paths. Under the circumstances, an optimal solution is no longer simply finding the shortest path for each agent. Instead, it should contain a collision-free path for every agent, with the lowest makespan, which is the number of time steps required for all agents to reach their target. However, minimizing the makespan and the sum of cost for all agents is a NP-hard problem. Given MAPF problems often require to be solved in real time with limited resources, minimizing only the makespan is a more practical approach.To achieve accurate search and high scalability, a MAPF algorithm must fulfill the following requirements: 1) it is capable to compute collision-free paths for all agents; 2) it can provides an accurate priority decision mechanism to ensure solution optimality; and 3) it should maintain the successful rate to obtain a solution as the number of agents increases. In this paper, we proposed a novel hierarchical pathfinding technique named Multi-Agent Hierarchical Evolutionary Heuristics A* (MA-HEHA*). Our contributions in this paper are: 1) we propose MA-HEHA* that can identify bottleneck areas to reduce collisions in abstract search; 2) our algorithm evolves heuristic functions by itself to avoid potential conflicts during local search; 3) we prove that MA-HEHA* maintain high successful rate when the scalability is high; 4) we evaluate MA-HEHA* on different types of MAPF problems to show its effectiveness. Our experiment results show that our MA-HEHA* can efficiently solve large scale MAPF problems compared to traditional MAPF approaches.",multimedia
10.1109/IJCIME49369.2019.00053,to_check,"2019 International Joint Conference on Information, Media and Engineering (IJCIME)",IEEE,2019-12-19 00:00:00,ieeexplore,Video Game Design and Performance Analysis Based on Reinforcement Learning,https://ieeexplore.ieee.org/document/9066349/,"The Reinforcement Learning (RL) implement real-time decision-making behavior in uncertain environment in order to optimize overall target. RL is considered to be the paradigm of future general artificial intelligence (AI) because it is very similar to the way human thinks, which is expected to be widely used in game designing. In order to make RL algorithm perform better in video games, the model-free algorithm is used and convolutional neural network (CNN) is used in the pre-processing. In this paper, the performance of Deep Q Network (DQN) and Double Deep Q Network (DDQN) algorithms were tested and compared in the ""Arkanoid Game"". The experiment shows that the DQN algorithm has a problem of overestimating the Q value and the DDQN algorithm can solve it well. In addition, the two algorithms are similar in terms of loss changes, but the learning effect of DDQN performs better.",multimedia
10.1109/ICIEAM51226.2021.9446291,to_check,"2021 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",IEEE,2021-05-21 00:00:00,ieeexplore,Face Detection in Real Time Live Video Using Yolo Algorithm Based on Vgg16 Convolutional Neural Network,https://ieeexplore.ieee.org/document/9446291/,"Face detection is not only one of the most studied topics in the computer vision field but also a very important task in many applications, such as security access control systems, video surveillance, human-computer interface, and image database management. Nowadays, various methods were developed for face detection systems like Viola-Jones, RCNN, SSD, and so on. Many researchers are still trying to improve face detection systems with various illustrations, poses, skin colors, and real-time detection. This paper intends to combine YOLO (You Only Look Once) algorithm with the VGG16 pre-trained convolutional neural network to propose an improvement for face detection systems. Experimental results show that proposed method has detected the test image set with over 95 % of average precision. Also, our proposed method considerably increased face detection speed in real-time live video. The experiment of this work was using the Image Processing Toolbox and the Deep Learning Toolbox in MATLAB.",multimedia
10.1109/NICS51282.2020.9335882,to_check,2020 7th NAFOSTED Conference on Information and Computer Science (NICS),IEEE,2020-11-27 00:00:00,ieeexplore,Online Video Stabilization Based on Converting Deep Dense Optical Flow to Motion Mesh,https://ieeexplore.ieee.org/document/9335882/,"Video stabilization is very necessary for shaky videos. Until now, there are many offline methods (using both past and future frames) for stabilization. These methods have good results for stabilizing, but not be consistent with real applications. So inspired by the approach, first, we divide each frame into grids and calculate motion vectors at each vertex. Second, accumulating motion mesh across past frames to get motion curves. Finally, smoothing these curves to stabilize video. The difference of our proposed method is the way to calculate motion mesh. Instead of propagating motion vectors at feature points to mesh vertexes, we take advantage of the power of deep learning network to estimate dense optical flow, then convert it to motion mesh. Our experiment has shown that output videos of our online method (only using past frames) have stability scores which are competitive with offline methods. Our method is still effective where the similarity between two consecutive frames is low (due to fast camera, fast zooming, etc.), in this case feature-based methods have not achieved good results.",multimedia
10.1109/ICCT.2015.7399847,to_check,2015 IEEE 16th International Conference on Communication Technology (ICCT),IEEE,2015-10-20 00:00:00,ieeexplore,The video people detection based on neural network,https://ieeexplore.ieee.org/document/7399847/,"In order to solve the problem of pedestrian detection algorithm, which presents the weak real-time video monitoring and the recognition rate for improvement, a kind of HOG-neural network pedestrian detection method based on gaussian mixture background deduction, HOG feature and BP neural network, is presented in this paper. Compared with the classical HOG-SVM method, many experiment results show that this method has a great increase in real-time, and certain promotion in terms of recognition rate, by training and testing the INRIA pedestrian database under the environment of Opencv.",multimedia
10.1109/TPAMI.2014.2361121,to_check,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2015-06-01 00:00:00,ieeexplore,Relative Hidden Markov Models for Video-Based Evaluation of Motion Skills in Surgical Training,https://ieeexplore.ieee.org/document/6915721/,"A proper temporal model is essential to analysis tasks involving sequential data. In computer-assisted surgical training, which is the focus of this study, obtaining accurate temporal models is a key step towards automated skill-rating. Conventional learning approaches can have only limited success in this domain due to insufficient amount of data with accurate labels. We propose a novel formulation termed Relative Hidden Markov Model and develop algorithms for obtaining a solution under this formulation. The method requires only relative ranking between input pairs, which are readily available from training sessions in the target application, hence alleviating the requirement on data labeling. The proposed algorithm learns a model from the training data so that the attribute under consideration is linked to the likelihood of the input, hence supporting comparing new sequences. For evaluation, synthetic data are first used to assess the performance of the approach, and then we experiment with real videos from a widely-adopted surgical training platform. Experimental results suggest that the proposed approach provides a promising solution to video-based motion skill evaluation. To further illustrate the potential of generalizing the method to other applications of temporal analysis, we also report experiments on using our model on speech-based emotion recognition.",multimedia
10.1109/ACCESS.2019.2902863,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,TensorFlow-Based Automatic Personality Recognition Used in Asynchronous Video Interviews,https://ieeexplore.ieee.org/document/8660507/,"With the development of artificial intelligence (AI), the automatic analysis of video interviews to recognize individual personality traits has become an active area of research and has applications in personality computing, human-computer interaction, and psychological assessment. Advances in computer vision and pattern recognition based on deep learning (DL) techniques have led to the establishment of convolutional neural network models that can successfully recognize human nonverbal cues and attribute their personality traits with the use of a camera. In this paper, an end-to-end AI interviewing system was developed using asynchronous video interview (AVI) processing and a TensorFlow AI engine to perform automatic personality recognition (APR) based on the features extracted from the AVIs and the true personality scores from the facial expressions and self-reported questionnaires of 120 real job applicants. The experimental results show that our AI-based interview agent can successfully recognize the “big five” traits of an interviewee at an accuracy between 90.9% and 97.4%. Our experiment also indicates that although the machine learning was conducted without large-scale data, the semisupervised DL approach performed surprisingly well with regard to APR despite the lack of labor-intensive manual annotation and labeling. The AI-based interview agent can supplement or replace existing self-reported personality assessment methods that job applicants may distort to achieve socially desirable effects.",multimedia
10.1109/ICCWAMTIP47768.2019.9067605,to_check,2019 16th International Computer Conference on Wavelet Active Media Technology and Information Processing,IEEE,2019-12-15 00:00:00,ieeexplore,A Deep Neural Network Object Detection Method Using Multiscale Poisson Fusion,https://ieeexplore.ieee.org/document/9067605/,"The deep neural network method has been widely used in the field of video object detection. However, the detection performance of such detectors are very sensitive to scaling change of targets. To improve detecting performance, it's necessary to use a large number of sample data at different scales to train the model, but generally we could not get real multi-scale data of one scene. Aiming at Faster-RCNN, a state-of-the-art object detector, this paper proposes a deep neural network object detection method based on multi-scale Poisson fusion, its augment samples using Poisson method, which extracts sample objects from the original video frames and uses the Laplacian differential operator to calculate the divergence of sample objects according to the divergence of target area. After multi scaling at the sample objects, the Poisson equation is used to merge the scaling objects into the background at the target area, thus generating a large number of sample data at different scales. The experiment proves that the multi-scale sample data which generated by Possion fusion method can effectively perform the fine-tuning training for Faster-RCNN and improve its performance when detecting objects at different scales.",multimedia
10.1109/ICSP.2018.8652388,to_check,2018 14th IEEE International Conference on Signal Processing (ICSP),IEEE,2018-08-16 00:00:00,ieeexplore,A Novel Multi-source Vehicle Detection Algorithm based on Deep Learning,https://ieeexplore.ieee.org/document/8652388/,"In this paper, a novel multi-source vehicle detection algorithm based on deep learning is proposed. In the proposed algorithm, in order to detect the vehicle objects, a fast deep learning algorithm based on convolutional neural network (CNN) is utilized to detect the vehicle objects, and the radar is used to obtain the position information about the vehicle objects. At the same time, a coordinate transformation method from radar coordinate system to video pixel coordinate system is presented, then the video detections and radar infromation are integrated to improve the detection performance of vehicles Finally, the experiment results based on the real datasets show that the proposed algorithm is very effective for the vehicle object detection and tracking.",multimedia
10.1109/ICDSE50459.2020.9310145,to_check,2020 International Conference on Data Science and Engineering (ICDSE),IEEE,2020-12-05 00:00:00,ieeexplore,Cockpit Display Graphics Symbol Detection for Software Verification Using Deep Learning,https://ieeexplore.ieee.org/document/9310145/,"In Software Development Life-cycle, Verification and Validation plays a very important role, especially in the case of Safety-Critical Industries like Aerospace. Display dashboard consists of multiple static and dynamic objects having affine transformation, graphics overlap, shadows and less inter symbol discriminative features compared to natural images. Manual Software graphics verification is an error-prone and time-consuming activity. In this paper, we propose a novel software graphics verification pipeline to verify graphics symbols and alphanumeric objects as per Software requirements. To the best of our knowledge, our proposed approach is the first study on deep learning-based graphics symbol detection from complex synthetic background which requires high model accuracy. We experiment using Single-shot Multibox Detector (SSD) and You Only Look Once (YOLO v2) to detect different Graphical symbols from display simulator real-time captured video frames. These detected objects are further classified based on their nature. Objects containing alphanumeric digits can be recognized using Optical Character Recognition and dynamic symbols are detected using object detection to infer other properties. Finally, all the extracted properties can be compared with test expectations to verify their correctness. The result shows superior accuracy of the SSD algorithm over other state-of-the-art object detection algorithms for detecting real-time graphics symbols.",multimedia
10.1049/cp.2012.1103,to_check,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),IET,2012-03-05 00:00:00,ieeexplore,Design of ADSP-BF548 fatigue driving detection system based on eye features,https://ieeexplore.ieee.org/document/6492710/,"To avoid traffic accidents caused by fatigue driving, this paper utilizes fatigue detection algorithm based on eye features to transplant on ADSP-BF548 hardware platform. The camera captures driver's face video in real time; it localizes driver's eye position and status via frame-difference projection, template matching and Kalman methods. At last, make a warning if driver's eye-close time proportion is more than the threshold in a period of time. The experiment shows this driver fatigue detection system based on ADSP-BF548 is of high accuracy and real-time.",multimedia
10.1109/iSAI-NLP48611.2019.9045445,to_check,2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),IEEE,2019-11-01 00:00:00,ieeexplore,Develop the Framework Conception for Hybrid Indoor Navigation for Monitoring inside Building using Quadcopter,https://ieeexplore.ieee.org/document/9045445/,"Building security is crucial, but guards and CCTV may be inadequate for monitoring all areas. A quadcopter (drone) with manual and autonomous control was used in a trial mission in this project. Generally, all drones can stream live video and take photos. They can also be adapted to assist better decision-making in emergencies that occur inside a building. In this paper, we show how to improve a quadcopter's ability to fly indoors, detect obstacles and react appropriately. This paper represents a new conceptual framework of hybrid indoor navigation ontology that analyzes a regular indoor route, including detection and avoidance of obstacles for the auto-pilot. An experiment with the system demonstrates improvements that occur in building surveillance and maintaining real-time situational awareness. The immediate objective is to show that the drone can serve as a reliable tool in security operations in a building environment.",multimedia
10.1109/ICBPE.2009.5384106,to_check,2009 International Conference on Biomedical and Pharmaceutical Engineering,IEEE,2009-12-04 00:00:00,ieeexplore,Feature selection and classification for Wireless Capsule Endoscopic frames,https://ieeexplore.ieee.org/document/5384106/,"wireless capsule endoscopy (WCE) is an important device to detect abnormalities in small intestine. Despite emerging technologies, reviewing capsule endoscopic video is a labor intensive task and very time consuming. Computational tools which automatically detect informative frames and tag abnormal conditions such as bleeding, ulcer or tumor will dramatically reduce the clinician's effort. In this paper, we explored various machine-learning methodologies based on different feature extraction and selection criteria, and developed an optimized classification method. The experiment results shows that, comparing to texture feature, using color feature for classification achieved better accuracy, regardless of machine-learning method chosen. The proposed method has been applied in real data taken from capsule endoscopic exams. For informative frames detection, classification method using color feature gives an accuracy of 94.10% and 93.44% for support vector machines (SVM) and neural network (NN) classifiers respectively. For the bleeding detection using color feature, the accuracy achieved 99.41% and 98.97% for SVM and NN respectively. In addition, we also investigated the computational time required for feature extraction and classification. In our experiments, color feature significantly outperformed texture feature in WCE image classification. The overall computational time (per frame) using color feature is 0.7125s (informative frame with SVM), 1.0329s (informative frame with NN), 0.51s (bleeding frame with SVM) and 1.2163s (bleeding frame with NN). Classifiers for more gastro-intestinal (GI) diseases detection will be developed based on this work subsequently.",multimedia
10.1109/SBR-LARS-R.2017.8215312,to_check,2017 Latin American Robotics Symposium (LARS) and 2017 Brazilian Symposium on Robotics (SBR),IEEE,2017-11-11 00:00:00,ieeexplore,First response fire combat: Deep leaning based visible fire detection,https://ieeexplore.ieee.org/document/8215312/,"Visible spectrum video based fire detection using non-stationary cameras has been an overlooked research problem. While many authors have successfully developed algorithms to identify and measure the proportions of uncontrolled fire using thermal or stationary surveillance cameras, the development of non-stationary systems provides a much larger application scope. We present a deep learning based approach, based on Google's Inception v3 and evaluate how the use of different optimizers, learning rates and reduction functions impact the convergence time and the results obtained. The experiment used a balanced two-class dataset, composed of images from videos captured through hand-held and drone attached cameras on real fire hazard situations. The results show that using any choice of algorithms, the Inception v3 architecture is able to converge and obtain results that surpass the state-of-the-art fire detection methods. We show that using a proper choice of learning rate, the network is able to achieve a 99% accuracy in less than one hundred interactions.",multimedia
10.1109/IVS.2000.898310,to_check,Proceedings of the IEEE Intelligent Vehicles Symposium 2000 (Cat. No.00TH8511),IEEE,2000-10-05 00:00:00,ieeexplore,Graphical models for driver behavior recognition in a SmartCar,https://ieeexplore.ieee.org/document/898310/,"In this paper we describe our SmartCar testbed: a real-time data acquisition system and a machine learning framework for modeling and recognizing driver maneuvers at a tactical level, with special emphasis on how the context affects the driver's performance. The perceptual input is multimodal: four video signals capture the contextual traffic, the driver's head and the driver's viewpoint; and a real-time data acquisition system records the car's brake, gear, steering wheel angle, speed and acceleration throttle signals. Over 70 drivers have driven the SmartCar for 1.25 hours in the greater Boston area. Graphical models, HMM and coupled HMM, have been trained using the experiment driving data to create models of seven different driver maneuvers: passing, changing lanes right and left, turning right and left, starting and stopping. We show that, on average, the predictive power of our models is of 1 second before the maneuver starts taking place. Therefore, these models would be essential to facilitate operating mode transitions between driver and driver assistance systems, to prevent potential dangerous situations and to create more realistic automated cars in car simulators.",multimedia
10.1109/IJCNN.2018.8489743,to_check,2018 International Joint Conference on Neural Networks (IJCNN),IEEE,2018-07-13 00:00:00,ieeexplore,Hand Gesture Recognition and Real-time Game Control Based on A Wearable Band with 6-axis Sensors,https://ieeexplore.ieee.org/document/8489743/,"Human-computer interaction introduces critical open door with the proceeds with improvement of wearable gadgets. Gesture recognition through smart devices is becoming a popular research direction. This paper proposes a hand gesture recognition and real-time game control system that is capable of continues human-computer interaction in view of an off-the-rack business wearable wristband. We utilize three-axis accelerator and gyroscope sensors embedded in smart band to collect hand motion information and use Kinect camera capture video information for manual segmentation during the training model phase. A continuous gesture segmentation algorithm based on sliding window and DTW algorithm is developed to detect meaningful gestures in the real-time game control stage. In addition, an android game named Fly Birds which is controlled by gesture recognition result is presented to simulate real-time human-computer interaction. Then, we classify the data in the window using common classifiers. Finally, our experimental results show that, we can accurately identify the designed gestures during the stage of static gesture recognition, and we also achieve a perfect interactive effect in the process of dynamic real-time game control. The experiment outcomes will advance the ascent of human-PC cooperation in view of hand gesture recognition and related applications will rise in vast numbers.",multimedia
10.1109/ChiCC.2016.7554485,to_check,2016 35th Chinese Control Conference (CCC),IEEE,2016-07-29 00:00:00,ieeexplore,Implement tracking algorithm using CNNs,https://ieeexplore.ieee.org/document/7554485/,"Convolutional neural networks (CNNs) is widely used as classifiers in the field of computer vision. The more complex a CNNs model is, the more accurate classification results will be. But a very deep network also requires a better GPU to train and test in a reasonable time. In this paper, we purpose a tracking algorithm using LeNet-5, and avoid computing complex handcrafted features from the raw inputs. Modifying the number of kernels improves the tracking accuracy of models, avoid over-fitting. The experiment of processing a video clip meets real-time requirement while using GPU and it shows our algorithm is more robust than traditional algorithm like particle filter to track single target under the complicated background.",multimedia
10.1109/ROMAN.2017.8172491,to_check,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),IEEE,2017-09-01 00:00:00,ieeexplore,Improving robot transparency: Real-time visualisation of robot AI substantially improves understanding in naive observers,https://ieeexplore.ieee.org/document/8172491/,"Deciphering the behaviour of intelligent others is a fundamental characteristic of our own intelligence. As we interact with complex intelligent artefacts, humans inevitably construct mental models to understand and predict their behaviour. If these models are incorrect or inadequate, we run the risk of self deception or even harm. Here we demonstrate that providing even a simple, abstracted real-time visualisation of a robot's AI can radically improve the transparency of machine cognition. Findings from both an online experiment using a video recording of a robot, and from direct observation of a robot show substantial improvements in observers' understanding of the robot's behaviour. Unexpectedly, this improved understanding was correlated in one condition with an increased perception that the robot was `thinking', but in no conditions was the robot's assessed intelligence impacted. In addition to our results, we describe our approach, tools used, implications, and potential future research directions.",multimedia
10.1109/CVPR.2018.00251,to_check,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,IEEE,2018-06-23 00:00:00,ieeexplore,Learning to Generate Time-Lapse Videos Using Multi-stage Dynamic Generative Adversarial Networks,https://ieeexplore.ieee.org/document/8578349/,"Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to 128 Ã- 128 resolution for 32 frames. Quantitative and qualitative experiment results demonstrate the superiority of our model over the state-of-the-art models.",multimedia
10.1109/DMAMH.2007.18,to_check,Second Workshop on Digital Media and its Application in Museum & Heritages (DMAMH 2007),IEEE,2007-12-12 00:00:00,ieeexplore,Network Adaptable Transmission Strategy Applied to H.264,https://ieeexplore.ieee.org/document/4414573/,"A real-time transmission strategy that adapts network conditions rapidly is brought forward, and it is applied to H.264 encoding standard. Firstly, the strategy forecasts initial network bandwidth using bi-search algorithm, and establishes a dropout rate-smoothing model. In addition, it adopts the AIMD algorithm to adjust the sending bit rates, and then calculates QP based on R-D model. Finally, it encodes the video sequences according to the QP that goes through the operation of smoothness. The result of experiment in model JM shows that, this method reduces the image dithering caused by the change of bit rate, makes better use of network, and improves the video quality.",multimedia
10.1109/ACCESS.2018.2823378,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,Fusion of Domain-Specific and Trainable Features for Gender Recognition From Face Images,https://ieeexplore.ieee.org/document/8331979/,"The popularity and the appeal of systems which are able to automatically determine the gender from face images are growing rapidly. Such a great interest arises from the wide variety of applications, especially in the fields of retail and video surveillance. In recent years, there have been several attempts to address this challenge, but a definitive solution has not yet been found. In this paper, we propose a novel approach that fuses domain-specific and trainable features to recognize the gender from face images. In particular, we use the SURF descriptors extracted from 51 facial landmarks related to eyes, nose, and mouth as domain-dependent features, and the COSFIRE filters as trainable features. The proposed approach turns out to be very robust with respect to the well-known face variations, including different poses, expressions, and illumination conditions. It achieves state-of-the-art recognition rates on the GENDER-FERET (94.7%) and on the labeled faces in the wild (99.4%) data sets, which are two of the most popular benchmarks for gender recognition. We further evaluated the method on a new data set acquired in real scenarios, the UNISA-Public, recently made publicly available. It consists of 206 training (144 male, 62 female) and 200 test (139 male, 61 female) images that are acquired with a real-time indoor camera capturing people in regular walking motion. Such experiment has the aim to assess the capability of the algorithm to deal with face images extracted from videos, which are definitely more challenging than the still images available in the standard data sets. Also for this data set, we achieved a high recognition rate of 91.5%, that confirms the generalization capabilities of the proposed approach. Of the two types of features, the trainable COSFIRE filters are the most effective and, given their trainable character, they can be applied in any visual pattern recognition problem.",multimedia
10.1109/PIERS-Spring46901.2019.9017530,to_check,2019 PhotonIcs & Electromagnetics Research Symposium - Spring (PIERS-Spring),IEEE,2019-06-20 00:00:00,ieeexplore,Accurate Image Recognition in Convolutional Neural Networks Based on Two-dimensional Discrete Fourier Transform,https://ieeexplore.ieee.org/document/9017530/,"A two-dimensional discrete Fourier transform (DFT) can transform an image from a spatial domain to a frequency domain. This conversion can more intuitively observe and process the image from the perspective of the frequency domain, and is more advantageous for operations such as frequency domain filtering. In recent years, convolutional neural networks (CNNs) have become a hot topic in the field of image recognition. Relevant algorithms are generally preprocessed using two-dimensional DFT before inputting the network. However, in practice, most common CNNs are processed in the real number domain, and the frequency domain information and phase information cannot be fully utilized. This paper aims to exploring the application of two-dimensional DFT in CNNs for image recognition. Due to the globality of Fourier transform, we use the grayscale image of human face that has been aligned as the training data to design the experiment. The experiment includes the following steps: the feature is identified in the frequency domain after the image undergoes a two-dimensional DFT; the frequency domain image after the two-dimensional DFT is input together with the original image to expand the input layer of the CNNs and is then identified. The experiment shows that the image can also recognize the features in the frequency domain after the two-dimensional DFT when the frequency domain features are good and the recognition accuracy can be greatly improved.",multimedia
10.1109/IMCEC51613.2021.9482361,to_check,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",IEEE,2021-06-20 00:00:00,ieeexplore,Based on the improved YOLOv3 under the Catenary Insulator Image Recognition,https://ieeexplore.ieee.org/document/9482361/,"In railway related supporting facilities, the catenary insulator is most among them very important one annulus, put forward different methods to the pretreatment of data sets, expands the capacity of data, loss of function optimization, deep learning of YOLOv3 insulator detection algorithm was improved, after the sampling by spatial attention mechanism and channel attention mechanism combining mechanism of cascading double attention to fusion filtering characteristics, improve the ability of feature extraction, the introduction of gaussian function to the maximum inhibition method was improved, reducing the presence of keep out target miss rate, improve the accuracy of insulator detecting, The identification of OCS insulators under complex background is completed by shortening the detection time of insulators, and the non-maximum suppression NMS is introduced. The experiment proves that the improved network performs well in the identification accuracy and identification time, and the accuracy is improved by 0.0216 on the basis of the original one. In addition, a Web interface is added to realize the online identification of insulators and meet the real-time requirements.",multimedia
10.1109/ITNEC.2017.8284852,to_check,"2017 IEEE 2nd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",IEEE,2017-12-17 00:00:00,ieeexplore,FPGA accelerates deep residual learning for image recognition,https://ieeexplore.ieee.org/document/8284852/,"Deep learning is greatly promoting the fast development of artificial intelligence. In the image classification area, the recognition rate and inference performance become the main challenges in the real application. Recently, residual network (ResNet) has shown a competitive accuracy and nice convergence behaviors in deep neural networks. In this paper, we are devoted to accelerate this promising framework in the inference application on FPGA (Field Programmable Gate Array) using OpenCL programming language. Firstly, a supplemented deep learning accelerator is constructed to realize the residual function in ResNet. Secondly, our construction reschedules three on-chip buffers in order to store the feature data and to stream it to processor elements alternately. In addition, we also implement data parallel and pipeline execution such that the filter parameters can be synchronously processed with the image data on FPGA. Moreover, we exploit a convertor to transform any ResNet in CAFFE framework into FPGA platform. It can generate the FPGA head files using its original prototxt files through the dictionary function of Python. The experiment result analysis shows that our acceleration has a competitive performance while maintaining the high accuracy rate. Finally, we provide a solution to accelerate any construction of ResNet using OpenCL programming language on FPGA.",multimedia
10.1109/ICCSCE.2017.8284432,to_check,"2017 7th IEEE International Conference on Control System, Computing and Engineering (ICCSCE)",IEEE,2017-11-26 00:00:00,ieeexplore,Entropy virus microscopy images recognition via neural network classifiers,https://ieeexplore.ieee.org/document/8284432/,"One of the topics that are commonly in focus of object detection and image recognition is virus detection. It is well known that to learn and detecting virus proven to be a challenging and quite complex task for computer systems under different noise level. This research work investigates the performances of preprocessing stages with Entropy feature extraction with Feed Forward Neural Network (FFNN) classifier under various levels of noise. The real time experiment conducted proved that the method proposed are efficient, robust, and excellent of which it has produced a results accuracy of up to 88% for biological viruses images classification.",multimedia
10.1109/ISSPA.2010.5605590,to_check,"10th International Conference on Information Science, Signal Processing and their Applications (ISSPA 2010)",IEEE,2010-05-13 00:00:00,ieeexplore,Self-connection architecture of hopfield model based on all-optical MZI-XNOR gate,https://ieeexplore.ieee.org/document/5605590/,"Many researches are conducted to improve Hopfield Neural Network performance especially for speed and memory capacity in different approaches. However, there is still a significant scope for developing HNN using Optical Logic Gates. We propose a new model of HNN based on all-optical XNOR logic gates for real time image recognition. Firstly, we improved HNN toward optimum learning and converging operations. We considered each unipolar image as a set of small blocks of 3-pixels as vectors for HNN. In addition, the weight matrices which have stability of unity at the diagonal perform clear converging in comparison with no self-connecting architecture. Synchronously, matrix-matrix multiplication operation would run optically in the second part, since we propose an array of all-optical XOR gates, which uses Mach-Zehnder Interferometer for neurons setup. The controlling system is to distribute and invert signals to achieve XNOR function. The preliminary experiment show positive results of the proposed system.",multimedia
10.1109/ICOIN.2001.905514,to_check,Proceedings 15th International Conference on Information Networking,IEEE,2001-02-02 00:00:00,ieeexplore,"Audio quality assessment in packet networks: an ""inter-subjective"" neural network model",https://ieeexplore.ieee.org/document/905514/,"Transmitting digital audio signals in real time over packet switched networks (e.g. the Internet) has set forth the need for developing signal processing algorithms that objectively evaluate audio quality. So far, the best way to assess audio quality are subjective listening tests, the most commonly used being the mean opinion score (MOS) recommended by the International Telecommunication Union (ITU). The goal of this paper is to show how artificial neural networks (ANNs) can be used to mimic the way human subjects estimate the quality of audio signals when distorted by changes in several parameters that affect the transmitted audio quality. To validate the approach, we carried out an MOS experiment for speech signals distorted by different values of IP-network parameters (e.g. loss rate, loss distribution, packetization interval, etc.), and changes in the encoding algorithm used to compress the original signal. Our results allow us to show that ANNs can capture the nonlinear mapping, between certain characteristics of audio signals and a subjective five points quality scale, ""built"" by a group of human subjects when participating in an MOS experiment, creating, in this way, an ""inter-subjective"" neural network (INN) model that might effectively ""evaluate"", in real time, the audio quality in packet switched networks.",multimedia
10.1109/IROS.2018.8594327,to_check,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2018-10-05 00:00:00,ieeexplore,Deep Reinforcement Learning for Audio-Visual Gaze Control,https://ieeexplore.ieee.org/document/8594327/,"We address the problem of audio-visual gaze control in the specific context of human-robot interaction, namely how controlled robot motions are combined with visual and acoustic observations in order to direct the robot head towards targets of interest. The paper has the following contributions: (i) a novel audio-visual fusion framework that is well suited for controlling the gaze of a robotic head; (ii) a reinforcement learning (RL) formulation for the gaze control problem, using a reward function based on the available temporal sequence of camera and microphone observations; and (iii) several deep architectures that allow to experiment with early and late fusion of audio and visual data. We introduce a simulated environment that enables us to learn the proposed deep RL model without the need of spending hours of tedious interaction. By thoroughly experimenting on a publicly available dataset and on a real robot, we provide empirical evidence that our method achieves state-of-the-art performance.",multimedia
10.1109/ASRU.2017.8268943,to_check,2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),IEEE,2017-12-20 00:00:00,ieeexplore,Direct modeling of raw audio with DNNS for wake word detection,https://ieeexplore.ieee.org/document/8268943/,"In this work, we develop a technique for training features directly from the single-channel speech waveform in order to improve wake word (WW) detection performance. Conventional speech recognition systems typically extract a compact feature representation based on prior knowledge such as log-mel filter bank energy (LFBE). Such a feature is then used for training a deep neural network (DNN) acoustic model (AM). In contrast, we directly train the WW DNN AM from the single-channel audio data in a stage-wise manner. We first build a feature extraction DNN with a small hidden bottleneck layer, and train this bottleneck feature representation using the same multi-task cross-entropy objective function as we use to train our WW DNNs. Then, the WW classification DNN is trained with input bottleneck features, keeping the feature extraction layers fixed. Finally, the feature extraction and classification DNNs are combined and then jointly optimized. We show the effectiveness of this stage-wise training technique through a set of experiments on real beam-formed far-field data. The experiment results show that the audioinput DNN provides significantly lower miss rates for a range of false alarm rates over the LFBE when a sufficient amount of training data is available, yielding approximately 12 % relative improvement in the area under the curve (AUC).",multimedia
10.1109/TASLP.2014.2337842,to_check,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2014-10-01 00:00:00,ieeexplore,Codebook-Based Audio Feature Representation for Music Information Retrieval,https://ieeexplore.ieee.org/document/6851913/,"Digital music has become prolific in the web in recent decades. Automated recommendation systems are essential for users to discover music they love and for artists to reach appropriate audience. When manual annotations and user preference data is lacking (e.g. for new artists) these systems must rely on content based methods. Besides powerful machine learning tools for classification and retrieval, a key component for successful recommendation is the audio content representation. Good representations should capture informative musical patterns in the audio signal of songs. These representations should be concise, to enable efficient (low storage, easy indexing, fast search) management of huge music repositories, and should also be easy and fast to compute, to enable real-time interaction with a user supplying new songs to the system. Before designing new audio features, we explore the usage of traditional local features, while adding a stage of encoding with a pre-computed codebook and a stage of pooling to get compact vectorial representations. We experiment with different encoding methods, namely the LASSO, vector quantization (VQ) and cosine similarity (CS). We evaluate the representations' quality in two music information retrieval applications: query-by-tag and query-by-example. Our results show that concise representations can be used for successful performance in both applications. We recommend using top- τ VQ encoding, which consistently performs well in both applications, and requires much less computation time than the LASSO.",multimedia
10.1109/ICASSP40776.2020.9053093,to_check,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2020-05-08 00:00:00,ieeexplore,Modeling Plate and Spring Reverberation Using A DSP-Informed Deep Neural Network,https://ieeexplore.ieee.org/document/9053093/,"Plate and spring reverberators are electromechanical systems first used and researched as means to substitute real room reverberation. Currently, they are often used in music production for aesthetic reasons due to their particular sonic characteristics. The modeling of these audio processors and their perceptual qualities is difficult since they use mechanical elements together with analog electronics resulting in an extremely complex response. Based on digital reverberators that use sparse FIR filters, we propose a signal processing-informed deep learning architecture for the modeling of artificial reverberators. We explore the capabilities of deep neural networks to learn such highly nonlinear electromechanical responses and we perform modeling of plate and spring reverberators. In order to measure the performance of the model, we conduct a perceptual evaluation experiment and we also analyze how the given task is accomplished and what the model is actually learning.",multimedia
10.1109/ACCESS.2020.3027619,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,"Hierarchical Transfer Learning for Multilingual, Multi-Speaker, and Style Transfer DNN-Based TTS on Low-Resource Languages",https://ieeexplore.ieee.org/document/9208651/,"This work applies a hierarchical transfer learning to implement deep neural network (DNN)-based multilingual text-to-speech (TTS) for low-resource languages. DNN-based system typically requires a large amount of training data. In recent years, while DNN-based TTS has made remarkable results for high-resource languages, it still suffers from a data scarcity problem for low-resource languages. In this article, we propose a multi-stage transfer learning strategy to train our TTS model for low-resource languages. We make use of a high-resource language and a joint multilingual dataset of low-resource languages. A pre-trained monolingual TTS on the high-resource language is fine-tuned on the low-resource language using the same model architecture. Then, we apply partial network-based transfer learning from the pre-trained monolingual TTS to a multilingual TTS and finally from the pre-trained multilingual TTS to a multilingual with style transfer TTS. Our experiment on Indonesian, Javanese, and Sundanese languages show adequate quality of synthesized speech. The evaluation of our multilingual TTS reaches a mean opinion score (MOS) of 4.35 for Indonesian (ground truth = 4.36). Whereas for Javanese and Sundanese it reaches a MOS of 4.20 (ground truth = 4.38) and 4.28 (ground truth = 4.20), respectively. For parallel style transfer evaluation, our TTS model reaches an F0 frame error (FFE) of 9.08%, 10.13%, and 8.43% for Indonesian, Javanese, and Sundanese, respectively. The results indicate that the proposed strategy can be effectively applied to the low-resource languages target domain. With a small amount of training data, our models are able to learn step by step from a smaller TTS network to larger networks, produce intelligible speech approaching the real human voice, and successfully transfer speaking style from a reference audio.",multimedia
10.1109/TransAI49837.2020.00010,to_check,2020 Second International Conference on Transdisciplinary AI (TransAI),IEEE,2020-09-23 00:00:00,ieeexplore,End-to-End Learning of Speech 2D Feature-Trajectory for Prosthetic Hands,https://ieeexplore.ieee.org/document/9253138/,"Speech is one of the most common forms of communication in humans. Speech commands are essential parts of multimodal controlling of prosthetic hands. In the past decades, researchers used automatic speech recognition systems for controlling prosthetic hands by using speech commands. Automatic speech recognition systems learn how to map human speech to text. Then, they used natural language processing or a look-up table to map the estimated text to a trajectory. However, the performance of conventional speech-controlled prosthetic hands is still unsatisfactory. Recent advancements in general-purpose graphics processing units (GPGPUs) enable intelligent devices to run deep neural networks in real-time. Thus, architectures of intelligent systems have rapidly transformed from the paradigm of composite subsystems optimization to the paradigm of end-to-end optimization. In this paper, we propose an end-to-end convolutional neural network (CNN) that maps speech 2D features directly to trajectories for prosthetic hands. The proposed convolutional neural network is lightweight, and thus it runs in real-time in an embedded GPGPU. The proposed method can use any type of speech 2D feature that has local correlations in each dimension such as spectrogram, MFCC, or PNCC. We omit the speech to text step in controlling the prosthetic hand in this paper. The network is written in Python with Keras library that has a TensorFlow backend. We optimized the CNN for NVIDIA Jetson TX2 developer kit. Our experiment on this CNN demonstrates a root-mean-square error of 0.119 and 20ms running time to produce trajectory outputs corresponding to the voice input data. To achieve a lower error in real-time, we can optimize a similar CNN for a more powerful embedded GPGPU such as NVIDIA AGX Xavier.",multimedia
10.1109/ROMAN.2017.8172449,to_check,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),IEEE,2017-09-01 00:00:00,ieeexplore,Exploring data augmentation methods in reverberant human-robot voice communication,https://ieeexplore.ieee.org/document/8172449/,"Collecting training data is not an easy task especially in situation involving robots that require tremendous physical effort. The ability to augment data through synthetic means is a convenient tool to solve this problem. Therefore it is important to evaluate the extent of the usefulness of augmented data. In this paper, we will explore data augmentation schemes in reverberant environment and investigate a method to effectively select data. We experiment in a real reverberant environment condition and investigate both the traditional automatic speech recognition (ASR) system based on gaussian mixture model-hidden markov model (GMM-HMM) and the most current system based on Deep Neural Networks (i.e, HMM-DNN). Our results show that the combination of data augmentation and data selection, further improves system performance. In our experiments, we used real test data in a reverberant hands-free human-robot communication scenario.",multimedia
10.1109/ACCESS.2020.3041901,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Japanese Pronunciation Evaluation Based on DDNN,https://ieeexplore.ieee.org/document/9276404/,"In recent years, speech recognition technology based on deep learning model has made great progress, and the accuracy of speech recognition has reached more than 90%. In foreign language learning, speech evaluation is an important application. Billions of foreign language learners need to practice effective pronunciation. However, due to the different goals between speech recognition and speech evaluation, a single speech recognition model cannot be directly applied to pronunciation evaluation. This paper proposes a DDNN (double-layer deep neural network) model, which includes the speech text alignment model and speech recognition model. In the first layer of the speech alignment model, a new Viterbi algorithm method is proposed to find the best path for the alignment of speech and text. In the second layer of speech evaluation and scoring, we are the first to use the CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network) on the encoding part of Attention. The accuracy of CTC model reaches 76.7%, and that of attention model is 81.2%. The experimental results show that the speech and text alignment method is effective, and the speech evaluation results based on the Attention model are better. The FRR (false rejection rate), FAR (false acceptance rate), and DER (diagnostic rate) in the Attention model were 4.5%, 5.1%, and 17.9%, respectively. At the same time, the evaluation of each sentence of the DDNN model in the online experiment is within 1 second, so the model can also be applied to the online real-time evaluation of speech pronunciation.",multimedia
10.1109/ACCESS.2021.3077886,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,"Multimodal Chain: Cross-Modal Collaboration Through Listening, Speaking, and Visualizing",https://ieeexplore.ieee.org/document/9424554/,"Language is an integral part of human interpersonal communication, which is conveyed through multiple sensory channels. This multisensory communication skill has motivated an extensive number of studies on multimodal information processing, which are trying to develop a system that mimics this natural behaviour. For example, automatic speech recognition (ASR) represents listening activity, text-to-speech (TTS) represents speaking, and various image processing models to represent visual perception. Most are trained and tuned independently using parallel examples from the source to the target modality. However, this is not the case in real-life situations, where a lot of paired data are unavailable. Inspired by this self-supervision of the human auditory and visual perception system, we proposed a multimodal chain mechanism with a weakly-supervised chain training strategy that is trained and tuned jointly. In our proposed framework, when the amount of paired training data are insufficient, collaboration among ASR and TTS, image captioning (IC), and image production models can improve their performance through single or dual-loop chain mechanisms. Our experiment result showed that by using such a closed-loop chain mechanism, we can improve a model with both unpaired and unrelated data from different modalities in a semi-supervised manner. Through the collaboration of speech and visual chains, we improve an ASR model performance with an image-only dataset while maintaining the performance of other models.",multimedia
10.1109/ICCT46805.2019.8947173,to_check,2019 IEEE 19th International Conference on Communication Technology (ICCT),IEEE,2019-10-19 00:00:00,ieeexplore,An Environment Learning Mechanism for Robust Speaker Recognition,https://ieeexplore.ieee.org/document/8947173/,"When application environment is inconsistent with training, the performance of speaker recognition system will drop significantly. Moreover, the real application environment is not able to be predicted in training stage, and it varies all time. In this paper, an environment self-learning method for robust speaker recognition is proposed. An improved Vector Taylor Series (VTS) is used to characterize the statistical distribution relationship between environment model and pure speaker model, and applied to the feature domain and model domain to compensate additive noise. When the environment changes, the prior environment noise data between speech intervals is collected and be used to update the Gaussian Mixture Model (GMM) of environment for compensating mismatches, so then to flexibly make the pure speaker model to fit the current application environment. The speaker identification experiment results show the proposed method improves the system performance significantly under different kinds of noise at low SNR.",multimedia
10.1109/CCPR.2009.5344040,to_check,2009 Chinese Conference on Pattern Recognition,IEEE,2009-11-06 00:00:00,ieeexplore,Design and Implementation of a Real-Time Speaker Identification System with Improved GMM,https://ieeexplore.ieee.org/document/5344040/,"The text-independent real-time speaker identification system is presented. It is based on Gaussian Mixture Model and MFCC (Mel frequency cepstral coefficients) method to extract the character of speech signal. The traditional method of GMM parameters initialization includes random method and k-means clustering are lack of clustering accuracy. A new approach which combines division and k-means clustering is presented and applied to the system. The system is realized under windows platform with good face. It includes voice collection and storage, speech pre-processing, MFCC extraction, GMM training and storage, speaker identification and so on. The experiment shows that the improved method as compared with the traditional method, the system average recognition rate has an increase of 18.34% and 7.98%. The system can achieve the error rate with 6.7% under the provided experimental condition.",multimedia
10.1007/s00500-021-05586-8,to_check,Soft Computing,Springer,2021-01-28 00:00:00,springer,Detection of shilling attack in recommender system for YouTube video statistics using machine learning techniques,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-05586-8,"Literature survey shows that the recommendation systems have been largely adapted and evaluated in various domains. Due to low performances from various cyber attacks, the adoption of recommender system is in the initial stage of defense systems. One of the most common attacks for recommender system is shilling attack. There are some existing techniques for identifying the shilling attacks built in the user ratings patterns. The performance of ratings on target items differs between the attack user profiles and actual user profiles. To differentiate the certain profiles, the affected profiles are known as attack profiles. Besides the shilling attacks, real cyber attacks are taking place in the community which are being solved by Petri Net methods. These attacks can be falsely predicted (shilling attacks) by the users which can raise security threats. For identifying various shilling attacks without a priori knowledge, Recommendation System suffers from low accuracy. Basically, recommendation attack is split into nuke and push attack that encourage and discourage the recommended target item. The strength of shilling attack is usually measured by filler size and attack size. An experiment over unsupervised machine learning algorithms with filler size 3% over 3%, 5%, 8% and 10% attack sizes is presented for Netflix dataset. Furthermore, we conducted an experiment on data of 26 K videos on the Trending YouTube Video Statistics, to predict the user preferences for a particular genre of videos using Machine Learning Algorithms. Based on the results, it observed that the Boosted Decision tree performs the best with an accuracy of 99 percent.",multimedia
10.1007/978-981-15-8599-9_49,to_check,Artificial Intelligence in China,Springer,2021-01-01 00:00:00,springer,Recognition of Grape Species with Small Samples Based on Attention Mechanism,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8599-9_49,"Take Turpan as an example, in recent years, the grape industry in Turpan has become one of the most important pillar industries for rural economic development and farmers’ income increase in Turpan. However, there are still many problems in the development of grape industry in Turpan area. Due to many and complicated grape varieties, it is difficult to identify them, and current identification efficiency is far from enough. Moreover, there is no large-scale open data set in grape recognition at present, and each image itself taken for the grape has much noise effect, which leads to low recognition accuracy. In this paper, a small sample of grape variety recognition method based on attention mechanism is used to fine-tune the CNN model and process the dataset image differently, and is compared with the traditional method. The experiment results show that the method can recognize different grape image types accurately with an accuracy of 93.72% under the condition of small sample. By the method, we can not only improve the efficiency of intelligent recognition, but also reduce the manpower cost, and thus realize the intelligent recognition of grape types.",multimedia
10.1007/11427445_55,to_check,Advances in Neural Networks – ISNN 2005,Springer,2005-01-01 00:00:00,springer,Speech Recognition of Finite Words Based on Multi-weight Neural Network,http://link.springer.com/openurl/pdf?id=doi:10.1007/11427445_55,"Under the guide of the novel biomimetics pattern recognition theory that is based on the character of human’s recognition, combining the character of traditional neural network, a new multi-weight neural network is constructed to realize the idea of that theory. Extraordinary results are obtained with the first use of the new multi-weight neural network in speech recognition of finite words. The experiment results show that the multi-weight neural network can not only recognize finite words correctly and promptly, but also keep quite high correct recognition rate under the circumstance of small-number samples.",multimedia
http://arxiv.org/abs/2102.07955v1,to_check,arxiv,arxiv,2021-02-16 04:27:19+00:00,arxiv,"Deep Learning based Multi-Source Localization with Source Splitting and
  its Effectiveness in Multi-Talker Speech Recognition",http://arxiv.org/abs/2102.07955v1,"Multi-source localization is an important and challenging technique for
multi-talker conversation analysis. This paper proposes a novel supervised
learning method using deep neural networks to estimate the direction of arrival
(DOA) of all the speakers simultaneously from the audio mixture. At the heart
of the proposal is a source splitting mechanism that creates source-specific
intermediate representations inside the network. This allows our model to give
source-specific posteriors as the output unlike the traditional multi-label
classification approach. Existing deep learning methods perform a frame level
prediction, whereas our approach performs an utterance level prediction by
incorporating temporal selection and averaging inside the network to avoid
post-processing. We also experiment with various loss functions and show that a
variant of earth mover distance (EMD) is very effective in classifying DOA at a
very high resolution by modeling inter-class relationships. In addition to
using the prediction error as a metric for evaluating our localization model,
we also establish its potency as a frontend with automatic speech recognition
(ASR) as the downstream task. We convert the estimated DOAs into a feature
suitable for ASR and pass it as an additional input feature to a strong
multi-channel and multi-talker speech recognition baseline. This added input
feature drastically improves the ASR performance and gives a word error rate
(WER) of 6.3% on the evaluation data of our simulated noisy two speaker
mixtures, while the baseline which doesn't use explicit localization input has
a WER of 11.5%. We also perform ASR evaluation on real recordings with the
overlapped set of the MC-WSJ-AV corpus in addition to simulated mixtures.",multimedia
http://arxiv.org/abs/2009.10283v1,to_check,arxiv,arxiv,2020-09-22 02:31:00+00:00,arxiv,End-to-End Learning of Speech 2D Feature-Trajectory for Prosthetic Hands,http://arxiv.org/abs/2009.10283v1,"Speech is one of the most common forms of communication in humans. Speech
commands are essential parts of multimodal controlling of prosthetic hands. In
the past decades, researchers used automatic speech recognition systems for
controlling prosthetic hands by using speech commands. Automatic speech
recognition systems learn how to map human speech to text. Then, they used
natural language processing or a look-up table to map the estimated text to a
trajectory. However, the performance of conventional speech-controlled
prosthetic hands is still unsatisfactory. Recent advancements in
general-purpose graphics processing units (GPGPUs) enable intelligent devices
to run deep neural networks in real-time. Thus, architectures of intelligent
systems have rapidly transformed from the paradigm of composite subsystems
optimization to the paradigm of end-to-end optimization. In this paper, we
propose an end-to-end convolutional neural network (CNN) that maps speech 2D
features directly to trajectories for prosthetic hands. The proposed
convolutional neural network is lightweight, and thus it runs in real-time in
an embedded GPGPU. The proposed method can use any type of speech 2D feature
that has local correlations in each dimension such as spectrogram, MFCC, or
PNCC. We omit the speech to text step in controlling the prosthetic hand in
this paper. The network is written in Python with Keras library that has a
TensorFlow backend. We optimized the CNN for NVIDIA Jetson TX2 developer kit.
Our experiment on this CNN demonstrates a root-mean-square error of 0.119 and
20ms running time to produce trajectory outputs corresponding to the voice
input data. To achieve a lower error in real-time, we can optimize a similar
CNN for a more powerful embedded GPGPU such as NVIDIA AGX Xavier.",multimedia
http://arxiv.org/abs/1910.10105v2,to_check,arxiv,arxiv,2019-10-22 16:45:38+00:00,arxiv,"Modeling plate and spring reverberation using a DSP-informed deep neural
  network",http://arxiv.org/abs/1910.10105v2,"Plate and spring reverberators are electromechanical systems first used and
researched as means to substitute real room reverberation. Nowadays they are
often used in music production for aesthetic reasons due to their particular
sonic characteristics. The modeling of these audio processors and their
perceptual qualities is difficult since they use mechanical elements together
with analog electronics resulting in an extremely complex response. Based on
digital reverberators that use sparse FIR filters, we propose a signal
processing-informed deep learning architecture for the modeling of artificial
reverberators. We explore the capabilities of deep neural networks to learn
such highly nonlinear electromechanical responses and we perform modeling of
plate and spring reverberators. In order to measure the performance of the
model, we conduct a perceptual evaluation experiment and we also analyze how
the given task is accomplished and what the model is actually learning.",multimedia
http://arxiv.org/abs/2003.08808v1,to_check,arxiv,arxiv,2020-03-16 00:38:13+00:00,arxiv,"Deep Learning for Automatic Tracking of Tongue Surface in Real-time
  Ultrasound Videos, Landmarks instead of Contours",http://arxiv.org/abs/2003.08808v1,"One usage of medical ultrasound imaging is to visualize and characterize
human tongue shape and motion during a real-time speech to study healthy or
impaired speech production. Due to the low-contrast characteristic and noisy
nature of ultrasound images, it might require expertise for non-expert users to
recognize tongue gestures in applications such as visual training of a second
language. Moreover, quantitative analysis of tongue motion needs the tongue
dorsum contour to be extracted, tracked, and visualized. Manual tongue contour
extraction is a cumbersome, subjective, and error-prone task. Furthermore, it
is not a feasible solution for real-time applications. The growth of deep
learning has been vigorously exploited in various computer vision tasks,
including ultrasound tongue contour tracking. In the current methods, the
process of tongue contour extraction comprises two steps of image segmentation
and post-processing. This paper presents a new novel approach of automatic and
real-time tongue contour tracking using deep neural networks. In the proposed
method, instead of the two-step procedure, landmarks of the tongue surface are
tracked. This novel idea enables researchers in this filed to benefits from
available previously annotated databases to achieve high accuracy results. Our
experiment disclosed the outstanding performances of the proposed technique in
terms of generalization, performance, and accuracy.",multimedia
http://arxiv.org/abs/2104.11598v1,to_check,arxiv,arxiv,2021-04-23 13:46:51+00:00,arxiv,"Reconstructing Speech from Real-Time Articulatory MRI Using Neural
  Vocoders",http://arxiv.org/abs/2104.11598v1,"Several approaches exist for the recording of articulatory movements, such as
eletromagnetic and permanent magnetic articulagraphy, ultrasound tongue imaging
and surface electromyography. Although magnetic resonance imaging (MRI) is more
costly than the above approaches, the recent developments in this area now
allow the recording of real-time MRI videos of the articulators with an
acceptable resolution. Here, we experiment with the reconstruction of the
speech signal from a real-time MRI recording using deep neural networks.
Instead of estimating speech directly, our networks are trained to output a
spectral vector, from which we reconstruct the speech signal using the WaveGlow
neural vocoder. We compare the performance of three deep neural architectures
for the estimation task, combining convolutional (CNN) and recurrence-based
(LSTM) neural layers. Besides the mean absolute error (MAE) of our networks, we
also evaluate our models by comparing the speech signals obtained using several
objective speech quality metrics like the mean cepstral distortion (MCD),
Short-Time Objective Intelligibility (STOI), Perceptual Evaluation of Speech
Quality (PESQ) and Signal-to-Distortion Ratio (SDR). The results indicate that
our approach can successfully reconstruct the gross spectral shape, but more
improvements are needed to reproduce the fine spectral details.",multimedia
http://arxiv.org/abs/2111.06276v1,to_check,arxiv,arxiv,2021-11-11 15:36:55+00:00,arxiv,"6D Pose Estimation with Combined Deep Learning and 3D Vision Techniques
  for a Fast and Accurate Object Grasping",http://arxiv.org/abs/2111.06276v1,"Real-time robotic grasping, supporting a subsequent precise object-in-hand
operation task, is a priority target towards highly advanced autonomous
systems. However, such an algorithm which can perform sufficiently-accurate
grasping with time efficiency is yet to be found. This paper proposes a novel
method with a 2-stage approach that combines a fast 2D object recognition using
a deep neural network and a subsequent accurate and fast 6D pose estimation
based on Point Pair Feature framework to form a real-time 3D object recognition
and grasping solution capable of multi-object class scenes. The proposed
solution has a potential to perform robustly on real-time applications,
requiring both efficiency and accuracy. In order to validate our method, we
conducted extensive and thorough experiments involving laborious preparation of
our own dataset. The experiment results show that the proposed method scores
97.37% accuracy in 5cm5deg metric and 99.37% in Average Distance metric.
Experiment results have shown an overall 62% relative improvement (5cm5deg
metric) and 52.48% (Average Distance metric) by using the proposed method.
Moreover, the pose estimation execution also showed an average improvement of
47.6% in running time. Finally, to illustrate the overall efficiency of the
system in real-time operations, a pick-and-place robotic experiment is
conducted and has shown a convincing success rate with 90% of accuracy. This
experiment video is available at https://sites.google.com/view/dl-ppf6dpose/.",multimedia
http://arxiv.org/abs/2006.00781v1,to_check,arxiv,arxiv,2020-06-01 08:14:10+00:00,arxiv,"Reducing the X-ray radiation exposure frequency in cardio-angiography
  via deep-learning based video interpolation",http://arxiv.org/abs/2006.00781v1,"Cardiac coronary angiography is a major technology to assist doctors during
cardiac interventional surgeries. Under the exposure of X-ray radiation,
doctors inject contrast agents through catheters to determine the position and
status of coronary vessels in real time. To get a coronary angiography video
with a high frame rate, the doctor needs to increase the exposure frequency and
intensity of the X-ray. This will inevitably increase the X-ray harm to both
patients and surgeons. In this work, we innovatively utilize a deep-learning
based video interpolation algorithm to interpolate coronary angiography videos.
Moreover, we establish a new coronary angiography image dataset ,which contains
95,039 triplets images to retrain the video interpolation network model. Using
the retrained network we synthesize high frame rate coronary angiography video
from the low frame rate coronary angiography video. The average peak signal to
noise ratio(PSNR) of those synthesized video frames reaches 34dB. Extensive
experiment results demonstrate the feasibility of using the video frame
interpolation algorithm to synthesize continuous and clear high frame rate
coronary angiography video. With the help of this technology, doctors can
significantly reduce exposure frequency and intensity of the X-ray during
coronary angiography.",multimedia
http://arxiv.org/abs/2104.04170v2,to_check,arxiv,arxiv,2021-04-09 02:58:59+00:00,arxiv,Stereo Matching by Self-supervision of Multiscopic Vision,http://arxiv.org/abs/2104.04170v2,"Self-supervised learning for depth estimation possesses several advantages
over supervised learning. The benefits of no need for ground-truth depth,
online fine-tuning, and better generalization with unlimited data attract
researchers to seek self-supervised solutions. In this work, we propose a new
self-supervised framework for stereo matching utilizing multiple images
captured at aligned camera positions. A cross photometric loss, an
uncertainty-aware mutual-supervision loss, and a new smoothness loss are
introduced to optimize the network in learning disparity maps end-to-end
without ground-truth depth information. To train this framework, we build a new
multiscopic dataset consisting of synthetic images rendered by 3D engines and
real images captured by real cameras. After being trained with only the
synthetic images, our network can perform well in unseen outdoor scenes. Our
experiment shows that our model obtains better disparity maps than previous
unsupervised methods on the KITTI dataset and is comparable to supervised
methods when generalized to unseen data. Our source code and dataset are
available at https://sites.google.com/view/multiscopic.",multimedia
http://arxiv.org/abs/2107.02174v2,to_check,arxiv,arxiv,2021-07-05 17:59:35+00:00,arxiv,What Makes for Hierarchical Vision Transformer?,http://arxiv.org/abs/2107.02174v2,"Recent studies indicate that hierarchical Vision Transformer with a macro
architecture of interleaved non-overlapped window-based self-attention \&
shifted-window operation is able to achieve state-of-the-art performance in
various visual recognition tasks, and challenges the ubiquitous convolutional
neural networks (CNNs) using densely slid kernels. Most follow-up works attempt
to replace the shifted-window operation with other kinds of cross-window
communication paradigms, while treating self-attention as the de-facto standard
for window-based information aggregation. In this manuscript, we question
whether self-attention is the only choice for hierarchical Vision Transformer
to attain strong performance, and the effects of different kinds of
cross-window communication. To this end, we replace self-attention layers with
embarrassingly simple linear mapping layers, and the resulting proof-of-concept
architecture termed as LinMapper can achieve very strong performance in
ImageNet-1k image recognition. Moreover, we find that LinMapper is able to
better leverage the pre-trained representations from image recognition and
demonstrates excellent transfer learning properties on downstream dense
prediction tasks such as object detection and instance segmentation. We also
experiment with other alternatives to self-attention for content aggregation
inside each non-overlapped window under different cross-window communication
approaches, which all give similar competitive results. Our study reveals that
the \textbf{macro architecture} of Swin model families, other than specific
aggregation layers or specific means of cross-window communication, may be more
responsible for its strong performance and is the real challenger to the
ubiquitous CNN's dense sliding window paradigm. Code and models will be
publicly available to facilitate future research.",multimedia
http://arxiv.org/abs/1911.07937v2,to_check,arxiv,arxiv,2019-10-31 09:14:28+00:00,arxiv,Inverse Graphics: Unsupervised Learning of 3D Shapes from Single Images,http://arxiv.org/abs/1911.07937v2,"Using generative models for Inverse Graphics is an active area of research.
However, most works focus on developing models for supervised and
semi-supervised methods. In this paper, we study the problem of unsupervised
learning of 3D geometry from single images. Our approach is to use a generative
model that produces 2-D images as projections of a latent 3D voxel grid, which
we train either as a variational auto-encoder or using adversarial methods. Our
contributions are as follows: First, we show how to recover 3D shape and pose
from general datasets such as MNIST, and MNIST Fashion in good quality. Second,
we compare the shapes learned using adversarial and variational methods.
Adversarial approach gives denser 3D shapes. Third, we explore the idea of
modelling the pose of an object as uniform distribution to recover 3D shape
from a single image. Our experiment with the CelebA dataset
\cite{liu2015faceattributes} proves that we can recover complete 3D shape from
a single image when the object is symmetric along one, or more axis whilst
results obtained using ModelNet40 \cite{wu20153d} show the potential
side-effects, in which the model learns 3D shapes such that it can render the
same image from any viewpoint. Forth, we present a general end-to-end approach
to learning 3D shapes from single images in a completely unsupervised fashion
by modelling the factors of variation such as azimuth as independent latent
variables. Our method makes no assumptions about the dataset, and can work with
synthetic as well as real images (i.e. unsupervised in true sense). We present
our results, by training the model using the $\mu$-VAE objective
\cite{ucar2019bridging} and a dataset combining all images from MNIST, MNIST
Fashion, CelebA and six categories of ModelNet40. The model is able to learn 3D
shapes and the pose in qood quality and leverages information learned across
all datasets.",multimedia
http://arxiv.org/abs/1312.5457v1,to_check,arxiv,arxiv,2013-12-19 09:40:03+00:00,arxiv,"Codebook based Audio Feature Representation for Music Information
  Retrieval",http://arxiv.org/abs/1312.5457v1,"Digital music has become prolific in the web in recent decades. Automated
recommendation systems are essential for users to discover music they love and
for artists to reach appropriate audience. When manual annotations and user
preference data is lacking (e.g. for new artists) these systems must rely on
\emph{content based} methods. Besides powerful machine learning tools for
classification and retrieval, a key component for successful recommendation is
the \emph{audio content representation}.
  Good representations should capture informative musical patterns in the audio
signal of songs. These representations should be concise, to enable efficient
(low storage, easy indexing, fast search) management of huge music
repositories, and should also be easy and fast to compute, to enable real-time
interaction with a user supplying new songs to the system.
  Before designing new audio features, we explore the usage of traditional
local features, while adding a stage of encoding with a pre-computed
\emph{codebook} and a stage of pooling to get compact vectorial
representations. We experiment with different encoding methods, namely
\emph{the LASSO}, \emph{vector quantization (VQ)} and \emph{cosine similarity
(CS)}. We evaluate the representations' quality in two music information
retrieval applications: query-by-tag and query-by-example. Our results show
that concise representations can be used for successful performance in both
applications. We recommend using top-$\tau$ VQ encoding, which consistently
performs well in both applications, and requires much less computation time
than the LASSO.",multimedia
http://arxiv.org/abs/2108.04602v1,to_check,arxiv,arxiv,2021-08-10 11:17:05+00:00,arxiv,"Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for
  Autonomous Driving",http://arxiv.org/abs/2108.04602v1,"Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results
of object detection, affinity computation and data association in real time.
This paper presents an efficient multi-modal MOT framework with online joint
detection and tracking schemes and robust data association for autonomous
driving applications. The novelty of this work includes: (1) development of an
end-to-end deep neural network for joint object detection and correlation using
2D and 3D measurements; (2) development of a robust affinity computation module
to compute occlusion-aware appearance and motion affinities in 3D space; (3)
development of a comprehensive data association module for joint optimization
among detection confidences, affinities and start-end probabilities. The
experiment results on the KITTI tracking benchmark demonstrate the superior
performance of the proposed method in terms of both tracking accuracy and
processing speed.",multimedia
http://arxiv.org/abs/2102.11931v3,to_check,arxiv,arxiv,2021-02-23 20:33:22+00:00,arxiv,"Boosting background suppression in the NEXT experiment through
  Richardson-Lucy deconvolution",http://arxiv.org/abs/2102.11931v3,"Next-generation neutrinoless double beta decay experiments aim for half-life
sensitivities of ~$10^{27}$ yr, requiring suppressing backgrounds to <1
count/tonne/yr. For this, any extra background rejection handle, beyond
excellent energy resolution and the use of extremely radiopure materials, is of
utmost importance. The NEXT experiment exploits differences in the spatial
ionization patterns of double beta decay and single-electron events to
discriminate signal from background. While the former display two Bragg peak
dense ionization regions at the opposite ends of the track, the latter
typically have only one such feature. Thus, comparing the energies at the track
extremes provides an additional rejection tool. The unique combination of the
topology-based background discrimination and excellent energy resolution (1%
FWHM at the Q-value of the decay) is the distinguishing feature of NEXT.
Previous studies demonstrated a topological background rejection factor of ~5
when reconstructing electron-positron pairs in the $^{208}$Tl 1.6 MeV double
escape peak (with Compton events as background), recorded in the NEXT-White
demonstrator at the Laboratorio Subterr\'aneo de Canfranc, with 72% signal
efficiency. This was recently improved through the use of a deep convolutional
neural network to yield a background rejection factor of ~10 with 65% signal
efficiency. Here, we present a new reconstruction method, based on the
Richardson-Lucy deconvolution algorithm, which allows reversing the blurring
induced by electron diffusion and electroluminescence light production in the
NEXT TPC. The new method yields highly refined 3D images of reconstructed
events, and, as a result, significantly improves the topological background
discrimination. When applied to real-data 1.6 MeV $e^-e^+$ pairs, it leads to a
background rejection factor of 27 at 57% signal efficiency.",multimedia
10.1016/j.eswa.2021.115484,to_check,Expert Systems with Applications,scopus,2021-12-01,sciencedirect,Acoustic recognition of noise-like environmental sounds by using artificial neural network,https://api.elsevier.com/content/abstract/scopus_id/85110098961,"In spite of establishing audio perception techniques and tremendous progress of computer capabilities, artificial perception of sound videlicet natural ability to hear is at the initial stage. The latest phenomena in the evolution of expert systems, like internet of things, put a focus to the mass applications that run on embedded platforms often with pure computational capacity. Modern sensing applications require simplicity, universality, and excellent performance. Insects, obviously, realize limited but satisfactory interaction with the environment using minimum resources. The experiment is motivated by the assumption that the similar principle of control can be applied in artificial control systems. In such a manner, theoretical and practical research was conducted in order of defining optimal procedure for the recognition of short, cognitively unpretentious noise-like sounds, in real conditions, on the basis of previous experience. The result is optimal hybrid procedure for the recognition of noise-like environmental sounds built of heuristic algorithms completely. The experiment reports the success in recognizing unfavourable sounds using frequency spectrum as feature vector. The ability of abstraction was tested by employing samples of different abstraction level and robustness with respect to white noise and confusing sounds. Optimal preprocessing was suggested for the improving accuracy, employing white noise and confusing sounds for estimating preprocessing parameters. Built of ultimate algorithms, the procedure is useful for a broad range of research. This is of exceptional importance because acoustic perception in its full complexity can be approached only if the problem is observed multidisciplinary.",multimedia
10.1016/j.aei.2021.101362,to_check,Advanced Engineering Informatics,scopus,2021-10-01,sciencedirect,Teaching and Learning Crystal structures through Virtual Reality based systems,https://api.elsevier.com/content/abstract/scopus_id/85112528485,"Teaching and learning through Virtual Reality(VR) is an emerging technology in the last few years. In this article, the development and use of a VR based teaching–learning system for crystal structures are discussed. The VR system is designed as a lab environment where a user can do experiments related to crystal structures. The VR system is designed in Unity,
                        1
                     
                     
                        1
                        
                           https://www.unity.com/.
                      and Oculus Rift S
                        2
                     
                     
                        2
                        
                           https://www.oculus.com/rift-s/.
                      is used as a VR headset. Currently, the system consists of three phases; in the first phase user can visualize the crystal lattice structures, wherein the second one a user can visualize the light interaction with the crystal lattice structure using a virtual torch ray. The third phase is the X-ray Diffraction (XRD) experiment. In this phase, users can perform the XRD experiment in the lab environment by taking a random crystal from a crystal dispenser machine and placing it in the X-ray machine which identifies the chosen crystalline material and analyses the unit cell. The incident ray colour changes when there is a peak found in the crystal for a better understanding of the user. There is also an interactive display where users can increase/decrease the angles of the radiation and also lock and unlock the experiment to view the diffraction plot for the crystal structure. In many cases, it was found that XRD and the crystal structure is available in the course syllabus but there are no experiments to enhance their learning. Therefore an experiment with 39 participants was performed where the maximum participants are new to crystallography. The study was conducted in two phases; in the first phase, participants are asked to watch video tutorials of the topic followed by questionnaires; in the second phase participants are asked to do the VR based experiment and followed by questionnaires related to overall study and experiment. From the analysis of the study we found that everyone found VR based teaching methods are better than traditional book/video studies. Study results give an average score of 56.74% in comparison to VR based learning approach with an average score of 93.81%. Participants who took part in the experiment found the experience interactive and motivating and found it helpful to learn elusive concepts, which can be learned when simulated. For example, one participant wrote: “The VR experience was surreal and was easy to control. Lucid user experience. Got a view of XRD like never seen before”.",multimedia
10.1016/j.buildenv.2021.108036,to_check,Building and Environment,scopus,2021-09-01,sciencedirect,Vision-based estimation of clothing insulation for building control: A case study of residential buildings,https://api.elsevier.com/content/abstract/scopus_id/85107702255,"Efforts have been made to estimate clothing insulation in real time, an element of thermal comfort for occupants. Nevertheless, an effective method to estimate clothing insulation in real time is lacking. In addition, there has been little debate on how to apply clothing insulation to building control in practice. The purpose of this study is to propose a method for estimating clothing insulation using deep learning-based vision recognition, which has recently attracted attention and implement building control based on clothing insulation. The study also evaluates the significance of the method in effective building control. The results demonstrated that the proposed framework, CloNet, showed an accuracy of 94% for the validation image dataset and 86% for the actual built environment. In addition, we proved that the proposed vision-based estimation method is very fast and practical for estimating clothing insulation. The control experiment showed that the CloNet-based predicted mean vote (PMV) control changed the set temperature in response to changes in the subject's clothing. Compared to the traditional PMV control, the CloNet-based PMV control improved the thermal preference and thermal comfort vote. These results prove that clothing insulation estimation can be useful for building control.",multimedia
10.1016/j.comcom.2021.03.015,to_check,Computer Communications,scopus,2021-08-01,sciencedirect,Precise grabbing of overlapping objects system based on end-to-end deep neural network,https://api.elsevier.com/content/abstract/scopus_id/85108697361,"In recent years, robotic arm technology is in dire need of reform because of the remarkable advances in artificial intelligence and computer vision. The traditional robotic arm techniques, e.g., template matching algorithm and iterative closest point algorithm, suffer from the low precision issue, especially when the target objects overlap with each other, resulting in inaccurate estimation of overlapping objects. This paper proposes a precise grabbing of overlapping objects system based on an end-to-end deep neural network. The successful grabbing is realized in the case of overlapping objects. First, the datasets needed for network training were established, utilizing structured light to obtain the point cloud information of the arbitrarily placed target objects. Furthermore, we collect the corresponding postures as data labels via the teaching device of the robotic arm, and train the network models using the datasets and labels. Finally, we can predict the postures of the target objects in real time and transmit the results to a robotic arm to complete the grabbing work. The experiment results indicate that the proposed grabbing system can grab small irregular objects accurately, only using the point cloud information, estimating the posture of multiple target objects in the scene simultaneously, and estimating the posture of overlapping small objects in the scene.",multimedia
10.1016/j.robot.2021.103775,to_check,Robotics and Autonomous Systems,scopus,2021-07-01,sciencedirect,6D pose estimation with combined deep learning and 3D vision techniques for a fast and accurate object grasping,https://api.elsevier.com/content/abstract/scopus_id/85102869529,"Real-time robotic grasping, supporting a subsequent precise object-in-hand operation task, is a priority target towards highly advanced autonomous systems. However, such an algorithm which can perform sufficiently-accurate grasping with time efficiency is yet to be found. This paper proposes a novel method with a 2-stage approach that combines a fast 2D object recognition using a deep neural network and a subsequent accurate and fast 6D pose estimation based on Point Pair Feature framework to form a real-time 3D object recognition and grasping solution capable of multi-object class scenes. The proposed solution has a potential to perform robustly on real-time applications, requiring both efficiency and accuracy. In order to validate our method, we conducted extensive and thorough experiments involving laborious preparation of our own dataset. The experiment results show that the proposed method scores 97.37% accuracy in 5cm5deg metric and 99.37% in Average Distance metric. Experiment results have shown an overall 62% relative improvement (5cm5deg metric) and 52.48% (Average Distance metric) by using the proposed method. Moreover, the pose estimation execution also showed an average improvement of 47.6% in running time. Finally, to illustrate the overall efficiency of the system in real-time operations, a pick-and-place robotic experiment is conducted and has shown a convincing success rate with 90% of accuracy. This experiment video is available at https://sites.google.com/view/dl-ppf6dpose/.",multimedia
10.1016/j.imu.2021.100544,to_check,Informatics in Medicine Unlocked,scopus,2021-01-01,sciencedirect,A framework for automatic hand range of motion evaluation of rheumatoid arthritis patients,https://api.elsevier.com/content/abstract/scopus_id/85102265736,"We propose a framework for evaluation of finger movement patterns on Rheumatoid Arthritis patients: flexion, extension, abduction and adduction. The framework uses a state-of-the-art 3D hand pose estimation method that runs in real-time, allowing users to visualize 3D skeleton tracking results at the same time as the depth images are acquired. We compute flexion and abduction angles from the obtained skeleton pose parameters. We performed data acquisition from a cohort of patients and a control set and compared the angles from those two sets of people. An analysis using time series similarity with frequency domain descriptors is adopted to characterize the movement patterns for flexion/extension. We performed classification experiments using these descriptors, thus distinguishing movement sequences of hands with rheumatoid arthritis from healthy hands. The descriptors used in the classification experiment were effective and reached average results of 
                        
                           89
                           %
                        
                      in scenarios of unseen subjects, and an average of 
                        
                           82
                           %
                        
                      in experiments with sample synthesis that allow a more robust statistical performance evaluation. Our framework allows the characterization of the current state of the disorder in each patient, with minimal intervention and reduced evaluation time.",multimedia
10.1016/j.compbiomed.2020.103886,to_check,Computers in Biology and Medicine,scopus,2020-08-01,sciencedirect,Skull shape reconstruction using cascaded convolutional networks,https://api.elsevier.com/content/abstract/scopus_id/85086987267,"Designing a cranial implant to restore the protective and aesthetic function of the patient’s skull is a challenging process that requires a substantial amount of manual work, even for an experienced clinician. While computer-assisted approaches with various levels of required user interaction exist to aid this process, they are usually only validated on either a single type of simple synthetic defect or a very limited sample of real defects. The work presented in this paper aims to address two challenges: (i) design a fully automatic 3D shape reconstruction method that can address diverse shapes of real skull defects in various stages of healing and (ii) to provide an open dataset for optimization and validation of anatomical reconstruction methods on a set of synthetically broken skull shapes.
                  We propose an application of the multi-scale cascade architecture of convolutional neural networks to the reconstruction task. Such an architecture is able to tackle the issue of trade-off between the output resolution and the receptive field of the model imposed by GPU memory limitations. Furthermore, we experiment with both generative and discriminative models and study their behavior during the task of anatomical reconstruction.
                  The proposed method achieves an average surface error of 
                        
                           0
                           .
                           59
                           
                           mm
                        
                      for our synthetic test dataset with as low as 
                        
                           0
                           .
                           48
                           
                           mm
                        
                      for unilateral defects of parietal and temporal bone, matching state-of-the-art performance while being completely automatic. We also show that the model trained on our synthetic dataset is able to reconstruct real patient defects.",multimedia
10.1016/j.dsp.2020.102756,to_check,Digital Signal Processing: A Review Journal,scopus,2020-07-01,sciencedirect,Faster-YOLO: An accurate and faster object detection method,https://api.elsevier.com/content/abstract/scopus_id/85084358845,"In the computer vision, object detection has always been considered one of the most challenging issues because it requires classifying and locating objects in the same scene. Many object detection approaches were recently proposed based on deep convolutional neural networks (DCNNs), which have been demonstrated to achieve outstanding object detection performance compared to other approaches. However, the supervised training of DCNNs mostly uses gradient-based optimization criteria, in which all parameters of hidden layers require multiple iterations, and often faces some problems such as local minima, intensive human intervention, time-consuming, etc. In this paper, we propose a new method called Faster-YOLO, which is able to perform real-time object detection. The deep random kernel convolutional extreme learning machine (DRKCELM) and double hidden layer extreme learning machine auto-encoder (DLELM-AE) joint network is used as a feature extractor for object detection, which integrating the advantages of ELM-LRF and ELM-AE. It takes the raw images directly as input and thus is suitable for the different datasets. In addition, most connection weights are randomly generated, so there are few parameter settings and training speed is faster. The experiment results on Pascal VOC dataset show that Faster-YOLO improves the detection accuracy effectively by 1.1 percentage points compared to the original YOLOv2, and an average 2X speedup compared to YOLOv3.",multimedia
10.1016/j.autcon.2019.103016,to_check,Automation in Construction,scopus,2020-02-01,sciencedirect,Full body pose estimation of construction equipment using computer vision and deep learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85075282634,"Construction sites are among the most hazardous places with various safety issues. The high rate of hazards on construction sites can be attributed to the dynamic and complex characteristics of construction-related entities, such as the movement of construction equipment and workers as well as the interactions among them. Tracking construction equipment and workers can help avoid potential collisions and other accidents to achieve safer on-site conditions. As construction equipment (e.g. excavators, trucks, cranes, and bulldozers) plays a significant role in construction projects, it is important to track the location, pose and movement of construction equipment. Currently, with the wide installation of surveillance cameras on construction sites, computer vision techniques are explored to process the captured surveillance videos and images, such as to monitor the site conditions and prevent potential hazards. Previous studies have attempted to identify and locate different types of construction equipment on construction sites based on surveillance videos using computer vision techniques. However, there are limited studies that automatically estimate the full body pose and movement of on-site construction equipment, which can greatly influence the safety condition of construction sites and the utilization of the equipment itself.
                  In this study, a methodology framework is developed for automatically estimating the poses of different construction equipment in videos captured on construction sites using computer vision and deep learning techniques. Firstly, keypoints of equipment are defined, based on which the images collected from the surveillance cameras are annotated to generate the ground truth labels. 70%, 10%, and 20% of the annotated image dataset are used for training, validation and testing, respectively. Then, the architectures of three types of deep learning networks i.e. Stacked Hourglass Network (HG), Cascaded Pyramid Network (CPN), and an ensemble model (HG-CPN) integrating Stacked Hourglass and Cascaded Pyramid Network are constructed and trained in the same training environment. After training, the three models are evaluated on the testing dataset in terms of normalized errors (NE), percentage of correct keypoints (PCK), area under the curve (AUC), detection speed, and training time. The experiment results demonstrate the promising performance of our proposed methodology framework for automatically estimating different full body poses of construction equipment with high accuracy and fast speed. It is indicated by experiments that both HG and CPN can achieve relative high accuracy, with a PCK value of 91.19% and 91.78% respectively for estimating the equipment full body poses. In addition, the ensemble model with online data augmentation can further improve the accuracy, achieving a NE of 14.57 × 10−
                     3, a PCK of 93.43%, and an AUC of 39.72 × 10−
                     3 at the detection speed of 125 millisecond (ms) per image. This study lays the foundation for applying computer vision and deep learning techniques in the full body pose estimation of construction equipment, which can contribute to the real-time safety monitoring on construction sites.",multimedia
10.1109/I2MTC.2018.8409699,to_check,2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),IEEE,2018-05-17 00:00:00,ieeexplore,A new method for the indicator of dynamic scheduling in life science laboratories using artificial neural networks,https://ieeexplore.ieee.org/document/8409699/,"Nowadays, a growing number of automation systems is applied in life science laboratories to increase the throughput and accuracy of the experiments. Meanwhile, complex investigations require cooperative work, which includes multiple automated workstations. In order to face the great challenge of the resource distribution, a hybrid scheduler is used to generate the time schedule before the execution and after an unexpected situation for the dynamic scheduling. The indicator, presented in this paper, can trigger the rescheduling procedure when the trigger conditions are satisfied and offer the transportation balance factor, which is the most important parameter for dynamic scheduling. An artificial neural network (ANN) is proposed to learn the relationship between the system operation data and the transportation balance factor. Then the ANN can forecast the balance factor fast and accurately based on the real current data of the system operation. Finally, an experiment proves that the proposed method is suitable to trigger the dynamic scheduling and accurate to generate the balance factor for the rescheduling, which provides a correct solution based on the unexpected situation.",science
10.1109/IGARSS.2012.6350750,to_check,2012 IEEE International Geoscience and Remote Sensing Symposium,IEEE,2012-07-27 00:00:00,ieeexplore,An autonomous robotic platform for ground penetrating radar surveys,https://ieeexplore.ieee.org/document/6350750/,"Detection of hidden surface crevasses on glaciers is a vital process involved in over-snow traverses for science and resupply missions in Polar regions. There are several areas warranting improvement in the current protocol for crevasse detection, which employs a human-operated ground penetrating radar (GPR) on a mid-weight tracked vehicle. In this fashion, a GPR scout team must plan an appropriate crevasse-free route by investigating paths across the glacier. This paper presents methods supporting a completely autonomous robotic system employing GPR probing of the glacier surface. We tested and evaluated three machine learning algorithms on post-processed Antarctic GPR data, collected by our robot and a Pisten Bully in 2009 and 2010 at McMurdo Station. We achieved 82% classification rate for a linear SVM, compared to 82% using logistic regression and 80% using a Bayes network for contrast. We also discuss independent versus sequential classification of GPR scans, and suggest improvements to or combinations of the most successful training models. Our experiment demonstrates the promise and reliability of real-time object detection with GPR.",science
10.1109/ICeLeTE.2013.6644359,to_check,2013 Second International Conference on E-Learning and E-Technologies in Education (ICEEE),IEEE,2013-09-25 00:00:00,ieeexplore,Clustering moodle data as a tool for profiling students,https://ieeexplore.ieee.org/document/6644359/,"This paper describes the first step of a research project with the aim of predicting students' performance during an online curriculum on a LMS and keeping them from falling behind. Our research project aims to use data mining, machine learning and artificial intelligence methods for monitoring students in e-learning trainings. This project takes the shape of a partnership between computer science / artificial intelligence researchers and an IT firm specialized in e-learning software. We wish to create a system that will gather and process all data related to a particular e-learning course. To make monitoring easier, we will provide reliable statistics, behaviour groups and predicted results as a basis for an intelligent virtual tutor using the mentioned methods. This system will be described in this article. In this step of the project, we are clustering students by mining Moodle log data. A first objective is to define relevant clustering features. We will describe and evaluate our proposal. A second objective is to determine if our students show different learning behaviours. We will experiment whether there is an overall ideal number of clusters and whether the clusters show mostly qualitative or quantitative differences. Experiments in clustering were carried out using real data obtained from various courses dispensed by a partner institute using a Moodle platform. We have compared several classic clustering algorithms on several group of students using our defined features and analysed the meaning of the clusters they produced.",science
10.1109/ACCESS.2020.3025229,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,A Design of Smart Beaker Structure and Interaction Paradigm Based on Multimodal Fusion Understanding,https://ieeexplore.ieee.org/document/9200490/,"Virtual chemistry experiment is an important teaching tool in middle schools. It can not only help students understand the experimental principles, but also help them memorize the experimental procedures. However, there are still some problems in virtual chemistry experiments. First, in existing research, user intentions are often misunderstood and cannot be accurately understood. Second, the existing research fail to identify the user's wrong actions, which reduce the accuracy of the experiment. Third, the user sense of operation and realism is not strong during the experiment, which reduces the user's experience. Finally, the lack of navigation guidance for experimental operations increases user learning time. In order to solve these problems. This paper proposes a scheme for establishing an intelligent navigational chemical laboratory based on multimodal fusion. First of all, we design a new smart beaker structure with perceptual ability, which can be used to complete most chemical experiments and give users a real sense of experience. Besides, we propose a multimodal fusion understanding algorithm, which reduces the misidentification of the experiment and better understands the real intention of the user. Finally, intelligent navigation and wrong behavior recognition functions are added to the experimental equipment, which improves the efficiency of human-computer interaction. The results show that Compared with the existing virtual laboratory or system, the chemical laboratory scheme proposed in this paper through multimodal fusion understanding algorithm greatly reduces the user's memory load and improves the success rate of the experiment. Moreover, through the combination of virtual and real, the virtual chemistry experiment not only improves the authenticity of the operation, but also stimulates the students' interest in learning, which is well received by users.",science
10.1109/ICNN.1996.548982,to_check,Proceedings of International Conference on Neural Networks (ICNN'96),IEEE,1996-06-06 00:00:00,ieeexplore,Comparison of brain structure to a backpropagation-learned-structure,https://ieeexplore.ieee.org/document/548982/,"This paper describes the results of experiments studying the circumstances under which an error-minimizing artificial neural network mimics the mammal visual system. The networks were trained to recognize handwritten-digits. The experiment was not intended to yield a high identification accuracy, but rather to generate a comparison of the neural networks to biology under different circumstances. Experiments were conducted with partially hand-set networks, freely-trained networks, and convolutionally constrained networks. The convolutional experiment, where certain weights were constrained to be identical, performed the best at digit recognition while also modeling parts of biology that we had not anticipated the network would model. Rather than using the input image to generate an edge detection outline, as occurs in the retina, the network's first layer modeled the cones themselves, reacting most to one color (black or white), but not performing any real processing.",science
10.1109/UIC-ATC.2013.85,to_check,2013 IEEE 10th International Conference on Ubiquitous Intelligence and Computing and 2013 IEEE 10th International Conference on Autonomic and Trusted Computing,IEEE,2013-12-21 00:00:00,ieeexplore,An Automatic Face Annotation System Featuring High Accuracy for Online Social Networks,https://ieeexplore.ieee.org/document/6726205/,"The development of fully automatic face annotation techniques in online social networks (OSNs) is currently very important for effective management and organization of the large numbers of personal photos shared on social network platforms. In this paper, we construct the personalized and adaptive Fused Face Recognition unit for each member, which uses the Adaboost algorithm to fuse several different types of base classifiers to produce highly reliable face annotation results. The experiment results demonstrate that our proposed approach achieves a significantly higher level of efficacy, outperforming other state-of-the-art face annotation methods for real-life personal photos featuring pose variations. Our evaluation methodologies produced respective F-measure and Similarity accuracy rates that were 57.99% and 54.23% higher for the proposed method in comparison to other tested methods.",science
10.1109/SOLI48380.2019.8955010,to_check,"2019 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)",IEEE,2019-11-08 00:00:00,ieeexplore,Iterative Adaptive Dynamic Programming Adviced Campus Scale Social Energy System Management,https://ieeexplore.ieee.org/document/8955010/,"Demand side management (DSM) schemes are becoming more and more sophisticated in modern power systems, in which energy consumption should be properly planned and controlled, together with social elements such as consumer experience, environmental influences, and management policy and rules. In this paper, inspired by the concept of “social energy”, we incorporate user experience evaluation, real-time electricity price calculation and energy consumption prediction into a unified campus-scale energy management system, to investigate the interactions among energy and social elements. The University of Denver's campus grid is used as the test bench, and energy consumption data were collected from 6 on-campus buildings to conduct the numerical experiments. Specifically, to deal with the “optimization-over-optimization” problem introduced by real-time pricing, and to tackle the high complexity problem brought by dynamic building consumption patterns, we implement neural network based iterative adaptive dynamic programming algorithm in this complex system to solve for the best overall system social cost. Experiment results show that the proposed algorithm is able to provide reasonable energy management advice in a practical socio-technical energy system.",science
10.1109/ICBASE51474.2020.00055,to_check,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE,2020-11-01 00:00:00,ieeexplore,Prediction of Trust Propagation in Social Commerce based on Ensemble Learning,https://ieeexplore.ieee.org/document/9403760/,"Accurate prediction of trust propagation in social commerce is vital to recommendation and promotion of commodities. Existing prediction models have some shortages, such as simple process of influencing factors of trust and low prediction precision. To address these problems, a prediction model based on Soft-Voting ensemble learning was proposed. Firstly, features of influencing factors of information propagation in social commerce were constructed from user attributes, information text and user interaction. Secondly, XGBoost, LightGBM and Catboost models were trained according to the above constructed features to predict trust propagation in social commerce. Finally, results of XGBoost, LightGBM and Catboost models were integrated using Soft-Voting technique as the final prediction results. An experiment on a real dataset of Sina Weibo was carried out, which proved the higher precision of Soft-Voting ensemble learning compared to those of XGBoost, LightGBM and Catboost models.",science
10.1109/ACCESS.2019.2929313,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,A Multi-Element Hybrid Location Recommendation Algorithm for Location Based Social Networks,https://ieeexplore.ieee.org/document/8764552/,"In the environment of data explosion, how to make an effective and accurate personalized point of interest (POI) recommendation in location-based social networks (LBSNs) is a challenging and meaningful task. Fortunately, there is a lot of information that we can use. We can make recommendations by mining the rich information hidden in user check-in records. In this paper, we propose a recommend system named GFP-LORE. Specifically, we have designed a framework, which integrates various influencing factors. First, we modeled friend sign-in frequencies and POI popularity as a power-law distribution and the experiment proved that it is effective. Then, we got the influence of geographic information by modeling it according to the longitude and latitude of the user's check-in location. After that, we sorted the historical check-in records of all users according to time and then mine an overall pattern of location transfer-order pattern. Then, we combine it with each user's own unique location transfer record to get the possibility of the user going to the next POI. Finally, we synthesize the above four influence factors into a unified correlation probability rating and recommend a new location by this probability rating. We tested our system on the open real check-in data set, and the results of our simulation experiments show that the recommendation effect of our system is better than those algorithms used in the contrast test.",science
10.1109/ACCESS.2021.3113320,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,A Pitman-Yor Process Self-Aggregated Topic Model for Short Texts of Social Media,https://ieeexplore.ieee.org/document/9540607/,"In recent years, with the rapid growth of social media, short texts have been very prevalent on the internet. Due to the limited length of each short text, word co-occurrence information in this type of documents is sparse. Conventional topic models based on word co-occurrence are unable to distill coherent topics on short texts. A state-of-the-art strategy is self-aggregated topic models which implicitly aggregate short texts into latent long documents. But these models have two problems. One problem is that the number of long documents should be defined explicitly and the inappropriate number leads to poor performance. Another problem is that latent long documents may bring non-semantic word co-occurrence which brings incoherent topics. In this article, we firstly apply the Chinese restaurant process to automatically generate the number of long documents according to the scale of short texts. Then to exclude non-semantic word co-occurrence, we propose a novel probabilistic model generating latent long documents in a more semantically way. Specifically, our model employs a pitman-yor process to aggregate short texts into long documents. This stochastic process can guarantee that the distribution between short texts and long documents following a power-law distribution which can be found in social media like Twitter. Finally, we compared our method with several state-of-the-art methods on four real short texts corpus. The experiment results show that our model performs superior to other methods with the metrics of topic coherence and text classification.",science
10.1109/TDSC.2016.2522436,to_check,IEEE Transactions on Dependable and Secure Computing,IEEE,2018-02-01 00:00:00,ieeexplore,Rumor Source Identification in Social Networks with Time-Varying Topology,https://ieeexplore.ieee.org/document/7393814/,"Identifying rumor sources in social networks plays a critical role in limiting the damage caused by them through the timely quarantine of the sources. However, the temporal variation in the topology of social networks and the ongoing dynamic processes challenge our traditional source identification techniques that are considered in static networks. In this paper, we borrow an idea from criminology and propose a novel method to overcome the challenges. First, we reduce the time-varying networks to a series of static networks by introducing a time-integrating window. Second, instead of inspecting every individual in traditional techniques, we adopt a reverse dissemination strategy to specify a set of suspects of the real rumor source. This process addresses the scalability issue of source identification problems, and therefore dramatically promotes the efficiency of rumor source identification. Third, to determine the real source from the suspects, we employ a novel microscopic rumor spreading model to calculate the maximum likelihood (ML) for each suspect. The one who can provide the largest ML estimate is considered as the real source. The evaluations are carried out on real social networks with time-varying topology. The experiment results show that our method can reduce 60 - 90 percent of the source seeking area in various time-varying social networks. The results further indicate that our method can accurately identify the real source, or an individual who is very close to the real source. To the best of our knowledge, the proposed method is the first that can be used to identify rumor sources in time-varying social networks.",science
10.1109/ACCESS.2017.2744646,to_check,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,User Identification Based on Display Names Across Online Social Networks,https://ieeexplore.ieee.org/document/8016580/,"User identification is very helpful for building a better profile of a user. Some works have been devoted to this issue. However, the existing works with a good performance are mainly based on the rich online data and do not consider the cost of online data acquisition. In this paper, we aim to address this issue with a lower cost of data acquisition. A machine learning-based solution is proposed solely based on the user's display names. It consists of three key steps: we first analyze the users' unique naming patterns that lead to information redundancies across sites; second, we construct features that exploit information redundancies; afterward, we employ machine learning method for user identification. The experiment shows that the proposed solution can provide excellent performance with F1 score reaching 96.24%, 92.49%, and 90.68% on three real different data sets, respectively. This paper shows the possibility of user identification with a lower cost of data acquisition.",science
10.1109/BDCloud.2014.84,to_check,2014 IEEE Fourth International Conference on Big Data and Cloud Computing,IEEE,2014-12-05 00:00:00,ieeexplore,Personalized Recommender System on Whom to Follow in Twitter,https://ieeexplore.ieee.org/document/7034812/,"Recommender systems have been widely used in social network sites. In this paper, we propose a novel approach to recommend new followees to Twitter users by learning their historic friends-adding patterns. Based on a user's past social graph and her interactions with other users, scores based on some of the commonly used recommendation strategies are calculated and passed into the learning machine along with the recently added list of followees of the user. Learning to rank algorithm then identifies the best combination of recommendation strategies the user adopted to add new followees in the past. Although users may not adopt any recommendation strategies explicitly, they may subconsciously or implicitly use some. If the actually added followees match with the ones suggested by the recommendation strategy, we consider users are implicitly using that strategy. The experiment using the real data collected from Twitter proves the effectiveness of the proposed approach.",science
10.1109/SIBGRAPI.2017.24,to_check,"2017 30th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)",IEEE,2017-10-20 00:00:00,ieeexplore,Real-Time Single-Shot Brand Logo Recognition,https://ieeexplore.ieee.org/document/8097304/,"The amount of data produced every day on the internet increases every day and with the increasing popularity of the social networks the number of published photos are huge, and those pictures contain several implicit or explicit brand logos. Detecting this logos in natural images can provide information about how widespread is a brand, discover unwanted copyright distribution, analyze marketing campaigns, etc. In this paper, we propose a real-time brand logo recognition system that outperforms all other state-of-the-art in two different datasets. Our approach is based on the Single Shot MultiBox Detector (SSD), we explore this tool in a different domain and also experiment the impact of training with pretrained weights and the impact of warp transformations in the input images. We conducted our experiments in two datasets, the FlickrLogos-32 (FL32) and the Logos-32Plus (L32plus), which is an extension of the training set of the FL32. On the FL32, we outperform the state-of-the-art by 2.5% the F-score and by 7.4% the recall. For the L32plus, we surpass the state-of-the-art by 1.2% the F-score and by 3.8% the recall.",science
10.1109/ASONAM49781.2020.9381325,to_check,2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),IEEE,2020-12-10 00:00:00,ieeexplore,TrustGCN: Enabling Graph Convolutional Network for Robust Sybil Detection in OSNs,https://ieeexplore.ieee.org/document/9381325/,"Detecting fake accounts (also called Sybils) is a fundamental security problem in online social networks (OSNs). Existing feature-based or social-graph-based approaches suffer from the key limitations: they can only leverage either node feature or graph structure properties such as fast-mixing and conductance, but not both. To overcome this shortcoming, we explore the introduction of recent advancements in deep neural networks for graph-structured data into Sybil detection field. These types of models enable integrating both user-level activities and graph-level structures for a new generation of feature-and-graph-based detection mechanisms. However, we find that although applying Graph Convolutional Networks (GCNs) are effective against naïve attacks, they are vulnerable to adversarial attacks in which fake accounts alter local edges and features with patterns to resemble real users. In this paper, we present TrustGCN, a Sybil-resilient defense algorithm that combines the idea of social-graph-based defense with GCN. TrustGCN first assigns trust scores to nodes based on the landing probability of short random walks that starts from known real accounts. As this short, supervised random walk is likely to stay within the subgraph consisting of real accounts, most real accounts receive higher trust scores than fakes. Then it introduces these trust scores as edge weights and adopts graph convolution operations to aggregate features of local graph neighborhoods over this weighted graph for classification. In this way, we prevent Sybil partners with low trust scores from contributing to the feature aggregation for a target node, thus is more robust against adverse manipulations of the attackers. Our experiment on real data demonstrates that TrustGCN significantly outperforms GCN in the robustness. To the best of our knowledge, this is the first attempt to combine social-graph-based defenses with graph neural networks into a unified model, paving the way for the robust feature-and-graph-based detection mechanisms.",science
10.1109/DSC.2019.00108,to_check,2019 IEEE Fourth International Conference on Data Science in Cyberspace (DSC),IEEE,2019-06-25 00:00:00,ieeexplore,Understanding Acdamic Impact Development by Predicting the G-index In Collaboration Networks,https://ieeexplore.ieee.org/document/9069370/,"Academic impact is an important factor in the research assessment of a scholar. The prediction of the individual future academic impact is beneficial to detecting talented scholars and allocating the academic resources more reasonably. Recently, many evaluation indicators based on academic impact have been proposed, among which the G-index is more objective than the citations and H-index that used more frequently. At present, the main contradiction is formed between the great values of the individual future academic impact in practice and the imperfect evaluation and prediction methods. In order to solve this contradiction, this paper chooses the objective and comprehensive evaluation indicator G-index to measure individual academic impact and proposes the solution of how to predict the G-index at a given time interval for the first time in relevant research fields. Firstly, different characteristic features are extracted based on the structure of the academic social network. Then, a prediction model based on deep learning is proposed to learn the function mapping relationship between the future G-index time with the current feature set. Finally, the proposed model is validated with real academic social network dataset. The experiment results show that the individual G-index development is tightly related to the scholar's characteristic features in the current academic social network and the deep learning model performs well in predicting the future G index.",science
10.1109/ACCESS.2017.2764750,to_check,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,A Novel Centrality Cascading Based Edge Parameter Evaluation Method for Robust Influence Maximization,https://ieeexplore.ieee.org/document/8076829/,"The research of social influence is an important topic in online social network analysis. Influence maximization is the problem of finding k nodes that maximize the influence spread in a specific social network. Robust influence maximization is a novel topic that focuses on the uncertainty factors among the influence propagation models and algorithms. It aims to find a seed set with a definite size that has robust performance with different influence functions under various uncertainty factors. In this paper, we propose a centrality-based edge activation probability evaluation method in the independent cascade model. We consider four different types of centrality measurement methods and add a modification coefficient to evaluate the edge probability. We also propose two algorithms, called NewDiscount and GreedyCIC, by incorporating the edge probability space into previous algorithms. With extensive experiments on various real online social network data sets, we find that our PageRank-based greedy algorithm has the best influence spreads and lowest running times, compared with other algorithms, on some large data sets. The experiment for evaluating the robustness performance shows that all algorithms have optimal robustness performance when the modification coefficient is set to 0.01 under the independent cascade model. This result suggests some further research directions under this model.",science
10.1109/ACCESS.2020.3022836,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Detecting and Evolving Microblog Community Based on Structure and Gravity Cohesion,https://ieeexplore.ieee.org/document/9193891/,"Microblogs are open and real-time online social network platforms used by people to make posts about their moods, experiences, and interests. It will be very significant to gather microblog users who have similar interests and hobbies into the same community. In this paper, we propose novel approaches for detecting and evolving dynamic microblog communities. First, inspired by the universal gravitation law, we redefine the gravitation relationships among microblog users. Based on the structure of the microblog social network, we define the basic nodes and their gravity tendency and propose the microblog community detection algorithm. Second, we determine the community changes in the microblog social networks at times $t$ and $t+1$ and propose a microblog community evolution algorithm. Third, we define the mutual transformation probability between communities at times $t$ and $t+1$ and propose the microblog community evolution behavior algorithm. The experiment includes a comparison and evaluation of the microblog community detection, evolution, and behavior extraction algorithms and the optimal ranges of the parameters involved in these algorithms. The experimental results indicate that our proposed algorithms have good performance compared to other benchmarking methods.",science
10.1109/ACCESS.2020.2968220,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,Dtree2vec: A High-Accuracy and Dynamic Scheme for Real-Time Book Recommendation by Serialized Chapters and Local Fine-Grained Partitioning,https://ieeexplore.ieee.org/document/8970611/,"With the rapid development of social computing technologies and online reading platforms, the proportion of e-books, especially online serialized novels, has been increasing. Identifying ways to add updated serialized books to readers' real-time recommendation lists has become an urgent problem to be solved. While serialized books are still underwritten and in the process of production, their features and categories can constantly evolve and change, lacking complete text content information and complete global scoring data. Therefore, this paper proposes a dynamic DTree2Vec scheme for serialized books that models text of varying degrees of completion to achieve unified semantic feature representation to measure the semantic relevance of books. At the same time, it ensures recommendation quality by tracking the update status of serialized books and by adding the subsequent chapter content in real time. This scheme establishes a dynamic hierarchical tree structure for serialized books and applies a cosine-type local reconstruction model to reconstruct new semantic features of books. In addition, the fine-grained factor of chapter partitioning is introduced to the reconstruction process to adjust the proportion of global and local features to better represent the semantic features of books. We analyzed effects of the content of serialized chapters on the recommendation results and used semantic features of the reconstruction to represent the content of serial novels in real time. The experiment results prove that the proposed DTree2Vec scheme can achieve higher degrees of recommendation accuracy when dealing with unfinished serialized books, effectively alleviating dynamic capture problems and real-time recommendations of book recommendations.",science
10.1109/TSP.2020.3019329,to_check,IEEE Transactions on Signal Processing,IEEE,2020-01-01 00:00:00,ieeexplore,Modeling of Spatio-Temporal Hawkes Processes With Randomized Kernels,https://ieeexplore.ieee.org/document/9177186/,"We investigate spatio-temporal event analysis using point processes. Inferring the dynamics of event sequences spatio-temporally has many practical applications including crime prediction, social media analysis, and traffic forecasting. In particular, we focus on spatio-temporal Hawkes processes that are commonly used due to their capability to capture excitations between event occurrences. We introduce a novel inference framework based on randomized transformations and gradient descent to learn the process. We replace the spatial kernel calculations by randomized Fourier feature-based transformations. The introduced randomization by this representation provides flexibility while modeling the spatial excitation between events. Moreover, the system described by the process is expressed within closed-form in terms of scalable matrix operations. During the optimization, we use maximum likelihood estimation approach and gradient descent while properly handling positivity and orthonormality constraints. The experiment results show the improvements achieved by the introduced method in terms of fitting capability in synthetic and real-life datasets with respect to the conventional inference methods in the spatio-temporal Hawkes process literature. We also analyze the triggering interactions between event types and how their dynamics change in space and time through the interpretation of learned parameters.",science
10.1109/TAMD.2010.2097260,to_check,IEEE Transactions on Autonomous Mental Development,IEEE,2011-03-01 00:00:00,ieeexplore,Using the Rhythm of Nonverbal Human–Robot Interaction as a Signal for Learning,https://ieeexplore.ieee.org/document/5664771/,"Human-robot interaction is a key issue in order to build robots for everyone. The difficulty for people to understand how robots work and how they must be controlled will be one of the mains limit for broad robotics. In this paper, we study a new way of interacting with robots without needing to understand how robots work or to give them explicit instructions. This work is based on psychological data showing that synchronization and rhythm are very important features for pleasant interaction. We propose a biologically inspired architecture using rhythm detection to build an internal reward for learning. After showing the results of keyboard interactions, we present and discuss the results of real human-robots (Aibo and Nao) interactions. We show that our minimalist control architecture allows the discovery and learning of arbitrary sensorimotor associations games with expert users. With nonexpert users, we show that using only the rhythm information is not sufficient for learning all the associations due to the different strategies used by the human. Nevertheless, this last experiment shows that the rhythm is still allowing the discovery of subsets of associations, being one of the promising signal of tomorrow social applications.",science
10.1007/978-981-16-1685-3_24,to_check,Recent Challenges in Intelligent Information and Database Systems,Springer,2021-01-01 00:00:00,springer,Real-Time Social Distancing Alert System Using Pose Estimation on Smart Edge Devices,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-1685-3_24,"This paper focuses on developing a social distance alert system using pose estimation for smart edge devices. Recently, with the rapid development of the Deep Learning model for computer vision, a vision-based automatic real-time warning system for social distance becomes an emergent issue. In this study, different from previous works, we propose a new framework for distance measurement using pose estimation. Moreover, the system is developed on smart edge devices, which is able to deal with moving cameras instead of fixed cameras of surveillance systems. Specifically, our method includes three main processes, which are video pre-processing, pose estimation, and object distance estimation. The experiment on coral board, an AI accelerator device, provides promising results of our proposed method in which the accuracies are able to achieve more than 85% from different datasets.",science
10.1007/s10844-018-0533-4,to_check,Journal of Intelligent Information Systems,Springer,2020-04-01 00:00:00,springer,"A deep architecture for depression detection using posting, behavior, and living environment data",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10844-018-0533-4,"The World Health Organization (WHO) predicts that depression disorders will be widespread in the next 20 years. These disorders may affect a person’s general health and habits such as altered sleeping and eating patterns in addition to their interpersonal relationships. Early depression detection and prevention therefore becomes an important issue. To address this critical issue, we recruited 1453 individuals who use Facebook frequently and collected their Facebook data. We then propose an automatic depression detection approach, named Deep Learning-based Depression Detection with Heterogeneous Data Sources (D3-HDS), to predict the depression label of an individual by analyzing his/her living environment, behavior, and the posting contents in the social media. The proposed method employs Recurrent Neural Networks to compute the posts representation of each individual. The representations are then combined with other content-based, behavior and living environment features to predict the depression label of the individual with Deep Neural Networks. To our best knowledge, this is the first attempt that simultaneously considers all the content-based, behavior, and living environment features for depression detection. The experiment results on a real dataset show that the performance of our approach significantly outperforms the other baselines.",science
10.1186/s13673-019-0205-6,to_check,Human-centric Computing and Information Sciences,Springer,2020-01-02 00:00:00,springer,Developing an online hate classifier for multiple social media platforms,https://www.biomedcentral.com/openurl?doi=10.1186/s13673-019-0205-6,"The proliferation of social media enables people to express their opinions widely online. However, at the same time, this has resulted in the emergence of conflict and hate, making online environments uninviting for users. Although researchers have found that hate is a problem across multiple platforms, there is a lack of models for online hate detection using multi-platform data. To address this research gap, we collect a total of 197,566 comments from four platforms: YouTube, Reddit, Wikipedia, and Twitter, with 80% of the comments labeled as non-hateful and the remaining 20% labeled as hateful. We then experiment with several classification algorithms (Logistic Regression, Naïve Bayes, Support Vector Machines, XGBoost, and Neural Networks) and feature representations (Bag-of-Words, TF-IDF, Word2Vec, BERT, and their combination). While all the models significantly outperform the keyword-based baseline classifier, XGBoost using all features performs the best (F1 = 0.92). Feature importance analysis indicates that BERT features are the most impactful for the predictions. Findings support the generalizability of the best model, as the platform-specific results from Twitter and Wikipedia are comparable to their respective source papers. We make our code publicly available for application in real software systems as well as for further development by online hate researchers.",science
10.1007/978-3-030-55393-7_15,to_check,"Knowledge Science, Engineering and Management",Springer,2020-01-01 00:00:00,springer,Seeds Selection for Influence Maximization Based on Device-to-Device Social Knowledge by Reinforcement Learning,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-55393-7_15,"Recently, how to use Device-to-Device (D2D) social knowledge to reduce the network traffic on mobile networks has become a hot topic. We aim to leverage D2D social knowledge to select influential users (seed users or seeds) for influence maximization to minimize network traffic. Lots of work has been done for seeds selection in a single community. However, few studies are about seeds selection in multiple communities. In this paper, we build a Multi-Community Coverage Maximization (MCCM) model to maximize the D2D social coverage so that the cellular network traffic can be minimized. We transform it into a resource allocation problem and use a Reinforcement Learning (RL) approach to tackle it. Specifically, we present a novel seeds allocation algorithm based on Value Iteration method. To reduce the time delay, we design an edge-cloud computing framework for our method by moving part of the computing tasks from the remote cloud to adjacent base stations (BSs). The experiment results on a realistic D2D data set show our method improves D2D coverage by 17.65% than heuristic average allocation. The cellular network traffic is reduced by 26.35% and the time delay is reduced by 63.53%.",science
http://arxiv.org/abs/2106.07114v1,to_check,arxiv,arxiv,2021-06-14 00:12:27+00:00,arxiv,"Intelligent Agent for Hurricane Emergency Identification and Text
  Information Extraction from Streaming Social Media Big Data",http://arxiv.org/abs/2106.07114v1,"This paper presents our research on leveraging social media Big Data and AI
to support hurricane disaster emergency response. The current practice of
hurricane emergency response for rescue highly relies on emergency call
centres. The more recent Hurricane Harvey event reveals the limitations of the
current systems. We use Hurricane Harvey and the associated Houston flooding as
the motivating scenario to conduct research and develop a prototype as a
proof-of-concept of using an intelligent agent as a complementary role to
support emergency centres in hurricane emergency response. This intelligent
agent is used to collect real-time streaming tweets during a natural disaster
event, to identify tweets requesting rescue, to extract key information such as
address and associated geocode, and to visualize the extracted information in
an interactive map in decision supports. Our experiment shows promising
outcomes and the potential application of the research in support of hurricane
emergency response.",science
10.1016/j.robot.2021.103830,to_check,Robotics and Autonomous Systems,scopus,2021-09-01,sciencedirect,Visual recognition of gymnastic exercise sequences. Application to supervision and robot learning by demonstration,https://api.elsevier.com/content/abstract/scopus_id/85109177424,"This work presents a novel software architecture to autonomously identify and evaluate the gymnastic activity that people are carrying out. It is composed of three different interconnected layers. The first corresponds to a Multilayer Perceptron (MLP) trained from a set of angular magnitudes derived from the information provided by the OpenPose library. This library works frame by frame, so some postures may be incorrectly detected due to eventual occlusions. The MLP layer makes it possible to accurately identify the posture a person is performing. A second layer, based on a Hidden Markov Model (HMM) and the Viterbi algorithm, filters the incorrect spurious postures. Thus, the accuracy of the algorithm is improved, leading to a precise sequence of postures. A third layer identifies the current exercise and evaluates whether the person is doing it at a correct speed. This layer uses an innovative Modified Levenshtein Distance (MLD), which considers not only the number of operations to transform a given sequence, but also the nature of the elements participating in the comparison. The system works in real time with little delay, thus recognizing sequences of arbitrary length and providing continuous feedback on the exercises being performed. An experiment carried out consisted in reproducing the output of the second layer on an autonomous Pepper robot that can be used in environments where physical exercise is performed, such as a residence for the elderly or others. It has reproduced different exercises previously executed by an instructor so that people can copy the robot. The article analyzes the current situation of the automated gymnastic activities recognition, presents the architecture, the different experiments carried out and the results obtained. The integration of the three components (MLP, HMM and MLD) results in a robust system that has allowed us to improve the results of previous works.",science
10.1016/j.engappai.2021.104192,to_check,Engineering Applications of Artificial Intelligence,scopus,2021-04-01,sciencedirect,HK–SEIR model of public opinion evolution based on communication factors,https://api.elsevier.com/content/abstract/scopus_id/85101562600,"Microblog, with its good interaction and convenient dissemination, has become the main platform for public opinion dissemination. How to discover the law of public opinion dissemination, and to identify the public opinion accurately have become the hot researches. In this paper, we define the user influence, topic popularity, topic interest to analysis the process of opinions fusion among the users under the interest and confidence threshold. We propose a new public opinion evolution HK–SEIR model which combines the opinion fusion HK and the epidemic transmission SEIR models. Firstly, the topic interest degree is added to the opinion fusion HK model, and the interaction behavior between the users under the interest and confidence threshold is analyzed. Then, we calculate the probability of topic propagation caused by the interaction of opinions between users under group pressure, and the probability that users change from the infected state to the removed state under topic popularity. Finally, we analyze the changes of the susceptible, exposed, infected and removed states in the process of public opinion communication. The experiment proves that the HK–SEIR model is closer to the work-rest rules of public opinion communication than SEIR, SIR model. The density peak time is closer to the peak of real public opinion communication. We find that the user interest is the main factor influencing the public opinion dissemination after the interaction of user opinions fusion reaches a certain degree. The negative public opinion of the higher proportion can easily reach the peak of public opinion propagation.",science
10.1016/j.conbuildmat.2020.121928,to_check,Construction and Building Materials,scopus,2021-02-22,sciencedirect,Employing a hybrid GA-ANN method for simulating fracture toughness of RCC mixture containing waste materials,https://api.elsevier.com/content/abstract/scopus_id/85098054250,"The present study is conducted to investigate the efficiency of evolutionary algorithms such as genetic algorithm (GA)-evolved neural network in estimating fracture properties of roller compacted concrete pavement (RCCP) mixtures with different compositions. The effect of waste materials, including reclaimed asphalt pavement (RAP) and crumb rubber on fracture toughness in both pure mode I and pure mod II, were investigated using a real coded GA and an evolution with a back-propagation algorithm. Moreover, the geometry effect of SCB and 4 PB specimens on predicting fracture toughness was evaluated using GA. To evaluate the GA-based neural network's performance, the NSE criterion was applied for fitness function, a different approach for fitting in this area. Many researchers have studied the fracture behavior of RCC in diverse modes. Still, their studies have been restricted to the materials' fracture behavior, which has rarely come to a model. As pure mode II fracture toughness experiment is a complicated procedure with high uncertainties, the introduced model provides a powerful tool for predicting mode II fracture toughness via mix composition and pure mode I fracture toughness of RCC based on GA. The proposed model outperforms the traditional artificial neural network (ANN) models. The geometry effect survey on predicting fracture toughness shows that the 4 PB fracture model has been better fitted to observed data. Also, the MSE results show that prismatic specimens present relatively reliable results than SCB specimens.",science
10.1016/j.lfs.2020.118549,to_check,Life Sciences,scopus,2020-12-15,sciencedirect,Estrogen-regulated expression of SK3 channel in rat colonic smooth muscle contraction,https://api.elsevier.com/content/abstract/scopus_id/85092697688,"Aims
                  Estrogen can induce inhibition of colonic smooth muscle contraction in male and female mice, which may lead to constipation; however, the mechanisms of inhibition are poorly understood. Hence, this study investigated the effect of estrogen on rat colonic smooth muscle contraction and role of small-conductance Ca2+-activated K+ 3 (SK3) and transcription factors (Sp1 and Sp3) in the underlying mechanisms.
               
                  Main methods
                  The experiment included 24 female Sprague-Dawley (SD) rats divided into 4 groups. The rats were oophorectomized surgically, and a silicone tube containing blank solvent, 0.3 mg/mL estrogen (E2), equal-concentration of estrogen and estrogen receptor antagonist (EI), and bovine serum albumin-E2 (BSA-E2) was implanted. The rats were sacrificed on day 14. The molecular insights were confirmed using real-time quantitative reverse transcription PCR (qRT-PCR) and western blot analyses to determine the effect of estrogenic stimulation on gene and protein expression analyses, respectively.
               
                  Key findings
                  The E2 group showed significantly greater SK3 expression (P < .005) compared with other groups and significantly lowers smooth muscle cell (SMC) contractility (P < .005). Estrogen stimulation and SK3 overexpression resulted in a significant decrease (P < .05) in Ca2+ mobilization in the E2 group versus the control group. Further, the E2 group showed significantly higher Sp1 mRNA (P < .05) but lower Sp3 mRNA expression (P < .05) and protein expression (P < .001) compared with other groups.
               
                  Significance
                  E2 may promote SK3 expression by its genomic effect and inhibit colonic contraction by affecting SK3 expression via an interaction between Sp1 and Sp3.",science
10.1016/j.isatra.2020.03.031,to_check,ISA Transactions,scopus,2020-08-01,sciencedirect,Virtual metrology of semiconductor PVD process based on combination of tree-based ensemble model,https://api.elsevier.com/content/abstract/scopus_id/85082842250,"In order to improve the accuracy of semiconductor wafer virtual metrology, and overcome the physical metrology delay of wafer acceptance test, a virtual physical vapor deposition metrology method based on combination of tree-based ensemble models is proposed to conduct online virtual metrology on semiconductor wafer electrical parameters, and use hyperparameter optimization technique to perform model optimization and to achieve real-time alarm on process deviation. This combination of tree-based ensemble model combines Bagging, Boosting, and Stacking techniques. First, based on 4 types of base learner, Random Forest, Extra-Trees, XGBoost, and lightGBM, preliminary virtual metrology is performed on wafer PVD process, and then transforms the predict results of the 4 base learners into meta feature vector as the input of meta learner lightGBM to perform further virtual metrology. The Sequential model-based optimization algorithm is used to improve the accuracy of virtual metrology. First, the initial hyperparameter of the sequential model-based optimization is initialized by using random sampling, then the combination model is approximated by the surrogate model of tree-structured Parzen estimator, and the recommended hyperparameters is obtained by using EI (Expected Improvement), and then the optimized combination model is obtained. Finally, the superiority of the method proposed in this paper is verified by studying the results comparing to the common virtual metrology methods on the PVD process. The experiment shows the result of resistivity metrology using the combination of tree-based ensemble models in the PVD process is significantly better than LASSO regression, partial least squares regression(PLSR), support vector machine(SVR), Gaussian process regression(GPR) and artificial neural network regression(ANN).",science
10.1016/j.ipm.2019.02.016,to_check,Information Processing and Management,scopus,2020-03-01,sciencedirect,Detecting breaking news rumors of emerging topics in social media,https://api.elsevier.com/content/abstract/scopus_id/85062093277,"Users of social media websites tend to rapidly spread breaking news and trending stories without considering their truthfulness. This facilitates the spread of rumors through social networks. A rumor is a story or statement for which truthfulness has not been verified. Efficiently detecting and acting upon rumors throughout social networks is of high importance to minimizing their harmful effect. However, detecting them is not a trivial task. They belong to unseen topics or events that are not covered in the training dataset. In this paper, we study the problem of detecting breaking news rumors, instead of long-lasting rumors, that spread in social media. We propose a new approach that jointly learns word embeddings and trains a recurrent neural network with two different objectives to automatically identify rumors. The proposed strategy is simple but effective to mitigate the topic shift issues. Emerging rumors do not have to be false at the time of the detection. They can be deemed later to be true or false. However, most previous studies on rumor detection focus on long-standing rumors and assume that rumors are always false. In contrast, our experiment simulates a cross-topic emerging rumor detection scenario with a real-life rumor dataset. Experimental results suggest that our proposed model outperforms state-of-the-art methods in terms of precision, recall, and F1.",science
10.1016/j.jksuci.2020.03.008,to_check,Journal of King Saud University - Computer and Information Sciences,scopus,2020-01-01,sciencedirect,Romanized Tunisian dialect transliteration using sequence labelling techniques,https://api.elsevier.com/content/abstract/scopus_id/85082820171,"In recent years, social web users in Arabic countries have been resorting to the dialects as a written language in their social exchanges. Arabic dialects derive from modern standard Arabic (MSA) and differ significantly from one country to another and one region to another. The use of these dialects has led to an increase of interest in the specificities of such informal languages and their automatic processing within the NLP community. In this work, we deal with the Tunisian dialect (TD) in particular. We address the issue of the automatic Latin to Arabic transliteration of TD language productions on the social web and propose an approach that models the transliteration as a sequence labeling task. At a word level, several techniques, based on machine and deep learning, have been tested for this study, using real word messages extracted from social networks. We experiment and compare three transliteration models: A Conditional Random Fields-based model (CRF), a Bidirectional Long Short-Term Memory based model (BLSTM), and a BLSTM based model with CRF decoding (BLSTM-CRF). The obtained results show that BLSTM-CRF, leads to the best performance, reaching 96.78% of correctly transliterated words. We also evaluate the BLSTM-CRF transliteration approach in context on a set of random TD messages extracted from the social web. We obtained a total error rate of 2.7%. 25% of which are context errors.",science
10.1016/j.cmpb.2018.11.002,to_check,Computer Methods and Programs in Biomedicine,scopus,2019-01-01,sciencedirect,Predicting combinative drug pairs via multiple classifier system with positive samples only,https://api.elsevier.com/content/abstract/scopus_id/85056787891,"Background and Objective
                  Due to the synergistic effects of drugs, drug combination is one of the effective approaches for treating complex diseases. However, the identification of drug combinations by dose-response methods is still costly. It is promising to develop supervised learning-based approaches to predict potential drug combinations on a large scale. Nevertheless, these approaches have the inadequate utilization of heterogeneous features, which causes the loss of information useful to classification. Moreover, they have an intrinsic bias, because they assume unknown drug pairs as non-combinations, of which some could be real drug combinations in practice.
               
                  Methods
                  To address above issues, this work first designs a two-layer multiple classifier system (TLMCS) to effectively integrate heterogeneous features involving anatomical therapeutic chemical codes of drugs, drug-drug interactions, drug-target interactions, gene ontology of drug targets, and side effects. To avoid the bias caused by labelling unknown samples as negative, it then utilizes the one-class support vector machines, (which requires no negative instance and only labels approved drug combinations as positive instances), as the member classifiers in TLMCS. Last, both a 10-fold cross validation (10-CV) and a novel prediction are performed to validate the performance of TLMCS.
               
                  Results
                  The comparison with three state-of-the-art approaches under 10-CV exhibits the superiority of TLMCS, which achieves the area under the receiver operating characteristic curve = 0.824 and the area under the precision-recall curve = 0.372. Moreover, the experiment under the novel prediction demonstrates its ability, where 9 out of the top-20 predicted combinative drug pairs are validated by checking the published literature. Furthermore, for each of the newly-validated drug combinations, this work analyses the combining mode of the member drugs and investigates their relationship in terms of drug targeting pathways.
               
                  Conclusions
                  The proposed TLMCS provides an effective framework to integrate those heterogeneous features and is trained by only positive samples such that the bias of taking unknown drug pairs as negative samples can be avoided. Furthermore, its results in the novel prediction reveal five types of drug combinations and three types of drug relationships in terms of pathways.",science
10.1016/j.asoc.2018.09.027,to_check,Applied Soft Computing Journal,scopus,2019-01-01,sciencedirect,Repository and Mutation based Particle Swarm Optimization (RMPSO): A new PSO variant applied to reconstruction of Gene Regulatory Network,https://api.elsevier.com/content/abstract/scopus_id/85055896289,"Particle Swarm Optimization (PSO) is a meta-heuristic approach based on swarm intelligence, which is inspired by the social behaviour of bird flocking or fish schooling. The main disadvantage of the basic PSO is that it suffers from premature convergence. To prevent the process of search from premature convergence as well as to improve the exploration and exploitation capability as a whole, here, in this paper, a modified variant, named Repository and Mutation based PSO (RMPSO) is proposed. In RMPSO variant, apart from applying five-staged successive mutation strategies for improving the swarm best as referred in Enhanced Leader PSO (ELPSO), two extra repositories have been introduced and maintained to store personal best and global best solutions having same fitness values. In each step, the personal and global best solutions are chosen randomly from their respective repositories which enhance exploration capability further, retaining the exploitation capability. The computational experiment on benchmark problem instances shows that in most of the cases, RMPSO performs better than other algorithms in terms of the statistical metrics taken into account. Moreover, the performance of the proposed algorithm remains consistent in most of the cases when the dimension of the problem is scaled up. RMPSO is further applied to a practical scenario: the reconstruction of Gene Regulatory Networks (GRN) based on Recurrent Neural Network (RNN) model. The experimental results ensure that the RMPSO performs better than the state-of-the-art methods in the synthetic gene data set (gold standard) as well as real gene data set.",science
10.1016/j.future.2018.06.044,to_check,Future Generation Computer Systems,scopus,2018-12-01,sciencedirect,AIEM: AI-enabled affective experience management,https://api.elsevier.com/content/abstract/scopus_id/85050100370,"Nowadays, with rapid development of artificial intelligence technology, the emerging human–machine interaction application researches grow up with machine intelligence, cognitive science and CEM (Customer Experience Management). This paper puts forward a new AIEM (AI-enabled affective experience management) method, blends AI and CEM in the emotion recognition and interactive intelligence application. Besides, in order to create good user experience, AIEM method also strives for the intelligence at various phases of emotion acquisition, emotion recognition, and emotion interaction. This paper introduces the composition and architecture of AIEM from three aspects, i.e. intelligent management of emotion data collection, accuracy management of emotion recognition, and real-time management of emotion interaction. Then we use advanced algorithm and model in two phases of emotion recognition algorithm and emotion computing offloading. Moreover, we select two deep learning algorithms (VGG-Net and Alex-Net) for facial expression recognition and speech emotion recognition, respectively. In the experiment using AIWAC system in real environment, we evaluate the emotion interaction delay in different computing nodes (Cloud and Edge) using AIEM method. Experiment results show that our method can provide intuitive and reasonable user experience management, and select suitable computing nodes for users. Finally, we provide summary and prospect for the future research proposal.",science
10.1016/j.foodchem.2018.02.002,to_check,Food Chemistry,scopus,2018-08-01,sciencedirect,Quadruplex gold immunochromatogaraphic assay for four families of antibiotic residues in milk,https://api.elsevier.com/content/abstract/scopus_id/85042846210,"In this study, we developed a quadruplex gold immunochromatogaraphic assay (GICA) for the simultaneous determination of four families of antibiotics including β-lactams, tetracyclines, streptomycin and chloramphenicol in milk. For qualitative analysis, the visual cut-off values were measured to be 2–100 ng/mL, 16–32 ng/mL, 50 ng/mL and 2.4 ng/mL for β-lactams, tetracyclines, streptomycin and chloramphenicol, respectively. For quantitative analysis, the detection ranges were 0.13–1 ng/mL for penicillin G, 0.13–8 ng/mL for tetracycline, 0.78–25 ng/mL for streptomycin, 0.019–1.2 ng/mL for chloramphenicol in milk respectively, with linear correlation coefficients higher than 0.97. The spiked experiment indicated that the mean recoveries ranged from 84.5% to 107.6% with coefficient of variations less than 16.2%, and real sample analysis revealed that the GICA can produce consistent results with instrumental analysis. These results demonstrated that this novel immunoassay is a promising approach for rapidly screening common antibiotic residues in milk.",science
10.1109/ICARCV.2018.8581349,to_check,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",IEEE,2018-11-21 00:00:00,ieeexplore,Activity Recognition Based on RGB-D and Thermal Sensors for Socially Assistive Robots,https://ieeexplore.ieee.org/document/8581349/,"For socially assistive robots, being able to recognize basic human actions is an important capability. The sensors, which are frequently mounted on most recent robots, such as RGB-D and thermal cameras, as well as the advances in deep learning have enabled the research on activity recognition to grow. In this paper, we collected our own dataset of actions in a home-like scenario, which contains thermal imagery in addition to RGB-D data and we proposed a method based on Long-term Recurrent Convolutional Networks (LRCN). We showed that our method has an accuracy comparable with the state-of-the-art. We also proved that thermal information can improve the recognition accuracy. Furthermore, we tested the real-time capability of our system and conducted a real-time experiment with a robot (Pepper robot from Softbank Robotics) so as to investigate the effect of a robot enabled with action recognition capability in a human-robot interaction.",robotics
10.1109/TCDS.2018.2846778,to_check,IEEE Transactions on Cognitive and Developmental Systems,IEEE,2019-09-01 00:00:00,ieeexplore,Adaptive Behavior Acquisition of a Robot Based on Affective Feedback and Improvised Teleoperation,https://ieeexplore.ieee.org/document/8383948/,"In socially assistive robotics, especially for children with autism spectrum disorder (ASD), adapting the behavior of the robot according to the personal characteristics of each individual is one of the important challenges. Machine learning techniques are promising approaches to endow a robot with the capability of adapting its behavior through the interaction. It is critical to prepare a rich data set such as a set of behaviors with teaching signals for each individual with ASD to allow application of the state-of-the-art machine learning techniques; however, this is typically difficult to prepare in advance owing to the diverseness of ASD and the complexity of the motion design of the robot. This paper proposes a framework to acquire the personalized behavior set of a robot by combining a robot teleoperation method and a wearable device for detecting the affective cue of a child with ASD while interacting with the robot. The developed system allows the human operator to improvise the robot's behavior flexibly in real-time to explore the preferred interaction manner and motion patterns of each child. The preferred motion patterns are extracted and evaluated based on the affective state of the child estimated by the wearable device, and stored in the personal database for each individual with ASD. We conducted a free-interaction experiment with ten participants with ASD and demonstrated that the proposed system successfully described the interaction between the robot and the participant for acquiring the appropriate behaviors of the robot.",robotics
10.1109/BioRob49111.2020.9224272,to_check,2020 8th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob),IEEE,2020-12-01 00:00:00,ieeexplore,Deep Learning of Movement Intent and Reaction Time for EEG-informed Adaptation of Rehabilitation Robots,https://ieeexplore.ieee.org/document/9224272/,"Mounting evidence suggests that adaptation is a crucial mechanism for rehabilitation robots in promoting motor learning. Yet, it is commonly based on robot-derived movement kinematics, which is a rather subjective measurement of performance, especially in the presence of a sensorimotor impairment. Here, we propose a deep convolutional neural network (CNN) that uses electroencephalography (EEG) as an objective measurement of two kinematics components that are typically used to assess motor learning and thereby adaptation: i) the intent to initiate a goal-directed movement, and ii) the reaction time (RT) for that movement. We evaluated our CNN on data acquired from an in-house experiment where 12 healthy subjects moved a rehabilitation robotic arm in four directions on a plane, in response to visual stimuli. Our CNN achieved average test accuracies of 80.08% and 79.82% in a binary classification of the intent (intent vs. no intent) and RT (slow vs. fast), respectively. Our results demonstrate how individual movement components implicated in distinct types of motor learning can be predicted from synchronized EEG data acquired before the start of the movement. Our approach can, therefore, inform robotic adaptation in real-time and has the potential to further improve one's ability to perform the rehabilitation task.",robotics
10.1109/IConSCS.2012.6502471,to_check,2012 1st International Conference on Systems and Computer Science (ICSCS),IEEE,2012-08-31 00:00:00,ieeexplore,Object recognition based on radial basis function neural networks: Experiments with RGB-D camera embedded on mobile robots,https://ieeexplore.ieee.org/document/6502471/,"An object recognition strategy based on artificial radial basis functions neural networks is presented in this paper. The general context of this work is to recognize object from captures made by a mobile robot. Unlike classical approaches which always select the closest object, our method outputs a set of potential candidates if the input information is not enough discriminant. There are three main steps in our approach: objects segmentation, signature extraction and classification. Segmentation is inspired from previous works and is shortly described. Signature extraction based on global geometric and color features is detailed. Classification based on artificial neural networks is also explained and architecture of the network is justified. Finally a real experiment made with a RGB-D camera mounted on a mobile robot is presented and classification results is criticized.",robotics
10.1109/TSMC.2019.2912715,to_check,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2021-04-01 00:00:00,ieeexplore,A Visual Leader-Following Approach With a T-D-R Framework for Quadruped Robots,https://ieeexplore.ieee.org/document/8709995/,"The quadruped robot imitates the motions of four-legged animals with a superior flexibility and adaptability to complex terrains, compared with the wheeled and tracked robots. Its leader-following ability is unique to help a human to accomplish complex tasks in a more convenient way. However, long-term following is severely obstructed due to the high-frequency vibration of the quadruped robot and the unevenness of terrains. To solve this problem, a visual approach under a novel T-D-R framework is proposed. The proposed T-D-R framework is composed of a visual tracker based on correlation filter, a person detector with deep learning, and a person re-identification (re-ID) module. The result of the tracker is verified by the detector to improve tracking performance. Especially, the re-ID module is introduced to handle distractions and occlusion caused by other persons, where the convolutional correlation filter (CCF) is employed to discriminate the leader among multiple persons through recording the appearance information in the long run. By comparing the results of the tracker and the detector as well as their similarity scores with the leader identified by the re-ID module, a stable and real-time tracking of the leader can be guaranteed. Experiments reveal that our approach is effective in handling distractions, appearance changes, and illumination variations. A long-distance experiment on a quadruped robot indicates the validity of the proposed approach.",robotics
10.1109/ITAIC.2019.8785467,to_check,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),IEEE,2019-05-26 00:00:00,ieeexplore,A Weights and Improved Adaptive Artificial Fish Swarm Algorithm for Path Planning,https://ieeexplore.ieee.org/document/8785467/,"A weight and improved adaptive artificial fish swarm algorithm(WIA-AFSA) is proposed to deal with the problem of mobile robots path planning have low optimization accuracy and premature convergence under real environment. Firstly, introduced an improved aggregation degree factor to obtain an adaptive step and visual strategy, which can reflect the actual changes in the optimal state of the artificial fish swarm search, and better balance the global and local search capabilities. At the same time, the weight evaluation factor is introduced in the pray, swarm and follow behavior of the artificial fish, which effectively avoids the algorithm falling into the local optimum and premature. The benchmark function is used to test the performance of the algorithm. The results show that the algorithm has good searching ability and convergence. Simulation experiments of path planning based on raster model were carried out to verify the superiority of wia-afsa algorithm in robot navigation. Finally, the path planning experiment of the robot in real environment proves the effectiveness and practicability of the proposed algorithm.",robotics
10.1109/SNPD.2016.7515880,to_check,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE,2016-06-01 00:00:00,ieeexplore,A novel fuzzy omni-directional gait planning algorithm for biped robot,https://ieeexplore.ieee.org/document/7515880/,"Aiming at the problems in gait planning of the biped robots, including the complex model, low stability, etc., a novel fuzzy omni-directional gait planning algorithm (FOGPA) is proposed. At first, this method puts forward a new separated omni-directional gait planning model, which combines the straight walking planning algorithm based on the improved Hermite interpolation and the rotation motion together. And then, a fuzzy gait parameter adjustment algorithm is put forward to control the gait parameters including the step size and rotation speed dynamically. At last, the fuzzy control results are used to get the gait data of robot real-timely. The experiment results show that the FOGPA improves the stability and robustness of gait in a certain degree and also improves the adaptability to the complex environment of the robot.",robotics
10.1109/ICRA48506.2021.9561373,to_check,2021 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2021-06-05 00:00:00,ieeexplore,Analyzing Neural Jacobian Methods in Applications of Visual Servoing and Kinematic Control,https://ieeexplore.ieee.org/document/9561373/,"Designing adaptable control laws that can transfer between different robots is a challenge because of kinematic and dynamic differences, as well as in scenarios where external sensors are used. In this work, we empirically investigate a neural networks ability to approximate the Jacobian matrix for an application in Cartesian control schemes. Specifically, we are interested in approximating the kinematic Jacobian, which arises from kinematic equations mapping a manipulator’s joint angles to the end-effector’s location. We propose two different approaches to learn the kinematic Jacobian. The first method arises from visual servoing where we learn the kinematic Jacobian as an approximate linear system of equations from the k-nearest neighbors for a desired joint configuration. The second, motivated by forward models in machine learning, learns the kinematic behavior directly and calculates the Jacobian by differentiating the learned neural kinematics model. Simulation experimental results show that both methods achieve better performance than alternative data-driven methods for control, provide closer approximations to the proper kinematics Jacobian matrix, and on average produce better-conditioned Jacobian matrices. Real-world experiments were conducted on a Kinova Gen-3 lightweight robotic manipulator, which includes an uncalibrated visual servoing experiment, a practical application of our methods, as well as a 7-DOF point-to-point task highlighting that our methods are applicable on real robotic manipulators.",robotics
10.1109/IEEECONF38699.2020.9389377,to_check,Global Oceans 2020: Singapore – U.S. Gulf Coast,IEEE,2020-10-30 00:00:00,ieeexplore,Automatic in-situ instance and semantic segmentation of planktonic organisms using Mask R-CNN,https://ieeexplore.ieee.org/document/9389377/,"Planktonic organisms form the principal source for consumers on higher trophic levels in the food chain. Studying their community dispersion is vital to our understanding of the planet's ecological systems. With the recent technological advancements in imaging systems, capturing images of planktons in-situ is made possible by embedding mobile underwater robots with sophisticated camera systems and computing power that implement deep machine learning approaches. Efforts of applying deep learning methods to plankton imaging systems have been limited to classification, while detection and segmentation has been left to traditional methods in this context. There is a variety of publicly available datasets made suited for planktonic species classification. These datasets consist of images of individual species. Thus, they do not represent the actual environment, which is usually given by a scene representation more suited for object localization, detection and semantic segmentation. In this paper we propose a novel custom dataset [1] from planktonic images captured in-situ in a lab environment suited for supervised learning of object detection and instance segmentation. The data is tested in experiments using the state-of-the-art deep learning visual recognition method of Mask R-CNN. The experiment results show the potential of this method and create a baseline analysis module for real-time in-situ image processing. We provide a comparison of how the method is performing when trained on automatically processed and annotated images from existing segmentation frameworks using traditional methods. This comparison illustrates the importance of utilizing proper data and the potential for success if provided<sup>11</sup>All results, code and metrics used for the experiments are provided in: https://github.com/AILARON/Segmentation",robotics
10.1109/IJCNN.2003.1223996,to_check,"Proceedings of the International Joint Conference on Neural Networks, 2003.",IEEE,2003-07-24 00:00:00,ieeexplore,"Developing early senses about the world: ""Object Permanence"" and visuoauditory real-time learning",https://ieeexplore.ieee.org/document/1223996/,"What ""constraints"" are exactly wired into the human developmental program? What ""constraints"" are minimally necessary for a developmental robot? These are open questions. In this paper, we propose a mechanism of developing experience-based priming - predicting the future contexts including sensation and action based on the previous experience - as a powerful ""constraint"" for developmental robots. We present an architecture that develops this priming capability through realtime online interactions with the environment. We report how our SAIL robot developed a sense of novelty in a well-known ""drawbridge"" experiment which sheds light on the controversial issue of ""object permanence"" in psychology. We further show how the proposed priming mechanism enabled SAIL to deal with a very challenging online learning setting: learning the name and property (e.g., size) of dynamically rotating objects through verbal dialogues.",robotics
10.1109/EMBC.2015.7318719,to_check,2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),IEEE,2015-08-29 00:00:00,ieeexplore,EEG error potentials detection and classification using time-frequency features for robot reinforcement learning,https://ieeexplore.ieee.org/document/7318719/,"In thought-based steering of robots, error potentials (ErrP) can appear when the action resulting from the brain-machine interface (BMI) classifier/controller does not correspond to the user's thought. Using the Steady State Visual Evoked Potentials (SSVEP) techniques, ErrP, which appear when a classification error occurs, are not easily recognizable by only examining the temporal or frequency characteristics of EEG signals. A supplementary classification process is therefore needed to identify them in order to stop the course of the action and back up to a recovery state. This paper presents a set of time-frequency (t-f) features for the detection and classification of EEG ErrP in extra-brain activities due to misclassification observed by a user exploiting non-invasive BMI and robot control in the task space. The proposed features are able to characterize and detect ErrP activities in the t-f domain. These features are derived from the information embedded in the t-f representation of EEG signals, and include the Instantaneous Frequency (IF), t-f information complexity, SVD information, energy concentration and sub-bands' energies. The experiment results on real EEG data show that the use of the proposed t-f features for detecting and classifying EEG ErrP achieved an overall classification accuracy up to 97% for 50 EEG segments using 2-class SVM classifier.",robotics
10.1109/ICSPCC50002.2020.9259462,to_check,"2020 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)",IEEE,2020-08-24 00:00:00,ieeexplore,Indoor Object Identification based on Spectral Subtraction of Acoustic Room Impulse Response,https://ieeexplore.ieee.org/document/9259462/,"Object identification in the room environment is a key technique in many advanced engineering applications such as the unidentified object recognition in security surveillance, human identification and barrier recognition for AI robots. The identification technique based on the sound field perturbation analysis is capable of giving immersive identification which avoids the occlusion problem in the traditional vision-based method. In this paper, a new insight into the relation between the object and the variation of the sound field is presented. The sound field difference before and after the object locates in the environment is analyzed using the spectral subtraction based on the room impulse response. The spectral subtraction shows that the energy loss caused by the sound absorption is the essential factor which perturbs the sound field. By using the energy loss with high uniqueness as the extracted feature, an object identification technique is constructed under the classical supervised pattern recognition framework. The experiment in a real room validates that the system has high identification accuracy. In addition, based on the feature property of position insensitivity, this technique can achieve high identifying accuracy with a quite small training data set, which demonstrates that the technique has potential to be used in real engineering applications.",robotics
10.1109/ICMLC.2005.1527447,to_check,2005 International Conference on Machine Learning and Cybernetics,IEEE,2005-08-21 00:00:00,ieeexplore,Obstacle avoidance with multi-objective optimization by PSO in dynamic environment,https://ieeexplore.ieee.org/document/1527447/,"The second order motion model is one of the fundamental questions, a mostly important object in motion planning research of mobile robots, especially in complex environment. Based on the research of the second order motion model, this paper puts forward a new method for adjusting robots to avoid obstacles in dynamic environment. A mathematical model is first established in which environmental information such as, destination of a mobile robot, velocity and direction of obstacles are considered. Secondly, a new particle swarm optimization (PSO) algorithm is used to search for solution of the multi-objective optimization problem as described in the mathematical model. Finally, by adjusting the velocity and direction of the mobile robot to avoid obstacles in real time, the robot can reach the goal safely. Simulation experiment shows that this method is better than tradition artificial potential field (APF) algorithm and its improved algorithm based on genetic algorithm for obstacle avoidance.",robotics
10.1109/SCIS-ISIS.2018.00037,to_check,2018 Joint 10th International Conference on Soft Computing and Intelligent Systems (SCIS) and 19th International Symposium on Advanced Intelligent Systems (ISIS),IEEE,2018-12-08 00:00:00,ieeexplore,Real-Time Image Semantic Segmentation Networks with Residual Depth-Wise Separable Blocks,https://ieeexplore.ieee.org/document/8716227/,"Semantic image segmentation plays a key role in obtaining pixel-level understanding of images. In recent years, researchers have tackled this problem by using deep learning methods instead of traditional computer vision methods. Because of the development of technologies like autonomous vehicles and indoor robots, segmentation techniques, that have not only high accuracy but also the capability of running in real-time on embedded platform and mobile devices, are in high demand. In this work, we have proposed a new convolutional module, named Residual depth-wise separable, and a fast and efficient convolutional neural network for segmentation. The proposed method is compared against other state of the art real-time models. The experiment results illustrate that our method is efficient in computation while achieves state of the art performance in term of accuracy.",robotics
10.1109/IWECAI50956.2020.00040,to_check,2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI),IEEE,2020-06-14 00:00:00,ieeexplore,Research on Multi-robot Task Allocation Algorithm Based on HADTQL,https://ieeexplore.ieee.org/document/9221763/,"This paper proposes forward a Heuristically Accelerated Dynamic Team Q-learning (HADTQL) algorithm for solving multi-robot collaborative task allocation problem based on multi-agent reinforcement learning. It aims at making multiple robots collaborate to avoid all obstacles and accomplish all tasks while optimizing the path they took relatively. Firstly, the author constructs an appropriate state action space according to the specific information about the environment. Secondly, the whole learning process is divided into two stages by using dynamic exploration coefficient, which ensures the diversity of the early learning and the stability of the later learning. Thirdly, in order to help robots with reasonable action selection, the improved reward function is adopted to provide real-time rewards by utilizing the experience generated by the reinforcement learning of multiple agents. Finally, the heuristic function is introduced to guide the multi-agent reinforcement learning for the next action selection. The simulation experiment shows that the proposed algorithm can find an optimal task execution sequence and complete all tasks collaboratively with a relatively optimal path under the premise of avoiding obstacles in the environment automatically. Compared with the Team Q-learning (TQL) algorithm, this algorithm can allocate tasks reasonably with the high effectiveness and practicability.",robotics
10.1109/ICRAE48301.2019.9043822,to_check,2019 4th International Conference on Robotics and Automation Engineering (ICRAE),IEEE,2019-11-24 00:00:00,ieeexplore,Sim-to-real: Six-legged Robot Control with Deep Reinforcement Learning and Curriculum Learning,https://ieeexplore.ieee.org/document/9043822/,"Six-Iegged robots have higher stability and balance, which helps them face more complex terrain conditions, such as sand, swamp, mine and so forth. Therefore, it is necessary to study the gait planning of six-legged robot to adapt to complex terrain. In order to control six-legged robots to adapt to different terrains, we adopt the method of deep reinforcement learning (DRL) to plan the gait of six-legged robots. The main idea is training the robot through Actor-Critic network with proximal policy optimization (PPO), in which outputs are step length, step height and orientation of the robot. This is an end-to-end approach, which tries to make the robot learn by itself and finally achieve its safe arrival to the target point through complex terrains. In order to train a good model for our robots, simplified environment is adopted to accelerate the training process. We also use curriculum learning to speed up and optimize the training. Then, we verify the reliability of the method in simulation platform and finally transfer the learned model to real robot. Our experiment shows the effectiveness of deep reinforcement learning for locomotion of six-legged robots, the acceleration of the training process by means of curriculum learning, and the improvement of the training effect.",robotics
10.1109/ROMAN.1995.531972,to_check,Proceedings 4th IEEE International Workshop on Robot and Human Communication,IEEE,1995-07-07 00:00:00,ieeexplore,A basic study on dynamic control of facial expressions for Face Robot,https://ieeexplore.ieee.org/document/531972/,"In order to develop an active human interface that realizes ""hear-to-heart"" virtual communication between an intelligent machine and human being, we have already reported the ""Face Robot"" which has a human-like face and can display facial expressions similar to that of a human being by using a flexible microactuator (FMA). For realizing real-time communication between intelligent machine and human being, the Face Robot must express its facial expressions at the almost same speed and in the same manner as a human being. However it is found that FMA can not cope with this kind of performance in expressing dynamic facial features. This paper deals with the development of new mini-actuator ""ACDIS"" for real-time display of Face Robot's facial expressions and also their control method. The developed double action piston type actuator is able to measure the displacement of the position in ACDIS by equipping a LED and a photo-transistor inside it. The opening time of the electro-magnetic valve is regulated for the displacement control of ACDIS by a PD control algorithm. The ACDIS is found to have sufficient performance in the speed of piston-movement and we undertake the experiment of real-time facial expression on the Face Robot and confirm that the display of human-like facial expression is successfully realized.",robotics
10.1109/ROBIO.2013.6739654,to_check,2013 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2013-12-14 00:00:00,ieeexplore,A new method for mobile robot arm blind grasping using ultrasonic sensors and Artificial Neural Networks,https://ieeexplore.ieee.org/document/6739654/,"The paper presents a new method to realize mobile robot arm grasping in indoor laboratory environments. This method adopts a blind strategy, which does not need the robot arms be mounted any kind sensors and avoid calculating the complex kinematic equations of the arms. The method includes: (a) two robot on-board ultrasonic sensors in base are utilized to measure the distances between the robot base and the front arm grasping tables; (b) an Artificial Neural Networks (ANN) is proposed to learn/establish the nonlinear relationship between the ultrasonic distances and the joint controlling values. After executing the training step using sampling data, the ANN can forecast/generate the next-step joint controlling values fast and accurately by inputting a new pair of real-time ultrasonic measured distances; (c) to let the blind strategy matching with the transportation process, an arm controlling component with user interfaces is developed; and (d) a method named training arm is adopted to prepare the training data for the training procedure of the ANN model. Finally, an experiment proves that the proposed strategy has good performance in both of the accuracy and the real-time computation, which can be applied to the real-time arm operations for the mobile robot transportation in laboratory automation.",robotics
10.1109/ICWAPR.2007.4420737,to_check,2007 International Conference on Wavelet Analysis and Pattern Recognition,IEEE,2007-11-04 00:00:00,ieeexplore,A new method of distance estimation for robot localization in real environment based on manifold learning,https://ieeexplore.ieee.org/document/4420737/,"A new distance estimation method for robot autonomous localization from high-dimensional camera images is proposed based on 4 popular manifold learning algorithms. The camera images are supposed to embed in a high-dimensional manifold, and then the dimension is reduced to estimate the corresponding coordinate of the robot. Two experiments show that the distance is estimated regardless of the illumination, motion noise and environment geometric features. Experiment results with 3 image sets acquiring from the real environment verify the feasibility and effectiveness of the scheme and algorithms proposed in this paper.",robotics
10.1109/CMCE.2010.5609659,to_check,"2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering",IEEE,2010-08-26 00:00:00,ieeexplore,Design of mobile robot system with remote control based on CAN-bus,https://ieeexplore.ieee.org/document/5609659/,"In order to realizing remote control and information collection quickly and reliably, the mobile robot with remote control is designed. In the paper, according to analysis of the overall structure, hardware circuit of the robot system is designed. Because the CAN2.0 standard only makes physical layer protocol and data link layer protocol, application layer protocol is ruled according to robot control system. In the last part of this paper, the software of master/slave computer is introduced in detail. The experiment shows that running performance of robot control system is balanced, efficient and has satisfied the practical demand.",robotics
10.1109/ICMTMA.2009.402,to_check,2009 International Conference on Measuring Technology and Mechatronics Automation,IEEE,2009-04-12 00:00:00,ieeexplore,Overall Inverse Kinematics Analysis of Parallel Robot Leg for Rescue Based on Rodrigues Parameters,https://ieeexplore.ieee.org/document/5203157/,"A new method to describe the position-stance of parallel robot leg was proposed based on the Rodrigues theory. Comparing with others methods, the kinematic model with Rodrigues parameters has the advantages including least computational parameters, no trigonometric function calculation and convenient real-time control. The model of the inverse kinematics was established and the inverse solutions of the position-stance were obtained by analyzing the topologic structure of the parallel robot leg with 3-RPS limb. According to the vectors of the manipulator, the velocity and acceleration models of moving platform, limbs and end-effector were deduced. By comparing with the normal walking gait of a human subject, the end-point trajectory of the parallel robot leg was better programmed. The experiment results showed the structure characteristics of the parallel robot leg and validated the model of the inverse kinematics. It was concluded that the parallel robot leg can fulfil the kinematic demand in the unconfigurable environment.",robotics
10.1109/ACCESS.2019.2899940,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,Multi-Task Cascaded Convolutional Networks Based Intelligent Fruit Detection for Designing Automated Robot,https://ieeexplore.ieee.org/document/8643367/,"Effective and efficient fruit detection is considered crucial for designing automated robot (AuRo) for yield estimation, disease control, harvesting, sorting, and grading. Several fruit detection schemes for designing AuRo have been developed during the last decades. However, conventional fruit detection methods are deficient in the real-time response, accuracy, and extensibility. This paper proposes an improved multi-task cascaded convolutional network-based intelligent fruit detection method. This method has the capability to make the AuRo work in real time with high accuracy. Moreover, based on the relationship between the diversity samples of the dataset and the parameters of neural networks' evolution, this paper presents an improved augmented method, a procedure that is based on image fusion to improve the detector performance. The experiment results demonstrated that the proposed detector performed immaculately both in terms of accuracy and time-cost. Furthermore, the extensive experiment also demonstrated that the proposed technique has the capacity and good portability to work with other akin objects conveniently.",robotics
10.1109/TNNLS.2020.2980038,to_check,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2021-03-01 00:00:00,ieeexplore,Mutual-Collision-Avoidance Scheme Synthesized by Neural Networks for Dual Redundant Robot Manipulators Executing Cooperative Tasks,https://ieeexplore.ieee.org/document/9072323/,"Collision between dual robot manipulators during working process will lead to task failure and even robot damage. To avoid mutual collision of dual robot manipulators while doing collaboration tasks, a novel recurrent neural network (RNN)-based mutual-collision-avoidance (MCA) scheme for solving the motion planning problem of dual manipulators is proposed and exploited. Because of the high accuracy and low computation complexity, the linear variational inequality-based primal-dual neural network is used to solve the proposed scheme. The proposed scheme is applied to the collaboration trajectory tracking and cup-stacking tasks, and shows its effectiveness for avoiding collision between the dual robot manipulators. Through network iteration and online learning, the dual robot manipulators will learn the ability of MCA. Moreover, a line-segment-based distance measure algorithm is proposed to calculate the minimum distance between the dual manipulators. If the computed minimum distance is less than the first safe-related distance threshold, a speed brake operation is executed and guarantees that the robot cannot exceed the second safe-related distance threshold. Furthermore, the proposed MCA strategy is formulated as a standard quadratic programming problem, which is further solved by an RNN. Computer simulations and a real dual robot experiment further verify the effectiveness, accuracy, and physical realizability of the RNN-based MCA scheme when manipulators cooperatively execute the end-effector tasks.",robotics
10.1109/ICINFA.2008.4608104,to_check,2008 International Conference on Information and Automation,IEEE,2008-06-23 00:00:00,ieeexplore,Application of reinforcement learning based on neural network to dynamic obstacle avoidance,https://ieeexplore.ieee.org/document/4608104/,"This paper focuses on the application of reinforcement learning to obstacle avoidance in dynamic environments. Behavior-based control architecture is more robust and better in real-time performance than conventional model based architecture in the control of mobile robot. An intelligent controller is proposed by integrating reinforcement learning with the behavior-based control architecture and applied to the obstacle avoidance. Neural network is used to approximate the Q-function to store the Q-value. By using the reinforcement learning, the mobile robot can learn to select proper behavior online without knowing the exact model of the system. In experiments, dynamic and static obstacles are placed in the environments separately. Experiment results show that the mobile robot can get to the target point without colliding with any obstacle after a period of learning.",robotics
10.1109/DEVLRN.2014.6983026,to_check,4th International Conference on Development and Learning and on Epigenetic Robotics,IEEE,2014-10-16 00:00:00,ieeexplore,Bootstrapping paired-object affordance learning with learned single-affordance features,https://ieeexplore.ieee.org/document/6983026/,"The aim of this paper is to propose a system where complex affordance learning is bootstrapped through using pre-learned basic-affordances as additional inputs of the complex affordance predictors or as cues in selecting the next objects to explore during learning. In the first stage, the robot learns affordances in the form of developing classifiers that predict effect categories given object features for different discrete actions applicable to single objects. These predictions are later added to robot's feature set as higher-level affordance features. In the second stage, the robot learns more complex multi-object affordances using object and affordance features. We first applied our idea in an artificial interaction database which includes discrete actions, several manually coded object categories, and actions effects. Finally, we validated our bootstrapping approach in a real robot with poke and stack actions. We expected to obtain higher performance with affordance-features especially in small training datasets as the object-robot-environment dynamics should have already been partially learned and encoded in affordances. The experiment results showed that complex affordance learning significantly speeds up with predictors that are bootstrapped with affordance-features compared to predictors that use low-level features such as shape descriptors. We also showed that by actively selecting the next objects and by increasing the diversity of the training set using a distance measure based on learned single-object affordances, the effect of bootstrapping can be further increased.",robotics
10.1109/TENCON.2016.7848000,to_check,2016 IEEE Region 10 Conference (TENCON),IEEE,2016-11-25 00:00:00,ieeexplore,Design of hardware circuit based on a neural network model for rapid detection of center of gravity position,https://ieeexplore.ieee.org/document/7848000/,"This paper proposes a rapid detection method for the center of gravity based on a neural network model. It is suitable for the rapid response requirement such as attitude control of a gait robot or real time torque control of a running car. The proposed method detects the center of gravity position on a straight line by using only the hardware circuit composing of common electronic devices instead of software, microprocessor and AD converter. The circuit employs some neural based comparators without the learning function to simplify the circuit structure. The detection circuit using some parallel processing neural comparators rapidly detects the center of gravity position on a straight line. In this paper, the circuit is designed and fabricated with electronic devices, and the circuit experiment shows the performance of the position detection.",robotics
10.1109/ROBIO.2010.5723534,to_check,2010 IEEE International Conference on Robotics and Biomimetics,IEEE,2010-12-18 00:00:00,ieeexplore,Development of a cognition system for analyzing rat's behaviors,https://ieeexplore.ieee.org/document/5723534/,"The interaction experiment, between a robot and a rat, will benefit significantly when the rat's actions can be recognized automatically in real time. Regarding quantitative behavior analysis, the number and duration of a rat's actions should be measured efficiently and accurately. Therefore, aiming at the above-mentioned objectives, a novel cognition system capable of detecting rats' actions has been proposed in this paper. The main function of this cognition system lies on the real-time recognition and offline analysis of rats' behaviors. Basic image processing algorithm as Labeling and Contour Finding were employed to extract feature parameters (body length, body area, body radius, rotational angle, and ellipticity) of rat's actions. These parameters are integrated as the input feature vector of NN (Neural Network) and SVM (Support Vector Machine) training system respectively. Preliminary experiments reveal that the grooming, rotating and rearing actions could be recognized with extremely high rate (more than 90%) by both NN and SVM. Compared to NN, SVM provides better recognition rate and less computational cost.",robotics
10.1109/ACIIAsia.2018.8470388,to_check,2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia),IEEE,2018-05-22 00:00:00,ieeexplore,Emotional Human Machine Conversation Generation Based on SeqGAN,https://ieeexplore.ieee.org/document/8470388/,"In recent years, artificial intelligence has made a significant breakthrough and progress in the field of humanmachine conversation. However, how to generate high-quality, emotional and subhuman conversation still a troublesome work. The key factor of man-machine dialogue is whether the chatbot can give a good response in content and emotional level. How to ensure that the robot understands the user's emotions, and consider the user's emotions then give a satisfactory response. In this paper, we add the emotional tags to the post and response from the dataset respectively. The emotional tags, as the emotional tags of post and response, represent the emotions expressed by this sentence. The purpose of our emotional tags is to make the chatbot understood the emotion of the input sequence more directly so that it has a recognition of the emotional dimension. In this paper, we apply the mechanism of GAN network on our conversation model. For the generator: We make full use of Encoder-Decoder structure form a seq2seq model, which is used to generate a sentence's response. For the discriminator: distinguish between the human-generated dialogues and the machine-generated ones.The outputs from the discriminator are used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. We cast our task as an RL(Reinforcement Learning) problem, using a policy gradient method to reward more subhuman conversational sequences, and in addition we have added an emotion tags to represent the response we want to get, which we will use as a rewarding part of it, so that the emotions of real responses can be closer to the emotions we specify. Our experiment shows that through the introduction of emotional intelligence, our model can generate responses appropriate not only in content but also in emotion, which can be used to control and adjust users emotion. Compared with our previous work, we get a better performance on the same data set, and we get less ''safe'' response than before, but there will be a certain degree of existence.",robotics
10.1109/IJCNN.2008.4634277,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,Learning of sensorimotor behaviors by a SASE agent for vision-based navigation,https://ieeexplore.ieee.org/document/4634277/,"In this paper, we propose a model to develop robotspsila covert and overt behaviors by using reinforcement and supervised learning jointly. The covert behaviors are handled by a motivational system, which is achieved through reinforcement learning. The overt behaviors are directly selected by imposing supervised signals. Instead of dealing with problems in controlled environments with a low-dimensional state space, our model is applied for the learning in non-stationary environments. Locally balanced incremental hierarchical discriminant regression (LBIHDR) tree is introduce to be the engine of cognitive mapping. Its balanced coarse-to-fine tree structure guarantees real-time retrieval in self-generated high-dimensional state space. Furthermore, K-nearest neighbor strategy is adopted to reduce training time complexity. Vision-based outdoor navigation are used as challenging task examples. In the experiment, the mean square error of heading direction is 0deg for re-substitution test and 1.1269deg for disjoint test, which allows the robot to drive without a big deviation from the correct path we expected. Compared with IHDR (W.S. Hwang and J. Weng, 2007), LBIHDR reduced the mean square error by 0.252deg and 0.5052deg, using re-substitution and disjoint test, respectively.",robotics
10.1109/ROBOT.2009.5152362,to_check,2009 IEEE International Conference on Robotics and Automation,IEEE,2009-05-17 00:00:00,ieeexplore,Learning to recognize familiar faces in the real world,https://ieeexplore.ieee.org/document/5152362/,"We present an incremental and unsupervised face recognition system and evaluate it offline using data which were automatically collected by Mertz, a robotic platform embedded in real human environment. In an eight-day-long experiment, the robot autonomously detects, tracks, and segments face images during spontaneous interactions with over 500 passersby in public spaces and automatically generates a data set of over 100,000 face images. We describe and evaluate a novel face clustering algorithm using these data (without any manual processing) and also on an existing face recognition database. The face clustering algorithm yields good and robust performance despite the extremely noisy data segmented from the realistic and difficult public environment. In an incremental recognition scheme evaluation, the system is correct 74% of the time when it declares ldquoI don't know this personrdquo and 75.1% of the time when it declares ldquoI know this person, he/she is ...rdquo The latter accuracy improves to 83.8% if the system is allowed some learning curve delay in the beginning.",robotics
10.1109/CEC.2015.7257174,to_check,2015 IEEE Congress on Evolutionary Computation (CEC),IEEE,2015-05-28 00:00:00,ieeexplore,Teaching a series of actions by the universal evaluations of each sensory information,https://ieeexplore.ieee.org/document/7257174/,"Various studies related to reinforcement learning(RL) have been performed. RL is a simple and powerful learning method so it is used for a real robot. In ordinary RL, a reward function is designed to teach a given task. Generally the design of this function is difficult and laborious work because of the need of the consideration about a given task and environment beforehand and the need of the expert knowledge about a reward. This means an agent learned from external sources, not autonomously. To solve this problem, we proposed the method of machine learning through interactions which is independent of any task and environment. In previous works for path planning problem, the method can lead an agent to a goal point by the interaction of an agent with a human and environment. But the effects from a human and environment were mixed and we did not confirm that a human can teach an agent a series of actions under the condition that environment forces an agent to take different actions. In this paper, we recruit four men as participants and instruct them not to lead a goal but to teach a route. We experiment with path planning problem and confirm that a human can teach an agent his/her intention perfectly.",robotics
10.1109/LRA.2021.3096192,to_check,IEEE Robotics and Automation Letters,IEEE,2021-10-01 00:00:00,ieeexplore,Formulation and Validation of an Intuitive Quality Measure for Antipodal Grasp Pose Evaluation,https://ieeexplore.ieee.org/document/9483652/,"This letter describes a novel grasp quality measure that we developed for evaluating antipodal grasp poses in real-time. To quantify the grasp quality, we compute a set of object movement features from analyzing the interaction between the gripper and the object's projections in the image space. The normalization and weights of the features are tuned to make practical and intuitive grasp quality predictions. To evaluate our grasp quality measure, we conducted a real robot grasping experiment with 1000 robot grasp trials on 10 household objects to examine the relationship between our grasp scores and the actual robot grasping results. The results show that the average grasp success rate increases, and the average amount of undesired object movement decreases as the calculated grasp score increases. We achieved a 100% grasp success rate from 100 grasps of the 10 objects when using our grasp quality measure in planning top quality grasps. In addition, we compared our quality measure with the Q measure and deep learning-based quality measures.",robotics
10.1007/978-981-16-7213-2_2,to_check,"Intelligent Equipment, Robots, and Vehicles",Springer,2021-01-01 00:00:00,springer,Object Detection of Basketball Robot Based on MobileNet-SSD,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7213-2_2,"Object detection is one of the research hotspots in the field of computer vision. In this paper, we use the lightweight network MobileNet combined with Single Shot Multibox Detector (SSD) to realize the object detection of the robot. SSD combined with MobileNet can effectively compress the size of the network model and improve the detection rate. The method does automatic extraction on the image features first, and add different size feature maps after the basic network, and then do convolution filtering on the multi dimension feature maps to get the object coordinate value and the object category. In the experiment, compared with the original vision method based on OpenCV, the MobileNet-SSD algorithm was less affected by illumination conditions in the object recognition process, and achieved the rapid and accurate recognition of the basketball robot on the ball.",robotics
10.1007/s00521-020-05032-0,to_check,Neural Computing and Applications,Springer,2021-01-01 00:00:00,springer,A proposed decentralized formation control algorithm for robot swarm based on an optimized potential field method,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-020-05032-0,"Lately, robot swarm has widely employed in many applications like search and rescue missions, fire forest detection and navigation in hazard environments. Each robot in a swarm is supposed to move without collision and avoid obstacles while performing the assigned job. Therefore, a formation control is required to achieve the robot swarm three tasks. In this article, we introduce a decentralized formation control algorithm based on the potential field method for robot swarm. Our formation control algorithm is proposed to achieve the three tasks: avoid obstacles in the environment, keep a fixed distance among robots to maintain a formation and perform an assigned task. An artificial neural network is engaged in the online optimization of the parameters of the potential force. Then, real-time experiments are conducted to confirm the reliability and applicability of our proposed decentralized formation control algorithm. The real-time experiment results prove that the proposed decentralized formation control algorithm enables the swarm to avoid obstacles and maintain formation while performing a certain task. The swarm manages to reach a certain goal and tracks a given trajectory. Moreover, the proposed decentralized formation control algorithm enables the swarm to escape from local minima, to pass through two narrow placed obstacles without oscillation near them. From a comparison between the proposed decentralized formation control algorithm and the traditional PFM, we obtained that NN-swarm successes to reach its goal with average accuracy 0.14 m compared to 0.22 m for the T-swarm. The NN-swarm also keeps a fixed distance between robots with a higher swarming error reaches 34.83%, while the T-swarm reaches 23.59%. Also, the NN-swarm is more accurate in tracking a trajectory with a higher tracking error reaches 0.0086 m compared to min. error of T-swarm equals to 0.01 m. Besides, the NN-swarm maintains formation much longer than T-swarm while tracking trajectory reaches 94.31% while the T-swarm reaches 81.07% from the execution time, in environments with different numbers of obstacles.",robotics
10.1007/s12559-014-9311-y,to_check,Cognitive Computation,Springer,2015-08-01 00:00:00,springer,Towards Autonomous Robots Via an Incremental Clustering and Associative Learning Architecture,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12559-014-9311-y,"This paper presents a novel architecture for associative learning and recall of different sensor and actuator patterns. A modular design allows the inclusion of various input and output modalities. The approach is a generic one that can deal with any kind of multidimensional real-valued data. Sensory data are incrementally grouped into clusters, which represent different categories of the input data. Clusters of different sensors or actuators are associated with each other based on the co-occurrence of corresponding inputs. Upon presenting a previously learned pattern as a cue, associated patterns can be recalled. The proposed architecture has been evaluated in a practical situation in which a robot had to associate visual patterns in the form of road signs with different configurations of its arm joints. This experiment assessed how long it takes to learn stable representations of the input patterns and tested the recall performance for different durations of learning. Depending on the dimensionality of the data, stable representations require many inputs to be formed and only over time similar small clusters are combined into larger clusters. Nevertheless, sufficiently good recall can be achieved earlier when the topology is still in an immature state and similar patterns are distributed over several clusters. The proposed architecture tolerates small variations in the inputs and can generalise over the varying perceptions of specific patterns but remains sensitive to fine geometrical shapes.",robotics
10.1016/j.asr.2021.04.023,to_check,Advances in Space Research,scopus,2021-08-15,sciencedirect,Interactive imitation learning for spacecraft path-planning in binary asteroid systems,https://api.elsevier.com/content/abstract/scopus_id/85107153834,"Exploration of small body systems poses the problem of designing path planning strategies for possibly uncharted environments. Traditional methods aimed at developing rigorous trajectory baselines may suffer inefficiencies, or turn infeasible when confronted with unknown dynamics. In strongly non-linear dynamics, mapping point design solutions from one dynamical regime to another may be hindered by underlying chaotic behavior. Rather than relying on baseline driven approaches, more generalized strategies may be found by observing human pilots controlling spacecraft motion within varying dynamical environments; the resultant data can then be utilized to initialize machine learning agents to provide more autonomous solutions. A previous numerical experiment resulted in a technical dataset comprising of human-based path planning strategies across a range of binary asteroid systems. This dataset is now used to train various imitation learning agents, and initiate the creation of a framework that integrates human–machine cooperation into the early training phases of artificial intelligent agents; the current application is for spacecraft guidance in binary asteroid systems, as a prototype of complex, potentially unknown, orbit dynamics. An interactive training architecture, based on the DAgger algorithm, is designed and employed to train both original and interactively coached agents, the latter stemming from both corrective and evaluative feedback by a real time human interactor. All agents were interactively trained for a predefined time period. The results from this investigation may provide the first, empirical observations of behavioral cloning within multi-body dynamics with largely randomized parameters, with some notable contributions including early characterization of training time, initial evidence of an autonomous agent learning meaningful policy features via imitation, and early identification of challenges in training fully autonomous agents for a multi-body dynamics path planning problem of this complexity and high dimensional state space.",robotics
10.1016/j.bspc.2021.102629,to_check,Biomedical Signal Processing and Control,scopus,2021-07-01,sciencedirect,Hand gestures recognition from surface electromyogram signal based on self-organizing mapping and radial basis function network,https://api.elsevier.com/content/abstract/scopus_id/85104067433,"Predicting the intention of human hand movements is a practical problem in prosthetic control. In recent years, surface electromyography (sEMG) has been widely used as a signal source in the field of wearable exoskeleton motion recognition and human-computer interaction. However, how to extract the information from sEMG signals and evaluate the intention of human hand movement effectively is still difficult. In order to achieve this goal, this work proposed a processing algorithm based on self-organizing mapping network (SOM) and radial basis neural network (RBF) for feature selection and classification recognition, then the principal component analysis (PCA) to reduce the size of feature vectors was used, finally used for pattern classification from sEMG signals to hand motion. In this research, the classification method mainly used the SOM method to find the hidden nodes centers of the RBF network, the Euclidean distance between the data centers was used to calculate the variance of the node and find the optimal center and radius of the radial basis function, so as to improve the learning performance of RBF network. In the experiment, the MYO armband sensor was used to sample the real sEMG signal data of 6 volunteers under 8 gestures. The experiment result show that the proposed algorithm as a classifier achieves a maximum recognition rate of 100 %, an average recognition accuracy of 96.875 ± 2.7296 %, and a response time of 0.437 s. Meanwhile, the effects of the proposed method on hand motion recognition with different classifiers (RBF with k-means, K-Nearest Neighbor, Multi-Layer Perceptron with Scaled Conjugate Gradient) were compared. The corresponding average accuracy rates were 95.833 ± 3.3244 % (RBF with k-means), 94.583 ± 2.243 % (KNN) and 88.89 ± 1.1324 % (MLP with SCG). Compared with existed methods, the advantages of the method proposed in this research are as follows: 1) This research selects the PCA method and threshold value method based on the short-term average energy (STAE) used to detect the active segment of sEMG signal, so as to select the appropriate feature vector; 2) The proposed algorithm of SOM combined with RBF has higher identification accuracy and efficiency than that of RBF with k-means, which is more conducive to distinguish different actions; 3) While ensuring real-time performance, it can accurately classify gestures that are easy to be confused, indicating that this classification method has a good application prospect in prosthetic control and other fields.",robotics
10.1016/j.psep.2020.05.017,to_check,Process Safety and Environmental Protection,scopus,2020-08-01,sciencedirect,Adsorption removal and reuse of phosphate from wastewater using a novel adsorbent of lanthanum-modified platanus biochar,https://api.elsevier.com/content/abstract/scopus_id/85085017552,"In this study, a novel adsorbent of lanthanum-modified platanus ball fiber biochar (La-TC) was developed for efficient adsorption and reuse of phosphate from actual wastewater. La-TC adsorbing phosphate could be used as agricultural fertilizer. The saturated adsorption capacity of phosphate on La-TC was 148.11 mg/g at dosage of 0.4 g/L, initial solution pH of 6.0, contact time of 20 min, and temperature of 35 ℃, which was much higher than most phosphate biochar adsorbents. Simultaneously, La-TC had a wide pH (3–9) adsorption stability, a strong ability to resist interference of anti-competitive anion (SO2−
                     4 and NO− 
                     3), and an outstanding regeneration ability. Fourier transform infrared spectroscopy (FTIR), and X-ray photoelectron spectroscopy (XPS) were used to reveal the adsorption mechanism of La-TC to phosphate, including electrostatic adsorption, ligand exchange, and complexation mechanisms. In the fixed bed column experiment of phosphorus removal from real wastewater, the maximum treated bed volume was 420, 440 and 480 (BV) under the bed flow of 1 mL/min and La-TC of 0.5, 0.75 and 1.0 g, respectively. Outcomes suggested that La-TC had a broad prospect of engineering application for removal and reuse of phosphate from wastewater, also realizing the resource utilization of wasted platanus ball.",robotics
10.1016/j.robot.2018.08.013,to_check,Robotics and Autonomous Systems,scopus,2018-12-01,sciencedirect,Neural network for black-box fusion of underwater robot localization under unmodeled noise,https://api.elsevier.com/content/abstract/scopus_id/85054170379,"The research on autonomous robotics has focused on the aspect of information fusion from redundant estimates. Choosing a convenient fusion policy, that reduces the impact of unmodeled noise, and is computationally efficient, is an open research issue. The objective of this work is to study the problem of underwater localization which is a challenging field of research, given the dynamic aspect of the environment. For this, we explore navigation task scenarios based on inertial and geophysical sensory. We propose a neural network framework named B-PR-F which heuristically performs adaptable fusion of information, based on the principle of contextual anticipation of the localization signal within an ordered processing neighborhood. In the framework black-box unimodal estimations are related to the task context, and the confidence on individual estimates is evaluated before fusing information. A study conducted in a virtual environment illustrates the relevance of the model in fusing information under multiple task scenarios. A real experiment shows that our model outperforms the Kalman Filter and the Augmented Monte Carlo Localization algorithms in the task. We believe that the principle proposed can be relevant to related application fields, involving the problem of state estimation from the fusion of redundant information.",robotics
10.1016/j.artint.2014.11.005,to_check,Artificial Intelligence,scopus,2017-06-01,sciencedirect,Model-based contextual policy search for data-efficient generalization of robot skills,https://api.elsevier.com/content/abstract/scopus_id/84919497776,"In robotics, lower-level controllers are typically used to make the robot solve a specific task in a fixed context. For example, the lower-level controller can encode a hitting movement while the context defines the target coordinates to hit. However, in many learning problems the context may change between task executions. To adapt the policy to a new context, we utilize a hierarchical approach by learning an upper-level policy that generalizes the lower-level controllers to new contexts. A common approach to learn such upper-level policies is to use policy search. However, the majority of current contextual policy search approaches are model-free and require a high number of interactions with the robot and its environment. Model-based approaches are known to significantly reduce the amount of robot experiments, however, current model-based techniques cannot be applied straightforwardly to the problem of learning contextual upper-level policies. They rely on specific parametrizations of the policy and the reward function, which are often unrealistic in the contextual policy search formulation. In this paper, we propose a novel model-based contextual policy search algorithm that is able to generalize lower-level controllers, and is data-efficient. Our approach is based on learned probabilistic forward models and information theoretic policy search. Unlike current algorithms, our method does not require any assumption on the parametrization of the policy or the reward function. We show on complex simulated robotic tasks and in a real robot experiment that the proposed learning framework speeds up the learning process by up to two orders of magnitude in comparison to existing methods, while learning high quality policies.",robotics
10.1016/j.ijleo.2015.07.153,to_check,Optik,scopus,2015-01-01,sciencedirect,Neural network based visual servo control for CNC load/unload manipulator,https://api.elsevier.com/content/abstract/scopus_id/84952362277,"A visual servo control strategy based on fuzzy-neural networks is proposed for an eye-in-hand CNC load/unload manipulator in this paper. As visual servo control is an uncertain nonlinear strong coupling system, the real-time computation of feature Jacobian matrix is very complicated, improving its poor real-time performance is a must. By approximating the mapping relationship between changes of target image features and robotic joints’ positions with fuzzy-neural networks, which has the advantages of strong learning capability and fast learning speed, a novel controller is designed to achieve an effective operation for CNC load/unload manipulator. The following experiment result indicates that compared with BP and RBF neural network the proposed visual servo controller is of higher precision and convergence rate, enhancing the robust capability and accelerating the response time of the control system.",robotics
10.1016/j.compedu.2011.07.003,to_check,Computers and Education,scopus,2011-12-01,sciencedirect,Hands-on experiences of undergraduate students in Automatics and Robotics using a virtual and remote laboratory,https://api.elsevier.com/content/abstract/scopus_id/79960893084,"Automatics and Robotics subjects are always greatly improved when classroom teaching is supported by adequate laboratory courses and experiments following the ""learning by doing"" paradigm, which provides students a deep understanding of theoretical lessons. However, expensive equipment and limited time prevent teachers having sufficient educational platforms, and several low cost and flexible solutions have been developed to permit an effective teaching in Automatics and Robotics at a reasonable cost. Virtual and remote laboratories are inside this group of solutions as Web-based experimentation tools which have demonstrated the importance and effectiveness of hand-on experiences. This paper presents an experience teaching based on a blended-learning method using as experimentation tool a virtual and remote robotic laboratory called RobUALab, which is also described in the paper, in Automatics and Robotics subjects of the Computer Science degree at the University of Alicante. Students experiment with a set of hand-on exercises about Automatics and Robotics using RobUALab, firstly in face-to-face classes where they experiment in-situ with the real plant and, afterwards, they access to the experimentation environment in order to finish remotely their practical exercises outside the laboratory. The results obtained in the evaluation of the educational methodology proposed attest its efficiency in terms of learning degree and performance of the students.",robotics
10.1109/ISAECT50560.2020.9523700,to_check,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),IEEE,2020-11-27 00:00:00,ieeexplore,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,https://ieeexplore.ieee.org/document/9523700/,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",autonomous vehicle
10.1016/j.automatica.2021.110007,to_check,Automatica,scopus,2022-01-01,sciencedirect,An analytic layer-wise deep learning framework with applications to robotics,https://api.elsevier.com/content/abstract/scopus_id/85118989490,"Deep learning (DL) has achieved great success in many applications, but it has been less well analyzed from the theoretical perspective. The unexplainable success of black-box DL models has raised questions among scientists and promoted the emergence of the field of explainable artificial intelligence (XAI). In robotics, it is particularly important to deploy DL algorithms in a predictable and stable manner as robots are active agents that need to interact safely with the physical world. This paper presents an analytic deep learning framework for fully connected neural networks, which can be applied for both regression problems and classification problems. Examples for regression and classification problems include online robot control and robot vision. We present two layer-wise learning algorithms such that the convergence of the learning systems can be analyzed. Firstly, an inverse layer-wise learning algorithm for multilayer networks with convergence analysis for each layer is presented to understand the problems of layer-wise deep learning. Secondly, a forward progressive learning algorithm where the deep networks are built progressively by using single hidden layer networks is developed to achieve better accuracy. It is shown that the progressive learning method can be used for fine-tuning of weights from convergence point of view. The effectiveness of the proposed framework is illustrated based on classical benchmark recognition tasks using the MNIST and CIFAR-10 datasets and the results show a good balance between performance and explainability. The proposed method is subsequently applied for online learning of robot kinematics and experimental results on kinematic control of UR5e robot with unknown model are presented.",robotics
10.1109/UEMCON.2018.8796670,to_check,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",IEEE,2018-11-10 00:00:00,ieeexplore,"Building Towards ""Invisible Cloak"": Robust Physical Adversarial Attack on YOLO Object Detector",https://ieeexplore.ieee.org/document/8796670/,"Deep learning based object detection algorithms like R-CNN, SSD, YOLO have been applied to many scenarios, including video surveillance, autonomous vehicle, intelligent robotics et al. With more and more application and autonomy left to deep learning based artificial intelligence, humans want to ensure that the machine does the best for them under their control. However, deep learning algorithms are known to be vulnerable to carefully crafted input known as adversarial examples which makes it possible for an attacker to fool an AI system. In this work, we explored the mechanism behind the YOLO object detector and proposed an optimization method to craft adversarial examples to attack the YOLO model. The experiment shows that this white box attack method is effective and has a success rate of 100% in crafting digital adversarial examples to fool the YOLO model. We also proposed a robust physical adversarial sticker generation method based on an extended Expectation Over Transformation (EOT) method(a method to craft adversarial example in the physical world). We conduct experiments to find the most effective approach to generate adversarial stickers. We tested the stickers both digitally as a watermark and physically showing it on an electronic screen on the front surface of a person. Our result shows that the sticker attack as a watermark has a success rate of 90% and 45% on photos taken indoors and on random 318 pictures from ImageNet. Our physical attack also has a success rate of 72% on photos taken indoors. We shared our project source code on the Github and our work is reproducible.",autonomous vehicle
10.1109/ACCESS.2021.3094063,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,Early-Stage Risk Prediction of Non-Communicable Disease Using Machine Learning in Health CPS,https://ieeexplore.ieee.org/document/9469813/,"Cyber-Physical Systems (CPS) embed computation and communication capability into its core to regulate physical processes and seamlessly mediate between the cyber and the physical world for various control and monitoring tasks. Health CPS, a variant of CPS in the healthcare sector, acts as a health monitoring system to dynamically capture, process, and analyze health sensor data through integrated internet of things (IoT)-enabled cyber-physical processes. These systems can suitably support patients suffering from non-communicable diseases (NCDs) or who are at risk of suffering from those. Identifying the risk of NCDs, such as heart disease and diabetes, requires artificial intelligence (AI) techniques into the core of health CPS. Recently, there has been growing interest to incorporate machine learning into CPS, which can facilitate the disease classification, detection, monitoring, and prediction of several NCDs. However, there is a shortage of visible work that focus on early-stage risk prediction of these diseases. In this work, we propose a novel machine learning based health CPS framework that addresses the challenge of effectively processing the wearable IoT sensor data for early risk prediction of diabetes as an example of NCDs. In the experiment, a verified diabetic dataset has been used for training, while the testing has been performed on an artificially generated data collection from sensors. The experiment with several machine learning algorithms shows the effectiveness of the proposed approach in achieving the maximum precision from the Random Tree algorithm, which requires a minimum time of 0.01s to construct a model and obtains 94% accuracy to predict the probability of diabetes at an early point.",health
10.1109/ICMA.2016.7558929,to_check,2016 IEEE International Conference on Mechatronics and Automation,IEEE,2016-08-10 00:00:00,ieeexplore,An artificial neural network based haptic rendering of contact with deformable bodies,https://ieeexplore.ieee.org/document/7558929/,"This paper presents an artificial neural network based 3-DOF haptic rendering scheme to render the contact force between a rigid object and a deformable body in a virtual environment. The finite-element method (FEM) technique is widely used for solving the deformation problem. However, this method has a heavy computational load to get accurate result, so it is difficult to apply this method to haptic simulating. To solve the challenging problem, in this paper, we presented a new motion control scheme to divide the motion of a rigid virtual object into three sub-movements along the three axes of a Cartesian-coordinate, based on which three single-input and single-output neural networks can be separately used to compute the three feedback force components along all the coordinate axes. The vector composition of the three force components is the feedback force exerted to a user through a haptic device. The proposed method can ensure the high accuracy and the high update rate of 3-DOF haptic rendering of deformable bodies. To testify the accuracy of the artificial neural network for haptic rendering, a medical robot is used to measure the data of the neural network training in the physical world, and a haptic device based experiment with a virtual environment validates the proposed algorithm for 3-DOF haptic rendering.",health
10.1109/ICCIS.2004.1460763,to_check,"IEEE Conference on Cybernetics and Intelligent Systems, 2004.",IEEE,2004-12-03 00:00:00,ieeexplore,Sensor fusion system for improving the recognition of 3D object,https://ieeexplore.ieee.org/document/1460763/,"Human being recognizes the physical world by integrating a great variety of sensory inputs, the information acquired by their own action, and their knowledge of the world using hierarchically parallel-distributed mechanism. In this paper, authors propose the sensor fusion system that can recognize multiple 3D objects from 2D projection images and tactile information. The proposed system focuses on improving object recognition rate. Unlike the conventional object recognition system that uses image sensor alone, the proposed method uses tactual sensors in addition to visual sensor. Tactual signals are obtained from the reaction force by the pressure sensors at the fingertips when unknown objects are grasped by four-fingered robot hand. The experiment evaluates the recognition rate and the number of learning iterations of various objects. The experimental results show that the proposed system can improve recognition rate and reduce learning time. These results verify the effectiveness of the proposed sensor fusion system as 3D object recognition scheme",multimedia
