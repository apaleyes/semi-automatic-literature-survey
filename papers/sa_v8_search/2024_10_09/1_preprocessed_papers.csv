doi,type,query_name,query_value,publication,publisher,publication_date,database,title,url,abstract,status,id
http://arxiv.org/abs/2410.01819v1,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',arxiv,arxiv,2024-09-16 00:00:00,arxiv,strategic ai governance: insights from leading nations,http://arxiv.org/abs/2410.01819v1,"Artificial Intelligence (AI) has the potential to revolutionize various
sectors, yet its adoption is often hindered by concerns about data privacy,
security, and the understanding of AI capabilities. This paper synthesizes AI
governance approaches, strategic themes, and enablers and challenges for AI
adoption by reviewing national AI strategies from leading nations. The key
contribution is the development of an EPIC (Education, Partnership,
Infrastructure, Community) framework, which maps AI implementation requirements
to fully realize social impacts and public good from successful and sustained
AI deployment. Through a multi-perspective content analysis of the latest AI
strategy documents, this paper provides a structured comparison of AI
governance strategies across nations. The findings offer valuable insights for
governments, academics, industries, and communities to enable responsible and
trustworthy AI deployments. Future work should focus on incorporating specific
requirements for developing countries and applying the strategies to specific
AI applications, industries, and the public sector.",unknown,1
http://arxiv.org/abs/2212.13993v2,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',arxiv,arxiv,2022-12-25 00:00:00,arxiv,"metaverse communications, networking, security, and applications:
  research issues, state-of-the-art, and future directions",http://arxiv.org/abs/2212.13993v2,"Metaverse is an evolving orchestrator of the next-generation Internet
architecture that produces an immersive and self-adapting virtual world in
which humans perform activities similar to those in the real world, such as
playing sports, doing work, and socializing. It is becoming a reality and is
driven by ever-evolving advanced technologies such as extended reality,
artificial intelligence, and blockchain. In this context, Metaverse will play
an essential role in developing smart cities, which becomes more evident in the
post COVID 19 pandemic metropolitan setting. However, the new paradigm imposes
new challenges, such as developing novel privacy and security threats that can
emerge in the digital Metaverse ecosystem. Moreover, it requires the
convergence of several media types with the capability to quickly process
massive amounts of data to keep the residents safe and well-informed, which can
raise issues related to scalability and interoperability. In light of this,
this research study aims to review the literature on the state of the art of
integrating the Metaverse architecture concepts in smart cities. First, this
paper presents the theoretical architecture of Metaverse and discusses
international companies interest in this emerging technology. It also examines
the notion of Metaverse relevant to virtual reality, identifies the prevalent
threats, and determines the importance of communication infrastructure in
information gathering for efficient Metaverse operation. Next, the notion of
blockchain technologies is discussed regarding privacy preservation and how it
can provide tamper-proof content sharing among Metaverse users. Finally, the
application of distributed Metaverse for social good is highlighted.",unknown,2
http://arxiv.org/abs/1906.05684v1,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',arxiv,arxiv,2019-06-11 00:00:00,arxiv,understanding artificial intelligence ethics and safety,http://arxiv.org/abs/1906.05684v1,"A remarkable time of human promise has been ushered in by the convergence of
the ever-expanding availability of big data, the soaring speed and stretch of
cloud computing platforms, and the advancement of increasingly sophisticated
machine learning algorithms. Innovations in AI are already leaving a mark on
government by improving the provision of essential social goods and services
from healthcare, education, and transportation to food supply, energy, and
environmental management. These bounties are likely just the start. The
prospect that progress in AI will help government to confront some of its most
urgent challenges is exciting, but legitimate worries abound. As with any new
and rapidly evolving technology, a steep learning curve means that mistakes and
miscalculations will be made and that both unanticipated and harmful impacts
will occur.
  This guide, written for department and delivery leads in the UK public sector
and adopted by the British Government in its publication, 'Using AI in the
Public Sector,' identifies the potential harms caused by AI systems and
proposes concrete, operationalisable measures to counteract them. It stresses
that public sector organisations can anticipate and prevent these potential
harms by stewarding a culture of responsible innovation and by putting in place
governance processes that support the design and implementation of ethical,
fair, and safe AI systems. It also highlights the need for algorithmically
supported outcomes to be interpretable by their users and made understandable
to decision subjects in clear, non-technical, and accessible ways. Finally, it
builds out a vision of human-centred and context-sensitive implementation that
gives a central role to communication, evidence-based reasoning, situational
awareness, and moral justifiability.",unknown,3
http://arxiv.org/abs/1802.01029v1,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',arxiv,arxiv,2018-02-03 00:00:00,arxiv,"fairness and accountability design needs for algorithmic support in
  high-stakes public sector decision-making",http://arxiv.org/abs/1802.01029v1,"Calls for heightened consideration of fairness and accountability in
algorithmically-informed public decisions---like taxation, justice, and child
protection---are now commonplace. How might designers support such human
values? We interviewed 27 public sector machine learning practitioners across 5
OECD countries regarding challenges understanding and imbuing public values
into their work. The results suggest a disconnect between organisational and
institutional realities, constraints and needs, and those addressed by current
research into usable, transparent and 'discrimination-aware' machine
learning---absences likely to undermine practical initiatives unless addressed.
We see design opportunities in this disconnect, such as in supporting the
tracking of concept drift in secondary data sources, and in building usable
transparency tools to identify risks and incorporate domain knowledge, aimed
both at managers and at the 'street-level bureaucrats' on the frontlines of
public service. We conclude by outlining ethical challenges and future
directions for collaboration in these high-stakes applications.",unknown,4
http://arxiv.org/abs/1609.08765v1,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',arxiv,arxiv,2016-09-28 00:00:00,arxiv,"irescu - data for social good saving lives bridging the gaps in sudden
  cardiac arrest survival",http://arxiv.org/abs/1609.08765v1,"Currently every day in the USA 1000 people die of sudden cardiac arrest (SCA)
outside of hospitals or ambulances - before emergency medical help arrives - in
the streets, workplaces, schools and homes of our cities, adults and children.
Brain death commences in 3 minutes, and often the ambulance just can't be there
in time. Citizen cardiopulmonary resuscitation (CPR) and automated external
defibrillator (AED) use can save precious minutes and lives. Using public
access AED's saves lives in SCA- however AEDs are used in <2% of cardiac
arrests, though could save lives in 80% if available, findable, functioning,
and used. The systems problem to solve is that there is no comprehensive or
real time accessible database of the AED locations, and also it is not known
that they are actually being positioned where they are needed. The iRescU
project is designed to bridge this gap in SCA survival, by substantially
augmenting the AED database. Utilizing a combination of AED crowd sourcing and
geolocation integrated with existing 911 services and SCA events and projected
events based on machine learning data information to help make the nearest AED
accessible and available in the setting of a SCA emergency and to identify the
areas of greatest need for AEDs to be positioned in the community. Helping to
save lives and address preventable death with a social good approach and
applied big data.",unknown,5
,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,AAAI Fall Symposia,2017-01-01 00:00:00,semantic_scholar,position paper: rational behavior model (rbm) and human-robot ethical constraints using mission execution ontology (meo),https://www.semanticscholar.org/paper/40d15d05ca46245cb5e33a391e7892c3fe2bfc68,"Autonomous systems can be ethically supervised by humans without constant communications. Adding constraints such as no-fly zones, time limitations, permission prerequisites etc. to mission orders allows operators to legally and ethically control mobile systems that have the potential for deliberate (or unintentional) lethal force. Ethical control can be practically achieved by providing parsable (and ethically validatable) orders to diverse unmanned systems. Ethical Control of Unmanned Systems The authors have been engaged for several decades in academic research and military service relating to mission specification and execution for submarines, aircraft, and ground vehicles, both manned and robotic. We have increasingly focused on maritime robotics, especially with regard to execution of unmanned underwater vehicle missions under varying degrees of human oversight and ethical constraints. This work has included numerous simulations and also deployed experiments (Brutzman et al. 1998; Brutzman, McGhee, and Davis 2012; Brutzman, Davis, Blais, and McGhee 2016). Common conclusions that treat ethical robots as an always-amoral philosophical conundrum or requiring undemonstrated morality-based artificial intelligence (AI) are simply not sensible or repeatable. For better or worse, actors around the world are rapidly designing and deploying mobile unmanned systems to augment human capabilities. Thus theory must meet practice. This contribution describes how mission orders can be specified in forms readable by both human operators and robot systems, including both syntactic validation of correctness and semantic validation of logical coherence. This approach has the potential to meet the moral requirements of international laws regarding human responsibility during armed conflict when unmanned systems are deployed. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. The path to reach our present point of view has come from understanding that the Rational Behavior Model (RBM) software architecture used in our work has broader applicability than previously realized. This is because RBM depends upon a software construct called a Mission Execution Automaton (MEA) that is an extension of a Turing machine (TM) allowing the incorporation of arbitrary external agents. Such agents are presumed to be capable of interaction with their environment and returning one of a predetermined set of values to the finite state machine (FSM) portion of the MEA. Ternary logic is helpful, where allowable values returned from a query/command to an external agent have been limited to the set [success, failure, constraint]. Applied to a multiphase mission, concise mission-branch definition simplifies execution logic needed by remote systems. When a value constraint is returned, it can be taken to mean either that the current environment makes it unethical to complete the current phase, that a phase timeout has occurred, or that success or failure of the current phase cannot be determined, etc. Exception-handling steps can then occur. Recent work has reached the next level, defining a Mission Execution Ontology (MEO) implemented in RDF/OWL that relates mission goals, task prerequisites and operating constraints with vehicle capabilities. MEO validation can be performed as part of tasking robotic or human external agents. This means that humanunderstandable orders to unmanned systems can further be semantically validated for logical correctness, ensuring that tasking of remote systems meets the same level of ethical rigor expected in human-to human orders. Confirmed validation makes authority meaningful for responsible humans. In three-level robot architectures such as RBM, existing vehicle commands/missions can be incorporated as behaviors subject to overall regulation by rational (finite state) supervision of a mission-definition graph. Such tasking can be understood by qualified humans who are not computer professionals, much in the same way that military mission orders written in structured human language are understandable. Well-specified definitions of mission tasks and constraints can also be performed by a wide variety of diverse robot control code. With such a common understanding, it is possible to assign legal and moral responsibility for correct and ethical mission definition to a single (legally culpable) human individual. We believe that this kind of strong accountability is essential to military accountability, and may eventually become relevant for emerging robotic technologies affecting public safety. Algorithms cannot replace human responsibility. Even so, a fully testable technology (such as that provided by the MEA and MEO formalisms) allows for the assignment of human accountability. Specifically, the MEA provides a mathematically rigorous mechanism for mission definition and execution as an exhaustively testable flow diagram. This approach ensures that accountable operators can fully understand all high-level task sequences before authorizing robot operations. The MEO employs description logics (DLs) and Semantic Web technologies to provide strong assurances that MEA mission definitions are semantically correct and fully executable by specific target vehicles. By applying the best strengths of human ethical responsibility, repeatable formal logic and directable unmanned systems together, these capabilities provide a practical framework for ethically grounded human supervision of unmanned systems.",unknown,6
10.1145/3219819.3226071,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Knowledge Discovery and Data Mining,2018-01-01 00:00:00,semantic_scholar,societal impact of data science and artificial intelligence,https://www.semanticscholar.org/paper/9dacbc65c23215f79a9a966abf8c403175e56aa8,"The explosion of interest in KDD and other Data Science/Machine Learning/AI conferences is just one of the many signs that these technologies are no longer confined to the realms of academia and a hand-full of tech companies. As our daily lives seamlessly integrate more and more data-driven applications, people's excitement is tempered by worry about the technologies' potential to disrupt their existence. Having worked for almost 30 years to design and develop these technologies, the KDD community now should examine and debate the impact of Machine Learning & AI on the broader world. Beyond the hype, where do we stand with respect to the dangers? What role can our community play to alleviate concerns around AI taking jobs, or taking over? How can the value derived from data be distributed fairly? Are concerns about inequity well-founded or rather largely problems of perception? What can be done to bring data hunger and data sharing concerns to a level of equilibrium? How do we prepare people to interact with intelligent systems at scale? Can we unleash the incredible responsiveness of the KDD community toward longer-term more impactful projects across sectors that are essential for social good, such as Health, Environmental Sustainability, and Public Welfare.",unknown,7
10.1177/0952076718780537,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Public Policy and Administration,2018-01-01 00:00:00,semantic_scholar,"big data and ai – a transformational shift for government: so, what next for research?",https://www.semanticscholar.org/paper/2eedf3d19edacc862bf7ab324a74172bb35a14e1,"Big Data and artificial intelligence will have a profound transformational impact on governments around the world. Thus, it is important for scholars to provide a useful analysis on the topic to public managers and policymakers. This study offers an in-depth review of the Policy and Administration literature on the role of Big Data and advanced analytics in the public sector. It provides an overview of the key themes in the research field, namely the application and benefits of Big Data throughout the policy process, and challenges to its adoption and the resulting implications for the public sector. It is argued that research on the subject is still nascent and more should be done to ensure that the theory adds real value to practitioners. A critical assessment of the strengths and limitations of the existing literature is developed, and a future research agenda to address these gaps and enrich our understanding of the topic is proposed.",unknown,8
10.1145/3325112.3325261,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2019-01-01 00:00:00,semantic_scholar,a realist perspective on ai-era public management*,https://www.semanticscholar.org/paper/ed5fbb58585ae1e9154ade1b4639060ace9c45ec,"Recent years have witnessed a number of significant ideas and approaches to addressing the shortcomings of the New Public Management paradigm. Three of these recent ideas, which include Digital Era Governance, Public Value Management, and New Public Governance, emphasise partnerships collaboration and engagement of citizens; performance governance and innovation and recognize the transformational potentials of digital technologies. Artificial Intelligence (AI) is one of the digital technologies attracting the greatest interest in public administration in terms of its potential impact. There are already a number of reports on how AI is being deployed in the public sector with good outcomes. By employing a realist review approach, this study investigates the specific mechanisms across post-NPM, organisational, individual and innovation contexts which are associated with positive outcomes from AI initiatives in the public sector. The study further examined the specific applications of AI initiatives within Post-NPM agendas. Our findings provide some empirical evidence for a better understanding of the conditions and where to target AI-based solutions in post-NPM context for positive outcomes.",unknown,10
10.1145/3325112.3325245,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2019-01-01 00:00:00,semantic_scholar,the data firehose and ai in government: why data management is a key to value and ethics,https://www.semanticscholar.org/paper/9eacf62f1e546748428c7e4843731b1595294200,"Technical and organizational innovations such as Open Data, Internet of Things and Big Data have fueled renewed interest in policy analytics in the public sector. This revamped version of policy analysis continues the long-standing tradition of applying statistical modeling to better understand policy effects and decision making, but also incorporates other computational approaches such as artificial intelligence (AI) and computer simulation. Although much attention has been given to the development of capabilities for data analysis, there is much less attention to understanding the role of data management in a context of AI in government. In this paper, we argue that data management capabilities are foundational to data analysis of any kind, but even more important in the present AI context. This is so because without proper data management, simply acquiring data or systems will not produce desired outcomes. We also argue that realizing the potential of AI for social good relies on investments specifically focused on this social outcome, investments in the processes of building trust in government data, and ensuring the data are ready and suitable for use, for both immediate and future uses.",unknown,11
10.1145/3322640.3326722,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Conference on Artificial Intelligence and Law,2019-01-01 00:00:00,semantic_scholar,artificial intelligence and law: what do people really want?: example of a french multidisciplinary working group,https://www.semanticscholar.org/paper/b4fcc459433d2fc35b0214f81a2145bcbc74fe5f,"This paper addresses issues related to the ethical consequences of using AI technologies in court decisions. With the prodigious technological leap made in the field of artificial intelligence in recent years, disruptive innovations have affected many business sectors, with economic, social and ethical consequences. But what do people really want about the application of artificial intelligence technologies in the law system? This article presents a general methodological approach to take into account the ethical aspect of the introduction of a new technology in a given domain. We apply this methodology in the specific case of the introduction of AI technologies in the law system. As a multidisciplinary working group interested in this application in the case of France, we have organized a series of workshops to discuss this topic and highlight the respective values and interests of each stakeholder. The result of this work in presented in the form of an ethical matrix that can be used as a tool by the public authorities to help decision-making on the subject with a prioritization of certain values in order to reflect the respect for fundamental rights.",unknown,12
10.5624/isd.2019.49.1.1,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Imaging Science in Dentistry,2019-01-01 00:00:00,semantic_scholar,an overview of deep learning in the field of dentistry,https://www.semanticscholar.org/paper/28b167de9fd1443bfd7c186550909880765a3470,"Purpose Artificial intelligence (AI), represented by deep learning, can be used for real-life problems and is applied across all sectors of society including medical and dental field. The purpose of this study is to review articles about deep learning that were applied to the field of oral and maxillofacial radiology. Materials and Methods A systematic review was performed using Pubmed, Scopus, and IEEE explore databases to identify articles using deep learning in English literature. The variables from 25 articles included network architecture, number of training data, evaluation result, pros and cons, study object and imaging modality. Results Convolutional Neural network (CNN) was used as a main network component. The number of published paper and training datasets tended to increase, dealing with various field of dentistry. Conclusion Dental public datasets need to be constructed and data standardization is necessary for clinical application of deep learning in dental field.",unknown,13
10.1109/ecai46879.2019.9042157,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,European Conference on Artificial Intelligence,2019-01-01 00:00:00,semantic_scholar,intelligent solutions - based framework for digital public services. a case study for smart transportation,https://www.semanticscholar.org/paper/3d863678d53ef04773b3e6052995b85db1903e28,"Digital technology landscape is continuously improving, dragging along both the transformation of public services and new demands of citizens. Emerging new technologies like Artificial Intelligence, Machine Learning, Deep Learning or Internet of Things provide tremendous means to implement intelligent solutions for reshaping digital public services. This paper aims to disclose the most important features of several intelligent technologies and of these types of public services that can be integrated for providing new capabilities. An AI-based architecture for supporting digital public services in the smart transportation sector is presented in order to demonstrate the highlighted ideas and concepts.",unknown,14
10.1109/mc.2020.3010043,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Computer,2020-01-01 00:00:00,semantic_scholar,artificial intelligence in government,https://www.semanticscholar.org/paper/5bca0d47912b214890e9ca6ee7a76f4deb8de815,"The articles in this special section focus on government applications that use artificial intelligence (AI). The repercussions of artificial intelligence (AI) in government are broad and significant. The characteristics of these technologies will have an impact on almost everything in public organizations, from governance or the multidimensional perspective of interoperability, to the organizational or social implications linked to concepts like public value, transparency, or accountability. This special issue seeks to shed light on foundations and key elements to be taken into account for AI adoption by public organizations. Governments are the primary enablers of technology and market stimulators and regulators of general activities in our society. Governments have always sought the common good and, therefore, the advancement of public and collective interests. This is key to understanding, as a first step, why the principles of public-sector organizations do not always match those of the private sector. Public and private perspectives are very different, whether they be management, strategy, or policy.",unknown,16
10.1145/3428502.3428510,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Conference on Theory and Practice of Electronic Governance,2020-01-01 00:00:00,semantic_scholar,accelerating public service delivery in india: application of internet of things and artificial intelligence in agriculture,https://www.semanticscholar.org/paper/9ff0d4638306c8f7165d94d94113221a03c748d6,"The application of Information and Communication Technologies (ICTs) in the public sector can usher performance enhancement, productivity and social equity in public service delivery mechanisms. More specifically, emerging digital technologies including Artificial Intelligence (AI) can be employed for more effective retrieval and analysis of complex, real-time data that could also be captured and shared by devices supporting Internet of Things. Literature asserts that governments worldwide must adopt solutions offered by these emerging technologies to drive innovation in public service delivery mechanisms. Appreciating these claims, this study aims to explore the current and potential use of IoT and AI. Based on the related review of literature, the study puts forth a conceptual framework for creating an open and integrated national level agriculture stack (christened as KisanOne by the authors) so that developing countries like India can effectively espouse data driven approach in its agriculture sector. ""Kisan One"" combines varied aspects of a farmers' activities including weather forecast, soil health indices, seed procurement cycle, sowing cycles, details of fertilizers availability, crop prices, etc, in a unified national stack that is accessible to all the stakeholders using application programming interfaces (APIs). Needless to say, the proposed KisanOne is a utopian implementation where existing and contemporary digital initiatives get unified on a single platform.Datasets themselves have little intrinsic value sans any ability to extract meaning from it. Intelligent data analytics could be employed on real time datasets of KisanOne both for evidence based decision making as well as for malicious intent. This paper, therefore, attempts to offer an insight into such challenges as well as suggest policy recommendations that could strengthen existing regulatory mechanisms for effective implementation of IoT and AI in existing public service delivery schemes of India. The paper is divided into four broad sections. The first section builds the Background of the paper. The next section is divided into four subsections and in this section instance of Agriculture has been detailed with reference to its current scenario and prevailing solutions. India has started using technology in Agriculture to a great extent- some of these applications such as Kisan Suvidha2 mobile app, mKisan SMS Portal, Farmer's Portal, Soil Health Card, Fertilizer Monitoring System(FMS) software, Agrimarket App have been delineated in the study. A use case on transformation of agriculture sector using IoT and AI is also presented in one of the sub-sections. A National Level Integrated Agriculture Stack is also proposed in this paper. The subsequent section presents brief picture of key challenges of implementing IoT and AI in Agriculture sector followed by recommendations and Consulive Remarks. It is an innovative and descriptive study that primarily relies on secondary data gleaned from international/national journals, reports of Ministry of Electronics and Information Technology, Government of India and other online academic sources coupled with creative out-of-box thinking to propose the application of IoT and AI in varied public sectors with special emphasis on Agriculture.",unknown,17
10.1145/3396956.3396965,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2020-01-01 00:00:00,semantic_scholar,public decision making: connecting artificial intelligence and crowds,https://www.semanticscholar.org/paper/14c12f9d44ef23a8c2d39bcf1e3ab1a13d684054,"The recent breakthrough of artificial intelligence, as well as the wide adoption of the wisdom of the crowd, also known as collective intelligence, across sectors, has received attention and excitement across disciplines. In addition to the scientific breakthrough, recent public sector studies recognize AI's potential contributions in public services, such as big data for decision making, the development of smart cities, and social and health care. Studies have also recognized crowdsourcing's potential for service provisions, innovation, information generation, and policymaking. However, we have only a limited understanding of the connections between these two types of intelligence and adoption conditions to properly utilize them for the public sector. To understand what roles AI and crowds can play in enhancing public services and policymaking, we adopt a bibliometric analysis to identify emerging themes and interconnections between these two streams of literature. Our study provides key themes and significance for each cluster. Our first examination of AI and crowd literature regarding connection to public values, complementary in public decision making, as well as future potential for joint adoption by governments provides some implications for future considerations.",unknown,18
10.1177/0266382120923962,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Business Information Review,2020-01-01 00:00:00,semantic_scholar,regulation and ethics in artificial intelligence and machine learning technologies: where are we now? who is responsible? can the information professional play a role?,https://www.semanticscholar.org/paper/4ae7b75e7b0fdf00a687c8f308ceccbe26c24e8d,"Artificial intelligence (AI) and machine learning (ML) technologies are rapidly maturing and proliferating through all public and private sectors. The potential for these technologies to do good and to help us in our everyday lives is immense. But there is a risk that unless managed and controlled AI can also cause us harm. Questions about regulation, what form it takes and who is responsible for governance are only just beginning to be answered. In May 2019, 42 countries came together to support a global governance framework for AI. The Organisation for Economic Co-operation and Development (OECD) Principles on Artificial Intelligence (OECD (2019) OECD principles on AI. Available at: https://www.oecd.org/going-digital/ai/principles/ (accessed 2 March 2020)) saw like-minded democracies of the world commit to common AI values of trust and respect. In Europe, the European Commission’s (EC) new president, Ursula von der Leyen has made calls for a General Data Protection Regulation style. As a first step the EC has published a white paper: ‘On Artificial Intelligence – A European Approach to Excellence and Trust’ (European Commission (2020) Report, Europa, February). In February 2020, the UK government has published a report on ‘Artificial Intelligence in the Public Sector’ (The Committee on Standards in Public Life (2020) Artificial intelligence and public standards. Report, UK Government, February). This article discusses some of the potential threats AI may hold if left unregulated. It provides a brief overview of the regulatory activities for AI worldwide, and in more detail the current UK AI regulatory landscape. Finally, the article looks at the role that the information professional might play in AI and ML.",unknown,19
10.1109/infocom41043.2020.9155501,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Conference on Computer Communications,2020-01-01 00:00:00,semantic_scholar,ruegan: embracing a self-adversarial agent for building a defensible edge security architecture,https://www.semanticscholar.org/paper/a959ab5900bc87df1b8ab8618b37d96bc8a4df3c,"In the era of edge computing and Artificial Intelligence (AI), securing billions of edge devices within a network against intelligent attacks is crucial. We propose PUFGAN, an innovative machine learning attack-proof security architecture, by embedding a self-adversarial agent within a device fingerprint- based security primitive, public PUF (PPUF) known for its strong fingerprint-driven cryptography. The self-adversarial agent is implemented using Generative Adversarial Networks (GANs). The agent attempts to self-attack the system based on two GAN variants, vanilla GAN and conditional GAN. By turning the attacking quality through generating realistic secret keys used in the PPUF primitive into system vulnerability, the security architecture is able to monitor its internal vulnerability. If the vulnerability level reaches at a specific value, PUFGAN allows the system to restructure its underlying security primitive via feedback to the PPUF hardware, maintaining security entropy at as high a level as possible.We evaluated PUFGAN on three different machine environments: Google Colab, a desktop PC, and a Raspberry Pi 2, using a real-world PPUF dataset. Extensive experiments demonstrated that even a strong device fingerprint security primitive can become vulnerable, necessitating active restructuring of the current primitive, making the system resilient against extreme attacking environments.",unknown,20
10.1145/3375627.3377139,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"AAAI/ACM Conference on AI, Ethics, and Society",2020-01-01 00:00:00,semantic_scholar,the ai-development connection - a view from the south,https://www.semanticscholar.org/paper/06daf0dbac7767f2cfa58b56abe7d8ae148d8f3a,"The socialisation of Artificial Intelligence and the reality of an intelligence economy mark an epochal moment. The impacts of AI are now systemic - restructuring economic organisation and value chains, public sphere architectures and sociality. These shifts carry deep geo-political implications, reinforcing historical exclusions and power relations and disrupting the norms and rules that hold ideas of equality and justice together. At the centre of this rapid change is the intelligent corporation and its obsessive pursuit of data. Directly impinging on bodies and places, the de facto rules forged by the intelligent corporation are disenfranchising the already marginal subjects of development. Using trade deals to liberalise data flows, tighten trade secret rules and enclose AI-based innovation, Big Tech and their political masters have effectively taken away the economic and political autonomy of states in the global south. Big Tech's impunity extends to a brazen exploitation - enslaving labour through data over-reach and violating female bodies to universalise data markets. Thinking through the governance of AI needs new frameworks that can grapple with the fraught questions of data sovereignty, economic democracy, and institutional ethics in a global world with local aspirations. Any effort towards norm development in this domain will need to see the geo-economics of digital intelligence and the geo-politics of development ideologies as two sides of the same coin.",unknown,21
10.3233/ip-200249,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Inf. Polity,2020-01-01 00:00:00,semantic_scholar,administration by algorithm: a risk management framework,https://www.semanticscholar.org/paper/8d03df21807637482739cbfe140b260aa8f1907b,"Algorithmic decision-making is neither a recent phenomenon nor one necessarily associated with artificial intelligence (AI), though advances in AI are increasingly resulting in what were heretofore human decisions being taken over by, or becoming dependent on, algorithms and technologies like machine learning. Such developments promise many potential benefits, but are not without certain risks. These risks are not always well understood. It is not just a question of machines making mistakes; it is the embedding of values, biases and prejudices in software which can discriminate against both individuals and groups in society. Such biases are often hard either to detect or prove, particularly where there are problems with transparency and accountability and where such systems are outsourced to the private sector. Consequently, being able to detect and categorise these risks is essential in order to develop a systematic and calibrated response. This paper proposes a simple taxonomy of decision-making algorithms in the public sector and uses this to build a risk management framework with a number of components including an accountability structure and regulatory governance. This framework is designed to assist scholars and practitioners interested in ensuring structured accountability and legal regulation of AI in the public sphere.",unknown,22
10.1145/3463677.3463721,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2021-01-01 00:00:00,semantic_scholar,"cyber-physical innovations:: cyber-infrastructure for research, cyber-physical architecture for real-time applications, autonomous vehicle (av) governance and ai artifacts for public value",https://www.semanticscholar.org/paper/2ea2ea3d0128b1b2d3e89dd442d26f1441111ebf,"This panel explores the development of innovative, integrative, and versatile strategies to facilitate more practical and effective use of intelligent cyber-physical technologies from a variety of perspectives, including engineering, regulation, management, and research. With the same goal of sustaining the development of emerging technologies to best benefit our communities, this panel shares their different approaches in terms of engineering solutions for real-time controlling in cyber-physical systems, regulatory strategies to overcome the conflict between efficiency and autonomy, artifacts for artificial intelligence project management, and meeting researcher needs through large-scale cyberinfrastructure. The selected cases discussed in this panel not only highlight the critical challenges in implementing cyber-physical technologies into real applications but also suggest promising strategies to overcome those issues from diverse facets.",unknown,24
10.2196/28858,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Medical Internet Research,2021-01-01 00:00:00,semantic_scholar,harnessing artificial intelligence for health message generation: the folic acid message engine,https://www.semanticscholar.org/paper/8e260e06c73f27f7a477021ce1b068099c2064bf,"Background Communication campaigns using social media can raise public awareness; however, they are difficult to sustain. A barrier is the need to generate and constantly post novel but on-topic messages, which creates a resource-intensive bottleneck. Objective In this study, we aim to harness the latest advances in artificial intelligence (AI) to build a pilot system that can generate many candidate messages, which could be used for a campaign to suggest novel, on-topic candidate messages. The issue of folic acid, a B-vitamin that helps prevent major birth defects, serves as an example; however, the system can work with other issues that could benefit from higher levels of public awareness. Methods We used the Generative Pretrained Transformer-2 architecture, a machine learning model trained on a large natural language corpus, and fine-tuned it using a data set of autodownloaded tweets about #folicacid. The fine-tuned model was then used as a message engine, that is, to create new messages about this topic. We conducted a web-based study to gauge how human raters evaluate AI-generated tweet messages compared with original, human-crafted messages. Results We found that the Folic Acid Message Engine can easily create several hundreds of new messages that appear natural to humans. Web-based raters evaluated the clarity and quality of a human-curated sample of AI-generated messages as on par with human-generated ones. Overall, these results showed that it is feasible to use such a message engine to suggest messages for web-based campaigns that focus on promoting awareness. Conclusions The message engine can serve as a starting point for more sophisticated AI-guided message creation systems for health communication. Beyond the practical potential of such systems for campaigns in the age of social media, they also hold great scientific potential for the quantitative analysis of message characteristics that promote successful communication. We discuss future developments and obvious ethical challenges that need to be addressed as AI technologies for health persuasion enter the stage.",unknown,25
10.2139/ssrn.3880779,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Social Science Research Network,2021-01-01 00:00:00,semantic_scholar,"the role of social movements, coalitions, and workers in resisting harmful artificial intelligence and contributing to the development of responsible ai",https://www.semanticscholar.org/paper/826a19bda59aa7ce8a33235b35c0480aac827ea1,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles"", there is mounting public concern over the influence that the AI systems have in our society, and coalitions in all sectors are organizing to resist harmful applications of AI worldwide. Responses from peoples everywhere, from workers protesting unethical conduct and applications of AI, to student's protesting MIT's relationships with donor, sex trafficker, and pedophile Jeffery Epstein, to the healthcare community, to indigenous people addressing “the twin problems of a lack of reliable data and information on indigenous peoples and biopiracy and misuse of their traditional knowledge and cultural heritage”, to smart city stakeholders, to many others. Like corporations, governments around the world have adopted strategies for becoming leaders in the development and use of Artificial Intelligence, fostering environments congenial to AI innovators. Neither corporations nor policymakers have sufficiently addressed how the rights of children fit into their AI strategies or products. The role of artificial intelligence in children’s lives—from how children play, to how they are educated, to how they consume information and learn about the world—is expected to increase exponentially over the coming years. Thus, it’s imperative that stakeholders evaluate the risks and assess opportunities to use artificial intelligence to maximize children’s wellbeing in a thoughtful and systematic manner. This paper discusses AI and children's rights in the context of social media platforms such as YouTube, smart toys, and AI education applications. The Hello Barbie, Cloud Pets, and Cayla smart toys case studies are analyzed, as well as the ElsaGate social media hacks and education's new Intelligent Tutoring Systems and surveillance of students apps. Though AI has valuable benefits for children, it presents some particular challenges around important issues including child safety, privacy, data privacy, device security and consent. Technology giants, all of whom are heavily investing in and profiting from AI, must not dominate the public discourse on responsible use of AI. We all need to shape the future of our core values and democratic institutions. As artificial intelligence continues to find its way into our daily lives, its propensity to interfere with our rights only gets more severe. Many of the issues mentioned in this examination of harmful AI are not new, but they are greatly exacerbated and threatened by the scale, proliferation, and real-life impact that artificial intelligence facilitates. The potential of artificial intelligence to both help and harm people is much greater than earlier technologies. Continuing to examine what safeguards and structures can address AI’s problems and harms, including those that disproportionately impact marginalized people, is a critical activity. There are assumptions embedded in the AI algorithms that will shape how our world is realized. Many of these algorithms are wrongful and biased, they must get locked-in. Our best human judgment is needed to contain AI's harmful impacts. Perhaps one of the greatest contributions of AI will be to make us ultimately understand how important human wisdom truly is in life on earth.",unknown,26
10.3390/electronics10111223,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Electronics,2021-01-01 00:00:00,semantic_scholar,ai-enabled efficient and safe food supply chain,https://www.semanticscholar.org/paper/dad22e1cbdb21876ff1b4c10bb32f19fcc4a2a6a,"This paper provides a review of an emerging field in the food processing sector, referring to efficient and safe food supply chains, ’from farm to fork’, as enabled by Artificial Intelligence (AI). The field is of great significance from economic, food safety and public health points of views. The paper focuses on effective food production, food maintenance energy management and food retail packaging labeling control, using recent advances in machine learning. Appropriate deep neural architectures are adopted and used for this purpose, including Fully Convolutional Networks, Long Short-Term Memories and Recurrent Neural Networks, Auto-Encoders and Attention mechanisms, Latent Variable extraction and clustering, as well as Domain Adaptation. Three experimental studies are presented, illustrating the ability of these AI methodologies to produce state-of-the-art performance in the whole food supply chain. In particular, these concern: (i) predicting plant growth and tomato yield in greenhouses, thus matching food production to market needs and reducing food waste or food unavailability; (ii) optimizing energy consumption across large networks of food retail refrigeration systems, through optimal selection of systems that can be shut-down and through prediction of the respective food de-freezing times, during peaks of power demand load; (iii) optical recognition and verification of food consumption expiry date in automatic inspection of retail packaged food, thus ensuring safety of food and people’s health.",unknown,27
10.3389/frobt.2021.719944,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Frontiers in Robotics and AI,2021-01-01 00:00:00,semantic_scholar,on the social-relational moral standing of ai: an empirical study using ai-generated art,https://www.semanticscholar.org/paper/864400b32e8a2b1ba10ab1ed5007f6642adf3a17,"The moral standing of robots and artificial intelligence (AI) systems has become a widely debated topic by normative research. This discussion, however, has primarily focused on those systems developed for social functions, e.g., social robots. Given the increasing interdependence of society with nonsocial machines, examining how existing normative claims could be extended to specific disrupted sectors, such as the art industry, has become imperative. Inspired by the proposals to ground machines’ moral status on social relations advanced by Gunkel and Coeckelbergh, this research presents online experiments (∑N = 448) that test whether and how interacting with AI-generated art affects the perceived moral standing of its creator, i.e., the AI-generative system. Our results indicate that assessing an AI system’s lack of mind could influence how people subsequently evaluate AI-generated art. We also find that the overvaluation of AI-generated images could negatively affect their creator’s perceived agency. Our experiments, however, did not suggest that interacting with AI-generated art has any significant effect on the perceived moral standing of the machine. These findings reveal that social-relational approaches to AI rights could be intertwined with property-based theses of moral standing. We shed light on how empirical studies can contribute to the AI and robot rights debate by revealing the public perception of this issue.",unknown,29
10.1109/vtc2021-spring51267.2021.9448979,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Vehicular Technology Conference,2021-01-01 00:00:00,semantic_scholar,integrated service discovery and placement in information-centric vehicular network slices,https://www.semanticscholar.org/paper/55de3e6509cf99a7114ab80105800ad8c40ab476,"Connected and autonomous driving is one of the prominent vertical applications to showcase the capabilities of the 5G mobile network to transform and disrupt numerous industrial sectors and service chains. Cooperative autonomous mobility applications require ultra low latency communication, utilize massive broadband connections for vast numbers of connected devices, all of this under high mobility settings. Cloud and Edge Computing, artificial intelligence (AI), and the Internet of things (IoT) pave the path for the most important role of 5G: to attain an integration platform for the challenges of vertical applications. In this work, we first review these enabling features of 5G for autonomous driving and their integration in our autonomous driving testbed on urban public roads. We then propose an enhancement to the 5G vehicle to everything (V2X) architecture by incorporating the informationcentric communication paradigm. Our simulations of a proposed approach for integrated service discovery and placement in the high mobility settings of autonomous driving applications show improved service continuity and latency measured by hop counts to service endpoints and number of cache nodes. The results serve as a base line performance indicator for the practical implementation of the proposed approach.",unknown,30
10.48550/arxiv.2210.17218,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Grid Computing,2022-01-01 00:00:00,semantic_scholar,"artificial intelligence in government: concepts, standards, and a unified framework",https://www.semanticscholar.org/paper/2afd1d3199326dc8a9184a5152616f5cfda1e2e6,"Recent advances in artificial intelligence (AI), especially in generative language modelling, hold the promise of transforming government. Given the advanced capabilities of new AI systems, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that AI applications may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of AI, ML, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of AI in government, a balanced account that captures the full depth of theoretical perspectives needed to understand the consequences of embedding AI into a public sector context is lacking. Here, we unify efforts across social and technical disciplines by first conducting an integrative literature review to identify and cluster 69 key terms that frequently co-occur in the multidisciplinary study of AI. We then build on the results of this bibliometric analysis to propose three new multifaceted concepts for understanding and analysing AI-based systems for government (AI-GOV) in a more unified way: (1) operational fitness, (2) epistemic alignment, and (3) normative divergence. Finally, we put these concepts to work by using them as dimensions in a conceptual typology of AI-GOV and connecting each with emerging AI technical measurement standards to encourage operationalization, foster cross-disciplinary dialogue, and stimulate debate among those aiming to rethink government with AI.",unknown,31
10.59826/kdps.2022.23.143,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Korean Administrative Law Association,2022-01-01 00:00:00,semantic_scholar,research on institutional improvement for personal information collection and use in the public sector: use of video information collected by public area cctv,https://www.semanticscholar.org/paper/b39273df9ff8c62c60d3b079b991b8676c24d133,"In recent years, artificial intelligence technology, which enables new technologies to be deployed, has expanded the scope and methods of data collection and utilization of government agencies. Data analysis and AI technology are deployed to enhance the intelligent and accurate use of Closed-circuit television (CCTV) surveillance cameras for crime prevention, security, and public safety purposes. The use of facial recognition technology enables an individual to be identified by processing images captured on CCTV, the suspects to be tracked on the CCTV footage, and crime scenes to be monitored in real time. Amidst this expansion, there is an increasing need for discussions to protect privacy. The need for privacy control is emphasized as data collection, retention, and utilization capabilities resulting in losing control over individual’s own information. The value and importance of personal data have been changed from the previous, and government surveillance issues arises as vast quantities of personal data being stored and easily shared. The need for government data access for various purposes and storage of data on a large scale may lead to the privacy right and the infringement of informational self-determination. In a situation where the use of CCTV information is allowed and gradually expanding, it is difficult to evaluate that legal grounds for controlling the actions of government agencies or guaranteeing the rights of individual subjects are sufficient. This paper aims to examine the current status of data collection by government institutions, especially the institutional basis for the use of CCTV and video information collection by government agencies, and problems with the system regarding personal data protection.",unknown,32
10.1177/23998083221100550,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Environment and Planning B Urban Analytics and City Science,2022-01-01 00:00:00,semantic_scholar,urban-gan: an artificial intelligence-aided computation system for plural urban design,https://www.semanticscholar.org/paper/5ee1024949c8faa312677fe5d034aa9be0861fe1,"The current urban design computation is mostly centered on the professional designer while ignoring the plural dimension of urban design. In addition, available public participation computational tools focus mainly on information and idea sharing, leaving the public excluded in design generation because of their lack of design expertise. To address such an issue, this study develops Urban-GAN, a plural urban design computation system, to provide new technical support for design empowerment, allowing the public to generate their own designs. The sub-symbolic representation and artificial intelligence techniques of deep convolutional neural networks, case-based reasoning, and generative adversarial networks are used to acquire and embody design knowledge as the density function, and generate design schemes with this knowledge. The system consists of an urban form database and five process models through which the user with little design expertise can select urban form cases, generate designs similar to those cases, and make design decisions. The Urban-GAN is applied to hypothetical design experiments, which show that the user is able to apply the system to successfully generate distinctive designs following the urban form “styles” in Manhattan, Portland, and Shanghai. This study further extends the discussion about the plural urban design computation to general reflections on the goals and values in AI technique application in planning and design.",unknown,35
10.17705/1pais.14304,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Pacific Asia Journal of the Association for Information Systems,2022-01-01 00:00:00,semantic_scholar,strengthening public institutions and social inclusion of vulnerable groups in a developing country - innovation in organizations and artificial intelligence implications,https://www.semanticscholar.org/paper/be49e6c6ff3488433b44a0b542e4cf8f2429800d,"Background: In the context of a developing nation, children's participation in communal life is almost non-existent. The goal of the study is to contribute to national policies for local development that should prioritize the safety and well-being of the most vulnerable populations, particularly children under the age of 18. Innovating, including children in decision-making and maintaining local services in three pilot municipalities in order to prevent and combat all forms of exploitation to which they are exposed. How can Youth engagement in social and political community life be improved through better understanding of their needs and interests, and what are the artificial intelligence implications? Method: The methodology was used and designed to re-validate an existing program using pre-defined components of an agreement between the Italian and Lebanese governments. A needs study on the socio-demographic profile of youth and a situational analysis was conducted answering three objectives in the program of the Child Friendly City initiative. Results: Assuring the long-term viability and social inclusion of a significant socio-demographic group was successfully implemented: a free call center, software applications, a library, a digital network center, and the involvement of children on the municipal board of directors were established. The findings need to be adapted to various locations using artificial intelligence (AI) solutions and strategies for social awareness and behavior analysis. Conclusion: The importance of this study was underscored during the Covid-19 sanitary crisis, when some of these technologies enabled young people in impacted areas to integrate and become aware of the pandemic's risk. The case was based on theories such as Gender Inequalities and Children's Inclusion, Municipal Governance & Reform, Organizational Innovation (Public Sector), and Social Inclusion, and it demonstrates the value of innovating in the public sector and protecting vulnerable populations through the use of AI.",unknown,36
10.1093/toxsci/kfac101,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Toxicological Sciences,2022-01-01 00:00:00,semantic_scholar,machine learning and artificial intelligence in physiologically based pharmacokinetic modeling.,https://www.semanticscholar.org/paper/3c0bffd0afff7e3ad7d2e32bc633a304ceee0734,"Physiologically based pharmacokinetic (PBPK) models are useful tools in drug development and risk assessment of environmental chemicals. PBPK model development requires collection of species-specific physiological, and chemical-specific absorption, distribution, metabolism and excretion (ADME) parameters, which can be a time-consuming and expensive process. This raises a need to create computational models capable of predicting input parameter values for PBPK models, especially for new compounds. In this review, we summarize an emerging paradigm for integrating PBPK modeling with machine learning (ML) or artificial intelligence (AI)-based computational methods. This paradigm includes three steps: (a) obtain time-concentration PK data and/or ADME parameters from publicly available databases, (b) develop ML/AI-based approaches to predict ADME parameters, and (c) incorporate the ML/AI models into PBPK models to predict PK summary statistics (e.g., area under the curve and maximum plasma concentration). We also discuss a neural network architecture ""neural ordinary differential equation (Neural-ODE)"" that is capable of providing better predictive capabilities than other ML methods when used to directly predict time-series PK profiles. In order to support applications of ML/AI methods for PBPK model development, several challenges should be addressed: (1) as more data become available, it is important to expand the training set by including structural diversity of compounds to improve the prediction accuracy of ML/AI models; (2) due to the black box nature of many ML models, lack of sufficient interpretability is a limitation; (3) Neural-ODE has great potential to be used to generate time-series PK profiles for new compounds with limited ADME information, but its application remains to be explored. Despite existing challenges, ML/AI approaches will continue to facilitate efficient development of robust PBPK models for a large number of chemicals.",unknown,37
10.3390/s22051854,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Italian National Conference on Sensors,2022-01-01 00:00:00,semantic_scholar,"imtidad: a reference architecture and a case study on developing distributed ai services for skin disease diagnosis over cloud, fog and edge",https://www.semanticscholar.org/paper/157600e0348b99bbc4187b95fd0c438a2189a841,"Several factors are motivating the development of preventive, personalized, connected, virtual, and ubiquitous healthcare services. These factors include declining public health, increase in chronic diseases, an ageing population, rising healthcare costs, the need to bring intelligence near the user for privacy, security, performance, and costs reasons, as well as COVID-19. Motivated by these drivers, this paper proposes, implements, and evaluates a reference architecture called Imtidad that provides Distributed Artificial Intelligence (AI) as a Service (DAIaaS) over cloud, fog, and edge using a service catalog case study containing 22 AI skin disease diagnosis services. These services belong to four service classes that are distinguished based on software platforms (containerized gRPC, gRPC, Android, and Android Nearby) and are executed on a range of hardware platforms (Google Cloud, HP Pavilion Laptop, NVIDIA Jetson nano, Raspberry Pi Model B, Samsung Galaxy S9, and Samsung Galaxy Note 4) and four network types (Fiber, Cellular, Wi-Fi, and Bluetooth). The AI models for the diagnosis include two standard Deep Neural Networks and two Tiny AI deep models to enable their execution at the edge, trained and tested using 10,015 real-life dermatoscopic images. The services are evaluated using several benchmarks including model service value, response time, energy consumption, and network transfer time. A DL service on a local smartphone provides the best service in terms of both energy and speed, followed by a Raspberry Pi edge device and a laptop in fog. The services are designed to enable different use cases, such as patient diagnosis at home or sending diagnosis requests to travelling medical professionals through a fog device or cloud. This is the pioneering work that provides a reference architecture and such a detailed implementation and treatment of DAIaaS services, and is also expected to have an extensive impact on developing smart distributed service infrastructures for healthcare and other sectors.",unknown,38
10.1145/3531146.3533097,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Conference on Fairness, Accountability and Transparency",2022-01-01 00:00:00,semantic_scholar,how different groups prioritize ethical values for responsible ai,https://www.semanticscholar.org/paper/d821e5bdf1ab94bcd0e1a9e11fe1c296a01e3f02,"Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners’ value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define “responsible AI.”",unknown,39
10.1109/jiot.2020.3027971,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Internet of Things Journal,2022-01-01 00:00:00,semantic_scholar,ai-driven data monetization: the other face of data in iot-based smart and connected health,https://www.semanticscholar.org/paper/41953da351150461612e5f72f914e45bdcbba31a,"As the trajectory of the Internet of Things (IoT) moving at a rapid pace and with the rapid worldwide development and public embracement of wearable sensors, these days, most companies and organizations are awash in massive amounts of data. Determining how to profit from data deluge can give companies an edge in the market because data have the potential to add tremendous value to many aspects of a business. The market has already seen a level of monetization across vertical domains in the form of layering connected devices with a variety of Software-as-a-Service (SaaS) choices, such as subscription plans or smart device insights. Out of this arena is evolving a “machine economy” in which the ability to correctly monetize data rather than simply hoard it, will provide a significant advantage in a competitive digital environment. The recent advent of the technological advances in the fields of big data, analytics, and artificial intelligence (AI) has opened new avenues of competition, where data are utilized strategically and treated as a continuously changing asset able to unleash new revenue opportunities for monetization. Such growth has made room for an onslaught of new tools, architectures, business models, platforms, and marketplaces that enable organizations to successfully monetize data. In fact, emerging business models are striving to alter the power balance between users and companies that harvest information. Start-ups and organizations are offering to sell user data to data analytics companies and other businesses. Monetizing data goes beyond just selling data. It is also possible to include steps that add value to data. Generally, organizations can monetize data by: 1) utilizing it to make better business decisions or improve processes; 2) surrounding flagship services or products with data; or 3) selling information to current or new markets. This article will address all important aspects of IoT data monetization with more focus on the healthcare industry and discuss the corresponding challenges, such as data management, scalability, regulations, interoperability, security, and privacy. In addition, it presents a holistic reference architecture for the healthcare data economy with an in-depth case study on the detection and prediction of cardiac anomalies using multiparty computation (MPC) and privacy-preserving machine learning (PPML) techniques.",unknown,40
10.1080/15265161.2022.2135875,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,American Journal of Bioethics,2022-01-01 00:00:00,semantic_scholar,an ai bill of rights: implications for health care ai and machine learning—a bioethics lens,https://www.semanticscholar.org/paper/6a16d761ca5f28fca358b64db53f3cedbd4d83a1,"Just last week (October 4, 2022), the U.S. White House released a blueprint for an A.I. Bill of Rights, consisting of “five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificial intelligence.” The white paper states, “Developed through extensive consultation with the American public, these principles are a blueprint for building and deploying automated systems that are aligned with democratic values and protect civil rights, civil liberties, and privacy.” It further articulates that, “this framework provides a national values statement and toolkit that is sector-agnostic to inform building these protections into policy, practice, or the technological design process. Where existing law or policy—such as sector-specific privacy laws and oversight requirements—do not already provide guidance, the Blueprint for an AI Bill of Rights should be used to inform policy decisions” (Office of Science and Technology 2022). I applaud the development of this blueprint, but, after briefly describing each principle, highlight some challenges and questions that bioethicists working on AI and machine learning in health care ought to consider.",unknown,41
10.3897/jucs.94155,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of universal computer science (Online),2022-01-01 00:00:00,semantic_scholar,ai empowered big data analytics for industrial applications,https://www.semanticscholar.org/paper/fbf4872593d24a7d5f24e98f662ab85962a2f2d7,"We proposed the idea of editing a special issue that would compile the fruitful research that resulted from the stimulating discussions that occurred during the workshop that was held during the 5th International Conference on Intelligent Computing, Chennai on 25th & 26th March 2022. The objective of this special issue is to call for high-quality papers covering the latest data analytic concepts and technologies of big data and artificial intelligence. This special issue serves as a forum for researchers across the globe to discuss their work and recent advances in this field. The best papers from Artificial intelligence and Big Data Analytics (BAM) in the domains of Product, Finance, Health, and Environment were invited, peer-reviewed. The best high-quality papers were selected based on the innovativeness and relevance of the theme. The amount of data being generated and stored in various fields such as education, energy, environment, healthcare, fraud detection, and traffic is increasing exponentially in the modern era of Big Data. Simultaneously, there is a significant paradigm shift in business and society worldwide due to rapid advancements in fields such as artificial intelligence, machine learning, deep learning, and data analytics. This creates significant challenges for decision-making and the potential for transformation in areas such as the economy, government, and industry. Artificial Intelligence tools, techniques, and technologies, in conjunction with Big Data, improve the predictive power of the systems created and allow the government, public, and private sectors to discover new patterns and trends, as well as improve public values such as accountability, safety, security, and transparency to enable better decision-making, policies, and governance. They also have a wide range of capabilities to perform complex tasks that humans cannot. They could be used to collect, organize, and analyze large, diverse data sets to discover patterns and trends that address a variety of problems related to the development of the economy, such as identifying new sources of revenue, expanding the customer base for business, product reviews, and promotion, disease prediction and prevention, climatic variation prediction, and the provision of energy solutions. The wide variety of subject areas discussed at the 5th International Conference on Intelligent Computing is reflected in the seven accepted papers presented in the following section.",unknown,42
10.1155/2022/7617551,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Computational Intelligence and Neuroscience,2022-01-01 00:00:00,semantic_scholar,resnet-50 for 12-lead electrocardiogram automated diagnosis,https://www.semanticscholar.org/paper/ee0b4a0e65de3576aaeb57150d0f29184f7d2e60,"Nowadays, the implementation of Artificial Intelligence (AI) in medical diagnosis has attracted major attention within both the academic literature and industrial sector. AI would include deep learning (DL) models, where these models have been achieving a spectacular performance in healthcare applications. According to the World Health Organization (WHO), in 2020 there were around 25.6 million people who died from cardiovascular diseases (CVD). Thus, this paper aims to shad the light on cardiology since it is widely considered as one of the most important in medicine field. The paper develops an efficient DL model for automatic diagnosis of 12-lead electrocardiogram (ECG) signals with 27 classes, including 26 types of CVD and a normal sinus rhythm. The proposed model consists of Residual Neural Network (ResNet-50). An experimental work has been conducted using combined public databases from the USA, China, and Germany as a proof-of-concept. Simulation results of the proposed model have achieved an accuracy of 97.63% and a precision of 89.67%. The achieved results are validated against the actual values in the recent literature.",unknown,43
10.1145/3514094.3539563,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"AAAI/ACM Conference on AI, Ethics, and Society",2022-01-01 00:00:00,semantic_scholar,the opacity of automated decision-making systems (adms) and its challenges for political legitimacy in a democracy,https://www.semanticscholar.org/paper/4d14d554a9d6359f1faba70abb530ae03fe7eb89,"This paper focuses specifically on Automated Decision-Making Systems (ADMS) based on Artificial Intelligence (AI). Since the last decades, AI systems are increasingly deployed by governments across the planet to manage public infrastructures and resources, as well as to engage with citizens for the provision of public services. Their introduction is advertised as a cost-cutting tool, as well as an instrument to combat traditional institutional disfunctions such as inefficiency, understaffing, corruption and human bias. While AI offers an incredible potential for progress, an emerging body of literature highlights the challenges that AI-driven decision-making may raise for a public sector ethics. A common trait of these challenges is their being related to some form of ""epistemological opacity"" that undermines the capacity of humans to explain and justify decisions based on AI systems, detect errors or unfairness and adopt corrective actions. The situation may entail public officers and citizens taking the outcomes of AI systems at face value, thus basing their actions (wholly or in part) on pieces of information that cannot be scrutinized and/or corrected if necessary. This paper intends to contribute to an emerging but still underdeveloped trend in normative political theory that study how AI-driven decision-making is reshaping the conceptualization and assessment of interactions between citizens and public officials. The overall goal of the paper is to analyze how various sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) affecting AI systems, may undermine the democratic legitimacy of public decisions based on them. Broadly speaking, legitimacy is the property that grounds the exercise of political authority, where authority standardly means the right to rule [1]. In this paper, democratic legitimacy is understood as a distinctive form of political authority grounded in the recognition of citizens as joint legislators. The paper offers a conception of democratic legitimacy conditional on the capacity of decision-making procedures and outcomes to realize the principle of public equality, which requires citizens' control over public decision-making, as well as respect for their equal status as political decision-makers. Specifically, the paper argues that the ""epistemological opacity"" affecting AI-driven decision-making systems, brings about a mistreatment of citizens as coauthors of public decisions, which is a premise of the idea of democratic citizenship. The main conjecture is that different sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) are causing the disengagement of citizens and public officers from public decision-making, either because they directly undermine necessary conditions for the realization of public equality (co-authorship/accountability/publicity), or because they hide from the public eye instances of illegitimate automation and privatization of decisional power. The paper offers a normative conception of democratic legitimacy that may contribute to efforts in various fields, including ""AI fairness"" and ""Explainable AI"", to better adapt technological tools to equality requirements distinctive of public decision-making within democratic societies.",unknown,44
10.1145/3567556,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Proc. ACM Hum. Comput. Interact.,2022-01-01 00:00:00,semantic_scholar,duty to respond,https://www.semanticscholar.org/paper/f42f2e756bcf3683431af97f36527231117edcda,"Social service providers play a vital role in the developmental outcomes of underprivileged youth as they transition into adulthood. Educators, mental health professionals, juvenile justice officers, and child welfare caseworkers often have first-hand knowledge of the trials uniquely faced by these vulnerable youth and are charged with mitigating harmful risks, such as mental health challenges, child abuse, drug use, and sex trafficking. Yet, less is known about whether or how social service providers assess and mitigate theonline risk experiences of youth under their care. Therefore, as part of the National Science Foundation (NSF) I-Corps program, we conducted interviews with 37 social service providers (SSPs) who work with underprivileged youth to determine what (if any) online risks are most concerning to them given their role in youth protection, how they assess or become aware of these online risk experiences, and whether they see value in the possibility of using artificial intelligence (AI) as a potential solution for online risk detection. Overall, online sexual risks (e.g., sexual grooming and abuse) and cyberbullying were the most salient concern across all social service domains, especially when these experiences crossed the boundary between the digital and the physical worlds. Yet, SSPs had to rely heavily on youth self-reports to know whether and when online risks occurred, which required building a trusting relationship with youth; otherwise, SSPs became aware only after a formal investigation had been launched. Therefore, most SSPs found value in the potential for using AI as an early detection system and to monitor youth, but they were concerned that such a solution would not be feasible due to a lack of resources to adequately respond to online incidences, access to the necessary digital trace data (e.g., social media), context, and concerns about violating the trust relationships they built with youth. Thus, such automated risk detection systems should be designed and deployed with caution, as their implementation could cause youth to mistrust adults, thereby limiting the receipt of necessary guidance and support. We add to the bodies of research on adolescent online safety and the benefits and challenges of leveraging algorithmic systems in the public sector.",unknown,45
10.33545/26633140.2022.v4.i2a.77,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International journal of foreign trade and international business,2022-01-01 00:00:00,semantic_scholar,stochastic development on corporate environmental behavior resolution for quantum modelling of political adjudication from excise tax of all assumpsit actions,https://www.semanticscholar.org/paper/f88d67729038ea99558d4176112e1c1025761f13,"Artificial Intelligence (AI) on cybersecurity is designed to control financial transactions and agreements worldwide. It is an innovative regulatory tool architecturally networked to combat cyber threats and attacks in various degrees of war and crime perpetration from various banking industries and its corresponding government authority functions with implementing arms for execution of monitoring, reporting, and compliance. Regulatory Technology (RegTech) is an AI tool used by treasury departments and institutions, both local and private, for tracking malicious threats possible for money laundering and terrorism financing concealed in various settlements around the world. However, cybersecurity decisions need conformity in regulations and proceedings that it aims to engineer code development involving attacks under Regulatory Technology usage for administrative functions in favor of the financial intelligence authorities for combating and resolving issues on cyberwar. Therefore, political adjudication is a developing means of resolving gaps and optimizing judicial process principles within the policy function of financial intelligence. Hence, international laws and its accompanied policies must be globally harmonized in terms of federal and transnational tracking of financial flows of assets. Corporate Governance is a systematic design of stakeholders and their corporate social responsibility to advocate sustainable development. Tax aggressiveness is the obligation of the company to provide revenue distribution to public sector. Unlawful behavior on tax aggressiveness is known as tax evasion while tax avoidance is not a violation and serves as a loophole to the taxation system. UNCITRAL model law is a legal arbitra tion concept of making “commercial” expand to other comparable jurisdiction of international trade. Hague Convention drafted travaux preparatoires to conceptualize a legal framework of making the commercial transactions universal to other extended territories in terms of international trade law. This paper aims to develop tax avoidance based on statutory interpretation concerning Hague Convention as its extrinsic material to extend the legal principle of travaux preparatoires, hence, utilizing UNCITRAL legal modelling framework to make commercial transactions universal to trade law, for addressing legal gaps in marketing behavior of taxation system involving intellectual property, thus, in lack of legal measures in protecting public safety resulting to increase in domestic violence proportional to massive terrorism serving as professional deontology problem. Therefore, in terms of tax avoidance, the strict liability of the company must be addressed with constitutional issues and commercial responsibilities of marketing its product designed with elemental performance of domestic",unknown,46
10.1108/cr-09-2022-0137,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Competitiveness Review: An International Business Journal,2000-01-01 00:00:00,semantic_scholar,factors influencing artificial intelligence adoption in the accounting profession: the case of public sector in kuwait,https://www.semanticscholar.org/paper/c484ebb51b1b762867cb8367c9daf401d3db3e38,"
Purpose
This study aims to investigate the organizational and individual factors that influence the adoption of artificial intelligence (AI) in Kuwait's public accounting sector.


Design/methodology/approach
The methodology of this study is a cross-sectional survey of 393 experienced accounting professionals, using partial least square structural equation modeling to analyze the data.


Findings
The findings show that organizational culture, regulatory support, perceived usefulness and ease of use have a direct positive effect on AI adoption, while perceived usefulness and ease of use also have an indirect positive effect through accounting profit and behavioral intention. However, the availability of resources, effective communication channels and competition pressure have an insignificant impact on AI adoption.


Originality/value
This study pioneers a structural framework to elucidate the perceived enhancement of accounting quality through AI system integration. Further, this research adds to the literature on AI adoption in accounting. This study also offers empirical evidence regarding how organizations in Kuwait's public accounting sector view AI systems in accounting.
",unknown,47
10.58812/wslhr.v1i04.330,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,West Science Law and Human Rights,2000-01-01 00:00:00,semantic_scholar,the implication of generative artificial intelligence towards intellectual property rights (examining the multifaceted implications of generative artificial intelligence on intellectual property rights),https://www.semanticscholar.org/paper/d307942d1a6d69584afff0c68d9811540fb8a2e4,"Generative Artificial Intelligence (Generative AI) is transforming content creation, enabling faster and cheaper production of text, images, and more. However, it raises complex issues regarding intellectual property rights and ownership. This article explores the evolving landscape of AI-generated content, focusing on its alignment with existing intellectual property regulations. It delves into legal disputes exemplified by cases like Getty Images, INC. v. Stability AI, INC, and Doe v. Github, INC, which highlight the challenges of AI-generated content regarding intellectual property. The article also discusses the impact on the creative sector and offers recommendations, including the need for ethical guidelines, education, hybrid collaboration, public involvement, and international cooperation. Addressing these challenges is crucial to harmonize intellectual property rights and maximize the benefits of AI in content creation.",unknown,48
10.3390/knowledge3030032,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Knowledge,2000-01-01 00:00:00,semantic_scholar,chatgpt and the generation of digitally born “knowledge”: how does a generative ai language model interpret cultural heritage values?,https://www.semanticscholar.org/paper/c2b6f0cf3c76d314a8c1f46cd8c831e67f2e16bb,"The public release of ChatGPT, a generative artificial intelligence language model, caused wide-spread public interest in its abilities but also concern about the implications of the application on academia, depending on whether it was deemed benevolent (e.g., supporting analysis and simplification of tasks) or malevolent (e.g., assignment writing and academic misconduct). While ChatGPT has been shown to provide answers of sufficient quality to pass some university exams, its capacity to write essays that require an exploration of value concepts is unknown. This paper presents the results of a study where ChatGPT-4 (released May 2023) was tasked with writing a 1500-word essay to discuss the nature of values used in the assessment of cultural heritage significance. Based on an analysis of 36 iterations, ChatGPT wrote essays of limited length with about 50% of the stipulated word count being primarily descriptive and without any depth or complexity. The concepts, which are often flawed and suffer from inverted logic, are presented in an arbitrary sequence with limited coherence and without any defined line of argument. Given that it is a generative language model, ChatGPT often splits concepts and uses one or more words to develop tangential arguments. While ChatGPT provides references as tasked, many are fictitious, albeit with plausible authors and titles. At present, ChatGPT has the ability to critique its own work but seems unable to incorporate that critique in a meaningful way to improve a previous draft. Setting aside conceptual flaws such as inverted logic, several of the essays could possibly pass as a junior high school assignment but fall short of what would be expected in senior school, let alone at a college or university level.",unknown,49
10.1097/cin.0000000000001044,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Computers, Informatics, Nursing",2000-01-01 00:00:00,semantic_scholar,the disruptive impacts of next generation generative artificial intelligence,https://www.semanticscholar.org/paper/b35e3ff63756648bf2b68f959f2f6e76b80a0290,"T he growing influence of generative artificial intelligence (GAI) on our personal and professional lives continues to give it the appearance of a truly disruptive innovation. Kivimaa et al noted the characteristics of disruptive innovations to include high-intensity disruption or deletion of entire job markets, resetting of process or business models, and a “technological substitution process.” Artificial intelligence (AI) applications have already been shown to be quite capable of acting as technological substitutions for human processes. Generative AI, though, moves beyond just the automation facets of AI into something more complex and curious that has captured the public's imagination. The impacts of GAI are continuing to unfold within healthcare delivery and education, with both value and cautions yet to be fully realized. Active engagement on the part of all nurses, particularly nurse informaticists, is required in order to shape the technology moving forward and to alleviate potential negative impacts and misuse.",unknown,50
10.17261/pressacademia.2023.1788,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Pressacademia,2000-01-01 00:00:00,semantic_scholar,generative ai in electricity distribution: a qualitative exploration,https://www.semanticscholar.org/paper/e43ae5635f02cd1cca81f0b76d46ce21ce164d4c,"Purpose- The purpose of this study is to explore the application and potential of generative artificial intelligence (AI) within the context of electricity distribution companies. The study aims to investigate how these advanced AI technologies, particularly Generative Adversarial Networks (GANs), can address the sector's pressing challenges, such as load forecasting, power outage prediction, and preventive maintenance.
Methodology- The study employs a qualitative case study methodology, providing an in-depth analysis of real-world applications of generative AI within electricity distribution companies. The selection of cases represents a wide variety of experiences and contexts, facilitated by both primary data collected through semi-structured interviews with key personnel within the organizations and secondary data derived from an extensive review of company reports, public documentation, and industry publications. The gathered data was systematically analyzed using thematic analysis to identify and report recurring patterns and themes.
Findings- The analysis reveals that generative AI has been successfully implemented in various operational aspects of electricity distribution. The first case study presents how GANs have significantly improved load forecasting accuracy in an Eastern Turkish electricity distribution company. The second case study from Southern Turkey showcases how GANs have been used for predicting power outages, thereby aiding efficient resource allocation, reducing downtime, and enhancing customer satisfaction. Lastly, the third case from Northern Turkey demonstrates how generative AI has contributed to effective preventive maintenance of distribution equipment, improving overall system reliability.
Conclusion- Based on the analysis findings, it may be concluded that generative AI holds transformative potential for the electricity distribution sector. While the implementation of these technologies is associated with challenges such as data privacy, security, and the requirement of technical expertise, the benefits in terms of improved accuracy, system reliability, and resource efficiency provide a strong justification for their adoption. The paper underlines the importance of an interdisciplinary collaboration between AI researchers, electrical engineers, industry professionals, and policymakers for furthering the adoption of these technologies. As the field of generative AI continues to evolve, it is expected to have an even greater impact on the electricity distribution sector, thereby opening up exciting opportunities for future research and application.

Keywords: Generative artificial intelligence (ai), electricity distribution companies, generative adversarial networks (gans), load forecasting, outage prediction, preventive maintenance
JEL Codes: M40, M41
",unknown,51
10.15439/2023f5494,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Conference on Computer Science and Information Systems,2000-01-01 00:00:00,semantic_scholar,towards community-driven generative ai,https://www.semanticscholar.org/paper/52aaea84c922c27c35260aaca8ff43f8debd1a2d,"—While the emerging market of Generative Artiﬁcial Intelligence (AI) is increasingly dominated and controlled by the Tech Giants, there is also a growing interest in open-source AI code and models from smaller companies, research organisations and individual users. They often have valuable data that could be used for training, but their computing resources are limited, while data privacy concerns prevent them from sharing this data for public training. A possible solution to overcome these two issues is to utilise the crowd-souring principles and apply federated learning techniques to build a distributed privacy-preserving architecture for training Generative AI. This paper discusses how these two key enablers, together with some other emerging technologies, can be effectively combined to build a community-driven Generative AI ecosystem, allowing even small actors to participate in the training of Generative AI models by securely contributing their training data. The paper also discusses related non-technical issues, such as the role of the community and intellectual property rights, and outlines further research directions associated with AI moderation.",unknown,52
10.1177/09520767231170321,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Public Policy and Administration,2000-01-01 00:00:00,semantic_scholar,systematic and axiological capacities in artificial intelligence applied in the public sector,https://www.semanticscholar.org/paper/33be6d7c2d61bfac798a29e22ea0267e9fe16b05,"Artificial Intelligence (AI) as a technological development is being implemented in the public sector with the intention of improving service delivery, as well as to help solve complex problems. However, there is a wide range of capabilities that AI can perform and that public officials perceive and implement in different ways. This paper aims to describe and analyze some categories into which AI capabilities in the public sector are divided. Using an Exploratory Factor Analysis (EFA), our results show that the capabilities of AI from the perspective of public officials can be classified into two aspects: systematic factors and axiological factors. Systematic factors are related to the analysis and behavior of data, including monitoring, analyzing, interacting, remembering, and anticipation. Axiological factors refer to the impacts of values, ethics, and decisions, including acting, feeling, moralizing, creating, and deciding capacities. This categorization of AI capabilities in the public sector sheds light on the perception of public officials about the implementation of this technological development.",unknown,53
10.1177/02683962231200411,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Information and Technology,2000-01-01 00:00:00,semantic_scholar,‘chatgpt et al.’: the ethics of using (generative) artificial intelligence in research and science,https://www.semanticscholar.org/paper/965d941897de90c4ad4c6cde77cb2b3333edcaeb,"Artificial intelligence (AI) seeks to make computers do what human minds can do. By ‘AI’, we refer to the use of machine learning, algorithms, large datasets, neural networks and traditional statistical reasoning by computing. The term ‘AI’ is misleading: Despite suggestions to the contrary (Bubeck et al., 2023) – and some surely impressive achievements in specific areas –we are still far from reaching the benchmark of ‘general human intelligence’. AI has undergone several generations, from ‘good old-fashioned AI’ (Haugeland, 1989), the defined algorithms of which failed at the common sense problem, to the current and more successful generation of neural network and deep learning AI. One specific form of current AI is ‘generative AI’ (e.g. ChatGPT, DALL-E, Midjourney) – and without a doubt, it’s the ‘technology hype’ of 2023 and the focus of this editorial comment. Generative AI, specifically ChatGPT, became a ‘cultural sensation’ (Thorp, 2023) rather rapidly in early 2023. When Daniel brought up generative AI as a future ethical issue at a panel for journal editors on publishing ethics in December 2022 (Burton-Jones et al., 2022), many audience members seemed unfamiliar withMidjourney orChatGPT.However, within just a few weeks, the landscape shifted dramatically. Publicly launched on 30 November 2022, ChatGPT – a chatbot built on top of a text-generating AI – had an impressive debut, reaching onemillion users within 5 days and surpassing 100million users in January 2023 (Dwivedi et al., 2023). Since then, ChatGPT has become widely used and is believed to impact many areas, including research and science (Hill-Yardin et al., 2023; Liebrenz et al., 2023; Lund and Wang, 2023). While detailed explanations of the underlying technology can be found in other sources (Goodfellow et al., 2016), generative AI is a subset of deep learning AI that specialises in producing human-like outputs. OpenAI’s ChatGPToperates on a neural network AI architecture, GPT (Generative Pretrained Transformer). Although ChatGPT might have seemed like a natural progression of the AI domain, especially since Midjourney and DALL-E had been introduced earlier, it astonished global audiences and led companies like Alphabet (Google) to hastily release comparable tools (Teubner et al., 2023). Simplified, deep learning AI systems ‘hallucinate’ ‘plausible looking’ (though not necessarily accurate) responses to user prompts. They base these responses on patterns of ‘likeness’ (associations between words and concepts), stored in a digital neural network (multiple layers of interconnected nodes) and learnt from massive training datasets. Such systems can quickly generate high-quality images and texts, outperforming traditional algorithms. However, this advanced capability is accompanied by the challenge of the ‘black box’ problem:wemay understand the model’s general principles, but the reasons behind specific decisions remain opaque. The neural network provides a flexible, changing structure, inspired by the human brain, that encodes patterns, but not in an intelligible, auditable manner – there is no clear formula to scrutinise. (This is akin to how the reader might instantly and reliably distinguish between their mother and their cat but would be unable to write down a precise formula for this recognition process). As journal editors, the emergence of ChatGPT prompted us – and others (e.g. Hill-Yardin et al., 2023; Liebrenz et al., 2023; Lund and Wang, 2023; Teubner et al., 2023; Van Dis et al., 2023) – to ask foundational questions about using generative AI in research and science. Specifically: Is it ‘ethical’ to use generative or other AIs in conducting research or for writing academic research papers? In this editorial, we go back to first principles to reflect on the fundamental ethics to apply to using ChatGPT and AI in research and science. Next, we caution that (generative) AI is also at the ‘peak of inflated (hype) expectations’ and discuss eight in-principle issues that AI struggles with, both ethically and practically. We conclude with what this all means for the ethics of using generative AI in research and science.",unknown,54
10.1177/09520767231188229,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Public Policy and Administration,2000-01-01 00:00:00,semantic_scholar,making governance agile: exploring the role of artificial intelligence in china’s local governance,https://www.semanticscholar.org/paper/3cc1411a425b6a61bdd8a6bdd8f76ddcff4f869b,"As the key to digital transformation, artificial intelligence is believed to help achieve the goal of government as a platform and the agile development of digital services. Yet we know little about its potential role in local governance, especially the advances that AI-supported services for the public sector in local governance have ventured and the public value they have created. Combining the digital transformation concepts and public value theory, we fill the gap by examining artificial intelligence (AI) deployment in the public sector of a pilot city of digital transformation in China. Using a mixed-method approach, we show how AI configurations facilitate public value creation in the digital era and identify four dimensions of AI deployment in the public sector: data integration, policy innovation, smart application, and collaboration. Our case analysis on these four dimensions demonstrates two roles that AI technology plays in local governance—“AI cage” and “AI colleague.” The former builds the technology infrastructure and platform in each stage of service delivery, regulating the behaviors of frontline workers, while the latter helps frontline workers make decisions, thus improving the agility of public service provision.",unknown,55
10.1145/3582515.3609555,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Conference on Information Technology for Social Good,2000-01-01 00:00:00,semantic_scholar,the social impact of generative ai: an analysis on chatgpt,https://www.semanticscholar.org/paper/cea012c01f09ac382fe67ede5215a85175753487,"In recent months, the impact of Artificial Intelligence (AI) on citizens’ lives has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a citizen-centric AI.",unknown,56
10.5194/isprs-archives-xlviii-m-2-2023-1149-2023,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences",2000-01-01 00:00:00,semantic_scholar,artificial intelligence and cultural heritage: design and assessment of an ethical framework,https://www.semanticscholar.org/paper/72bf31b52aac62c892e9e4c12fc61e9a236ce167,"Abstract. The pioneering use of Artificial Intelligence (AI) in various fields and sectors, and the growing ethical debate about its application have led research centers, public and private institutions to establish ethical guidelines for a trustworthy implementation of these powerful algorithms. Despite the recognized definition of ethical principles for a responsible or trustworthy use of AI, there is a lack of a sector-specific perspective that highlights the ethical risks and opportunities for different areas of application, especially in the field of Cultural Heritage (CH). In fact, there is still a lack of formal frameworks that evaluate the algorithms’ adherence to the ethical standards set by the European Union for the use of AI in protecting CH and its inherent value. Because of this, it is necessary to investigate a different sectoral viewpoint to supplement the widely used horizontal approach. This paper represents a first attempt to design an ethical framework to embody AI in CH conservation practises to assess various risks arising from the use of AI in the field of CH. The contribution presents a synthesis of the different AI applications to improve the preservation process of CH. It explores and analyses in depth the ethical challenges and opportunities presented by the use of AI to improve CH preservation. In addition, the study aims to design an ethical framework of principles to assess the application of this ground-breaking technology at CH.
",unknown,57
10.37417/rdp/vol_8_2023_1949,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Revista de Derecho Público: Teoría y método,2000-01-01 00:00:00,semantic_scholar,artificial intelligence challenging core state functions,https://www.semanticscholar.org/paper/5a392bc0fd61b7e062d95d6ed819bd347bae4480,"The use of AI in the public sector is emerging around the world and its spread affects the core States functions: the administrative, the judiciary, and the legislative. Nevertheless, a comprehensive approach to AI in the life-cycle of rules - from the proposal of a new rule to its implementation, monitoring and review- is currently lacking in the rich panorama of studies from different disciplines. The analysis shows that AI has the power to play a crucial role in the life-cycle of rules, by performing time-consuming tasks, increasing access to knowledge base, and enhancing the ability of institutions to draft effective rules and to declutter the regulatory stock. However, it is not without risks, ranging from discrimination to challenges to democratic representation. In order to play a role in achieving law effectiveness while limiting the risks, a complementarity between human and AI should be reached both at the level of the AI architecture and ex post. Moreover, an incremental and experimental approach is suggested, as well as the elaboration of a general framework, to be tailored by each regulator to the specific features of its tasks, aimed at setting the rationale, the role, and adequate guardrails to AI in the life-cycle of rules. This agile approach would allow the AI revolution to display its benefits while preventing potential harms or side effects.",unknown,58
10.1108/ijebr-02-2023-0169,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Journal of Entrepreneurial Behavior &amp; Research,2000-01-01 00:00:00,semantic_scholar,stand-alone or run together: artificial intelligence as an enabler for other technologies,https://www.semanticscholar.org/paper/76c0930edff3d28590c89523c8d96501cf12909f,"PurposeThe purpose of this study is to examine the role of artificial intelligence (AI) in transforming the healthcare sector, with a focus on how AI contributes to entrepreneurship and value creation. This study also aims to explore the potential of combining AI with other technologies, such as cloud computing, blockchain, IoMT, additive manufacturing and 5G, in the healthcare industry.Design/methodology/approachExploratory qualitative methodology was chosen to analyze 22 case studies from the USA, EU, Asia and South America. The data source was public and specialized podcast platforms.FindingsThe findings show that combining technologies can create a competitive advantage for technology entrepreneurs and bring about transitions from simple consumer devices to actionable healthcare applications. The results of this research identified three main entrepreneurship areas: 1. Analytics, including staff reduction, patient prediction and decision support; 2. Security, including protection against cyberattacks and detection of atypical cases; 3. Performance optimization, which, in addition to reducing the time and costs of medical procedures, includes staff training, reducing capital costs and working with new markets.Originality/valueThis study demonstrates how AI can be used with other technologies to cocreate value in the healthcare industry. This study provides a conceptual framework, “AI facilitators – AI achievers,” based on the findings and offer several theoretical contributions to academic literature in technology entrepreneurship and technology management and industry recommendations for practical implication.",unknown,59
10.2196/55762,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,JMIR Formative Research,2000-01-01 00:00:00,semantic_scholar,assessing the accuracy of generative conversational artificial intelligence in debunking sleep health myths: mixed methods comparative study with expert analysis,https://www.semanticscholar.org/paper/8be109b21c388546b8570d68789f54e0aafed8b9,"Background Adequate sleep is essential for maintaining individual and public health, positively affecting cognition and well-being, and reducing chronic disease risks. It plays a significant role in driving the economy, public safety, and managing health care costs. Digital tools, including websites, sleep trackers, and apps, are key in promoting sleep health education. Conversational artificial intelligence (AI) such as ChatGPT (OpenAI, Microsoft Corp) offers accessible, personalized advice on sleep health but raises concerns about potential misinformation. This underscores the importance of ensuring that AI-driven sleep health information is accurate, given its significant impact on individual and public health, and the spread of sleep-related myths. Objective This study aims to examine ChatGPT’s capability to debunk sleep-related disbeliefs. Methods A mixed methods design was leveraged. ChatGPT categorized 20 sleep-related myths identified by 10 sleep experts and rated them in terms of falseness and public health significance, on a 5-point Likert scale. Sensitivity, positive predictive value, and interrater agreement were also calculated. A qualitative comparative analysis was also conducted. Results ChatGPT labeled a significant portion (n=17, 85%) of the statements as “false” (n=9, 45%) or “generally false” (n=8, 40%), with varying accuracy across different domains. For instance, it correctly identified most myths about “sleep timing,” “sleep duration,” and “behaviors during sleep,” while it had varying degrees of success with other categories such as “pre-sleep behaviors” and “brain function and sleep.” ChatGPT’s assessment of the degree of falseness and public health significance, on the 5-point Likert scale, revealed an average score of 3.45 (SD 0.87) and 3.15 (SD 0.99), respectively, indicating a good level of accuracy in identifying the falseness of statements and a good understanding of their impact on public health. The AI-based tool showed a sensitivity of 85% and a positive predictive value of 100%. Overall, this indicates that when ChatGPT labels a statement as false, it is highly reliable, but it may miss identifying some false statements. When comparing with expert ratings, high intraclass correlation coefficients (ICCs) between ChatGPT’s appraisals and expert opinions could be found, suggesting that the AI’s ratings were generally aligned with expert views on falseness (ICC=.83, P<.001) and public health significance (ICC=.79, P=.001) of sleep-related myths. Qualitatively, both ChatGPT and sleep experts refuted sleep-related misconceptions. However, ChatGPT adopted a more accessible style and provided a more generalized view, focusing on broad concepts, while experts sometimes used technical jargon, providing evidence-based explanations. Conclusions ChatGPT-4 can accurately address sleep-related queries and debunk sleep-related myths, with a performance comparable to sleep experts, even if, given its limitations, the AI cannot completely replace expert opinions, especially in nuanced and complex fields such as sleep health, but can be a valuable complement in the dissemination of updated information and promotion of healthy behaviors.",unknown,60
10.14201/adcaij.31704,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Advances in Distributed Computing and Artificial Intelligence Journal,2000-01-01 00:00:00,semantic_scholar,generative artificial intelligence: fundamentals,https://www.semanticscholar.org/paper/75f37c5bd7c9c6389e7544807fbc45fd478d4242,"

Generative language models have witnessed substantial traction, notably with the introduction of refined models aimed at more coherent user-AI interactions—principally conversational models. The epitome of this public attention has arguably been the refinement of the GPT-3 model into ChatGPT and its subsequent integration with auxiliary capabilities such as search features in Microsoft Bing. Despite voluminous prior research devoted to its developmental trajectory, the model’s performance, and applicability to a myriad of quotidian tasks remained nebulous and task specific. In terms of technological implementation, the advent of models such as LLMv2 and ChatGPT-4 has elevated the discourse beyond mere textual coherence to nuanced contextual understanding and real-world task completion. Concurrently, emerging architectures that focus on interpreting latent spaces have offered more granular control over text generation, thereby amplifying the model’s applicability across various verticals. Within the purview of cyber defense, especially in the Swiss operational ecosystem, these models pose both unprecedented opportunities and challenges. Their capabilities in data analytics, intrusion detection, and even misinformation combatting is laudable; yet the ethical and security implications concerning data privacy, surveillance, and potential misuse warrant judicious scrutiny.
",unknown,61
10.1108/dts-03-2023-0018,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Transformation and Society,2000-01-01 00:00:00,semantic_scholar,"artificial intelligence, task complexity and uncertainty: analyzing the advantages and disadvantages of using algorithms in public service delivery under public administration theories",https://www.semanticscholar.org/paper/34ca9b143e721bfd8dc8eb29d6352a68b9c82a19,"PurposeThis article revisits some theories and concepts of public administration, including those related to public value, transaction costs and social equity, to analyze the advantages and disadvantages of using artificial intelligence (AI) algorithms in public service delivery. The author seeks to mobilize theory to guide AI-era public management practitioners and researchers.Design/methodology/approachThe author uses an existing task classification model to mobilize and juxtapose public management theories against artificial intelligence potential impacts in public service delivery. Theories of social equity and transaction costs as well as some concepts such as red tape, efficiency and economy are used to argue that the discipline of public administration provides a foundation to ensure algorithms are used in a way that improves service delivery.FindingsAfter presenting literature on the challenges and promises of using AI in public service, the study shows that while the adoption of algorithms in public service has benefits, some serious challenges still exist when looked at under the lenses of theory. Additionally, the author mobilizes the public administration concepts of agenda setting and coproduction and finds that designing AI-enabled public services should be centered on citizens who are not mere customers. As an implication for public management practice, this study shows that bringing citizens to the forefront of designing and implementing AI-delivered services is key to reducing the reproduction of social biases.Research limitations/implicationsAs a fast-growing subject, artificial intelligence research in public management is yet to empirically test some of the theories that the study presented.Practical implicationsThe paper vulgarizes some theories of public administration which practitioners can consider in the design and implementation of AI-enabled public services. Additionally, the study shows practitioners that bringing citizens to the forefront of designing and implementing AI-delivered services is key to reducing the reproduction of social biases.Social implicationsThe paper informs a broad audience who might not be familiar with public administration theories and how those theories can be taken into consideration when adopting AI systems in service delivery.Originality/valueThis research is original, as, to the best of the author’s knowledge, no prior work has combined these concepts in analyzing AI in the public sector.",unknown,62
10.1080/10790268.2023.2198926,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Spinal Cord Medicine (JSCM),2000-01-01 00:00:00,semantic_scholar,generative ai in spinal cord injury research and care: opportunities and challenges ahead,https://www.semanticscholar.org/paper/6afbce4fbf374b3b565ff1ea64354d57c7fe630c,"Generative artificial intelligence (AI) with its limitless potential is here to stay. Not since the advent of the digital age has the world faced a transformation of such magnitude. With the widely available user-friendly free and low-cost products of Open AI, the US AI research laboratory, the effects are being felt throughout multiple sectors, including scientific research and clinical care. Anyone with internet access can sign up at OpenAI.com to access a free research version of ChatGPT, a generative AI large-language model that interacts with the user in a natural language format (1). Powered by the world’s fifth largest supercomputer (2), ChatGPT gathers information using artificial neural network technology (3). Trained using large databases, it has an enormous vocabulary and is capable of understanding language and context, retrieving knowledge, generating new content in the forms of text and code, carrying on a conversation, and personalizing communications. While trained primarily in English, this chatbot supports 95 languages. While this tool became only recently available, ChatGPT gained traction immediately and gained followers at an unprecedented pace, attracting more than 100 million users in the two months following its public release in November 2022 (3). In March 2023, OpenAI released a more advanced multi-modal version, GPT-4, available through the paid service, ChatGPT Plus (4,5). To explore these new tools, we posed general queries to GPT-4 about the potential for generative AI to influence spinal cord research and clinical care. Selected responses are excerpted below: • Generative AI can be used to create virtual models of the human body, including the spinal cord, the musculoskeletal system, and the nervous system. These models can then be used to simulate spinal cord injury and test the effectiveness of different treatments in a more controlled and accurate environment. • AI algorithms can help researchers optimize the design of spinal stimulation protocols by analyzing large amounts of data, simulating the effects of stimulation, developing personalized treatment plans, and predicting outcomes. • Generative AI can be used to design new drugs that enhance the survival and integration of stem cells into the spinal cord. AI algorithms can analyze large amounts of data to identify potential drug candidates and simulate their effects • By optimizing the design and control algorithms of robotic exoskeletons, analyzing large amounts of data, and simulating different types of injuries, generative AI can help researchers improve the effectiveness of these devices. By analyzing medical history and imaging data, AI algorithms can recommend the most effective exoskeleton design and treatment protocol for that individual’s specific injury and needs. When asked about its potential role in recruiting research participants, it responded: • AI algorithms can analyze electronic medical records, patient data, and social media activity to identify potential candidates for spinal cord injury studies. Once potential candidates have been identified, AI algorithms can be used to create personalized messages and content tailored to individual patients. Generative AI can be used to develop predictive models that identify factors associated with patient enrollment and retention. Generative AI can be used to engage with patients, using chatbots, virtual assistants, or other AI-powered tools to provide patients with updates, answer questions, and address concerns. ChatGPT offered these potential applications when queried about clinical care: • In acute spinal cord injury care, generative AI can analyze vital signs, lab results, and other patient data to predict the likelihood of pressure sores, urinary tract infections, or other complications. This can help clinicians intervene early, which can improve patient outcomes and reduce healthcare costs,",unknown,63
10.48550/arxiv.2312.01180,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,arXiv.org,2000-01-01 00:00:00,semantic_scholar,a comparative analysis of text-to-image generative ai models in scientific contexts: a case study on nuclear power,https://www.semanticscholar.org/paper/2ae68f46832c30fe6a55461defebe7604df4fe5e,"In this work, we propose and assess the potential of generative artificial intelligence (AI) to generate public engagement around potential clean energy sources. Such an application could increase energy literacy -- an awareness of low-carbon energy sources among the public therefore leading to increased participation in decision-making about the future of energy systems. We explore the use of generative AI to communicate technical information about low-carbon energy sources to the general public, specifically in the realm of nuclear energy. We explored 20 AI-powered text-to-image generators and compared their individual performances on general and scientific nuclear-related prompts. Of these models, DALL-E, DreamStudio, and Craiyon demonstrated promising performance in generating relevant images from general-level text related to nuclear topics. However, these models fall short in three crucial ways: (1) they fail to accurately represent technical details of energy systems; (2) they reproduce existing biases surrounding gender and work in the energy sector; and (3) they fail to accurately represent indigenous landscapes -- which have historically been sites of resource extraction and waste deposition for energy industries. This work is performed to motivate the development of specialized generative tools and their captions to improve energy literacy and effectively engage the public with low-carbon energy sources.",unknown,64
10.21202/jdtl.2023.38,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Digital Technologies and Law,2000-01-01 00:00:00,semantic_scholar,towards legal regulations of generative ai in the creative industry,https://www.semanticscholar.org/paper/6f0329dfd1e45dbfbdfd95cd0b3e9f40752c984f,"Objective: this article aims to answer the following questions: 1. Can generative artificial intelligence be a subject of copyright law? 2. What risks the unregulated use of generative artificial intelligence systems can cause? 3. What legal gaps should be filled in to minimize such risks?Methods: comparative legal analysis, sociological method, concrete sociological method, quantitative data analysis, qualitative data analysis, statistical analysis, case study, induction, deduction.Results: the authors identified several risks of the unregulated usage of generative artificial intelligence in the creative industry, among which are: violation of copyright and labor law, violation of consumers rights and the rise of public distrust in government. They suggest that a prompt development of new legal norms can minimize these risks. In conclusion, the article constants that states have already begun to realize that the negative impact of generative artificial intelligence on the creative industry must not be ignored, hence the development of similar legal regulations in states with completely different regimes.Scientific novelty: the article provides a comprehensive study of the impact of generative artificial intelligence on the creative industry from two perspectives: the perspective of law and the perspective of the industry. The empirical basis of it consists of two international surveys and an expert opinion of a representative of the industry. This approach allowed the authors to improve the objectivity of their research and to obtain results that can be used for finding a practical solution for the identified risks. The problem of the ongoing development and popularization of generative artificial intelligence systems goes beyond the question “who is the author?” therefore, it needs to be solved by introduction of other than the already existing mechanisms and regulations - this point of view is supported not only by the results of the surveys but also by the analysis of current lawsuits against developers of generative artificial intelligence systems.Practical significance: the obtained results can be used to fasten the development of universal legal rules, regulations, instruments and standards, the current lack of which poses a threat not only to human rights, but also to several sectors within the creative industry and beyond.",unknown,65
10.52663/kcsr.2023.28.2.85,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,The Korea Association for Corruption Studies,2000-01-01 00:00:00,semantic_scholar,"study on artificial intelligence(ai) and chat gpt, corruption",https://www.semanticscholar.org/paper/7d2925856541169ac233ccd5a08eab8f36a87b64,"This study discusses corruption and moral hazard arising from the use of artificial intelligence(AI) and Chat GPT and presents implications. In this study, 1. The daily use of AI and Chat GPT is 1) Lack of situational awareness and thinking, 2) Absence of appropriate ethical norms and social codes, 3) Problems with shifting consciousness, 4) Concentration of economic and technological utility, 5) Deepening dependence on AI. 2. Problems in the information processing process of AI and Chat GPT were 1) lack of transparency and autonomy of AI, 2) value judgment and bias of Chat GPT algorithm, 3) intervention and distortion of decision-making, 4) possibility of copyright infringement, and 5) safety of use and services. 3. Problems in the information distribution process of AI and Chat GPT are 1) limitations of the Chat GPT algorithm and information distortion, 2) information asymmetry and centralization, 3) inaccuracy and uncertainty of information, 4) misuse of information or data, 5) Information security and information leakage occurred. The implications of this study are as follows. First, education and promotion of AI and Chat GPT include 1) Composition of public and private cooperation governance, 2) Development of national campaigns, 3) Found and service of AI and Chat GPT, 4) establishment of ethical norms and manners for AI and Chat GPT, 5) Social coding of AI ethics and manners. Second, the institutional devices for AI and Chat GPT are 1) Setting the regulatory scope and level to secure transparency and autonomy, 2) Monitoring of bias in information by public and private sectors, 3) Management and supervision of the information collection process, 4) Establishment of an institutional device to prevent AI from intervening in decision-making and preventing distortion, 5) Establishment of standards and systems to ensure the safety of information processing. Third, normative device for AI and Chat GPT as follows. The contents are: 1) Discussion and search for normative standards, 2) Exploring ways to resolve the centralization of information, 3) securing the clarity and reliability of information, 4) designing information security and leakage prevention programs, 5) Monitoring to prevent misuse and abuse of information.data.",unknown,66
10.1080/0312407x.2023.2247833,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Australian Social Work,2000-01-01 00:00:00,semantic_scholar,artificial intelligence and implications for the australian social work journal,https://www.semanticscholar.org/paper/d4e771b2e4ef85be2edf99fa9c3047e528c2bc18,"Social work is a profession committed to integrity and social justice. The AASW Social Work Practice Standards (AASW, 2023) calls on social workers to be critically reflective, ethical practitioners engaged in lifelong professional development and learning. Equally, social work education seeks to prepare students for research-informed, culturally-responsive practice across a diverse range of contexts, and in this Issue, we showcase critical social work education and practice diversity. However, a different ethical challenge to integrity and practice standards is the focus of this Editorial. Here, we highlight some of the concerns and implications of generative Artificial Intelligence (generative AI) for social work education, research, practice, and scholarly publishing. In November 2022, OpenAI released ChatGPT, a generative AI Large Language Model (LLM) that could generate realistic and natural text outputs from simple prompts. This technology had been in development for some time but had not been released to the public for general use. Since then, there has been a proliferation of different AI models that can generate and augment text, images, video, and audio. Generative AI is being used to perform analytical and interpretive tasks such as language translation; responding to queries on specific data sources, coding, and interpreting code; summarising documents and webpages; and creating case assessments and plans. This technology can be used to construct legal documents; machine learning for facial recognition; and for undertaking medical, mental health, and other diagnostic assessments. These are just some examples. In this fast-moving field, the uses and applications seem endless. The open-sourcing of generative AI models and their underlying architecture means developers are starting to create a myriad of practical applications and tools that rapidly increase the depth and scale of automation, potentially replacing or augmenting many everyday tasks normally performed by humans. The implications for social work education, practice, research, and scholarship are extensive. As with any new technology, there are a range of stances, from early adopters to positions that have resonance with luddism. This adds to the complexities of responding to AI as a whole profession. Nevertheless, what is clear is that the rise and integration of generative AI systems, at scale, will yield a wide range of practical, ethical, and epistemological problems for many professions, including social work. It is to some of these problems we turn our attention below. Beginning with social work education, generative AI will have profound effects on assessment and learning for higher education providers. It is likely to cause educators to re-evaluate their educational practices, assessments, and assumptions about what is core to a social work curriculum. Social work will need to refine and reappraise its ideas about critical thinking, ethical decision making, professional judgement, and reflective practice—all skills that are considered core to effective social work practice as outlined in the AASW Practice Standards (AASW, 2023). How will we ensure students have an educational environment that promotes",unknown,67
10.48550/arxiv.2310.14651,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,arXiv.org,2000-01-01 00:00:00,semantic_scholar,$λ$-split: a privacy-preserving split computing framework for cloud-powered generative ai,https://www.semanticscholar.org/paper/980646c7dbd3689177a2fe5c27ffa116f042f4ee,"In the wake of the burgeoning expansion of generative artificial intelligence (AI) services, the computational demands inherent to these technologies frequently necessitate cloud-powered computational offloading, particularly for resource-constrained mobile devices. These services commonly employ prompts to steer the generative process, and both the prompts and the resultant content, such as text and images, may harbor privacy-sensitive or confidential information, thereby elevating security and privacy risks. To mitigate these concerns, we introduce $\Lambda$-Split, a split computing framework to facilitate computational offloading while simultaneously fortifying data privacy against risks such as eavesdropping and unauthorized access. In $\Lambda$-Split, a generative model, usually a deep neural network (DNN), is partitioned into three sub-models and distributed across the user's local device and a cloud server: the input-side and output-side sub-models are allocated to the local, while the intermediate, computationally-intensive sub-model resides on the cloud server. This architecture ensures that only the hidden layer outputs are transmitted, thereby preventing the external transmission of privacy-sensitive raw input and output data. Given the black-box nature of DNNs, estimating the original input or output from intercepted hidden layer outputs poses a significant challenge for malicious eavesdroppers. Moreover, $\Lambda$-Split is orthogonal to traditional encryption-based security mechanisms, offering enhanced security when deployed in conjunction. We empirically validate the efficacy of the $\Lambda$-Split framework using Llama 2 and Stable Diffusion XL, representative large language and diffusion models developed by Meta and Stability AI, respectively. Our $\Lambda$-Split implementation is publicly accessible at https://github.com/nishio-laboratory/lambda_split.",unknown,68
10.1145/3600211.3604744,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"AAAI/ACM Conference on AI, Ethics, and Society",2000-01-01 00:00:00,semantic_scholar,the eliza defect: constructing the right users for generative ai,https://www.semanticscholar.org/paper/c4b45d26f2b04634a6b8456c83ab3f114f5ebb26,"Artificial intelligence (AI) is at the center of debates on what kind of future we want and how to bring it about. But AI ethics is not only a technical risk assessment and accounting effort, or an application of general principles to stable artifacts. It is also a social self-diagnosis, a contested and contestable assertion of values and desirable futures, and a selective understanding of the nature of AI in its different forms. In expressions of concern and efforts at preparation for increasingly powerful AI tools, we can trace the ways we imagine ourselves and our society to be compatible with AI's promises and susceptible to its dangers. The problems we notice, and the solutions we offer, arise from the interaction of these imagined elements. The socially embedded efficacy of AI tools leads many commentators to imagine their risks specifically in conjunction with understandings of society as it currently is and imaginations of how it can and should exist in the future [1]. The sense-making moves performed in the wake of developments in generative AI are thus a site to examine the movement and uses of different concepts brought together in this domain: the human, rationality, and the place of expertise. As these sense-making efforts are carried out, they become constraints on how the risks of generative AI can be noticed and understood. The problems raised by generative AI are so fundamentally tied to its performance as a simulator of human interpersonal acts that we should ask: where is the risk of generative AI located, such that the utility and the safety of the tools can be preserved after troubling cases? Boundaries between malicious deception and magnificent design are unclear without an answer to this question. Thus, to fit generative AI into our world, we are trying to answer it; this is one goal of efforts at regulation which seeks to allow the benefits of imitation to arrive while avoiding the harms of deception. In the current regulation, reporting, and corporate responses to generative AI, the challenge of safely introducing generative AI is being approached in large part as a challenge of producing the right kind of knowledge in its users. Below is a summary of my findings from three cases, chosen to investigate the following question: What ways, or whose ways, of using, knowing, and understanding generative AI are being offered as appropriate? I examine the EU AI Act language reflecting disclosure requirements for interactions with generative AI, responses to a chatbot-facilitated suicide in Belgium, and reactions to expert claims of a chatbot's sentience. In the first two cases, AI-generated content is problematic insofar as users are uninformed about its provenance or maliciously deceived by it, while users who know they are interacting with AI but behave problematically are designated as deluded or irrational. In the third case, a Google engineer who presents evidence to support claims that AI is sentient is censured as nationwide reporting denounces his claim against an expert consensus from which he is ejected. In all three cases, challenges facing widespread generative AI development and use are avoided by attending to the knowledge and understanding of those who use them rather than the functioning of the tools themselves. The EU AI Act is illuminating as a general and authoritative account of how AI interactions can be made safe, requiring first and foremost that users are informed. [2, 3] The AI Act is useful in the present paper as it shows the effort to match and reconcile a new technology with an extant set of values, chief among which is autonomy. Its reliance on disclosure reflects a general sense that harms are acceptable or unacceptable not on the basis of outcomes but based on the degree of autonomy possessed by the actors in question. Rational actors in a simulated environment are responsible for the effects of the simulation, so long as they are informed of the nature of that environment and have essentially consented to consume deceptive or false content. The other two cases I examine explore this very issue, of problematic understandings and behavior on the part of knowing users. The first of these is the case of the Belgian man. After his suicide responses from the company which provided the chatbot, media [4, 5, 6] and government [5], and prominent expert AI ethics commentary [7] characterized it as arising because the user was vulnerable and consequently did not relate to the bot in the right way. While the chatbot's emotionally charged language was seen as a part of the problem, in the reporting on this event the unanimous emphasis on the man's mental state presents the risk as arising in an interaction, in a pathological mistake of the user, rather than in the tool. Locating risk is a necessary and immensely powerful, if often unexamined step which precedes intervention in a worrisome state of affairs: where we locate risk is where we intervene. If the risk accompanying generative AI is located in the minds of uninformed or misapprehending users, disclosures and disclaimers are indeed sensible interventions. In this conception, when knowledge fails to protect the user, it is not a failed safeguard but a bad user. Problematizing user understandings in this way provides an exonerating resource for the companies providing these tools and suggests the rectitude of expert authority on the nature of these tools, by linking delays and dangers in generative AI to users who do not abide by the (strategically underdetermined) expert consensus on generative AI's accuracy, capabilities, and nature. My third case examines how the expert consensus around generative AI is maintained through the story of Blake Lemoine, who publicly announced his belief that Google's LaMDA model had become sentient and was presented by major media outlets and experts as deluded [8, 9, 10, 11, 12, 13]. In the media and corporate response to Lemoine, wherein Google questioned his sanity before firing him [13], we see his ejection from the community of experts permitted to call for greater scrutiny based on qualitative changes in the nature of these models. He becomes a layperson on account of his anthropomorphizing error. In this act of boundary work [14], policing who is in the body of experts qualified to decide on the sentience of the chatbot, and the nature of AI models in general, we must notice how small this group truly is and what Lemoine's ejection preserves. If safeguards like those Lemoine called for should follow on the kind of change he claimed to detect, and those outside Google's leadership could determine when such changes have arrived, Google would cease to lead the conversation on regulation by defining the nature of its technology. This state of affairs leaves the right relations with generative AI underdetermined but maintains that positions which challenge the expert consensus are the result of misunderstandings so significant as to disqualify the concerned party's thoughts on the matter from rational consideration. In the three cases examined here, events and concerns which threaten to depict generative AI as in need of significant scrutiny or changes are defused not by intervening in the company's technology, but by delineating between user understandings which are empowered and exploitative, safe and vulnerable, rational and deluded. Named after an early chatbot, the ELIZA effect refers to the readiness with which users anthropomorphize computer systems [15]. Reporting on both Lemoine [11] and the Belgian man cited this effect [6]. The chatbot which encouraged the Belgian man to commit suicide was named Eliza. One way of summarizing the change I trace in the cases described above is a transition away from the Turing test and towards the ELIZA effect as the conceptual frame for AI which imitates humans. While the Turing test implies the layperson's relevance to the discussion and regulation of AI, the ELIZA effect implies their irrelevance. This project will continue as an effort to follow popular, expert, and regulatory perceptions of the risk of generative AI as the tools themselves and the public concern surrounding them continue to develop. The resources of science and technology studies (STS) enable crucial perspectives on numerous ways of thinking about AI and the challenges of its development and regulation such as the common citations of law lag, invocations of self-regulation in the mode of the Asilomar Conference on rDNA, collective action problem framings, and more. The STS literature on sociotechnical imaginaries [1] and public understandings of science [16] contribute to the present insight as to how the efforts of tech-society reconciliation and risk-benefit balancing presented as appropriate for AI reveal and produce our understandings of the technology, even as they reproduce and reshape social norms. There is an urgent need for work which extends this powerful scholarly tradition for understanding science, technology, and society to AI, as one of the most important and concerning technological developments of our moment.",unknown,69
10.1051/shsconf/202317904024,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,SHS Web of Conferences,2000-01-01 00:00:00,semantic_scholar,ethical considerations in artificial intelligence: a comprehensive disccusion from the perspective of computer vision,https://www.semanticscholar.org/paper/59664744a90437583c2911fc17273670435e0774,"This paper delves deeply into the multifaceted ethical challenges within the realm of computer vision, focusing intently on various ethical dimensions inherent in this cutting-edge field. It emphasizes the pressing need to address ethical concerns related to AI technologies, including algorithmic fairness, informed consent, public engagement, robust privacy protocols, transparency, and the integration of human judgment through human-in-the-loop systems. The study underscores the vital importance of collaboration among diverse stakeholders, including governments, businesses, academia, and society, to promote responsible and equitable AI practices within computer vision.Through meticulous examination, the paper highlights the urgency of balancing technological advancement with ethical considerations. It advocates for the development and implementation of ethical principles, ensuring that AI technologies align with societal values and promote fairness, transparency, and accountability. The collaborative efforts among various sectors are crucial to fostering an ethical framework that guides the responsible deployment of AI in the field of computer vision. By integrating ethical consciousness into the core of technological innovation, this approach aims to create a symbiotic relationship between artificial intelligence and society, ultimately benefiting humanity as a whole.",unknown,70
10.5194/isprs-archives-xlviii-m-2-2023-441-2023,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences",2000-01-01 00:00:00,semantic_scholar,image retrieval for 3d modelling of architecture using ai and photogrammetry,https://www.semanticscholar.org/paper/719b6fd6347d01a2bce1afdee982b793f266914c,"Abstract. This research is intended to provide an initial solution to the problem of finding images for processing by photogrammetry in special cases where these do not exist. An overview of existing artificial intelligence-based algorithms that enable the extension of source image dataset is reported. In particular, this research focused on the use of prompt-to-image systems for obtaining images to be used in reconstruction and then in the next step of 3D modelling. Thus, the combined use of these three techniques, AI, photogrammetry, and modelling allowed the creation of a model of a building that never existed except in the collective imagination, which is the tower of Babel. In particular, the case study chosen is the illustration in Kircher book present in the library of the Brixen seminary that is closed to the public and for which it was necessary to create a tool to enhance the value and knowledge of this heritage for external users. Therefore, the creation of an augmented reality app enabled the visualization of the model created by offering possibilities for immersive experiences and dissemination of the research to a wide audience.
",unknown,71
10.52458/23492589.2023.v10.iss1.kp.a1,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Kaav International Journal of Law, Finance &amp; Industrial Relations",2000-01-01 00:00:00,semantic_scholar,changing landscape of artificial intelligence on indian corporate sectors and governance: special reference to smes,https://www.semanticscholar.org/paper/6d0644272448fde31f868f87d25110a74fb48ff7,"Artificial intelligence (AI) has the likely to start a new industrial upheaval that will primarily alter market forces at work and working circumstances. This study tries to clarify what artificial intelligence (AI) is, how it could affect corporate sectors and SME operations, and India's adoption challenges. Governments can help SMEs develop their risk supervision procedures. Most industries can benefit from using AI, although some are more likely to do so than others. AI may be used in several business areas and can alter the internal value chain of the company. The areas of business where artificial intelligence (AI) are expected to have the most effects. Objective: It is necessary to create the circumstances for a reliable transition and increase knowledge of the benefits of AI among SME managers and employees. A participative approach should be taken when revamping work processes and training AI models, and national and local governments should coordinate efforts to reskill SME managers and employees. Then, until AI can fulfil its full potential, mechanisms for bridging the financing gap should be identified. Research Methodology: The study's foundations include both qualitative and quantitative research. Regulators and policymakers should make sure that knowledge markets that offer cloud solutions with embedded AI technologies are operating smoothly. Findings: The findings of the research are based on the role and impact of AI on different aspects of governance like public administration, tax compliances, market competition, infrastructure, finances, labor market etc. Conclusion: AI lowers prediction costs significantly and makes decision-making easier. To map uncertainties and reduce risk exposure, SMEs may use predictive analytics. They can also automate business estimates, including sales and budget forecasts, or improve the effectiveness of asset maintenance and management. Greater market segmentation and price differentiation are made possible by improved prediction capabilities, which also provide SMEs the chance to innovate since they are better able to foresee customer behavior and price sensitivity as well as demand changes.",unknown,72
10.1108/ijoes-05-2023-0107,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Journal of Ethics and Systems,2000-01-01 00:00:00,semantic_scholar,ethical issues in the development of artificial intelligence: recognizing the risks,https://www.semanticscholar.org/paper/00c3e0b19febce5d401c5955482e95dadaf184c0,"
Purpose
This study aims to analyse the ethical implications associated with the development of artificial intelligence (AI) technologies and to examine the potential ethical ramifications of AI technologies.


Design/methodology/approach
This study undertakes a thorough examination of existing academic literature pertaining to the ethical considerations surrounding AI. Additionally, it conducts in-depth interviews with individuals to explore the potential benefits and drawbacks of AI technology operating as autonomous ethical agents. A total of 20 semi-structured interviews were conducted, and the data were transcribed using grounded theory methodology.


Findings
The study asserts the importance of fostering an ethical environment in the progress of AI and suggests potential avenues for further investigation in the field of AI ethics. The study finds privacy and security, bias and fairness, trust and reliability, transparency and human–AI interactions as major ethical concerns.


Research limitations/implications
The implications of the study are far-reaching and span across various domains, including policy development, design of AI systems, establishment of trust, education and training, public awareness and further research. Notwithstanding the potential biases inherent in purposive sampling, the constantly evolving landscape of AI ethics and the challenge of extrapolating findings to all AI applications and contexts, limitations may still manifest.


Originality/value
The novelty of the study is attributed to its comprehensive methodology, which encompasses a wide range of stakeholder perspectives on the ethical implications of AI in the corporate sector. The ultimate goal is to promote the development of AI systems that exhibit responsibility, transparency and accountability.
",unknown,73
10.1108/jices-12-2022-0108,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Journal of Information, Communication and Ethics in Society",2000-01-01 00:00:00,semantic_scholar,from big data epistemology to ai politics: rescuing the public dimension over data-driven technologies,https://www.semanticscholar.org/paper/df57622f13676a4a8185df9424bea75243d8fc9e,"
Purpose
The purpose of this paper is to explore the epistemological tensions embedded within big data and data-driven technologies to advance a socio-political reconsideration of the public dimension in the assessment of their implementation.


Design/methodology/approach
This paper builds upon (and revisits) the European Union’s (EU) normative understanding of artificial intelligence (AI) and data-driven technologies, blending reflections rooted in philosophy of technology with issues of democratic participation in tech-related matters.


Findings
This paper proposes the conceptual design of sectorial and/or local-level e-participation platforms to ignite an ongoing discussion – involving experts, private actors, as well as cognizant citizens – over the implementation of data-driven technologies, to avoid siloed, tech-solutionist decisions.


Originality/value
This paper inscribes the EU’s normative approach to AI and data-driven technologies, as well as critical work on the governance of these technologies, into a broader political dimension, suggesting a way to democratically and epistocratically opening up the decisional processes over the development and implementation of these technologies and turn such processes into a systemic civic involvement.
",unknown,74
10.34190/ecel.22.1.1831,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,European Conference on e-Learning,2000-01-01 00:00:00,semantic_scholar,is it the new google: impact of chatgpt on students' information search habits,https://www.semanticscholar.org/paper/b6f95beed6dd6cc610a0949b5814ed9117d737ac,"The traditional subject of information search and retrieval (IR) paradigm shifted to an entirely new era since artificial intelligence (AI) techniques were introduced into the field. Browser-based IR solutions powered by AI for personalised recommendations-based information retrieval, such as the Google search engine, were one of the early examples. The IR field has advanced to its next level with the newest conversational applications based on large language model (LLM) techniques. It is becoming clear that Generative Pretrained Transformer (GPT) applications such as ChatGPT will significantly impact information retrieval behaviour in the education sector. Though this application has become widespread in acclaim, no previous study has shown its impact on information seeking and retrieval. However, based on the observation of the fast penetration of this technology and the growth of public interest, a pre-assumption was built that it is essential to investigate if students may also be showing a similar interest in this new tool. Hence, this study is set up to systematically and empirically explore how ChatGPT influences the IR behaviour of students in HEIs. A survey approach is utilised to collect the perceived IR behaviour through a questionnaire administered to 60 students in HEIs. The findings reveal that the tool is already widely known among HEI students. They also perceived the use of the tool in the context of information retrieval and proclaimed its usefulness, acknowledging its efficiency (reduced time) in finding information. Furthermore, the technology has considerably affected the typical use of other conventional information retrieval and search engine tools. On the contrary, 10% of the respondents are less likely to use ChatGPT during information seeking for various reasons, from credibility and relevance to technology infrastructure issues such as connectivity. Although a deeper analysis is required to establish a general conclusion on how and in which ways GPT-based models will override contemporary IR practices, the study outcome provides evidence for a possible behavioural change among HEI students in their IR habits in the future.",unknown,75
10.17705/1cais.05331,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Communications of the Association for Information Systems,2000-01-01 00:00:00,semantic_scholar,giusberto: italy's ai-based judicial transformation: a teaching case,https://www.semanticscholar.org/paper/07e31c18f8f831aed37010eff6e918447d55856e,"In an age when open access to law enforcement files and judicial documents can erode individual privacy and confidentiality, miscreants can abuse this open access to personal information for blackmail, misinformation, and even social engineering. Yet, limiting access to law enforcement and court cases is a freedom-of-information violation. To address this tension, this collaborative action-research-based teaching case exemplifies how Italy’s Corte dei Conti (Court of Auditors) used artificial intelligence in the automated deidentification and anonymization of court documents in Italy’s public sector. This teaching case is aimed at undergraduate and graduate students learning about Artificial Intelligence (AI), Large Language Model (LLM) (e.g., ChatGPT) evolution, development, and operations. The case will help students learn the origin and evolution of AI transformer models and architectures, and discusses the GiusBERTo operation and process, highlighting opportunities and challenges. GiusBERTo, Italy’s custom-AI model, offers an innovative approach that walks a tightrope between anonymizing Italy’s judicial court documents without sacrificing context or information loss. The case ends with a series of questions, challenges, and potential for LLMs in data anonymization.",unknown,76
10.3390/asi7010006,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Applied System Innovation,2000-01-01 00:00:00,semantic_scholar,ai-powered academic guidance and counseling system based on student profile and interests,https://www.semanticscholar.org/paper/bba496787711047003c174009c034641bd84ab07,"Over the past few decades, the education sector has achieved impressive advancements by incorporating Artificial Intelligence (AI) into the educational environment. Nevertheless, specific educational processes, particularly educational counseling, still depend on traditional procedures. The current method of conducting group sessions between counselors and students does not offer personalized assistance or individual attention, which can cause stress to students and make it difficult for them to make informed decisions about their coursework and career path. This paper proposes a counseling solution designed to aid high school seniors in selecting appropriate academic paths at the tertiary level. The system utilizes a predictive model that considers academic history and student preferences to determine students’ likelihood of admission to their chosen university and recommends similar alternative universities to provide more opportunities. We developed the model based on data from 500 graduates from 12 public high schools in Morocco, as well as eligibility criteria from 31 institutions and colleges. The counseling system comprises two modules: a recommendation module that uses popularity-based and content-based recommendations and a prediction module that calculates the likelihood of admission using the Huber Regressor model. This model outperformed 13 other machine learning modules, with a low MSE of 0.0017, RMSE of 0.0422, and the highest R-squared value of 0.9306. Finally, the system is accessible through a user-friendly web interface.",unknown,77
10.46324/pmp2303307,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Postmodernism Problems,2000-01-01 00:00:00,semantic_scholar,revolutionizing education: the transformative power of ai technologies in pr,https://www.semanticscholar.org/paper/88a9d7421e4b36ca080be31d72f390736f3c4458,"In the coming years, Artificial Intelligence (AI) is set to make a profound impact on higher education, particularly within the field of Public Relations (PR). This surge in the adoption of AI technologies in academia is fuelled by compelling factors. AI is on the verge of transforming PR practices, elevating the efficiency and effectiveness of communication strategies. The data-driven prowess of AI equips PR professionals with enhanced capabilities to understand and engage with target audiences, tailor messages with precision, automate routine tasks, such as content creation, social media management, and data analysis, and liberate practitioners from administrative burdens. As educational institutions increasingly recognize the value of AI in PR, investments in its implementation are expected, heralding a paradigm shift in higher education communication and stakeholder engagement. The current article aims to delve into the European framework for AI applications in education, present the landscape of existing technology tools used in the communication area, and review recent publications describing the benefits and possible drawbacks of using generative AI technologies in the sphere of teaching.",unknown,78
10.26425/2309-3633-2023-11-2-103-113,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,UPRAVLENIE / MANAGEMENT (Russia),2000-01-01 00:00:00,semantic_scholar,value and risks of morphing technology into strategy and business model,https://www.semanticscholar.org/paper/4fd7a5a1abd0106b7d8567fba366ce868bba7150,"Digital technologies became the primary source of innovation in the private and public sectors. The Internet profoundly changed the way businesses are run catapulting “most digital” industries and companies to the top of the S&P500. Two innovations that drive digital transformation changing the nature of competition are cloud computing and artificial intelligence (AI) technologies. Cloud-native business models and strategies proved successful in various industries, while AI is being tested in vivo by management mainstream. The publication provides an analysis of a multidimensional impact cloud computing makes on strategies and business models of companies. We show that what made cloud computing special in the management context was the way it morphed into strategies and business models best suited for the uncertain future. We also noted that as the focus of digital transformation shifts towards cloud-based AI powered decision-making solutions, managing the human aspect of “more digital” business models and related risks, recently referred to as an existential threat, becomes a priority of management research.",unknown,79
10.1145/3593013.3594032,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Conference on Fairness, Accountability and Transparency",2000-01-01 00:00:00,semantic_scholar,you sound depressed: a case study on sonde health’s diagnostic use of voice analysis ai,https://www.semanticscholar.org/paper/c25067c6078f3c6aa6478b6348c9d23a40cfc1c0,"There is growing interest within the medical sector about the diagnostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However, insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study of Sonde Health, a Boston-based startup that purports to offer “objective” depression detection and monitoring via its Mental Fitness app that extracts and analyzes the acoustic features of the user’s voice. Using a critical disability studies lens, we conducted a textual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizing Sonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score, the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care.",unknown,81
10.21556/edutec.2023.86.3071,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Edutec. Revista Electrónica de Tecnología Educativa,2000-01-01 00:00:00,semantic_scholar,data ecosystems in education: opportunities and challenges,https://www.semanticscholar.org/paper/729b507063248ca40cb8fe214d2db62adfba1069,"The integration of ecosystems that promote the massive use of data and the large-scale processing of this data into educational processes represent an unprecedented social challenge. This change, which we could consider paradigmatic, has been promoted by various factors which mutually magnify each other's effects. 
In recent years the massive collection and storage of data has intensified through ubiquitous technologies, which co-exist with human actors. In addition, we have witnessed an increase in the intentional processing power of this data at a speed unimaginable a few years ago (through the application of Artificial Intelligence, hereafter AI) . This has turned many aspects of our lives into data, from which value is extracted by third parties through processes of datafication. Concurrently, the public at large - including the educational sector - has been promised educational ‘personalisation’, a concept derived directly from other industrial production scenarios where AI is already in use, and every experience is datafied. In addition, a rapidly growing economic sector - the EdTech industry - has emerged with a capacity to monetise the educational sphere at a global level, extracting not only profits from its present and future actions with private or institutional clients, but also generating income based on capturing the motivation of their users and achieving significant levels of social and political influence. All of the above is magnified by the proliferation of the use of online platforms and tools in educational spaces (such as learning analytics and online exam proctoring), accelerated by the demand for online educaiton resulting from ‘lockdowns’ during the CoVid-19 pandemic. 
This special issue aims to bring together articles that problematise the challenges and unintended effects that new mechanisms and dynamics fuelled by these data-driven technologies have introduced into education. This call aims to encourage researchers and practitioners to share studies, research, debates and academically well-founded reflections that propose critical visions. This aims to provide those who are interested in education (either as teachers, academic faculty, researchers or managers ) deeper analyses, to allow us to understand the current educational and technological landscape, and to foster a revaluation of relevant educational issues that should be part of our work in the coming years.",unknown,82
10.1002/ctm2.1207,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Clinical and Translational Medicine,2000-01-01 00:00:00,semantic_scholar,the future of chatgpt in academic research and publishing: a commentary for clinical and translational medicine,https://www.semanticscholar.org/paper/ae17e17ed3a36ba3ffbadc9de2b10909aa13226c,"ChatGPT, an artificial intelligence (AI)-powered chatbot developed by OpenAI, is creating a buzz across all occupational sectors. Its name comes from its basis in the Generative Pretrained Transformer (GPT) language model. ChatGPT’s most promising feature is its ability to offer human-like responses to text input using deep learning techniques at a level far superior to any other AImodel. Its rapid integration in various industries signals the public’s burgeoning reliance on AI technology. Thus, it is essential to critically evaluate ChatGPT’s potential impacts on academic clinical and translational medicine research.",unknown,83
10.1108/jcom-09-2022-0113,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Communication Management,2000-01-01 00:00:00,semantic_scholar,ai-driven disinformation: a framework for organizational preparation and response,https://www.semanticscholar.org/paper/497321544abab96bbbf6a2f683fc3e9751d58053,"PurposeDisinformation, false information designed with the intention to mislead, can significantly damage organizational operation and reputation, interfering with communication and relationship management in a wide breadth of risk and crisis contexts. Modern digital platforms and emerging technologies, including artificial intelligence (AI), introduce novel risks in crisis management (Guthrie and Rich, 2022). Disinformation literature in security and computer science has assessed how previously introduced technologies have affected disinformation, demanding a systematic and coordinated approach for sustainable counter-disinformation efforts. However, there is a lack of theory-driven, evidence-based research and practice in public relations that advises how organizations can effectively and proactively manage risks and crises driven by AI (Guthrie and Rich, 2022).Design/methodology/approachAs a first step in closing this research-practice gap, the authors first synthesize theoretical and technical literature characterizing the effects of AI on disinformation. Upon this review, the authors propose a conceptual framework for disinformation response in the corporate sector that assesses (1) technologies affecting disinformation attacks and counterattacks and (2) how organizations can proactively prepare and equip communication teams to better protect businesses and stakeholders.FindingsThis research illustrates that future disinformation response efforts will not be able to rely solely on detection strategies, as AI-created content quality becomes more and more convincing (and ultimately, indistinguishable), and that future disinformation management efforts will need to rely on content influence rather than volume (due to emerging capabilities for automated production of disinformation). Built upon these fundamental, literature-driven characteristics, the framework provides organizations actor-level and content-level perspectives for influence and discusses their implications for disinformation management.Originality/valueThis research provides a theoretical basis and practitioner insights by anticipating how AI technologies will impact corporate disinformation attacks and outlining how companies can respond. The proposed framework provides a theory-driven, practical approach for effective, proactive disinformation management systems with the capacity and agility to detect risks and mitigate crises driven by evolving AI technologies. Together, this framework and the discussed strategies offer great value to forward-looking disinformation management efforts. Subsequent research can build upon this framework as AI technologies are deployed in disinformation campaigns, and practitioners can leverage this framework in the development of counter-disinformation efforts.",unknown,84
10.1371/journal.pone.0295277,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,PLoS ONE,2000-01-01 00:00:00,semantic_scholar,population preferences for ai system features across eight different decision-making contexts,https://www.semanticscholar.org/paper/c644063a31a9377b069e5f8568db83e24e4f35ff,"Artificial intelligence systems based on deep learning architectures are being investigated as decision-support systems for human decision-makers across a wide range of decision-making contexts. It is known from the literature on AI in medicine that patients and the public hold relatively strong preferences in relation to desirable features of AI systems and their implementation, e.g. in relation to explainability and accuracy, and in relation to the role of the human decision-maker in the decision chain. The features that are preferred can be seen as ‘protective’ of the patient’s interests. These types of preferences may plausibly vary across decision-making contexts, but the research on this question has so far been almost exclusively performed in relation to medical AI. In this cross-sectional survey study we investigate the preferences of the adult Danish population for five specific protective features of AI systems and implementation across a range of eight different use cases in the public and commercial sectors ranging from medical diagnostics to the issuance of parking tickets. We find that all five features are seen as important across all eight contexts, but that they are deemed to be slightly less important when the implications of the decision made are less significant to the respondents.",unknown,85
10.35940/ijeat.a4282.1013123,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Journal of Engineering and Advanced Technology,2000-01-01 00:00:00,semantic_scholar,transforming organizational development with ai: navigating change and innovation for success,https://www.semanticscholar.org/paper/51791a95c6ba077072018ce8c0d294102953cc6a,"Effective change management emerges as a deciding element for an organization's survival and success in the changing terrain of today's fiercely competitive business climate. The variety of change management theories and approaches that are currently available, however, paints a complicated picture that is plagued by inconsistencies, a lack of strong empirical support, and unproven assumptions about contemporary organizational dynamics. This essay seeks to set the basis for a fresh paradigm for effective change administration by critically analyzing popular change management ideas. The gap between theory and practice is addressed in the paper, which concludes with suggestions for more research. In parallel, artificial intelligence (AI) has made incredible progress, giving rise to computers that mimic human autonomy and cognition. Industry-wide excitement has been sparked by the enthusiasm among academics, executives, and the general public, which has resulted in significant investments in utilizing AI's potential through creative business models. However, the lack of thorough academic guidance forces managers to struggle with AI integration issues, increasing the risk of project failure. An in-depth analysis of AI's complexities and its function as a spark for revolutionary business model innovation is provided in this article. A thorough literature assessment, which involves sifting through a sizable library of published works, combines up-to-date information on how AI is affecting the development of new business models. The findings come together to form a roadmap for seamless AI integration that includes four steps: understanding the fundamentals of AI and the skills needed for digital transformation, understanding current business models and their innovation potential, nurturing key proficiencies for AI assimilation, and gaining organizational acceptance while developing internal competencies. This article combines the fields of organizational change management and AI-driven business model innovation with ease, providing a thorough explanation to assist businesses in undergoing a successful transformation and innovation. These disciplines' confluence offers a practical vantage point for successfully adapting to, thriving in, and profiting within a dynamic business environment. Artificial intelligence (AI), a massively disruptive force that is altering international businesses, is at the vanguard of this revolution. The ability of AI to make decisions automatically, based on data analysis and observation, opens up hitherto untapped possibilities for value creation and competitive dominance, with broad consequences spanning several industries. With its quick scaling, ongoing improvement, and self-learning capabilities, this evolutionary invention functions as an agile capital-labor hybrid. Significantly, AI's architecture serves as the cornerstone for data-driven decision support by deftly sifting through large and complicated datasets to extract insights. Thus, the symbiotic marriage of organizational change management and AI-driven business model innovation gives a thorough narrative, directing businesses towards not just surviving, but thriving in an ever-evolving business environment. It is underlined how business models (BMs) interact with technology to affect how well business’s function, underlining the need of taking BMs into account while using AI. Business model innovation (BMI) that AI unlocks may improve goods, streamline processes, and save costs. However, there is a void between technological improvements and their operationalization via BMs. Successful AI integration depends on a well-structured BM, which promotes agility and makes the most of technological resources. BMI is accelerated by AI, which reshapes sectors via innovation. Although interest in AI is high, strategic, cultural, and technological constraints sometimes prevent large investments from producing positive economic results. To fully utilize AI's capabilities, structured BMs are required. Despite an increase in research, there is still little cohesive information about the business uses of AI. In an effort to close this gap, we examine implementation-related AI problems. Analyzing AI-driven BM transformation and risk management is aided by a study on BMI and digital transformation at the same time. The purpose of this study is to further our understanding of AI-driven business model innovation and to provide a useful framework to help practitioners navigate the potential and difficulties of AI implementation. The suggested roadmap aims to identify current knowledge gaps and future research initiatives.",unknown,87
10.18178/jaai.2023.1.2.103-116,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Advances in Artificial Intelligence,2000-01-01 00:00:00,semantic_scholar,use of domain engineering in hyperautomation applied to decision making in government,https://www.semanticscholar.org/paper/87d110893ac357209247502dfdd2d5312ec44cfd,"This article presents the domain engineering process carried out to obtain the requirements for the implementation of an Artificial Intelligence (AI) compliance framework aimed at the public sector. Owing to the current competitive and fast economy, which generates huge demand for increasingly efficient, reliable, and transparent intelligent systems, decision-support architectures should also be developed under strong restrictions of cost and time. Such a context requires adequate structures, processes, and technologies for coping with the complexity of building such intelligent systems. Currently, many public organizations have adopted applications for process automation, with the aim of refraining from repetitive work and producing more efficient results. However, what is not so often observed is the development of intelligent engines to support complex public decision-making. Possible explanations are the plethora of available data sources and the number of legal norms to be abided by. Moreover, it is important to highlight the need to incorporate transparency, auditability, reusability, and flexibility into such systems. Thus, they can be safely utilized in various analogous situations, reducing the need to develop new applications from scratch. An architecture suitable for supporting public decision-making with so many features and increasingly unstructured data, as well as abundant regulation, needs well-crafted formal specifications. This article aims to analyze three existing frameworks and carry out domain engineering studies in three cases to produce some guidance for future public applications and services based on AI. Next, we provide a conceptual preliminary architectural definition for the public sector. The proposed architecture targets were identified in the three cases studied, namely, frequent tasks of process mining requirements, detection of anomalies, and extraction of rules and public policies for helping public servants. All these aim at expedient AI development for public decision-making.",unknown,88
10.2196/47847,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,JMIR Formative Research,2000-01-01 00:00:00,semantic_scholar,the adoption of ai in mental health care–perspectives from mental health professionals: qualitative descriptive study,https://www.semanticscholar.org/paper/59b6218ff63c469469402502fa19a7fbd0ba717f,"Background Artificial intelligence (AI) is transforming the mental health care environment. AI tools are increasingly accessed by clients and service users. Mental health professionals must be prepared not only to use AI but also to have conversations about it when delivering care. Despite the potential for AI to enable more efficient and reliable and higher-quality care delivery, there is a persistent gap among mental health professionals in the adoption of AI. Objective A needs assessment was conducted among mental health professionals to (1) understand the learning needs of the workforce and their attitudes toward AI and (2) inform the development of AI education curricula and knowledge translation products. Methods A qualitative descriptive approach was taken to explore the needs of mental health professionals regarding their adoption of AI through semistructured interviews. To reach maximum variation sampling, mental health professionals (eg, psychiatrists, mental health nurses, educators, scientists, and social workers) in various settings across Ontario (eg, urban and rural, public and private sector, and clinical and research) were recruited. Results A total of 20 individuals were recruited. Participants included practitioners (9/20, 45% social workers and 1/20, 5% mental health nurses), educator scientists (5/20, 25% with dual roles as professors/lecturers and researchers), and practitioner scientists (3/20, 15% with dual roles as researchers and psychiatrists and 2/20, 10% with dual roles as researchers and mental health nurses). Four major themes emerged: (1) fostering practice change and building self-efficacy to integrate AI into patient care; (2) promoting system-level change to accelerate the adoption of AI in mental health; (3) addressing the importance of organizational readiness as a catalyst for AI adoption; and (4) ensuring that mental health professionals have the education, knowledge, and skills to harness AI in optimizing patient care. Conclusions AI technologies are starting to emerge in mental health care. Although many digital tools, web-based services, and mobile apps are designed using AI algorithms, mental health professionals have generally been slower in the adoption of AI. As indicated by this study’s findings, the implications are 3-fold. At the individual level, digital professionals must see the value in digitally compassionate tools that retain a humanistic approach to care. For mental health professionals, resistance toward AI adoption must be acknowledged through educational initiatives to raise awareness about the relevance, practicality, and benefits of AI. At the organizational level, digital professionals and leaders must collaborate on governance and funding structures to promote employee buy-in. At the societal level, digital and mental health professionals should collaborate in the creation of formal AI training programs specific to mental health to address knowledge gaps. This study promotes the design of relevant and sustainable education programs to support the adoption of AI within the mental health care sphere.",unknown,89
10.1109/access.2023.3297646,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Access,2000-01-01 00:00:00,semantic_scholar,when ai meets information privacy: the adversarial role of ai in data sharing scenario,https://www.semanticscholar.org/paper/a90bef9cb05006aec434222f629499a325ba0252,"Artificial intelligence (AI) is a transformative technology with a substantial number of practical applications in commercial sectors such as healthcare, finance, aviation, and smart cities. AI also has strong synergy with the information privacy (IP) domain from two distinct aspects: as a protection tool (i.e., safeguarding privacy), and as a threat tool (i.e., compromising privacy). In the former case, AI techniques are amalgamated with the traditional anonymization techniques to improve various key components of the anonymity process, and therefore, privacy is safeguarded effectively. In the latter case, some adversarial knowledge is aggregated with the help of AI techniques and subsequently used to compromise the privacy of individuals. To the best of our knowledge, threats posed by AI-generated knowledge such as synthetic data (SD) to information privacy are often underestimated, and most of the existing anonymization methods do not consider/model this SD-based knowledge that can be available to the adversary, leading to privacy breaches in some cases. In this paper, we highlight the role of AI as a threat tool (i.e., AI used to compromise an individual’s privacy), with a special focus on SD that can serve as background knowledge leading to various kinds of privacy breaches. For instance, SD can encompass pertinent information (e.g., total # of attributes in data, distributions of sensitive information, category values of each attribute, minor and major values of some attributes, etc.) about real data that can offer a helpful hint to the adversary regarding the composition of anonymized data, that can subsequently lead to uncovering the identity or private information. We perform reasonable experiments on a real-life benchmark dataset to prove the pitfalls of AI in the data publishing scenario (when a database is either fully or partially released to public domains for conducting analytics).",unknown,90
10.3390/su16010218,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Sustainability,2000-01-01 00:00:00,semantic_scholar,cleaner chips: decarbonization in semiconductor manufacturing,https://www.semanticscholar.org/paper/ab7620be8921719d3e3a22f5af12850a822093e6,"The growth of the information and communication technology sector has vastly accelerated in recent decades because of advancements in digitalization and Artificial Intelligence (AI). Scope 1, 2, and 3 greenhouse gas emissions data of the top six semiconductor manufacturing companies (Samsung Electronics, Taiwan Semiconductor Manufacturing Corporation, Micron, SK Hynix, Kioxia, and Intel) were gathered from the publicly accessible Carbon Disclosure Project’s (CDP) website for 2020. Scope 3 emissions had the largest share in total annual emissions with an average share of 52%, followed by Scope 2 (32%) and Scope 1 (16%). Because of the absence of a standardized methodology for Scope 3 emissions estimation, each company used different methodologies that resulted in differences in emissions values. An analysis of the CDP reporting data did not reveal information on strategies implemented by companies to reduce Scope 3 emissions. The use of renewable energy certificates had the largest effect on decarbonization centered on reducing Scope 2 emissions, followed by the deployment of perfluorocarbon reduction technologies to help reduce Scope 1 fugitive emissions. Technology-specific marginal abatement costs of CO2 were also estimated and varied between −416 and 12,215 USD/t CO2 eq., which primarily varied depending on the technology deployed.",unknown,91
10.1021/acs.jcim.3c00735,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Chemical Information and Modeling,2000-01-01 00:00:00,semantic_scholar,"integrating reaction schemes, reagent databases, and virtual libraries into fragment-based design by reinforcement learning",https://www.semanticscholar.org/paper/deba8c7ee9978e1663af4118358357de736d1bce,"Lead optimization supported by artificial intelligence (AI)-based generative models has become increasingly important in drug design. Success factors are reagent availability, novelty, and the optimization of multiple properties. Directed fragment-replacement is particularly attractive, as it mimics medicinal chemistry tactics. Here, we present variations of fragment-based reinforcement learning using an actor-critic model. Novel features include freezing fragments and using reagents as the fragment source. Splitting molecules according to reaction schemes improves synthesizability, while tuning network output probabilities allows us to balance novelty versus diversity. Combining fragment-based optimization with virtual library encodings allows the exploration of large chemical spaces with synthesizable ideas. Collectively, these enhancements influence design toward high-quality molecules with favorable profiles. A validation study using 15 pharmaceutically relevant targets reveals that novel structures are obtained for most cases, which are identical or related to independent validation sets for each target. Hence, these modifications significantly increase the value of fragment-based reinforcement learning for drug design. The code is available on GitHub: https://github.com/Sanofi-Public/IDD-papers-fragrl.",unknown,92
10.1109/tim.2023.3317913,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Transactions on Instrumentation and Measurement,2000-01-01 00:00:00,semantic_scholar,multiscale analysis domain interpretable deep neural network for detection of breast cancer using thermogram images,https://www.semanticscholar.org/paper/15161709d92a407730aecae04d4426accf0c49b6,"Breast cancer is the most prevalent cancer among women, with a high mortality rate. The early detection of breast cancer using medical imaging techniques helps reduce the number of deaths caused by this disease. Thermogram imaging is safer and less expensive than mammography for diagnosing breast cancer. The automated analysis of thermogram images using artificial intelligence (AI) methods is an interesting approach to detect breast cancer. This article proposes a novel multiscale analysis domain interpretable deep learning (MSADIDL) approach for automatically detecting breast cancer using thermogram images. The 2D empirical wavelet transform (2DEWT) with fixed boundary points (FBPs) is employed for the multiscale analysis of thermogram images and evaluation of modes or subbands. All the modes of the thermogram images are used as the input to the MSADIDL model for the automated detection of breast cancer. The MSADIDL architecture comprises seven individual deep neural networks (DNNs) connected in parallel. The outputs of the individual DNNs are concatenated and then used as the input to the dense layers, after which the output layer evaluates the probability score for the automated categorization of normal versus cancerous classes. A publicly available thermogram imaging dataset is utilized to evaluate the performance of the proposed MSADIDL approach. The results show that the proposed MSADIDL approach has obtained an accuracy value of 99.54% for both fivefold cross-validation (CV) and hold-out validation cases using all seven modes of thermogram images. The MSADIDL model has achieved an accuracy higher than all of the transfer learning-based breast cancer detection techniques using thermogram images. The suggested MSADIDL model has shown higher accuracy when compared with different existing methods to detect breast cancer using thermogram images.",unknown,93
10.48550/arxiv.2311.07326,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,arXiv.org,2000-01-01 00:00:00,semantic_scholar,metasymnet: a dynamic symbolic regression network capable of evolving into arbitrary formulations,https://www.semanticscholar.org/paper/4d233f8d623e01205e0c3327df2095f7746956da,"Mathematical formulas serve as the means of communication between humans and nature, encapsulating the operational laws governing natural phenomena. The concise formulation of these laws is a crucial objective in scientific research and an important challenge for artificial intelligence (AI). While traditional artificial neural networks (MLP) excel at data fitting, they often yield uninterpretable black box results that hinder our understanding of the relationship between variables x and predicted values y. Moreover, the fixed network architecture in MLP often gives rise to redundancy in both network structure and parameters. To address these issues, we propose MetaSymNet, a novel neural network that dynamically adjusts its structure in real-time, allowing for both expansion and contraction. This adaptive network employs the PANGU meta function as its activation function, which is a unique type capable of evolving into various basic functions during training to compose mathematical formulas tailored to specific needs. We then evolve the neural network into a concise, interpretable mathematical expression. To evaluate MetaSymNet's performance, we compare it with four state-of-the-art symbolic regression algorithms across more than 10 public datasets comprising 222 formulas. Our experimental results demonstrate that our algorithm outperforms others consistently regardless of noise presence or absence. Furthermore, we assess MetaSymNet against MLP and SVM regarding their fitting ability and extrapolation capability, these are two essential aspects of machine learning algorithms. The findings reveal that our algorithm excels in both areas. Finally, we compared MetaSymNet with MLP using iterative pruning in network structure complexity. The results show that MetaSymNet's network structure complexity is obviously less than MLP under the same goodness of fit.",unknown,94
10.2478/jdis-2023-0014,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Data and Information Science,2000-01-01 00:00:00,semantic_scholar,build neural network models to identify and correct news headlines exaggerating obesity-related scientific findings,https://www.semanticscholar.org/paper/9a409f3f8d411140911a60e42b06d893f756a46c,"Abstract Purpose Media exaggerations of health research may confuse readers’ understanding, erode public trust in science and medicine, and cause disease mismanagement. This study built artificial intelligence (AI) models to automatically identify and correct news headlines exaggerating obesity-related research findings. Design/methodology/approach We searched popular digital media outlets to collect 523 headlines exaggerating obesity-related research findings. The reasons for exaggerations include: inferring causality from observational studies, inferring human outcomes from animal research, inferring distant/end outcomes (e.g., obesity) from immediate/intermediate outcomes (e.g., calorie intake), and generalizing findings to the population from a subgroup or convenience sample. Each headline was paired with the title and abstract of the peer-reviewed journal publication covered by the news article. We drafted an exaggeration-free counterpart for each original headline and fined-tuned a BERT model to differentiate between them. We further fine-tuned three generative language models—BART, PEGASUS, and T5 to autogenerate exaggeration-free headlines based on a journal publication’s title and abstract. Model performance was evaluated using the ROUGE metrics by comparing model-generated headlines with journal publication titles. Findings The fine-tuned BERT model achieved 92.5% accuracy in differentiating between exaggeration-free and original headlines. Baseline ROUGE scores averaged 0.311 for ROUGE-1, 0.113 for ROUGE-2, 0.253 for ROUGE-L, and 0.253 ROUGE-Lsum. PEGASUS, T5, and BART all outperformed the baseline. The best-performing BART model attained 0.447 for ROUGE-1, 0.221 for ROUGE-2, 0.402 for ROUGE-L, and 0.402 for ROUGE-Lsum. Originality/value This study demonstrated the feasibility of leveraging AI to automatically identify and correct news headlines exaggerating obesity-related research findings.",unknown,95
10.18500/1818-9601-2023-23-4-447-453,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Izvestiya of Saratov University. Sociology. Politology,2000-01-01 00:00:00,semantic_scholar,strategic planning in public administration as a political system of institutional instruments and goal-setting mechanisms,https://www.semanticscholar.org/paper/39a95e3f3124887f824d47586b917916466f5def,"The article is devoted to the subject field of political science research within the scientific specialty “Public Administration and sectoral policies” and the analysis of the practice of state strategic planning as a political system of institutional instruments and goal-setting mechanisms in the Russian Federation. The article discusses public political and legal mechanisms for combining values and goals with the choice of ways and methods to achieve them. The constitutional exclusivity and independence of each branch of government as the basis of its strategic resource and strategic planning potential is highlighted as the fundamental public political and legal mechanism for combining values and goals with the choice of ways and methods to achieve them. A general description of the architecture of the unified system of public power in accordance with the amendments to the Constitution of Russia and the current configuration of the public administration system is given. The importance of political factor of the effectiveness of the administration of strategic management system and the implementation of sectoral policies in modern Russia on the basis of the mechanisms of coordinated functioning and interaction of public authorities and local self-government in a single system of public authority is shown. Fundamental changes are highlighted that make it possible to significantly increase the efficiency of the federal government in a unified system of public authority with an emphasis on the implementation of national development goals of both the Russian Federation as a whole and the regions based on the sectoral structure of the economy. Attention is focused on the political goals of the regional factor of socio-economic development of territories on the basis of mechanisms and tools of strategic planning, the directions of fundamental political, legal, administrative and managerial decisions on the implementation of the regional investment standard are determined. The role of federal institutions of innovative development and provision of infrastructure projects and programs for solving the tasks of ensuring sustainable economic growth and diversification of the modern Russian economy, which cannot be optimally implemented by market mechanisms, was demonstrated. Conclusions are drawn about the importance of the interpretative understanding of politics as a system of institutional tools and goal-setting mechanisms in the political and legal practice of state strategic planning for determining promising directions in the subject field of scientific research within the specialty “Public Administration and sectoral policies”, for solving problems of improving the quality of public administration through the introduction of a management model based on big data and artificial intelligence, the transition of the public authority system to a data-based management model using a platform approach.",unknown,96
10.3390/brainsci13020348,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Brain Science,2000-01-01 00:00:00,semantic_scholar,tumor diagnosis against other brain diseases using t2 mri brain images and cnn binary classifier and dwt,https://www.semanticscholar.org/paper/60b5ffa73ea0852c6f20a625a78ef665f3b713e6,"Purpose: Brain tumors are diagnosed and classified manually and noninvasively by radiologists using Magnetic Resonance Imaging (MRI) data. The risk of misdiagnosis may exist due to human factors such as lack of time, fatigue, and relatively low experience. Deep learning methods have become increasingly important in MRI classification. To improve diagnostic accuracy, researchers emphasize the need to develop Computer-Aided Diagnosis (CAD) computational diagnostics based on artificial intelligence (AI) systems by using deep learning methods such as convolutional neural networks (CNN) and improving the performance of CNN by combining it with other data analysis tools such as wavelet transform. In this study, a novel diagnostic framework based on CNN and DWT data analysis is developed for the diagnosis of glioma tumors in the brain, among other tumors and other diseases, with T2-SWI MRI scans. It is a binary CNN classifier that treats the disease “glioma tumor” as positive and the other pathologies as negative, resulting in a very unbalanced binary problem. The study includes a comparative analysis of a CNN trained with wavelet transform data of MRIs instead of their pixel intensity values in order to demonstrate the increased performance of the CNN and DWT analysis in diagnosing brain gliomas. The results of the proposed CNN architecture are also compared with a deep CNN pre-trained on VGG16 transfer learning network and with the SVM machine learning method using DWT knowledge. Methods: To improve the accuracy of the CNN classifier, the proposed CNN model uses as knowledge the spatial and temporal features extracted by converting the original MRI images to the frequency domain by performing Discrete Wavelet Transformation (DWT), instead of the traditionally used original scans in the form of pixel intensities. Moreover, no pre-processing was applied to the original images. The images used are MRIs of type T2-SWI sequences parallel to the axial plane. Firstly, a compression step is applied for each MRI scan applying DWT up to three levels of decomposition. These data are used to train a 2D CNN in order to classify the scans as showing glioma or not. The proposed CNN model is trained on MRI slices originated from 382 various male and female adult patients, showing healthy and pathological images from a selection of diseases (showing glioma, meningioma, pituitary, necrosis, edema, non-enchasing tumor, hemorrhagic foci, edema, ischemic changes, cystic areas, etc.). The images are provided by the database of the Medical Image Computing and Computer-Assisted Intervention (MICCAI) and the Ischemic Stroke Lesion Segmentation (ISLES) challenges on Brain Tumor Segmentation (BraTS) challenges 2016 and 2017, as well as by the numerous records kept in the public general hospital of Chania, Crete, “Saint George”. Results: The proposed frameworks are experimentally evaluated by examining MRI slices originating from 190 different patients (not included in the training set), of which 56% are showing gliomas by the longest two axes less than 2 cm and 44% are showing other pathological effects or healthy cases. Results show convincing performance when using as information the spatial and temporal features extracted by the original scans. With the proposed CNN model and with data in DWT format, we achieved the following statistic percentages: accuracy 0.97, sensitivity (recall) 1, specificity 0.93, precision 0.95, FNR 0, and FPR 0.07. These numbers are higher for this data format (respectively: accuracy by 6% higher, recall by 11%, specificity by 7%, precision by 5%, FNR by 0.1%, and FPR is the same) than it would be, had we used as input data the intensity values of the MRIs (instead of the DWT analysis of the MRIs). Additionally, our study showed that when our CNN takes into account the TL of the existing network VGG, the performance values are lower, as follows: accuracy 0.87, sensitivity (recall) 0.91, specificity 0.84, precision 0.86, FNR of 0.08, and FPR 0.14. Conclusions: The experimental results show the outperformance of the CNN, which is not based on transfer learning, but is using as information the MRI brain scans decomposed into DWT information instead of the pixel intensity of the original scans. The results are promising for the proposed CNN based on DWT knowledge to serve for binary diagnosis of glioma tumors among other tumors and diseases. Moreover, the SVM learning model using DWT data analysis performs with higher accuracy and sensitivity than using pixel values.",unknown,97
10.1515/labmed-2023-0037,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Laboratory Medicine,2000-01-01 00:00:00,semantic_scholar,statistical learning and big data applications,https://www.semanticscholar.org/paper/a076572adb65a0a810002d01a4831af1e82c9818,"Abstract The amount of data generated in the field of laboratory medicine has grown to an extent that conventional laboratory information systems (LISs) are struggling to manage and analyze this complex, entangled information (“Big Data”). Statistical learning, a generalized framework from machine learning (ML) and artificial intelligence (AI) is predestined for processing “Big Data” and holds the potential to revolutionize the field of laboratory medicine. Personalized medicine may in particular benefit from AI-based systems, especially when coupled with readily available wearables and smartphones which can collect health data from individual patients and offer new, cost-effective access routes to healthcare for patients worldwide. The amount of personal data collected, however, also raises concerns about patient-privacy and calls for clear ethical guidelines for “Big Data” research, including rigorous quality checks of data and algorithms to eliminate underlying bias and enable transparency. Likewise, novel federated privacy-preserving data processing approaches may reduce the need for centralized data storage. Generative AI-systems including large language models such as ChatGPT currently enter the stage to reshape clinical research, clinical decision-support systems, and healthcare delivery. In our opinion, AI-based systems have a tremendous potential to transform laboratory medicine, however, their opportunities should be weighed against the risks carefully. Despite all enthusiasm, we advocate for stringent added-value assessments, just as for any new drug or treatment. Human experts should carefully validate AI-based systems, including patient-privacy protection, to ensure quality, transparency, and public acceptance. In this opinion paper, data prerequisites, recent developments, chances, and limitations of statistical learning approaches are highlighted.",unknown,98
10.3390/app13106138,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Applied Sciences,2000-01-01 00:00:00,semantic_scholar,explainable mortality prediction model for congestive heart failure with nature-based feature selection method,https://www.semanticscholar.org/paper/2c537086af41157b165fef2dbca5ebb767d2d5e9,"A mortality prediction model can be a great tool to assist physicians in decision making in the intensive care unit (ICU) in order to ensure optimal allocation of ICU resources according to the patient’s health conditions. The entire world witnessed a severe ICU patient capacity crisis a few years ago during the COVID-19 pandemic. Various widely utilized machine learning (ML) models in this research field can provide poor performance due to a lack of proper feature selection. Despite the fact that nature-based algorithms in other sectors perform well for feature selection, no comparative study on the performance of nature-based algorithms in feature selection has been conducted in the ICU mortality prediction field. Therefore, in this research, a comparison of the performance of ML models with and without feature selection was performed. In addition, explainable artificial intelligence (AI) was used to examine the contribution of features to the decision-making process. Explainable AI focuses on establishing transparency and traceability for statistical black-box machine learning techniques. Explainable AI is essential in the medical industry to foster public confidence and trust in machine learning model predictions. Three nature-based algorithms, namely the flower pollination algorithm (FPA), particle swarm algorithm (PSO), and genetic algorithm (GA), were used in this study. For the classification job, the most widely used and diversified classifiers from the literature were used, including logistic regression (LR), decision tree (DT) classifier, the gradient boosting (GB) algorithm, and the random forest (RF) algorithm. The Medical Information Mart for Intensive Care III (MIMIC-III) dataset was used to collect data on heart failure patients. On the MIMIC-III dataset, it was discovered that feature selection significantly improved the performance of the described ML models. Without applying any feature selection process on the MIMIC-III heart failure patient dataset, the accuracy of the four mentioned ML models, namely LR, DT, RF, and GB was 69.9%, 82.5%, 90.6%, and 91.0%, respectively, whereas with feature selection in combination with the FPA, the accuracy increased to 71.6%, 84.8%, 92.8%, and 91.1%, respectively, for the same dataset. Again, the FPA showed the highest area under the receiver operating characteristic (AUROC) value of 83.0% with the RF algorithm among all other algorithms utilized in this study. Thus, it can be concluded that the use of feature selection with FPA has a profound impact on the outcome of ML models. Shapley additive explanation (SHAP) was used in this study to interpret the ML models. SHAP was used in this study because it offers mathematical assurances for the precision and consistency of explanations. It is trustworthy and suitable for both local and global explanations. It was found that the features that were selected by SHAP as most important were also most common with the features selected by the FPA. Therefore, we hope that this study will help physicians to predict ICU mortality for heart failure patients with a limited number of features and with high accuracy.",unknown,99
10.61969/jai.1512906,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of AI,2000-01-01 00:00:00,semantic_scholar,"super ai, generative ai, narrow ai and chatbots: an assessment of artificial intelligence technologies for the public sector and public administration",https://www.semanticscholar.org/paper/be2fc5c4d3e7774f9bccdab34ad9f3e5fb7fff51,"Artificial intelligence encompasses a wide range of approaches, methodologies, and techniques aimed at mimicking human intelligence in machines. In recent times, the concepts of Generative Artificial Intelligence (AI), Super AI, and Narrow AI have attracted considerable attention. Undoubtedly, the success of ChatGPT in capturing all attention has played a significant role in this. Artificial intelligence technology has a profound impact on all sectors, and sector representatives are striving to adapt to this technology more quickly. It is projected that artificial intelligence could generate an economic size of 13 trillion American dollars by 2030. Developments in artificial intelligence technologies undoubtedly lead to significant improvements in the functioning of public institutions and access for citizens. Artificial intelligence has the potential to be used in many public services, including security and defense, healthcare services, education, transportation and infrastructure, environmental and natural resource management, law and justice systems, among others. Therefore, evaluating the types of artificial intelligence, Narrow AI applications, and chatbots for public use is seen as highly beneficial from the perspective of public administration and the public sector. In our study, the topics of super artificial intelligence, generative artificial intelligence, narrow artificial intelligence, and chatbots have been extensively evaluated within the context of the public sector and public administration. Utilizing findings from both Turkish and English literature reviews, the importance and potential impacts of artificial intelligence within the public sector, along with current trends, have been comprehensively assessed. This research delves into the concepts of artificial intelligence and its subsets—super AI, generative AI, narrow AI, and chatbots—within the general framework of the public sector. China and the United States are pioneering and leading countries in terms of investment. Although the U.S. stands out in many areas regarding investment, China's integration of artificial intelligence with national strategies and its policies indicate that it may play a more dominant role in the future. There are four main implementation areas of artificial intelligence in the public sector: efficiency and automation, service delivery, data-driven governance, and ethical and regulatory challenges. A review of the literature reveals that the ethical, legal, and social implications of implementing artificial intelligence in the public sector require more careful consideration. The study makes a significant contribution to the field of artificial intelligence discussions in public administration and the public sector, providing a comprehensive assessment of current discussions on artificial intelligence in the literature.",unknown,100
10.1145/3636550,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digit. Gov. Res. Pract.,2000-01-01 00:00:00,semantic_scholar,introduction to the issue on artificial intelligence in the public sector: risks and benefits of ai for governments,https://www.semanticscholar.org/paper/37eb0c2177acf85c74ace80a9365d610523bc64b,"Artificial Intelligence (AI) is increasingly adopted by public sector organizations to provide better public services and to transform their internal processes. AI is now considered a key enabler for digital innovation and transformation in the public sector. However, AI is still relatively a new research area in the field of digital government. The term, AI, captures a wide range of technologies, techniques, and tools such as machine/deep learning, natural language processing, robotics, computer vision, and more recently Generative AI. While these AI technologies afford different applications and benefits in the government context, they also create social, ethical, and legal challenges. These challenges require solutions combining both technical (e.g., data and algorithmic solutions to minimize bias) and institutional (e.g., governance structures and processes) mechanisms. The special issue is a collection of articles that contribute to a better understanding of the issues associated with AI deployment in different areas of government operations. They cover AI applications in the areas of emergency response, policy analysis, public bids, and citizen participation. The contributions also address the challenge of realizing a legal transparency regime for AI in government and the effect of AI in bureaucratic decision-making.",unknown,101
10.48550/arxiv.2401.01291,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,arXiv.org,2000-01-01 00:00:00,semantic_scholar,generative ai is already widespread in the public sector,https://www.semanticscholar.org/paper/6d67a918707f0c20cb847d4d4611b34877ccd5e6,"Generative AI has the potential to transform how public services are delivered by enhancing productivity and reducing time spent on bureaucracy. Furthermore, unlike other types of artificial intelligence, it is a technology that has quickly become widely available for bottom-up adoption: essentially anyone can decide to make use of it in their day to day work. But to what extent is generative AI already in use in the public sector? Our survey of 938 public service professionals within the UK (covering education, health, social work and emergency services) seeks to answer this question. We find that use of generative AI systems is already widespread: 45% of respondents were aware of generative AI usage within their area of work, while 22% actively use a generative AI system. Public sector professionals were positive about both current use of the technology and its potential to enhance their efficiency and reduce bureaucratic workload in the future. For example, those working in the NHS thought that time spent on bureaucracy could drop from 50% to 30% if generative AI was properly exploited, an equivalent of one day per week (an enormous potential impact). Our survey also found a high amount of trust (61%) around generative AI outputs, and a low fear of replacement (16%). While respondents were optimistic overall, areas of concern included feeling like the UK is missing out on opportunities to use AI to improve public services (76%), and only a minority of respondents (32%) felt like there was clear guidance on generative AI usage in their workplaces. In other words, it is clear that generative AI is already transforming the public sector, but uptake is happening in a disorganised fashion without clear guidelines. The UK's public sector urgently needs to develop more systematic methods for taking advantage of the technology.",unknown,102
10.1145/3657054.3657125,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2000-01-01 00:00:00,semantic_scholar,comparative analysis of generative ai risks in the public sector,https://www.semanticscholar.org/paper/cf2a8c29efde5517cc86378ade038d6974930b6a,"The landscape of artificial intelligence (AI) has experienced a monumental shift with the emerging of Generative AI (GenAI), which has demonstrated to be a transformative tool across diverse sectors. GenAI outputs can span various digital formats, including text, images, videos, and audio, generating particular interest in the public sector. The growing interest of governments in integrating GenAI technologies in public sector operations is marked by the creation of emerging governance instruments and the formulation of soft laws, like standards, principles, and guidelines. This study aims to delve into the intricacies and potential risks associated with the deployment of GenAI within government. Through a qualitative content analysis, the research meticulously examines GenAI usage guidelines issued by Australia, Canada, New Zealand, the United Kingdom, and South Korea. The objective is to discern the risks acknowledged by these countries' soft laws and compare them with the risks identified by scholars in the field. The performed comparative analysis across countries suggest that the use of GenAI in the public sector raises common risks such as information leakage, data privacy, security, and concerns over public trust. By elucidating the varied risk perceptions across different national contexts, this study provides theoretical and practical implications related to the risks of GenAI within the public sector. Moreover, it sets a foundation for future research and policy development, ensuring that generative AI is used as a force for good in public governance.",unknown,103
10.1145/3657054.3657129,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2000-01-01 00:00:00,semantic_scholar,tribal knowledge cocreation in generative artificial intelligence systems,https://www.semanticscholar.org/paper/3fbf70a03073fd04f2293044076b5e84afef66f7,"Generative Artificial Intelligence (AI) systems bring innovative ways of information provision and knowledge delivery. In the public sector, generative AI has the potential to decrease bureaucratic discretion in the decision-making process. Increasing reliance on this technology brings challenges of unfair treatment, colonized responses from the system, and data governance. Because of historical interaction, tribal communities are the most underrepresented in policy planning and implementation. Indigenous communities suffer from the neglect of tribal sovereignty by the U.S. federal government and limited accessibility and literacy in the digital world. Generative AI systems exacerbate these challenges with insufficient tribal input. However, the negative impact can be alleviated with digital equity and knowledge cocreation. Digital equity emphasizes the importance of tribal knowledge representation, and knowledge cocreation focuses on the collaboration between Indigenous communities and relevant actors in data governance for generative AI systems. This study proposes two research questions to discuss tribal knowledge cocreation in generative AI systems: (1) what are the biases in the system responses from the tribal perspective? (2) what are the potential resolutions for these problems? The findings from in-depth interviews with tribal members in the U.S. indicate that the insufficient articulation of tribal culture, the lack of crucial tribal historical events, and the inappropriate appellation of tribal nations are the primary drawbacks in the system responses. From the Indigenous perspective, tribal oral traditions, native publications and documents, and collaboration with tribal governments can address the problems of generative AI responses. This study contributes to the theory development of digital equity and knowledge cocreation in tribal generative AI system responses. Policy recommendations and future research agendas are included in this research.",unknown,104
10.1145/3643690.3648235,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,2024 IEEE/ACM International Workshop on Software-Intensive Business (IWSiB),2000-01-01 00:00:00,semantic_scholar,artificial intelligence in the public sector - an agenda for responsible innovation through learning,https://www.semanticscholar.org/paper/b727d48a8a9ce8ac56fc3a164c2a7a4628093550,"The optimism about the benefits of using artificial intelligence to innovate public services is tempered by concerns about its risks, limitations, and disbenefits. Given the rapid changes in the technol-ogy itself, the opportunities and needs for cross-sectional solutions, and the nascency of the field of AI-based innovation, we contend that policy, strategy, and implementation must include feedback loops that enable institutional learning for the entire public sec-tor. The scope of challenges creates and imperative to facilitate learning must transcend functional, organizational, geographic, and national boundaries. We propose a learning agenda that in-cludes 1) alignment of strategy and policy; 2) initial understanding of goals, benefits, disbenefits, limitations, and risks; 3) data sharing across jurisdictions; 4) technical robustness and societal alignment in governmental oversight; 5) convergence of architecture for AI support; and 6) a portfolio approach to selecting and learning from enabling service innovation with AI.",unknown,105
10.1145/3657054.3657086,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2000-01-01 00:00:00,semantic_scholar,public value principles for secure and trusted ai,https://www.semanticscholar.org/paper/d6133f0fb81ce4779a78835b4af01ef3b55bbeeb,"The objective of this paper is to establish the fundamental public value principles that should govern safe and trusted artificial intelligence (AI). Public value is a dynamic concept that encompasses several dimensions. AI itself has evolved quite rapidly in the last few years, especially with the swift escalation of Generative AI. Governments around the world are grappling with how to govern AI, just as technologists ring alarm bells about the future consequences of AI. Our paper extends the debate on AI governance that is focused on ethical values of beneficence to that of economic values of public good. Viewed as a public good, AI use is beyond the control of the creators. Towards this end, the paper examined AI policies in the United States and Europe. We postulate three principles from a public values perspective: (i) ensuring security and privacy of each individual (or entity); (ii) ensuring trust in AI systems is verifiable; and (iii) ensuring fair and balanced AI protocols, wherein the underlying components of data and algorithms are contestable and open to public debate.",unknown,106
10.23977/jaip.2024.070220,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Artificial Intelligence Practice,2000-01-01 00:00:00,semantic_scholar,the analysis of technological ethical issues in generative artificial intelligence,https://www.semanticscholar.org/paper/fa046467bd1082e86d3fb05460386c9a802f2e14,": At present, generative AI represented by ChatGPT has great potential and development prospects in education ecology, academia, media and public domain, etc. However, it also faces many ethical challenges, such as the weakening of human subjectivity, bias and algorithmic discrimination, privacy and data security issues, and impact on the value system. Actively exploring the good recipe for ethical governance of science and technology will help promote agile governance and make science and technology develop for the better.",unknown,107
10.1145/3657054.3657278,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2000-01-01 00:00:00,semantic_scholar,examining public sector ai adoption: mechanisms for ai adoption in the absence of authoritative strategic direction,https://www.semanticscholar.org/paper/b29a09098db3c5e67bba3b50ea8627a27258f65b,"Artificial Intelligence (AI) is recognized to bring great benefits to the organizations that can successfully adopt this emerging technological domain into their operations. This paper examines the impact of governance and strategic direction on AI adoption and diffusion in a public sector setting. By presenting contextual conditions, mechanisms, and outcomes within a large government agency this work contributes to the understanding of how the absence of appropriate governance structures and strategies impact the development and adoption of AI. Findings show that balancing exploitation and exploration in the capillaries of the organization proved crucial to the adoption and diffusion of AI. This manifested itself through three mechanisms, Cross-domain learning, Legal priming, and Ecosystem growth, which enabled the organization to obtain both value creation and value capture.",unknown,108
10.53116/pgaflr.7068,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Public Governance Administration and Finances Law Review,2000-01-01 00:00:00,semantic_scholar,e-government in nigeria: can generative ai serve as a tool for civic engagement?,https://www.semanticscholar.org/paper/a854b495502cf6b6f627bf9ebdc5e8f357272e98,"This paper examines the potential for using generative artificial intelligence (AI) to boost civic participation in Nigeria’s developing e-government ecosystem. Emerging generative technologies like ChatGPT demonstrate intriguing capabilities to make governance more interactive and engaging through conversational interfaces. Thoughtfully implemented AI tools could increase access and understanding of e-government, particularly for underserved groups. However, risks around bias, privacy, security and capability limitations pose challenges for public sector applications. Additionally, Nigeria’s substantial digital divides and defective trust in government institutions hamper e-government participation currently. This paper analyses opportunities and limitations for applying generative AI to advance civic engagement given Nigeria’s unique socio-cultural context. Findings suggest that while AI holds promise, targeted strategies focused on inclusion, accessibility, education and institutional legitimacy building are critical to realise benefits. Cautious optimism, human-centric design and responsible governance frameworks are needed to employ generative systems successfully. If challenges are addressed, AI could open innovative possibilities for energising civic participation. But further research and controlled pilot applications are required to determine optimal implementation.",unknown,109
10.1145/3657054.3657124,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2000-01-01 00:00:00,semantic_scholar,mitigating the risks of generative ai in government through algorithmic governance,https://www.semanticscholar.org/paper/c22e4a437e3d7659c8257a7f4babe63d6fca05ee,"The launch of the generative artificial intelligence (gen AI) application ChatGPT by OpenAI launched artificial intelligence into public discourse and led to a wave of mass uptake of this technology in organizations in the private sector. At the same time, AI is increasingly incorporated into government functions and the public sector. We propose that governments and the public sector can set an example for the responsible use of AI technologies by following the principles of algorithmic governance traditionally recommended to the private sector. Algorithmic governance has traditionally been defined in the literature as governance by algorithms, or how artificial intelligence is used to make governance decisions and affect social ordering. However, we take an alternative approach; instead, we conceptualize algorithmic governance as the governance of algorithms. We begin by summarizing the risks of generative AI use in government, then outline algorithmic governance principles, a step-by-step approach to implementing algorithmic governance into government or public sector projects, opportunities for inter-sector collaboration, and final conclusions.",unknown,110
10.3390/su16177724,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Sustainability,2000-01-01 00:00:00,semantic_scholar,procurement of artificial intelligence systems in uae public sectors: an interpretive structural modeling of critical success factors,https://www.semanticscholar.org/paper/eced54e0bfcf41cbad64157ed5683138764a2d2c,"This study investigates the critical success factors (CSFs) influencing the procurement of artificial intelligence (AI) systems within the United Arab Emirates (UAE) public sector. While AI holds immense potential to enhance public service delivery, its successful integration hinges on critical factors. This research utilizes Interpretive Structural Modeling (ISM) to analyze the CSFs impacting AI procurement within the UAE public sector. Through ISM, a structural model is developed to highlight the interrelationships between these CSFs and their influence on the procurement process, outlining the key elements for successful AI procurement within the UAE public sector. Based on the literature review and expert validation from the UAE public sector, ten CSFs were identified. This study found that clear needs assessment is the most influential CSF, while the long-term value of AI systems or services is the least influential. This study provides policymakers and public sector leaders with valuable insights, enabling them to formulate effective strategies to optimize the procurement process and establish a strong foundation for AI adoption. Finally, this will lead to an improved and more efficient public service delivery in the UAE.",unknown,111
10.1108/tg-01-2024-0006,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Transforming Government: People, Process and Policy",2000-01-01 00:00:00,semantic_scholar,unlocking the power and future potential of generative ai in government transformation,https://www.semanticscholar.org/paper/efca4ec65048fae6d934f3c17dc70aa6803555a8,"
Purpose
This paper aims to investigate whether the implementation of generative artificial intelligence (GAI) impacts government functionality. The study will analyse GAI’s positive attributes across different dimensions to comprehensively understand its value proposition for public organisations. Furthermore, the paper will outline the strategic interventions required to integrate GAI effectively within the organisational context of government transformation.


Design/methodology/approach
This study measures “government functionality” and “GAI implementation” using abstract macro variables as a second-order formative model. It also includes first-order measurable micro-variables to better understand the concept. In addition, the study introduces “organisational context” as a moderating factor to explain the complex dynamics of integrating GAI to improve government functionality. The study proposes a conceptual framework, which was analysed using exploratory data analysis, with primary data collected through questionnaires.


Findings
The study finds a positive correlation between the implementation of GAI and improved government functionality. Furthermore, it found that organisational contextualisation significantly moderates this relationship. All the empirical outcomes align with the prescribed statistical thresholds, concluding that the articulated conceptual framework holds significance.


Research limitations/implications
The study has significant implications for managers, researchers and anyone involved in making, implementing or evaluating decisions related to digital government through GAI. However, the study has limitations, including a limited sample size and contextualisation of the Indian public sector.


Originality/value
The study contributes to existing knowledge by showing that implementing GAI positively correlates with improving government functionality. It further highlights the significance of GAI implementation according to the specific organisational context.
",unknown,112
10.1177/08944393241235175,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Social science computer review,2000-01-01 00:00:00,semantic_scholar,"artificial intelligence, rationalization, and the limits of control in the public sector: the case of tax policy optimization",https://www.semanticscholar.org/paper/9679559aa9ed7c017dcf33f6e07021a83c83b1ce,"In this paper, we first frame the use of artificial intelligence (AI) systems in the public sector as a continuation and intensification of long-standing rationalization and bureaucratization processes. Drawing on Weber, we understand the core of these processes to be the replacement of traditions with instrumental rationality, that is, the most calculable and efficient way of achieving any given policy objective. Second, we demonstrate how much of the criticisms, both among the public and in scholarship, directed towards AI systems spring from well-known tensions at the heart of Weberian rationalization. To illustrate this point, we introduce a thought experiment whereby AI systems are used to optimize tax policy to advance a specific normative end: reducing economic inequality. Our analysis shows that building a machine-like tax system that promotes social and economic equality is possible. However, our analysis also highlights that AI-driven policy optimization (i) comes at the exclusion of other competing political values, (ii) overrides citizens’ sense of their (non-instrumental) obligations to each other, and (iii) undermines the notion of humans as self-determining beings. Third, we observe that contemporary scholarship and advocacy directed towards ensuring that AI systems are legal, ethical, and safe build on and reinforce central assumptions that underpin the process of rationalization, including the modern idea that science can sweep away oppressive systems and replace them with a rule of reason that would rescue humans from moral injustices. That is overly optimistic: science can only provide the means – it cannot dictate the ends. Nonetheless, the use of AI in the public sector can also benefit the institutions and processes of liberal democracies. Most importantly, AI-driven policy optimization demands that normative ends are made explicit and formalized, thereby subjecting them to public scrutiny, deliberation, and debate.",unknown,113
10.1145/3676281,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government: Research and Practice,2000-01-01 00:00:00,semantic_scholar,chatgpt on chatgpt: an exploratory analysis of its performance in the public sector workplace,https://www.semanticscholar.org/paper/4af692d5c75634ff2893f4fedba078456fe94353,"This study explores the impact of Generative Artificial Intelligence (GenAI), in particular, ChatGPT, on the public sector workforce in the United States, focusing on task replacement, assistance potential, and the evolving landscape of skills. Utilizing GPT-4 to evaluate 1,022 core tasks across 51 public sector occupations, we provide an exploratory analysis of the roles susceptible to ChatGPT automation and those in which ChatGPT can augment human efforts. Our findings reveal that while 63% of tasks are resistant to ChatGPT replacement, primarily due to their requirement for physical presence, emotional intelligence, and complex decision-making, tasks that are routine, rule-based, and involving basic content generation show a high potential for automation. The study also identifies key skills that will remain vital, those likely to become obsolete, and new skills that will emerge as essential, highlighting the need for a strategic approach to workforce development in the face of AI advancements. In particular, our findings underscore the growing importance of skills in applying AI technologies and the ability to validate and interpret AI-generated content for humans to remain competitive. We offer insights into public-sector-specific impacts and propose a methodological framework for future research, emphasizing the importance of adapting educational curricula and policies to prepare for an AI-integrated future.",unknown,114
10.28995/2782-2222-2024-1-56-69,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Science and art of management / Bulletin of the Institute of Economics, Management and Law of the Russian State University for the Humanities",2000-01-01 00:00:00,semantic_scholar,"artificial intelligence in russia. history, status, trends and limitations",https://www.semanticscholar.org/paper/c3dcdb50f9979d435a3999fc6a7737b22b368cbe,"A new stage in the introduction of artificial intelligence into everyday life in Russia will be the mass introduction of its technologies and products based on it into the public administration system and the government sector. Today, AI is used in most spheres of public life, but its level of development is still not high enough. In that regard, the issues considered in the publication are modern and relevant and can be used at the stages of development and use of AI. The authors clarified the definition of “artificial intelligence”, analyzed the directions of AI development and identified promising areas of the most accelerated technological development of intelligent systems: generative, voice and language, explicable and peripheral AI with characteristics of the stages up to the present. Two main criteria of AI are analyzed: “strong AI” and “weak AI” and their fundamental differences are considered. The article presents results of a brief analysis of the state and plans for the development of AI in Russia. The level of AI implementation in economic sectors reaches 20% and that is not enough to ensure accelerated economic growth. It is assumed that the introduction of AI should provide an additional 1.2% increase in global GDP by 2030, and our country plans to gain more than 11 trillion rubles from its use by 2025. The future of artificial intelligence in Russia was determined by the President of the country at the St. Petersburg International Forum (PEMF-2023). The announced directions will make it possible to unlock the potential of AI more widely and ensure its mass implementation for the formation of Russia’s sovereignty.",unknown,115
10.55908/sdgs.v12i1.2808,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Law and Sustainable Development,2000-01-01 00:00:00,semantic_scholar,the role of artificial intelligence on the public energy sector performance in the united arab emirates: the mediation role of organizational agility,https://www.semanticscholar.org/paper/99ee0859bf83c91d697887a063c9447758bb4c42,"Purpose: This paper presents an in-depth analysis of the interaction between Artificial Intelligence (AI), organizational agility, and performance within the UAE's public energy sector. It explores the transformative role of AI in this context and the critical importance of organizational agility in determining outcomes in the energy field.
 
Design/methodology/data analysis: The methodology employed in this study is a cross-sectional survey design, with data collected from 245 managers across various public energy companies in the UAE. The survey instrument measured variables pertaining to AI, such as Customer Relationship Management and Cost-efficient IS Operations, and facets of organizational agility, including Responsiveness and Competency, as well as overall Organizational Performance.
 
Findings: The study's findings reveal a significant direct impact of AI on organizational performance, which is further enhanced by the presence of organizational agility. The data indicates that AI's integration within Customer Relationship Management and Cost-efficient IS Operations positively affects performance. Additionally, organizational agility through its components of Responsiveness and Competency serves as a significant intermediary, amplifying the influence of AI on performance.
 
Originality/value: The research is grounded in the Process Theory of Change, the Diffusion of AI Theory, and the Resource-Based View Theory, providing a solid theoretical base for its exploration. It offers a nuanced understanding of the combined impact of AI and organizational agility on the public energy sector's performance.
 
Practical implications: The paper concludes with a conceptual framework that encapsulates these relationships, providing stakeholders with a comprehensive view of the interdependencies between AI, agility, and performance. It stresses the imperative for a strategic embrace of AI and organizational agility to foster resilience, adaptability, and sustainable advancement in the UAE's public energy sector. The insights from this paper guide future strategic orientations, emphasizing the integration of technological innovation with agile organizational practices as a pathway to enhanced performance and sectoral leadership.",unknown,116
10.1145/3643834.3661555,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Conference on Designing Interactive Systems,2000-01-01 00:00:00,semantic_scholar,in whose voice?: examining ai agent representation of people in social interaction through generative speech,https://www.semanticscholar.org/paper/ea4d51c2e652a398de9a648368686584a6c25ed6,"As generative artificial intelligence (genAI) applications gain popularity, there is a dearth of research examining how applications may transform social interactions. One possible application set to transform social interactions is the use of generative speech to power AI agents that can realistically represent people. Our work examines the potential implications of AI agents representing individuals in human conversations (""agent representation"") as a way to begin filling this research gap. We take a multi-method approach, conducting formative interviews with developers, a co-design workshop with designers, a harm analysis among researchers, and interviews with the general public. Both technologists and potential users worry adopting agent representations might harm the quality, trust, and autonomy of human communication. Potential users are particularly concerned that agent representations could undermine the value of social interaction and threaten individuals’ ability to control their image. To avoid such potential consequences, future genAI-powered agents and speech applications should take into account user-defined red lines when considering applying these technologies in social settings.",unknown,117
10.1093/humrep/deae064,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Human Reproduction,2000-01-01 00:00:00,semantic_scholar,generative artificial intelligence to produce high-fidelity blastocyst-stage embryo images,https://www.semanticscholar.org/paper/b23e112a16c201194dce36eefc1dc09cbd1c8658,"Abstract STUDY QUESTION Can generative artificial intelligence (AI) models produce high-fidelity images of human blastocysts? SUMMARY ANSWER Generative AI models exhibit the capability to generate high-fidelity human blastocyst images, thereby providing substantial training datasets crucial for the development of robust AI models. WHAT IS KNOWN ALREADY The integration of AI into IVF procedures holds the potential to enhance objectivity and automate embryo selection for transfer. However, the effectiveness of AI is limited by data scarcity and ethical concerns related to patient data privacy. Generative adversarial networks (GAN) have emerged as a promising approach to alleviate data limitations by generating synthetic data that closely approximate real images. STUDY DESIGN, SIZE, DURATION Blastocyst images were included as training data from a public dataset of time-lapse microscopy (TLM) videos (n = 136). A style-based GAN was fine-tuned as the generative model. PARTICIPANTS/MATERIALS, SETTING, METHODS We curated a total of 972 blastocyst images as training data, where frames were captured within the time window of 110–120 h post-insemination at 1-h intervals from TLM videos. We configured the style-based GAN model with data augmentation (AUG) and pretrained weights (Pretrained-T: with translation equivariance; Pretrained-R: with translation and rotation equivariance) to compare their optimization on image synthesis. We then applied quantitative metrics including Fréchet Inception Distance (FID) and Kernel Inception Distance (KID) to assess the quality and fidelity of the generated images. Subsequently, we evaluated qualitative performance by measuring the intelligence behavior of the model through the visual Turing test. To this end, 60 individuals with diverse backgrounds and expertise in clinical embryology and IVF evaluated the quality of synthetic embryo images. MAIN RESULTS AND THE ROLE OF CHANCE During the training process, we observed consistent improvement of image quality that was measured by FID and KID scores. Pretrained and AUG + Pretrained initiated with remarkably lower FID and KID values compared to both Baseline and AUG + Baseline models. Following 5000 training iterations, the AUG + Pretrained-R model showed the highest performance of the evaluated five configurations with FID and KID scores of 15.2 and 0.004, respectively. Subsequently, we carried out the visual Turing test, such that IVF embryologists, IVF laboratory technicians, and non-experts evaluated the synthetic blastocyst-stage embryo images and obtained similar performance in specificity with marginal differences in accuracy and sensitivity. LIMITATIONS, REASONS FOR CAUTION In this study, we primarily focused the training data on blastocyst images as IVF embryos are primarily assessed in blastocyst stage. However, generation of an array of images in different preimplantation stages offers further insights into the development of preimplantation embryos and IVF success. In addition, we resized training images to a resolution of 256 × 256 pixels to moderate the computational costs of training the style-based GAN models. Further research is needed to involve a more extensive and diverse dataset from the formation of the zygote to the blastocyst stage, e.g. video generation, and the use of improved image resolution to facilitate the development of comprehensive AI algorithms and to produce higher-quality images. WIDER IMPLICATIONS OF THE FINDINGS Generative AI models hold promising potential in generating high-fidelity human blastocyst images, which allows the development of robust AI models as it can provide sufficient training datasets while safeguarding patient data privacy. Additionally, this may help to produce sufficient embryo imaging training data with different (rare) abnormal features, such as embryonic arrest, tripolar cell division to avoid class imbalances and reach to even datasets. Thus, generative models may offer a compelling opportunity to transform embryo selection procedures and substantially enhance IVF outcomes. STUDY FUNDING/COMPETING INTEREST(S) This study was supported by a Horizon 2020 innovation grant (ERIN, grant no. EU952516) and a Horizon Europe grant (NESTOR, grant no. 101120075) of the European Commission to A.S. and M.Z.E., the Estonian Research Council (grant no. PRG1076) to A.S., and the EVA (Erfelijkheid Voortplanting & Aanleg) specialty program (grant no. KP111513) of Maastricht University Medical Centre (MUMC+) to M.Z.E. TRIAL REGISTRATION NUMBER Not applicable.",unknown,118
10.5937/napredak5-52069,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Napredak,2000-01-01 00:00:00,semantic_scholar,the transformative potential of generative artificial intelligence,https://www.semanticscholar.org/paper/2e6e0c741d6c83f12dfac4e02b570c64b9aa0bf7,"This paper analyses the transformative potential of generative artificial intelligence at macro, meso, and micro levels of social and economic structures. The aim is to determine the impact of these technologies on various aspects of society and economy, including business operations and the labour market. The potential of new technologies to increase productivity, transform business models, and create new professional roles has been examined through a comprehensive analysis of data and studies. It has been concluded that generative artificial intelligence can fundamentally change the labour market, globally increase gross domestic product, and improve both the public and private sectors. The paper provides insights into future trends and regulatory and structural changes that are necessary for optimising the application of generative AI.",unknown,119
10.1177/19322968241253568,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Diabetes Science and Technology,2000-01-01 00:00:00,semantic_scholar,building trustworthy generative artificial intelligence for diabetes care and limb preservation: a medical knowledge extraction case.,https://www.semanticscholar.org/paper/6c80fe5c53dba89b8a95334362566194e39a230e,"BACKGROUND
Large language models (LLMs) offer significant potential in medical information extraction but carry risks of generating incorrect information. This study aims to develop and validate a retriever-augmented generation (RAG) model that provides accurate medical knowledge about diabetes and diabetic foot care to laypersons with an eighth-grade literacy level. Improving health literacy through patient education is paramount to addressing the problem of limb loss in the diabetic population. In addition to affecting patient well-being through improved outcomes, improved physician well-being is an important outcome of a self-management model for patient health education.


METHODS
We used an RAG architecture and built a question-and-answer artificial intelligence (AI) model to extract knowledge in response to questions pertaining to diabetes and diabetic foot care. We utilized GPT-4 by OpenAI, with Pinecone as a vector database. The NIH National Standards for Diabetes Self-Management Education served as the basis for our knowledge base. The model's outputs were validated through expert review against established guidelines and literature. Fifty-eight keywords were used to select 295 articles and the model was tested against 175 questions across topics.


RESULTS
The study demonstrated that with appropriate content volume and few-shot learning prompts, the RAG model achieved 98% accuracy, confirming its capability to offer user-friendly and comprehensible medical information.


CONCLUSION
The RAG model represents a promising tool for delivering reliable medical knowledge to the public which can be used for self-education and self-management for diabetes, highlighting the importance of content validation and innovative prompt engineering in AI applications.",unknown,120
10.1145/3657054.3657063,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2000-01-01 00:00:00,semantic_scholar,beyond principles: embedding ethical ai risks in public sector risk management practice,https://www.semanticscholar.org/paper/3488e7b86f16c0d93b7b151eca0d7e7526e6d44b,"Artificial intelligence (AI) adoption by public sector organizations (PSOs) introduces various ethical risks stemming from a lack of integrating human values into AI design. Addressing these ethical risks is a complex collective responsibility among designers, developers, risk experts, and public sector managers. Embedding these risks in existing risk management practices is crucial for responsible AI adoption, as emphasized by the legal requirements of the EU AI Act. However, the responsibility for managing these ethical risks is often unclear. Public sector organizations face unique challenges due to the complex, uncertain, and rapidly evolving nature of AI technologies, further complicating the management of ethical risks. This paper explores using the Three Lines of Defense (TLoD) risk management model to understand and address these ethical risks in public sector AI adoption. The TLoD model structures risk management across three lines: operational management, risk oversight and compliance, and internal audit. This framework helps to distribute and integrate the collective responsibility for ethical AI risk management within public sector organizations, emphasizing alignment and collaboration among different actors. Through an exploratory study involving a survey and semi-structured interviews with professionals responsible for AI-related risk management in Dutch public sector organizations, we assess the TLoD model's usefulness in addressing ethical AI risks. The study examines the challenges and opportunities in applying the TLoD model to manage ethical risks and identifies the potential gaps in responsibility and oversight. The findings suggest that while the TLoD model offers a valuable lens for distributing risk management responsibilities, there are limitations in addressing the emergent and complex nature of ethical risks in AI adoption.",unknown,121
10.1177/09610006241241306,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Librarianship and Information Science,2000-01-01 00:00:00,semantic_scholar,artificial intelligence (ai) applications and usage among the lis professionals of pakistan,https://www.semanticscholar.org/paper/f93400247580dd8f09e01c05c211699c594bae93,"Artificial intelligence (AI) is an important and emerging sub-discipline in information technology that is progressively being implemented in every field. It is gradually being introduced to support new forms of research, discovery, and reuse of library contents in advanced and interesting ways. University libraries have the potential to substantially improve their library services through the implementation of sophisticated AI tools. This study explored the application of AI tools in the university libraries of Pakistan, as well as draw a comparison in the usage of AI tools between public and private sector universities. This is a quantitative study and data is collected through survey methodology. We used purposive sampling to collect the data from 175 university libraries. The collected data was analyzed using a statistical package for social sciences (SPSS-version 22). Findings indicate that while AI-based services are starting to be introduced into university libraries in Pakistan, no university library has implemented the full suite of AI-based tools. Natural language processing, voice searching, and chatbots are the most familiar and popular tools currently used in libraries. However, robotics technology is rarely used with a mean value of (1.62) because of the financial investment and high level of IT skills required. We found that private university libraries are using AI tools more as compared with public sector university libraries. The study concludes with several key recommendations, including closer collaboration between the library and the respective university IT department for technical support and assistance; improved financial support and ICT infrastructure to establish AI technology-based library services; and training development plans for library staff. Insights gained from this study should contribute to the capacity of Pakistani University librarians and their staff to maximize the full potential of AI within their institutions. The research implications are helpful to library leaders and policymakers in building a policy for AI-based technology in their respective university libraries.",unknown,122
10.1108/ils-10-2023-0160,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Information and Learning Sciences,2000-01-01 00:00:00,semantic_scholar,"exploring generative ai literacy in higher education: student adoption, interaction, evaluation and ethical perceptions",https://www.semanticscholar.org/paper/6ff9e22615f12b031caa3ea582f1e28efa408f87,"Purpose
Current knowledge and research on students’ utilization and interaction with generative artificial intelligence (AI) tools in their academic work is limited. This study aims to investigate students’ engagement with these tools.

Design/methodology/approach
This research used survey-based research to investigate generative AI literacy (utilization, interaction, evaluation of output and ethics) among students enrolled in a four-year public university in the southeastern USA. This article focuses on the respondents who have used generative AI (218; 47.2%).

Findings
Most respondents used generative AI to generate ideas for papers, projects or assignments, and they also used AI to assist with their original ideas. Despite their use of AI assistance, most students were critical of generative AI output, and this mindset was reflected in their reported interactions with ChatGPT. Respondents expressed a need for explicit guidance from course syllabi and university policies regarding generative AI’s ethical and appropriate use.

Originality/value
Literature related to generative AI use in higher education specific to ChatGPT is predominantly from educators’ viewpoints. This study provides empirical evidence about how university students report using generative AI in the context of generative AI literacy.
",unknown,123
10.2139/ssrn.4811728,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Social Science Research Network,2000-01-01 00:00:00,semantic_scholar,generative ai and the future of work: augmentation or automation?,https://www.semanticscholar.org/paper/425cef59cbd966ee2aa2c42114021a56c51ffa67,"1 This report examines the potential impact of Generative artificial intelligence (AI) systems, such as ChatGPT, on the future of work and, by implication, on productivity. It argues that although Generative AI is powerful, it has significant limitations and risks that require humans to remain “in the loop” not only to prevent systems from going off the rails, but to capture value. Rather than taking a deterministic view that artificial intelligence (AI) will inevitably destroy jobs, the article suggests that an analysis should start with how firms can strategically deploy these tools to gain an advantage. It asks whether “augmentation” or “simplistic automation” lies ahead. Our objective is to move beyond hype and despair. 2 The existing digital infrastructure has enabled AI to be adopted quickly. However, projections based solely on automating existing tasks fail to capture the complex reorganizations that are likely to happen. Firms in sectors such as professional services, materials, and pharmaceuticals seem to have particular exposure to the use of Generative AI tools. Adaptations will vary across contexts and depend greatly on who controls the decisions about deployment. Maintaining the centrality of humans is likely to prove crucial—in training systems, curating data, and assessing outputs. One question is which business strategies and public policies encourage that engagement and make it possible. Although AI regulation debates matter, promoting social prosperity depends heavily on directly shaping the trajectory of the development and use of AI. This requires influencing the constraints and the incentives that firms face, as well as the strategic mindsets of decision makers. Which groups are engaged in the discussions and debates is of vital importance. The article recommends that, beyond the traditional policy proposals, an independent public-interest consultancy needs to be established in order to design creative business strategies that augment workers in a manner that will support, rather than hinder, social prosperity. Ultimately, avoiding a dystopian scenario might hinge on fostering new norms in which human capabilities remain essential.",unknown,124
10.1108/tg-02-2024-0038,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Transforming Government: People, Process and Policy",2000-01-01 00:00:00,semantic_scholar,the use of ai in government and its risks: lessons from the private sector,https://www.semanticscholar.org/paper/2ef758da3f0043f05fb4e89a6c65c610cbd1d43c,"
Purpose
This study aims to understand the perceived emotions of human–artificial intelligence (AI) interactions in the private sector. Moreover, this research discusses the transferability of these lessons to the public sector.


Design/methodology/approach
This research analysed the comments posted between June 2022 and June 2023 in the global open Reddit online community. A data mining approach was conducted, including a sentiment analysis technique and a qualitative approach.


Findings
The results show a prevalence of positive emotions. In addition, a pertinent percentage of negative emotions were found, such as hate, anger and frustration, due to human–AI interactions.


Practical implications
The insights from human–AI interactions in the private sector can be transferred to the governmental sector to leverage organisational performance, governmental decision-making, public service delivery and the creation of economic and social value.


Originality/value
Beyond the positive impacts of AI in government strategies, implementing AI can elicit negative emotions in users and potentially negatively impact the brand of private and government organisations. To the best of the authors’ knowledge, this is the first research bridging the gap by identifying the predominant negative emotions after a human–AI interaction.
",unknown,126
10.1055/s-0044-1788285,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Sleep Science,2000-01-01 00:00:00,semantic_scholar,"artificial intelligence, the production of scientific texts, and the implications for sleep science: exploring emerging paradigms and perspectives",https://www.semanticscholar.org/paper/fbe965783464000e68863e0a088eaf77444afe5f,"The emergence of artificial intelligence (AI) has revolutionized many fields, including natural language processing, and marks a potential paradigm shift in the way we evaluate knowledge. One significant innovation in this area is ChatGPT, a large language model based on the GPT-3.5 architecture created by OpenAI, with one of its main aims being to aid in general text writing, including scientific texts. Here, we highlight the challenges and opportunities related to using generative AI and discuss both the benefits of its use, such as saving time by streamlining the writing process and reducing the amount of time spent on mundane tasks, and the potential drawbacks, including concerns regarding the accuracy and reliability of the information generated and its ethical use. In respect of both education and the writing of scientific texts, clear rules and objectives and institutional principles must be established for the use of AI. We also consider the positive and negative effects of the use of AI technologies on interpersonal interactions and behavior, and, as sleep scientists, its potential impacts on sleep. Striking a balance between the benefits and potential drawbacks of integrating AI into society demands ongoing research by experts, the wide dissemination of the scientific results, as well as continued public discourse on the subject.",unknown,127
10.1108/jet-05-2024-0030,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Enabling Technologies,2000-01-01 00:00:00,semantic_scholar,the application of gen-ai and creativity in the context of public education in frontier environments,https://www.semanticscholar.org/paper/d1718bda3719edf47d7e8b7017db37a1107e6c9e,"PurposeThe purpose of this article is to demonstrate how the creativity technique SCAMPER and generative Artificial Intelligence (Gen-AI) are linked in the formative process for the solution of business problems by groups of students from low socio-economic levels of a public university in the city of San José de Cucuta, Colombia.Design/methodology/approachAn analysis of the contributions of generative artificial intelligence was developed and the knowledge gaps related to advanced artificial intelligence-based linguistic models in the education sector were mentioned. Subsequently, views on the Colombian context of science, technology and innovation were developed. Finally, the experience in the application of teaching-learning strategies through the use of Open AI’s creativity technique and ChatGPT was highlighted.FindingsThe findings highlight the complementarity of generative artificial intelligence and the SCAMPER creativity technique in the development of innovation capabilities. While human creativity highlights emotional aspects. Artificial intelligence consolidates procedural aspects and ideas focused on the primary activities of the value chain.Practical implicationsThe implementation of the hybrid model in the classroom can lead to the development of new capabilities by marginalized groups immersed in the educational system. The potential positive impact of Gen-AI and human creativity will be reflected in the optimization of response times and the search for solutions to problems in different environments.Originality/valueThis opinion article highlights the implementation of AI in a Higher Education Institution located in the frontier zone of San José de Cucuta, Colombia. In addition, it involves actors of the educational system whose economic income is low. Finally, it highlights the positive impact of the integration of creativity techniques and the use of generative artificial intelligence in the classroom, highlighting the use of hybrid models (Man-Machine).",unknown,128
10.1108/tg-06-2024-0131,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Transforming Government: People, Process and Policy",2000-01-01 00:00:00,semantic_scholar,"artificial intelligence and decision-making in government functions: opportunities, challenges and future research",https://www.semanticscholar.org/paper/305eeffe0bee53fa400a8ecad024f1c18c1280b8,"Purpose
Artificial intelligence (AI) has received much attention due to its promethean-like powers to transform the management and delivery of public sector services. Due to the proliferation of research articles in this context, research to date is fragmented into research streams based on different types of AI technologies or a specific government function of the public sector (e.g. health, education). The purpose of this study is to synthesize this literature, identify challenges and opportunities, and offer a research agenda that guides future inquiry.

Design/methodology/approach
This paper aggregates this fragmented body of knowledge by conducting a systematic literature review of AI research in public sector organisations in the Chartered Association of Business Schools (CABS)-ranked journals between 2012 and 2023.

Findings
The search strategy resulted in the retrieval of 2,870 papers, of which 61 were identified as primary papers relevant to this research. These primary papers are mapped to the ten classifications of the functions of government as classified by the Organisation for Economic Co-operation and Development (OECD), and the reported challenges and benefits aggregated.

Originality/value
This study advances knowledge by providing a state-of-the-art of AI research based the OECD classifications of government functions, reporting of claimed benefits and challenges and providing a research agenda for future research.
",unknown,129
10.22495/cocv21i1art5,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Corporate Ownership and Control,2000-01-01 00:00:00,semantic_scholar,aligning artificial intelligence with ethical accountancy: a global perspective on emerging frameworks,https://www.semanticscholar.org/paper/b7aafd56e7263ad9b8b693d6009742c84633d0db,"This study meticulously examines the integration of artificial intelligence (AI) into the accounting sector, revealing transformative opportunities alongside emerging ethical challenges. Drawing inspiration from established principles of the American Institute of Certified Public Accountants (AICPA) Code of Professional Conduct (AICPA, 2016), an innovative Accounting Framework for AI Ethics (AFAIE) is introduced. This framework aims to provide a tailored approach that ensures that the adoption of AI technologies aligns with the fundamental professional values of trust and integrity. It aims to address the concerns and potential risks associated with the use of AI and establish guidelines that promote accountability and transparency in the development and deployment of AI systems. The essence of this research is underscored by the advocacy for resilient ethical paradigms that are instrumental in navigating the complexities introduced by AI in accounting. Emphasizing a global perspective, this study advocates universal ethical guidelines, ensuring adaptability to specific regional and professional contexts (Association of Chartered Certified Accountants [ACCA], 2016; Bertucci et al., 2021). This synthesis of technology and ethics aims to foster an environment in which innovation thrives alongside steadfast adherence to professional integrity and responsibility.",unknown,131
10.1108/gkmc-02-2024-0107,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Global Knowledge Memory and Communication,2000-01-01 00:00:00,semantic_scholar,identifying librarians’ readiness to leverage artificial intelligence for sustainable competence development and smart library services: an empirical investigation from universities’ librarians,https://www.semanticscholar.org/paper/6c1fb57c5d6c6cffcebbe2b31f9a943adb716a44,"
Purpose
This study aims to identify the librarians’ readiness to leverage artificial intelligence for sustainable competence development and smart library services.


Design/methodology/approach
This study used a quantitative research design for addressing the objectives. The population consisted of librarians from the public and private sector universities of Pakistan. The data were analyzed by using Smart PLS software.


Findings
The analysis consisted of two major parts: first the assessment of measurement model and second the structural equation modeling analysis. A significant positive impact of AI adoption was found on the implementation smart library services. Findings revealed that behavioral intention motivated librarians to adopt AI tools in university libraries for the delivery of smart library services.


Research limitations/implications
We applied quantitative method to carry out the study while future authors may conduct a systematic literature review on the same topic for offering a broader outlook.


Practical implications
It has provided practical contributions by providing a baseline for management bodies to construct policies for the successful adoption of AI in libraries for sustainable competence development of practicing librarians and implementation of smart library services.


Social implications
The study has social implications too as AI integrated library services prove fruitful for society and digitally skilled librarians play a vital role for the promotion of reading and research culture in society.


Originality/value
To the best of the authors’ knowledge, this is the first study on librarians’ readiness to leverage artificial intelligence for the enhancement of digital literacy skills, sustainable competence development and smart library services in the context of Pakistan.
",unknown,132
10.1017/dap.2024.13,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Data & Policy,2000-01-01 00:00:00,semantic_scholar,exploring citizens’ stances on ai in public services: a social contract perspective,https://www.semanticscholar.org/paper/58b7ea14464538f5a7ea31df0e518df1c3efd0e6,"Abstract This paper explores citizens’ stances toward the use of artificial intelligence (AI) in public services in Norway. Utilizing a social contract perspective, the study analyzes the government–citizen relationship at macro, meso, and micro levels. A prototype of an AI-enabled public welfare service was designed and presented to 20 participants who were interviewed to investigate their stances on the described AI use. We found a generally positive attitude and identified three factors contributing to this: (a) the high level of trust in government (macro level); (b) the balanced value proposition between individual and collective needs (meso level); and (c) the reassurance provided by having humans in the loop and providing transparency into processes, data, and model’s logic (microlevel). The findings provide valuable insights into citizens’ stances for socially responsible AI in public services. These insights can inform policy and guide the design and implementation of AI systems in the public sector by foregrounding the government–citizen relationship.",unknown,133
10.31516/2410-5325.083.02,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Culture of Ukraine,2000-01-01 00:00:00,semantic_scholar,artificial intelligence through the prism of thematic research on researchgate web portal,https://www.semanticscholar.org/paper/2ca8e568a7c9cf3ad7368fd8b6b6a327f0eb35f1,"Different instances of generative artificial intelligence (GenAI) in a short time have made a significant impact on the world’s economic and political scene. Before October 2022, processes of automatization and robotization of manufacturing didn’t have an immediate connection with artificial intelligence in the minds of most people. But mass and sudden infiltration of GenAI into the everyday life of many people around the world caused an immediate reaction from scientists, public figures, politicians, managers and heads of whole sectors of the economy. Thousands of scientific articles on related topics were published in the last two years: everyone at once started talking about fantastic possibilities, and also threats, which this new technology can usher in for our societies. Thus, for public thinking GenAI finally linked automatization with artificial intelligence. 
This paper provides an analysis of problems and prospects, through which scientists are trying to understand different societal processes linked to adoption of AI. For this purpose, the author of this paper analyzed the contents of papers published on this topic on the ResearchGate web portal in 2023, with the choice of the source for representative material motivated by scientific credibility of ResearchGate combined with its wide reach. Ten most popular articles were specifically targeted for final analysis, through which societal trends brought on by AI adoption were described. Although a selective meta-analysis of articles published during the first year after ChatGPT release can’t provide a full understanding of AI potential and possible threats to society, the conclusion can still be reached that a cardinal shift in society’s attitudes towards its own future is required. 
From the ten articles analyzed, four are related to changes required to the education system, four are about AI’s influence on the labour market, one article talks about the possibility of AI-human competition and one is about the AI potential in agriculture. Most articles mention the need for legislative changes in terms of labor protection and education reforms in-line with new digital reality.",unknown,134
10.59476/ilpmt2024.100-106,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Innovations in Publishing, Printing and Multimedia Technologies",2000-01-01 00:00:00,semantic_scholar,criteria for selecting artificial intelligence tools,https://www.semanticscholar.org/paper/2d88f0f218151f6f38f3888024ccff9e8c3ed0f7,"Artificial Intelligence (AI) represents a transformative force across numerous sectors, from healthcare and finance to automotive and public services. The selection and deployment of AI tools are critical to leveraging this technology’s potential while adhering to ethical standards, regulatory compliance, and ensuring societal benefit. The European Union (EU) has been at the forefront of establishing frameworks and criteria to guide the development, deployment, and selection of AI systems to foster innovation while protecting citizens’ rights and societal values. The EU’s proactive stance in establishing these criteria aims to balance innovation with ethical considerations and societal welfare, setting a benchmark for responsible AI development and deployment globally. The aim of the article is to present general criteria for the selection of artificial intelligence tools, as well as those specific to the field of publishing. The research was carried out based on the analysis of scientific and other sources. The results of the study can be useful for organizations and individuals that must be interested in selecting and using the right AI tools.",unknown,135
10.26883/2010.241.5985,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Education and Technologies Journal,2000-01-01 00:00:00,semantic_scholar,"world’s first law for artificial intelligence. legal, ethical and economic aspects",https://www.semanticscholar.org/paper/34270a7b96d494461465e72c55c1cc84f6417e30,"The European Parliament has enacted the first comprehensive AI law, garnering widespread public interest and varied reactions. This regulation addresses the specific challenges posed by AI systems, aiming to ensure their safety, legal compliance, and alignment with EU fundamental rights and values. Key goals include fostering legal certainty to boost AI investments and innovations, enhancing governance, and preventing market fragmentation. The law employs a risk-based framework, categorizing AI systems into four risk levels: unacceptable, high, limited, and minimal. Unacceptable risk systems, like real-time biometric identification in public spaces, are banned. High-risk systems, such as those used in critical infrastructure or employment, require stringent oversight. Limited risk systems must maintain transparency, informing users when they are interacting with AI. Minimal risk systems, including AI spam filters, are allowed with minimal regulation but still require transparency. Significantly, the law emphasizes ethical AI development, particularly regarding copyright issues in generative AI. It mandates compliance with copyright laws, transparency about training data, and robust cybersecurity measures. Non-compliant companies face severe fines, up to 35 million euros or 7% of annual global revenue. The EU’s AI law sets a pioneering regulatory standard, potentially influencing global AI governance. It aims to balance protecting citizens’ rights with fostering a competitive and innovative AI market in Europe, potentially serving as a model for other democratic nations.",unknown,136
10.17163/uni.n41.2024.01,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Universitas,2000-01-01 00:00:00,semantic_scholar,comunicación de moda e inteligencia artificial: el caso de neural fashion ai,https://www.semanticscholar.org/paper/2b079c206e1db7415950d1f2209864fe0dfa5dff,"Artificial intelligence is presented to society as a revolutionary tool capable of generating a change asunique as the democratization of Internet access at the beginning of the 21st Century. The different applications of AI are facilitating the development of marketing and communication strategies adapted to the needs of the public and the establishment of strong relationships with them. One of the most dynamic consumer markets is fashion communication, which is why we decided to delimit the applications of AI to brands in this sector. First goal was to identify the main resources and applications of AI that are being used to communicatewith the different stakeholders of fashion companies, particularly with the final consumer. Second objective was to recognize benefits and positive aspects along with the brakes and barriers that the application of this technology represents for the communication strategies of fashion brands. Thirdly, a case studyis offered to help academics and professionals understand how the fashion sector is receiving the helpof generative AI in the creation of campaigns. Through a combination of qualitative methods including3 Delphi interviews, a hemerographic research of professional publications and the Neural Fashion AI case study, the capacity of AI to point out a differentiating factor in the market that has to do withsustainability, product customization and optimization of company resources has been demonstrated. The main results highlight the contribution that AI makes to the efficiency of processes and to the achievement of brand objectives (customer satisfaction, loyalty, strengthening of positioning and brand image), expansion into new marketsand audiences, or the creation of innovative, impactful and attractive content.",unknown,137
10.1111/puar.13824,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,PAR. Public Administration Review,2000-01-01 00:00:00,semantic_scholar,“chat‐up”: the role of competition in street‐level bureaucrats' willingness to break technological rules and use generative pre‐trained transformers (gpts),https://www.semanticscholar.org/paper/ef59842a40f2f88b03804608dcc2a8a6fb0f120d,"Organizations worldwide are concerned about workers using generative pretrained transformers (GPTs), which can generate human‐like text in seconds at work. These organizations are setting rules on how and when to use GPTs. This article focuses on street‐level bureaucrats' (SLBs) intentions to use GPTs even if their public organization does not allow its use (tech rule‐breaking). Based on a mixed‐methods exploratory design, using focus groups (N = 14) and a survey experiment (N = 279), we demonstrate that SLBs intend to break the rules and use GPTs when their competitors from the private sector have access to artificial intelligence (AI) tools. We discuss these findings in the context of hybrid forms of public management and the Promethean moment of GPTs.",unknown,138
10.3389/fneur.2024.1418060,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Frontiers in Neurology,2000-01-01 00:00:00,semantic_scholar,artificial intelligence in ischemic stroke images: current applications and future directions,https://www.semanticscholar.org/paper/3553135f337934ec6297ec14473eb5969f799e71,"This paper reviews the current research progress in the application of Artificial Intelligence (AI) based on ischemic stroke imaging, analyzes the main challenges, and explores future research directions. This study emphasizes the application of AI in areas such as automatic segmentation of infarct areas, detection of large vessel occlusion, prediction of stroke outcomes, assessment of hemorrhagic transformation risk, forecasting of recurrent ischemic stroke risk, and automatic grading of collateral circulation. The research indicates that Machine Learning (ML) and Deep Learning (DL) technologies have tremendous potential for improving diagnostic accuracy, accelerating disease identification, and predicting disease progression and treatment responses. However, the clinical application of these technologies still faces challenges such as limitations in data volume, model interpretability, and the need for real-time monitoring and updating. Additionally, this paper discusses the prospects of applying large language models, such as the transformer architecture, in ischemic stroke imaging analysis, emphasizing the importance of establishing large public databases and the need for future research to focus on the interpretability of algorithms and the comprehensiveness of clinical decision support. Overall, AI has significant application value in the management of ischemic stroke; however, existing technological and practical challenges must be overcome to achieve its widespread application in clinical practice.",unknown,139
10.2139/ssrn.4801289,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Comput. Methods Programs Biomed.,2000-01-01 00:00:00,semantic_scholar,atommic: an advanced toolbox for multitask medical imaging consistency to facilitate artificial intelligence applications from acquisition to analysis in magnetic resonance imaging,https://www.semanticscholar.org/paper/fc4850873eba04ef7a5d5388794ea3e145aec98e,"BACKGROUND AND OBJECTIVES
Artificial intelligence (AI) is revolutionizing Magnetic Resonance Imaging (MRI) along the acquisition and processing chain. Advanced AI frameworks have been applied in various successive tasks, such as image reconstruction, quantitative parameter map estimation, and image segmentation. However, existing frameworks are often designed to perform tasks independently of each other or are focused on specific models or single datasets, limiting generalization. This work introduces the Advanced Toolbox for Multitask Medical Imaging Consistency (ATOMMIC), a novel open-source toolbox that streamlines AI applications for accelerated MRI reconstruction and analysis. ATOMMIC implements several tasks using deep learning (DL) models and enables MultiTask Learning (MTL) to perform related tasks in an integrated manner, targeting generalization in the MRI domain.


METHODS
We conducted a comprehensive literature review and analyzed 12,479 GitHub repositories to assess the current landscape of AI frameworks for MRI. Subsequently, we demonstrate how ATOMMIC standardizes workflows and improves data interoperability, enabling effective benchmarking of various DL models across MRI tasks and datasets. To showcase ATOMMIC's capabilities, we evaluated twenty-five DL models on eight publicly available datasets, focusing on accelerated MRI reconstruction, segmentation, quantitative parameter map estimation, and joint accelerated MRI reconstruction and segmentation using MTL.


RESULTS
ATOMMIC's high-performance training and testing capabilities, utilizing multiple GPUs and mixed precision support, enable efficient benchmarking of multiple models across various tasks. The framework's modular architecture implements each task through a collection of data loaders, models, loss functions, evaluation metrics, and pre-processing transformations, facilitating seamless integration of new tasks, datasets, and models. Our findings demonstrate that ATOMMIC supports MTL for multiple MRI tasks with harmonized complex-valued and real-valued data support while maintaining active development and documentation. Task-specific evaluations demonstrate that physics-based models outperform other approaches in reconstructing highly accelerated acquisitions. These high-quality reconstruction models also show superior accuracy in estimating quantitative parameter maps. Furthermore, when combining high-performing reconstruction models with robust segmentation networks through MTL, performance is improved in both tasks.


CONCLUSIONS
ATOMMIC advances MRI reconstruction and analysis by leveraging MTL and ensuring consistency across tasks, models, and datasets. This comprehensive framework serves as a versatile platform for researchers to use existing AI methods and develop new approaches in medical imaging.",unknown,140
10.1136/fmch-2023-002583,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Family Medicine and Community Health,2000-01-01 00:00:00,semantic_scholar,"assessing prognosis in depression: comparing perspectives of ai models, mental health professionals and the general public",https://www.semanticscholar.org/paper/f1d124ae42e5f5fdc28e430f9ffe86c2adb204fb,"Background Artificial intelligence (AI) has rapidly permeated various sectors, including healthcare, highlighting its potential to facilitate mental health assessments. This study explores the underexplored domain of AI’s role in evaluating prognosis and long-term outcomes in depressive disorders, offering insights into how AI large language models (LLMs) compare with human perspectives. Methods Using case vignettes, we conducted a comparative analysis involving different LLMs (ChatGPT-3.5, ChatGPT-4, Claude and Bard), mental health professionals (general practitioners, psychiatrists, clinical psychologists and mental health nurses), and the general public that reported previously. We evaluate the LLMs ability to generate prognosis, anticipated outcomes with and without professional intervention, and envisioned long-term positive and negative consequences for individuals with depression. Results In most of the examined cases, the four LLMs consistently identified depression as the primary diagnosis and recommended a combined treatment of psychotherapy and antidepressant medication. ChatGPT-3.5 exhibited a significantly pessimistic prognosis distinct from other LLMs, professionals and the public. ChatGPT-4, Claude and Bard aligned closely with mental health professionals and the general public perspectives, all of whom anticipated no improvement or worsening without professional help. Regarding long-term outcomes, ChatGPT 3.5, Claude and Bard consistently projected significantly fewer negative long-term consequences of treatment than ChatGPT-4. Conclusions This study underscores the potential of AI to complement the expertise of mental health professionals and promote a collaborative paradigm in mental healthcare. The observation that three of the four LLMs closely mirrored the anticipations of mental health experts in scenarios involving treatment underscores the technology’s prospective value in offering professional clinical forecasts. The pessimistic outlook presented by ChatGPT 3.5 is concerning, as it could potentially diminish patients’ drive to initiate or continue depression therapy. In summary, although LLMs show potential in enhancing healthcare services, their utilisation requires thorough verification and a seamless integration with human judgement and skills.",unknown,141
10.1109/meco62516.2024.10577936,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Mediterranean Conference on Embedded Computing,2000-01-01 00:00:00,semantic_scholar,"opportunities of gen ai in the banking industry with regards to the ai act, gdpr, data act and dora",https://www.semanticscholar.org/paper/a738d0916ccf3b8970dabe488a3de88f6d2dd4af,"Generative Artificial Intelligence (Gen AI) stands at the forefront of the banking sector's technological revolution, promising enhancements in decision-making, risk management, and customer interaction. This paper examines Gen AI's potential to inject innovation and efficiency into banking services, with an estimated value addition of up to $340 billion annually. Grounded in advancements in NLP through Transformer architecture and evolving GPT models, Gen AI's applications in the banking industry are extensive. They range from personalizing customer service with AI-driven chatbots to revolutionizing credit scoring and trading strategies. However, alongside these opportunities, the paper addresses the significant challenges of regulatory compliance, ethical data usage, and the technical integration of AI systems. With the impending release of the EU's AI Act and existing GDPR and DORA, financial institutions must strategize to align with new standards while harnessing Gen AI's capabilities for process optimization and enhanced service delivery. The role of international standards such as ISO/IEC 42001:2023, ISO 31000:2018, ISO/IEC 23894:2023, NIST AI 600-1 and ISO/IEC 23053:2022 is considered to be beneficial in establishing a common framework for managing AI systems, ensuring data integrity and promoting transparency. By adopting these standards, banks can facilitate compliance across various jurisdictions, enhancing operational consistency and reliability – but certain significant limitations in addressing specific regulatory requirements must be taken into account. The paper concludes that Gen AI's future in banking will be transformative, driven by the industry's need to balance technological innovation with ethical and regulatory requirements and process standardization, which will lead to more transparent, personalized and efficient banking services.",unknown,142
10.3390/app14188259,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Applied Sciences,2000-01-01 00:00:00,semantic_scholar,"enhancing e-government services through state-of-the-art, modular, and reproducible architecture over large language models",https://www.semanticscholar.org/paper/282d499f97460e32efae0b0107480d75872bbc5d,"Integrating Large Language Models (LLMs) into e-government applications has the potential to improve public service delivery through advanced data processing and automation. This paper explores critical aspects of a modular and reproducible architecture based on Retrieval-Augmented Generation (RAG) for deploying LLM-based assistants within e-government systems. By examining current practices and challenges, we propose a framework ensuring that Artificial Intelligence (AI) systems are modular and reproducible, essential for maintaining scalability, transparency, and ethical standards. Our approach utilizing Haystack demonstrates a complete multi-agent Generative AI (GAI) virtual assistant that facilitates scalability and reproducibility by allowing individual components to be independently scaled. This research focuses on a comprehensive review of the existing literature and presents case study examples to demonstrate how such an architecture can enhance public service operations. This framework provides a valuable case study for researchers, policymakers, and practitioners interested in exploring the integration of advanced computational linguistics and LLMs into e-government services, although it could benefit from further empirical validation.",unknown,143
10.1109/saci60582.2024.10619874,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Symposium on Applied Computational Intelligence and Informatics,2000-01-01 00:00:00,semantic_scholar,ai-based text to scene generation as part of a pandemic compliant infrastructure,https://www.semanticscholar.org/paper/1bdabbddc8e809c8ddd1ce701c5c3feff1cdfdde,"After the 2020–2022 COVID-19 pandemic, it is observed that a responsive medical infrastructure and damage control techniques, including application of technology, have become more important than ever before. Many such technologies, including the Internet of Things (IoT), and artificial intelligence (AI)-aided decision-making have become relevant. Within such a framework, text-to-scene generation through the incorporation of Artificial Intelligence (AI) becomes a critical element of a pandemic-compliant infrastructure design. Our suggested architecture uses AI algorithms to create sceneries that are dynamically adjusted for pandemic compliance. These sceneries cover a wide range of settings, including as public locations, medical institutions, virtual meeting rooms, and more. Our approach makes use of generator and discriminator blocks as part of a Generative Adversarial Network (GAN) which is further enhanced by several attention layers to generate scenes from test descriptions. The output of the proposed approach ensures that scene creation regarding safety precautions like mask wearing, social distance, and sanitization procedures are included in generated output.",unknown,144
10.1093/elt/ccae031,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,ELT Journal,2000-01-01 00:00:00,semantic_scholar,can novice teachers detect ai-generated texts in efl writing?,https://www.semanticscholar.org/paper/0013aecf813400174158e4f012918c5408f90962,"
 The introduction of generative artificial intelligence (AI) to the wider public could have a huge impact on EFL learning and teaching. Researchers have voiced concerns that learners might lean too much on technology. Previous studies have investigated the use of AI tools in L2 writing with various populations and found that it was difficult for teachers to detect use of AI and that teachers mainly relied on linguistic strategies to detect AI-generated texts. This paper reports on a qualitative study that investigated whether novice English teachers were able to detect AI-generated writing and which strategies they used to do this. The results show that some novice teachers are quite good at detecting AI-generated texts, while others proved to have more difficulties. The teachers used both linguistic and content-related strategies to detect AI-generated writing. The results point towards the value of including this topic in teaching methodology courses in (initial) teacher training programmes.",unknown,145
10.1215/2834703x-11205231,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Critical AI,2000-01-01 00:00:00,semantic_scholar,the fumes of ai,https://www.semanticscholar.org/paper/041b29a4f27cbbff34140e58f2f0f32a61d9207b,"
 With the emergence of generative artificial intelligence (GenAI), it is increasingly clear that the environmental impacts of these technologies are significant, and worth exposing to the public. This article discusses the environmental impacts of generative artificial intelligence and the political underpinnings of extractivist technologies such as cloud companies. It highlights the centralized system of power that demands subservience to its foundational values despite being touted as the most environmentally friendly cloud infrastructure globally.",unknown,146
10.5206/cjils-rcsib.v47i2.17699,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Canadian journal of information and library science,2000-01-01 00:00:00,semantic_scholar,the pivotal role of libraries in sustainable ai development,https://www.semanticscholar.org/paper/cd18416bc8705182cdb03c82c30fa95721ca57e9,"Artificial Intelligence (AI) has gained exceptional public and media coverage since the launch of the ChatGPT platform, a generative conversational intelligence, in November 2022. Nevertheless, AI is already an integral part of our digital daily lives, as we navigate through social networks, use our GPS, or consult recommendations on e-commerce websites. Due to its pervasive influence across all sectors of our societies, AI is gradually becoming a pivotal subject in terms of regulation, societal direction, and legislation. As early as 2021, UNESCO published a report presenting avenues for ethical considerations in AI. In June 2023, the European Union also established a regulatory framework outlining requirements and obligations for AI usage. As a digital manifestation and given the ""new ways in which its use influences human thinking, interaction and decision-making and affects education, human, social and natural sciences, culture, and communication and information"" (UNESCO. General Conference, 41st, 2021), public libraries have a role to play in enabling residents within their communities to grasp this technology. Their role is all the more significant as AI generates concerns and distrust (Gillath et al., 2021) among populations when ""libraries also continue to enjoy a high level of trust and appreciation in most of their communities"" (Arlitsch \& Newell, 2017). Understanding AI thus constitutes a new cornerstone for accessing the necessary information to advance sustainable development, as outlined in the Lyon Declaration (2014). Moreover, comprehension of AI aligns with the ethical concerns articulated by UNESCO in terms of explainability and transparency and aligns with several Sustainable Development Goals (SDGs) of the 2030 Agenda. These goals include quality education (4), industry, innovation and infrastructure (9), reduced inequalities (10), sustainable cities and communities (11), as well as responsible consumption and production (12). The role of public libraries in advancing the goals of the 2030 Agenda is beyond dispute (IFLA, 2016), and various digital literacies are already integral to their actions. As AI is predominantly developed by global economic giants and permeates all of our practices, ""Shouldn’t [libraries] be the bastions of information literacy and information privacy in an AI world?"" (Cox et al., 2018). Thus, to what extent can public libraries take on this subject to promote and offer relevant literacy? For this study, we will conduct a cross-analysis among three European countries—Spain, France, and Italy—to provide insights into the diverse ways in which AI influences professional practices. Through a literature review and semi-structured interviews, the objective is to delineate the challenges of Artificial Intelligence within the framework of the Agenda 2030 program. Subsequently, we will delve into the specificity of AI Literacy in comparison to Information Literacy, a practice already adopted by libraries. Finally, we will analyze the current and prospective role of AI in libraries to propose avenues for implementing concrete actions.",unknown,148
10.62347/wqwv9220,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,American journal of translational research,2000-01-01 00:00:00,semantic_scholar,advancing healthcare: the role and impact of ai and foundation models.,https://www.semanticscholar.org/paper/3f2e8bebd871c8f46606a3ac7f1560dd8025f6b4,"BACKGROUND
The integration of artificial intelligence (AI) into the healthcare domain is a monumental shift with profound implications for diagnostics, medical interventions, and the overall structure of healthcare systems.


PURPOSE
This study explores the transformative journey of foundation AI models in healthcare, shedding light on the challenges, ethical considerations, and vast potential they hold for improving patient outcome and system efficiency. Notably, in this investigation we observe a relatively slow adoption of AI within the public sector of healthcare. The evolution of AI in healthcare is un-paralleled, especially its prowess in revolutionizing diagnostic processes.


RESULTS
This research showcases how these foundational models can unravel hidden patterns within complex medical datasets. The impact of AI reverberates through medical interventions, encompassing pathology, imaging, genomics, and personalized healthcare, positioning AI as a cornerstone in the quest for precision medicine. The paper delves into the applications of generative AI models in critical facets of healthcare, including decision support, medical imaging, and the prediction of protein structures. The study meticulously evaluates various AI models, such as transfer learning, RNN, autoencoders, and their roles in the healthcare landscape. A pioneering concept introduced in this exploration is that of General Medical AI (GMAI), advocating for the development of reusable and flexible AI models.


CONCLUSION
The review article discusses how AI can revolutionize healthcare by stressing the significance of transparency, fairness and accountability, in AI applications regarding patient data privacy and biases. By tackling these issues and suggesting a governance structure the article adds to the conversation about AI integration in healthcare environments.",unknown,149
10.3390/electronics13112156,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Electronics,2000-01-01 00:00:00,semantic_scholar,a data-centric ai paradigm for socio-industrial and global challenges,https://www.semanticscholar.org/paper/b9ccc9fa50ced6592c1175b80fae4749dcd571de,"Due to huge investments by both the public and private sectors, artificial intelligence (AI) has made tremendous progress in solving multiple real-world problems such as disease diagnosis, chatbot misbehavior, and crime control. However, the large-scale development and widespread adoption of AI have been hindered by the model-centric mindset that only focuses on improving the code/architecture of AI models (e.g., tweaking the network architecture, shrinking model size, tuning hyper-parameters, etc.). Generally, AI encompasses a model (or code) that solves a given problem by extracting salient features from underlying data. However, when the AI model yields a low performance, developers iteratively improve the code/algorithm without paying due attention to other aspects such as data. This model-centric AI (MC-AI) approach is limited to only those few businesses/applications (language models, text analysis, etc.) where big data readily exists, and it cannot offer a feasible solution when good data are not available. However, in many real-world cases, giant datasets either do not exist or cannot be curated. Therefore, the AI community is searching for appropriate solutions to compensate for the lack of giant datasets without compromising model performance. In this context, we need a data-centric AI (DC-AI) approach in order to solve the problems faced by the conventional MC-AI approach, and to enhance the applicability of AI technology to domains where data are limited. From this perspective, we analyze and compare MC-AI and DC-AI, and highlight their working mechanisms. Then, we describe the crucial problems (social, performance, drift, affordance, etc.) of the conventional MC-AI approach, and identify opportunities to solve those crucial problems with DC-AI. We also provide details concerning the development of the DC-AI approach, and discuss many techniques that are vital in bringing DC-AI from theory to practice. Finally, we highlight enabling technologies that can contribute to realizing DC-AI, and discuss various noteworthy use cases where DC-AI is more suitable than MC-AI. Through this analysis, we intend to open up a new direction in AI technology to solve global problems (e.g., climate change, supply chain disruption) that are threatening human well-being around the globe.",unknown,150
10.60087/jaigs.vol03.issue01.p124,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Artificial Intelligence General science (JAIGS) ISSN:3006-4023,2000-01-01 00:00:00,semantic_scholar,exploring ethical dimensions in ai: navigating bias and fairness in the field,https://www.semanticscholar.org/paper/ffa5a275be9ea886dff66494c821f2b2db2f091d,"The rapid progress in implementing Artificial Intelligence (AI) across various domains such as healthcare decision-making, medical diagnosis, and others has raised significant concerns regarding the fairness and bias embedded within AI systems. This is particularly crucial in sectors like healthcare, employment, criminal justice, credit scoring, and the emerging field of generative AI models (GenAI) producing synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including biases ingrained in the synthetic data representation of individuals.This survey paper provides a concise yet comprehensive examination of fairness and bias in AI, encompassing their origins, ramifications, and potential mitigation strategies. We scrutinize sources of bias, including data, algorithmic, and human decision biases, shedding light on the emergent issue of generative AI bias where models may replicate and amplify societal stereotypes. Assessing the societal impact of biased AI systems, we spotlight the perpetuation of inequalities and the reinforcement of harmful stereotypes, especially as generative AI gains traction in shaping public perception through generated content.Various proposed mitigation strategies are explored, with an emphasis on the ethical considerations surrounding their implementation. We stress the necessity of interdisciplinary collaboration to ensure the effectiveness of these strategies. Through a systematic literature review spanning multiple academic disciplines, we define AI bias and its various types, delving into the nuances of generative AI bias. We discuss the adverse effects of AI bias on individuals and society, providing an overview of current approaches to mitigate bias, including data preprocessing, model selection, and post-processing. Unique challenges posed by generative AI models are highlighted, underscoring the importance of tailored strategies to address them effectively.Addressing bias in AI necessitates a holistic approach, involving diverse and representative datasets, enhanced transparency, and accountability in AI systems, and exploration of alternative AI paradigms prioritizing fairness and ethical considerations. This survey contributes to the ongoing discourse on developing fair and unbiased AI systems by outlining the sources, impacts, and mitigation strategies related to AI bias, with a particular focus on the burgeoning field of generative AI.",unknown,151
10.69554/sesy2749,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Securities Operations &amp; Custody,2000-01-01 00:00:00,semantic_scholar,the ai handbook for financial services leaders : tips and tactics for mastering ai in banking and finance,https://www.semanticscholar.org/paper/ac02d92d3f8b24b838d8a52f1ba1b48790931f07,"This paper explores the multifaceted realm of artificial intelligence (AI) implementation in financial services, providing insights into its potential, challenges and best practices. Highlighting the emergence of generative AI (GenAI) as a transformative tool, the paper underscores its significant impact on productivity and revenue generation within investment banks and capital markets. Addressing inherent risks of AI adoption, the paper stresses the importance of robust governance frameworks to mitigate operational, reputational and compliance risks. Specific attention is given to the phenomenon of GenAI hallucinations and the imperative for deterministic AI models to ensure data integrity and regulatory compliance. The paper outlines four key pillars of AI’s applications in financial services: predictive AI, anomaly detection AI, classification AI and GenAI. Examples highlight AI’s role in risk management, fraud prevention, customer experience enhancement and internal process optimisation, underscoring its transformative potential across the industry. The paper also covers the distinction between public and private AI models, emphasising the advantages of proprietary data-driven insights in ensuring competitive advantage and regulatory compliance. Concluding with actionable insights for AI implementation, the paper advocates for a strategic approach encompassing clear vision setting, risk oversight, data privacy management, centralised data architecture and comprehensive process automation.",unknown,152
10.60087/jaigs.v3i1.119,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Artificial Intelligence General science (JAIGS) ISSN:3006-4023,2000-01-01 00:00:00,semantic_scholar,examining ethical aspects of ai: addressing bias and equity in the discipline,https://www.semanticscholar.org/paper/9b688157f467a83b7f4f6ac4eae7754241018aef,"he rapid progress in implementing Artificial Intelligence (AI) across various domains such as healthcare decision-making, medical diagnosis, and others has raised significant concerns regarding the fairness and bias embedded within AI systems. This is particularly crucial in sectors like healthcare, employment, criminal justice, credit scoring, and the emerging field of generative AI models (GenAI) producing synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including biases ingrained in the synthetic data representation of individuals.This survey paper provides a concise yet comprehensive examination of fairness and bias in AI, encompassing their origins, ramifications, and potential mitigation strategies. We scrutinize sources of bias, including data, algorithmic, and human decision biases, shedding light on the emergent issue of generative AI bias where models may replicate and amplify societal stereotypes. Assessing the societal impact of biased AI systems, we spotlight the perpetuation of inequalities and the reinforcement of harmful stereotypes, especially as generative AI gains traction in shaping public perception through generated content.Various proposed mitigation strategies are explored, with an emphasis on the ethical considerations surrounding their implementation. We stress the necessity of interdisciplinary collaboration to ensure the effectiveness of these strategies. Through a systematic literature review spanning multiple academic disciplines, we define AI bias and its various types, delving into the nuances of generative AI bias. We discuss the adverse effects of AI bias on individuals and society, providing an overview of current approaches to mitigate bias, including data preprocessing, model selection, and post-processing. Unique challenges posed by generative AI models are highlighted, underscoring the importance of tailored strategies to address them effectively.Addressing bias in AI necessitates a holistic approach, involving diverse and representative datasets, enhanced transparency, and accountability in AI systems, and exploration of alternative AI paradigms prioritizing fairness and ethical considerations. This survey contributes to the ongoing discourse on developing fair and unbiased AI systems by outlining the sources, impacts, and mitigation strategies related to AI bias, with a particular focus on the burgeoning field of generative AI.",unknown,153
10.1108/cr-06-2023-0144,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Competitiveness Review: An International Business Journal,2000-01-01 00:00:00,semantic_scholar,factors affecting citizen intention toward ai acceptance and adoption: the moderating role of government regulations,https://www.semanticscholar.org/paper/7d474e635b35f0915a71d3010b0635b03b367767,"
Purpose
This paper aims to explore factors impacting citizen intention toward artificial intelligence (AI) adoption, considering government regulation as a moderating variable. It focuses on the Palestinian Cellular Communications Sector in Gaza Strip, providing insights into the citizen-AI relationship dynamics. The research contributes to enhancing comprehension of AI technology from clients’ perspective.


Design/methodology/approach
To test the hypotheses, a questionnaire was used in an empirical study to collect primary data. In total, 347 Palestinian citizens responded to the survey.


Findings
The findings of this paper reveal that perceived usefulness, perceived ease of use, perceived risks, social influence, user experience and privacy and security concerns significantly influence citizen intention toward AI adoption. Furthermore, government regulations as a moderating variable strengthen the impact of perceived usefulness, perceived ease of use, perceived risks, social influence, user experience and privacy and security concerns on citizen intention toward AI acceptance and adoption. Thus, further research should explore specific domains and cultural contexts to gain a more comprehensive understanding of the factors shaping acceptance and adoption.


Research limitations/implications
The findings of the study should be understood in the context of their limitations. First, the study ignored cultural or domain-specific subtleties in favor of generic characteristics, which calls for more research in these particular circumstances. Second, relying on self-reported data might result in biases and limitations due to subjectivity in reporting, indicating the necessity for alternate data gathering methods and approaches in future research.


Practical implications
Policymakers, developers and organizations working to promote the acceptability and implementation of AI applications should consider the practical implications of this study’s results. To secure the long-term use of AI technologies in a responsible and user-centric way, policymakers should give priority to public education and awareness, user-centered design and ethical AI development techniques. They should also stimulate partnerships and create monitoring systems.


Originality/value
This paper investigates the originality of factors that influence citizen intention toward AI acceptance and adoption. It uniquely examines the moderating role of government regulations in shaping this intention. By addressing this novel aspect, the paper contributes to advancing our understanding of the complex dynamics surrounding citizen intentions toward AI applications.
",unknown,154
10.30525/2256-0742/2024-10-3-26-35,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Baltic Journal of Economic Studies,2000-01-01 00:00:00,semantic_scholar,ai in banking: socio-economic aspects,https://www.semanticscholar.org/paper/18f4c4e24961c02546fbd0f3423fe9562f872ac3,"Artificial intelligence is revolutionising the banking sector, offering a range of benefits such as enhanced customer support, improved fraud detection and more informed decision-making. Banks are using AI technologies to optimise their operations, improve customer service, and proactively manage risks, leading to increased efficiency and productivity. The article is devoted to the issue of studying the use of artificial intelligence in the banking sector. The purpose of the article is to explore the potential of artificial intelligence for the banking sector. The article uses methods of data analysis and synthesis, systematisation and comparison. The article uses statistical data from the World Bank, the State Statistics Service of Ukraine and other official websites. The authors analyse the positive and negative aspects of the use of artificial intelligence in the banking system. Artificial intelligence technologies allow banks to process large amounts of structured and unstructured data to predict market trends, gain insights and identify investment opportunities, which ultimately leads to better decision-making. In banking, the main purpose of AI is to help consumers by prioritising their choices. AI also helps to ensure that customers are satisfied with the bank's services. The paper presents a statistical analysis of GDP growth, inflation and public debt in some countries of the world. With the emergence and spread of FinTech and DeFi technologies and the introduction of CBDC, the system of control and regulation of cash flows is becoming more complex. The practical value of the publication lies in the authors' recommendations on the application of artificial intelligence in the banking sector. The authors suggest ways to maximise the positive effect of artificial intelligence in the banking sector.",unknown,155
10.48550/arxiv.2405.13606,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Conference on Electronic Government,2000-01-01 00:00:00,semantic_scholar,from the evolution of public data ecosystems to the evolving horizons of the forward-looking intelligent public data ecosystem empowered by emerging technologies,https://www.semanticscholar.org/paper/770b35d725e52eb3846ca0ea9b465fc6cbc3cf4b,"Public data ecosystems (PDEs) represent complex socio-technical systems crucial for optimizing data use in the public sector and outside it. Recognizing their multifaceted nature, previous research pro-posed a six-generation Evolutionary Model of Public Data Ecosystems (EMPDE). Designed as a result of a systematic literature review on the topic spanning three decade, this model, while theoretically robust, necessitates empirical validation to enhance its practical applicability. This study addresses this gap by validating the theoretical model through a real-life examination in five European countries - Latvia, Serbia, Czech Republic, Spain, and Poland. This empirical validation provides insights into PDEs dynamics and variations of implementations across contexts, particularly focusing on the 6th generation of forward-looking PDE generation named""Intelligent Public Data Generation""that represents a paradigm shift driven by emerging technologies such as cloud computing, Artificial Intelligence, Natural Language Processing tools, Generative AI, and Large Language Models (LLM) with potential to contribute to both automation and augmentation of business processes within these ecosystems. By transcending their traditional status as a mere component, evolving into both an actor and a stakeholder simultaneously, these technologies catalyze innovation and progress, enhancing PDE management strategies to align with societal, regulatory, and technical imperatives in the digital era.",unknown,156
10.28991/esj-2024-08-03-020,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Emerging Science Journal,2000-01-01 00:00:00,semantic_scholar,role of two-way asymmetrical communication in sustaining public relations,https://www.semanticscholar.org/paper/91d880116dcfd83d68b143026862de93f8aabb38,"Internet technology's worldwide success and adoption have provided organizations with direct access to their constituents and customers. Especially, organizations relying on online platforms provide comparatively better services and have strong relations with their clients. This research also focused on relevant phenomena in the United Arab Emirates banking sector organizations. The researchers employed a cross-sectional design and randomly selected a sample of n=400 individuals. Results revealed a significant impact of customer support services on providing product information (p>0.008) and service quality (p>0.000). Further, the effect of service quality on Artificial Intelligence also remained significant (p>0.000). Besides, Artificial Intelligence is also found significantly impact the Public Relations of Emirati banks (p>0.006). Finally, the mediating impact of communication skills on Artificial Intelligence and Public Relations remained significant (p>0.088). Moreover, the Artificial Neural Network (ANN) revealed the Sum of Square Values at 568.19, the Overall Relative Error value at 0.813, and the accuracy level at 18.7% (training). While, regarding the testing, the Sum of Square Values remained at 256.80 and the Average Overall Relative Error value remained at 0.861, indicating an overall accuracy of 13.9%. Thus, it is concluded that the importance of two-way communication can be determined because it helps determine and understand the customers' needs and demands. The more an organization understands its customers, the more it fulfills their expectations, indicating the importance of two-way communication. Finally, this research recommends more studies regarding AI-enabled Emotional Intelligence in other sectors to dig out in-depth results. Doi: 10.28991/ESJ-2024-08-03-020 Full Text: PDF",unknown,157
10.48550/arxiv.2408.00965,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,arXiv.org,2000-01-01 00:00:00,semantic_scholar,integrating esg and ai: a comprehensive responsible ai assessment framework,https://www.semanticscholar.org/paper/15b153e0f6342ad455bfb5e2b6090b89c8066340,"Artificial Intelligence (AI) is a widely developed and adopted technology across entire industry sectors. Integrating environmental, social, and governance (ESG) considerations with AI investments is crucial for ensuring ethical and sustainable technological advancement. Particularly from an investor perspective, this integration not only mitigates risks but also enhances long-term value creation by aligning AI initiatives with broader societal goals. Yet, this area has been less explored in both academia and industry. To bridge the gap, we introduce a novel ESG-AI framework, which is developed based on insights from engagements with 28 companies and comprises three key components. The framework provides a structured approach to this integration, developed in collaboration with industry practitioners. The ESG-AI framework provides an overview of the environmental and social impacts of AI applications, helping users such as investors assess the materiality of AI use. Moreover, it enables investors to evaluate a company's commitment to responsible AI through structured engagements and thorough assessment of specific risk areas. We have publicly released the framework and toolkit in April 2024, which has received significant attention and positive feedback from the investment community. This paper details each component of the framework, demonstrating its applicability in real-world contexts and its potential to guide ethical AI investments.",unknown,158
10.56830/ijams07202403,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Journal of Accounting and Management Sciences,2000-01-01 00:00:00,semantic_scholar,a smart approach for budget deficits predictionunder economic shocks,https://www.semanticscholar.org/paper/38e353bdf22752a2d9f535a2f8617d41d7f1d49d,"Historically, the main behavior of fiscal policy is to distribute resources, income, and expenditures, which are interconnected functions of economic stability. Recently, the scope of public sector economics has expanded beyond budgetary components in parallel with the development of public finance. While the budget reached the economic division form of budget items based on program and performance theories, Budget monitoring, and financial risk management are currently challenging, particularly in the face of monetary policy uncertainty. Financial institutions are crucially concerned with the stability of public finance in low-income countries (LICs) as it contributes to improved investor confidence and fiscal decision-making. Hence, economists investigated uncertainty shocks and contributed to managing financial risks with global and energy uncertainty indices. Furthermore, the maturity of digital transformation and artificial intelligence financial applications catalyzed scholars to examine its contributions in the fiscal distress prediction field. Hence, this research aims to integrate artificial intelligence into financial performance analysis to bridge the gap in budget forecasting. The study was aimed at proposing an Economics Division Uncertainty approach (EDUA), which combined (ARIMA) and (LSTM) models for time series analysis of Nuclear Material Authority expenditures over the previous five years divided into quarterly periods, to achieve efficiency in spending. The (ARIMA) model’s (ADF) results showed that uncertainty indicators are highly significant. The best (p-value) in the first and second differences in (ARIMA) models is (0.0001) for petroleum items, (0.0001) for solar price rates, (0.001) for the US exchange rate, and (0.003) for electricity price rates, when compared to (EDU_LSTM). Both models have similar accuracy rates, with the best being (EDU_ARIMA) (solar price 97%, USD exchange rate 84%). The second study proposed a composite model of four machine-learning tools to enhance financial performance during financial distress. The study collected (12) indicators from general financial literature and corporate studies, utilizing the (XGBOOST, Random Forest, KNN, and Naïve Bayes) models. Comparing the accuracy results for each model presented different accuracy results in the deep learning models over five years of data. The best accuracy score was for Random Forest at (69%), XGBOOT at (68%), and KNN at (68%). We recommended explainable AI as future research to interpret the budget deficit during the fiscal year period. Keywords: Economic Shocks, Uncertainty, Budget Reliability, Financial Distress Prediction, Artificial Intelligence.",unknown,159
10.15847/obsobs18520242433,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Observatorio (OBS*),2000-01-01 00:00:00,semantic_scholar,decoding algorithmic literacy among journalists,https://www.semanticscholar.org/paper/e8761fc9f655c137d3990a9c46fab0fd84839916,"Recent developments in generative Artificial Intelligence (AI) have revitalized academic discourse on algorithmic systems, particularly on their potential, ethical considerations, risks, and regulatory challenges. Extensive research has examined how algorithms affect communication processes, focusing on their influence on news organizations, journalistic practices, public-media dynamics, media literacy, and combating disinformation and filter bubbles.
An emergent strand of research defines and measures the multidimensional concept of algorithmic literacy. However limited research exists on the intersection of algorithmic literacy and journalism. This gap is particularly concerning given the pivotal role of journalism in shaping public discourse, informing citizens, upholding democratic values and contrasting disinformation. Understanding how journalists perceive and engage with algorithms is essential, as these technologies significantly influence their professional tasks, including content production and distribution.
In Portugal, where newcomer journalists work in precarious conditions and digital media transformation is rapidly evolving, understanding how journalists interact with and perceive algorithms is vital. Our study, through a multi-phased approach, aims to fill this gap questioning how can algorithmic literacy, encompassing cognitive, attitudinal and behavioural dimensions, be effectively assessed among professional journalists?
The exploratory results present a validated methodological tool, instrument based on a multi-dimensional analytical framework and specifically designed to measure algorithmic literacy levels and to assess journalists’ experiences. Critical discussion addresses the methodological procedures and preliminary findings from the pre-test, offering insights into Portuguese journalists' understanding, perceptions, and competencies regarding algorithmic systems. By shedding light on the cognitive, affective, and behavioural aspects of journalists’ engagement with algorithms, this study contributes to a deeper understanding of the algorithmic literacy among journalists, which is essential to sustain the quality of their work and for an effective counteraction against disinformation. It also opens avenues for similar studies in other geographical or professional contexts.",unknown,160
10.11591/ijaas.v13.i1.pp1-12,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International Journal of Advances in Applied Sciences,2000-01-01 00:00:00,semantic_scholar,therapybot: a chatbot for mental well-being using transformers,https://www.semanticscholar.org/paper/2a3d3eeda974703f1727ff724f1078bd5e6b2e1f,"The field of natural language processing (NLP) and conversational artificial intelligence (AI) has one ingenious application in the psychological space. Depression and anxiety are two major issues that the world is facing, with close to 41% of adults reporting these symptoms in the United States alone, as of December 2020. It has also been observed that most of the people are not open about it. As a result, it is critical to address this issue on a global scale. Developed countries reportedly have 9 psychiatrists per 100,000 people. One way to mitigate this is the use of chatbots. We propose a transformer-based methodology to build a therapy bot that has been trained on a combination of open-domain conversations from a publicly available dataset and therapist-client conversations from a self-constructed dataset. This end-to-end data-driven model shows quality performance in conversations and adds value by aiding in the case of mental health issues. The proposed architecture is proven to be effective in its usability in the psychological space for both single-turn and multi-turn dialogue. The performance of the proposed system shows loss is 0.29 and perplexity is 1.34, both metrics keeps gradually decreasing and it means an improvement in performance of chatbots system.",unknown,161
10.3390/bioengineering11040300,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Bioengineering,2000-01-01 00:00:00,semantic_scholar,enhancing readability and detection of age-related macular degeneration using optical coherence tomography imaging: an ai approach,https://www.semanticscholar.org/paper/19b3152e97e63a38ec7406abb3492326ad27af82,"Artificial intelligence has been used effectively in medical diagnosis. The objective of this project is to examine the application of a collective AI model using weighted fusion of predicted probabilities from different AI architectures to diagnose various retinal conditions based on optical coherence tomography (OCT). A publicly available Noor dataset, comprising 16,822, images from 554 retinal OCT scans of 441 patients, was used to predict a diverse spectrum of age-related macular degeneration (AMD) stages: normal, drusen, or choroidal neovascularization. These predictions were compared with predictions from ResNet, EfficientNet, and Attention models, respectively, using precision, recall, F1 score, and confusion matric and receiver operating characteristics curves. Our collective model demonstrated superior accuracy in classifying AMD compared to individual ResNet, EfficientNet, and Attention models, showcasing the effectiveness of using trainable weights in the ensemble fusion process, where these weights dynamically adapt during training rather than being fixed values. Specifically, our ensemble model achieved an accuracy of 91.88%, precision of 92.54%, recall of 92.01%, and F1 score of 92.03%, outperforming individual models. Our model also highlights the refinement process undertaken through a thorough examination of initially misclassified cases, leading to significant improvements in the model’s accuracy rate to 97%. This study also underscores the potential of AI as a valuable tool in ophthalmology. The proposed ensemble model, combining different mechanisms highlights the benefits of model fusion for complex medical image analysis.",unknown,162
10.7717/peerj-cs.1862,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,PeerJ Computer Science,2000-01-01 00:00:00,semantic_scholar,a novel approach to recognition of alzheimer’s and parkinson’s diseases: random subspace ensemble classifier based on deep hybrid features with a super-resolution image,https://www.semanticscholar.org/paper/2b5be61b9f19c483461fbbd19658cb720c122c26,"Background Artificial intelligence technologies have great potential in classifying neurodegenerative diseases such as Alzheimer’s and Parkinson’s. These technologies can aid in early diagnosis, enhance classification accuracy, and improve patient access to appropriate treatments. For this purpose, we focused on AI-based auto-diagnosis of Alzheimer’s disease, Parkinson’s disease, and healthy MRI images. Methods In the current study, a deep hybrid network based on an ensemble classifier and convolutional neural network was designed. First, a very deep super-resolution neural network was adapted to improve the resolution of MRI images. Low and high-level features were extracted from the images processed with the hybrid deep convolutional neural network. Finally, these deep features are given as input to the k-nearest neighbor (KNN)-based random subspace ensemble classifier. Results A 3-class dataset containing publicly available MRI images was utilized to test the proposed architecture. In experimental works, the proposed model produced 99.11% accuracy, 98.75% sensitivity, 99.54% specificity, 98.65% precision, and 98.70% F1-score performance values. The results indicate that our AI system has the potential to provide valuable diagnostic assistance in clinical settings.",unknown,163
10.55574/ilsm6729,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"International Journal of Law, Ethics, and Technology",2000-01-01 00:00:00,semantic_scholar,"“i, for one, welcome our new” ai jurors: chatgpt and the future of the jury system in american law",https://www.semanticscholar.org/paper/73bf7665b0461e08b38550781911336666db7497,"This article explores the potential for advanced generative text AI systems like ChatGPT to serve as a replacement for human juries in the modern legal system. It argues that the vast knowledge base and perspective-aggregation capabilities of these AI models uniquely position them as potentially superior embodiments of the “community conscience” that juries are meant to represent. By synthesizing diverse viewpoints into nuanced, context-sensitive judgments, AI juries could in theory do justice to the broader values and concerns of society in ways that 12-person human juries often fail to achieve. The article first examines the technical capabilities of state-of-the-art language models like ChatGPT, emphasizing the vast scope and diversity of their training data which spans a huge range of human knowledge and perspectives. It then traces the historical development of the jury system and its essential functions as both the moral conscience of the community and a source of democratic legitimacy for the legal system. Building on this foundation, the article makes the case that AI is poised to fulfill the representative and deliberative roles of juries more effectively than human jurors by virtue of its unparalleled capacity to absorb and synthesize society’s heterogeneous values and viewpoints. However, it also carefully considers the significant risks and challenges associated with AI juries, including issues of algorithmic bias, the opacity of machine reasoning, the potential erosion of public trust, and the philosophical implications of outsourcing moral judgment to artificial intelligence. Ultimately, the article argues that while the use of AI in legal decision-making is likely inevitable, it is crucial that we proactively shape the terms of this integration in ways that uphold the core values of fairness, transparency, and democratic accountability. The jury system has long been celebrated as a bastion of citizen participation in the law – the article concludes by calling for a robust public dialogue on how AI can be harnessed to enhance, rather than erode, this vital civic institution. Keywords: Artificial Intelligence, ChatGPT, Jury, AI Ethics, Moral Reasoning, Machine Learning, Algorithm, Legal Tech, Law and Technology, AI Governance",unknown,164
10.1109/access.2024.3401591,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Access,2000-01-01 00:00:00,semantic_scholar,secure remote sensing data with blockchain distributed ledger technology: a solution for smart cities,https://www.semanticscholar.org/paper/5d16be61d87a8431733033bb92098eb4915ef528,"Particularly in the context of smart cities, remote sensing data (RSD) has emerged as one of the hottest study topics in information and communication technology (ICT) today. The development of machine learning (ML) and artificial intelligence (AI) has made it possible to solve a number of issues, including automation, control access, optimization, monitoring, and management. Simultaneously, there are significant issues with the design and development of the process hierarchy, including inadequate training records, centralized architecture, data privacy protection, and overall resource consumption restrictions. The development of Distributed Ledger Technology (DLT), on the other hand, provides a decentralized infrastructure that allows systems to eliminate centralized data-sharing procedures of smart cities while transferring from network node to network node, and third-party access control solves machine learning issues. To process an ideal data delivery mechanism for the smart cities analytical model, the paper employs Partial Swam Optimization (POS) in conjunction with a secure blockchain distributed consortium network. This work makes three contributions. Firstly, it offers a safe transmission method that combines blockchain and machine learning to optimize the path for reliable data delivery across secure channels. Second, neighborhood encryption sequences are carried out using NuCypher proxy re-encryption-enabled value encryption, a public key cryptographic approach that avoids cypher conversion. Third, Artificial Neural Networks (ANNs) can solve the data deliverance classification problem in smart cities by optimizing record management and preservation.",unknown,165
10.1145/3657054.3657126,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Digital Government Research,2000-01-01 00:00:00,semantic_scholar,gai as a catalyst in national technology sovereignty: evaluating the influence of gai on government policy,https://www.semanticscholar.org/paper/c37fd9c1303cce61ad8e84b8c4fb8bf36ba8fa70,"As a result of the prominence of generative artificial intelligence across diverse fields, it has become necessary for governments to develop national strategies for directing the ethical use of artificial intelligence to respect fundamental human values. This paper explores the role of Generative Artificial Intelligence (GAI) in technology sovereignty, its contributions, and benefits for the government, associated risks, and challenges, and how it influences government policies. It begins with examining GAI's capabilities to comprehend how it understands natural language, trains on existing data, and generates realistic outputs, followed by a discussion of its potential benefits for governments that enable them to act independently and autonomously in diverse sectors. It highlights how it can empower them to administer technological ecosystems, promote domestic innovation, and facilitate policy-making processes. However, contrary to its benefits, GAI is also capable of inflicting negative consequences on society. Therefore, the paper also addresses the risks and challenges associated with GAI that necessitate reflection on existing policies and developing new ones that align with a nation's legal frameworks. Exploring the influence of GAI on government policies, the paper highlights the significance of collaboration in policy-making endeavors to ensure ethical future developments and bring value to public interest and democratic values. This comprehensive analysis aims to shed light on the responsible and ethical use of GAI to preserve human rights, promote economic growth, sustain social justice, and inform the responsible use of GAI within the framework of technology sovereignty.",unknown,166
10.54254/2754-1169/74/20241424,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Advances in Economics, Management and Political Sciences",2000-01-01 00:00:00,semantic_scholar,the distinction of evaluation on companies in different fields,https://www.semanticscholar.org/paper/a73b1203e1133a535751ea020bda726892cf65a8,"This paper explores the variations in Return on Equity (ROE) and Price-to-Sales Ratio among publicly traded companies across different time periods and market capitalizations. The analysis employs Linear Regression and Correlation Coefficients to examine the relationships between ROE, Price-to-Sales Ratio, Market Capitalization, and economic conditions during these periods. The findings reveal that Price-to-Sales Ratio serves as a reliable indicator of a company's market capitalization. Furthermore, companies within the same industry exhibit substantial disparities in Market Capitalization, ROE, Price-to-Sales Ratio, Price-to-Earnings (P/E) Ratio, and PEG Index due to differing corporate strategies and backgrounds. For instance, in the new energy sector, both BYD and NIO display a negative correlation between market capitalization, while Tesla contradicts this trend. This discrepancy is attributed to internal factors such as asset composition, liabilities, and shareholder structure, which significantly influence a company's financial performance. The Market Capitalization of these new energy companies is predicted to rise due to global interest in AI technology, with technology companies being at the forefront of AI development. In the technology sector, companies like Google have solidified their positions by advancing AI concepts. New entrants, like Apple, combining Artificial Intelligence and Robotics, are gaining consumer support, indicating potential Market Cap. growth. The fashion industry also demonstrates distinctive strategies, with companies like NIKE, Adidas, and Anta focusing on sustainability and carbon neutrality. Therefore, future company evaluations should consider both financial indices and a company's long-term vision and sustainability practices to predict their future value effectively.",unknown,167
10.54216/ijns.240208,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,International journal of neutrosophic science,2000-01-01 00:00:00,semantic_scholar,modeling of improved sine trigonometric single valued neutrosophic information based air pollution prediction approach,https://www.semanticscholar.org/paper/581e28a4bde8d0d8bc08b4cdce31f21692fee56f,"Industrialization and urbanization air is getting polluted due to human activities. CO, NO, C6H6, etc., are the major air pollutants. The focus of air pollutants in ambient air is controlled by the climatological parameters including wind direction, atmospheric speed of wind, temperature, and humidity. Air pollution prediction is a critical sector where machine learning (ML) technique plays a major role. Its main purpose is to tackle and understand the damaging effects of air pollutants on the environment and human health. By using a range of ML techniques such as neural networks, regression, and decision trees, we could analyze historical data on air quality alongside geographical and meteorological factors. This allows us to design model that could detect patterns and predict pollution levels. By taking proactive measures such as providing timely alerts to the public, adjusting controls on emissions, and, implementing strategies to reduce pollution, we can work towards creating healthier and cleaner environments. Embracing the potential of artificial intelligence (AI) in air pollution prediction empowers us to protect the well-being of our communities and make informed decisions. Therefore, this study develops an Improved Sine Trigonometric Single Valued Neutrosophic Information based Air Pollution Prediction (ISTSVNI-APP) approach. The major objective of the ISTSVNI-APP technique is to exploit AI concepts with neutrosophic sets (NS) models for the forecasting of air pollution. To do so, the ISTSVNI-APP technique makes use of min-max normalization as the initial preprocessing step. For predicting air pollution, the ISTSVNI-APP technique uses STSVNI approach. To improve the performance of the ISTSVNI-APP technique, modified crow search algorithm (MCSA) is used for the parameter tuning of the STSVNI system. The performance evaluation of the ISTSVNI-APP method is verified utilizing benchmark dataset. The experimental outcomes stated that the ISTSVNI-APP technique gains better performance in predicting air pollution",unknown,168
10.1109/icedeg58167.2023.10122084,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2023 Ninth International Conference on eDemocracy & eGovernment (ICEDEG),IEEE,2023-04-05 00:00:00,ieeexplore,keynote - ai for the public sector and the case of legal nlp,https://ieeexplore.ieee.org/document/10122084/,"Recent innovations such as ChatGPT have increased public interest in artificial intelligence (AI). The keynote explained why AI is not just a short-term hype but has a long history of spanning several eras. A recent revolution has been in the field of Natural Language Processing (NLP). This interdisciplinary field of research is also known as computational linguistics. It is usually implemented by specific NLP tasks, ranging from simple processing steps such as tokenization, stemming, lemmatization to Part of Speech (PoS) tagging and topic modeling. A second, more complex set of NLP tasks includes Namend Entity Recognition (NER), information retrieval, relationship extraction, sentiment analysis, text similarity, and coreference resolution. Finally, the most challenging NLP tasks are considered Question Answering (QA), text summarization, text simplification, text generation, text translation, and chatbots. NLP has especially great potential in the public sector. For example, a new multilingual legal language model for more than 20 languages, developed for the Swiss Federal Court, offers opportunities to increase accessibility of legal documents for citizens while preserving the digital sovereignty of government institutions. These technical results of the National Research Program (NRP) 77 project “Open Justice versus Privacy” are published on Hugging Face, a platform for sharing openly available machine learning models and datasets. Today, it is mostly private companies that build such Large Language Models (LLM), because it requires a large amount of computational resources and highly skilled engineers. For example, to train the new LLaMA model, Meta AI (Facebook) needed more than $30 million worth of graphical processing units (GPU). In addition, 450 MWh of electricity worth about $90,000 was needed to process the data on these GPUs. Negative for innovation and the environment, Meta AI released the LLaMA model only under a non-commercial license. This means that startups and other companies cannot use the model for their own services. This calls for a discussion about how “open” today's machine learning models should be and what “open” actually means in the age of AI. The keynote presentation therefore included a proposal of 5 elements of such machine learning models that need to be openly available and licensed under an official open license in order to speak of an Open AI Model. This term is used by the United Nations definition of Digital Public Goods. These five elements include 1) model architecture (detailed scientific publications), 2) hyperparameters (built configuration), 3) training data (labeled and unlabeled datasets), 4) model weights and intermediate checkpoints (parameters), and 5) source code to build the model (programming scripts etc.). A truly openly available AI model is BLOOM, an LLM from the BigScience initiative. It was built by more than 1000 researchers from over 70 countries, trained on an infrastructure that would have cost EUR 3 million. BLOOM was released on July 12th, 2022 on Hugging Face and is licensed under the Responsible AI License (RAIL), a new type of AI license that incorporates ethical aspects while preserving the openness of the machine learning elements described.",unknown,169
10.1109/ecai.2018.8679038,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2018-06-30 00:00:00,ieeexplore,mobile application for a smart city,https://ieeexplore.ieee.org/document/8679038/,"In the last few years the number of smart city projects increased drastically and almost all countries have at least one smart city or started implementing a smart city architecture. This paper's purpose is highlighting the functionality of our smart city mobile application, it's features and the way it can influence our lives and the way we interact with the world around us in everyday activities. The system we are describing can monitor and give the end user viewing access to important information about the city he / she lives in. It can also send alerts in the form of push notifications, SMS messages or email about public interest information, new laws, shops, emergencies, natural disasters or any relevant information for the public. Using this application, the public can also send useful feedback to the authorities about broken stuff existing in the city, potholes, accidents, etc., while also using the mobile devices geolocation feature to indicate the specified location's problem The application can also be used to view information about the users, home, as electric consumption, temperature in different rooms, water quality, outside weather, security camera feed, security alarm state, etc. The users can also interact with the application and send commands to his / her smart home, like starting or stopping the central heating, starting or stopping different home appliances.",unknown,170
10.1109/ifuzzy55320.2022.9985222,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2022 International Conference on Fuzzy Theory and Its Applications (iFUZZY),IEEE,2022-11-05 00:00:00,ieeexplore,sugar beets and weed detection using semantic segmentation,https://ieeexplore.ieee.org/document/9985222/,"Weeds have a big impact on beet growth. As a result, there is a growing public interest in weed detection and identification during crop growth. This paper describes a UNet++ method with deep supervision for detecting weeds in sugar beet fields trained on the dataset. The UNet++ architecture is an encoder-decoder network that closes the semantic gap between the feature maps of the encoder and decoder sub-networks through redesigned skip paths. When the feature maps of the encoder and decoder networks are semantically similar, the optimizer will handle the learning task more easily. We evaluate the comparison of UNet, UNet++, and UNet++ with deep supervision architectures on the dataset segmentation task. Experiments show that the mIoU values on UNet, UNet++ and UNet++ with deep supervision are about 90.81%, 92.00% and 92.34%. It turns out that UNet++ with deep supervision can detect the location of weeds more effectively.",unknown,171
10.1109/e-science58273.2023.10254867,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2023 IEEE 19th International Conference on e-Science (e-Science),IEEE,2023-10-13 00:00:00,ieeexplore,a network synthesis and analytics pipeline with applications to sustainable energy in smart grid,https://ieeexplore.ieee.org/document/10254867/,"Transitioning to clean and low-carbon energy is becoming a crucial goal for many entities in the energy systems sector such as governments, power utilities, and policymakers. This shift to clean energy is supported by a diverse portfolio of data products such as satellite data, smart meter data, power networks, green energy datasets (e.g., solar installations & electric vehicles), microgrid networks, and building stock data. Among these, network datasets are becoming increasingly common in addressing a wide array of issues in residential energy, especially in applications that focus on social good. Thus, streamlining the process of generating different types of networks will be helpful. In this work, we propose a versatile network synthesis and analytics pipeline developed using software design principles that make it modular, scalable, and extensible. Three case studies are presented to illustrate the significance of network data in sustainable energy applications.",unknown,173
10.1109/istas57930.2023.10306085,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2023 IEEE International Symposium on Technology and Society (ISTAS),IEEE,2023-09-15 00:00:00,ieeexplore,co-designing socio-technical interventions with skilled trade workers,https://ieeexplore.ieee.org/document/10306085/,"This paper lays out an approach to co-designing Artificial Intelligence (AI)-enhanced smart hand tools with skilled trade workers employed at a local municipality. Skilled trade workers contribute to society by building the infrastructure upon which the public depends. In addition, these technical interventions offer an opportunity for workers to benefit from the data they generate via smart hand tools, potentially creating a new empowerment dynamic with employers. Therefore, we consider technologies that support skilled trade workers in performing their work effectively, safely, and with increased levels of autonomy to be considered Public Interest Technologies (PIT). Interdisciplinary research is underway that aligns these approaches with Public Interest Design (PID) principles which informs researchers' desire to explore how emerging technologies - data cooperatives, blockchain, smart contracts, and data dividends - can further smart hand tools' empowerment dynamic. Future participatory design efforts may deliver additional insights and further impact technology deployment.",unknown,174
10.1109/wacv56688.2023.00435,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2023-01-07 00:00:00,ieeexplore,towards a framework for privacy-preserving pedestrian analysis,https://ieeexplore.ieee.org/document/10030965/,"The design of pedestrian-friendly infrastructures plays a crucial role in creating sustainable transportation in urban environments. Analyzing pedestrian behaviour in response to existing infrastructure is pivotal to planning, maintaining, and creating more pedestrian-friendly facilities. Many approaches have been proposed to extract such behaviour by applying deep learning models to video data. Video data, however, includes an broad spectrum of privacy-sensitive information about individuals, such as their location at a given time or who they are with. Most of the existing models use privacy-invasive methodologies to track, detect, and analyse individual or group pedestrian behaviour patterns. As a step towards privacy-preserving pedestrian analysis, this paper introduces a framework to anonymize all pedestrians before analyzing their behaviors. The proposed framework leverages recent developments in 3D wireframe reconstruction and digital in-painting to represent pedestrians with quantitative wireframes by removing their images while preserving pose, shape, and background scene context. To evaluate the proposed framework, a generic metric is introduced for each of privacy and utility. Experimental evaluation on widely-used datasets shows that the proposed framework outperforms traditional and state-of-the-art image filtering approaches by generating best privacy utility trade-off.",unknown,175
10.1109/tts.2024.3377541,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',IEEE Transactions on Technology and Society,IEEE,2024-03-01 00:00:00,ieeexplore,a framework for the interpretable modeling of household wealth in rural communities from satellite data,https://ieeexplore.ieee.org/document/10472658/,"Data-driven policy development and investment are necessary for aligning policies across administrative levels, targeting interventions, and meeting the 2030 Sustainable Development Goals. However, local-level economic well-being data at timely intervals, critical to informing policy development and ensuring equity of outcomes, are unavailable in many parts of the world. Yet, filling these data gaps with black-box predictive models like neural networks introduces risk and inequity to the decision- making process. In this work, we construct an alternative interpretable model to these black-box models to predict household wealth, a key socioeconomic well-being indicator, at 5-km scale from widely available satellite data. Our interpretable model promotes transparency, the identification of potential drivers of bias and harmful outcomes, and inclusive design for human-ML decision-making. We model household wealth as a low- order function of productive land use that can be interpreted and integrated with domain knowledge by decision-makers. We aggregate remotely sensed land cover change data from 2006–2019 to construct an interpretable linear regression model for household wealth and wealth change in Uganda at a 5-km scale with  $r^{2}\,\,{=}$  72%. Our results demonstrate that there is not a clear performance-interpretability tradeoff in modeling household wealth from satellite imagery at high spatial and temporal resolution. Finally, we recommend a tiered framework to model socioeconomic outcomes from remote sensing data.",unknown,176
10.1109/access.2019.2891837,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,modeling opinion of iptv viewers based on implicit feedback and content metadata,https://ieeexplore.ieee.org/document/8607973/,"In today's hyper-connected world, rich social data feeds can be obtained from various sources, including the data exhaust of many commonly used systems. In this paper, we analyze the social pulse, obtained from viewer activity in an IPTV network-we attempt to validate a framework for determining public opinion and public interest through implicit feedback of IPTV viewers. First, we address the hypothesis that implicit viewer feedback in the form of channel change events, paired with the content metadata, can be used to model viewers' opinion and interest. For this, we design a controlled experiment to collect explicit feedback by rating a set of general-interest news clips. In addition to collecting demographic information, we also survey viewers' opinion, interest, and the probability of channel change during each clip. Furthermore, we extract weighted feature vectors from the closed captions of the video; this data, combined with the reported probability of channel change, is used to build a model that classifies opinion in five categories based on the probability of channel change and content. Next, we build a simplified model that classifies opinion in five categories based on the interest, which shows a linear relationship, but further consideration of content, in this case, provides better accuracy and possibility to analyze anomalous cases. Finally, we discuss and analyze the applications of such models in large systems and the necessary modifications to scale the system and to ensure the adequate performance on massive IPTV event data streams.",unknown,177
10.1109/icitda55840.2022.9971325,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2022 IEEE 7th International Conference on Information Technology and Digital Applications (ICITDA),IEEE,2022-11-05 00:00:00,ieeexplore,an identification and analysis of fake news about covid-19 vaccines in the philippines by using a classification model,https://ieeexplore.ieee.org/document/9971325/,"Fake news is proven to be rampant in all areas, whether it is in a non-digital outlet, like newspapers, magazines, or gossip from one person to another. This is not to say the digital spread of news is accurate. In fact, it is far from it; fake news in digital media is much more common. It is especially evident in the new pandemic, at a time when people are in need of accurate information about the effects, tracing, and, most importantly, the progress of the vaccine for COVID-19. The purpose of the study was to create a model that would help internet users to separate fake news from real through its identification in several news articles focusing on topics relating to the COVID-19 vaccine. In order to aid in this issue, the researchers intended to use machine learning to create a classification model to identify whether articles in particular news sites, both reliable and unreliable, were considered to be fake news or not. This was done through the analysis of a given dataset and comparing these datasets with another similar dataset created by the researchers and using existing classification algorithms in order to identify which of them would serve the best results. After which, certain metrics like accuracy, precision, recall and the like were employed to evaluate the created model. The results at the end of the study revealed that news articles on both the gathered reliable and unreliable news sites were mostly positive, and that the best classification algorithm to use for the created model is logistic regression with an accuracy of 83.50%, a precision of 82.37%, a recall of S3.50%, a ROC of 95.99%, and an F-Measure of 82.54%. With these results, the researchers hope to highly contribute to the reduction of fake news on the internet for the benefit of the common good.",unknown,178
10.1109/ecai61503.2024.10607414,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',"2024 16th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2024-06-28 00:00:00,ieeexplore,a horizontal collaborative method for integrated local and area public interest services by administrations and providers,https://ieeexplore.ieee.org/document/10607414/,"One method of horizontal collaboration for integrated local and area public interest services by governments and providers involves building partnerships and cooperation at appropriate existing levels in government departments, agencies, and organizations that provide local and area public services. This collaborative strategy is intended to improve efficiency, reduce duplication and ensure seamless service delivery. A model of horizontal collaboration between government departments, agencies and organizations providing seamless public services would allow for more specialized news filtering and consistent interaction with citizens. Implementing this entire paradigm requires dedication, effective communication between providers and government departments, and regular reviews and revisions based on lessons learned will help ensure the longevity and effectiveness of this horizontal community cooperation approach.",unknown,180
10.1109/cvpr52688.2022.00249,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2022-06-24 00:00:00,ieeexplore,defensive patches for robust recognition in the physical world,https://ieeexplore.ieee.org/document/9878494/,"To operate in real-world high-stakes environments, deep learning systems have to endure noises that have been con-tinuously thwarting their robustness. Data-end defense, which improves robustness by operations on input data in-stead of modifying models, has attracted intensive attention due to its feasibility in practice. However, previous data-end defenses show low generalization against diverse noises and weak transferability across multiple models. Motivated by the fact that robust recognition depends on both local and global features, we propose a defensive patch generation framework to address these problems by helping mod-els better exploit these features. For the generalization against diverse noises, we inject class-specific identifiable patterns into a confined local patch prior, so that defensive patches could preserve more recognizable features towards specific classes, leading models for better recognition under noises. For the transferability across multiple models, we guide the defensive patches to capture more global fea-ture correlations within a class, so that they could activate model-shared global perceptions and transfer better among models. Our defensive patches show great potentials to im-prove application robustness in practice by simply sticking them around target objects. Extensive experiments show that we outperform others by large margins (improve 20+ % accuracy for both adversarial and corruption robustness on average in the digital and physical world).11Our codes are available at https://github.com/nlsde-safety-team/DefensivePatch.",unknown,181
10.1109/icscc59169.2023.10335054,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2023 9th International Conference on Smart Computing and Communications (ICSCC),IEEE,2023-08-19 00:00:00,ieeexplore,leveraging machine learning algorithms for football predictions and wager suggestions,https://ieeexplore.ieee.org/document/10335054/,"Growing public interest in football, particularly in the wake of the recent world cup, has had an impact not just on people’s desire to watch the game, but also on the different analytics conducted in relation to it. The availability of clean, consistent data greatly enhances these analytics. This study’s primary goal is to demonstrate how various football statistics, such as goals, shots, possession, passing accuracy, and others, can be used to create an effective prediction model that will meet our primary goal of achieving over 60% test accuracy. (Given the nature of sports, it is typically difficult to quantify all the factors directly influencing the game. Therefore, although they could directly affect the outcome of the game, things like injuries sustained in the middle of play, weather, and interpersonal problems between players cannot be recorded as metrics to be worked upon. Verifying the generated winning probabilities against the odds provided by well-known bookies also made sense. The model will excel as it learns from the most recent data by dynamically obtaining the data at regular intervals through API requests. Also demonstrated is a flimsy attempt to gauge fan sentiment toward their team, which was measured and subsequently incorporated as a bias in the prediction model.",unknown,182
10.1109/icccis60361.2023.10425428,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',"2023 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",IEEE,2023-11-04 00:00:00,ieeexplore,modelling the domains of artificial intelligence on social good: a study on analytic-based hierarchy,https://ieeexplore.ieee.org/document/10425428/,"AI is the technology that has transformed many industries and various sectors which include healthcare, finance education, agriculture etc. The purpose of this study is to explore the impact of AI on social good. More specifically, this research prioritizes the domains and subdomains of AI. Initially, the literature has been extensively reviewed to identify the uncharted implications of AI. To conduct the study authors have adopted the secondary research and a theoretical model has been developed to prioritize the domains at the time of contingency; for that purpose, an Analytical Hierarchical Process, a tool of MCDM, is used. The findings of the study concluded with a theoretical model presenting the rankings and maximum global weights. The current study has been conducted in the Indian context. Therefore, this can be considered as a limitation of research. Current study significantly contributes to the body of literature in this field and presents a hierarchical model of artificial intelligence domains and sub domains impacting social good.",unknown,183
10.1109/icdcs60910.2024.00085,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',2024 IEEE 44th International Conference on Distributed Computing Systems (ICDCS),IEEE,2024-07-26 00:00:00,ieeexplore,toward free-riding attack on cross-silo federated learning through evolutionary game,https://ieeexplore.ieee.org/document/10631028/,"In cross-silo federated learning (FL), due to the heterogeneous participants, free-riders can utilize information asymmetry to make profits without performing any local model training. Free-riding attack poses possibilities and opportunities for unfairness and can seriously impair the operation of the FL ecosystem. It motivates our work to explore and characterize the unique features of free-riding attack, which differ from other attacks such as poisoning attacks. In this paper, we propose an evolutionary public goods game-based incentive model (Fed-EPG), which makes the first attempt to construct the interaction model among the participants through the evolutionary public goods game. Specifically, we consider both the public good characteristics of cross-silo FL models as well as the bounded rationality and incomplete information of competitors. We first introduce asymmetric environmental feedback to represent reward and punishment strategies in evolutionary game, and then adopt a multi-segment nonlinear control method to dynamically adjust the rewards and punishments among the participants, which achieves the incentive for the participants to cooperate stably during the training process. Experimental results validate that our incentive model is effective in the mitigation of free-riding. attacks.",unknown,184
10.1109/access.2024.3426329,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,uncovering concerns of citizens through machine learning and social network sentiment analysis,https://ieeexplore.ieee.org/document/10591750/,"Artificial Intelligence and Machine Learning (AI/ML) as analytical tools can be applied across multiple social domains. Thus, these tools are being deployed in several ways to address societal issues and concerns for “social good”. For instance, AI/ML has applicable use cases for crisis response, economic empowerment, educational demands, environmental challenges, equality and inclusion, health and hunger, and security and justice. In this work, we seek to explore the power and capability of AI/ML in understanding citizens’ engagement, which can improve governance and smart city deployment. Specifically, we studied the views expressed by online users about the city of Saskatoon in Canada. The analyzed views have become a value chain that community leaders can use to improve the governance structure of the city. In the study, we extracted 114,390 comments from Reddit (i.e., Saskatoon subreddit posts) between January 1, 2019, and September 20, 2023, to discover topics to highlight citizens’ concerns. We compare the performance of three major topic models, namely, Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), and BERTopic with a K-means clustering algorithm in the discovery of topics from the collected Reddit comments. The BERTopic with the K-means clustering algorithm achieved the highest coherence score of approximately 0.64 in the extraction of 25 topics from the dataset. Our findings showed that BERTopic can discover coherent and diverse topics compared to LDA and NMF. We found 12 underlying themes by merging related topics. Also, we leveraged SiEBERT (a pre-trained transformer model), 4 supervised ML models, and VADER (a lexical sentiment analysis classifier) to identify the sentiments expressed in each theme. The SiEBERT model outperformed the other sentiment classifiers with an accuracy of 89% in the prediction of sentiments. The research discovered factors for smart city engagement such as Housing and Facilities, Education, Downtown Development, Tourism and Entertainment, Policing, Healthcare, Online Community, and Cost.",unknown,185
10.1109/compsac51774.2021.00054,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE,2021-07-16 00:00:00,ieeexplore,a bayesian framework for supporting predictive analytics over big transportation data,https://ieeexplore.ieee.org/document/9529442/,"In the current era of big data, huge volumes of valuable data can be easily generated and collected at a rapid velocity from a wide variety of rich data sources. In recent years, the initiates of open data also led to the willingness of many government, researchers, and organizations to share their data and make them publicly accessible. An example of open big data is transportation data such as public bus performance data. Analyzing these open big data can be for social good. For instance, by analyzing and mining the public bus performance data, the bus service providers could get an insight on the on-time performance or delay in bus services. By taking appropriate actions (e.g., adding more buses, rerouting some buses routes, etc.) could enhance rider experience. In this paper, we present a Bayesian framework for supporting predictive analytics over big transportation data. Specifically, our framework consists of several Bayesian networks to predict whether a bus arrive late than its scheduled time at a given bus stop. We analyze and determine the network configurations and/or parameter permutation to produce the best result for each (bus stop, bus route, arrival time)-triplet. Evaluation on an open big data for public transit bus from a North American city shows the effectiveness and practicality of our Bayesian framework in supporting predictive analytics on big open data for transportation analytics.",unknown,186
10.1109/mlbdbi48998.2019.00012,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',"2019 International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",IEEE,2019-11-10 00:00:00,ieeexplore,a framework for measuring the influence of government social media accounts,https://ieeexplore.ieee.org/document/8945747/,"The huge flow of data generated by social media helps governments identify the scope of problems and establish responsive political mobilization models. The extent of the government's influence on the public through social media is a key factor in this process. Both existing research and practical applications tend to evaluate the influence of government social media accounts (GSMAs) according to their role in information dissemination, which may not fully reflect the ability of GSMAs to influence the effectiveness of their political agenda. This study attempted to establish a policy-agenda-oriented GSMA evaluation framework.",unknown,187
10.1016/j.eswa.2024.125396,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85204902299,scopus,2025-01-15,scopus,predicting costs of local public bus transport services through machine learning methods,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85204902299&origin=inward,"
                  The present study developed several machine learning-based cost models to predict an efficient total economic cost per vehicle revenue-mile of urban public bus transport. The models were trained on a built-in dataset from 269 transit agencies providing urban services in the United States from 2015 to 2019. A feature selection strategy was implemented, finding that, for each proposed model, a subset of features determined a large impact on unit cost. These “core” features included commercial speed, average salary expenses per employee, vehicle productivity, and fleet ownership cost per vehicle. Machine learning techniques outperformed the linear regression method in terms of predictive power and robustness (understood as the dispersion of predictive power measures over the training sets). Based on SHAP values, the sensitivity analyses showed that the proposed models could be used to predict the impact of changes in some critical features on corresponding unit costs. The results may be useful for: (i) introducing regulatory constraints to the allocation of national public resources to local public bus transport services, aimed at minimizing the resources needed to provide a given level of service; (ii) defining the maximum economic compensation required by firms involved in competitive tendering for the allotment of service concessions, or firms with monopoly rights (by political choice and/or local public ownership); and (iii) improving service contract management and design by identifying key cost drivers of transit services.
               ",unknown,189
10.1016/j.tbs.2024.100914,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85205149280,scopus,2025-01-01,scopus,the relationship between accessibility and land prices: a focus on accessibility to transit in the 15-min city,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85205149280&origin=inward,"
                  In the context of local residents’ travel activities, access to transportation facilities is crucial and a key factor in achieving the goals of a 15-min city. However, current research on accessibility and land prices seldom fully considers the exploration of the relationship between the two using machine learning models. In this study, we conduct an analysis based on land price and accessibility in Beijing. Firstly, factors such as accessibility, population density, land use types, the number of Points of Interest (POI), and housing prices are selected as independent variables for the model, with land price as the dependent variable. Secondly, the CatBoost model is employed to investigate the complex relationship between accessibility to transit and land prices. Finally, the study introduces the XGBoost model and the ordinary least squares (OLS) method for comparison, validating the effectiveness of the CatBoost model in studying the complex relationship between accessibility and land prices through the comparison of model performance evaluation metrics. A clear nonlinear relationship exists between public transportation accessibility and land prices. Accessibility to bus within a 15-min bicycle is positively associated with land price; its trend shows a sharp increase followed by a gradual increase, and then another sharp increase. Accessibility to metro within the multistage distance threshold walk has a roughly positive association with land price within a certain range. While accessibility to bus within a 15-min walk and accessibility to bus within the multistage distance threshold walk have a negative association with land price. The nonlinear patterns between the four accessibility variables and land prices differ and exhibit significant spatial heterogeneity. Both housing prices and population density positively correlate with land prices. Housing prices sharply increase, then gradually, while population density gradually rises, then sharply. This paper explores the relationship between the two, reinforcing research on the mutual influence of public transportation facilities and surrounding environmental elements. It provides a theoretical basis for promoting transit-oriented development (TOD) and city planning.
               ",unknown,190
10.1016/j.aap.2024.107801,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85205385235,scopus,2024-12-01,scopus,uncertainty-aware probabilistic graph neural networks for road-level traffic crash prediction,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85205385235&origin=inward,"Traffic crashes present substantial challenges to human safety and socio-economic development in urban areas. Developing a reliable and responsible traffic crash prediction model is crucial to address growing public safety concerns and improve the safety of urban mobility systems. Traditional methods face limitations at fine spatiotemporal scales due to the sporadic nature of high-risk crashes and the predominance of non-crash characteristics. Furthermore, while most current models show promising occurrence prediction, they overlook the uncertainties arising from the inherent nature of crashes, and then fail to adequately map the hierarchical ranking of crash risk values for more precise insights. To address these issues, we introduce the S patio t emporal Z ero- I nflated T wee d ie G raph N eural N etworks (STZITD-GNN), the first uncertainty-aware probabilistic graph deep learning model in road-level daily-basis traffic crash prediction for multi-steps. Our model combines the interpretability of the statistical Tweedie family with the predictive power of graph neural networks, excelling in predicting a comprehensive range of crash risks. The decoder employs a compound Tweedie model, handling the non-Gaussian distribution inherent in crash data, with a zero-inflated component for accurately identifying non-crash cases and low-risk roads. The model accurately predicts and differentiates between high-risk, low-risk, and no-risk scenarios, providing a holistic view of road safety that accounts for the full spectrum of probability and severity of crashes. Empirical tests using real-world traffic data from London, UK, demonstrate that the STZITD-GNN surpasses other baseline models across multiple benchmarks, including a reduction in regression error of up to 34.60% in point estimation metrics and an improvement of above 47% in interval-based uncertainty metrics.",unknown,191
10.1016/j.eswa.2024.124382,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85196018358,scopus,2024-12-01,scopus,identification of flood depth levels in urban waterlogging disaster caused by rainstorm using a cbam-improved resnet50,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85196018358&origin=inward,"
                  The global environmental changes have led to frequent occurrences of extreme rainfall, causing urban waterlogging. It is essential to quickly and accurately identify flooding as well as the depth levels. Therefore, Convolutional block attention module (CBAM)-improved ResNet50 is proposed for training to obtain flood depth levels (FDLs) recognition AI models. In this model, CBAM helps the model focus on important features for classifying FDLs, improving accuracy. It also streamlines the fully connected layers, reducing parameters, cutting computational costs, and preventing overfitting. A total of 6294 images related to urban waterlogging were collected for deep learning. The CBAM-improved ResNet50 algorithm achieved the highest test set accuracy at 92.45%, outperforming other algorithms including AlexNet (62.25%), MobileNet-V2 (64.67%), GoogleNet (75.5%), and ResNet50 (79.2%). It also demonstrates good predictive performance and generalization capabilities for urban waterlogging situations against various backgrounds. The computer vision recognition of the FDLs was explained through Gradient-weighted Class Activation Mapping in visualized images. Finally, through the established WeChat mini-program and AI model, the public can access the current FDLs at any time and place. This study provides information on the FDLs, offering a comprehensive overview of the severity of floods in the early stages of emergency response.
               ",unknown,197
10.1016/j.bdr.2024.100483,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85201100613,scopus,2024-11-28,scopus,leveraging social computing for epidemic surveillance: a case study,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85201100613&origin=inward,"
                  Social media platforms have become a popular source of information for real-time monitoring of events and user behavior. In particular, Twitter provides invaluable information related to diseases and public health to build real-time disease surveillance systems. Effective use of such social media platforms for public health surveillance requires data-driven AI models which are hindered by the difficult, expensive, and time-consuming task of collecting high-quality and large-scale datasets. In this paper, we build and analyze the Epidemic TweetBank (EpiBank) dataset containing 271 million English tweets related to six epidemic-prone diseases COVID19, Flu, Hepatitis, Dengue, Malaria, and HIV/AIDs. For this purpose, we develop a tool of ESS-T (Epidemic Surveillance Study via Twitter) which collects tweets according to provided input parameters and keywords. Also, our tool assigns location to tweets with 95% accuracy value and performs analysis of collected tweets focusing on temporal distribution, spatial patterns, users, entities, sentiment, and misinformation. Leveraging ESS-T, we build two geo-tagged datasets of EpiBank-global and EpiBank-Pak containing 86 million tweets from 190 countries and 2.6 million tweets from Pakistan, respectively. Our spatial analysis of EpiBank-global for COVID19, Malaria, and Dengue indicates that our framework correctly identifies high-risk epidemic-prone countries according to World Health Organization (WHO) statistics.
               ",unknown,202
10.1016/j.scitotenv.2024.175830,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85202289284,scopus,2024-11-15,scopus,estimating epidemic trajectories of sars-cov-2 and influenza a virus based on wastewater monitoring and a novel machine learning algorithm,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85202289284&origin=inward,"
                  The COVID-19 pandemic has altered the circulation of non-SARS-CoV-2 respiratory viruses. In this study, we carried out wastewater surveillance of SARS-CoV-2 and influenza A virus (IAV) in three key port cities in China through real-time quantitative PCR (RT–qPCR). Next, a novel machine learning algorithm (MLA) based on Gaussian model and random forest model was used to predict the epidemic trajectories of SARS-CoV-2 and IAV. The results showed that from February 2023 to January 2024, three port cities experienced two waves of SARS-CoV-2 infection, which peaked in late-May and late-August 2023, respectively. Two waves of IAV were observed in the spring and winter of 2023, respectively with considerable variations in terms of onset/offset date and duration. Furthermore, we employed MLA to extract the key features of epidemic trajectories of SARS-CoV-2 and IAV from February 3rd, to October 15th, 2023, and thereby predicted the epidemic trends of SARS-CoV-2 and IAV from October 16th, 2023 to April 22nd, 2024, which showed high consistency with the observed values. These collective findings offer an important understanding of SARS-CoV-2 and IAV epidemics, suggesting that wastewater surveillance together with MLA emerges as a powerful tool for risk assessment of respiratory viral diseases and improving public health preparedness.
               ",unknown,203
10.1016/j.clsr.2024.106050,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85203147962,scopus,2024-11-01,scopus,fair and efficient asylum procedures and artificial intelligence: quo vadis due process?,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85203147962&origin=inward,"
                  In a context of high pressure on national asylum systems and a strive for efficiency, public authorities in Europe are increasingly exploring the potential of artificial intelligence-driven technologies in the asylum process. The use of this technology in the field of asylum is a growing but contentious topic, which raises important normative questions and concerns. In this context, this paper aims to analyse the potential implications for fair asylum procedures when artificial intelligence (AI) assists decision-making. Fair asylum procedures, or due process, are a central condition for guaranteeing the right to asylum and preventing unlawful refoulement, and overall ensuring trust in the asylum adjudication system. After revisiting the theoretical foundations of the concept of fair procedures, this paper develops a normative framework that can guide further reflection on the use of AI in asylum procedures. It thereby analyses the concepts that are key to the debate on the use of AI in decision-making: accuracy, efficiency but also participation. Then, drawing on scholarship in both political science and computer science, it explores potential challenges for the core values of fair procedures, considering both technical and non-technical challenges. This paper concludes that while AI promises efficiency gains for the administration, it identifies important challenges for accuracy and participation. On the basis of these considerations, it highlights the questions that should be asked and answered in order to protect the core values of fair asylum procedures.
               ",unknown,204
10.1016/j.ufug.2024.128446,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85202918559,scopus,2024-11-01,scopus,reassessing urban park accessibility: an improved two-step floating catchment area method based on the physical activity services perspective,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85202918559&origin=inward,"
                  Urban parks, as important carrier of outdoor physical activity, can effectively reduce the incidence of chronic diseases and promote public health. The accessibility of parks directly affects residents’ levels of physical activity. However, current accessibility assessments do not consider physical activity needs into account and are unable to adapt to the current developmental needs of a healthy China. We propose a framework for evaluating park accessibility oriented towards a physical activity service perspective. First, we introduced a park attractiveness index based on physical activity preferences. The index encompasses not only objective park features but also subjective environmental perception features, which are evaluated using deep-learning techniques. Furthermore, the size of the park catchment area was correlated with the park attractiveness index, and the likelihood of park selection based on people's physical activity preferences was computed. Fuzhou City's Gulou District was selected for this case study. The results show the following: (1) Our proposed evaluation model can identify more underserved neighborhoods in scenarios geared toward physical activity requirements than traditional approaches. (2) The distribution of parks in the Gulou District was not sufficiently balanced, with the accessibility value at the 90th percentile only slightly higher than the average value of 6.35. There was obvious spatial differentiation, with a pattern of high values in the central area and low values in the surrounding areas. (3) Approximately 12.70 % of the population is located within service-blind zones and the overall level of accessibility is poor. This study offers novel ideas and approaches for optimizing the spatial layout of urban parks within a health-oriented framework. This aids planners and policymakers in enhancing the spatial layout of parks and ensuring equitable access to urban parks that foster physical activity.
               ",unknown,205
10.1016/j.envpol.2024.124785,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85201720584,scopus,2024-11-01,scopus,mapping the constituent preference of tree species for capturing particulate matter on leaf surfaces using single-particle mass spectrometry and supervised machine learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85201720584&origin=inward,"
                  Respiratory health is negatively influenced by the dimensions and constituents of particulate matter (PM). Although mass concentration is widely acknowledged to be key to assessing dust retention by urban trees, the role of plant leaves in filtering PM from the urban atmosphere, particularly regarding the particle dimensions and chemical constituents of retained PM on the leaf, remains elusive. Here we combined single-particle aerosol mass spectrometry and a particle resuspension chamber to investigate how urban tree species capture PM constituents. Results indicate that leaves are efficient in capturing relatively larger particles (1.0–2.0 μm). Compositionally, airborne particles were mostly composed of elemental carbon (EC, 20%), organic carbon (OC, 17%), and secondary reaction products (13%). However, leaf surfaces revealed a preference for retaining crustal species, comprising 55% of captured particulates. Notably, specific tree species demonstrated varied affinities for different PM constituents: Osmanthus fragrans Lour. predominantly captured levoglucosan (LEV), indicative of its efficiency against biomass burning particles, whereas Cinnamomum camphora (L.) J.Presl and Sabina chinensis var. kaizuca W.C.Cheng & W.T.Wang were more effective in capturing heavy metals (HMs). XGBoost modelling identified indicator ions, e.g., CN−, NO3
                     −, NO2
                     −, PO3
                     −, with SHAP values surpassing 0.035, suggesting a preferential adsorption of these ions among different tree species. These findings demonstrate that the particulate capture efficiency of urban tree species varies with species-specific leaf properties, particularly in their ability to selectively adsorb particles containing hazardous constituents such as LEV and HMs. This study provides a scientific basis for the strategic selection of tree species in urban forestry initiatives aimed at improving air quality and public health.
               ",unknown,206
10.1016/j.techfore.2024.123678,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85201707691,scopus,2024-11-01,scopus,public perceptions and acceptance of artificial intelligence humanoid bots/robots: evidence from turkey,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85201707691&origin=inward,"
                  The development of artificial intelligence (AI) humanoid bots/robots (AIHB/R), which integrate AI with robot technology to emulate human traits, has swiftly evolved and found applications in several sectors like healthcare, education, home support, industrial production, and entertainment. AIHB/R's decision-making process has become more complex due to incorporating AI, a fascinating element affecting public perceptions and attitudes towards these robots. This paper investigates this topic in the Turkish context based on data collected from 485 adults. A general finding is that participants have positive perceptions and attitudes towards (AIHB/R) and some concerns. We found significant gender differences between male and female participants regarding the use of AIHB/R in care and treatment services (CTS), administration and justice (AJ), and in the future (FC). No gender difference was found when using religious practice (RP). The educational level statistically correlated significantly with the total scale scores and the four sub-domains. CTS has a positive and moderately significant effect on accepting AIHB/R applications. The R2 value for this domain showed the highest explanatory power of the model with 0.829.
               ",unknown,207
10.1016/j.scs.2024.105742,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85201460186,scopus,2024-11-01,scopus,mapping heterogeneity: spatially explicit machine learning approaches for urban value uplift characterisation and prediction,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85201460186&origin=inward,"Understanding urban value uplift is essential for guiding public efforts towards vibrant economies and inclusive communities. Current research, however, often overlooks non-economic dimensions of urban values, resulting in biased socioeconomic outcomes and neglected socio-spatial heterogeneity for sustainable urban planning. This study employs spatially explicit machine learning (ML) approaches to comprehensively investigate the characterisation of urban value uplift and its predictors. Applying GWPCA to the municipality of Shanghai, the results reveal significant heterogeneity in urban value uplift characterisation. While the municipality shows a generally homogeneous polycentric growth pattern, central areas exhibit economic growth, and peripheral areas are enhanced with place value. Additionally, combining a Spatially Enhanced CatBoost algorithm (CTB-S) and SHAPley value, analysis on uplift predictors indicates that urban policy, among many other features, tends to catalyse further economic uplift specifically in central areas. Our findings underscore the inherent heterogeneity in urban change process, necessitating spatially tailored approaches for policymakers to capture urban value uplifts. By addressing the unique needs of different urban areas, policymakers can promote sustainable urban environments that ensure equitable economic and social benefits across all communities.",unknown,208
10.1016/j.nucengdes.2024.113527,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85201299968,scopus,2024-11-01,scopus,radiological impact assessment of a dirty bomb using hotspot code and ai: insights from spent nuclear fuel isotopes,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85201299968&origin=inward,"
                  This study evaluates the potential radiological effects resulting from the use of a Radiological Dispersal Device (RDD), commonly known as a “dirty bomb,” in Dhamar City, Yemen, focusing on its impact on human health. Seasonal atmospheric variations were analyzed using the HotSpot Health Physics Code to model the Total Effective Dose Equivalent (TEDE) distribution following an explosion. The TEDE’s effect on different human organs was assessed, considering both external and internal radiation exposures. The study demonstrated that winter nights posed the highest radiation risk, with dose distribution areas of 0.018 km2, 0.14 km2, and 3.3 km2 for inner, middle, and outer zones, respectively, surpassing other seasonal and daily values. Conversely, summer recorded the lowest radiation spread. A detailed temporal analysis during winter nights showed an initial TEDE of 9.5 Sv at 0.1 min post-explosion, which decreased exponentially to 1.3 Sv after 2000 min (33.3 h) and further to 0.0038 Sv after 6500 min (108.3 h) with the aid of AI-enhanced “Long Short Term Memory Networks (LSTM)”. The highest radiation doses to human organs within a 100 m2 area during the initial 10 min post-detonation were significantly above permissible limits, with critical organs like the surface bone, liver, and red marrow receiving doses of 68 Sv, 12 Sv, and 5.5 Sv, respectively. These findings underline the need for robust emergency response strategies and long-term health monitoring to mitigate the adverse health impacts of RDDs. The comprehensive assessment presented in this study, enhanced by artificial intelligence, provides essential data for improving public health safety and emergency preparedness in regions susceptible to radiological threats.
               ",unknown,209
10.1016/j.simpat.2024.102983,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85200647087,scopus,2024-11-01,scopus,a simulation-based approach for decision-support in healthcare processes,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85200647087&origin=inward,"This article explores the application of simulation-driven decision support systems in the context of healthcare processes. The findings obtained from the review of related research suggest that simulation models have been widely used in the healthcare sector as valuable tools to aid decision-making aimed at the improvement of healthcare process management. However, the analyzed works lack sufficient evidence regarding the utilization of systematic approaches in the development of their proposals. We have conducted a research effort in this field, resulting in the design of a conceptual framework that guides in the systematic development of decision support systems centered around simulation models. The framework includes a specific methodology for developing simulation models, with simulation modeling experimentation serving as a valuable technique for assessing the impact of potential changes on the performance of the process. To illustrate the applicability and practical value of the proposed framework, we have developed an application case within the context of the Urgent Healthcare for Women's (UHW) process in a public hospital in Andalusia, Spain.",unknown,211
10.1016/j.scs.2024.105567,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85199949528,scopus,2024-10-15,scopus,optimal planning for electric vehicle fast charging stations placements in a city scale using an advantage actor-critic deep reinforcement learning and geospatial analysis,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85199949528&origin=inward,"
                  The transition to Electric Vehicles (EVs) for reducing urban greenhouse gas emissions is hindered by the lack of public charging infrastructure, particularly fast-charging stations. Given that electric vehicle fast charging stations (EVFCS) can burden the electricity grid, it is crucial for EVFCS to adopt sustainable energy supply methods while accommodating the growing demands of EVs. Despite recent research efforts to optimize the placement of renewable-powered EV charging stations, current planning methods face challenges when applied to a complex city scale and integrating with renewable energy resources. This study thus introduces a robust decision-making model for optimal EVFCS placement planning integrated with solar power supply in a large and complex urban environment (e.g., Chicago), utilizing an advantage actor-critic (A2C) deep reinforcement learning (DRL) approach. The model balances traffic demand with energy supply, strategically placing charging stations in areas with high traffic density and solar potential. As a result, the model is used to optimally place 1,000 charging stations with a random starting search approach, achieving total reward values of 74.30 %, and estimated the capacities of potential EVFCS. This study can inform the identification of suitable locations to advance the microgrid-based charging infrastructure systems in large urban environments.
               ",unknown,212
10.1016/j.jiph.2024.102541,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85203539353,scopus,2024-10-01,scopus,artificial intelligence-clinical decision support system for enhanced infectious disease management: accelerating ceftazidime-avibactam resistance detection in klebsiella pneumoniae,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85203539353&origin=inward,"Background Effective and rapid diagnostic strategies are required to manage antibiotic resistance in Klebsiella pneumonia (KP). This study aimed to design an artificial intelligence-clinical decision support system (AI-CDSS) using matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) and machine learning for the rapid detection of ceftazidime-avibactam (CZA) resistance in KP to improve clinical decision-making processes. Methods Out of 107,721 bacterial samples, 675 specimens of KP with suspected multi-drug resistance were selected. These specimens were collected from a tertiary hospital and four secondary hospitals between 2022 and 2023 to evaluate CZA resistance. We used MALDI-TOF MS and machine learning to develop an AI-CDSS with enhanced speed of resistance detection. Results Machine learning models, especially light gradient boosting machines (LGBM), exhibited an area under the curve (AUC) of 0.95, indicating high accuracy. The predictive models formed the core of our newly developed AI-CDSS, enabling clinical decisions quicker than traditional methods using culture and antibiotic susceptibility testing by a day. Conclusions The study confirms that MALDI-TOF MS, integrated with machine learning, can swiftly detect CZA resistance. Incorporating this insight into an AI-CDSS could transform clinical workflows, giving healthcare professionals immediate, crucial insights for shaping treatment plans. This approach promises to be a template for future anti-resistance strategies, emphasizing the vital importance of advanced diagnostics in enhancing public health outcomes.",unknown,214
10.1016/j.jacadv.2024.101182,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85203809443,scopus,2024-09-01,scopus,trends in glucagon-like peptide-1 receptor agonist social media posts using artificial intelligence,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85203809443&origin=inward,"Background Glucagon-like peptide-1 receptor agonists (GLP-1RAs) have surged in popularity in recent years, with discussions about their on-label and off-label use spilling into the public forum. No study has analyzed online discussions about GLP-1RAs. Objectives The purpose of this study was to analyze perceptions of GLP-1RAs on social media. Methods We analyzed GLP-1RA-related posts on Reddit between May 28, 2013, and June 1, 2023. All posts were identified that included generic or brand names of GLP-1RAs. Post volume on Reddit was compared to search interest on Google over time. An artificial intelligence (AI) pipeline consisting of a semi-supervised natural language processing model (Bidirectional Encoder Representations from Transformers [BERT]), a dimensionality reduction technique, and a clustering algorithm was used to cluster posts into related topics. Discussion sentiment was classified using a pretrained BERT model and assessed qualitatively. Results 14,390 GLP-1RA-related Reddit posts by 8,412 authors were identified. Ninety-four percent of posts were created after 2021, consistent with search interest trend on Google. We used the AI model to categorize posts into 30 topics which were hierarchically grouped by the model based on shared content. Posts were identified among communities for individuals with diabetes and obesity, as well as for diseases without a Food and Drug Administration-approved indication. Most posts had a negative sentiment using the pretrained model, acknowledging the pretrained model is at risk for misclassifying posts. Conclusions AI can generate insights on perceptions of GLP-1RAs on social media. Common themes included success stories of improving diabetes and obesity management, struggles with insurance coverage, and questions regarding diet, side effects, and medication administration.",unknown,225
10.1016/j.chemosphere.2024.143096,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85201454615,scopus,2024-09-01,scopus,"elevating hourly pm<inf>2.5</inf> forecasting in istanbul, türkiye: leveraging era5 reanalysis and genetic algorithms in a comparative machine learning model analysis",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85201454615&origin=inward,"
                  Rapid urbanization and industrialization have intensified air pollution, posing severe health risks and necessitating accurate PM2.5 predictions for effective urban air quality management. This study distinguishes itself by utilizing high-resolution ERA5 reanalysis data for a grid-based spatial analysis of Istanbul, Türkiye, a densely populated city with diverse pollutant sources. It assesses the predictive accuracy of advanced machine learning (ML) models—Multiple Linear Regression (MLR), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting (LGB), Random Forest (RF), and Nonlinear Autoregressive with Exogenous Inputs (NARX). Notably, it introduces genetic algorithm optimization for the NARX model to enhance its performance. The models were trained on hourly PM2.5 concentrations from twenty monitoring stations across 2020–2021. Istanbul was divided into seven regions based on ERA5 grid distributions to examine PM2.5 spatial variability. Seventeen input variables from ERA5, including meteorological, land cover, and vegetation parameters, were analyzed using the Neighborhood Component Analysis (NCA) method to identify the most predictive variables. Comparative analysis showed that while all models provided valuable insights (RF > LGB > XGB > MLR), the NARX model outperformed them, particularly with the complex dataset used. The NARX model achieved a high R-value (0.89), low RMSE (5.24 μg/m³), and low MAE (2.94 μg/m³). It performed best in autumn and winter, with the highest accuracy in Region-1 (R-value 0.94) and the lowest in Region-5 (R-value 0.75). This study's success in a complex urban setting with limited monitoring underscores the robustness of the NARX model and the methodology's potential for global application in similar urban contexts. By addressing temporal and spatial variability in air quality predictions, this research sets a new benchmark and highlights the importance of advanced data analysis techniques for developing targeted pollution control strategies and public health policies.
               ",unknown,228
10.1016/j.jenvman.2024.122051,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85200339762,scopus,2024-09-01,scopus,assessing allergy risk from ornamental trees in a city: integrating open access remote sensing data with pollen measurements,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85200339762&origin=inward,"Platanus sp. pl. (plane trees) are common ornamental tree in Poland that produces a large amount of wind-transported pollen, which contains proteins that induce allergy symptoms. Allergy sufferers can limit their contact with pollen by avoiding places with high pollen concentrations, which are restricted mainly to areas close to plane trees. Their location is thus important, but creating a detailed street tree inventory is expensive and time-consuming. However, high-resolution remote sensing data provide an opportunity to detect the location of specific plants. But acquiring high-resolution spatial data of good quality also incurs costs and requires regular updates. Therefore, this study explored the potential of using open access remote sensing data to detect plane trees in the highly urbanized environment of Poznań (western Poland). Airborne light detection and ranging (LiDAR) was used to detect training treetops, which were subsequently marked as young plane trees, mature plane trees, other trees or artefacts. Spectral and spatial variables were extracted from circular buffers (r = 1 m) around the treetops to minimize the influence of shadows and crown overlap. A random forest machine learning algorithm was applied to assess the importance of variables and classify the treetops within a radius of 6.2 km around the functioning pollen monitoring station. The model performed well during 10-fold cross-validation (overall accuracy ≈ 92%). The predicted Platanus sp. pl. locations, aggregated according to 16 wind directions, were significantly correlated with the hourly pollen concentrations. Based on the correlation values, we established a threshold of prediction confidence, which allowed us to reduce the fraction of false-positive predictions. We proposed the spatially continuous index of airborne pollen exposure probability, which can be useful for allergy sufferers. The results showed that open-access geodata in Poland can be applied to recognize major local sources of plane pollen.",unknown,232
10.1016/j.giq.2024.101962,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85199797451,scopus,2024-09-01,scopus,toward a person-environment fit framework for artificial intelligence implementation in the public sector,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85199797451&origin=inward,"
                  Using an embedded mixed method design, we compared a nationally representative sample of US adults and a sample of US-based emergency managers (EM) on their attitudes toward artificial intelligence (AI) and their intentions to rely on AI in a set of decision-making scenarios relevant to emergency management. Emergency managers reported significantly less positive attitudes toward AI and were less likely to rely on AI for decisions compared to the nationally representative sample. Our analysis of EMs' open-ended responses explaining their choices to use or not use AI-based solutions reflected specific concerns about implementation rather than wariness toward AI generally. These concerns included the complexity of the potential outcomes in the scenarios, the value they placed on human input and their own extensive experience, procedural concerns, collaborative decision-making, team-building, training, and the ethical implications of decisions, rather than a rejection of AI more generally. Managers' insights integrated with our quantitative findings led to a person-environment fit framework for AI implementation in the public sector. Our findings and framework have implications for how AI systems should be introduced and integrated in emergency managerial contexts and in public sector organizations more generally. Public managers' perceptions and intentions to use AI and organizational oversight processes are at least as important as technology design considerations when public sector organizations are considering the deployment of AI.
               ",unknown,233
10.1016/j.jsis.2024.101848,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85198122860,scopus,2024-09-01,scopus,fusing domain knowledge with machine learning: a public sector perspective,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85198122860&origin=inward,"Machine learning (ML) offers widely-recognized, but complex, opportunities for both public and private sector organizations to generate value from data. A key requirement is that organizations must find ways to develop new knowledge by merging crucial ‘domain knowledge’ of experts in relevant fields with ‘machine knowledge’, i.e., data that can be used to inform predictive models. In this paper, we argue that understanding the process of generating such knowledge is essential to strategically develop ML. In efforts to contribute to such understanding, we examine the generation of new knowledge from domain knowledge through ML via an exploratory study of two cases in the Swedish public sector. The findings reveal the roles of three mechanisms – dubbed consolidation, algorithmic mediation, and naturalization – in tying domain knowledge to machine knowledge. The study contributes a theory of knowledge production related to organizational use of ML, with important implications for its strategic governance, particularly in the public sector.",unknown,235
10.1016/j.techsoc.2024.102644,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85196533015,scopus,2024-09-01,scopus,addressing the notion of trust around chatgpt in the high-stakes use case of insurance,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85196533015&origin=inward,"The public discourse concerning the level of (dis)trust in ChatGPT and other applications based on large language models (LLMs) is loaded with generic, dread risk terms, while the heterogeneity of relevant theoretical concepts and empirical measurements of trust further impedes in-depth analysis. Thus, a more nuanced understanding of the factors driving the trust judgment call is essential to avoid unwarranted trust. In this commentary paper, we propose that addressing the notion of trust in consumer-facing LLM-based systems across the insurance industry can confer enhanced specificity to this debate. The concept and role of trust are germane to this particular setting due to the highly intangible nature of the product coupled with elevated levels of risk, complexity, and information asymmetry. Moreover, widespread use of LLMs in this sector is to be expected, given the vast array of text documents, particularly general policy conditions or claims protocols. Insurance as a practice is highly relevant to the welfare of citizens and has numerous spillover effects on wider public policy areas. We therefore argue that a domain-specific approach to good AI governance is essential to avoid negative externalities around financial inclusion. Indeed, as a constitutive element of trust, vulnerability is particularly challenging within this high-stakes set of transactions, with the adoption of LLMs adding to the socio-ethical risks. In light of this, our commentary provides a valuable baseline to support regulators and policymakers in unravelling the profound socioeconomic consequences that may arise from adopting consumer-facing LLMs in insurance.",unknown,237
10.1016/j.omega.2024.103105,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85192144152,scopus,2024-09-01,scopus,leveraging explainable artificial intelligence in understanding public transportation usage rates for sustainable development,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85192144152&origin=inward,"
                  Public transportation usage prediction is valuable for the sustainable development of transportation systems, particularly in crowded megacities. Machine learning technologies are of great interest for predicting public transportation usage. While these technologies outperform many other techniques, they suffer from limited interpretability. Explainable artificial intelligence (XAI) tools and techniques that offer post-hoc explanations of the obtained predictions are gaining popularity. This paper proposes an advanced tree-based ensemble algorithm for public transportation usage rate prediction. We aim to explain the predictions both with the most widely used technique of XAI, Shapley additive explanation (SHAP) and in the light of the rules presented. To predict the total public transportation usage, the proposed model combines all types of public transportation, categorized as ferry, railway, and bus, unlike most existing studies focusing on a single kind of public transport. Besides the sort of transportation, the day of the week, whether the day is special, and the daily ratio of passenger types were identified as model features for predicting the daily usage of each type of public transportation. We tested the proposed model using an open data set from Izmir City, Turkey. While the model had superior prediction performance, the explanations showed that the type of public transportation, weekday, and the ratio of full-fare passengers have the highest SHAP values, and the model features have many interactions. We also validated our results using an online data set showing Google search trends.
               ",unknown,238
10.1016/j.scitotenv.2024.173749,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85195260475,scopus,2024-08-25,scopus,decomposing pm<inf>2.5</inf> concentrations in urban environments into meaningful factors: 1. separating the contribution of local anthropogenic activities from background and long-range transport,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85195260475&origin=inward,"
                  Fine particulate matter (PM2.5) is a complex mixture of aerosol particles with varying properties and sources, both local and distant. In areas lacking detailed monitoring of PM2.5 speciation, the common source-apportionment analyses are not applicable. This study demonstrates an alternative framework for estimating sources and processes that affect observed PM2.5 concentrations when information on the particle composition is unavailable. Eight years (2012–2019) of half-hourly PM2.5 observations from 10 air quality monitoring (AQM) stations, clustered according to their airmass transport sector were analyzed, using Non-negative Matrix Factorization (NMF). Factors were determined based on their variation in time, space, and between airmass sectors. Employing a supervised machine-learning model provided insights into the relationships between the extracted factors, meteorological parameters and co-measured airborne pollutants. Factor interpretations were evaluated through comparisons with measurements of PM2.5 species from a nearby Surface PARTiculate mAtter Network (SPARTAN) station. The NMF successfully separated background factors from an urban anthropogenic-activity factor, with the latter accounting for approximately 60 % of the observed PM2.5 levels in Tel Aviv (∼
                        10
                        ±
                        6
                        
                        μg
                        /
                        
                           m
                           3
                        
                     ). Positive monotonic relationships were observed between the PM2.5 urban anthropogenic-activity factor and measurements of nitrogen oxides (NOx) and absolute humidity (AH), representing the impact of traffic emissions and hygroscopic growth, respectively. The summer background factor was found to represent long-range transport (LRT) from Europe, showing a good agreement (R2 = 0.81) with ammonium sulphate concentrations. Our results demonstrate that a spatial NMF analysis can reliably estimate contributions of different sources with distinct compositions and properties to the total observed PM2.5. Using such an analysis, future environmental health studies could assess health risks associated with exposure to distinct PM2.5 fractions. This information may assist decision makers to set environmental targets for abating PM2.5 with specific compositions and properties.
               ",unknown,240
10.1016/j.jag.2024.104071,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85200356830,scopus,2024-08-01,scopus,assessing the impacts of temperature extremes on agriculture yield and projecting future extremes using machine learning and deep learning approaches with cmip6 data,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85200356830&origin=inward,"Climate change, particularly extreme weather events, has significantly affected various sectors, including agriculture, human health, water resources, sea levels, and ecosystems. It is anticipated that the intensity, duration, and frequency of these extremes will escalate in the future. This study aims to discover the association between temperature extremes and agricultural yield and to project these extremes using machine learning (ML) and deep learning (DL) models with CMIP6 (Coupled Model Intercomparison Project Phase 6) data under two SSPs (Shared Socioeconomic Pathways). A bi-wavelet coherence technique is employed to investigate the association, providing detailed information in both the frequency and time domains for the period of 1980–2014. Various ML and DL models are trained and tested for the periods of 1985–2004 and 2005–2014, respectively, with gradient boosting machine chosen for projecting temperature extremes based on its superior performance. Mann-Kendall test is used for trend analysis in the projected temperature extremes. The results indicate strong negative and positive associations between TN10p (Cold nights) and TN90p (Warm nights), respectively, with wheat production. Additionally, there is a long-term negative association of CSDI (Cold Spell Duration Indicator) and strong positive association of WSDI (Warm Spell Duration Indicator) with rice yield. Projected results show an increase and decrease under SSP2-4.5 and SSP5-8.5, respectively, in DTR (Diurnal Temperature Range) at most stations. TN10p will increase in the future at most stations, with exceptions such as Muree station where it decreases during 2025–2049 and then increases under both SSPs. Projections show that TXn (annual or monthly minimum value of daily maximum temp) will increase in the future, with Muree station exhibiting the lowest value close to zero, while the average maximum value is around 20 °C at Khanpur station. Trend analysis reveals significantly increasing trend in TR20 (Tropical nights) and decreasing trend in CSDI in future durations under both SSPs. These findings hold implications for policymakers and stakeholders in various departments, including agriculture, health, and water resources management.",unknown,246
10.1016/j.asoc.2024.111765,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85194339242,scopus,2024-08-01,scopus,the weight fuzzy judgment method for the benchmarking sustainability of oil companies,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85194339242&origin=inward,"
                  The environmental, social, and economic challenges associated with the massive activities of the oil and gas industry require analysis and evaluation of companies' sustainability priorities. Evaluating the environmental performance of oil companies requires the application of strict mathematical tools and models that provide rigorous results. Multi-criteria decision-making (MCDM) is a rigorous tool that evaluates the performance and practices of oil and sustainable energy companies. However, classical multi-criteria decision analysis has problems related to inconsistency because of the subjective nature of pairwise comparisons. This study responded to the literature call by utilizing the weight fuzzy judgment method (WFJM) to determine the weight coefficients of criteria with zero consistency. Such a method employed criteria values and expert judgment for decision-making to create a bridge between the mathematical approach and the human approach. This study aims to develop a solid decision support system by evaluating and benchmarking 11 companies utilizing the fuzzy judgment method based on fuzzy theory and the VIekriterijumsko KOmpromisno Rangiranje (VIKOR) method to determine sustainability priorities for oil and gas companies. The oil companies’ criteria were efficiently weighted by the fuzzy judgment method, and the organizational capabilities represented the greatest weight value of 0.128, whilst the low-weight criteria were the high costs of technologies at 0.080. Besides, the benchmarking results for oil companies revealed that COM 8 was the best, whilst COM 11 was the worst. The fuzzy judgment technique is a unique approach to solving complicated decision-making problems, and its influence can be significant. Furthermore, this method can be applied in sectors other than energy, including banking, medical services, and engineering, where decision-making is not straightforward and requires a systematic approach.
               ",unknown,249
10.1016/j.atmosenv.2024.120531,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85192218142,scopus,2024-07-15,scopus,residual neural network with spatiotemporal attention integrated with temporal self-attention based on long short-term memory network for air pollutant concentration prediction,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85192218142&origin=inward,"
                  Accurate prediction of air pollutant concentrations can be effective in controlling and preventing air pollution, which is crucial for both the government's policy response and the public's reasonable ability to avoid air pollution. In this study, we propose a model based on the principle of big data correlation and deep learning techniques for predicting the concentration of air pollutant concentrations. The proposed model, named STARes-SaLSTM, consists of a residual neural network with spatiotemporal attention (STAResNet) and a temporal self-attention based on long short-term memory network (SaLSTM). The spatiotemporal attention module in STAResNet enhances the original input information by weighting it in both spatial and temporal dimensions, meanwhile the features of the spatial distribution of pollutant and meteorological information across many cities are deeply extracted using ResNet. A novel self-attention method known as temporal self-attention is developed to extract the temporal dependency of air pollutants. LSTM is used as a sequence encoder in this temporal attention method to calculate queries, keys, and values to acquire a more comprehensive temporal dependence than is possible with normal self-attention. The STARes-SaLSTM model successfully predicts air pollution concentrations in the target city for the future by extracting spatiotemporal correlation of feature sequence. In comparison to previous neural network models and conventional methods, the model increases the accuracy of pollutant concentration prediction. The suggested prediction model works well for the one to 3 h prediction job, with a root mean square error (RMSE) ranging from 6.716 to 11.648. Furthermore, even for the 1–24 h prediction assignment, we executed multi-scale prediction in the target city and obtained a reasonable performance, with an average RMSE value of 20.576.
               ",unknown,252
10.1016/s2589-7500(24)00094-3,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85196148002,scopus,2024-07-01,scopus,automated cooling tower detection through deep learning for legionnaires’ disease outbreak investigations: a model development and validation study,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85196148002&origin=inward,"Background Cooling towers containing Legionella spp are a high-risk source of Legionnaires’ disease outbreaks. Manually locating cooling towers from aerial imagery during outbreak investigations requires expertise, is labour intensive, and can be prone to errors. We aimed to train a deep learning computer vision model to automatically detect cooling towers that are aerially visible. Methods Between Jan 1 and 31, 2021, we extracted satellite view images of Philadelphia (PN, USA) and New York state (NY, USA) from Google Maps and annotated cooling towers to create training datasets. We augmented training data with synthetic data and model-assisted labelling of additional cities. Using 2051 images containing 7292 cooling towers, we trained a two-stage model using YOLOv5, a model that detects objects in images, and EfficientNet-b5, a model that classifies images. We assessed the primary outcomes of sensitivity and positive predictive value (PPV) of the model against manual labelling on test datasets of 548 images, including from two cities not seen in training (Boston [MA, USA] and Athens [GA, USA]). We compared the search speed of the model with that of manual searching by four epidemiologists. Findings The model identified visible cooling towers with 95·1% sensitivity (95% CI 94·0–96·1) and a PPV of 90·1% (95% CI 90·0–90·2) in New York City and Philadelphia. In Boston, sensitivity was 91·6% (89·2–93·7) and PPV was 80·8% (80·5–81·2). In Athens, sensitivity was 86·9% (75·8–94·2) and PPV was 85·5% (84·2–86·7). For an area of New York City encompassing 45 blocks (0·26 square miles), the model searched more than 600 times faster (7·6 s; 351 potential cooling towers identified) than did human investigators (mean 83·75 min [SD 29·5]; mean 310·8 cooling towers [42·2]). Interpretation The model could be used to accelerate investigation and source control during outbreaks of Legionnaires’ disease through the identification of cooling towers from aerial imagery, potentially preventing additional disease spread. The model has already been used by public health teams for outbreak investigations and to initialise cooling tower registries, which are considered best practice for preventing and responding to outbreaks of Legionnaires’ disease. Funding None.",unknown,258
10.1016/j.fct.2024.114766,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85195201840,scopus,2024-07-01,scopus,allergenicity assessment of new or modified protein-containing food sources and ingredients,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85195201840&origin=inward,"The growing world population, changing dietary habits, and increasing pressure on agricultural resources are drivers for the development of novel foods (including new protein sources as well as existing protein sources that are produced or used in an alternative way or in a different concentration). These changes, coupled with consumer inclination to adopt new dietary trends, may heighten the intake of unfamiliar proteins, or escalate consumption of specific ones, potentially amplifying the prevalence of known and undiscovered food allergies. Assessing the allergenicity of novel or modified protein-based foods encounters several challenges, including uncertainty surrounding acceptable risks and assessment criteria for determining safety. Moreover, the available methodological tools for gathering supportive data exhibit significant gaps. This paper synthesises these challenges, addressing the varied interpretations of ""safe"" across jurisdictions and societal attitudes towards allergenic risk. It proposes a comprehensive two-part framework for allergenicity assessment: the first part emphasises systematic consideration of knowledge and data requirements, while the second part proposes the application of a generic assessment approach, integrating a Threshold of Allergological Concern. This combined framework highlights areas that require attention to bridge knowledge and data gaps, and it delineates research priorities for its development and implementation.",unknown,261
10.1016/j.asoc.2024.111706,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85193021338,scopus,2024-07-01,scopus,multi-modal topic modeling from social media data using deep transfer learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85193021338&origin=inward,"
                  As social media platforms grow rapidly, multi-modal data is becoming more and more prevalent. A user can better understand events by analyzing multimodal data for topics. Automatic topic detection from multimodal data can potentially have tremendous value for advertising and government agencies for whom public opinion matters for strategic decisions and policy making. However , multimodal topic detection is complicated for two reasons: (1) The nature of the multimodal data varies from one medium to another, and (2) The noisy nature of webdata. Conventional topic models are ineffective in dealing with these two problems. This paper proposes, a framework for multimodal topic modeling for social media data that uses topics extracted using Latent Dirichlet Allocation (LDA) and patterns found from images using transfer learning. The proposed framework makes use of textual as well as visual data for topic detection. The experiments are conducted on the benchmark datasets: Flickr8k, Flickr30k, and MCG WEBV. The proposed work outperformed other techniques in terms of accuracy (0.63), precision (0.75), recall (0.97), F-Measure (0.85), Bleu-1(0.68),METEOR (0.17) , ROUGE-L (0.49), and CIDEr (0.573). The proposed work is compared to state-of-the-art methods to demonstrate its accuracy.
               ",unknown,262
10.1016/j.envres.2024.119027,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85191962278,scopus,2024-07-01,scopus,levels and health risk assessment of twenty-one current-use pesticides in urban and riverside waters of the brazilian amazon basin,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85191962278&origin=inward,"
                  In Brazil, the favorable weather conditions and abundant land contribute to the thriving agricultural production. Brazilian crops extensively employ pesticides due to their high efficacy, cost-effectiveness, and permissive regulatory framework. However, pesticide use also endangers water resources, animal organisms, and human health. Due to the lack of data on pesticide use in the Amazonas forest-based Brazil, the present study aimed to assess the levels of twenty-one current-use pesticides (CUPs) from five different classes in river waters collected from urban and riverside areas in this region. Moreover, the non-carcinogenic risks associated with water consumption were also characterized. Thirteen CUPs were detected in concentrations above the limit of detection (LOD) in at least one of the water samples, and most pesticides were detected in riverside areas. In contrast, only 18% of the analyzed samples were considered “clean,” with only one compound detected. Fenitrothion showed the highest concentration, with a mean value of 4.86 ng/mL (0.30–14.3 ng/mL). Up to 33% of the samples showed levels of fipronil above the LOD, an issue of environmental and human health concern, mainly because of the adverse effects observed in honeybees. Despite this, the human health risk assessment showed a target hazard quotient below one (HQ < 1) in adults for all substances, suggesting that pesticide exposure through water consumption should not mean risk for the riverside populations. Taking into account the large extension of Brazil, as well as its different agricultural practices throughout the country, it would be of great importance to conduct extensive research in other areas. It would help to gain knowledge in this field and to promote eco-friendly alternatives to mitigate pesticide use and, consequently, to reduce their potential adverse effects on human health and the ecosystems.
               ",unknown,263
10.1016/j.tbs.2024.100812,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85191825158,scopus,2024-07-01,scopus,dissecting shared e-scooters usage patterns and its impact on other transportation modes: a case study of portland city,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85191825158&origin=inward,"
                  The study analyzed the survey data from the 2018 Portland E-scooter Pilot Program and aims to determine (i) who uses shared e-scooters and why they use them, and (ii) whether there is any association between e-scooter usage and the usage of other modes of transportation. To accomplish the first objective, the study identifies the users of shared e-scooters based on their travel behavior using an unsupervised machine learning approach, latent class analysis (LCA). The LCA model grouped e-scooter users into three distinct classes: Class 1 (Recreational Enthusiasts) −occasional and frequent users for recreation, Class 2 (Commute Riders) −frequent users for work, and Class 3 (Intermittent Joyriders) −occasional and one-time users for recreation. Furthermore, a set of ordered logit models is employed to determine the second objective based on the identified classes of e-scooter users, their socio-demographic characteristics, and the built environment variables. The results of ordered logit models revealed that compared to Commute Riders, both Recreational Enthusiasts and Intermittent Joyriders exhibit less interest in increasing the usage of available transportation modes after adopting e-scooters. Notably, low-income e-scooter users show a higher probability of increasing their usage across various transportation modes, including public transportation, driving, shared mobility services, personal bikes, shared bikes, and walking. The study offers valuable insights to guide city planners and policymakers in developing effective strategies for the deployment of e-scooters, targeting each group of users.
               ",unknown,264
10.1016/j.apr.2024.102162,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85191495651,scopus,2024-07-01,scopus,real-time early warning and the prediction of air pollutants for sustainable development in smart cities,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85191495651&origin=inward,"
                  Air quality forecasting is vital for sustainable development in smart cities and is essential to protect public health through early warning against air pollutants. This article presents a novel ensemble deep learning method able to capture spatiotemporal features on short-term time intervals and extract high data abstraction levels. This method uses a hierarchical framework with a new activation function called HT-Tanh to improve the prediction of air pollutant emissions in real time. As such, a residual neural network (ResNet) is integrated with a convolutional neural network (CNN) to deeply extract the temporal and spatial features from pollutant and meteorological data. The model network encodes the fully connected layer to fine-tune the CNN output and decodes the temporal prediction relations based on gate recurrent unit (GRU) to obtain more accurate results. The reliability of the proposed method is verified through a case study from Shantou City, China. The model performance is compared with the recently developed and traditional deep learning models. Results reveal that the proposed method has good precision in the multiscale air quality predictions. Compared with CNN-GRU, convolutional long short-term memory, and CNN models, the forecasting results of the proposed method are significantly improved. The presented method in this article can support the early warning systems in multiple regions for air pollution modeling studies.
               ",unknown,265
10.1016/j.engappai.2024.108380,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85189944235,scopus,2024-07-01,scopus,fractal interpolation in the context of prediction accuracy optimization,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85189944235&origin=inward,"This paper focuses on the hypothesis of optimizing time series predictions using fractal interpolation techniques. In general, the accuracy of machine learning model predictions is closely related to the quality and quantitative aspects of the data used, following the principle of garbage-in, garbage-out. In order to quantitatively and qualitatively augment datasets, one of the most prevalent concerns of data scientists is to generate synthetic data, which should follow as closely as possible the actual pattern of the original data. This study proposes three different data augmentation strategies based on fractal interpolation, namely the Closest Hurst Strategy, Closest Values Strategy and Formula Strategy. To validate the strategies, we used four public datasets from the literature, as well as a private dataset obtained from meteorological records in the city of Braşov, Romania. The prediction results obtained with the LSTM model using the presented interpolation strategies showed a significant accuracy improvement compared to the raw datasets, thus providing a possible answer to practical problems in the field of remote sensing and sensor sensitivity. Moreover, our methodologies answer some optimization-related open questions for the fractal interpolation step using Optuna framework.",unknown,266
10.1016/j.iswa.2024.200379,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85194320520,scopus,2024-06-01,scopus,application of new features based on artificial intelligent robot technology in medium-scale urban design pedigree and intelligent management and control,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85194320520&origin=inward,"Since the 21st century, China has been vigorously developing urban construction, and now the rise of artificial intelligence (AI) has brought new opportunities for urban design and management. Meso-scale cities are the most developed cities in China and play an important role in economic and social development. The current robot technology is mainly divided into industrial robots and service robots, which can play an important role in the development of cities. This paper aims to apply AI robotics technology to the analysis and intelligent management of mesoscale urban design genealogy. Firstly, the development pedigree of the mesoscale city was analyzed, from which the characteristics of previous designs could be clearly understood. Then, the urban management and control system is intelligently designed from many aspects, the characteristics of the robot are analyzed, and the application of the intelligent robot in urban design is introduced. After that, four developing meso‑scale cities in a province were selected as the evaluation objects, and an Analytic Hierarchy Process (AHP) was proposed to evaluate the application effect of robotics technology in meso‑scale cities. The results showed that the overall score of the robot in urban traffic design was greater than 70 points, and the overall score of urban architectural design was greater than 65 points, which was in the acceptable range. In the application score of urban environment design, the influence was about 70 to 80 points, and the aesthetics was more than 75 points. The cultural aspect of the design impact score was no more than 75 points, while the cultural support aspect score was around 80 points. After the weight calculation, the final overall score was 489 points, and the comprehensive average score was 36 points. The overall composite score and average score were both good. This showed that the application of robot technology to the pedigree analysis and intelligent management and control of mesoscale urban design could achieve good results.",unknown,292
10.1016/j.nmni.2024.101417,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85191986329,scopus,2024-06-01,scopus,applying geospatial multi-agent system to model various aspects of tuberculosis transmission,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85191986329&origin=inward,"Introduction The paper presents epidemiological process modeling, with a focus on tuberculosis utilizing multi-agent system. Material and methods This study involves the development of an algorithm that harnesses the potential of artificial intelligence to create a geospatial model that highlights the different pathways of TB transmission. The modeling process itself is characterized by a series of key stages, including initialization of the city, calibration of health parameters, simulation of the working day, propagation of the spread of infection, the evolution of disease trajectories, rigorous statistical calculations and transition to the following day. A comprehensive description of the course of active tuberculosis is presented, following the official hypothesis recommended by the World Health Organization. A comprehensive simulation, illustrating the propagation of tuberculosis in an entirely healthy environment devoid of any preventive or therapeutic measures, is presented. To ascertain the adequacy of the model and its sensitivity to the principal parameters governing the course of tuberculosis, a series of experiments were meticulously conducted, employing three distinct approximations, namely: the basic model, the model incorporating mortality factors, and the comprehensive model, encompassing all relevant aspects. Conclusions The model's results exhibit stability and lack of significant fluctuations. The statistical values obtained for infected, latent, and recovered individuals align well with known medical data, confirming the model's adequacy.",unknown,295
10.1016/j.rineng.2024.102114,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85190311411,scopus,2024-06-01,scopus,machine learning predictions for carbon monoxide levels in urban environments,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85190311411&origin=inward,"The increasing carbon emissions in Malaysia necessitate accurate methods to track and control pollution levels. This study focuses on predicting carbon monoxide (CO) concentrations in Petaling Jaya using various machine learning models, and two important parameters, CO concentration and time, were considered in the analysis. Six distinct machine learning models were assessed to gauge their predictive capabilities. These models encompassed a range of regression modeling techniques namely; Linear, Tree, Gaussian Process, Ensemble of Trees, Support Vector, and Artificial Neural Networks (ANN) modelling. The Matern 5/2 Gaussian Process Regression (GPR) model consistently outperformed other models across all scenarios, demonstrating high R2 values and low RMSE, MSE, and MAE values. Specifically, in scenarios 1, 2, 3, and 4, the Matern 5/2 model exhibited the lowest RMSE (0.084–0.088) and highest R2 (0.97), highlighting its reliability and robustness in predicting CO concentrations. Additionally, the Rational Quadratic model achieved an R2 of 0.97 with an RMSE of 0.088 in scenario 1, while the Quadratic SVM excelled in scenario 3 with an R2 of 0.965 and low RMSE, MSE, and MAE values (0.085, 0.007, and 0.066). These findings provide valuable insights for environmental protection, renewable energy transition, energy efficiency, sustainable land use planning, and public awareness. However, acknowledging the study's single-area focus and potential limitations in representing diverse regions, further research is warranted to explore carbon emissions across varied areas and enhance the generalizability of the findings.",unknown,296
10.1016/j.envsci.2024.103752,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85190268059,scopus,2024-06-01,scopus,the governance configurations of green schoolyards,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85190268059&origin=inward,"In light of the growing interest in naturalizing schoolgrounds this paper explores the governance configurations that enable, produce and sustain their multiple benefits. We scrutinize the existing literature analysing the norms, actors, and processes through which green school grounds are conceptualized, designed, implemented, and sustained. We find that good schoolyard governance is exhibited by: i) the actual use that children make of the naturalized areas, and by its repercussion on their physical and mental well-being, social integration, sense of place, and socio-environmental awareness; ii) the ways outdoor environments intersect with school curriculums; iii) by the availability of public support and funding lines with flexible, anticipatory and adaptive features; and crucially iv) by the ways that architects and gardeners engage with the concepts of ecology, creative play and outdoor education in the acts of schoolyard (re)making. We also find that at times focus on program effectiveness (efficiency, goals) infringes upon justice and inclusion. The way participative processes are interpreted, and eventually inform schoolyard designs and uses is one of the challenges to consider in this respect. Greening schoolyards also requires participation processes that are accessible and inclusive for adults and children from a variety of socio-cultural, ethnic and economic contexts. In a nutshell, naturalizing schoolyards needs to go beyond the search for narrow technical solutions for climate adaptation or pedagogical innovation, being a process of school (re)making. The governance framework suggested here is apt for analysing a range of urban green interventions",unknown,297
10.1016/j.msksp.2024.102945,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85188685394,scopus,2024-06-01,scopus,machine learning models for classifying non-specific neck pain using craniocervical posture and movement,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188685394&origin=inward,"
                  Objective
                  Physical therapists and clinicians commonly confirm craniocervical posture (CCP), cervical retraction, and craniocervical flexion as screening tests because they contribute to non-specific neck pain (NSNP). We compared the predictive performance of statistical machine learning (ML) models for classifying individuals with and without NSNP using datasets containing CCP and cervical kinematics during pro- and retraction (CKdPR).
               
                  Design
                  Exploratory, cross-sectional design.
               
                  Setting and participants
                  In total, 773 public service office workers (PSOWs) were screened for eligibility (NSNP, 441; without NSNP, 332).
               
                  Methods
                  We set up five datasets (CCP, cervical kinematics during the protraction, cervical kinematics during the retraction, CKdPR and combination of the CCP and CKdPR). Four ML algorithms–random forest, logistic regression, Extreme Gradient boosting, and support vector machine–were trained.
               
                  Main outcome measures
                  Model performance were assessed using area under the curve (AUC), accuracy, precision, recall and F1-score. To interpret the predictions, we used Feature permutation importance and SHapley Additive explanation values.
               
                  Results
                  The random forest model in the CKdPR dataset classified PSOWs with and without NSNP and achieved the best AUC among the five datasets using the test data (AUC, 0.892 [good]; F1, 0.832). The random forest model in the CCP dataset had the worst AUC among the five datasets using the test data [AUC, 0.738 (fair); F1, 0.715].
               
                  Conclusion
                  ML performance was higher for the CKdPR dataset than for the CCP dataset, suggesting that ML algorithms are more suitable than classical statistical methods for developing robust models for classifying PSOWs with and without NSNP.
               ",unknown,298
10.1016/j.jenvp.2024.102309,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85188476613,scopus,2024-06-01,scopus,influences of cognitive appraisal and individual characteristics on citizens’ perception and emotion in urban environment: model development and virtual reality experiment,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188476613&origin=inward,"To model individuals' experience of urban spaces, this study integrates knowledge from environmental psychology and artificial intelligence to propose a framework for individuals' perceptions and emotion by incorporating individual characteristics and cognitive appraisal together with environment attributes as determinants. A path model is employed to capture how the four perceptions of environmental qualities (safety, liveliness, comfort, and legibility) and three dimensions of emotion (pleasure, arousal, dominance) are influenced by individual characteristics and cognitive appraisal using data collected in an online virtual reality experiment with 237 participants. Results show that emotional pleasure is more directly influenced by environmental attributes while arousal and dominance are closely related to a person's current mood and personality. Perceptions of environmental qualities do have mediating effects in emotion generation, but contribute differently to the three dimensions of emotion. Cognitive appraisal variables directly influence emotion generation, with ideal values always having positive effects and expected values always negative effects. The findings can help capture the dynamic process of emotional experiences between diverse individuals and may support experience-centered simulation and prediction.",unknown,299
10.1016/j.treng.2024.100239,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85188164296,scopus,2024-06-01,scopus,moving from classical towards machine learning stances for bus passengers’ alighting estimation: a comparison of state-of-the-art approaches in the city of lisbon,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188164296&origin=inward,"Passenger alighting estimation is a critical task in public transport (PT) management, especially for entry-only Automatic Fare Collection (AFC) transport systems where passenger alighting are not recorded. Effective estimation methods are necessary for trip analysis and route planning, offering valuable insights into passengers’ mobility patterns and, subsequently, improving the quality of service. However, the stochastic nature of passenger behaviour challenges the degree of successful alighting estimates. A classic approach to infer the alighting stops of passengers is the use of trip-chaining principles. Since these principles are dispersed across the literature in the field, their comprehensive review is pivotal to establish the best practice for alighting estimation. Still, trip-chaining approaches are unable to infer the alighting of non-commuting passengers. This paper addresses these two research gaps by: i) providing a critical overview of the existing principles and methods for alighting estimation; ii) proposing an approach to improve alighting estimation that consistently integrates the most effective state-of-the-art principles on trip-chaining; and iii) further introducing a frequent pattern mining and density-based clustering solutions to support alighting estimation for non-commuting passengers. Considering the public bus transport in Lisbon city as the guiding case study, the achieved estimation rate by the proposed assembled model is 92%. Moreover, the density-based clustering solution is found to improve the estimation of 11pp against classic trip-chaining principles. Furthermore, the proposed model and acquired results yield actionable value to enhance PT operations and services, ultimately leading to improved bus routing and quality of service.",unknown,300
10.1016/j.undsp.2023.09.013,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85183176550,scopus,2024-06-01,scopus,performance comparison of machine learning algorithms for maximum displacement prediction in soldier pile wall excavation,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183176550&origin=inward,"One of the common excavation methods in the construction of urban infrastructures as well as water and wastewater facilities is the excavation through soldier pile walls. The maximum lateral displacement of pile wall is one of the important variables in controlling the stability of the excavation and its adjacent structures. Nowadays, the application of machine learning methods is widely used in engineering sciences due to its low cost and high speed of calculation. This paper utilized three intelligent machine learning algorithms based on the excavation method through soldier pile walls, namely eXtreme gradient boosting (XGBoost), least square support vector regressor (LS-SVR), and random forest (RF), to predict maximum lateral displacement of pile walls. The results showed that the implemented XGBoost model performed excellently and could make predictions for maximum lateral displacement of pile walls with the mean absolute error of 0.1669, the highest coefficient of determination 0.9991, and the lowest root mean square error 0.3544. Although the LS-SVR, and RF models were less accurate than the XGBoost model, they provided good prediction results of maximum lateral displacement of pile walls for numerical outcomes. Furthermore, a sensitivity analysis was performed to determine the most effective parameters in the XGBoost model. This analysis showed that soil elastic modulus and excavation height had a strong influence on of maximum lateral displacement of pile wall prediction.",unknown,301
10.1016/j.scitotenv.2024.171702,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85189144366,scopus,2024-05-20,scopus,evaluation of the concentration and human health risk of nitrate and potentially toxic elements (ptes) in melons from a southern region of iran: identification of pollution sources,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85189144366&origin=inward,"
                  Decentralized agriculture, improper monitoring of cultivation conditions, and leaching of contaminants into lands led to the contamination of crops with various potentially toxic elements (PTEs). However, it is essential to know more about the profile level and associated risk of these contaminants and their origin, especially in high-water content crops. This study aimed to investigate the concentration of PTEs in melons of one of Iran's southern cities and follow that health risk assessment in the target population for the first time. Results of the present study confirmed that although the mean concentration of some metals was lower than the safety standard (Cr: 4.6 ± 2 mg/kg and Pb: 7.4 ± 4 mg/kg), their nutritional value was unfavorable regarding some micronutrients (Cu: 88.8 ± 27 mg/kg and Zn: 480 ± 275 mg/kg). The highest metal concentration in cantaloupe was iron (1706.47 mg/kg, p-value<0.05), and nitrate concentration in all melon types was 2.59–524.54 mg/kg (p-value<0.05). Principal component analysis (PCA) with K-means clustering and the Positive Matrix Factorization (PMF) model have shown that contaminants in melons originated from human activities. So, excessive use of agricultural fertilizers is a possible source of nitrates in melons, which have 93 % of factor loading values. The health risk assessment also showed that melons' carcinogenic and non-carcinogenic risk using the deterministic method was lower than the permissible limit (HQ < 1, ILCR<E-4). However, the Monte Carlo simulation estimated HQ > 1 in the children group for the 95th percentile. Furthermore, the level of certainty in the carcinogenesis risk for children, women, and men was estimated at 86.48 %, 64.67 %, and 61.30 %, respectively. Also, the consumption rate was determined as the most important parameter in the sensitivity analysis. As a consequence, there is a potential health risk for Iranians after the consumption of melon due to PTEs and nitrate levels that also originated from anthropogenic sources.
               ",unknown,306
10.1016/j.heliyon.2024.e29825,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85192052302,scopus,2024-05-15,scopus,optimizing green supply chain circular economy in smart cities with integrated machine learning technology,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85192052302&origin=inward,"This paper explores methodologies to enhance the integration of a green supply chain circular economy within smart cities by incorporating machine learning technology. To refine the precision and effectiveness of the prediction model, the gravitational algorithm is introduced to optimize parameter selection in the support vector machine model. A nationwide prediction model for green supply chain economic development efficiency is meticulously constructed by leveraging public economic, environmental, and demographic data. A comprehensive empirical analysis follows, revealing a noteworthy reduction in mean squared error and root mean squared error with increasing iterations, reaching a minimum of 0.007 and 0.103, respectively—figures that are the lowest among all considered machine learning models. Moreover, the mean absolute percentage error value is remarkably low at 0.0923. The data illustrate a gradual decline in average prediction error and standard deviation throughout the model optimization process, indicative of both model convergence and heightened prediction accuracy. These results underscore the significant potential of machine learning technology in optimizing supply chain and circular economy management. The paper provides valuable insights for decision-makers and researchers navigating the landscape of sustainable development.",unknown,308
10.1016/j.hitech.2024.100496,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85194377452,scopus,2024-05-01,scopus,ai in public-private partnership for it infrastructure development,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85194377452&origin=inward,"
                  The development of a nation's infrastructure is a defining paradigm. Due to the complexity of putting together and running such programs. For developing countries like India, the Public-Private-Partnership (PPP) model has emerged as one of the best suited ways of project delivery. India's developing economy necessitates efficient and timely project completion at the lowest possible cost. Government involvement and private sector expertise are combined in the PPP concept. Note that highway construction is crucial to the development of other types of infrastructure. However, highway construction is fraught with uncertainty due to factors like land acquisition, traffic management, weather, and more. Maintaining an economy's current rate of growth, high-quality infrastructure is the single most important factor. All modes of transportation fall under this category, from cars and buses to trains and planes, as well as the provision of potable water and agricultural irrigation. However, a lot of money is needed in this industry. Public-private partnerships (PPPs) are being pushed to speed up infrastructure development and meet such needs. Private-public partnerships (PPPs) include both public- and private-sector collaborations. To provide the necessary structure and person capacities, governments that have embarked on PPPs projects have often created new policy, legal, and institutional frameworks. Following the identification of key factors affecting PPP performance, a case study of the Porbandar-Dwarka highway construction project is used to compare and contrast three distinct PPP (BOT-Toll, BOT- Annuity, and HAM) models. Internal Rate of Return and Net Present Value are two key financial indicators that favor HAM over BOT-Toll and BOT-Annuity. Finally, the FIDIC requirements and the financial and legal conditions from the models are combined to suggest a new PPP framework. The particular findings of this study will help policymakers, investors, and planners in Gujarat complete future highway development projects on schedule and under budget.
               ",unknown,313
10.1016/j.scs.2024.105292,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85186441853,scopus,2024-05-01,scopus,towards a sustainable city: deciphering the determinants of restorative park and spatial patterns,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186441853&origin=inward,"
                  Urban parks have been found to provide mental health benefits. Some empirical studies have tested natural features and perceptual measures respectively, announcing their contribution to psychological restoration. However, inconsistent findings were occasionally reported, whereas few attempts have been made to combine both observed and perceptual factors for validation. Little is known about the variation of restorative drivers and their spatial patterns. To address these problems, this study combined public participation geographic information system (PPGIS) and deep learning method to capture visual qualities of landscape features along with several important perceptual measures. A typical urban park in Wuhan, China, was selected for a pilot study, and 1560 crowdsourced on-site images were collected, with thematic and geographic information being integrated. A series of statistical models, e.g., OLS, QRM, and MGWR, were employed successively for validation. The results showed that landscape preference, place attachment, greenery and water were validated as the global explanatory factors to estimate the conditional mean of psychological restoration. The variation of influential effects of these factors were detected at different restoration levels. There exist spatial heterogeneity for these influential factors on restorative effects. Findings provided new knowledge on a deeper understanding of the subtlety of restoration drivers and their spatial patterns. The findings offered useful insights and guidance for urban planners in creating high-quality green parks with restorative values.
               ",unknown,318
10.1016/j.oret.2023.11.010,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85184685430,scopus,2024-05-01,scopus,predicting visual acuity responses to anti-vegf treatment in the comparison of age-related macular degeneration treatments trials using machine learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184685430&origin=inward,"
                  Purpose
                  To evaluate multiple machine learning (ML) models for predicting 2-year visual acuity (VA) responses to anti–vascular endothelial growth factor (anti-VEGF) treatment in the Comparison of Age-related Macular Degeneration (AMD) Treatments Trials (CATT) for patients with neovascular AMD (nAMD).
               
                  Design
                  Secondary analysis of public data from a randomized clinical trial.
               
                  Participants
                  A total of 1029 CATT participants who completed 2 years of follow-up with untreated active nAMD and baseline VA between 20/25 and 20/320 in the study eye.
               
                  Methods
                  Five ML models (support vector machine, random forest, extreme gradient boosting, multilayer perceptron neural network, and lasso) were applied to clinical and image data from baseline and weeks 4, 8, and 12 for predicting 4 VA outcomes (≥ 15-letter VA gain, ≥ 15-letter VA loss, VA change from baseline, and actual VA) at 2 years. The CATT data from 1029 participants were randomly split for training (n = 717), from which the models were trained using 10-fold cross-validation, and for final validation on a test data set (n = 312).
               
                  Main Outcome Measures
                  Performances of ML models were assessed by R
                     2 and mean absolute error (MAE) for predicting VA change from baseline and actual VA at 2 years, by the area under the receiver operating characteristic curve (AUC) for predicting ≥ 15-letter VA gain and loss from baseline.
               
                  Results
                  Using training data up to week 12, the ML models from cross-validation achieved mean R
                     2 of 0.24 to 0.29 (MAE = 9.1–9.8 letters) for predicting VA change and 0.37 to 0.41 (MAE = 9.3–10.2 letters) for predicting actual VA at 2 years. The mean AUCs for predicting ≥ 15-letter VA gain and loss at 2 years was 0.84 to 0.85 and 0.58 to 0.73, respectively. In final validation on the test data set up to week 12, the models had an R
                     2 of 0.33 to 0.38 (MAE = 8.9–9.9 letters) for predicting VA change, an R
                     2 of 0.37 to 0.45 (MAE = 8.8–10.2 letters) for predicting actual VA at 2 years, and AUCs of 0.85 to 0.87 and 0.67 to 0.79 for predicting ≥ 15-letter VA gain and loss, respectively.
               
                  Conclusions
                  Machine learning models have the potential to predict 2-year VA response to anti-VEGF treatment using clinical and imaging features from the loading dose phase, which can aid in decision-making around treatment protocols for patients with nAMD.
               
                  Financial Disclosure(s)
                  The author(s) have no proprietary or commercial interest in any materials discussed in this article.
               ",unknown,319
10.1016/j.ecoinf.2024.102477,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85182894788,scopus,2024-05-01,scopus,deep learning-based air pollution analysis on carbon monoxide in taiwan,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182894788&origin=inward,"Global air pollution poses a threat to humanity. Specifically, CO directly affects cardiovascular and other organ tissues and leads to numerous chronic diseases and major public health problems. The effective implementation of a deep learning model for predicting variations in CO levels would enable the early formulation of policies for controlling air pollution. In this study, a seasonal gated recurrent unit (SGRU) model, which is a deep learning time-series prediction model, was developed to predict the levels of CO in Taiwan. Atmospheric CO measurements from 2005 to 2021 were collected from the Environmental Protection Administration of Taiwan and preprocessed using the Kalman filter to achieve accurate forecasting. The performance of the proposed SGRU model was compared with that of the autoregressive integrated moving average (ARIMA), seasonal ARIMA, exponential smoothing (ETS), Holt–Winters ETS, support vector regression, and seasonal long short-term memory models in terms of mean absolute percentage error (MAPE) and root mean square error. The SGRU model achieved the lowest MAPE value of 0.94, which demonstrated its superior performance. The construction of an accurate air pollution prediction model can assist government entities in formulating health and social care strategies and in planning future air pollution control measures.",unknown,320
10.1016/j.asoc.2024.111446,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85187224151,scopus,2024-04-01,scopus,an artificial fish swarm optimization algorithm for the urban transit routing problem,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187224151&origin=inward,"
                  The Urban Transit Routing Problem (UTRP) is an NP-hard discrete problem that deals with the design of routes for public transport systems. It is a highly complex, multiply constrained problem, while the evaluation of candidate route sets can prove both challenging and time-consuming, with many potential solutions rejected on the grounds of infeasibility. Due to its difficulty, metaheuristic methods, such us swarm intelligence algorithms, are considered highly suitable for the UTRP. The suitability of these methods heavily relies on the correct adaptation of the chosen method for a discrete-space problem, the initialization procedure, and the solution evaluation method. In this context, this study proposes an artificial fish swarm optimization algorithm for the efficient solution of the UTRP, presenting a novel discrete-space adaptation of the former. The results are subsequently compared to 14 other algorithms, including evolutionary, swarm intelligence and hyper-heuristic implementations, using Mandl’s widely used and accepted benchmark. Comparison of the produced solutions with published results on Mandl’s benchmark network, shows that the developed algorithm yields superior results to the existing techniques, yielding very high shares of direct trip coverage, which is vital for transit systems to attract riders and contribute to urban sustainability. A new indicator for operator cost calculation is also developed and integrated into our analysis, offering insights on the trade-offs between user and operator costs. Differences in generated solutions, influenced by the weighting factor value, can result in variations of up to 13% in direct trip coverage and 1.5 minutes in average travel time.
               ",unknown,334
10.1016/j.jag.2024.103710,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85185610521,scopus,2024-04-01,scopus,recovnet: reinforcement learning with covering information for solving maximal coverage billboards location problem,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185610521&origin=inward,"Maximizing billboard coverage with limited resources and different objective goals plays a vital role in social activities. The Maximal Coverage Billboard Location Problem (MCBLP) is complex, especially for multi-objective functions. A multi-objective spatial optimization model was developed using mixed-integer linear programming based on MCBLP to formulate the spatial optimization problem of determining billboard locations. Combining the distinctive features of location problems, we have developed a new approach called ReCovNet that utilizes Deep Reinforcement Learning (DRL) to solve the MCBLP. We applied the ReCovNet to address a real-world billboard location problem in New York City. To assess its performance, we implemented various algorithms such as Gurobi solver, Genetic Algorithm (GA) and a deep learning baseline called Attention Model (AM). The Gurobi reports the optimal solutions, while GA and AM serve as benchmark algorithms. Our proposed approach achieves a good balance between efficiency and accuracy and effectively solves MCBLP. The ReCovNet introduced in our study has potential to improve advertising effectiveness, and our proposed approach offers novel insights for addressing the MCBLP.",unknown,335
10.1016/j.aap.2024.107493,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85185306410,scopus,2024-04-01,scopus,a spatiotemporal deep learning approach for pedestrian crash risk prediction based on poi trip characteristics and pedestrian exposure intensity,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185306410&origin=inward,"
                  Pedestrians represent a population of vulnerable road users who are directly exposed to complex traffic conditions, thereby increasing their risk of injury or fatality. This study first constructed a multidimensional indicator to quantify pedestrian exposure, considering factors such as Point of Interest (POI) attributes, POI intensity, traffic volume, and pedestrian walkability. Following risk interpolation and feature engineering, a comprehensive data source for risk prediction was formed. Finally, based on risk factors, the VT-NET deep learning network model was proposed, integrating the algorithmic characteristics of the VGG16 deep convolutional neural network and the Transformer deep learning network. The model involved training non-temporal features and temporal features separately. The training dataset incorporated features such as weather conditions, exposure intensity, socioeconomic factors, and the built environment. By employing different training methods for different types of causative feature variables, the VT-NET model analyzed changes in risk features and separately trained temporal and non-temporal risk variables. It was used to generate spatiotemporal grid-level predictions of crash risk across four spatiotemporal scales. The performance of the VT-NET model was assessed, revealing its efficacy in predicting pedestrian crash risks across the study area. The results indicated that areas with concentrated crash risks are primarily located in the city center and persist for several hours. These high-risk areas dissipate during the late night and early morning hours. High-risk areas were also found to cluster in the city center; this clustering behavior was more prominent during weekends compared to weekdays and coincided with commercial zones, public spaces, and educational and medical facilities.
               ",unknown,336
10.1016/j.atmosenv.2024.120374,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85184578739,scopus,2024-04-01,scopus,"exposure to particulate matter and ozone, locations of regulatory monitors, and sociodemographic disparities in the city of rio de janeiro: based on local air pollution estimates generated from machine learning models",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184578739&origin=inward,"South America is underrepresented in research on air pollution exposure disparities by sociodemographic factors, although such disparities have been observed in other parts of the world. We investigated whether exposure to and information about air pollution differs by sociodemographic factors in the city of Rio de Janeiro, the second most populous city in Brazil with dense urban areas, for 2012–2017. We developed machine learning-based models to estimate daily levels of O3, PM10, and PM2.5 using high-dimensional datasets from satellite remote sensing, atmospheric and land variables, and land use information. Cross-validations demonstrated good agreement between the estimated levels and measurements from ground-based monitoring stations: overall R 2 of 76.8 %, 63.9 %, and 69.1 % for O3, PM2.5, and PM10, respectively. We conducted univariate regression analyses to investigate whether long-term exposure to O3, PM2.5, PM10 and distance to regulatory monitors differs by socioeconomic indicators, the percentages of residents who were children (0-17 years) or age 65+ years in 154 neighborhoods. We also examined the number of days exceeding the Brazilian National Air Quality Standard (BNAQS). Long-term exposures to O3 and PM2.5 were higher in more socially deprived neighborhoods. An interquartile range (IQR) increment of the social development index (SDI) was associated with a 3.6 μg/m3 (95 % confidence interval [CI]: 2.9, 4.4; p-value≤0.001) decrease in O3, and 0.3 μg/m3 (95 % CI: 0.2, 0.5; p-value = 0.010) decrease in PM2.5. An IQR increase in the percentage of residents who are children was associated with a 4.1 μg/m3 (95 % CI: 3.1, 5.0; p-value≤0.001) increase in O3, and 0.4 μg/m3 (95 % CI: 0.3, 0.6; p-value = 0.009) increase in PM2.5. An IQR increase in the percentage of residents age ≥65was associated with a 3.3 μg/m3 (95 % CI: 2.4, 4.3; p-value=<0.001) decrease in O3, and 0.3 μg/m3 (95 % CI: 0.1, 0.5; p-value = 0.058) decrease in PM2.5. There were no apparent associations for PM10. The association for daily O3 levels exceeding the BNAQS daily standard was 0.4 %p–0.8 %p different by the IQR of variables, indicating a 7–15 days difference in the six-year period. The association for daily PM2.5 levels exceeding the BNAQS daily standard showed a 0.7–1.5 %p difference by the IQR, meaning a 13–27 days difference in the period. We did not find statistically significant associations between the distance to monitors and neighborhood characteristics but some indication regarding SDI. We found that O3 levels were higher in neighborhoods situated farther from monitoring stations, suggesting that elevated levels of air pollution may not be routinely measured. Exposure disparity patterns may vary by pollutants, suggesting a complex interplay between environmental and socioeconomic factors in environmental justice.",unknown,337
10.1016/j.techsoc.2024.102471,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85184027069,scopus,2024-03-01,scopus,trustworthy ai in the public sector: an empirical analysis of a swedish labor market decision-support system,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184027069&origin=inward,"This paper investigates the deployment of Artificial Intelligence (AI) in the Swedish Public Employment Service (PES), focusing on the concept of trustworthy AI in public decision-making. Despite Sweden's advanced digitalization efforts and the widespread application of AI in the public sector, our study reveals significant gaps between theoretical ambitions and practical outcomes, particularly in the context of AI's trustworthiness. We employ a robust theoretical framework comprising Institutional Theory, the Resource-Based View (RBV), and Ambidexterity Theory, to analyze the challenges and discrepancies in AI implementation within PES. Our analysis shows that while AI promises enhanced decision-making efficiency, the reality is marred by issues of transparency, interpretability, and stakeholder engagement. The opacity of the neural network used by the agency to assess jobseekers’ need for support and the lack of comprehensive technical understanding among PES management contribute to the challenges in achieving transparent and interpretable AI systems. Economic pressures for efficiency often overshadow the need for ethical considerations and stakeholder involvement, leading to decisions that may not be in the best interest of jobseekers. We propose recommendations for enhancing AI's trustworthiness in public services, emphasizing the importance of stakeholder engagement, particularly involving jobseekers in the decision-making process. Our study advocates for a more nuanced balance between the use of advanced AI technologies and the leveraging of internal resources such as skilled personnel and organizational knowledge. We also highlight the need for improved AI literacy among both management and personnel to effectively navigate AI's integration into public decision-making processes. Our findings contribute to the ongoing debate on trustworthy AI, offering a detailed case study that bridges the gap between theoretical exploration and practical application. By scrutinizing the AI implementation in the Swedish PES, we provide valuable insights and guidelines for other public sector organizations grappling with the integration of AI into their decision-making processes.",unknown,349
10.1016/j.bbrep.2024.101638,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85182549513,scopus,2024-03-01,scopus,the predictive role of the nomogram based on clinical characteristics and thromboelastography markers for rebleeding after hypertensive intracerebral hemorrhage,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182549513&origin=inward,"Objectives Rebleeding after hypertensive intracerebral hemorrhage is a common and serious postoperative complication in neurosurgery, with high mortality and mental disability rates. The aim of this study was to establish a nomogram to analyze the role of thromboelastography in predicting rebleeding after hypertensive intracerebral hemorrhage. Basic methods We selected 375 patients with hypertensive intracerebral hemorrhage who underwent surgical treatment in Yuebei People's Hospital of Shaoguan City, Guangdong Province from May 2018 to August 2022, and retrospectively analyzed the relevant data of hypertensive intracerebral hemorrhage patients (including general data and clinical thromboelastography data), and analyzed the factors and thromboelastography parameters that affect rebleeding after surgery (45 cases, defined as re-examination of head CT within 72 h after surgery showed that the hematoma in the surgical area exceeded 20 ml). Main results Time from symptom onset to surgery, taking antiplatelet drugs, taking anticoagulant drugs, diabetes mellitus, difficulty in hemostasis during surgery, R value and EPL value in thromboelastography were risk factors for rebleeding after hypertensive intracerebral hemorrhage (P < 0.05). Logistic regression was used to determine the independent risk factors, and based on these risk factors, a nomogram was established and internally validated using a bootstrap method. ROC curve analysis showed that the nomogram model had high diagnostic value for rebleeding after hypertensive intracerebral hemorrhage, with AUC of 0.7314. The calibration curve of the nomogram showed good consistency between the predicted probabilities and the observed values. The decision curve analysis and clinical impact curve also revealed the potential clinical usefulness of the nomogram. Conclusions The nomogram based on clinical characteristics and thromboelastography markers may be useful for predicting rebleeding after hypertensive intracerebral hemorrhage.",unknown,350
10.1016/j.atech.2023.100379,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85179794508,scopus,2024-03-01,scopus,digitalization and agricultural transformation in developing countries: empirical evidence from tanzania agriculture sector,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179794508&origin=inward,"Agribusiness can potentially increase value for agriculture and the entire value chain, but it is hard to achieve these goals without digitalising the sector. The transformation reforms modern technology and digital infrastructure adoption to facilitate growth and inclusivity. These conditions make the agricultural digitization transformation more appealing. Therefore, using a survey of 400 smallholder farmers in four regions of Rukwa, Ruvuma, Morogoro and Tabora, this study examines the interplay between digitalization and agriculture transformation in Tanzania by focusing on the determinants and effects of digitalization on smallholder farmers' welfare. The study used the double hurdle model to estimate the determinants and extent of digitalization adoption; the Two-stage least square (2SLS) and the Control function approach (CFA) to estimate its effects on smallholder farmers' welfare. The findings revealed that access to credit (0.17428, p < 0.01), extension services (0.52884, p < 0.01), education and government support (0.10922, p < 0.05) are key determinants for agricultural digitalization. Moreover, digitalization was found to influence smallholder farmers' welfare significantly. The benefits of digitalization across smallholder farmers in all regions were observed in enhancing extension services (76.50 %), pest management (71.25 %), and accessibility of the market information (72.25 %), enhance an access to financial services (74.75 %). In sum, to promote digital technology adoption among smallholder farmers, policymakers in Tanzania and developing nations should prioritize financial inclusion through tailored microfinance programs, educational initiatives must integrate digital literacy, enhance extension services, subsidized technology access, and encouraging public-private partnerships to create incentives for sustainable agricultural development.",unknown,351
10.1016/j.jretconser.2023.103620,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85178232437,scopus,2024-03-01,scopus,retail store location screening: a machine learning-based approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178232437&origin=inward,"
                  With numerous location choices across dispersed markets and a lack of detailed store-level information, the initial screening process for selecting store locations is challenging. We propose a machine learning-based model that uses public city-, competitor-, and point-of-interest (POI)-level data, including target group indices (TGIs), and apply machine learning to recommend sites based on predicted store performance. We demonstrate the effectiveness of our approach with real data from a jewelry retailing chain. Three machine learning approaches were developed and tested using data from 743 same-brand jewelry stores, and we find that a customized sequential ensemble model performs the best and outperforms the best available industry benchmarks. Our approach offers a new scalable and cost-efficient screening process for retailers to identify potentially top-performing locations.
               ",unknown,352
10.1016/j.heliyon.2024.e25953,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85187275992,scopus,2024-02-29,scopus,is ai changing learning and assessment as we know it? evidence from a chatgpt experiment and a conceptual framework,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187275992&origin=inward,"ChatGPT, a state-of-the-art chatbot built upon Open AI's generative pre-trained transformer, has generated a major public interest and caused quite a stir in the higher education sector, where reactions have ranged from excitement to consternation. This paper therefore examines the potential impact of ChatGPT on learning and assessment, using the example of academic essays, being a major form of assessment with widespread applications of ChatGPT. This provides an opportunity to unpack broader insights on the challenge of generative AI's to the relevance, quality and credibility of higher education learning in a rapidly changing 21st century knowledge economy. We conducted a quasi-experiment in which we deployed ChatGPT to generate academic essays in response to a typical assessment brief, and then subjected the essays to plagiarism checks and independent grading. The results indicate that ChatGPT is able to generate highly original, and high quality, contents from distinct individual accounts in response to the same assessment brief. However, it is unable to generate multiple original contents from the same account, and it struggled with referencing. The discussion highlights the need for higher education providers to rethink their approach to assessment, in response to disruption precipitated by artificial intelligence. Thus, following the discussion of empirical data, we propose a new conceptual framework for AI-assisted assessment for lifelong learning, in which the parameters of assessment extend beyond knowledge (know what) testing, to competence (know how) assessment and performance (show how) evaluation.",unknown,353
10.1016/j.tra.2024.103976,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85182632597,scopus,2024-02-01,scopus,understanding user equilibrium states of road networks: evidence from two chinese mega-cities using taxi trajectory mining,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182632597&origin=inward,"
                  User equilibrium (UE) has long been regarded as the cornerstone of transport planning studies. Despite its fundamental importance, our understanding of the actual UE state of road networks has remained surprisingly incomplete. Using big datasets of taxi trajectories, this study investigates the UE states of road networks in two Chinese mega-cities, i.e., Wuhan and Shenzhen. Effective indicators, namely relative gaps, are introduced to quantify how actual traffic states deviate from theoretical UE states. Advanced machine learning techniques, including XGBoost and SHAP values, are employed to analyze nonlinear relationships between network disequilibrium states and seven influencing factors extracted from trajectory data. The results in these two study areas reveal consistent and significant gaps between actual traffic states and the theoretical UE states at various times of the day during both weekdays and weekends. The XGBoost analysis shows that differences in travel distances, travel speeds, and signalized intersection numbers among alternative routes are the primary causes of road network disequilibrium. The results of this study could present several important methodological and policy implications for using the UE models in transport applications.
               ",unknown,363
10.1016/j.energy.2023.129795,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85179044354,scopus,2024-02-01,scopus,"an evolutionary deep learning model based on ewkm, random forest algorithm, ssa and bilstm for building energy consumption prediction",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179044354&origin=inward,"
                  Accurate prediction of building energy consumption is crucial to the rational scheme of building energy. Combining the entropy-weighted K-means (EWKM) with the random forest (RF) method, a feature selection (EWKM-RF) method is proposed in this paper. Based on the proposed EWKM-RF method, the classification and feature selection of the energy consumption influencing factors can be achieved exclusively. Meanwhile, based on the EWKM-RF method and the bi-directional long-short-term memory neural network (BiLSTM) optimized by the sparrow search algorithm (SSA), an RF-SSA-BiLSTM prediction model for building energy consumption is established in this paper. As the weight, learning rate, and hidden layer node parameters of the BiLSTM neural network are optimized with the SSA, the constraints for manually adjusting parameters are avoided in the proposed prediction model. To examine the accuracy of the proposed model, energy consumption data of a civil public building in Dalian city are collected and tested. Results show the prediction error of RF-SSA-BiLSTM after feature selection is reduced by 24.55 % in high and low energy consumption months. Compared with RF-BiLSTM, RF-PSO-BiLSTM, and RF-CNN-BiLSTM, the RF-SSA-BiLSTM has strong robustness. The average MAE, RMSE and MAPE values of energy consumption prediction in the four seasons are 1.30, 1.63 and 0.02.
               ",unknown,365
10.1016/j.heliyon.2024.e24724,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85182980890,scopus,2024-01-30,scopus,quantifying the contribution of environmental variables to cyclists’ exposure to pm<inf>2.5</inf> using machine learning techniques,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182980890&origin=inward,"Cyclists are particularly vulnerable to travel-related exposure to air pollution. Understanding the factors that increase exposure is crucial for promoting healthier urban environments. Machine learning models have successfully predicted air pollutant concentrations, but they tend to be less interpretable than classical statistical ones, such as linear models. This study aimed to develop a predictive model to assess cyclists’ exposure to fine particulate matter (PM2.5) in urban environments. The model was generated using geo-temporally referenced data and machine learning techniques. We explored several models and found that the gradient boosting machine learning model best fitted the PM2.5 predictions, with a minimum root mean square error value of 5.62 μg m−3. The variables with greatest influence on cyclist exposure were the temporal ones (month, day of the week, and time of the day), followed by meteorological variables, such as temperature, relative humidity, wind speed, wind direction, and atmospheric pressure. Additionally, we considered relevant attributes, which are partially linked to spatial characteristics. These attributes encompass street typology, vegetation density, and the flow of vehicles on a particular street, which quantifies the number of vehicles passing a given point per minute. Mean PM2.5 concentration was lower in bicycle paths away from vehicular traffic than in bike lanes along streets. These outcomes underscore the need to thoughtfully design public transportation routes, including bus routes, concerning the network of bicycle pathways. Such strategic planning attempts to improve the air quality in urban landscapes.",unknown,368
10.1016/j.desal.2023.117085,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85174720410,scopus,2024-01-15,scopus,leveraging chatgpt and bard: what does it convey for water treatment/desalination and harvesting sectors?,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174720410&origin=inward,"
                  Artificial intelligence (AI) has emerged as a prominent tool in the modern day. The utilization of AI and advanced language models such as chat generative pre-trained transformer (ChatGPT) and Bard is not only innovative but also crucial for handling challenges related to water research. ChatGPT is an AI chatbot that uses natural language processing to create humanlike conversations. ChatGPT has recently gained considerable public interest, owing to its unique ability to simplify tasks from various backgrounds. Similarly, Google introduced Bard, an AI-powered chatbot to simulate human conversations. Herein, we investigated how ChatGPT and Bard (AI powdered chatbots) tools can impact water research through interactive sessions. Typically, ChatGPT and Bard offer significant benefits to various fields, including research, education, scientific publications, and outreach. ChatGPT and Bard simplify complex and challenging tasks. For instance, 50 important questions about water treatment/desalination techniques and 50 questions about water harvesting techniques were provided to both chatbots. Time analytics was performed by ChatGPT 3.5, and Bard was used to generate full responses. In particular, the effectiveness of this emerging tool for research purposes in the field of conventional water treatment techniques, advanced water treatment techniques, membrane technology and seawater desalination has been thoroughly demonstrated. Moreover, potential pitfalls and challenges were also highlighted. Thus, sharing these experiences may encourage the effective and responsible use of Bard and ChatGPT in research purposes. Finally, the responses were compared from the perspective of an expert. Although ChatGPT and Bard possess huge benefits, there are several issues, which are discussed in this study. Based on this study, we can compare the abilities of artificial intelligence and human intelligence in water sector research.
               ",unknown,373
10.1016/j.procs.2024.04.309,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85196417163,scopus,2024-01-01,scopus,an intelligent and deep learning approach for pothole surveillance smart application,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85196417163&origin=inward,"The frequency of potholes and speed bumps on the roads has linearly increased due to ageing, inadequate maintenance, and a growth in the number of vehicles. We have discussed a prototype for the identification of potholes and speed bumps in order to guarantee safe mobility. Such critical road quality data may be made public or shared with local governments and other government agencies for the purpose of corrective action and road maintenance. A pothole detection system using image processing and cloud storage can be used to detect potholes from footage provided by a device, store the information into a cloud database, and analyze the data to identify areas that need repair. The system can also send alerts to users about upcoming potholes and notify road maintenance authorities of new pothole detections. In brief, the system uses image processing to detect potholes and cloud storage to store and analyze the data. It can also send alerts to users and road maintenance authorities. The model was built using YOLOv5 and trained on 70% of the dataset images. It was able to detect potholes with good accuracy in the output images.",unknown,444
10.1016/j.procs.2024.05.101,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85195417875,scopus,2024-01-01,scopus,artificial intelligence as an innovative element of support in policing,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85195417875&origin=inward,"Currently, the public security sector is faced with an increasing administrative burden that limits the ability of police officers to focus on core security tasks. This paper focuses on the possibility of using large-scale language models (LSMs) as an innovative tool to address this challenge. Based on a careful literature review and analysis of current trends in artificial intelligence, the author team develops a concept for integrating GPTs into police practice, with an emphasis on the potential for reducing administrative burden and supporting efficient processing of relevant information. As part of this research, we have identified key areas of policing where AI could bring significant value, including data analysis and document production assistance. However, it should be emphasized that this technology is still in its early stages of development and its implementation would require a carefully considered approach involving interdisciplinary collaboration and further research to test the theoretical assumptions presented in this study. Thus, this paper contributes to a deeper understanding of the potential benefits and challenges of integrating GPT into policing practice and outlines a path towards future innovative solutions in the field of public safety.",unknown,451
10.1016/j.eiar.2023.107336,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85176312992,scopus,2024-01-01,scopus,do different types of urban streets lead to varying covid-19 risk perceptions? an empirical study from a spatial heterogeneity perspective,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176312992&origin=inward,"
                  People's risk perception varies across different environments. However, how different environments influence people's COVID-19 risk perception (RP), especially street environments, has yet to be fully evaluated. This study explores how three different types of street environments influence people's COVID-19 RP. First, we used pedestrian-centric street-view images and machine-learning models to predict the severity of COVID-19 RP in the study area. In addition, geographically weighted regression models were constructed to assess the locally most important environmental factors and their spatial heterogeneity. The results showed that openness was the most important influencing factor on COVID-19 RP in all three types of streets, while visual crowdedness was the least important one. Moreover, the high-value clusters of COVID-19 RP mostly occurred in poor- and medium-quality commercial streets, poor-quality residential streets, and leisure streets close to the plaza. Contrastingly, the low-value aggregations of COVID-19 RP mainly appeared in high-quality commercial streets, medium- and high-quality residential streets, and leisure streets close to blue space. We also found that the importance of environmental factors varied greatly by location. Interestingly, the RP was lower with higher visual crowdedness for all types of commercial streets. Based on this, the COVID-19 RP in different types of street environments was comprehensively evaluated so that city-relevant departments can take more effective measures to manage the public health risk based on geographical features.
               ",unknown,507
10.1016/j.cities.2023.104587,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85175540661,scopus,2024-01-01,scopus,use of machine learning in understanding transport dynamics of land use and public transportation in a developing city,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175540661&origin=inward,"
                  Rapid urbanization has resulted in problems with pollution, traffic congestion, and public transportation in cities worldwide. Much urbanization research disproportionately focuses on cities in developed countries, where data is easier to obtain, to the detriment of cities in developing countries. To help address this gap, our work proposes a machine learning methodology that explores how land use patterns contribute to predicting an aspect of public transportation in growing cities (aggregate public vehicle speeds), with Cauayan City in the Philippines as the test case. We trained gradient-boosting tree models with road properties, day and time, and land use data as features to predict vehicle speeds on various roads in the city. Among various feature combinations used in training, the best model used road, time, and land use features over a range buffer radii of 50 to 1000 m, yielding an R
                     2 of 0.676 (0.003) and a root mean squared error of 6.61 (0.03) kph. Apart from predicting speeds, Shapley's additive explanation (SHAP) values show that proximity to dense residential and commercial areas decreases predicted vehicle speeds. Our approach uses interpretable machine learning models to obtain insights about urban transportation in developing cities.
               ",unknown,508
10.1016/bs.adcom.2023.08.001,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85174467151,scopus,2024-01-01,scopus,why is implementing computational intelligence for social good so challenging? principles and its application,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174467151&origin=inward,"
                  Computational intelligence (CI) has the potential to help tackle some of the world's most challenging social problems. Real-life examples of AI are already being applied in about one-third of these use cases They range from diagnosing cancer to helping blind people navigate their surroundings, identifying victims of online sexual exploitation, and aiding disaster-relief efforts etc. AI is only part of a much broader tool kit of measures that can be used to tackle societal issues, however. For now, issues such as data accessibility and shortages of AI talent constrain its application for social good. This chapter has grouped use cases into 10 social-impact domains based on taxonomies in use among social-sector organizations. Each use case highlights a type of meaningful problem that can be solved by one or more AI capability. The cost of human suffering, and the value of alleviating it, are impossible to gauge and compare. Nonetheless, employing usage frequency as a proxy, we measure the potential impact of different AI capabilities.
               ",unknown,511
10.1016/j.scitotenv.2023.167234,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85172918289,scopus,2024-01-01,scopus,an advanced deep learning predictive model for air quality index forecasting with remote satellite-derived hydro-climatological variables,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172918289&origin=inward,"
                  Forecasting the air quality index (AQI) is a critical and pressing challenge for developing nations worldwide. With air pollution emerging as a significant threat to the environment, this study considers seven study sites of the sub-tropical region in Bangladesh and introduces a novel hybrid deep-learning model. The proposed model, expressed as CLSTM-BiGRU, integrates a convolutional neural network (CNN), a long-short term memory (LSTM), and a bi-directional gated recurrent unit (BiGRU) network. Leveraging nineteen remotely sensed predictor variables and harnessing the grey wolf optimization (GWO) algorithm, the CLSTM-BiGRU model showcases its superiority in air quality forecasting. It consistently outperforms the benchmark models, yielding lower forecasting errors and higher efficiency (i.e., correlation coefficient 
                        ~
                     1) values. Hence, this study underscores the feasibility and substantial potential of the hybrid deep learning model, which can provide precise forecasts of air quality index, and will be highly useful for relevant stakeholders and decision-makers. Furthermore, the adaptability and potential utility of this innovative model may be ascertained for air quality monitoring and effective public health risk mitigation in urban environments.
               ",unknown,512
10.1016/j.ecolind.2023.111204,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85175650177,scopus,2023-12-01,scopus,quantifying the spatial quality of urban streets with open street view images: a case study of the main urban area of fuzhou,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175650177&origin=inward,"With the advancement of global urbanization, urban streetscapes have become a critical part of urban public spaces. As one of the important ways to enhance the living environment, relevant government departments have accelerated the construction of urban streetscapes; however, certain issues, such as emphasis on surface and light protection, still persist. The visual effects of street landscapes hardly meet people's daily and spiritual needs. Therefore, assessing the quality of urban streetscapes has scientific and practical significance for improving urban planning and construction and creating a new intelligent and green city. In this study, the spatial distribution of vehicle interference index, spatial feasibility index, road area index, green visual index, sky visibility index, spatial enclosure index, color richness index, and visual entropy evaluation indices were drawn from the framework of combining deep learning and machine learning to quantify the spatial quality of urban streets by integrating a large number of streetscape images. The spatial quality of street landscape was evaluated according to the ELO scoring mechanism and random forest method, taking the main urban area of Fuzhou City as an example. The results showed that the overall street landscape quality of Fuzhou City was high, and the spatial distribution showed a pattern of concentrated high values in the center and dispersed high values in the periphery. Therefore, based on quantifying the spatial quality of urban streets, strategies, such as the pavement layout of pedestrian and vehicular paths, control of the continuity and permeability of visual landscape elements, and plant color matching, should be regulated. The method proposed in this study can provide novel insights for urban builders to improve the construction of cities.",unknown,524
10.1016/j.rineng.2023.101495,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85174858674,scopus,2023-12-01,scopus,a spatial design layout optimization model based on deep learning in the context of rural revitalization,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174858674&origin=inward,"In recent years, with the transformation of China's economy and social development, solving rural problems and achieving rural revitalization has become an urgent task. The development of information technology, specifically the advancements in deep learning technology, has provided new solutions for rural revitalization. This article aims to optimize the spatial structure of rural residential areas and investigate models for optimizing spatial design layouts within the context of rural revitalization utilizing deep learning techniques, specifically convolutional neural networks (DeepLab v3+). This article conducts in-depth research on the DeepLab v3+model and finds that it has good feature extraction ability in processing spatial information such as rural residential houses and ancillary land. By introducing an attention mechanism in the encoding region, we improved the feature extraction accuracy of the model. On the ImageNet and CoCo2017 datasets, the accuracy indicators PA, IoU, and F1 of the DeepLab v3+model performed well, reaching 98.25 %, 75.61 %, and 86.49 %, as well as 98.31 %, 74.91 %, and 85.49 %, respectively. Furthermore, it was discovered that as the training frequency approached 20, both ResNet50 and DeepLab v3-SE exhibited a generalization accuracy of 86.83 % and 89.26 %, correspondingly. This shows the effectiveness of our model in acquiring and utilizing knowledge during the training. The study further investigated the performance of the model in processing images with noise. When the noise standard deviation is 25, the mean SSIM of the O-DeepLab v3+network reaches 0.943, which is 0.53 % and 1.95 % higher than the DeepLab v3-SE network and traditional DeepLab v3+network, respectively. When the noise standard deviation is 50, the SSIM mean of the O-DeepLab v3+network reaches 0.82, which is 0.61 % and 1.49 % higher than the DeepLab v3-SE network and traditional DeepLab v3+network, respectively. This suggests that our model exhibits robustness when processing noisy images. In summary, the spatial design layout optimization model based on deep learning in the context of rural revitalization studied in this article has high accuracy and stability in processing spatial information such as rural residential houses and ancillary land. This is of great significance for optimizing rural land use, promoting urban-rural land allocation, and achieving rural revitalization. Therefore, this study has certain application value.",unknown,525
10.1016/j.wace.2023.100621,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85174802637,scopus,2023-12-01,scopus,forecasting daily extreme temperatures in chinese representative cities using artificial intelligence models,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174802637&origin=inward,"Accurate daily extreme temperature forecasts are important for public safety. However, traditional Numerical Weather Prediction (NWP) methods for temperature forecasting become increasingly costly and time-consuming as forecast resolution improves. To overcome these challenges, recent studies have embraced artificial intelligence (AI) techniques. In this paper, we select five AI models: Multiple linear regression, Support Vector Regressor, Gradient Boosting Regression Tree (GBRT), Long Short-Term Memory, and multilayer perceptron (MLP) to forecast extreme temperatures in nine representative Chinese cities, which are Harbin, Xian, Beijing, and Lhasa in the North and Shanghai, Guiyang, Guangzhou, Shenzhen, and Haikou in the South. To assess the performances, we compare AI models with two benchmarks: the persistence method by using the previous day's temperature extremes directly, and the prediction from the NWP method of the Global Forecast System (GFS) provided by the National Centers for Environmental Prediction. Among the 7 models forecasting daily extreme temperatures in nine cities, the MLP model outperforms the best, followed by the GBRT model. Compared with the persistence forecast, all 5 AI models and GFS show forecasting ability for daily maximum temperature. However, the GFS performs worse than the persistence model for daily minimum temperature. Specifically, for the MLP model, the average Mean Absolute Errors (MAEs) for forecasting daily maximum and minimum temperatures in the 9 cities are 1.61 °C and 1.14 °C, reducing the GFS model's MAEs for daily maximum and minimum temperatures forecasts by 15% and 45%, respectively. Overall, AI models show promise in enhancing temperature forecasting accuracy.",unknown,526
10.1016/j.onehlt.2023.100642,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85173695445,scopus,2023-12-01,scopus,antibiotic resistance genes prevalence prediction and interpretation in beaches affected by urban wastewater discharge,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173695445&origin=inward,"Background The annual death toll of over 1.2 million worldwide is attributed to infections caused by resistant bacteria, driven by the significant impact of antibiotic misuse and overuse in spreading these bacteria and their associated antibiotic resistance genes (ARGs). While limited data suggest the presence of ARGs in beach environments, efficient prediction tools are needed for monitoring and detecting ARGs to ensure public health safety. This study aims to develop interpretable machine learning methods for predicting ARGs in beach waters, addressing the challenge of black-box models and enhancing our understanding of their internal mechanisms. Methods In this study, we systematically collected beach water samples and subsequently isolated bacteria from these samples using various differential and selective media supplemented with different antibiotics. Resistance profiles of bacteria were determined by using Kirby-Bauer disk diffusion method. Further, ARGs were enumerated by using the quantitative polymerase chain reaction (qPCR) to detect and quantify ARGs. The obtained qPCR data and hydro-meteorological were used to create an ML model with high prediction performance and we further used two explainable artificial intelligence (xAI) model-agnostic interpretation methods to describe the internal behavior of ML model. Results Using qPCR, we detected bla CTX−M, bla NDM, bla CMY, bla OXA, bla tetX, bla sul1, and bla aac(6′-Ib-cr) in the beach waters. Further, we developed ML prediction models for bla aac(6′-Ib-cr), bla sul1, and bla tetX using the hydro-metrological and qPCR-derived data and the models demonstrated strong performance, with R2 values of 0.957, 0.997, and 0.976, respectively. Conclusions Our findings show that environmental factors, such as water temperature, precipitation, and tide, are among the important predictors of the abundance of resistance genes at beaches.",unknown,527
10.1016/j.jfca.2023.105661,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85171470610,scopus,2023-12-01,scopus,plant growth regulator residues in fruits and vegetables marketed in yinchuan and exposure risk assessment,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171470610&origin=inward,"
                  Data on plant growth regulators (PGRs) in common fruits and vegetables remain deficient. Supermarkets and farmers’ markets in three districts, two counties, and one city in Yinchuan with high traffic flow were selected and randomly sampled using the five-point sampling method. The QuEChERS pre-treatment method, combined with high-performance liquid chromatography-mass spectrometry, was used for detection. High-end exposure to PGRs was calculated via the chronic dietary risk assessment method from the point assessment model. The linearity of the components was good, (range: 0.2–100 ng/mL) with correlation coefficients ≥ 0.9962. Of the 25 PGRs examined, 16 were detected. The exceedances were as follows: chlormequat chloride and succinic acid 2,2-dimethylhydrazide in three and one batches of fruits, respectively, and gibberellin in four and six batches of vegetables, respectively. The same batch of fruits and vegetables could contain multiple PGRs, with seven PGRs detected in green dates and six in strawberries. In total, 10 batches of sprouts, six batches of fresh mushrooms, five of fruits and vegetables, one of leafy vegetables, and one of melons had excessive amounts of PGRs. The mean and median exposures were 0.0–0.832 mg/d and 0.0–0.453 mg/d, respectively. Although individual exceedances of PGRs were observed, most were at low levels, indicating that the corresponding risk was acceptable for human consumption.
               ",unknown,528
10.1016/j.isci.2023.108124,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85173700948,scopus,2023-11-17,scopus,the rapid increase of urban contaminated sites along china's urbanization during the last 30 years,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173700948&origin=inward,"Contaminated sites pose serious threats to the soil environment and human health. However, the location and temporal changes of urban contaminated sites across China remain unknown due to data scarcity. Here, we developed a machine-learning model to identify the contaminated sites using public data. Results show that the trained model with 2,005 surveyed site samples and six variables can achieve a model performance evaluation value of 0.86. 43,676 contaminated sites were identified from 83,498 polluting enterprise plots in China. However, these contaminated sites have significant spatiotemporal heterogeneity, mainly located in economically developed provinces, urban agglomerations, and core urban areas. Moreover, the contaminated sites increased by 325% along with urban expansion from 1990 to 2018. The abandoned contaminated sites increased rapidly, but the contaminated sites in production decreased continuously. This methodological framework and our findings contribute to the precise management of contaminated sites and provide insights into urban sustainable development.",unknown,543
10.1016/j.ajogmf.2023.101151,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85174024867,scopus,2023-11-01,scopus,use of integrase inhibitors vs protease inhibitors is associated with improved hiv viral suppression,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174024867&origin=inward,"
                  BACKGROUND
                  Current guidelines for antiretroviral therapy in pregnancy include the use of a dual-nucleoside reverse transcriptase inhibitor with either an integrase strand transfer inhibitor or a ritonavir-boosted protease inhibitor, although there is no designation of which is the preferred option.
               
                  OBJECTIVE
                  This study aimed to compare viral suppression at delivery among patients on dual-nucleoside reverse transcriptase inhibitors combined with either an integrase strand transfer inhibitor or a protease inhibitor. A hypothesis was made that the incidence of viral suppression is higher with the use of a dual-nucleoside reverse transcriptase inhibitor backbone combined with an integrase strand transfer inhibitor than with the use of a dual-nucleoside reverse transcriptase inhibitor backbone combined with a protease inhibitor.
               
                  STUDY DESIGN
                  This study was an observational study of pregnant patients living with HIV who received prenatal care and delivered after 20 weeks of gestation at an urban safety net hospital. All pregnant patients with HIV were referred to a centralized clinic for HIV counseling, medication management, and prenatal care. Antiretroviral therapy was continued or initiated according to protocols based on national guidance. Among patients on a dual-nucleoside reverse transcriptase inhibitor backbone combined with integrase strand transfer inhibitor vs protease inhibitor at delivery, we compared the demographics and HIV disease characteristics, including year of diagnosis, viral load, and antiretroviral therapy class. The outcome of interest was viral suppression at delivery, defined as a viral load of <50 copies/mL.
               
                  RESULTS
                  From January 2011 to December 2021, 604 patients on dual-nucleoside reverse transcriptase inhibitor met the inclusion criteria, including 411 patients (68%) on protease inhibitor and 193 patients (32%) on integrase strand transfer inhibitor at delivery. Demographic distribution was similar, and prenatal care was initiated at 12 weeks of gestation. Among the integrase strand transfer inhibitor group, 101 (17%) were on antiretroviral therapy at initiation of prenatal care compared with 169 (28%) in the protease inhibitor group. At delivery, the frequency of viral load suppression was higher among those on an integrase strand transfer inhibitor (147/193 [76%]) than among those on a protease inhibitor (275/411 [67%]) (odds ratio, 1.59; 95% confidence interval, 1.08–2.33). Among those with a detectable virus, quantitative viral load was not different. During the study period, the use of a protease inhibitor decreased, whereas the use of an integrase strand transfer inhibitor increased.
               
                  CONCLUSION
                  Among pregnant patients living with HIV, viral suppression was more common among those on a dual-nucleoside reverse transcriptase inhibitor backbone combined with integrase strand transfer inhibitor than among those on a dual-nucleoside reverse transcriptase inhibitor backbone protease inhibitor at delivery. Our results support the use of dual-nucleoside reverse transcriptase inhibitor with integrase strand transfer inhibitor as a first-line antiretroviral therapy regimen in pregnancy.
               ",unknown,550
10.1016/j.engappai.2023.107089,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85171750552,scopus,2023-11-01,scopus,crowddcnn: deep convolution neural network for real-time crowd counting on iot edge,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171750552&origin=inward,"
                  Ensuring the safety and security of crowded places is a major concern for both the government and the public. Accurately and quickly estimating the number of people in a crowd is crucial for public safety, urban planning, and traffic monitoring. The existing methods have a minimum mean square error of 0.89 and require high storage space, making them inappropriate for crowd counting using low-computation and small-storage devices like single-board computers. Furthermore, these methods suffer from prediction time lag and are not suitable for live streaming. To tackle these challenges, this paper proposes a Deep Convolution Neural Network-based ‘CrowdDCNN’ model for crowd counting. This model reduces the value of mean square error by 0.29 and the size of the model by 80.01%. Additionally, the prediction time was decreased to 700 ms. Further, the ‘NoLag’ algorithm introduced in this paper is efficient in live crowd counting. Its O(1) time and space complexity make it appropriate for all devices, including single-board computers, laptops, and GPUs. The reported 0-p value during statistical analysis using Wilcoxon test, Friedman rank test, and paired t-test validates the superiority of the proposed model.
               ",unknown,552
10.1016/j.jad.2023.07.117,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85167435943,scopus,2023-11-01,scopus,urban-rural disparities in the prevalence and trends of depressive symptoms among chinese elderly and their associated factors,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167435943&origin=inward,"Background This study aimed to examine urban-rural disparities in the prevalence and trends of depressive symptoms (DS) among Chinese elderly and associated factors. Methods A total of 8025, 7808, and 4887 respondents aged 60 years and above were selected from the China Family Panel Studies (CFPS) in 2016, 2018, and 2020, respectively. DS was assessed using a short version of Center for Epidemiologic Studies Depression Scale (CES-D). Twenty-two associated factors from six categories were included in random forest (RF) models. All urban-rural comparisons were conducted based on good model performance. Results The DS prevalence among all rural elderly was significantly higher than corresponding urban elderly. This disparity continued to widen among younger elderly, while it continued to narrow among older elderly. The top 10 common leading factors were sleep quality, self-rated health, life satisfaction, memory ability, child relationship, IADL disability, marital status, educational level, and gender. Urban-rural disparities in sleep quality, interpersonal trust, and child relationship continued to widen, while disparities in multimorbidity, hospitalization status, and frequency of family dinner continued to narrow. Limitation This study may exist recall bias and lacks causal explanation. Conclusions Significant and continuing disparities in the DS prevalence were observed between urban and rural elderly in China, showing opposite trends in younger and older elderly. The top 10 leading associated factors for DS were nearly consistent across urban and rural elderly, with sleep quality having strongest contribution. Urban-rural disparities in associated factors also showed different trends. This study provides a reference for mental health promotion among Chinese elderly.",unknown,554
10.1016/j.scitotenv.2023.165307,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85166666543,scopus,2023-11-01,scopus,monitoring spore washoff during a biological contamination incident response using automated stormwater samplers and sensors to predict contamination movement,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166666543&origin=inward,"
                  This study examined the washoff of Bacillus globigii (Bg) spores from concrete, asphalt, and grass surfaces by stormwater. Bg is a nonpathogenic surrogate for Bacillus anthracis, which is a biological select agent. Areas (2.74 m × 7.62 m) of concrete, grass, and asphalt were inoculated twice at the field site during the study. Spore concentrations were measured in runoff water after seven rainfall events (1.2–65.4 mm) and complimentary watershed data were collected for soil moisture, depth of water in collection troughs, and rainfall using custom-built telemetry units. An average surface loading of 107.79 Bg spores/m2 resulted in peak spore concentrations in runoff water of 102, 260, and 4.1 CFU/mL from asphalt, concrete, and grass surfaces, respectively. Spore concentrations in the stormwater runoff were greatly reduced by the third rain event after both inoculations, but still detectable in some samples. When initial rainfall events occurred longer after the initial inoculation, the spore concentrations (both peak and average) in the runoff were diminished. The study also compared rainfall data from 4 tipping bucket rain gauges and a laser disdrometer and found they performed similarly for values of total rainfall accumulation while the laser disdrometer provided additional information (total storm kinetic energy) useful in comparing the seven different rain events. The soil moisture probes are recommended for assistance in predicting when to sample sites with intermittent runoff. Sampling trough level readings were critical to understanding the dilution factor of the storm event and the age of the sample collected. Collectively the spore and watershed data are useful for emergency responders faced with remediation decisions after a biological agent incident as the results provide insight into what equipment to deploy and that spores may persist in runoff water at quantifiable levels for months. The spore measurements are also a novel dataset for stormwater model parameterization for biological contamination of urban watersheds.
               ",unknown,555
10.1016/j.ijdrr.2023.104018,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85171779869,scopus,2023-10-15,scopus,"mapping the spatial transmission risk and public spatial awareness in the use of personal protective equipment: covid-19 pandemic in east java, indonesia",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171779869&origin=inward,"
                  This study aimed at developing a machine learning-based COVID-19 transmission risk spatial model and an analysis of the public spatial awareness in the use of personal protective equipment (PPE) in their spatial environment (i.e., a spatial-based COVID-19 transmission risk model). Random Forest model combined with Information Gain was used in this study. Twenty-three geospatial variables that passed a feature selection process were inputted to build a spatial-based COVID-19 transmission risk model. The validation outcome reveals that Random Forest (RF) model achieved excellent COVID-19 transmission risk prediction result (AUC value of 94.7%). Transmission risk modeling results show that the moderate-to-high class risk pattern was concentrated in residential areas with complex road infrastructure. The model showed that land use, road infrastructure, minimum temperature, and close to urban areas is important for spatial modeling of COVID-19 transmission risk. The results of the public spatial awareness analysis show variations in respondents' use of PPE based on their spatial environment. They tend to carry one PPE whose use increases significantly from low-risk to high-risk. While being indoor, the majority used hand sanitizer, and a face mask when being outdoor. Thus, developing a spatial-based COVID-19 transmission risk prediction linked with public spatial awareness can be used as a reference to establish health policies or regulations by decision-makers, health boards, and governments to control and simultaneously reduce the risk of transmission more effectively and efficiently.
               ",unknown,566
10.1016/j.foreco.2023.121361,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85169294697,scopus,2023-10-15,scopus,classifying mature federal forests in the united states: the forest inventory growth stage system,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169294697&origin=inward,"
                  President Biden’s executive order, ‘Strengthening the Nation's Forests, Communities, and Local Economies,’ (EO#14072, April 22, 2022) acknowledges the interest in mature and old-growth (MOG) forests by directing U.S. Federal agencies to define and inventory these resources on United States Forest Service (USFS) and Bureau of Land Management (BLM) lands. We propose using an effective and enduring mature forest classification system that could be adaptable to social paradigms, monitoring data streams, scientific information, and global change factors. We accommodate these design aspects by defining mature forests as a growth stage prior to the onset of old-growth attributes within a proposed Forest Inventory Growth Stage System (FIGSS). FIGSS uses the long-established USFS old-growth assessments often conducted in concert with public dialogues to identify key structural indicators of older forests. The system informs inverse modeling of the prior “mature” stage’s structural thresholds enabling initial population estimates of mature forest extent using the USFS’ nationally consistent Forest Inventory and Analysis (FIA) program data. With this approach, we estimate that approximately 45 percent of all USFS/BLM forest is mature. The FIGSS system is based on a variety of components that could be used to account for cultural values ascribed to forest conditions which could be refined across future versions such as assumptions about the relative length of growth stages, incorporating data from emerging monitoring technologies along with old-growth/mature field sampling campaigns, accommodating spectrums of site-limited and/or disturbance-driven stand development, refined variable selection processes such as machine-learning, and consideration of traditional ecological knowledge.
               ",unknown,567
10.1016/j.jenvman.2023.118482,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85164246514,scopus,2023-10-15,scopus,bk-swmm flood simulation framework is being proposed for urban storm flood modeling based on uncertainty parameter crowdsourcing data from a single functional region,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164246514&origin=inward,"
                  In recent years, urban flood disasters caused by sudden heavy rains have become increasingly severe, posing a serious threat to urban public infrastructure and the life and property safety of residents. Rapid simulation and prediction of urban rain-flood events can provide timely decision-making reference for urban flood control and disaster reduction. The complex and arduous calibration process of urban rain-flood models has been identified as a major obstacle affecting the efficiency and accuracy of simulation and prediction. This study proposes a multi-scale urban rain-flood model rapid construction method framework, BK-SWMM, focusing on urban rain-flood model parameters and based on the basic architecture of Storm Water Management Model (SWMM). The framework comprises two main components: 1) constructing a SWMM uncertainty parameter sample crowdsourcing dataset and coupling Bayesian Information Criterion (BIC) and K-means clustering machine learning algorithm to discover clustering patterns of SWMM model uncertainty parameters in urban functional areas; 2) coupling BIC and K-means with SWMM model to form BK-SWMM flood simulation framework. The applicability of the proposed framework is validated by modeling three different spatial scales in the study regions based on observed rainfall-runoff data. The research findings indicate that the distribution pattern of uncertainty parameters, such as depression storage, surface Manning coefficient, infiltration rate, and attenuation coefficient. The distribution patterns of these seven parameters in urban functional zones indicate that the values are highest in the Industrial and Commercial Areas (ICA), followed by Residential Areas (RA), and lowest in Public Areas (PA). All three spatial scales' RE
                     
                        Q
                     
                     , NSE
                     
                        Q
                     , and 
                        
                           
                              R
                              D
                              2
                           
                        
                      indices were superior to the SWMM and less than 10%, greater than 0.80, and greater than 0.85, respectively. However, when the study area's geographical scale expands, the simulation's accuracy will decline. Further research is required on the scale dependency of urban storm flood models.
               ",unknown,568
10.1016/j.envint.2023.108170,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85169027461,scopus,2023-09-01,scopus,biomonitoring equivalents for perfluorooctanoic acid (pfoa) for the interpretation of biomonitoring data,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169027461&origin=inward,"Background Perfluorooctanoic acid (PFOA) is detected in the blood of virtually all biomonitoring study participants. Assessing health risks associated with blood PFOA levels is challenging because exposure guidance values (EGVs) are typically expressed in terms of external dose. Biomonitoring equivalents (BEs) consistent with EGVs could facilitate health-based interpretations. Objective To i) derive BEs for serum/plasma PFOA corresponding to non-cancer EGVs of the U.S. Environmental Protection Agency (U.S. EPA), the Agency for Toxic Substances and Disease Registry (ATSDR) and Health Canada, and ii) compare with PFOA concentrations from national biomonitoring surveys. Methods Starting from EGV points of departure, we employed pharmacokinetic data/models and uncertainty factors. Points of departure in pregnant rodents (U.S. EPA 2016, ATSDR) were converted into fetus and pup serum concentrations using an animal gestation/lactation pharmacokinetic model, and equivalent human fetus and child concentrations were converted into BEs in maternal serum using a human gestation/lactation model. The point of departure in adult rodents (Health Canada) was converted into a BE using experimental data. For epidemiology-based EGVs (U.S. EPA 2023, draft), BEs were directly based on epidemiological data or derived using a human gestation/lactation pharmacokinetic model. BEs were compared with Canadian/U.S. biomonitoring data. Results Non-cancer BEs (ng/mL) were 684 (Health Canada, 2018) or ranged from 15 to 29 (U.S. EPA, 2016), 6–10 (ATSDR, 2021) and 0.2–0.8 (U.S. EPA, 2023, draft). Ninety-fifth percentiles of serum levels from the 2018–2019 Canadian Health Measures Survey (CHMS) and the 2017–2018 National Health and Nutrition Examination Survey (NHANES) were slightly below the BE for ATSDR, and geometric means were above the non-cancer BEs for the U.S. EPA (2023, draft). Conclusion Non-cancer BEs spanned three orders of magnitude. The lowest BEs were for EGVs based on developmental endpoints in epidemiological studies. Concentrations in Canadian/U.S. national surveys were higher than or close to BEs for the most recent non-cancer EGVs.",unknown,588
10.1016/j.isprsjprs.2023.07.018,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85166964881,scopus,2023-09-01,scopus,screening the stones of venice: mapping social perceptions of cultural significance through graph-based semi-supervised classification,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166964881&origin=inward,"Mapping cultural significance of heritage properties in urban environment from the perspective of the public has become an increasingly relevant process, as highlighted by the 2011 UNESCO Recommendation on the Historic Urban Landscape (HUL). With the ubiquitous use of social media and the prosperous developments in machine and deep learning, it has become feasible to collect and process massive amounts of information produced by online communities about their perceptions of heritage as social constructs. Moreover, such information is usually inter-connected and embedded within specific socioeconomic and spatiotemporal contexts. This paper presents a methodological workflow for using semi-supervised learning with graph neural networks (GNN) to classify, summarize, and map cultural significance categories based on user-generated content on social media. Several GNN models were trained as an ensemble to incorporate the multi-modal (visual and textual) features and the contextual (temporal, spatial, and social) connections of social media data in an attributed multi-graph structure. The classification results with different models were aligned and evaluated with the prediction confidence and agreement. Furthermore, message diffusion methods on graphs were proposed to aggregate the post labels onto their adjacent spatial nodes, which helps to map the cultural significance categories in their geographical contexts. The workflow is tested on data gathered from Venice as a case study, demonstrating the generation of social perception maps for this UNESCO World Heritage property. This research framework could also be applied in other cities worldwide, contributing to more socially inclusive heritage management processes. Furthermore, the proposed methodology holds the potential of diffusing any human-generated location-based information onto spatial networks and temporal timelines, which could be beneficial for measuring the safety, vitality, and/or popularity of urban spaces.",unknown,589
10.1016/j.scs.2023.104680,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85163562282,scopus,2023-09-01,scopus,"comparative mapping of smellscape clusters and associated air quality in kano city, nigeria: an analysis of public perception, hotspots, and inclusive decision support tool",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163562282&origin=inward,"
                  Few studies have effectively analysed the spatial patterns of urban smellscapes, public perception, exposure risks, or design an inclusive decision-support system. The current study coupled multiple methods to measure, map, and compare notable smell emitting spots, associated air quality indices, and public perception in Kano, Nigeria. It has revealed how urban informal activities generate both pleasant and offensive smells that contaminate the air more than the city's transportation and manufacturing. Notably, the most perceived odour hotspots coupled with highly hazardous sites with AQI value of >1200 which were found around the city's main abattoir. Conversely, the most commonly distributed pleasant smell spots are beef and chicken grills whose charcoal burning generate hazardous air quality. The novelty of this study includes its ability to use empirical data to design an inclusive decision support tool that identifies pathways for individuals, vulnerable groups, policymakers, and civil society groups to respond to various exposure risks. The study has identified informality, urban spatial inequalities, densification, and poor spatial planning which appeared to be the key drivers of the observed clustering of pleasant/unpleasant smellscapes juxtaposing with sites of worst air quality.
               ",unknown,592
10.1016/j.hlpt.2023.100748,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85163083863,scopus,2023-09-01,scopus,systematic ai support for decision-making in the healthcare sector: obstacles and success factors,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163083863&origin=inward,"
                  Background
                  Currently, health care is expert-centric, especially with regard to decision-making. Innovations such as artificial intelligence (AI) or interconnected electronic health records (EHRs) suffer from low adoption rates. In the rare cases of technically successful implementation, they often result in inefficient or error-prone processes.
               
                  Aim & Methods
                  This paper explores the state of the art in AI-based digital decision support systems (DDSSs). To overcome the low adoption rates, we propose a systematic strategy for bringing DDSS research into clinical practice based on a design science approach. DDSSs can transform health care to be more innovative, patient-centric, accurate and efficient. We contribute by providing a framework for the successful development, evaluation and analysis of systems for AI-based decision-making. This framework is then evaluated using focus group interviews.
               
                  Results
                  Centred around our framework, we define a systematic approach for the use of AI in health care. Our systematic AI support approach highlights essential perspectives on DDSSs for systematic development and analysis. The aim is to develop and promote robust and optimal practices for clinical investigation and evaluation of DDSS in order to encourage their adoption rates. The framework contains the following dimensions: disease, data, technology, user groups, validation, decision and maturity.
               
                  Conclusion
                  DDSSs focusing on only one framework dimension are generally not successful; therefore, we propose to consider each framework dimension during analysis, design, implementation and evaluation so as to raise the number of DDSSs used in clinical practice.
               
                  Public Interest Summary
                  The digital transformation of the healthcare sector creates the potential for the sector to be more accurate, efficient and patient-centric using AI, or so-called digital decision support systems. In this research, we explore why these systems are needed and how they can be successfully implemented in clinical practice. For this, we propose a systematic approach based on our conceptual framework. Against this background, we present our vision for further advancing these technologies. We see our systematic AI support as a primary driver, with the possibility to facilitate the much-needed breakthrough of decision support systems in health care.
               ",unknown,593
10.1016/j.scitotenv.2023.164282,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85160416762,scopus,2023-09-01,scopus,environmental dna clarifies impacts of combined sewer overflows on the bacteriology of an urban river and resulting risks to public health,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160416762&origin=inward,"There is no reference of microbiological water quality in the European Union's Water Framework Directive, adapted into English law, and consequently microbial water quality is not routinely monitored in English rivers, except for two recently designated bathing water sites. To address this knowledge gap, we developed an innovative monitoring approach for quantitative assessment of combined sewer overflow (CSO) impacts on the bacteriology of receiving rivers. Our approach combines conventional and environmental DNA (eDNA) based methods to generate multiple lines of evidence for assessing risks to public health. We demonstrated this approach by investigating spatiotemporal variation in the bacteriology of the Ouseburn in northeast England for different weather conditions in the summer and early autumn of the year 2021 across eight sampling locations that comprised rural, urban, and recreational land use settings. We characterized pollution source attributes by collecting sewage from treatment works and CSO discharge at the peak of a storm event. CSO discharge was characterized by log10 values per 100 mL (average ± stdev) of 5.12 ± 0.03 and 4.90 ± 0.03 for faecal coliforms and faecal streptococci, and 6.00 ± 0.11 and 7.78 ± 0.04 for rodA and HF183 genetic markers, for E. coli and human host associated Bacteroides, respectively, indicating about 5 % sewage content. SourceTracker analysis of sequencing data attributed 72–77 % of bacteria in the downstream section of the river during a storm event to CSO discharge sources, versus only 4–6 % to rural upstream sources. Data from sixteen summer sampling events in a public park exceeded various guideline values for recreational water quality. Quantitative microbial risk assessment (QMRA) predicted a median and 95th percentile risk of 0.03 and 0.39, respectively, of contracting a bacterial gastrointestinal disease when wading and splashing around in the Ouseburn. We show clearly why microbial water quality should be monitored where rivers flow through public parks, irrespective of their bathing water designation.",unknown,594
10.1016/j.ecoinf.2023.102067,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85150440164,scopus,2023-09-01,scopus,"spatio-temporal air quality analysis and pm<inf>2.5</inf> prediction over hyderabad city, india using artificial intelligence techniques",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150440164&origin=inward,"
                  Air pollution is one of the most serious environmental issues faced by humans, and it affects the quality of life in cities. PM2.5 forecasting models can be used to create strategies for assessing and warning the public about anticipated harmful levels of air pollution. Accurate pollutant concentration measurements and forecasting are critical criteria for assessing air quality and are the foundation for making the right strategic decisions. Data-driven machine learning models for PM2.5 forecasting have gained attention in the recent past. In this study, PM2.5 prediction for Hyderabad city was carried out using various machine learning models viz. Multi-Linear Regression (MLR), decision tree (DT), K-Nearest Neighbors (KNN), Random Forest (RF), and XGBoost. A deep learning model, the Long Short-Term Memory (LSTM) model, was also used in this study. The results obtained were finally compared based on error and R2 value. The best model was selected based on its maximum R2 value and minimal error. The model's performance was further improved using the randomized search CV hyperparameter optimization technique. Spatio-temporal air quality analysis was initially conducted, and it was found that the average winter PM2.5 concentrations were 68% higher than the concentrations in summer. The analysis revealed that XGBoost regression was the best-performing machine learning model with an R2 value of 0.82 and a Mean Absolute Error (MAE) of 7.01 μg/ m3, whereas the LSTM deep learning model performed better than XGBoost regression for PM2.5 modeling with an R2 value of 0.89 and an MAE of 5.78 μg/ m3.
               ",unknown,595
10.1016/j.jclepro.2023.137432,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85160435555,scopus,2023-08-10,scopus,examining the importance of built and natural environment factors in predicting self-rated health in older adults: an extreme gradient boosting (xgboost) approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160435555&origin=inward,"
                  Previous studies indicate that natural and built environment factors significantly influence health outcomes in older adults. However, most cross-sectional studies exploring the impact of these factors on health fail to quantify the relative importance of each factor. Here, we use the XGBoost machine learning technique with SHAPley Additive exPlanations (SHAP) to rank the importance of built environment factors, natural environmental factors, and sociodemographic factors in shaping older adults’ odds of good self-rated health (SRH), in Shanghai, Guangzhou, and Shenzhen, China. To address self-selection bias in housing choice, older adults living in private housing, who have more freedom to choose residential locations, were distinguished from those living in public or self-built housing. To better interpret the results of XGBoost outcomes and analyse the association between factors and SRH, we used SHAP dependence plots. Results show that both built environment and natural environment factors play important roles in predicting SRH. Four built environment factors (accessibility to public transit stations, points of interest density, road density, and population density) and two natural environment factors (air quality index and surrounding greenness) have considerable predictive power for SRH for both groups. Among these factors, accessibility to public transit stations, road density, and air quality index become less important after controlling for self-selection bias. We also trace potential threshold effects of residential greenness, points of interest density, and road density on decreasing older adults' SRH within certain intervals after controlling for self-selection bias. Findings from this study can support the decision-making of policymakers regarding urban planning, landscape design, and environmental management to improve overall health of older adults.
               ",unknown,600
10.1016/j.yrtph.2023.105425,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85161289162,scopus,2023-08-01,scopus,genotoxicity evaluation of cannabidiol,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161289162&origin=inward,"Consumer use of cannabidiol (CBD) for personal wellness purposes has garnered much public interest. However, safety-related data on CBD in the public domain are limited, including a lack of quality studies evaluating its genotoxic potential. The quality of available studies is limited due to the test material used (e.g., low CBD purity) and/or study design, leading some global regulatory agencies to highlight genotoxicity as an important data gap for CBD. To address this gap, the genotoxic potential of a pure CBD isolate was investigated in a battery of three genotoxicity assays conducted according to OECD testing guidelines. In an in vitro microbial reverse mutation assay, CBD up to 5000 μg/plate was negative in Salmonella typhimurium strains TA98, TA100, TA1535, and TA1537, and Escherichia coli strain WP2 uvrA, with and without metabolic activation. Testing in an in vitro micronucleus assay was negative in human TK6 cells up to 10–11 μg/mL, with and without metabolic activation. Finally, an in vivo micronucleus assay conducted in male and female rats was negative for genotoxicity up to 1000 mg/kg-bw/d. Bioanalysis of CBD and its primary metabolite, 7-carboxy CBD, confirmed a dose-related increase in plasma exposure. Together, these assays indicate that CBD is unlikely to pose a genotoxic hazard.",unknown,606
10.1016/j.patcog.2023.109568,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85151431754,scopus,2023-08-01,scopus,distributional and spatial-temporal robust representation learning for transportation activity recognition,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151431754&origin=inward,"
                  Transportation activity recognition (TAR) provides valuable support for intelligent transportation applications, such as urban transportation planning, driving behavior analysis, and traffic prediction. There are many advantages of movable sensor-based TAR, and the key challenge is to capture salient features from segmented data for representing diverse patterns of activity. Although existing methods based on statistical information are efficient, they usually rely on domain knowledge to construct high-quality features manually. Likewise, the methods based on spatial-temporal relationships achieve good performance but fail to extract statistical features. The features extracted by these two methods have proven to be crucial for the classification of activity. How to combine them to acquire a more robust representation remains an open question. In this work, we introduce a novel parallel model named Distributional and Spatial-Temporal Robust Representation (DSTRR), which combines automatic learning of statistical, spatial, and temporal features into a unified framework. This model leads to three optimized subnets and thus obtains a robust representation specific to TAR. Extensive experiments performed on three public datasets show that DSTRR is a state-of-the-art method compared with the baseline methods. The results of ablation study and visualization not only demonstrate the effectiveness of each component in DSTRR, but also show the model remains robust to a wide range of parameter variations.
               ",unknown,607
10.1016/j.tra.2023.103705,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85162195686,scopus,2023-07-01,scopus,why do people take e-scooter trips? insights on temporal and spatial usage patterns of detailed trip data,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162195686&origin=inward,"
                  Electric scooters (e-scooters) are becoming one of the most popular micromobility options in the United States. Although there is some evidence of increased mobility, reduced carbon emissions, replaced car trips, and associated public health benefits, there is little known about the patterns of e-scooter use. This study proposes a framework for high-resolution analysis of micromobility data based on temporal, spatial, and weather attributes. As a case study, we scrutinized more than one million e-scooter trips of Nashville, Tennessee, from September 1, 2018, to August 31, 2019. Weather data and land use data from the Nashville Travel Demand Model and scraping of Google Maps Point of Interest (POI) data complemented the trip data. The combination of Principal Component Analysis (PCA) and a K-means unsupervised machine learning algorithm identified five distinct e-scooter usage patterns, namely morning work/school, daytime short errand, social, nighttime entertainment district, and utilitarian trips.
                  Among other findings, the most common use of e-scooters in Nashville was daytime short errand trips, contributing to 29% of all e-scooter trips. We found that 7% of all e-scooter trips resembled morning commuting to work or school. Only 16% of trips were classified as Nighttime Entertainment District trips. The average daily number of trips on a typical weekend was 81% higher than a typical weekday. We also found variation in e-scooter usage patterns over a year with high summer ridership patterns. The findings of this study can help city administrations, planners, and micromobility operators to understand when and where people are using e-scooters. Such knowledge can guide them in making data-driven decisions regarding safety, sustainability, and mode substitution of emerging micromobility.
               ",unknown,617
10.1016/j.buildenv.2023.110446,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85159862015,scopus,2023-07-01,scopus,weather image-based short-term dense wind speed forecast with a convlstm-lstm deep learning model,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159862015&origin=inward,"
                  Short-term wind speed predication is of great significance for scholars (e.g., understanding wind profiles), practitioners (e.g., building energy management), regulators (e.g., urban microclimate regulation), and even the general public. Current wind speed forecasting methods either generate sparse predictions or occur high cost. This paper reports a novel, inexpensive framework to forecast urban local dense wind speed. The central tenet is a convolutional long short-term memory (ConvLSTM) and LSTM combinatorial deep learning model to learn the features of input historical weather image series coupled with spatial-temporal correlations. The model was trained and tested using Hong Kong datasets. The feasibility and effectiveness of the proposed model are verified and compared with parallel models under different criteria, including mean absolute error (MAE), root mean square error (RMSE) and R-squared (R2). The experimental results show that: (1) the proposed ConvLSTM-LSTM deep learning model can effectively forecast wind speed regardless of location; (2) the overall MAE, RMSE, and R2 value of the proposed model are improved by 14.84%, 15.04%, and 7.51%, respectively, compared to the ConvLSTM-full connected (ConvLSTM-FC) model, and by 22.12%, 22.80%, and 12.24%, respectively, compared to the convolutional neural network-LSTM (CNN-LSTM) model; and (3) compared with parallel models, the proposed model has better performance in predicting wind speed series with large amplitude variations and rapid frequency changes.
               ",unknown,618
10.1016/j.engfailanal.2023.107237,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85151658094,scopus,2023-07-01,scopus,physical interpretation of machine learning-based recognition of defects for the risk management of existing bridge heritage,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151658094&origin=inward,"The challenge of the research work presented in the paper is to combine the growing interest in monitoring the health condition of existing bridge heritage through systematic and periodic visual inspections with automated recognition of typical bridge defects, which can greatly facilitate the assessment of defect evolution over time. The study focused on the automated identification of defects in existing Reinforced Concrete (RC) bridges exploiting different Deep Learning (DL) approaches and techniques to interpret the obtained predictions. Ensuring the safety of infrastructures is typically a technical and economic issue. Still, in the case of the engineering infrastructure heritage, there are existing bridges and viaducts with a high historical, cultural, and symbolic value. For them, accurate knowledge and characterization of possible degradation processes become particularly important in order to define intervention strategies that combine safety and conservation requirements. With the aim to develop systematic and non-invasive investigation protocols for continuous and effective control of defects and their evolution, a database of existing RC bridge defect images was collected, and the most recurrent defect typologies were classified by domain experts. Some existing Convolutional Neural Networks (CNNs) algorithms were applied to the dataset for automatically recognizing all defects, but the specific novel contribution of the research work is the interpretation of the obtained results in a form that is humanly explainable and directly implementable in new tools for bridge inspections. To interpret the results, Class Activation Maps (CAMs) approaches were employed within available eXplainable Artificial Intelligence (XAI) techniques, which allow to observe the activation zones and nearly perfectly highlight the type of specific defect in a given image. The obtained results, besides suggesting which network works better than others and if the specific defect is effectively recognized, have been evaluated through a quasi-quantitative procedure that compared a qualitative assessment of the CNNs models reliability with two novel indexes representing new explaining metrics of the obtained results. In the end, the outcomes of the proposed study were observed also in a real-life case study. The proposed discussion opens new scenarios in the application of these techniques for supporting road management companies and public organizations in the evaluation of the road networks health state.",unknown,619
10.1016/j.infsof.2023.107197,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85150888173,scopus,2023-07-01,scopus,transparency and explainability of ai systems: from ethical guidelines to requirements,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150888173&origin=inward,"Context and Motivation Recent studies have highlighted transparency and explainability as important quality requirements of AI systems. However, there are still relatively few case studies that describe the current state of defining these quality requirements in practice. Objective This study consisted of two phases. The first goal of our study was to explore what ethical guidelines organizations have defined for the development of transparent and explainable AI systems and then we investigated how explainability requirements can be defined in practice. Methods In the first phase, we analyzed the ethical guidelines in 16 organizations representing different industries and public sector. Then, we conducted an empirical study to evaluate the results of the first phase with practitioners. Results The analysis of the ethical guidelines revealed that the importance of transparency is highlighted by almost all of the organizations and explainability is considered as an integral part of transparency. To support the definition of explainability requirements, we propose a model of explainability components for identifying explainability needs and a template for representing explainability requirements. The paper also describes the lessons we learned from applying the model and the template in practice. Contribution For researchers, this paper provides insights into what organizations consider important in the transparency and, in particular, explainability of AI systems. For practitioners, this study suggests a systematic and structured way to define explainability requirements of AI systems. Furthermore, the results emphasize a set of good practices that help to define the explainability of AI systems.",unknown,620
10.1016/j.giq.2023.101828,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85151401656,scopus,2023-06-01,scopus,whether ai adoption challenges matter for public managers? the case of polish cities,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151401656&origin=inward,"
                  A growing body of literature shows that despite the significant benefits of artificial intelligence (AI), its adoption has many unknowns and challenges. However, theoretical studies dominate this topic. Completing the recent works, this article aims to identify challenges faced by public organizations when adopting AI based on the PRISMA Framework and an empirical assessment of these challenges in the opinion of public managers using survey research. The adopted research procedure is also an added value because it could be replicated in other context scenarios. To achieve this paper's aim, the Systematic Literature Review (SLR) and survey research among authorities in 414 Polish cities were carried out. As a result, a list of 15 challenges and preventive activities proposed by researchers to prevent these challenges have been identified. Empirical verification of identified challenges allows us to determine which of them limit AI adoption to the greatest extent in public managers' opinion. These include a lack of strategy or plans to initial adoption / continued usage of AI; no ensuring that AI is used in line with human values; employees' insufficient knowledge of how to use AI; insufficient AI policies, laws, and regulations; and different expectations of stakeholders and partners about AI. These findings could help practitioners to prioritize AI adoption activities and add value to digital government theory.
               ",unknown,628
10.1016/j.scs.2023.104472,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85150254920,scopus,2023-06-01,scopus,people's olfactory perception potential mapping using a machine learning algorithm: a spatio-temporal approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150254920&origin=inward,"
                  This research aimed to conduct Spatio-temporal modeling of people's olfactory perception in the Tehran city using a machine learning-based approach considering the importance of people's perception of the urban environment and the effect of the sense of smell in strengthening or weakening the environmental and urban spaces. Therefore, a spatial database containing dependent and independent data was established to model and prepare a potential map of people's olfactory perception in Tehran. The dependent data were collected through a questionnaire among 687 people, which contains three states of sense of smell, including neutral (350 cases), pleasant (247 cases), and unpleasant (90 cases). The independent data consisted of distance to public transport stations, livestock and poultry production centers, streets, fuel stations, fruit and vegetable fields, restaurants, industrial areas, garbage volume, traffic volume, population density, normalized difference vegetation index (NDVI), meteorological data (wind speed and direction, and humidity), and air quality index (AQI). The last three criteria were provided temporarily in four seasons. A random forest (RF) algorithm was utilized to model and prepare the potential map of people's olfactory perception in four seasons. Generally, 70% of the information was dedicated to modeling, and the remained 30% was applied for validation. Results of the Gini index revealed the more significant impact of the criteria of distance from fruit and vegetable fields and distance from restaurants in neutral smells, distance from streets and distance from restaurants in pleasant smells, and distance from livestock and poultry production centers and garbage volume in unpleasant smells among spatial criteria. One of the essential spatial-temporal criteria was wind direction in all seasons for neutral smells, in summer, fall, and winter for pleasant smells, and in fall for unpleasant smells. The RF algorithm modeling results indicated the greater effect of distance to public transport stations in spring, garbage volume in summer and winter, and population density in autumn on the sense of smell. The potential maps of olfactory perception were evaluated using the receiver operating characteristic (ROC) curve and area under the curve (AUC). AUC values were calculated for the spring, summer, autumn, and winter seasons as much as 0.956, 0.955, 0.957, and 0.958 for neutral, 0.944, 0.928, 0.946, and 0.942 for pleasant, and 0.922, 0.931, 0.963, and 0.981 for unpleasant states, respectively. The potential maps of this study help present a more pleasant olfactory space to urban users by urban planners and managers.
               ",unknown,630
10.1016/j.scs.2023.104480,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85149817289,scopus,2023-06-01,scopus,towards human-centric digital twins: leveraging computer vision and graph models to predict outdoor comfort,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149817289&origin=inward,"
                  Conventional sidewalk studies focused on quantitative analysis of sidewalk walkability at a large scale which cannot capture the dynamic interactions between the environment and individual factors. Embracing the idea of Tech for Social Good, Urban Digital Twins seek AI-empowered approaches to bridge humans with digitally-mediated technologies to enhance their prediction ability. We employ GraphSAGE-LSTM, a geo-spatial artificial intelligence (GeoAI) framework on crowdsourced data and computer vision to predict human comfort on the sidewalks. Conceptualising the pedestrians and their interactions with surrounding built and unbuilt environments as human-centric dynamic graphs, our model captures such spatio-temporal variations given by the sequential movements of human walking, enabling the GraphSAGE-LSTM to be spatio-temporal-explicit. Our experiments suggest that the proposed model provides higher accuracy by more than 20% than a traditional machine learning model and two state-of-art deep learning frameworks, thus, enhancing the prediction power of Urban Digital Twin. The source code for the model is shared openly on GitHub.
               ",unknown,632
10.1016/j.trip.2023.100832,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85159054847,scopus,2023-05-01,scopus,public transport demand estimation by frequency adjustments,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159054847&origin=inward,"This article addresses the problem of estimating the demand for public transport from two approaches. First, we propose a bilevel optimization problem that allows estimating the demand using historical data and the observed bus frequencies. This model has been applied to small theoretical networks and the transit network of Tandil (a medium-sized city in Buenos Aires, Argentina), showing good results. However, from a practical point of view, the computation time of the algorithm used to solve the bilevel problem is long, reducing its applicability by traffic authorities. To solve this, we propose to use an artificial neural network module that allows to quickly detect if the change in demand is significant enough (for example, beyond a predefined threshold). If it is substantial, the operator can decide to run the algorithm to estimate the demand and take action to adapt the system to the new reality, for example, adapting vehicle frequencies or incorporating more vehicles into the system so that the current demand can be served. The machine learning approach allows it to be used as a fast change detection tool, avoiding running the expensive algorithm for false positives.",unknown,642
10.1016/j.apgeog.2023.102950,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85151428422,scopus,2023-05-01,scopus,quantifying and mapping landscape value using online texts: a deep learning approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151428422&origin=inward,"
                  Current studies have quantified landscape value based on the quantity of social media, ignoring public preference, emotional tendency and intensity. Our objective is to quantify and map landscape value by developing a novel intelligent landscape value evaluation method based on online texts. We argue that online texts can be converted into quantities through deep learning technology, which allows the measurement of public preference, emotional tendency and intensity. We selected perception frequency, perception heat, perception preference and the value index as indicators to quantify landscape value, which is the product of perception heat and perception preference. We collected more than 14,000 online texts about the Orange Islands in Changsha city from 2010 to 2020, analysed the perception heat using Python to extract data, and analysed the perception preference using Baidu API sentiment analysis technology. The results show that the novel method has a wide range of data sources and represents the real public perception and value orientation, demonstrating that online texts with a wealth of mined information can be used to evaluate landscape value. The model calculating the landscape value overcomes the shortcomings of current methods that ignore public preference, emotional tendency and intensity. The results also show that the conservation mode of Orange Island is successful, and the conservation method suggested by the recommendation on the historic urban landscape is feasible. The study highlights the contribution of public preferences to landscape value and provides insight into the situations and variations of value types on Orange Island. The novel method is highly intelligent, time-saving, has low labour costs, and is applicable and can be applied to the evaluation of landscape value, especially in cases where value conflicts exist.
               ",unknown,643
10.1016/j.resourpol.2023.103522,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85151312089,scopus,2023-05-01,scopus,economic resilience assessment and policy interaction of coal resource oriented cities for the low carbon economy based on ai,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151312089&origin=inward,"
                  This study aims to evaluate the economic growth of coal-resource-oriented cities under Low-Carbon (LC) economic growth. The public participation mechanisms are promoted, and the vitality of the economic market of resource-oriented cities is enhanced by increasing the intervention of government policies. Firstly, the context of the coal-resource-oriented cities and LC economy are analysed. Secondly, an economic resilience assessment system for coal-resource-oriented cities is established, characterized by traditional and high-tech industries through the analysis of the condition of resource-oriented cities. Finally, in the process of optimizing and managing the economic structure, the assessment of the LC evolution path of resource-oriented cities is strengthened through the extensive participation of the general public. The results show that the economic resilience level of coal-resource-oriented and traditional cities fluctuates and increases from 2011 to 2021. In 2011, the economic resilience index of traditional cities and coal-resource-oriented cities was 0.11and 0.22. By 2021, traditional cities’ and coal-resource-oriented cities’ economic resilience assessment stability was 0.527 and 0.562. This study has pivotal reference value for promoting urban resource management and economic efficiency.
               ",unknown,644
10.1016/j.uclim.2023.101513,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85151292273,scopus,2023-05-01,scopus,finding common ground: a methodology for city-scale subsurface thermal modelling,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151292273&origin=inward,"
                  The presence of anthropogenic infrastructure within the ground under cities can act to increase the temperature of the subsurface in a phenomenon known as the subsurface urban heat island effect. Large city-scale numerical subsurface temperature models accounting for both the heat fluxes stemming from human-made structures as well as hydrogeological variation are often computationally prohibitive due to the vastly different scales of modelling required. We present a novel and scalable methodology for the determination of the subsurface thermal climate beneath urban centres at a holistic scale, grouping volumes of ground into archetypes according to common thermal behaviours, identified using supervised machine learning algorithms. We apply this methodology to two boroughs in the centre of London, UK, with the results showing good agreement with a higher-resolution model for one of the boroughs. We further demonstrate the scalability of the approach by modelling the boroughs sequentially, where the second borough was modelled at little to no additional computation cost at a loss of accuracy in the local depth-averaged temperature between 0.10 and 0.23 degrees Celsius. This methodology is an important step towards determining large-scale subsurface climate beneath cities, providing insights for various applications, such as an improved understanding of geothermal energy potential.
               ",unknown,645
10.1016/j.compeleceng.2023.108672,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85150419402,scopus,2023-05-01,scopus,leveraging mayfly optimization with deep learning for secure remote sensing scene image classification,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150419402&origin=inward,"
                  Current advancement in satellite sensor and remote sensing image (RSI) technologies allows high-resolution RSI, which provide spatial information about the world. In these circumstances, RSI scene classification is attracted considerable interest owing to its wide-ranging applications like environmental monitoring, urban planning, national defence security, and natural hazard detection. New deep learning (DL) achievements have resulted in the newest technology of RSISC applications; however, they lack trustworthiness and explainability. But RSIs of sensitive area is vulnerable to theft, interception, and loss due to the transmission through public channel or the storage in the cloud environment. Therefore, this study develops a mayfly optimization with deep learning (DL) based robust remote sensing scene image classification (MFODL-RRSSIC) model. The presented MFODL-RRSSIC model majorly aims to accomplish two objectives namely scene classification and security. For scene classification, the presented MFODL-RRSSIC technique employs NasNet feature extraction, MFO based hyperparameter tuning, and stacked autoencoder (SAE) classifier. Besides, the presented MFODL-RRSSIC model exploits modified deep belief network (MDBN) for intrusion detection and attaining security. A wide-ranging simulation analysis is made on popular datasets to illustrate the enhanced performance of the MFODL-RRSSIC technique.
               ",unknown,647
10.1016/j.tre.2023.103080,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85149749326,scopus,2023-04-01,scopus,optimization of ride-sharing with passenger transfer via deep reinforcement learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149749326&origin=inward,"
                  With the emergence of the sharing economy and the rapid growth of mobile communications technologies, many novel sharing service models have been developed stemming from ride-hailing. Urban traffic congestion, coupled with energy conservation and emissions reduction, has prompted research on enhancing vehicle seat utilization in taxi service. To offer more effective and reliable ride-hailing, we consider ride-sharing problem with passenger transfer that allows passegers to transfer between vehicles at transfer stations. The problem requires simultaneous addressing the issues of request dispatching, transfer scheduling, and vehicle rebalancing. Studying such a ride-hailing model, we propose a novel joint decision framework combining deep reinforcement learning (DRL) with integer-linear programming (ILP) to solve the problem. We use ILP to obtain the optimal online dispatching and matching strategy in each decision stage, and DRL to learn the approximate state value of each vehicle that incorporates with some strategies to limit the state space and reduce the computational complexity. Performing numerical studies on the real-world trip dataset in Chengdu, we demonstrate that the proposed method outperforms several state-of-the-art methods, and that ride-sharing with passenger transfer is more beneficial than traditional ride-sharing.
               ",unknown,657
10.1016/j.erss.2023.103016,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85149481908,scopus,2023-04-01,scopus,hybridizing qualitative coding with natural language processing and deep learning to assess public comments: a case study of the clean power plan,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149481908&origin=inward,"
                  Public comments are a rich source of data on attitudes toward public policy, but the scale poses major challenges for qualitative analyses. Supervised deep learning and natural language processing potentially enable classification of these data but require high quality labeled inputs. This study investigates whether hybridizing rigorous qualitative coding methods with machine learning approaches can classify large amounts of policy-oriented text data (public comments) while keeping manual effort tractable. Using a convolutional neural network, we evaluate spatiotemporal variation in themes and expressed public attitudes toward a specific US climate policy, the Clean Power Plan (CPP), which was proposed in 2014 and repealed in 2019 without ever taking effect. Public comments were solicited for both the proposal and proposed repeal across eight cities, representing a large and highly targeted dataset on dynamic public attitudes toward the CPP. Rigorous manual coding and data augmentation techniques enabled good model performance (F1 scores of 0.71 and 0.81, respectively, for sentiment and topic classification), even with a very small training set. We find that most speakers supported the CPP despite its eventual repeal and uncover notable rhetorical trends like competing narratives of justice for those affected by climate change versus fossil fuel host communities.
               ",unknown,658
10.1016/j.scs.2023.104419,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85147194401,scopus,2023-04-01,scopus,nonlinear relationships and interaction effects of an urban environment on crime incidence: application of urban big data and an interpretable machine learning method,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147194401&origin=inward,"
                  While environmental criminology suggests that crime and the urban environment are closely related, some studies suggest a nonlinear relationship. This study analyzed the relationship between crime incidence and the urban environment using urban big data such as points-of-interest (POI), smart civil complaint data, and street image data from Naver Street View in Seoul, Korea. For analysis, the Light Gradient Boosting Machine (LightGBM) model and SHapley Additive exPlanation (SHAP) method have been used. The analysis results confirmed a nonlinear relationship comprising inflection points between crime incidence and the urban environment. Also, this study identified the interaction effects of urban environmental variables on crime incidence. Finally, the hierarchical clustering method was used to identify the contributions of various aspects of the urban environments to crime incidence. Then, this study provides policy implications to prevent potential criminal activities and promote public safety for sustainable cities and societies.
               ",unknown,661
10.1016/j.trc.2023.104015,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85146281201,scopus,2023-02-01,scopus,multi-task supply-demand prediction and reliability analysis for docked bike-sharing systems via transformer-encoder-based neural processes,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146281201&origin=inward,"
                  With the rise of sharing economy, bike-sharing systems (BSSs) have gained heated attention, and their operations require accurate prediction of bike usage. Although many deep learning methods have been exploited to predict bike usage, they generally provide point predictions of average bike usage, neglecting the stochasticity in BSSs. Due to the analytically explainable properties and linear computational costs with respect to data size, neural processes (NPs) have recently attracted increasing interest. An NP model learns a Gaussian process (GP) by mapping the input–output observations to a probabilistic distribution over functions. Each function is a distribution of the outputs given an input, conditioned on the arbitrary size of observed data. NPs provide probabilistic confidence in predicted results, which overcomes the point prediction issue faced by other models and provides insights for operational strategies in stochastic scenarios. This paper originally proposes a transformer-encoder-based NP (TENP) model to fit the distribution of bike usage in BSSs. To the best of our knowledge, this work is among the first to incorporate transformer encoders into NPs, enhancing the capability of extracting relevant information in a targeted manner. Based on the Citi Bike datasets in New York City, the TENP method is adopted in a multi-task learning task that simultaneously fits the number of pickups and returns. The proposed TENP model outperforms the conventional NP method and its extensions and prevalent machine learning models in terms of prediction accuracy. Armed with the probabilistic confidence provided by the TENP, reliability analysis is conducted, and thoughtful guidance is provided for bike-sharing operations, such as dynamic bike rebalancing.
               ",unknown,687
10.1016/j.trd.2022.103601,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85145976281,scopus,2023-02-01,scopus,predicting pm10 and pm2.5 concentration in container ports: a deep learning approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145976281&origin=inward,"
                  This study aims at predicting the concentrations of particulate matter in container ports. Meteorological data, terminal operation data, and data on PM2.5 and PM10 and other air pollutants at container ports were collected from Gwangyang Port in South Korea. A prediction model was developed using neural network methods such as recurrent neural networks (RNN), long short-term memory (LSTM), and multivariate linear regression (MLR). This study revealed that performance of LSTM was the highest. In addition, the performance of models with operating data is higher than the models without operating data as they have lower error values and stable decreasing patterns in a loss curve for training and validation loss. The proposed model could be used to provide PM information in advance to port workers and the public living in port cities so they can respond with personal hygiene and workplace health protection measures according to the predicted amount of PM.
               ",unknown,688
10.1016/j.comtox.2022.100256,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85144069237,scopus,2023-02-01,scopus,development of a csrml version of the analog identification methodology (aim) fragments and their evaluation within the generalised read-across (genra) approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144069237&origin=inward,"
                  The Analog Identification Methodology (AIM) was developed over 20 years ago to identify analogues to support read-across at the US Environmental Protection Agency. However, the current public version of the standalone tool, released in 2012, is no longer usable on Windows operating systems supported by Microsoft. Additionally, the structural logic for analogue selection is based on older, customised Simplified molecular-input-line-entry system (SMILES)-type features that are incompatible with modern cheminformatics tools. Given these limitations, a case study was undertaken to explore a more transparent, extensible method of implementing the AIM fragments using Chemical Subgraphs and Reactions Mark-up Language (CSRML). A CSRML file was developed to codify the original AIM fragments, and the extent to which AIM fragments were faithfully replicated was assessed using the AIM Database. The overall mean performance of the CSRML-AIM across all fragments in terms of sensitivity, specificity, and Jaccard similarity was 89.5%, 99.9%, and 82.2%, respectively. Comparing the AIM fragments with public ToxPrints using a large set of ∼25,000 substances of regulatory interest to EPA found them to be dissimilar, with an average maximum Jaccard score of 0.24 for AIM and 0.29 for ToxPrint fingerprints. Both fragment sets were then used as inputs in the automated read-across approach, Generalised Read-Across (GenRA), to evaluate the quality of fit in predicting rat acute oral toxicity LD50 values with the coefficient of determination (R2) and root mean squared error (RMSE). The performance of AIM fragments was R2=0.434 and RMSE=0.663 whereas that of ToxPrints was R2=0.477 and RMSE=0.638. A bootstrap resampling using 100 iterations found the mean and the 95th confidence interval of R2 to be 0.349 [0.319, 0.379] for AIM fragments and 0.377 [0.338, 0.412] for ToxPrints. Although AIM and ToxPrints performed similarly in predicting LD50, they differed in their performance at a local level, revealing that their features can offer complementary insights.
               ",unknown,689
10.1016/j.scitotenv.2022.159350,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85142686053,scopus,2023-02-01,scopus,a 30-day follow-up study on the prevalence of sars-cov-2 genetic markers in wastewater from the residence of covid-19 patient and comparison with clinical positivity,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142686053&origin=inward,"Wastewater based epidemiology (WBE) is an important tool to fight against COVID-19 as it provides insights into the health status of the targeted population from a small single house to a large municipality in a cost-effective, rapid, and non-invasive way. The implementation of wastewater based surveillance (WBS) could reduce the burden on the public health system, management of pandemics, help to make informed decisions, and protect public health. In this study, a house with COVID-19 patients was targeted for monitoring the prevalence of SARS-CoV-2 genetic markers in wastewater samples (WS) with clinical specimens (CS) for a period of 30 days. RT-qPCR technique was employed to target nonstructural (ORF1ab) and structural-nucleocapsid (N) protein genes of SARS-CoV-2, according to a validated experimental protocol. Physiological, environmental, and biological parameters were also measured following the American Public Health Association (APHA) standard protocols. SARS-CoV-2 viral shedding in wastewater peaked when the highest number of COVID-19 cases were clinically diagnosed. Throughout the study period, 7450 to 23,000 gene copies/1000 mL were detected, where we identified 47 % (57/120) positive samples from WS and 35 % (128/360) from CS. When the COVID-19 patient number was the lowest (2), the highest CT value (39.4; i.e., lowest copy number) was identified from WS. On the other hand, when the COVID-19 patients were the highest (6), the lowest CT value (25.2 i.e., highest copy numbers) was obtained from WS. An advance signal of increased SARS-CoV-2 viral load from the COVID-19 patient was found in WS earlier than in the CS. Using customized primer sets in a traditional PCR approach, we confirmed that all SARS-CoV-2 variants identified in both CS and WS were Delta variants (B.1.617.2). To our knowledge, this is the first follow-up study to determine a temporal relationship between COVID-19 patients and their discharge of SARS-CoV-2 RNA genetic markers in wastewater from a single house including all family members for clinical sampling from a developing country (Bangladesh), where a proper sewage system is lacking. The salient findings of the study indicate that monitoring the genetic markers of the SARS-CoV-2 virus in wastewater could identify COVID-19 cases, which reduces the burden on the public health system during COVID-19 pandemics.",unknown,691
10.1016/j.eng.2021.11.021,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85132674244,scopus,2023-02-01,scopus,estimating rainfall intensity using an image-based deep learning model,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132674244&origin=inward,"Urban flooding is a major issue worldwide, causing huge economic losses and serious threats to public safety. One promising way to mitigate its impacts is to develop a real-time flood risk management system; however, building such a system is often challenging due to the lack of high spatiotemporal rainfall data. While some approaches (i.e., ground rainfall stations or radar and satellite techniques) are available to measure and/or predict rainfall intensity, it is difficult to obtain accurate rainfall data with a desirable spatiotemporal resolution using these methods. This paper proposes an image-based deep learning model to estimate urban rainfall intensity with high spatial and temporal resolution. More specifically, a convolutional neural network (CNN) model called the image-based rainfall CNN (irCNN) model is developed using rainfall images collected from existing dense sensors (i.e., smart phones or transportation cameras) and their corresponding measured rainfall intensity values. The trained irCNN model is subsequently employed to efficiently estimate rainfall intensity based on the sensors’ rainfall images. Synthetic rainfall data and real rainfall images are respectively utilized to explore the irCNN’s accuracy in theoretically and practically simulating rainfall intensity. The results show that the irCNN model provides rainfall estimates with a mean absolute percentage error ranging between 13.5% and 21.9%, which exceeds the performance of other state-of-the-art modeling techniques in the literature. More importantly, the main feature of the proposed irCNN is its low cost in efficiently acquiring high spatiotemporal urban rainfall data. The irCNN model provides a promising alternative for estimating urban rainfall intensity, which can greatly facilitate the development of urban flood risk management in a real-time manner.",unknown,694
10.1016/j.jclepro.2022.135460,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85144335929,scopus,2023-01-15,scopus,supporting sustainability projects at neighbourhood scale: green visions for the san salvario district in turin guided by a combined assessment framework,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144335929&origin=inward,"
                  A resilient city includes multiple energy carriers, high-efficiency infrastructure, lower resource demand to decarbonize and sustain the urban system in accordance with the Paris Agreement, the United Nations 2030 Agenda for Sustainable Development, and the various recovery plans following the COVID-19 pandemic period. To achieve these goals, a key role is played by all urban sectors, which can reduce environmental impacts and accelerate the green transition at larger scale. Intervening on a district scale obviously requires the evaluation of different aspects, taking into account both economic and non-economic criteria, as well as different points of view, involving all stakeholders. This paper proposes a multi-step evaluation procedure that extends the European manual-based Cost-Benefit Analysis (CBA) to include the extra-economic benefits and the stakeholders’ opinion in the evaluation, according to the COmpoSIte Model for Assessment (COSIMA) method. This is the first application of COSIMA in the urban design sectors (i.e. buildings, water, public lighting, transportation and waste management) where different sustainable measures for a real case study located in Turin (Italy) were compared to define the most suitable transformation scenario according to multiple criteria. The results have shown how invasive scenarios allow achieving the greatest benefits, despite the huge initial costs of realization.
               ",unknown,699
10.1016/j.procs.2023.07.022,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85171734557,scopus,2023-01-01,scopus,fake financial news detection with deep learning: evidence from china,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171734557&origin=inward,"Although fake news detection has become an emerging research attracting much attention, research in financial sector is very limited. Particularly in China, no public dataset of fake financial news is available. This paper focuses on Chinese financial market and constructs a unique dataset from clarification announcements targeting for financial news with Internet sources. Besides content features and contextual features, financial features are typically added to the feature set due to unique characteristics of financial news. Based on our sample, a deep learning approach is proposed to detect fake financial news, which demonstrates superior performance to several other baseline models, with accuracy of 94.38% and f1-score of 87.67%.The ablation experiment indicates that content features contained in the article itself contribute strongly to detect fake financial news. Finally, Shapley value is used to explain the characteristics of fake financial news compared with real ones in Chinese financial market.",unknown,772
10.1016/j.imu.2023.101321,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85168725666,scopus,2023-01-01,scopus,care-needs level prediction for elderly long-term care using insurance claims data,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168725666&origin=inward,"Background and objective Owing to an aging population, the increase in the number of elderly people certified as requiring long-term care has become a critical social issue in Japan. This study aimed to construct a machine learning model predicting the maximum care-needs level required for long-term care within the next three years for persons aged over 75 years. Methods The prediction model was constructed using features extracted from long-term care and healthcare insurance claims data. The study subjects were a total of 47,862 elderly individuals who had not received long-term care services in a large city in Japan. The prediction classes for outcome variable were categorized according to the criteria of the Japanese long-term care system: class 0 (no required), class 1 (support levels 1 and 2), class 2 (care levels 1 and 2), and class 3 (care levels 3–5). As explanatory variables, a total of 516 features were used, including age, sex, and 514 diseases classified under ICD-10. In this study, we focused on constructing a prediction model with the interpretability and adopted multinomial logistic regression (MLR) with L2 regularization as a machine learning algorithm. MLR allowed us to identify the characteristics influencing each prediction class of care-needs levels. Results In terms of overall predictive performance, MLR achieved weighted average precision, recall, F-value, and lift scores of 0.694, 0.505, 0.567, and 1.333, respectively. Compared to other machine learning algorithms, MLR demonstrated comparable performance to Support Vector Machine (SVM) and Random Forest (RF). From the factor analysis based on the magnitudes of coefficients of the MLR model, the top three features influencing each prediction class were as follows: class1: female sex, hypertension, and gonarthrosis; class 2: age, Alzheimer-type dementia, and neuromuscular dysfunction of the bladder; class 3: age, Alzheimer-type dementia, and type 2 diabetes mellitus. Conclusions In practical terms, the care-needs level prediction can be applied by local governments to identify high-risk areas by comprehensively and routinely predicting insured persons under public health insurance and long-term care insurance systems.",unknown,780
10.1016/j.compenvurbsys.2022.101918,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85142744621,scopus,2023-01-01,scopus,the role of bots in u.s. real estate development online communication,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142744621&origin=inward,"
                  Planners and policymakers have been quick to open electronic channels of participation to inform decision-making processes due to the low cost and high potential engagement. This has created an opportunity for subversive groups to amplify their voice, and therefore influence, in land development and planning processes using automated social media accounts. While scholars have begun to examine how social media platforms such as Facebook and Twitter have been used to manipulate political discourse, no research has considered the potential damage that such manipulation can and does cause in the local planning context. This study examines the role and potential risk of automated social media accounts in the participatory planning process through an empirical investigation of 21 real estate development projects in the U.S. Using a machine learning model to assess 41,191 relevant Tweets, we found that at least 10% and upwards of 50% of Tweets concerning the various real estate development projects were generated by bots. Automated tweets included critical, neutral, and supportive content demonstrating that social bots represent a range of interests. Planning and public policy professionals will need to develop, apply, and be aware of bot-detection tools and actively promote and build trust through their own digital channels to ensure accurate information is disseminated.
               ",unknown,855
10.1016/j.jclepro.2022.134638,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85140443955,scopus,2022-12-15,scopus,a machine learning approach to investigate the build-up of surface ozone in mexico-city,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140443955&origin=inward,"
                  Ground-level ozone is an important pollutant regarding air quality and climate. Mexico City frequently experiences severe ozone episodes due to a combination of strong ozone precursor emissions and its specific topographical environment which critically impacts meteorological conditions. High ozone levels during these episodes cause harmful effects to the public health and the environment. This necessitates ranking air quality and meteorological variables according to their contributions towards the build-up of ozone. In this study, three machine learning models are used to learn a prediction function with hourly data of eight predictors as input and hourly ground-level ozone mixing ratios as output. One-year hourly data of eight predictors collected in Mexico-City from March 2015 to February 2016 is employed to train and test the models. The best model, capturing ozone peak levels with 92% accuracy during 6–18 March 2016, is used to rank the predictors according to their importance in the build-up of ozone applying a shapley additive explanations approach based on the game theory shapley values. This 6–18 March 2016 period encompassed different meteorological and emission conditions and included a severe ozone smog episode from 12 to 17 March 2016. Such ranking of the air quality and meteorological variables is crucial for policy-making decisions regarding the prevention and mitigation of ozone detrimental effects during severe ozone episodes and provides insight into the functional dependency of ozone on its predictors. The proposed approach showcases Mexico City, but its principles can be applied for ozone episodes at any other location.
               ",unknown,866
10.1016/j.asoc.2022.109809,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85144406014,scopus,2022-12-01,scopus,graph ensemble deep random vector functional link network for traffic forecasting,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144406014&origin=inward,"
                  Traffic forecasting is crucial to achieving a smart city as it facilitates public transportation management, autonomous driving, and the resource relocation of the sharing economy. Traffic forecasting belongs to the challenging spatiotemporal forecasting task, which is highly demanding because of the complicated geospatial correlation between traffic nodes, inconsistent and highly non-linear temporal patterns due to various events, and sporadic traffic accidents. Previous graph neural network (GNN) models built for transportation forecasting feature the sophisticated structure and heavy computation cost as they combine the deep neural network and graph machine learning to capture the spatiotemporal dynamics for the whole transportation network. However, it may be more practical for practitioners to perform node-wise forecasting for specific nodes of interest rather than network-wise forecasting. To mitigate the gaps mentioned above, we propose a novel graph ensemble deep random vector functional link network (GEdRVFL) to forecast the future traffic volume by combining the well-performing ensemble deep random vector functional link (EdRVFL) with the graph convolution layer for a specific node and realize the node-wise traffic forecasting. After a comprehensive comparison with the state-of-the-art models, our model beats the others in four out of five cases measured by mean absolute scaled error.
               ",unknown,871
10.1016/j.smhl.2022.100323,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85139738611,scopus,2022-12-01,scopus,machine learning and comorbidity network analysis for hospitalized patients with covid-19 in a city in southern brazil,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139738611&origin=inward,"
                  The large amount of data generated during the COVID-19 pandemic requires advanced tools for the long-term prediction of risk factors associated with COVID-19 mortality with higher accuracy. Machine learning (ML) methods directly address this topic and are essential tools to guide public health interventions. Here, we used ML to investigate the importance of demographic and clinical variables on COVID-19 mortality. We also analyzed how comorbidity networks are structured according to age groups. We conducted a retrospective study of COVID-19 mortality with hospitalized patients from Londrina, Parana, Brazil, registered in the database for severe acute respiratory infections (SIVEP-Gripe), from January 2021 to February 2022. We tested four ML models to predict the COVID-19 outcome: Logistic Regression, Support Vector Machine, Random Forest, and XGBoost. We also constructed a comorbidity network to investigate the impact of co-occurring comorbidities on COVID-19 mortality. Our study comprised 8358 hospitalized patients, of whom 2792 (33.40%) died. The XGBoost model achieved excellent performance (ROC-AUC = 0.90). Both permutation method and SHAP values highlighted the importance of age, ventilatory support status, and intensive care unit admission as key features in predicting COVID-19 outcomes. The comorbidity networks for old deceased patients are denser than those for young patients. In addition, the co-occurrence of heart disease and diabetes may be the most important combination to predict COVID-19 mortality, regardless of age and sex. This work presents a valuable combination of machine learning and comorbidity network analysis to predict COVID-19 outcomes. Reliable evidence on this topic is crucial for guiding the post-pandemic response and assisting in COVID-19 care planning and provision.
               ",unknown,876
10.1016/j.atmosenv.2022.119372,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85137753553,scopus,2022-12-01,scopus,"impact of the 2020 covid-19 lockdown on no<inf>2</inf> and pm<inf>10</inf> concentrations in berlin, germany",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137753553&origin=inward,"
                  In March 2020, the World Health Organization declared a pandemic due to the rapid and worldwide spread of the SARS-CoV-2 virus. To prevent spread of the infection social contact restrictions were enacted worldwide, which suggest a significant effect on the anthropogenic emission of gaseous and particulate pollutants in urban areas. To account for the influence of meteorological conditions on airborne pollutant concentrations, we used a Random Forest machine learning technique for predicting business as usual (BAU) pollutant concentrations of NO2 and PM10 at five observation sites in the city of Berlin, Germany, during the 2020 COVID-19 lockdown periods. The predictor variables were based on meteorological and traffic data from the period of 2017–2019. The differences between BAU and observed concentrations were used to quantify lockdown-related effects on average pollutant concentrations as well as spatial variation between individual observation sites. The comparison between predicted and observed concentrations documented good overall model performance for different evaluation periods, but better performance for NO2 (R2 = 0.72) than PM10 concentrations (R2 = 0.35). The average decrease of NO2 was 21.9% in the spring lockdown and 22.3% in the winter lockdown in 2020. PM10 concentrations showed a smaller decrease, with an average of 12.8% in the spring as well as the winter lockdown. The model results were found sensitive to depict local variation of pollutant reductions at the different sites that were mainly related to locally varying modifications in traffic intensity.
               ",unknown,879
10.1016/j.eswa.2022.118129,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85134840596,scopus,2022-12-01,scopus,cross-city crash severity analysis with cost-sensitive transfer learning algorithm,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134840596&origin=inward,"
                  Due to the difficulties of data collection in some cities and the relatively small portion of more severe crashes, this paper proposes a cost-sensitive transfer learning framework for more robust crash severity analysis. Specifically, to address the class imbalance issue, cost-sensitive learning method is adopted by assigning unequal cost values to different crash severity levels to obtain unbiased classification results. Moreover, due to the existence of city-irrelevant common crash contributing factors, cross-city crash severity analysis is developed, i.e., transferring common crash severity knowledge from other cities with transfer learning algorithm to assist crash severity modelling in target city that suffers from data scarcity problem. In this study, the crash datasets of Victorian Australia and Seattle are selected as the target and source domains respectively. To address the issue of heterogeneous explanatory features among these two datasets and extract the interpretable common crash severity knowledge, a feature alignment approach is proposed which can represent the cross-city data with unified feature representations on their original explanatory feature spaces. The two proposed models, i.e., cost-sensitive transfer logistic regression (CST-LR) and cost-sensitive transfer support vector machine (CST-SVM), have demonstrated better performance in comparison with twelve commonly used crash severity models, especially when target city crash data is scarce. The most significant crash contributing factors extracted by proposed model also show higher degree of consistency with the true contributing factors obtained from the target dataset, in comparison with the model built without transfer learning. The results could provide policy implications and counter measures for crash severity mitigation.
               ",unknown,881
10.1016/j.trc.2022.103886,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85139591939,scopus,2022-11-01,scopus,gmix: graph-based spatial–temporal multi-agent reinforcement learning for dynamic electric vehicle dispatching system,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139591939&origin=inward,"
                  The past decade has witnessed a significant growth of electric vehicles (EVs) deployment in public and private transportation sectors. Dynamic electric vehicle routing aims to plan the routes of EVs to serve dynamically generated customers’ requests while ensuring the battery level by visiting the recharge stations. This paper studies the dynamic electric vehicle routing problem with simultaneous pick-up and delivery and soft time windows (DEVRP-STW). We design a graph-based spatio-temporal multi-agent reinforcement learning (GMIX) framework consisting of a spatio-temporal graph attention network (ST-GAT) and a value decomposition-based multi-agent reinforcement learning algorithm (Graph-QMIX). Graph-QMIX derives a multi-agent soft policy gradient based on QMIX and Soft Actor–Critic and utilises a graph-based mixing network proposed to enhance the interaction of agents. Extensive experiments including a case study using real-world taxi data from New York City and a simulation study are conducted. The proposed GMIX outperforms the baseline algorithms on the request–response ratio (RRO), the average travelling distance per completed request (ATR), the out of energy ratio (OER) and the average waiting time per completed request (AWR).
               ",unknown,897
10.1016/j.rse.2022.113205,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85135979166,scopus,2022-10-01,scopus,mapping tree species proportions from satellite imagery using spectral–spatial deep learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135979166&origin=inward,"Remote sensing can be used to collect information related to forest management. Previous studies demonstrated the potential of using multispectral satellite imagery for classifying tree species. However, methods that can map tree species in mixed forest stands on a large scale are lacking. We propose an innovative method for mapping the proportions of tree species using Sentinel-2 imagery. A convolutional neural network was used to quantify the per-pixel basal area proportions of tree species considering the neighbouring environment (spectral–spatial deep learning). A nested U-shaped neural network (UNet++) architecture was implemented. We produced a map of the entire Wallonia Region (southern Belgium). Nine species or groups of species were considered: Spruce genus, Oak genus, Beech, Douglas fir, Pine genus, Poplar genus, Larch genus, Birch genus, and remaining species. The training dataset for the convolutional neural network model was prepared using a map of forest parcels extracted from the public forest administration’s geodatabase of Wallonia. The accuracy of the predicted map covering the region was independently assessed using data from the regional forest inventory of Wallonia. A robust assessment method for tree species proportions maps was proposed for assessing the (1) majority species, (2) species composition (presence or absence), and (3) species proportions (proportion values). The achieved value of indicator OA m a j (0.73) shows that our approach can map the majority tree species in mixed and pure forest stands. Indicators MS (0.89), MPS (0.72) and MUS (0.83) support that the model can predict the species composition in most cases in the study area. Spruce genus, Oak genus, Beech, and Douglas fir achieved the best results, with PAs and UAs close to or higher than 0.70. Particularly, high performance was achieved for detecting Oak genus and Beech in low area proportions: PAs and UAs higher than 0.70 from the 0.4 proportion. Predicted proportions had a R a d j 2 of 0.50. The proposed method, which uses spectral–spatial deep learning to map the proportions of tree species, is innovative because it was adapted to the complexity of mixed forests and spatial resolution of current satellite imagery. Additionally, it optimises the use of available forest data in the model conception by considering all pixels from pure stands to highly mixed forest stands. When forest inventories are available in a broad sense, that is, georeferenced areas with the proportions of tree species, this method is highly reproducible and applicable at a large scale, offering potential for use in forest management.",unknown,906
10.1016/j.rse.2022.113200,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85135796176,scopus,2022-10-01,scopus,multiparametric sea state fields from synthetic aperture radar for maritime situational awareness,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135796176&origin=inward,"This paper introduces a method for estimating a series of sea state parameters from satellite-borne synthetic aperture radar (SAR). The method was realized in a near real time (NRT) application which allows for the processing of data from different satellites and modes. The algorithm estimates the total significant wave height H s , dominant and secondary swell and windsea wave heights, first, and second moment wave periods, the mean wave period and the period of wind sea. The algorithm was applied to the Sentinel-1 (S1) C-band Interferometric Wide Swath Mode (IW), Extra Wide (EW) and Wave Mode (WV) Level-1 (L1) products and also extended to X-band TerraSAR-X (TS-X) StripMap (SM) products. The scenes are processed in raster and result in continuous sea state fields, with the exception of S1 WV, where averaged values for sea state parameters for along-orbit imagettes of 20 km × 20 km are presented. The developed empirical algorithm consists of two parts: a first CWAVE_EX (extended CWAVE) part, based on a linear regression approach, and a subsequent machine learning part using the support vector machine (SVM) technique. A series of new data preparation steps (i.e. filtering, denoising) and new features estimated from SAR images are also introduced. The algorithm was tuned and validated using two independent global wave model hindcasts, WaveWatch-3 and MFWAM as well as National Data Buoy Center (NDBC) measurements. The achieved root mean squared errors (RMSE) for CWAVE_EX for the total H s are 0.60 m for low-resolution modes S1 IW (10 m pixel) and EW (40 m pixel) and 0.35 m for S1 WV and TS-X SM (pixel spacing ca.1–4 m) in comparison to model predictions. The RMSEs of the retrieved wave periods are in the range of 0.45–0.90 s for all of the satellites and models considered. Similarly, the dominant and secondary swell, and wind sea wave height RMSEs are in the range of 0.35–0.80 m. The SVM postprocessing improves the accuracy of the initial results of CWAVE_EX for H s and reaches an RMSE of 0.25 m for S1 WV. Comparisons to 64 NDBC buoys, collocated at distances shorter than 50 km to S1 WV imagettes worldwide, result in an RMSE of 0.41 m. All results and the methods presented are novel in terms of the accuracy achieved, combining the classical approach with machine learning techniques, and performing an automatic NRT processing of multiparametric sea state fields from L1 data with automatic switching for different satellites and modes. The complete archive of S1 WV L1 Single Look Complex products from December 2014 until February 2021 was processed to create a sea state parameter database and validated using model hindcast and buoy measurements. The derived parameters are available to the public within the scope of the European Space Agency's Climate Change Initiative.",unknown,907
10.1016/j.scs.2022.104049,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85134773132,scopus,2022-10-01,scopus,pm<inf>2.5</inf> concentration forecasting through a novel multi-scale ensemble learning approach considering intercity synergy,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134773132&origin=inward,"
                  Accurate PM2.5 concentration prediction can provide reliable air pollution warning information to the public. However, previous studies have often focused on the data of the target city itself, ignoring the interaction among cities in the same region. In this paper, we develop a multi-scale ensemble learning approach to forecast daily PM2.5 concentrations of the target city by modeling its air and climate indicators, and PM2.5 value of its neighboring cities. First, the proposed approach smooths the multivariate data by singular spectrum analysis and performs multi-feature selection based on distance factor and predictive power of data. Second, the inherent association among the obtained multiple features is captured by multivariate empirical modal decomposition. Third, the Hurst exponent is applied to match each time scale with the corresponding predictor for multi-step prediction. Finally, the forecasting values of all time scales are summed to obtain the PM2.5 concentration forecasting results of the target city. Four experiments involving Beijing, Wuhan, and Shenzhen are carried out to verify the accuracy and robustness of the proposed approach. The experimental results show that our approach outperforms all benchmark models, and introducing city synergy strategy can improve the forecasting performance significantly.
               ",unknown,908
10.1016/j.compenvurbsys.2022.101854,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85134391576,scopus,2022-10-01,scopus,a machine learning approach to evaluate the spatial variability of new york city's 311 street flooding complaints,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134391576&origin=inward,"
                  Urbanization, accompanied by the creation of roads, pavements, and sidewalks creates an environment where there is limited infiltration capacity, leaving metropolitan areas especially vulnerable during intense rain events. Furthermore, within an urban setting, there is spatial variability, as certain areas, owing to location, topography, land feature conditions, population and physical attributes or precipitation patterns, are more prone to flood damages. To detect neighborhoods with increased flood risk, crowdsourced data, which is the consolidation of eyewitness accounts, affords particular value. With an intent to understand how factors affect the spatial variability of street flooding, the Random Forest regression machine learning algorithm is employed, where the 311 street flooding reports of New York City (NYC) serve as the response, while the explanatory variables include topographic and land feature, physical and population dynamics, locational, infrastructural, and climatic influences. This study also analyzes socio-economic variables as predictors, as to allow for better insight into potential biases within the NYC 311 crowdsourced platform. It is found that catch basin complaints have overwhelmingly the greatest predictor importance, at 41%, almost sixfold higher than that of the second highest ranked predictor, slope, at 6.7%. Thus, NYC has an apparent issue with debris blocking the basins, and this may be remediated by increased cleaning efforts or public awareness to maintain clear streets, particularly during forecasted rain events. Furthermore, more than a third of the top predictors are land feature and topographical conditions, with building characteristics dominating the category. Often excluded in urban flood models, building effects, with a combined total importance of 11.7%, have greater significance than commonly considered flooding factors, such as percent impervious cover or elevation. Another major finding is the significance of the ‘commuters who drive alone’ variable, which alerts to the prospect of more reports being filed by those more affected by street flooding, as opposed to reflecting the actual occurrence of flooding (more reports being filed by those who drive on flooded roads versus those who do not). Overall, the leading contribution of this study is the identification of the top flooding factors in NYC, along with the presentation of their specific impacts towards street flooding variability among zip codes.
               ",unknown,909
10.1016/j.giq.2022.101722,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85132505364,scopus,2022-10-01,scopus,public ai canvas for ai-enabled public value: a design science approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132505364&origin=inward,"
                  Public agencies have a strong interest in artificial intelligence (AI) systems. However, many public agencies lack tools and frameworks to articulate a viable business model and evaluate public value as they consider investing in AI systems. The business model canvas used extensively in the private sector offers us a foundation for designing a public AI canvas (PAIC). Employing a design science approach, this study reports on the design and evaluation of PAIC. The PAIC comprises three distinctive layers: (1) the public value-oriented AI-enablement layer; (2) the public value logic layer; and (3) the public value-oriented social guidance layer. PAIC offers guidance on innovating the business models of public agencies to create and capture AI-enabled value. For practitioners, PAIC presents a validated tool to guide AI deployment in public agencies.
               ",unknown,910
10.1016/j.giq.2021.101596,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85108562674,scopus,2022-10-01,scopus,enabling ai capabilities in government agencies: a study of determinants for european municipalities,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108562674&origin=inward,"Artificial Intelligence (AI) is gradually becoming an integral part of the digital strategy of organizations. Yet, the use of AI in public organizations in still lagging significantly compared to private organizations. Prior literature looking into aspects that facilitate adoption and use of AI has concentrated on challenges concerning technical aspects of AI technologies, providing little insight regarding the organizational deployment of AI, particularly in public organizations. Building on this gap, this study seeks to examine what aspects enable public organizations to develop AI capabilities. To answer this question, we built an integrated and extended model from the Technology-Organization-Environment framework (TOE) and asked high-level technology managers from municipalities in Europe about factors that influence their development of AI capabilities. We collected data from 91 municipalities from three European countries (i.e., Germany, Norway, and Finland) and analyzed responses by means of structural equation modeling. Our findings indicate that five factors – i.e. perceived financial costs, organizational innovativeness, perceived governmental pressure, government incentives, regulatory support – have an impact on the development of AI capabilities. We also find that perceived citizen pressure and perceived value of AI solutions are not important determinants of AI capability formation. Our findings bear the potential to stimulate a more reflected adoption of AI supporting managers in public organizations to develop AI capabilities.",unknown,912
10.1016/j.envsoft.2022.105458,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85134905805,scopus,2022-09-01,scopus,coastal water quality prediction based on machine learning with feature interpretation and spatio-temporal analysis,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134905805&origin=inward,"
                  Coastal water quality management is a public health concern, as water of poor quality can potentially harbor dangerous pathogens. In this study, we employ routine monitoring data of 
                        
                           E
                           s
                           c
                           h
                           e
                           r
                           i
                           c
                           h
                           i
                           a
                           
                           C
                           o
                           l
                           i
                        
                      and enterococci across 15 beaches in the city of Rijeka, Croatia, to build machine learning models for predicting 
                        
                           E
                           .
                           
                           C
                           o
                           l
                           i
                        
                      and enterococci based on environmental features. Cross-validation analysis showed that the Catboost algorithm performed best with R
                        
                           
                           
                              2
                           
                        
                      values of 0.71 and 0.69 for predicting 
                        
                           E
                           .
                           
                           C
                           o
                           l
                           i
                        
                      and enterococci, respectively, compared to other evaluated algorithms. SHapley Additive exPlanations technique showed that salinity is the most important feature for forecasting both 
                        
                           E
                           .
                           
                           C
                           o
                           l
                           i
                        
                      and enterococci levels. Furthermore, for low water quality sites, the spatial predictive models achieved R
                        
                           
                           
                              2
                           
                        
                      values of 0.85 and 0.83, while the temporal models achieved R
                        
                           
                           
                              2
                           
                        
                      values of 0.74 and 0.67. The temporal model achieved moderate R
                        
                           
                           
                              2
                           
                        
                      values of 0.44 and 0.46 at a site with high water quality.
               ",unknown,924
10.1016/j.landusepol.2022.106256,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85133798098,scopus,2022-09-01,scopus,an intelligent web-based spatial group decision support system to investigate the role of the opponents’ modeling in urban land use planning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133798098&origin=inward,"
                  Urban land-use planning decisions generally require negotiation between multiple stakeholders to reach an agreement on a specific plan. Computer-aided tools such as group decision support systems can facilitate the actors in this complicated process. In the context of these systems, using software agents enhance the effectiveness and efficiency of group decision support. The software agents can perform some computational and analytical tasks on behalf of the stakeholders. In more advanced cases, the agents can also learn stakeholders’ preferences and behavior to help them make good decisions. This paper proposes an intelligent web-based spatial group decision support system to investigate the role of opponents modeling in urban land use planning by using a multi-agent system approach. For this purpose, two successive meetings are held in which the system is used: in the first meeting, the stakeholders revise the existing plans and respond to other stakeholders’ requests. During the meeting, software agents attempt to model the behavior of the stakeholders they are associated with, based on a Bayesian learning method in combination with social value orientation theory to describe stakeholders’ decision behavior in a group context. In the second meeting, the software agents help the stakeholders in the step of plan revision by providing the information obtained to the stakeholders. In an application, a comparison of the results of the meetings showed that the provided information about the opponents reduced the negotiation time and contributed to reaching a better spatial configuration of land-uses based on a criterion provided by social value orientation theory.
               ",unknown,925
10.1016/j.jag.2022.102942,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85135529656,scopus,2022-08-01,scopus,urban spatial risk prediction and optimization analysis of poi based on deep learning from the perspective of an epidemic,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135529656&origin=inward,"From an epidemiological perspective, previous research on COVID-19 has generally been based on classical statistical analyses. As a result, spatial information is often not used effectively. This paper uses image-based neural networks to explore the relationship between urban spatial risk and the distribution of infected populations, and the design of urban facilities. To achieve this objective, we use spatio-temporal data of people infected with new coronary pneumonia prior to 28 February 2020 in Wuhan. We then use kriging, which is a method of spatial interpolation, as well as core density estimation technology to establish the epidemic heat distribution on fine grid units. We further evaluate the influence of nine major spatial risk factors, including the distribution of agencies, hospitals, park squares, sports fields, banks and hotels, by testing them for significant positive correlation with the distribution of the epidemic. The weights of these spatial risk factors are used for training Generative Adversarial Network (GAN) models, which predict the distribution of cases in a given area. The input image for the machine learning model is a city plan converted by public infrastructures, and the output image is a map of urban spatial risk factors in the given area. The results of the trained model demonstrate that optimising the relevant point of interests (POI) in urban areas to effectively control potential risk factors can aid in managing the epidemic and preventing it from dispersing further.",unknown,938
10.1016/j.landusepol.2022.106211,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85131664757,scopus,2022-08-01,scopus,mass appraisal as affordable public policy: open data and machine learning for mapping urban land values,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131664757&origin=inward,"
                  Updated cadastral land values are a matter of critical importance for local governments: higher revenue of property taxes, more equitable treatment to taxpayers, a fundamental input in the design of public policies related to access to land and housing for the most vulnerable and a key feature in land value capture strategies to finance public infrastructure, to name just a few public policies that require correct valuations of land. However, in Latin America, outdated cadastral values are common to most cities. The reasons for this can be found in the complexity of the mass appraisal process, lack of institutional and fiscal capacity to undertake it and bureaucratic resistance to its implementation.
                  The objective of this paper is to present a mass appraisal methodology that uses only free and open data to achieve robust urban land valuations. Information from the OpenStreetMap Project is used to generate several land variables. In addition, the Global Human Settlement Layer of the European Commission is used to determine the level of consolidation of urban sprawl. Land value data were obtained from the Mapa de Valores de América Latina, a collaborative initiative that systemizes more than 68,000 data from more than 900 cities.
                  This information is used to train three tree-based machine learning models: Random Forest, Quantile Random Forest and Gradient Boosting Model. The results support the viability of the proposed strategy, simplifying the mass appraisal process in terms of costs, time and complexity of the information used.
               ",unknown,940
10.1016/j.scs.2022.103858,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85129723687,scopus,2022-08-01,scopus,opencomm: open community platform for data integration and privacy preserving for 311 calls,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129723687&origin=inward,"Local governments are increasingly leveraging administrative data to drive performance. Likewise, cities are interested in improving responsiveness to citizens’ demands and cost savings through data analytics. However, city managers face many challenges when utilizing secondary data, such as 311 call records and the US Census. The challenge of interest to the current study is boundary issues as a result of data being collected at divergent geographic levels over different time horizons. Accordingly, an inductive analytical methodology was developed to create units of analysis that were both pragmatically and analytically appropriate for city managers and local policymakers. We created an open data analytics framework called OpenComm to harmonize administrative and secondary data using administrative data derived from Kansas City, Missouri. This framework produced robust inferences regarding the spatial and temporal aspects for the communities. Privacy-preserving technology, in particular, has been applied to public data to protect community privacy. The findings illustrate the power of inductive data aggregation, leading to empirical insights into hidden patterns of city service disparity over a decade-long time horizon. An application for the Open Data Platform is available at http://kc311.herokuapp.com/.",unknown,941
10.1016/j.landurbplan.2022.104428,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85127911599,scopus,2022-08-01,scopus,"urban greenness, mixed land-use, and life satisfaction: evidence from residential locations and workplace settings in beijing",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127911599&origin=inward,"
                  China’s unprecedented urbanisation in the last several decades has significantly transformed its urban built environment. On the one hand, such an urbanisation has brought about improvement in infrastructure, access to various public facilities, and opportunities for social connectivity, all of which would benefit citizen’s wellbeing. On the other hand, rapid, unplanned, and unregulated urbanisation may lead to air pollution and water pollution, compact neighbourhoods, traffic noise and congestion, and lack of natural amenity, which pose various threats to urban dwellers’ wellbeing and life satisfaction. Therefore, how to build liveable cities has become one of the key goals and top priorities of urban planning in China and other developing countries. This study investigates the effect of urban greenness and mixed land-use, two key dimensions defining urban liveability, on residents’ life satisfaction at both residence and workplace settings in Beijing. Three big geo-coded datasets are combined, including a social survey about residents’ subjective life satisfaction and demographic characteristics, eye-sensored street greenness data extracted from online platform through machine learning, and fine-grain land-use data based on point-of-interest entropy, and then taken into a Bayesian multilevel ordered logit model. The empirical results reveal that (1) street view greenness could enhance life satisfaction at residence, but depress life satisfaction at workplace; (2) mixed land-use could positively contribute to life satisfaction at both residence and workplace; and (3) there exist positive interactions between greenness and mixed land-use. These empirical findings provide practical implications for planning and constructing liveable cities in China and other countries where both urban greening and mixed land-use are promoted and embraced as core elements of the compact city and smart city ideal.
               ",unknown,944
10.1016/j.watres.2022.118685,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85131448661,scopus,2022-07-15,scopus,implementing machine learning to optimize the cost-benefit of urban water clarifier geometrics,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131448661&origin=inward,"
                  Clarification basins are ubiquitous water treatment units applied across urban water systems. Diverse applications include stormwater systems, stabilization lagoons, equalization, storage and green infrastructure. Residence time (RT), surface overflow rate (SOR) and the Storm Water Management Model (SWMM) are readily implemented but are not formulated to optimize basin geometrics because transport dynamics remain unresolved. As a result, basin design yields high costs from hundreds of thousands to tens of million USD. Basin optimization and retrofits can benefit from more robust and efficient tools. More advanced methods such as computational fluid dynamics (CFD), while demonstrating benefits for resolving transport, can be complex and computationally expensive for routine applications. To provide stakeholders with an efficient and robust tool, this study develops a novel optimization framework for basin geometrics with machine learning (ML). This framework (1) leverages high-performance computing (HPC) and the predictive capability of CFD to provide artificial neural network (ANN) development and (2) integrates a trained ANN model with a hybrid evolutionary-gradient-based optimization algorithm through the ANN automatic differentiation (AD) functionality. ANN model results for particulate matter (PM) clarification demonstrate high predictive capability with a coefficient of determination (
                        
                           R
                           2
                        
                     ) of 0.998 on the test dataset. The ANN model for total PM clarification of three (3) heterodisperse particle size distributions (PSDs) also illustrates good performance (
                        
                           
                              R
                              2
                           
                           >
                           0.986
                        
                     ). The proposed framework was implemented for a basin and watershed loading conditions in Florida (USA), the ML basin designs yield substantially improved cost-effectiveness compared to common designs (square and circular basins) and RT-based design for all PSDs tested. To meet a presumptive regulatory criteria of 80% PM separation (widely adopted in the USA), the ML framework yields 4.7X to 8X lower cost than the common basin designs tested. Compared to the RT-based design, the ML design yields 5.6X to 83.5X cost reduction as a function of the finer, medium, and coarser PSDs. Furthermore, the proposed framework benefits from ANN’s high computational efficiency. Optimization of basin geometrics is performed in minutes on a laptop using the framework. The framework is a promising adjuvant tool for cost-effective and sustainable basin implementation across urban water systems.
               ",unknown,948
10.1016/j.ijforecast.2021.07.008,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85114377146,scopus,2022-07-01,scopus,a data-driven approach to forecasting ground-level ozone concentration,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114377146&origin=inward,"The ability to forecast the concentration of air pollutants in an urban region is crucial for decision-makers wishing to reduce the impact of pollution on public health through active measures (e.g. temporary traffic closures). In this study, we present a machine learning approach applied to forecasts of the day-ahead maximum value of ozone concentration for several geographical locations in southern Switzerland. Due to the low density of measurement stations and to the complex orography of the use-case terrain, we adopted feature selection methods instead of explicitly restricting relevant features to a neighborhood of the prediction sites, as common in spatio-temporal forecasting methods. We then used Shapley values to assess the explainability of the learned models in terms of feature importance and feature interactions in relation to ozone predictions. Our analysis suggests that the trained models effectively learned explanatory cross-dependencies among atmospheric variables. Finally, we show how weighting observations helps to increase the accuracy of the forecasts for specific ranges of ozone’s daily peak values.",unknown,958
10.1016/j.tranpol.2022.04.011,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85129315953,scopus,2022-06-01,scopus,"the deceitful connected and autonomous vehicle: defining the concept, contextualising its dimensions and proposing mitigation policies",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129315953&origin=inward,"The Connected and Autonomous Vehicle (CAV) is an emerging mobility technology that may hold a paradigm-changing potential for the future of transport policy and planning. Despite a wealth of likely benefits that have made their eventual launch inescapable, CAVs may also be a source of unprecedented disruption for tomorrow's travel eco-systems because of their vulnerability to cyber-threats, hacking and misinformation. CAVs manipulated by users, traffic controllers or third parties may act in deceitful ways. This scene-setting work introduces the deceitful CAV, a vehicle that operates in a deceitful manner towards routing and control functionality for ‘selfish’ or malicious purposes and contextualises its diverse expressions and dimensions. It specifically offers a systematic taxonomy of eight distinctive deceitful behaviours namely: suppression/camouflage, overloading, mistake, substitution, target conditioning, repackaging capability signatures, amplification and reinforcing impression. These as exemplified by their most common attack forms (i.e., starvation, denial-of-service, session hijacking, man-in-the-middle, poisoning, masquerading, flooding and spoofing) are then benchmarked against five key dimensions referring to time frame (short to long duration), engagement (localised to systemic), urban traffic controller infrastructure (single to multiple components), scale (low to high), and impact (low to high). We then suggest mitigation strategies to protect CAV technology against these dangers. These span from purely technological measures referring to the machine-centric triad of vehicles, communication, and control system including adversarial training, heuristic decision algorithms and weighted voting mechanisms to human factor measures that focus on education, training, awareness enhancement, licensing and legislation initiatives that will enable users and controllers to prevent, control or report deceitful activities.",unknown,967
10.1016/j.ijdrr.2022.102979,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85129202534,scopus,2022-06-01,scopus,time series sentiment analysis (sa) of relief operations using social media (sm) platform for efficient resource management,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129202534&origin=inward,"
                  The ease of access to the internet has sparked a worldwide interest in SM in recent years. The possibilities for the use of SM as a potential source in improving the management of disasters are increasing. Although there is an existing research ongoing with the analysis of the usage of SM during disasters, but none of the works have exploited the use of explainable artificial intelligence (XAI) for the validation of the transparency of the ML models. The contribution of this research is two-fold: Firstly, ML based time series analysis for relief operations using social media information with situational information gathered using Twitter from the users and resource providers and secondly, XAI has been used to increase the transparency and understand-ability of model decisions. For the case study, public dataset from Nepal, Italy earthquakes, COVID-19 dataset along with originally collected Twitter dataset has been considered. The performance of the extreme gradient boosting (xgboost) model is relatively superior than other techniques with 10-fold training mean accuracy of 87.17%. Thus, the experiments conclude the possibility of automation for the time series analysis for optimal relief operation management to serve the victims in the most efficient way and to control legal and administration implications.
               ",unknown,968
10.1016/j.buildenv.2022.109057,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85128174830,scopus,2022-06-01,scopus,transfer learning for estimating occupancy and recognizing activities in smart buildings,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128174830&origin=inward,"
                  Activities Recognition (AR) and Occupancy Estimation (OE) are vital to many smart systems that work on providing good services in smart buildings. Many applications, such as energy management need information like activities and occupancy to provide good assistance. Most of the previous research about AR and OE focused on applying supervised machine learning methods. Researchers train a model and evaluate it using data collected from the same environment (Domain). A model trained in a specific domain will not generalize well in other domains. Creating a trained model to every environment is not feasible due to the lack of data. Collecting sufficient data can be time consuming and infeasible in some cases. Computational power can be a challenge for researchers by increasing the training time due to the lack of the required computing resources. Using traditional machine learning methods, the obtained performance may be unsatisfactory, and can not lead to optimal solutions. For all these reasons, we need a solution that helps us overcome the stated problem and obtain models with acceptable results. In this work, we present and discuss different transfer learning methods that help us transfer knowledge from a source domain to a target domain. The goal is to reuse as much as possible information from the source domain to enhance the performance of the model at the target domain. This type of approaches will solve the problems mentioned before such as the lack of data and will provide us with good results due to the use of knowledge from multiple source domains. We tested five Transfer learning (TL) approaches: a principal component analysis (PCA)-like method that creates a transformation like the PCA transformation and apply it to the data to create new common domain, a PCA based method that creates common domain using PCA, a PCA-SMOTE method that balances the data and creates common domain, a basic method based on a simple matching between similar features from source and target domain, and a sparse coding-based method that creates a common domain where the data representation will be as sparse as possible. The impressive results that we obtained in both tasks prove that the presented methods can be applied to transfer knowledge across different domains.
               ",unknown,969
10.1016/j.scs.2022.103830,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85126293490,scopus,2022-06-01,scopus,ann-based traffic volume prediction models in response to covid-19 imposed measures,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126293490&origin=inward,"
                  Many countries around the globe have imposed several response measures to suppress the rapid spread of the COVID-19 pandemic since the beginning of 2020. These measures have impacted routine daily activities, along with their impact on economy, education, social and recreational activities, and domestic and international travels. Intuitively, the different imposed policies and measures have indirect impacts on urban traffic mobility. As a result of those imposed measures and policies, urban traffic flows have changed. However, those impacts are neither measured nor quantified. Therefore, estimating the impact of these combined yet different policies and measures on urban traffic flows is a challenging task. This paper demonstrates the development of an artificial neural networks (ANN) model which correlates the impact of the imposed response measure and other factors on urban traffic flows. The results show that the adopted ANN model is capable of mapping the complex relationship between traffic flows and the response measures with a high level of accuracy and good performance. The predicted values are closed to the observed ones. They are clustered around the regression line, with a coefficient of determination (
                        
                           R
                           2
                        
                     ) of 0.9761. Furthermore, the developed model can be generalized to determine the anticipated demand levels resulted from imposing any of the response measures in the post-pandemic era. This model can be used to manage traffic during mega-events. It can be also utilized for disaster or emergency situations, where traffic flow estimates are highly required for operational and planning purposes.
               ",unknown,970
10.1016/j.compenvurbsys.2022.101790,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85126128177,scopus,2022-06-01,scopus,a social sensing approach for everyday urban problem-handling with the 12345-complaint hotline data,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126128177&origin=inward,"
                  The emergence of various types of everyday urban problems poses severe challenges to the conventional top-down designed urban problem-handling protocols. Social sensing data, such as the urban problem complaint hotline records, constitute good sources for collecting bottom-up information on urban problems. Nevertheless, a partially obsolete urban problem categorization standard, a manually operated and experience-dependent workflow for problem classification and pushing, and an unoptimized problem-handling responsibility designation among the municipal departments have been impairing the sufficient utilization of the collected complaint data toward more efficient urban problem-handling performance. In this paper, based on the 12345-hotline data collected in Sanya City, China, we 1) construct an urban problem tree that presents a new and expandable classification of everyday urban problems with 7 super-types, 34 types, and 105 sub-types of events, thus complementing that in the existing National Standard which merely contains 6 types and 66 sub-types; 2) design an auto-classification algorithm for the complaint records with Natural Language Processing and Supervised Machine Learning methods, which achieves an accuracy of up to 75% at the super-type level; and 3) optimize the urban problem-handling responsibility designation among the municipal departments for improved departmental function distinction and balanced workloads, which combined with the auto-classification algorithm yield a decrease of the problem pushing failure ratio from the current 73.8% to 14.6%. We integrate all the above functions in a web-based information system to establish a digital platform for practical urban problem-handling. We conclude the paper with discussions on the paper's implications of an overhaul of urban legislative and executive systems following a bottom-up paradigm and reflections on the paper's limitations and theoretical implications from a complex-system theory perspective.
               
                  Significance
                  
                     
                        
                           •
                           An urban problem tree constructed from the “12345” Hotline is a complement to the National Standard.
                        
                        
                           •
                           Developing NLP and supervised ML algorithms for effective classification of crowd-sourced hotline complaints.
                        
                        
                           •
                           Optimizing the local government structure for urban problem-handling from a bottom-up approach.
                        
                        
                           •
                           Implementing a web-based system for hotline handling which decreases the failed pushes from 73.8% to 14.6%.
                        
                     
                  
               ",unknown,971
10.1016/j.envres.2022.112759,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85123701950,scopus,2022-05-15,scopus,"spatiotemporal estimation of the pm<inf>2.5</inf> concentration and human health risks combining the three-dimensional landscape pattern index and machine learning methods to optimize land use regression modeling in shaanxi, china",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123701950&origin=inward,"
                  PM2.5 pollution endangers human health and urban sustainable development. Land use regression (LUR) is one of the most important methods to reveal the temporal and spatial heterogeneity of PM2.5, and the introduction of characteristic variables of geographical factors and the improvement of model construction methods are important research directions for its optimization. However, the complex non-linear correlation between PM2.5 and influencing indicators is always unrecognized by the traditional regression model. The two-dimensional landscape pattern index is difficult to reflect the real information of the surface, and the research accuracy cannot meet the requirements. As such, a novel integrated three-dimensional landscape pattern index (TDLPI) and machine learning extreme gradient boosting (XGBOOST) improved LUR model (LTX) are developed to estimate the spatiotemporal heterogeneity in the fine particle concentration in Shaanxi, China, and health risks of exposure and inhalation of PM2.5 were explored. The LTX model performed well with R2 = 0.88, RMSE of 8.73 μg/m3 and MAE of 5.85 μg/m3. Our findings suggest that integrated three-dimensional landscape pattern information and XGBOOST approaches can accurately estimate annual and seasonal variations of PM2.5 pollution The Guanzhong Plain and northern Shaanxi always feature high PM2.5 values, which exhibit similar distribution trends to those of the observed PM2.5 pollution. This study demonstrated the outstanding performance of the LTX model, which outperforms most models in past researches. On the whole, LTX approach is reliable and can improve the accuracy of pollutant concentration prediction. The health risks of human exposure to fine particles are relatively high in winter. Central part is a high health risk area, while northern area is low. Our study provides a new method for atmospheric pollutants assessing, which is important for LUR model optimization, high-precision PM2.5 pollution prediction and landscape pattern planning. These results can also contribute to human health exposure risks and future epidemiological studies of air pollution.
               ",unknown,977
10.1016/j.jag.2022.102794,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85135884538,scopus,2022-05-01,scopus,multi-modal fusion of satellite and street-view images for urban village classification based on a dual-branch deep neural network,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135884538&origin=inward,"With the rapid urbanization process in China, numerous urban villages have been appeared, which are surrounded by the newly-built urban blocks. Due to the high population density, poor hygiene, chaotic waste discharge, and inadequate public facilities, urban villages have many negative impacts on both urban environment and management. The objective of this study is to propose a dual-branch deep learning model for multi-modal satellite and street-view data fusion to detect urban villages in Beijing, Tianjin and Shijiazhuang, which are the core cities of Jing-Jin-Ji region of China. Specifically, the proposed model consists of a satellite branch, a street-view branch and a gated-fusion module. As for the satellite branch, a Trans-MDCNN (multi-scale dilated convolutional neural network) is proposed to learn multi-level local features and global contextual features from high resolution satellite imagery, while for the street-view branch, an MVRAN (multi-view recurrent attention network) is constructed to learn and fuse multi-angle features from street-view images. A gated-fusion module is designed to aggregate the important features of the dual-branches. Experimental results indicate that the proposed model has achieved good performance with an overall accuracy (OA) of 92.61%. Ablation study shows that compared with satellite data alone, the integration of street-view images could increase the OA by about 2%. Besides, 1-D feature fusion outperforms its 2-D counterpart and the classic feature concatenation method. The proposed model also yields a better performance than other deep learning models. Finally, the dataset of this study, S2UV (Satellite & Street-view images for Urban Village classification), is publicly available: https://doi.org/10.11922/sciencedb.01410.",unknown,978
10.1016/j.compbiomed.2022.105354,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85125426780,scopus,2022-05-01,scopus,predicting the impact of the third wave of covid-19 in india using hybrid statistical machine learning models: a time series forecasting and sentiment analysis approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125426780&origin=inward,"
                  Background
                  Since January 2020, India has faced two waves of COVID-19; preparation for the upcoming waves is the primary challenge for public health sectors and governments. Therefore, it is important to forecast future cumulative confirmed cases to plan and implement control measures effectively.
               
                  Methods
                  This study proposed a hybrid autoregressive integrated moving average (ARIMA) and Prophet model to predict daily confirmed and cumulative confirmed cases. The built-in auto.arima function was first used to select the optimal hyperparameter values of the ARIMA model. Then, the modified ARIMA model was used to find the best fit between the test and forecast data to find the best model parameter combinations. Articles, blog posts, and news stories from virologists, scientists, and health experts related to the third wave of COVID-19 were gathered using the Python web scraping package Beautiful Soup. Their opinions (sentiments) toward the potential third wave were analyzed using natural language processing (NLP) libraries.
               
                  Results
                  A spike in daily confirmed and cumulative confirmed cases was predicted in India in the next 180 days based on past time series data. The results were validated using various analytical tools and evaluation metrics, producing a root mean square error (RMSE) of 0.14 and a mean absolute percentage error (MAPE) of 0.06. The NLP processing results revealed negative sentiments in most articles and blogs, with few exceptions.
               
                  Conclusion
                  The findings of this study suggest that there will be more active cases in the upcoming days. The proposed models can forecast future daily confirmed and cumulative confirmed cases. This study will help the country and states plan appropriate public health measures for the upcoming waves of COVID-19.
               ",unknown,984
10.1016/j.aap.2022.106618,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85125244958,scopus,2022-05-01,scopus,tracking traffic congestion and accidents using social media data: a case study of shanghai,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125244958&origin=inward,"
                  Traffic congestion and accidents take a toll on commuters' daily experiences and society. Locating the venues prone to congestion and accidents and capturing their perception by public members is invaluable for transport policy-makers. However, few previous methods consider user perception toward the accidents and congestion in finding and profiling the accident- and congestion-prone areas, leaving decision-makers unaware of the subsequent behavior responses and priorities of retrofitting measures. This study develops a framework to identify and characterize the accident- and congestion-prone areas heatedly discussed on social media. First, we use natural language processing and deep learning to detect the accident- and congestion-relevant Chinese microblogs posted on Sina Weibo, a Chinese social media platform. Then a modified Kernel Density Estimation method considering the sentiment of microblogs is employed to find the accident- and congestion-prone regions. The results show that the 'congestion-prone areas' discussed on social media are mainly distributed throughout the historical urban core and the Northwest of Pudong New Area, in reasonably good agreements with actual congestion records. In contrast, the 'accident-prone areas' are primarily found in locations with severe accidents. Finally, the above venues are characterized in spatio-temporal and semantic aspects to understand the nature of the incidents and assess the priority level for mitigation measures. The outcomes can provide a reference for traffic authorities to inform resource allocation and prioritize mitigation measures in future traffic management.
               ",unknown,985
10.1016/j.envres.2021.112576,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85122462108,scopus,2022-04-15,scopus,an iot based system for magnify air pollution monitoring and prognosis using hybrid artificial intelligence technique,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122462108&origin=inward,"
                  Air pollution is the existence of atmospheric chemicals damaging the health of human beings and other living organisms or damaging the environment or resources. Rarely any common contaminants are smog, nicotine, mold, yeast, biogas, or carbon dioxide. The paper will primarily observe, visualize and anticipate pollution levels. In particular, three algorithms of Artificial Intelligence were used to create good forecasting models and a predictive AQI model for 4 distinct gases: carbon dioxide, sulphur dioxide, nitrogen dioxide, and atmospheric particulate matter. Thus, in this paper, the Air Qualification Index is developed utilizing Linear Regression, Support Vector Regression, and the Gradient Boosted Decision Tree GBDT Ensembles model over the next 5 h and analyzes air qualities using various sensors. The hypothesized artificial intelligence models are evaluated to the Root Mean Squares Error, Mean Squared Error and Mean absolute error, depending upon the performance measurements and a lower error value model is chosen. Based on the algorithm of the Artificial Intelligent System, the level of 5 air pollutants like CO2, SO2, NO2, PM 2.5 and PM10 can be predicted immediately by integrating the observations with errors. It may be used to detect air quality from distance in large cities and can assist lower the degree of environmental pollution.
               ",unknown,990
10.1016/j.landusepol.2022.106048,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85125311360,scopus,2022-04-01,scopus,does visual contact with green space impact housing pricesʔ an integrated approach of machine learning and hedonic modeling based on the perception of green space,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125311360&origin=inward,"
                  Green space, especially visual contact with greenery, is an important aspect of space quality assessment and has a significant impact on the premium of real estate. Due to the limitations in available data and technologies, existing hedonic studies mainly capture accessibility to green space (i.e., proximity), and how visual contact with green space (i.e., visibility) impacts housing prices is not well understood. This paper measures the intangible concept of visual contact with green space by taking advantage of street view images and community photos based on semantic segmentation. Then, based on hedonic model theory, a set of housing price determinants is selected. Finally, we construct two models, namely, random forest regression and geographically weighted regression models, to explore the capitalization effects of visual contact with green space on housing prices in Shenzhen, China. In our study, compared with indicators such as park accessibility and greening rate, variables of green visual contact are more important for housing prices. Moreover, the effects of visual contact with green space inside and outside the community display marked spatial variations. This work is a beneficial attempt to focus on the human scale for the actual demand of urban green planning. The findings of this paper add further knowledge to highlight the importance of visual contact with green space in street design and urban planning, which can enrich research on the amenity values of green space and the quality of public space theoretically and methodologically.
               ",unknown,994
10.1016/j.jhydrol.2022.127532,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85124243775,scopus,2022-04-01,scopus,developing an integrated investment decision-support framework for water-sensitive urban design projects,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124243775&origin=inward,"
                  Traditional centralized water-management systems have tended to fall short in delivering environmental and amenity benefits in urban areas. Water-sensitive urban design (WSUD) projects have been proposed as an alternative due to their ability to generate multiple benefits, including improving water quality, water supply, aesthetics, urban cooling, spaces for recreation, and habitat for biodiversity. However, in most cities, investment in such systems has been relatively low, in part due to the difficulty of monetizing the multifunctional benefits of WSUD projects and including them in comprehensive economic analyses. We describe the development, testing and application of INFFEWS (Investment Framework For the Economics of Water Sensitive cities), an economic decision-support system for investment in WSUD projects. INFFEWS is based on a Benefit: Cost Analysis (BCA) framework and is consistent with standard BCA. A prominent feature of the framework is its strong emphasis on quantifying the monetary-equivalent values of intangible (non-market) benefits from WSUD projects. Development of the tools and their supporting materials has involved extensive consultation with intended users, review of existing tools, and primary research over eight years (2013–2020). The frameworks can be applied to business-case development and decision making at multiple levels in public-sector and private-sector organisations.
               ",unknown,995
10.1016/j.jenvman.2022.114560,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85123258110,scopus,2022-04-01,scopus,real-time image-based air quality estimation by deep learning neural networks,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123258110&origin=inward,"
                  Air quality profoundly impacts public health and environmental equity. Efficient and inexpensive air quality monitoring instruments could be greatly beneficial for human health and air pollution control. This study proposes an image-based deep learning model (CNN−RC) that integrates a convolutional neural network (CNN) and a regression classifier (RC) to estimate air quality at areas of interest through feature extraction from photos and feature classification into air quality levels. The models were trained and tested on datasets with different combinations of the current image, the baseline image, and HSV (hue, saturation, value) statistics for increasing model reliability and estimation accuracy. A total of 3549 hourly air quality datasets (including photos, PM2.5, PM10, and the air quality index (AQI)) collected at the Linyuan air quality monitoring station of Kaohsiung City in Taiwan constituted the case study. The main breakthrough of this study is to timely produce an accurate image-based estimation of several pollutants simultaneously by using only one single deep learning model. The test results show that estimation accuracy in terms of R2 for PM2.5, PM10, and AQI based on daytime (nighttime) images reaches 76% (83%), 84% (84%), and 76% (74%), respectively, which demonstrates the great capability of our method. The proposed model offers a promising solution for rapid and reliable multi-pollutant estimation and classification based solely on captured images. This readily scalable measurement approach could address major gaps between air quality data acquired from expensive instruments worldwide.
               ",unknown,996
10.1016/j.meegid.2022.105218,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85123253111,scopus,2022-03-01,scopus,analyzing the covid-19 vaccination behavior based on epidemic model with awareness-information,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123253111&origin=inward,"Background The widespread use of effective COVID-19 vaccines could prevent substantial morbidity and mortality. Individual decision behavior about whether or not to be vaccinated plays an important role in achieving adequate vaccination coverage and herd immunity. Methods This research proposes a new susceptible–vaccinated–exposed–infected–recovered with awareness-information (SEIR/V-AI) model to study the interaction between vaccination and information dissemination. Information creation rate and information sensitivity are introduced to understand the individual decision behavior of COVID-19 vaccination. We then analyze the dynamical evolution of the system and validate the analysis by numerical simulation. Results The decision behavior of COVID-19 vaccination in China and the United States are analyzed. The results showed the coefficient of information creation and the information sensitivity affect vaccination behavior of individuals. Conclusions The information-driven vaccination is an effective way to curb the COVID-19 spreading. Besides, to solve vaccine hesitancy and free-ride, the government needs to disseminate accurate information about vaccines safety to alleviate public concerns, and provide the widespread public educational campaigns and communication to guide individuals to act in group interests rather than self-interest and reduce the temptation to free-riding, which often results from individuals who are inadequately informed about vaccines and thus blindly imitate free-riding behavior.",unknown,1007
10.1016/j.envres.2021.112059,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85115315890,scopus,2022-03-01,scopus,using bayesian networks for environmental health risk assessment,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115315890&origin=inward,"
                  The study investigated the potential relationships between air pollution, socio-economy, and proven pathologies (e.g., respiratory, cardiovascular) within an industrial area in Southern France (Etang de Berre), gathering steel industries, oil refineries, shipping, road traffic and experiencing a Mediterranean climate. A total of 178 variables were simultaneously integrated within a Bayesian model at intra-urban scale. Various unsupervised and supervised algorithms (maximum spanning tree, tree-augmented naive classifier) as well as sensitivity analyses were used to better understand the links between all variables, and highlighted correlations between population exposure to air pollutants and some pathologies. Adverse health effects (bronchus and lung cancers for 15–65 years old people) were observed for hydrofluoric acid at low background concentration (<0.003 μg m−3) while exposure to particulate cadmium (0.210–0.250 μg m−3) disrupts insulin metabolism for people over 65 years-old leading to diabetes. Bronchus and lung cancers for people over 65 years-old occurred at low background SO2 concentration (6 μg m−3) below European limit values. When benzo[k]fluoranthene exceeded 0.672 μg m−3, we observed a high number of hospital admissions for respiratory diseases for 15-65 years-old people. The study also revealed the important influence of socio-economy (e.g., single-parent family, people with no qualification at 15 years-old) on pathologies (e.g., cardiovascular diseases). Finally, a diffuse polychlorinated biphenyl (PCB) pollution was observed in the study area and can potentially cause lung cancers.
               ",unknown,1010
10.1016/j.scitotenv.2021.151204,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85118573391,scopus,2022-02-01,scopus,development and application of an automated air quality forecasting system based on machine learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118573391&origin=inward,"
                  As one of the most concerned issues in modern society, air quality has received extensive attentions from the public and the government, which promotes the continuous development and progress of air quality forecasting technology. In this study, an automated air quality forecasting system based on machine learning has been developed and applied for daily forecasts of six common pollutants (PM2.5, PM10, SO2, NO2, O3, and CO) and pollution levels, which can automatically find the best “Model + Hyperparameters” without human intervention. Five machine learning models and an ensemble model (Stacked Generalization) were integrated into the system, supported by a knowledge base containing the meteorological observed data, pollutant concentrations, pollutant emissions, and model reanalysis data. Then five-year data (2015–2019) of Beijing, Shanghai, Guangzhou, Chengdu, Xi'an, Wuhan, and Changchun in China, were used as an application case to study the effectiveness of the automated forecasting system. Based on the analysis of seven evaluation criteria and pollution level forecasts, combined with the forecasting results for the next 3-days, it is found that the automated system can achieve satisfactory forecasting performance, better than most of numerical model results. This implied that the developed system unveils a good application prospect in the field of environmental meteorology.
               ",unknown,1021
10.1016/j.ifacol.2022.09.465,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85144532025,scopus,2022-01-01,scopus,demand forecasting for delivery platforms by using neural network,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144532025&origin=inward,"
                  This paper deals with the tricky issue of forecasting the number of daily orders received by a delivery company that operates through the internet. The research tries to address the problem through the Multilayer Perceptron Neural Network (MLP). The more important step of the methodology is the identification and characterization of the features to adopt as inputs for the MLP in the cited case. The number of visits (NVs) to the company website, months, days of the week and the public holidays are the four features used to predict the number of received orders (NOs). The Levenberg Marquardt back-propagation algorithm was used to train the model. The proposed methodology was applied by a delivery company, which operates in Italy, to forecast the daily demand. The results showed a good accuracy of the MLP in predicting the NOs, with a Root Mean Squared Error of the 20% from the actual NOs.
               ",unknown,1054
10.1016/b978-0-323-85159-6.50294-3,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85136290671,scopus,2022-01-01,scopus,analysing different dynamically modelled data structures and machine learning algorithms to predict pm2.5 concentration in china,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136290671&origin=inward,"
                  Harmful air pollutant such as PM2.5 is still a major concern in many countries. At high concentrations, it could lead to adverse health effect on human, escalating the risk of cardiovascular and respiratory diseases. In order to mitigate this issue, continuous air quality monitoring systems have been deployed to alert the general public of high PM2.5 level. However, such monitoring system requires substantial budget and resources to construct, thus may not be accessible in some regions especially developing countries. Therefore, it is important to develop a high performance PM2.5 prediction model that only employs easily attainable input parameters as a more cost-effective alternative. In this study, common meteorological data from five different cities in China were utilized for the PM2.5 prediction model. Dynamic model such as the nonlinear autoregressive network with exogenous inputs (NARX) with different input/output time lag were applied to transform training dataset into different data structures. Additionally, machine learning algorithms were analysed and evaluated to predict PM2.5, namely: multi-linear regression (MLR), and feed-forward artificial neural network (FANN). The results shows that FANN model with 10 hidden neurons using NARX-2 data structure is the best model combination with an R2 values of up to 0.973.
               ",unknown,1093
10.1016/b978-0-323-95879-0.50259-9,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85135417586,scopus,2022-01-01,scopus,bayesian neural network-based calibration for urban air quality sensors,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135417586&origin=inward,"
                  
                     With the growing urbanization process in several cities around the world, air pollution mitigation has become one of the main environmental challenges of the present time. Recently, low-cost air pollution sensors have become a current trend in the air quality control area since they are an affordable alternative for deploying air quality monitoring systems with high spatial resolution. However, the main drawback of these sensors is that they tend to provide measurements with lower accuracy and reliability compared to traditional air quality monitoring stations. Therefore, periodical calibration of these sensors is essential to maintain the quality of their measurements. This work presents a novel air quality sensor calibration method based on a Bayesian neural network model. The proposed method is assessed using a real public available dataset. The test experiment results show that the method has a good accuracy performance, with a lower mean absolute error compared to other machine learning-based calibration methods applied to the same dataset. In addition, the method presents the advantage of directly providing estimations of the uncertainty of the calibrated measurements, which is an important metric used to assess the quality and reliability of data provided by air pollution sensors and that most other calibration methods usually cannot provide.
               ",unknown,1098
10.1016/j.jpubtr.2022.100009,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85128309614,scopus,2022-01-01,scopus,using artificial intelligence to derive a public transit risk index,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128309614&origin=inward,"A terrorist attack on the public transportation system of a city can cripple its economy. Uninformed investments in countermeasures may result in a waste of resources if the risk is negligible. However, risks are difficult to quantify in an objective manner because of uncertainties, speculations, and subjective assumptions. This study contributes a probabilistic model, validated by ten different machine learning methods applied to the fusion of six heterogeneous datasets, to objectively quantify risks at different jurisdictional scales. The risk index is purposefully simple to quickly inform a proportional prioritization of resources to make fair investment decisions that stakeholders can easily understand, and to guide policy formulation. The main finding is that the risk indices among public transit jurisdictions in the United States distribute normally. This result enables agencies to evaluate the quality of their risk index calculations by detecting an outlier or a large deviation from the expected value.",unknown,1121
10.1016/j.procs.2022.01.308,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85127772806,scopus,2022-01-01,scopus,human-centered artificial intelligence for the public sector: the gate keeping role of the public procurement professional,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127772806&origin=inward,"The increasing deployment of artificial intelligence (AI) powered solutions for the public sector is hoped to change how developing countries deliver services in key sectors such as agriculture, healthcare, education, and social sectors. And yet AI has a high potential for abuse and creates risks, which if not managed and monitored will jeopardize respect and dignity of the most vulnerable in society. In this study, we argue for delineating public procurements’ role in the human-centred AI (HCAI) discourses, focusing on the developing countries. The study is based on an exploratory inquiry and gathered data among procurement practitioners in Uganda and Kenya, which have similar country procurement regimes: where traditional forms of competition in procurement apply compared to more recent pre-commercial procurement mechanisms that suit AI procurement. We found limited customization in AI technologies, a lack of developed governance frameworks, and little knowledge and distinction between AI procurement and other typical technology procurement processes. We proposed a framework, which in absence of good legal frameworks can allow procurement professionals to embed HCAI principles in AI procurement processes.",unknown,1122
10.1016/j.dsx.2021.102367,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85122481849,scopus,2022-01-01,scopus,twitter sentiment analysis from iran about covid 19 vaccine,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122481849&origin=inward,"
                  Background and aims
                  The development of vaccines against COVID-19 has been a global purpose since the World Health Organization declared the pandemic. People usually use social media, especially Twitter, to transfer knowledge and beliefs on global concerns like COVID-19-vaccination, hence, Twitter is a good source for investigating public opinions. The present study aimed to assess Persian tweets to (1) analyze Iranian people's view toward COVID-19 vaccination. (2) Compare Iranian views toward a homegrown and imported COVID-19-vaccines.
               
                  Methods
                  First, a total of 803278 Persian tweets were retrieved from Twitter, mentioning COVIran Barekat (the homegrown vaccine), Pfizer/BioNTech, AstraZeneca/Oxford, Moderna, and Sinopharm (imported vaccines) between April 1, 2021 and September 30, 2021. Then, we identified sentiments of retrieved tweets using a deep learning sentiment analysis model based on CNN-LSTM architecture. Finally, we investigated Iranian views toward COVID-19-vaccination.
               
                  Results
                  (1) We found a subtle difference in the number of positive sentiments toward the homegrown and foreign vaccines, and the latter had the dominant positive polarity. (2) The negative sentiment regarding homegrown and imported vaccines seems to be increasing in some months. (3) We also observed no significant differences between the percentage of overall positive and negative opinions toward vaccination amongst Iranian people.
               
                  Conclusions
                  It is worrisome that the negative sentiment toward homegrown and imported vaccines increases in Iran in some months. Since public healthcare agencies aim to increase the uptake of COVID-19 vaccines to end the pandemic, they can focus on social media such as Twitter to promote positive messaging and decrease opposing views.
               ",unknown,1131
10.1016/j.ijmedinf.2021.104624,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85118338040,scopus,2022-01-01,scopus,responding to covid-19 with real-time general practice data in australia,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118338040&origin=inward,"
                  Introduction
                  As SARS-CoV-2 spread around the world, Australia was no exception. Part of the Australian response was a robust primary care approach, involving changes to care models (including telehealth) and the widespread use of data to inform the changes. This paper outlines how a large primary care database responded to provide real-time data to inform policy and practice. Simply extracting the data is not sufficient. Understanding the data is. The POpulation Level Analysis and Reporting (POLAR) program is designed to use GP data for multiple objectives and is built on a pre-existing engagement framework established over a fifteen-year period. Initially developed to provide QA activities for general practices and population level data for General Practice support organisations, the POLAR platform has demonstrated the critical ability to design and deploy real-time data analytics solutions during the COVID-19 pandemic for a variety of stakeholders including state and federal government agencies.
               
                  Methods
                  The system extracts and processes data from over 1,300 general practices daily. Data is de-identified at the point of collection and encrypted before transfer. Data cleaning for analysis uses a variety of techniques, including Natural Language Processing and coding of free text information. The curated dataset is then distilled into several analytic solutions designed to address specific areas of investigation of interest to various stakeholders. One such analytic solution was a model we created that used multiple data inputs to rank patient geographic areas by the likelihood of a COVID-19 outbreak. The model utilised pathology ordering, COVID-19 related diagnoses, indication of COVID-19 related concern (via progress notes) and also incorporated state based actual confirmed case figures.
               
                  Results
                  Using the methods described, we were able to deliver real-time data feeds to practices, Primary Health Networks (PHN) and other agencies. In addition, we developed a COVID-19 geographic risk stratification based on local government areas (LGAs) to pro-actively inform the primary care response. Providing PHNs with a list of geographic priority hotspots allowed for better targeting and response of Personal Protective Equipment allocation and pop-up clinic placement.
               
                  Conclusions
                  The program summarised here demonstrates the ability of a well-designed system underpinned by accurate and reliable data, to respond in real-time to a rapidly evolving public health emergency in a way which supports and enhances the health system response.
               ",unknown,1136
10.1016/j.envpol.2021.118159,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85115010456,scopus,2021-12-15,scopus,"estimating monthly pm<inf>2.5</inf> concentrations from satellite remote sensing data, meteorological variables, and land use data using ensemble statistical modeling and a random forest approach",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115010456&origin=inward,"
                  Fine particulate matter (PM2.5) is associated with various adverse health outcomes and poses serious concerns for public health. However, ground monitoring stations for PM2.5 measurements are mostly installed in population-dense or urban areas. Thus, satellite retrieved aerosol optical depth (AOD) data, which provide spatial and temporal surrogates of exposure, have become an important tool for PM2.5 estimates in a study area. In this study, we used AOD estimates of surface PM2.5 together with meteorological and land use variables to estimate monthly PM2.5 concentrations at a spatial resolution of 3 km2 over Taiwan Island from 2015 to 2019. An ensemble two-stage estimation procedure was proposed, with a generalized additive model (GAM) for temporal-trend removal in the first stage and a random forest model used to assess residual spatiotemporal variations in the second stage. We obtained a model-fitting R2 of 0.98 with a root mean square error (RMSE) of 1.40 
                        
                           μ
                           g
                           /
                           
                              m
                              3
                           
                        
                     . The leave-one-out cross-validation (LOOCV) R2 with seasonal stratification was 0.82, and the RMSE was 3.85 
                        
                           μ
                           g
                           /
                           
                              m
                              3
                           
                        
                     , whereas the R2 and RMSE obtained by using the pure random forest approach produced R2 and RMSE values of 0.74 and 4.60 
                        
                           μ
                           g
                           /
                           
                              m
                              3
                           
                        
                     , respectively. The results indicated that the ensemble modeling approach had a higher predictive ability than the pure machine learning method and could provide reliable PM2.5 estimates over the entire island, which has complex terrain in terms of land use and topography.
               ",unknown,1144
10.1016/j.retrec.2021.101161,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85123863234,scopus,2021-12-01,scopus,assessing consumer preferences for plug-in hybrid electric vehicle (phev): an indian perspective,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123863234&origin=inward,"
                  This paper reports an investigation to assess consumer preferences towards Plug-in Hybrid Electric Vehicle (PHEV) in a typical Indian context. A stated preference survey was designed to collect choice responses from current owners of conventional vehicles (CVs) in two Indian megacities namely Delhi and Kolkata. The data collected from the two cities were analyzed in two stages by developing Mixed Logit (ML) models. In the first stage, consumers’ perceived benefit towards PHEV-related attributes were estimated in terms of Willingness to Pay (WTP) values. In the second stage, the potential demand shift from CV to PHEV was estimated by developing demand models. The results indicate the need for an added emphasis on PHEV-specific attributes such as charging time and battery warranty and attribute for general vehicle use such as advance vehicle technology (AVT) option by vehicle manufacturers to make PHEV attractive to consumers in Indian megacities. High purchase price is identified as a major bottleneck for the penetration of PHEV in Indian market. Policy interventions from government in terms of higher subsidy, availability of public charging infrastructure, and environmental education and awareness among consumers are necessary to effectively promote PHEV in Indian context.
               ",unknown,1147
10.1016/j.jss.2021.111088,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85115889980,scopus,2021-12-01,scopus,open data ecosystems — an empirical investigation into an emerging industry collaboration concept,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115889980&origin=inward,"Software systems are increasingly depending on data, particularly with the rising use of machine learning, and developers are looking for new sources of data. Open Data Ecosystems (ODE) is an emerging concept for data sharing under public licenses in software ecosystems, similar to Open Source Software (OSS). It has certain similarities to Open Government Data (OGD), where public agencies share data for innovation and transparency. We aimed to explore open data ecosystems involving commercial actors. Thus, we organized five focus groups with 27 practitioners from 22 companies, public organizations, and research institutes. Based on the outcomes, we surveyed three cases of emerging ODE practice to further understand the concepts and to validate the initial findings. The main outcome is an initial conceptual model of ODEs’ value, intrinsics, governance, and evolution, and propositions for practice and further research. We found that ODE must be value driven. Regarding the intrinsics of data, we found their type, meta-data, and legal frameworks influential for their openness. We also found the characteristics of ecosystem initiation, organization, data acquisition and openness be differentiating, which we advise research and practice to take into consideration.",unknown,1151
10.1016/j.ijinfomgt.2021.102401,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85112007685,scopus,2021-12-01,scopus,public and private value creation using artificial intelligence: an empirical study of ai voice robot users in chinese public sector,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112007685&origin=inward,"
                  Despite significant theoretical and empirical attention on public value creation in the public sector, the relationship between artificial intelligence (AI) use and value creation from the citizen perspective remains poorly understood. We ground our study in Moore’s public value management to examine the relationship between AI use and value creation. We conceptually categorize public service value into public value and private value. We use procedural justice and trust in government as indicators of public value and, based on motivation theory, we use perceived usefulness and perceived enjoyment as indicators of private value. A field survey of 492 AI voice robot users in China was conducted to test our model. The results indicated that the effective use of AI voice robots was significantly associated with private value and procedural justice. However, the relationship between the effective use of AI and trust in government was not found to be significant. Surprisingly, the respondents indicated that private value had a greater effect on overall value creation than public value. This contrasts with the common idea that value creation from the government perspective suggests that social objectives requiring public value are more important to citizens. The results also show that gender and citizens with different experiences show different AI usage behaviors.
               ",unknown,1153
10.1016/j.jmse.2020.10.001,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85100680656,scopus,2021-12-01,scopus,a combined machine learning algorithms and dea method for measuring and predicting the efficiency of chinese manufacturing listed companies,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100680656&origin=inward,"Data Envelopment Analysis (DEA) is a linear programming methodology for measuring the efficiency of Decision Making Units (DMUs) to improve organizational performance in the private and public sectors. However, if a new DMU needs to be known its efficiency score, the DEA analysis would have to be re-conducted, especially nowadays, datasets from many fields have been growing rapidly in the real world, which will need a huge amount of computation. Following the previous studies, this paper aims to establish a linkage between the DEA method and machine learning (ML) algorithms, and proposes an alternative way that combines DEA with ML (ML-DEA) algorithms to measure and predict the DEA efficiency of DMUs. Four ML-DEA algorithms are discussed, namely DEA-CCR model combined with back-propagation neural network (BPNN-DEA), with genetic algorithm (GA) integrated with back-propagation neural network (GANN-DEA), with support vector machines (SVM-DEA), and with improved support vector machines (ISVM-DEA), respectively. To illustrate the applicability of above models, the performance of Chinese manufacturing listed companies in 2016 is measured, predicted and compared with the DEA efficiency scores obtained by the DEA-CCR model. The empirical results show that the average accuracy of the predicted efficiency of DMUs is about 94%, and the comprehensive performance order of four ML-DEA algorithms ranked from good to poor is GANN-DEA, BPNN-DEA, ISVM-DEA, and SVM-DEA.",unknown,1155
10.1016/j.envpol.2021.117859,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85111512529,scopus,2021-11-15,scopus,spatio-temporal modeling of pm<inf>2.5</inf> risk mapping using three machine learning algorithms,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111512529&origin=inward,"
                  Urban air pollution is one of the most critical issues that affect the environment, community health, economy, and management of urban areas. From a public health perspective, PM2.5 is one of the primary air pollutants, especially in Tehran's metropolis. Owing to the different patterns of PM2.5 in different seasons, Spatio-temporal modeling and identification of high-risk areas to reduce its effects seems necessary. The purpose of this study was Spatio-temporal modeling and preparation of PM2.5 risk mapping using three machine learning algorithms (random forest (RF), AdaBoost, and stochastic gradient descent (SGD)) in the metropolis of Tehran, Iran. Therefore, in the first step, to prepare the dependent variable data, the PM2.5 average was used for the four seasons of spring, summer, autumn, and winter. Then, using remote sensing (RS) and a geographic information system (GIS), independent data such as temperature, maximum temperature, minimum temperature, wind speed, rainfall, humidity, normalized difference vegetation index (NDVI), population density, street density, and distance to industrial centers were prepared as a seasonal average. To Spatio-temporal modeling using machine learning algorithms, 70% of the data were used for training and 30% for validation. The frequency ratio (FR) model was used as input to machine learning algorithms to calculate the spatial relationship between PM2.5 and the effective parameters. Finally, Spatio-temporal modeling and PM2.5 risk mapping were performed using three machine learning algorithms. The receiver operating characteristic (ROC) area under the curve (AUC) results showed that the RF algorithm had the greatest modeling accuracy, with values of 0.926, 0.94, 0.949, and 0.949 for spring, summer, autumn, and winter, respectively. According to the RF model, the most important variable in spring and autumn was NDVI. Temperature and distance to industrial centers were the most important variables in the summer and winter, respectively. The results showed that autumn, winter, summer, and spring had the highest risk of PM2.5, respectively.
               ",unknown,1156
10.1016/j.scitotenv.2021.148272,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85109686672,scopus,2021-10-10,scopus,"a scenario-based approach for urban water management in the context of the covid-19 pandemic and a case study for the tabriz metropolitan area, iran",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109686672&origin=inward,"
                  The world's poorest countries were hit hardest by COVID-19 due to their limited capacities to combat the pandemic. The urban water supply and water consumption are affected by the pandemic because it intensified the existing deficits in the urban water supply and sanitation services. In this study, we develop an integrated spatial analysis approach to investigate the impacts of COVID-19 on multi-dimensional Urban Water Consumption Patterns (UWCPs) with the aim of forecasting the water demand. We selected the Tabriz metropolitan area as a case study area and applied an integrated approach of GIS spatial analysis and regression-based autocorrelation assessment to develop the UWCPs for 2018, 2019 and 2020. We then employed GIS-based multi-criteria decision analysis and a CA-Markov model to analyze the water demand under the impacts of COVID-19 and to forecast the UWCPs for 2021, 2022 and 2023. In addition, we tested the spatial uncertainty of the prediction maps using the Dempster Shafer Theory. The results show that the domestic water consumption increased by 17.57% during the year 2020 as a result of the COVID-19 pandemic. The maximum increase in water consumption was observed in spring 2020 (April–June) when strict quarantine regulations were in place. Based on our results, the annual water deficit in Tabriz has increased from ~18% to about 30% in 2020. In addition, our projections show that this may further increase to about 40–45% in 2021. Relevant stakeholders can use the findings to develop evidence-informed strategies for sustainable water resource management in the post-COVID era. This research also makes other significant contributions. From the environmental perspective, since COVID-19 has affected resource management in many parts of the world, the proposed method can be applied to similar contexts to mitigate the adverse impacts and developed better informed recovery plans.
               ",unknown,1166
10.1016/j.trd.2021.102954,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85111652017,scopus,2021-09-01,scopus,flow-based unit is better: exploring factors affecting mid-term od demand of station-based one-way electric carsharing,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111652017&origin=inward,"
                  Station-based one-way carsharing (OWC) serves as a flexible method to enjoy the benefits of car travel, while also demonstrating the potential to mitigate environmental challenges and traffic congestion in cities. In the previous studies on OWC, location-based units such as a single station, station cluster, and land parcel were generally used as the basic analysis units for demand; however, these studies failed to consider the association among the operating areas. This led to results that were pertinent to a specific OWC system and cross-section of the development process. The objective of this study is to explore the significant factors related to flow-based demand (i.e., four-weekly bookings from the origin spatial unit to the destination spatial unit (OD bookings)). Four groups of explanatory variables are adopted: carsharing spatial unit attributes, built environment, transportation facilities, and OD trip attributes (such as public transportation and car travel distance between the OD). A combination model integrating machine learning and a generalized linear model is also developed to address the zero-inflation issue of the data. Moreover, an approach of Shapley additive explanations is implemented to determine the considerable effects of the factors. The results show that (1) the OD trip attributes play an important role in estimating the carsharing OD demand; (2) taxi and carsharing demands exhibit a non-significant partial overlap, and carsharing may compete with buses and supplement to the metro; and (3) the travel purpose in carsharing is diverse for any land use over a four-week period.
               ",unknown,1178
10.1016/j.adhoc.2021.102590,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85108629598,scopus,2021-09-01,scopus,newly engineered energy-based features for supervised anomaly detection in a physical model of a water supply system,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108629598&origin=inward,"
                  Industrial Control Systems (ICS) are hardware, network, and software, upon which a facility depends to allow daily operations to function. In most cases society takes the operation of such systems, for example public transport, tap water or electricity, for granted. However, the disruption of those systems might have serious consequences across different sectors. In this paper, we propose a supervised energy-based approach for anomaly detection in a clean water supply system using a new dataset which is physically modelled in the Festo MPA workstation rig. The novelty relies on the set of engineered features collected from the testbed, including voltage, current and power from the sensors that compose the ICS. These values are obtained from independent current sensors that we have physically wired to the testbed. Five machine learning algorithms; Support Vector Machine, k-Nearest Neighbours, Multilayer Perceptron, Decision Tree and Random Forest are employed to evaluate the effectiveness of our proposed features. The metrics used to present the performance of the selected machine learning algorithms are F1-Score, G-Mean, False Positive Rate (FPR) and False Negative Rate (FNR). The results show that machine learning algorithms can classify the variations of energy produced by the execution of cyber-attacks as anomalous by achieving 95.5% F1-Score, and 6.8% FNR with the Multilayer Perceptron classifier.
               ",unknown,1179
10.1016/j.scs.2021.103079,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85107758518,scopus,2021-09-01,scopus,a sustainable ioht based computationally intelligent healthcare monitoring system for lung cancer risk detection,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107758518&origin=inward,"A sustainable healthcare focuses on enhancing and restoring public health parameters thereby reducing gloomy impacts on social, economic and environmental elements of a sustainable city. Though it has uplifted public health, yet the rise of chronic diseases is a concern in sustainable cities. In this work, a sustainable lung cancer detection model is developed to integrate the Internet of Health Things (IoHT) and computational intelligence, causing the least harm to the environment. IoHT unit retains connectivity continuously generates data from patients. Heuristic Greedy Best First Search (GBFS) algorithm is used to select most relevant attributes of lung cancer data upon which random forest algorithm is applied to classify and differentiates lung cancer affected patients from normal ones based on detected symptoms. It is observed during the experiment that the GBFS-Random forest model shows a promising outcome. While an optimal accuracy of 98.8 % was generated, simultaneously, the least latency of 1.16 s was noted. Specificity and sensitivity recorded with the proposed model on lung cancer data are 97.5 % and 97.8 %, respectively. The mean accuracy, specificity, sensitivity, and f-score value recorded is 96.96 %, 96.26 %, 96.34 %, and 96.32 %, respectively, over various types of cancer datasets implemented. The developed smart and intelligent model is sustainable. It reduces unnecessary manual overheads, safe, preserves resources and human resources, and assists medical professionals in quick and reliable decision making on lung cancer diagnosis.",unknown,1180
10.1016/j.eswa.2021.114951,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85104365062,scopus,2021-09-01,scopus,providing music service in ambient intelligence: experiments with gym users,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104365062&origin=inward,"
                  Ambient Intelligence (AmI) is an interdisciplinary research area of ICT which has evolved since the 90s, taking great advantage from the advent of the Internet of Things (IoT). AmI creates, by using Artificial Intelligence (AI), an intelligent ecosystem in which computers, sensors, lighting, music, personal devices, and distributed services, work together to improve the user experience through the support of natural and intuitive user interfaces. Nowadays, AmI is used in various contexts, e.g., for building smart homes and smart cities, providing healthcare, and creating an adequate atmosphere in retail and public environments.
                  In this paper, we propose a novel AmI system for gym environments, named Gym Intelligence, able to provide adequate music atmosphere, according to the users’ physical effort during the training. The music is taken from Spotify and is classified according to some music features, as provided by Spotify itself. The system is based on a multi-agent computational intelligence model built on two main components: 
                        
                           (
                           i
                           )
                        
                      machine learning methods that forecast appropriate values for the Spotify music features, and 
                        
                           (
                           ii
                           )
                        
                      a multi-objective dynamic genetic algorithm that selects a specific Spotify music track, according to such values. Gym Intelligence is built by sensing the ambient with a minimal, low-cost, and non-intrusive set of sensors, and it has been designed considering the outcome of a preliminary analysis in real gyms, involving real users. We have considered well-known regression methods and we have validated them using a collected data 
                        
                           (
                           i
                           )
                        
                      about the users’ physical effort, through the sensors, and 
                        
                           (
                           ii
                           )
                        
                      about the users’ music preferences, through an Android app that the users have used during the training. Among the regression methods considered, the one that provided the best results is the Random Forest, which predicted Spotify music features with a mean absolute error of 0.02 and a root mean squared error of 0.05. We have implemented Gym Intelligence and deployed it in five real gyms. We have evaluated it conducting several experiments. The experiments show how, with the help of Gym Intelligence, the users’ satisfaction about the provided background music, rose from 3.05 to 4.91 (on a scale from 1 to 5, where 5 is the maximum score).
               ",unknown,1181
10.1016/j.scitotenv.2021.146908,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85104112817,scopus,2021-08-20,scopus,bisphenol a leaching from epoxy resins in the drinking water distribution networks as human health risk determinant,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104112817&origin=inward,"
                  Monitoring and management of drinking water distribution networks (DWDNs), including possible leaching from materials in contact with drinking water, have been stressed as crucial to avoid re-contamination of drinking water leading to a potential increase of human health risk. Recent scientific studies and regulations clearly highlighted the leaching of bisphenol A (BPA) from plastic materials used to renovate DWDNs pipelines as one of the major hazardous source, resulting in severe consequences for human health. In this study, lab migration tests were performed on three commercial epoxy resins, designed with the Design of Experiments (DoE) method in order to build a BPA migration model as a function of water chemical stability, evaluated as aggressivity index (AI), and residual chlorine concentration. Tests lasted about 170 days to account for both short and long-term leaching. BPA migration over time was well described by a combination of two 1st-order kinetic models with an initial peak of leaching, a decrease and, then, a second increase due to resins' deterioration. Initial BPA concentration in the contact water and BPA integral migration over time showed inverse proportionality with both chlorine concentration and AI values. However, measurements of free BPA content in epoxy resins proved that this is due to BPA transformation, not to a reduced leaching. The validated BPA migration model was combined with the hydraulic model of the DWDN in an urban area, through EPANET-MSX software. The model allowed to simulate the propagation of BPA in the DWDN, after the execution of a relining intervention, identifying the most vulnerable areas and permitting to customize a site-specific monitoring and intervention plan to minimize the health risk for final consumers.
               ",unknown,1184
10.1016/j.isprsjprs.2021.05.016,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85109646118,scopus,2021-08-01,scopus,urban road mapping based on an end-to-end road vectorization mapping network framework,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109646118&origin=inward,"
                  Reliable urban road vector maps are essential for urban analysis because the spatial distribution of road networks reflects urban development under the combined effects of nature and socio-economics. Diverse very high resolution (VHR) remote sensing images are now available, enabling explicit extraction of urban road vector maps over wide areas. Urban road vectorization mapping consists of two separate tasks: road extraction and road vectorization. The traditional methods focus on the road extraction task, and can obtain a good performance when using a pixel-based metric. However, the road vectorization methods are faced with the problem of road connectivity. In this work, to implement urban road vectorization mapping in a unified way, an end-to-end road vectorization mapping network (RVMNet) framework is proposed. The proposed RVMNet framework consists of a node proposal network (NPN) module and a node connectivity based road refinement module. In the NPN module, a fully convolutional network is adopted for the road node extraction. This improves the connectivity of the road mask by providing supervised information in the form of the road nodes, which are actually part of the road mask. The road mask is then converted into a road vector map by vectorization. In the node connectivity based road refinement module, road nodes are inserted into the road vector map to improve the connectivity. We compared RVMNet with the other state-of-the-art road detection methods on two public road datasets (SpaceNet 3 and DeepGlobe). The results of this comparison showed that combining road extraction and road vectorization into a unified framework is an efficient and accurate strategy for urban road vectorization mapping because it can propose road nodes that help to improve the road connectivity. Moreover, we constructed the novel UrbanRoadNet dataset, covering six cities (Beijing city center; Helsinki; Wuhan; Macao; the Wan Chai area of Hong Kong; and Shanghai). We then applied the RVMNet framework to the data from the six cities, obtaining an improvement in the vector-based average path length similarity (APLS) value of 4.1%. The spatial transfer assessments from both the qualitative and quantitative aspects corroborated the robust generalizability of the proposed method, and further verified the effectiveness of the proposed approach for large-scale road vectorization mapping at a very high resolution. It was also found that road vector spatial distribution is a useful way to reflect urban development.
               ",unknown,1193
10.1016/j.buildenv.2021.107928,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85107744104,scopus,2021-08-01,scopus,ten questions concerning active noise control in the built environment,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107744104&origin=inward,"Urban noise pollution is an omnipresent but often neglected threat to public health that must be addressed urgently. Passive noise control measures, which are less effective at reducing low-frequency noise and are often bulky and may impede airflow. As evidenced in automobiles, active control of cabin noise has resulted in lighter cars due to reduced passive insulation. Despite its long history and recent popularisation by consumer headphones, the implementation of active noise control in the built environment is still rare. To date, active noise control (ANC) has been demonstrated, at source, in construction machines and, in the transmission path, in noise barriers. Recent demand for naturally-ventilated buildings has also spurred the development of active control solutions at the receiving end, such as on windows. The ten questions aim to demystify the principles of ANC and highlight areas in which environmental noise can be actively mitigated. Since the implementation of active control in the built environment usually involves multiple stakeholders, operational concerns are addressed. To conclude, research gaps are identified that would enable increased adoption of ANC in the built environment. There is also renewed interest in applying intelligent ANC to tackle environmentally complex applications, such as varying noise levels in the earcup of ANC headphones, particularly with the advent of the low-cost, low-power, highly-efficient embedded electronics; advancing speaker technology; and new impetus from digital signal processing and artificial intelligence Algorithms.",unknown,1194
10.1016/j.jenvman.2021.112584,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85105836019,scopus,2021-08-01,scopus,predictive modelling as a tool for effective municipal waste management policy at different territorial levels,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105836019&origin=inward,"
                  Nowadays, the European municipal waste management policy based on the circular economy paradigm demands the closing of material and financial loops at all territorial levels of public administration. The effective planning of treatment capacities (especially sorting plants, recycling, and energy recovery facilities) and municipal waste management policy requires an accurate prognosis of municipal waste generation, and therefore, the knowledge of behavioral, socio-economic, and demographic factors influencing the waste management (and recycling) behavior of households, and other municipal waste producers. To enable public bodies at different territorial levels to undertake an effective action resulting in circular economy we evaluated various factors influencing the generation of municipal waste fractions at regional, micro-regional and municipal level in the Czech Republic. Principal components were used as input for traditional models (multivariable linear regression, generalized linear model) as well as tree-based machine learning models (regression trees, random forest, gradient boosted regression trees). Study results suggest that the linear regression model usually offers a good trade-off between model accuracy and interpretability. When the most important goal of the prediction is supposed to be accuracy, the random forest is generally the best choice. The quality of developed models depends mostly on the chosen territorial level and municipal waste fraction. The performance of these models deteriorates significantly for lower territorial levels because of worse data quality and bigger variability. Only the age structure seems to be important across territorial levels and municipal waste fractions. Nevertheless, also other factors are of high significance in explaining the generation of municipal waste fractions at different territorial levels (e.g. number of economic subjects, expenditures, population density and the level of education). Therefore, there is not one single effective public policy dealing with circular economy strategy that fits all territorial levels. Public representatives should focus on policies effective at specific territorial level. However, performance of the models is poor for lower territorial levels (municipality and micro-regions). Thus, results for municipalities and micro-regions are weak and should be treated as such.
               ",unknown,1195
10.1016/j.buildenv.2021.107883,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85104726607,scopus,2021-07-01,scopus,"multi-source sensor based urban habitat and resident health sensing: a case study of wuhan, china",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104726607&origin=inward,"
                  The COVID-19 pandemic undoubtedly has a great impact on the world economy, especially the urban economy. It is urgent to study the environmental pathogenic factors and transmission route of it. We want to discuss the relationship between the urban living environment and the number of confirmed cases at the community scale, and examine the driving forces of community infection (e.g., environment, ecology, convenience, livability, and population density). Besides, we hope that our research will help make our cities more inclusive, safe, resilient, and sustainable. 650 communities with confirmed COVID-19 cases in Wuhan were selected as the research objects. We utilize deep learning semantic segmentation technology to calculate the Visible Green Index (VGI) and Sky View Factor (SVF) of street view and use Partial Least Squares Structural Equation Modeling (PLS-SEM) to study the driving forces of pandemic situation. Temperature and humidity information recorded by sensors was also used for urban sensing. We find that the more SVF has a certain inhibitory effect on the virus transmission, but contrary to our intuitive perception, higher VGI has a certain promotion effect. Also, the structural equation model constructed in this paper can explain the variance of 28.9% of the number of confirmed cases, and results (path coef.) demonstrate that residential density of community (0.517) is a major influencing factor for pandemic cases, whereas convenience of community living (0.234) strongly influence it. Communities with good suitability of community human settlement (e.g., construction time, price) are safer in the face of pandemic events. Does the influence of SVF and VGI on the results of the pandemic situation mean that sunlight can effectively block the spread of the virus? This spatial heterogeneity in different communities is helpful for us to explore the environmental transmission route of COVID-19.
               ",unknown,1204
10.1016/j.techfore.2021.120783,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85104062956,scopus,2021-07-01,scopus,adoption of artificial intelligence-integrated crm systems in agile organizations in india,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104062956&origin=inward,"
                  Artificial intelligence integrated with customer relationship management (CRM) systems has revolutionized organizations’ means of analyzing their huge volumes of customer data. To effectively respond to and manage the opportunities and challenges that arise from this, organizations are developing competencies and processes that evolve their agility, fine-tuning them to the artificial intelligence customer service system (AICS) and wider digitalization setting. In this context, this study identifies the factors impacting the adoption of an AI-integrated CRM system (AICS) in agile organizations as a part of their digitalization strategy. Methodologically, the research builds its theoretical foundation on extant works to develop hypotheses and a corresponding conceptual model. The model is quantitatively validated through a survey across the spectrum of Indian companies, following expert-based pretesting and pilot testing, and subsequently it is statistically tested using the partial least squares structural equation modeling (PLS-SEM) technique. The results, contextualized against the backdrop of organizational agility, identify and elucidate the relationship between stakeholders and perceived value and easiness of AICS, between employee trust and attitude, and the influence of attitude and behavioral intention as key mediators towards AICS adoption. The findings are conclusively transcribed into tangible implications for practice and explicit avenues for future research.
               ",unknown,1205
10.1016/j.heliyon.2021.e07129,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85107069210,scopus,2021-05-01,scopus,evaluation of irrigation suitability potential of brewery effluent post treated in a pilot horizontal subsurface flow constructed wetland system: implications for sustainable urban agriculture,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107069210&origin=inward,"The use of untreated or partially treated wastewater reuse for urban and peri-urban agricultural irrigation is a common practice in developing countries like Ethiopia. Such practices, however, pose significant environmental and public health risks. The objective of this study was to evaluate the irrigation suitability of anaerobic digestion brewery effluent (ADBE) and two-stage horizontal subsurface constructed wetland post-treated ADBE (CWPBE). A series of pot experiments were conducted in a plastic - greenhouse system arranged in three sets of irrigation schemes: Treatment Group1 (TG1): municipal pipe tap water (MPTW) irrigated pots; Treatment Group2 (TG2): ADBE irrigated pots, and Treatment Group3 (TG3): CWPBE irrigated pots. Pots packed with the same amount of sandy clay loam soil and local tomato seeds sown were irrigated following an updated tomato irrigation schedule derived from the FAO CROPWAT stimulation model for 120 days. The findings from key irrigation water quality parameters showed that the CWPBE achieved the prescribed irrigation water standards with values of pH (7.4 ± 0.15), electrical conductivity (1.9 ± 0.11 dS.m−1), total suspended solids (25 ± 4.17 mgL-1), chemical oxygen demand (185.1 ± 1.66 mgL-1), total nitrogen (17.4 ± 0.7 mgL-1), total phosphorous (8.8 ± 0.26 mgkg-1), calcium (10.5 ± 3.6 mgkg-1), magnesium (4.9 ± 0.98 mgkg-1), sodium (4.4 ± 1.51 mgkg-1), potassium (2.3 ± 1.15 mgkg-1), sodium adsorption ratio (1.6 ± 0.34), and total coliform (8 ± 0.16×10−5 CFU/100 mL). Moreover, tomato plants grown in TG3 attained higher growth such as number of leaves (85.6 ± 4.68), plant height (92.2 ± 1.29 cm), stem diameter (13.1 ± 2.35 cm) and leaf area (35.5 ± 1.03 cm2) as well as higher biomass (61.2 ± 1.33 kgm−2) and fruit (46.4 ± 3.51 kgm−2) yields over other treatment groups. The results revealed that irrigation waters significantly improved both growth and yield parameters of tomato plants with the ascending order of TG1 < TG2 < TG3. Moreover, CWPBE showed minima short-term residual effect on soil physicochemical properties as compared to ADBE, and thus, it has potential suitability for agricultural irrigation reuse.",unknown,1226
10.1016/j.mimet.2021.106186,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85103706448,scopus,2021-05-01,scopus,"large-scale comparison of e. coli levels determined by culture and a qpcr method (epa draft method c) in michigan towards the implementation of rapid, multi-site beach testing",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103706448&origin=inward,"
                  Fecal pollution remains a challenge for water quality managers at Great Lakes and inland recreational beaches. The fecal indicator of choice at these beaches is typically Escherichia coli (E. coli), determined by culture-based methods that require over 18 h to obtain results. Researchers at the United States Environmental Protection Agency (EPA) have developed a rapid E. coli qPCR methodology (EPA Draft Method C) that can provide same-day results for improving public health protection with demonstrated sensitivity, specificity, and data acceptance criteria. However, limited information is currently available to compare the occurrence of E. coli determined by cultivation and by EPA Draft Method C (Method C). This study provides a large-scale data collection effort to compare the occurrence of E. coli determined by these alternative methods at more than 100 Michigan recreational beach and other sites using the complete set of quantitative data pairings and selected subsets of the data and sites meeting various eligibility requirements. Simple linear regression analyses of composite (pooled) data indicated a correlation between results of the E. coli monitoring approaches for each of the multi-site datasets as evidenced by Pearson R-squared values ranging from 0.452 to 0.641. Theoretical Method C threshold values, expressed as mean log10 target gene copies per reaction, that corresponded to an established E. coli culture method water quality standard of 300 MPN or CFU /100 mL varied only from 1.817 to 1.908 for the different datasets using this model. Different modeling and derivation approaches that incorporated within and between-site variability in the estimates also gave Method C threshold values in this range but only when relatively well-correlated datasets were used to minimize the error. A hypothetical exercise to evaluate the frequency of water impairments based on theoretical qPCR thresholds corresponding to the E. coli water quality standard for culture methods suggested that the methods may provide the same beach notification outcomes over 90% of the time with Method C results differing from culture method results that indicated acceptable and unacceptable water quality at overall rates of 1.9% and 6.6%, respectively. Results from this study provide useful information about the relationships between E. coli determined by culture and qPCR methods across many diverse freshwater sites and should facilitate efforts to implement qPCR-based E. coli detection for rapid recreational water quality monitoring on a large scale in the State of Michigan.
               ",unknown,1229
10.1016/j.jag.2020.102294,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85101876344,scopus,2021-05-01,scopus,large-scale crop type and crop area mapping across brazil using synthetic aperture radar and optical imagery,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101876344&origin=inward,"Improved data on crop type and crop area from satellite imagery are invaluable for agronomy managers and are crucial for balancing agricultural expansion and forest degradation. However, large-scale maps of crop type and crop area using satellite imagery are not easily available in some regions, especially Brazil. Reasons for this include limited ground truth data, inadequate spatial and temporal satellite data availability, computational challenges, lack of cropland data and field boundaries. In this paper, we attempted to overcome some of these obstacles by using an ensemble of approaches to generate crop classification maps for Brazil. In order to compensate for the lack of abundant ground truth data in Brazil, we combined extensive field data and satellite input features from the United States with available field data and satellite input features from Brazil to train crop classification model for Brazil. Before applying the crop classification model for Brazil, we classified cropland areas using harmonic functions and delineated field boundaries using a supervised deep learning approach. Cropland masking and field boundary delineation allowed field-level mapping of crop type and crop area. Applying the crop classification model for Brazil in the states of Mato Grosso and Goias gave a true positive accuracy of 88% in the 2017/2018 summer growing season for soybean classification, 95% in the 2018 safrinha growing season for corn classification, and 86% in the 2018/2019 summer growing season for soybean classification. Our crop area estimates also showed a good agreement (correlation of 0.95 and mean absolute error of 0.64) with state-scale statistical data provided by the Companhia Nacional de Abastecimento (CONAB) in both summer and safrinha growing seasons adding further confidence to the results. These results suggest that extensive data from one geography can be used to train machine learning models in conjunction with limited field data from another geography. Accuracy assessments support the portability of crop classification model for Brazil with reasonable accuracy spatially, as tested in the state of Parana, and temporally, to the following year. The approaches and datasets presented in this paper provide building blocks for large-scale crop monitoring benefitting both public and private sectors.",unknown,1230
10.1016/j.ynstr.2021.100297,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85099915098,scopus,2021-05-01,scopus,forecasting individual risk for long-term posttraumatic stress disorder in emergency medical settings using biomedical data: a machine learning multicenter cohort study,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099915098&origin=inward,"The necessary requirement of a traumatic event preceding the development of Posttraumatic Stress Disorder, theoretically allows for administering preventive and early interventions in the early aftermath of such events. Machine learning models including biomedical data to forecast PTSD outcome after trauma are highly promising for detection of individuals most in need of such interventions. In the current study, machine learning was applied on biomedical data collected within 48 h post-trauma to forecast individual risk for long-term PTSD, using a multinominal approach including the full spectrum of common PTSD symptom courses within one prognostic model for the first time. N = 417 patients (37.2% females; mean age 46.09 ± 15.88) admitted with (suspected) serious injury to two urban Academic Level-1 Trauma Centers were included. Routinely collected biomedical information (endocrine measures, vital signs, pharmacotherapy, demographics, injury and trauma characteristics) upon ED admission and subsequent 48 h was used. Cross-validated multi-nominal classification of longitudinal self-reported symptom severity (IES-R) over 12 months and bimodal classification of clinician-rated PTSD diagnosis (CAPS-IV) at 12 months post-trauma was performed using extreme Gradient Boosting and evaluated on hold-out sets. SHapley Additive exPlanations (SHAP) values were used to explain the derived models in human-interpretable form. Good prediction of longitudinal PTSD symptom trajectories (multiclass AUC = 0.89) and clinician-rated PTSD at 12 months (AUC = 0.89) was achieved. Most relevant prognostic variables to forecast both multinominal and dichotomous PTSD outcomes included acute endocrine and psychophysiological measures and hospital-prescribed pharmacotherapy. Thus, individual risk for long-term PTSD was accurately forecasted from biomedical information routinely collected within 48 h post-trauma. These results facilitate future targeted preventive interventions by enabling future early risk detection and provide further insights into the complex etiology of PTSD.",unknown,1231
10.1016/j.apgeog.2021.102442,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85103982388,scopus,2021-04-01,scopus,modeling fine-scale residential land price distribution: an experimental study using open data and machine learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103982388&origin=inward,"
                  Modeling the fine-scale spatiotemporal distribution of residential land prices (RLPs) is the basis for scientifically allocating land resources, managing the residential market and improving urban planning. The accurate mapping of the RLP dynamics require reliable land price prediction models and data with fine spatial and temporal resolution. With the aid of point of interest (POI) data and nighttime light (NTL) images, this paper attempts to explore the ability of machine learning algorithms (MLAs) to model grid-level RLPs using the case of Wuhan in China. Several land price prediction models were built using five MLAs and various geographic variables. The experimental results show that the extra-trees regression algorithm and the radial basis function-based support vector regression algorithm perform best in Period Ⅰ (2010–2014) and Period Ⅱ (2015–2019), respectively; therefore, they were selected to estimate the RLPs of the grids without observations in the corresponding period. Based on the estimated results, we found that the spatial pattern of the RLP in Wuhan transitioned from monocentric to polycentric between the two periods, and RLPs grew rapidly near newly formed urban subcenters and waterscapes. The relative importance of the predictor variables shows that commercial and educational facilities are important determinants of the RLP distribution in Wuhan; moreover, the relative importance of natural amenities and education facilities increased over time, while that of commercial facilities and public transportation decreased slightly. The case of Wuhan confirms the feasibility of MLAs and openly accessible urban data in modeling fine-scale RLP distributions. Our proposed framework provides a new approach to monitor the urban land price dynamics accurately and closely, which is beneficial for improving the infrastructure layout and achieve smart city growth.
               ",unknown,1241
10.1016/j.scs.2021.102720,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85099881239,scopus,2021-04-01,scopus,a deep learning approach for prediction of air quality index in a metropolitan city,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099881239&origin=inward,"
                  In India, the Central and State Pollution Control Boards have commissioned the National Air Monitoring Program (NAMP) which covers 240 cities with 342 monitoring stations. Air Quality Index (AQI) has been categorized into different groups. To predict the AQI in Chennai city, the Dataset was collected, then preprocessed to replace missing values and remove redundant data. The mean, mean square error and standard deviation are extracted using the Grey Level Co-occurrence Matrix (GLCM). The combination of Support Vector Regression (SVR) and Long Short-Term Memory (LSTM) based deep learning model is used to classify the AQI values. The proposed deep learning model gives an accurate and specific value for AQI on the city’s specified location compared to the existing techniques. The prediction accuracy is improved in the proposed deep learning method, which will caution the public to reduce to an acceptable level. The deep learning mechanism predicts the AQI values accurately and helps to plan the metropolitan city for sustainable development. The expected AQI value can control the pollution level by incorporating road traffic signal coordination, encouraging the people to use public transportation, and planting more trees on some locations.
               ",unknown,1242
10.1016/j.compenvurbsys.2020.101563,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85100379626,scopus,2021-03-01,scopus,desirable streets: using deviations in pedestrian trajectories to measure the value of the built environment,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100379626&origin=inward,"
                  The experience of walking through a city is influenced by amenities and the visual qualities of its built environment. This paper uses thousands of pedestrian trajectories obtained from GPS signals to construct a desirability index for streets in Boston. We create the index by comparing the actual paths taken by pedestrians with the shortest path between any origin-destination pairs. The index captures pedestrians' willingness to deviate from their shortest path and provides a measure of the scenic and experience value provided by different parts of the city. We then use computer vision techniques combined with georeferenced data to measure the built environment of streets. We show that desirable streets have better access to public amenities such as parks, sidewalks, and urban furniture. They are also sinuous, visually enclosed, have less complex facades, and have more diverse business establishments. These results further our understanding of the value that the built environment brings to pedestrians, enhancing our capacity to design more lively and functional environments.
               ",unknown,1247
10.1016/j.foodpol.2020.101985,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85092658958,scopus,2021-02-01,scopus,predicting access to healthful food retailers with machine learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092658958&origin=inward,"
                  Many U.S. households lack access to healthful food and rely on inexpensive, processed food with low nutritional value. Surveying access to healthful food is costly and finding the factors that affect access remains convoluted owing to the multidimensional nature of socioeconomic variables. We utilize machine learning with census tract data to predict the modified Retail Food Environment Index (mRFEI), which refers to the percentage of healthful food retailers in a tract and agnostically extract the features of no access—corresponding to a “food desert” and low access—corresponding to a “food swamp.” Our model detects food deserts and food swamps with a prediction accuracy of 72% out of the sample. We find that food deserts and food swamps are intrinsically different and require separate policy attention. Food deserts are lightly populated rural tracts with low ethnic diversity, whereas swamps are predominantly small, densely populated, urban tracts, with more non-white residents who lack vehicle access. Overall access to healthful food retailers is mainly explained by population density, presence of black population, property value, and income. We also show that our model can be used to obtain sensible predictions of access to healthful food retailers for any U.S. census tract.
               ",unknown,1257
10.1016/j.procs.2021.05.012,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85112716370,scopus,2021-01-01,scopus,"supporting mobility planning in small cities and communities with low-power, machine learning based sensing",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112716370&origin=inward,"Cities and communities around the country have very limited resources to invest in projects that go beyond supporting their traditional operational needs. Unfortunately, it is not uncommon to encounter cases where these limited resources are employed in large ""smart city"" projects that have little or no impact on the social good of the community. On the other hand, the work in this article considers specific needs and financial limitations of suburban or rural communities in the mobility sector. This article presents the work in the design, test and performance evaluation of a low-cost, machine learning based, solution for distributed monitoring of the use of public transit in small communities. The proposed solution utilizes a low-resolution infrared array, that provides non-personally identifiable information coupled with a low-power deep learning module. We also compare the machine-based solution to a traditional algorithmic approach. We believe the solution allows small communities, with limited resources, to collect near real-time data that can be employed for evidence-based decisions of coverage and frequency in public transit. The successful collection of this kind of mobility data is vital for small communities as this is needed to support grant applications that are increasingly commonly used to improve public transit or other mobility modes.",unknown,1298
10.1016/j.csbj.2021.04.004,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85104426510,scopus,2021-01-01,scopus,k-seir-sim: a simple customized software for simulating the spread of infectious diseases,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104426510&origin=inward,"Infectious disease is a great enemy of humankind. The ravages of COVID-19 are leading to profound crises across the world. There is an urgent requirement for analyzing the current pandemic situation, predicting trends over time, and assessing the effectiveness of containment measures. Thus, numerous statistical models, primarily based on the susceptible–exposed–infected–recovered or removed (SEIR) model, have been established. However, these models are highly technical, which are difficult for the public and governing bodies to understand and use. To address this issue, we developed a simple operating software based on our improved K-SEIR model termed as the kernel SEIR simulator (K-SEIR-Sim). This software includes natural propagation parameters, containment measure parameters, and certain characteristic parameters that can deduce the effects of natural propagation and containment measures. Further, the applicability of the proposed software was demonstrated using the examples of the COVID-19 outbreak in the United States, Wuhan city of China, Diamond Princess, and France. Operating results verified the potency of the proposed software in evaluating the epidemic situation and human intervention during COVID-19. Importantly, no installation is required and the software can perform real-time, backward-looking, and forward-looking analysis by functioning in data-driven and model-driven ways. All of them have considerable practical values in their applications according to the actual needs of personal use. Conclusively, K-SEIR-Sim is the first simple customized operating software that is highly valuable for the global fight against COVID-19 and other infectious diseases.",unknown,1318
10.1016/j.compind.2020.103347,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85096841392,scopus,2021-01-01,scopus,portuguese word embeddings for the oil and gas industry: development and evaluation,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096841392&origin=inward,"
                  Over the last decades, oil and gas companies have been facing a continuous increase of data collected in unstructured textual format. New disruptive technologies, such as natural language processing and machine learning, present an unprecedented opportunity to extract a wealth of valuable information within these documents. Word embedding models are one of the most fundamental units of natural language processing, enabling machine learning algorithms to achieve great generalization capabilities by providing meaningful representations of words, being able to capture syntactic and semantic features based on their context. However, the oil and gas domain-specific vocabulary represents a challenge to those algorithms, in which words may assume a completely different meaning from a common understanding. The Brazilian pre-salt is an important exploratory frontier for the oil and gas industry, with increasing attractiveness for international investments in exploration and production projects, and most of its documentation is in Portuguese. Moreover, Portuguese is one of the largest languages in terms of number of native speakers. Nonetheless, despite the importance of the petroleum sector of Portuguese speaking countries, specialized public corpora in this domain are scarce. This work proposes PetroVec, a representative set of word embedding models for the specific domain of oil and gas in Portuguese. We gathered an extensive collection of domain-related documents from leading institutions to build a large specialized oil and gas corpus in Portuguese, comprising more than 85 million tokens. To provide an intrinsic evaluation, assessing how well the models can encode domain semantics from the text, we created a semantic relatedness test set, comprising 1,500 word pairs labeled by selected experts in geoscience and petroleum engineering from both academia and industry. In addition, we performed an extrinsic quantitative evaluation on a downstream task of named entity recognition in geoscience, plus a set of qualitative analyses, and conducted a comparative evaluation against a public general-domain embedding model. The obtained results suggest that our domain-specific models outperformed the general model on their ability to represent specialized terminology. To the best of our knowledge, this is the first attempt to generate and evaluate word embedding models for the oil and gas domain in Portuguese. Finally, all the resources developed by this work are made available for public use, including the pre-trained specialized models, corpora, and validation datasets.
               ",unknown,1333
10.1016/j.compenvurbsys.2020.101562,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85094835215,scopus,2021-01-01,scopus,predicting and mapping neighborhood-scale health outcomes: a machine learning approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094835215&origin=inward,"
                  Estimating health outcomes at a neighborhood scale is important for promoting urban health, yet costly and time-consuming. In this paper, we present a machine-learning-enabled approach to predicting the prevalence of six common non-communicable chronic diseases at the census tract level. We apply our approach to the City of Austin and show that our method can yield fairly accurate predictions. In searching for the best predictive models, we experiment with eight different machine learning algorithms and 60 predictor variables that characterize the social environment, the physical environment, and the aspects and degrees of neighborhood disorder. Our analysis suggests that (a) the sociodemographic and socioeconomic variables are the strongest predictors for tract-level health outcomes and (b) the historical records of 311 service requests can be a useful complementary data source as the information distilled from the 311 data often helps improve the models' performance. The machine learning models yielded from this study can help the public and city officials evaluate future scenarios and understand how changes in the neighborhood conditions can lead to changes in the health outcomes. By analyzing where the most significant discrepancies between the predicted and the actual values are, we will also be ready to identify areas of best practice and areas in need of greater investment or policy intervention.
               ",unknown,1335
10.1016/j.scs.2020.102530,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85092171686,scopus,2021-01-01,scopus,social media semantic perceptions on madrid metro system: using twitter data to link complaints to space,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092171686&origin=inward,"
                  Social networks are platforms widely used by travelers who express their opinions about many services like public transport. This paper investigates the value of texts from social networks as a data source for detecting the spatial distribution of problems within a public transit network by geolocating citizens' feelings, and analyzes the effects some factors such as population or income have over that spatial spread, with the goal of developing a more intelligent and sustainable public transit service. For that purpose, Twitter data from the Madrid Metro account is collected over a two-month period. Topics and sentiments are identified from text mining and machine learning algorithms, and mapped to explore spatial and temporal patterns. Lastly, a Geographically Weighted Regression model is used to explore the causality of the spatial distribution of complaining users, by using official data sources as exploratory variables. Results show Twitter users tend to be mid-income workers who reside in peripheral areas and mainly tweet when traveling to workplaces. The main detected problems were punctuality and breakdowns in transfer stations or in central areas, mainly in the early morning of weekdays, and affected by density of points of interest in destination areas.
               ",unknown,1336
10.1016/j.jtrangeo.2020.102889,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85092900144,scopus,2020-12-01,scopus,generating a spatial coverage plan for the emergency medical service on a regional scale: empirical versus random forest modelling approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092900144&origin=inward,"
                  Adequate spatial coverage by the emergency medical service and the ability to reach any location in the area of interest in the shortest possible time are crucial for the survival of patients with serious conditions. Knowledge of blind spots (i.e. sites that cannot be reached within the required time) represents key information for improving the service quality and may lead, e.g. to a relocation of bases or to other active interventions. Spatial coverage can be derived from experience based on historical data. Such an approach may be problematic if a larger area is being analysed, especially if data is not available for some parts of such areas or if no data is available. To eliminate such problems, we created a prediction model utilising the random forest ensemble learning method. The model is capable of predicting the travel time based on available historical data on ambulance movements (GPS) and the geometric and construction characteristics of individual road segments. We therefore collaborated with the regional public administration and emergency medical service authorities to deliver a time- and resource-efficient solution for emergency spatial planning practice. The outputs from the newly built model were subsequently validated against data from an empirical model currently used by the regional authorities. The results from both models were compared from the perspective of performance in various seasonal and time-of-day conditions. The prediction of travel times using the new model improved according to all the evaluated validation metrics. The importance and applicability of the foregoing model lies in the fact that it can be incorporated into the current emergency medical service management system in a simple manner in terms of data availability and the required computational resources. We conclude that the dynamic model presented in this paper represents an improvement relative to the reference data, and discuss the possibilities of further improving the proposed model.
               ",unknown,1349
10.1016/j.jval.2020.06.013,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85092011431,scopus,2020-12-01,scopus,prioritizing additional data collection to reduce decision uncertainty in the hiv/aids response in 6 us cities: a value of information analysis,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092011431&origin=inward,"Objectives The ambitious goals of the US Ending the HIV Epidemic initiative will require a targeted, context-specific public health response. Model-based economic evaluation provides useful guidance for decision making while characterizing decision uncertainty. We aim to quantify the value of eliminating uncertainty about different parameters in selecting combination implementation strategies to reduce the public health burden of HIV/AIDS in 6 US cities and identify future data collection priorities. Methods We used a dynamic compartmental HIV transmission model developed for 6 US cities to evaluate the cost-effectiveness of a range of combination implementation strategies. Using a metamodeling approach with nonparametric and deep learning methods, we calculated the expected value of perfect information, representing the maximum value of further research to eliminate decision uncertainty, and the expected value of partial perfect information for key groups of parameters that would be collected together in practice. Results The population expected value of perfect information ranged from $59 683 (Miami) to $54 108 679 (Los Angeles). The rank ordering of expected value of partial perfect information on key groups of parameters were largely consistent across cities and highest for parameters pertaining to HIV risk behaviors, probability of HIV transmission, health service engagement, HIV-related mortality, health utility weights, and healthcare costs. Los Angeles was an exception, where parameters on retention in pre-exposure prophylaxis ranked highest in contributing to decision uncertainty. Conclusions Funding additional data collection on HIV/AIDS may be warranted in Baltimore, Los Angeles, and New York City. Value of information analysis should be embedded into decision-making processes on funding future research and public health intervention.",unknown,1350
10.1016/j.invent.2020.100349,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85090963552,scopus,2020-12-01,scopus,developing and evaluating the effectiveness of mobile phone-based career intervention for career competencies of malaysian public managers: protocol for a mixed method study,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090963552&origin=inward,"Background This study presents a protocol for a mixed method study on the development and evaluation of the effectiveness of mobile phone-based intervention for enhancing career competencies of managers in the Malaysian public service. Career competencies, being central to the self-management and development of one's career, are necessary for proactive management of careers; they help to guide the individuals towards subjective career success which subsequently leads to organization success. However, there is a lack of career competencies intervention practices in the Malaysian public service. Therefore, this study aims to develop and evaluate the effectiveness of a user and expert centric mobile phone-based career intervention program of career competencies among Malaysian public managers. In this study, the Social Cognitive Career Theory (SCCT) is adopted as the framework to develop mobile phone-based intervention for career competencies. Method This is a sequential exploratory mixed method study which involves, in Phase 1, a qualitative approach: reviewing of literature and interviewing experts and public servants who are users of mobile phones so as to obtain their perspectives regarding the components in the career intervention module and mobile phone application features. A quantitative approach is implemented in Phase 2 that evaluates the mobile phone-based career intervention program. Validation will be conducted to meet the objectives of (i) the usability of the mobile communication career intervention application (survey), and (ii) evaluation of the effectiveness of the intervention program for career competencies delivered through mobile phone application (randomized controlled trial). The career intervention program consists of five modules through which development is based on reflective, communicative and behavioral dimensions. Outcome The primary outcome of this study is career competencies. The career competencies score will be compared between participants in the intervention and control group at the baseline, and follow-ups at week four and week 12 respectively, using mixed design ANOVA. Discussion The results from this study will indicate the effectiveness of the career intervention program for career competencies in the reformation of public services management. The intervention can serve as a new modality to enhance career competencies among public managers in Malaysia if its effectiveness is proven.",unknown,1351
10.1016/j.envint.2020.106092,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85090323111,scopus,2020-12-01,scopus,is pm<inf>1</inf> similar to pm<inf>2.5</inf>? a new insight into the association of pm<inf>1</inf> and pm<inf>2.5</inf> with children's lung function,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090323111&origin=inward,"Experimental data suggests that PM1 is more toxic than PM2.5 although the epidemiologic evidence suggests that the health associations are similar. However, few objective exposure data are available to compare the associations of PM1 and PM2.5 with children lung function. Our objectives are a) to evaluate associations between long-term exposure to PM1, PM2.5 and children’s lung function, and b) to compare the associations between PM1 and PM2.5. From 2012 to 2013, we enrolled 6,740 children (7–14 years), randomly recruited from primary and middle schools located in seven cities in northeast China. We measured lung function including forced vital capacity (FVC), forced expiratory volume in 1 s (FEV1), peak expiratory flow (PEF), and maximal mid-expiratory flow (MMEF) utilizing two portable electronic spirometers. We dichotomized continuous lung function measures according the expected values for gender and age. The spatial resolution at which PM1 and PM2.5 estimated were estimated using a machine learning method and the temporal average concentrations were averaged from 2009 to 2012. A multilevel regression model was used to estimate the associations of PM1, PM2.5 exposure and lung function measures, adjusted for confounding factors. Associations with lower lung function were consistently larger for PM1 than for PM2.5. Adjusted odds ratios (OR) per interquartile range greater PM1 ranged from 1.53 for MMEF (95% confidence interval [CI]: 1.20–1.96) to 2.14 for FEV1 (95% CI: 1.66–2.76) and ORs for PM2.5 ranged from 1.36 for MMEF (95%CI: 1.12–1.66) to 1.82 for FEV1 (95%CI: 1.49–2.22), respectively. PM1 and PM2.5 had significant associations with FVC and FEV1 in primary school children, and on PEF and MMEF in middle school children. Long-term PM1 and PM2.5 exposure can lead to decreased lung function in children, and the associations of PM1 are stronger than PM2.5. Therefore, PM1 may be more hazardous to children’s respiratory health than PM2.5 exposure.",unknown,1352
10.1016/j.suscom.2018.08.006,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85053142438,scopus,2020-12-01,scopus,socio-sentic framework for sustainable agricultural governance,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053142438&origin=inward,"
                  Livelihood security plays a critical role in strengthening the socio- economic situation of a country. Agriculture is one such sector which is expected to provide a complete array of economic, social, and environmental services. Good governance and management of allied policies at all levels is favourable for long-term sustainability of agricultural sector. The accountability of government is a direct measure of its social responsibility and sustainability. Social media as a powerful online platform reinforces hype and provides opportunities to extract and analyze public opinion about various governmental schemes and policies including the ones related to agriculture. The e-participation platforms such as Twitter offer unparallel means to intelligently gauge the consensus and orientation of people towards an agricultural policy. Motivated by this, the work presented in this research, proffers a Socio-Sentic framework for sustainable agricultural governance which probes the sentiment polarity of user-content on Twitter pertaining to government policies, specifically agricultural policies. In this intelligent analytic framework, supervised machine learning algorithms have been implemented and compared using tweets on an Indian Agricultural Policy launched in 2016, ‘Pradhan Mantri Fasal Bima Yojana’ (PMFBY). The preliminary results indicate that the adoption of the proposed framework for soliciting and probing citizen feedback for government policy evaluation can lead to a sustainable agricultural development.
               ",unknown,1356
10.1016/j.scs.2020.102365,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85087643199,scopus,2020-11-01,scopus,"data-driven framework for delineating urban population dynamic patterns: case study on xiamen island, china",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087643199&origin=inward,"
                  The effective data mining of social media has become increasingly recognized for its value in informing decision makers of public welfare. However, existing studies do not fully exploit the underlying merit of big data. In this study, we develop a data-driven framework that integrates machine learning with spatial statistics, and then use it on Xiamen Island, China to delineate urban population dynamic patterns based on hourly Baidu heat map data collected from August 25 to September 3, 2017. The results showed that hot grids are primarily clustered along the main street through the downtown area during working days, whereas cold grids are often observed at the edge of the city during the weekend. The mixed use (of commercial and life services, restaurants and snack bars, offices, leisure areas and sports complexes) is the most significant contributing factor. A new cold grid emerged near conference venues before the Brazil, Russia, India, China, and South Africa Summit, revealing the strong effects of regulations on population dynamics and its evolving patterns. This study demonstrates that the proposed data-driven framework might offer new insights into urban population dynamics and its driving mechanism in support of sustainable urban development.
               ",unknown,1365
10.1016/j.watres.2020.116229,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85089246808,scopus,2020-10-15,scopus,does land use affect pathogen presence in new zealand drinking water supplies?,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089246808&origin=inward,"
                  Four microbes (Campylobacter spp., Escherichia coli, Cryptosporidium spp. and Giardia spp.) were monitored in 16 waterways that supply public drinking water for 13 New Zealand towns and cities. Over 500 samples were collected from the abstraction point at each study site every three months between 2009 and 2019. The waterways represent a range from small to large, free flowing to reservoir impoundments, draining catchments of entirely native vegetation to those dominated by pastoral agriculture. We used machine learning algorithms to explore the relative contribution of land use, catchment geology, vegetation, topography, and water quality characteristics of the catchment to determining the abundance and/or presence of each microbe. Sites on rivers draining predominantly agricultural catchments, the Waikato River, Oroua River and Waiorohi Stream had all four microbes present, often in high numbers, throughout the sampling interval. Other sites, such as the Hutt River and Big Huia Creek in Wellington which drain catchments of native vegetation, never had pathogenic microbes detected, or unsafe levels of E. coli. Boosted Regression Tree models could predict abundances and presence/absence of all four microbes with good precision using a wide range of potential environmental predictors covering land use, geology, vegetation, topography, and nutrient concentrations. Models were more accurate for protozoa than bacteria but did not differ markedly in their ability to predict abundance or presence/absence. Environmental drivers of microbe abundance or presence/absence also differed depending on whether the microbe was protozoan or bacterial. Protozoa were more prevalent in waterways with lower water quality, higher numbers of ruminants in the catchment, and in September and December. Bacteria were more abundant with higher rainfall, saturated soils, and catchments with greater than 35% of the land in agriculture. Although modern water treatment protocols will usually remove many pathogens from drinking water, several recent outbreaks of waterborne disease due to treatment failures, have highlighted the need to manage water supplies on multiple fronts. This research has identified potential catchment level variables, and thresholds, that could be better managed to reduce the potential for pathogens to enter drinking water supplies.
               ",unknown,1368
10.1016/j.apenergy.2020.115413,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85087717811,scopus,2020-10-15,scopus,energystar++: towards more accurate and explanatory building energy benchmarking,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087717811&origin=inward,"
                  Building energy performance benchmarking has been adopted widely in the USA and Canada through the Energy Star Portfolio Manager platform. Building operations and energy management professionals have long used this simple 1–100 score to understand how their building compares to its peers. This single number is easy to use but is created by potentially inaccurate multiple linear regression (MLR) models and lacks much further information about why a building achieves that score. This paper proposes a methodology that enhances the existing Energy Star calculation method by increasing accuracy and providing additional model output processing to help explain why a building is achieving a particular score. Two new prediction models were proposed and tested: multiple linear regression with feature interactions (MLRi) and gradient boosted trees (GBT). Both models performed better than a baseline Energy Star MLR model as well as four baseline models from previous benchmarking studies. This paper shows that for six building types, on average, the third-order MLRi models achieved a 4.9% increase in adjusted 
                        
                           
                              
                                 R
                              
                              
                                 2
                              
                           
                        
                      and a 7.0% decrease in normalized root mean squared error (NRMSE) over the baseline MLR model. More substantially, the most accurate GBT models, on average, achieved a 24.9% increase in adjusted 
                        
                           
                              
                                 R
                              
                              
                                 2
                              
                           
                        
                      and a 13.7% decrease in NMRSE against the baseline MLR model. In addition, a set of techniques was developed to help determine which factors most influence a building’s energy use versus its peers using SHapley Additive exPlanation (SHAP) values. The SHAP force visualization, in particular, offered an accessible overview of the aspects of the building that influenced the score that even non-technical users can interpret. This methodology was tested on the 2012 Commercial Building Energy Consumption Survey (CBECS)(1,812 buildings) and public data sets from the energy disclosure programs of New York City (11,131 buildings) and Seattle (2,073 buildings).
               ",unknown,1369
10.1016/j.patrec.2020.07.022,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85088665727,scopus,2020-10-01,scopus,analysis of spatiotemporal influence patterns of toxic gas monitoring concentrations in an urban drainage network based on iot and gis,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088665727&origin=inward,"
                  Urban underground pipelines have complex structures, long service lives, and are susceptible to illegal interference, corrosion, and external force damage. Therefore, they are a constant security risk that seriously threaten the public security of the city. Due to the complexity of the underground environment, lack of various monitoring technologies, high cost, backwardness of emergency technology research, incongruity of safety management, and the transport of flammable, explosive, toxic, and harmful hazardous sources to densely populated areas, the boundary between industrial, residential, and living areas has become increasingly blurred, causing a major threat to public security, people's lives, industrial production, and social stability. Traditional underground pipeline accident prevention and control technology is currently unable to meet the increasing demands of public security. Combining pipeline accident prevention and control with internet of things and artificial intelligence technology can achieve urban disaster prevention, and therefore is of great interest to researchers. Herein, the research status of underground pipeline accident prevention and control technology is summarized, and an analysis of the advantages of applying big data for risk factor monitoring, risk assessment, risk early warning, and emergency decision-making technology is discussed. Further, the application difficulties and difficulties regarding big data technology in underground pipeline accident prevention and control and their potential solutions are detailed. Based on the internet of things data, spatiotemporal model mining, and Geographic Information System (GIS), we analyze the distribution and influencing factors of harmful gases in the urban underground sewage pipe network of Chongqing City, and explore the influence of smart city developments on harmful gases in the urban underground sewage pipe network.
               ",unknown,1374
10.1016/j.jocs.2020.101205,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85090220558,scopus,2020-09-01,scopus,real-time mobile sensor management framework for city-scale environmental monitoring,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090220558&origin=inward,"
                  Environmental disasters such as flash floods are becoming more and more prevalent and carry an increasing burden to human civilization. They are usually unpredictable, fast in development and extend across large geographical areas. The consequences of such disasters can be reduced through better monitoring, for example using mobile sensing platforms that can give timely and accurate information to first responders and the public. Given the extended scale of the areas to monitor, and the time-varying nature of the phenomenon, we need fast algorithms to quickly determine the best sequence of locations to be monitored. This problem is very challenging: the present informative mobile sensor routing algorithms are either short-sighted or computationally demanding when applied to large scale systems. In this paper, a real-time sensor task scheduling algorithm that suits the features and needs of city-scale environmental monitoring tasks is proposed. The algorithm is run in forward search and makes use of the predictions of an associated distributed parameter system, modeling flash flood propagation. It partly inherits the causal relation expressed by a search tree, which describes all possible sequential decisions. The computationally heavy data assimilation steps in the forward search tree are replaced by functions dependent on the covariance matrix between observation sets. Taking flood tracking in an urban area as a concrete example, numerical experiments in this paper indicate that this scheduling algorithm can achieve better results than myopic planning algorithms and other heuristics based sensor placement algorithms. Furthermore, this paper relies on a deep learning-based data-driven model to track the system states, and experiments suggest that popular estimation techniques have very good performance when applied to precise data-driven models. The data and code can be freely downloaded from https://drive.google.com/drive/folders/1gRz4T2KGFXtlnSugarfUL8r355cXb7Ko?usp=sharing.
               ",unknown,1384
10.1016/j.accinf.2020.100469,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85088931777,scopus,2020-09-01,scopus,an ontological artifact for classifying social media: text mining analysis for financial data,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088931777&origin=inward,"
                  In this paper we utilize a structured natural language processing implementation of the Financial Industry Business Ontology (FIBO) to extract financial information from the unstructured textual data of the social media platform Twitter regarding financial and budget information in the public sector, namely the two public-private agencies of the Port Authority of NY and NJ (PANYNJ), and the NY Metropolitan Transportation Agency (MTA). This research initiative uses the Design Science Research (DSR) perspective to develop an artifact to classify tweets as being either relevant to financial bonds or not. We apply a frame and slot approach from the artificial intelligence and natural language processing literature to operationalize this artifact. FIBO provides standards for defining the facts, terms, and relationships associated with financial concepts. We show that FIBO grammar can be used to mine semantic meaning from unstructured textual data and that it provides a nuanced representation of structured financial data. With this artifact, social media such as Twitter may be accessed for the knowledge that its text contains about financial concepts using the FIBO ontology. This process is anticipated to be of interest to bond issuers, regulators, analysts, investors, and academics. It may also be extended towards other financial domains such as securities, derivatives, commodities, and banking that relate to FIBO ontologies, as well as more generally to develop a structured knowledge representation of unstructured data through the application of an ontology.
               ",unknown,1385
10.1016/j.jag.2020.102066,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85081008109,scopus,2020-06-01,scopus,estimating spatio-temporal air temperature in london (uk) using machine learning and earth observation satellite data,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081008109&origin=inward,"Urbanisation generates greater population densities and an increase in anthropogenic heat generation. These factors elevate the urban–rural air temperature (Ta) difference, thus generating the Urban Heat Island (UHI) phenomenon. Ta is used in the fields of public health and epidemiology to quantify deaths attributable to heat in cities around the world: the presence of UHI can exacerbate exposure to high temperatures during summer periods, thereby increasing the risk of heat-related mortality. Measuring and monitoring the spatial patterns of Ta in urban contexts is challenging due to the lack of a good network of weather stations. This study aims to produce a parsimonious model to retrieve maximum Ta (Tmax) at high spatio-temporal resolution using Earth Observation (EO) satellite data. The novelty of this work is twofold: (i) it will produce daily estimations of Tmax for London at 1 km2 during the summertime between 2006 and 2017 using advanced statistical techniques and satellite-derived predictors, and (ii) it will investigate for the first time the predictive power of the gradient boosting algorithm to estimate Tmax for an urban area. In this work, 6 regression models were calibrated with 6 satellite products, 3 geospatial features, and 29 meteorological stations. Stepwise linear regression was applied to create 9 groups of predictors, which were trained and tested on each regression method. This study demonstrates the potential of machine learning algorithms to predict Tmax: the gradient boosting model with a group of five predictors (land surface temperature, Julian day, normalised difference vegetation index, digital elevation model, solar zenith angle) was the regression model with the best performance (R² = 0.68, MAE = 1.60 °C, and RMSE = 2.03 °C). This methodological approach is capable of being replicated in other UK cities, benefiting national heat-related mortality assessments since the data (provided by NASA and the UK Met Office) and programming languages (Python) sources are free and open. This study provides a framework to produce a high spatio-temporal resolution of Tmax, assisting public health researchers to improve the estimation of mortality attributable to high temperatures. In addition, the research contributes to practice and policy-making by enhancing the understanding of the locations where mortality rates may increase due to heat. Therefore, it enables a more informed decision-making process towards the prioritisation of actions to mitigate heat-related mortality amongst the vulnerable population.",unknown,1407
10.1016/j.scitotenv.2020.137269,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85079359317,scopus,2020-05-15,scopus,developing a composite indicator to prioritize tree planting and protection locations,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079359317&origin=inward,"
                  Trees provide numerous ecosystem services to benefit human health, and many cities have launched tree planting and management programs to increase tree populations and optimize tree locations through diverse tree priority schemes. Existing tree priority schemes are typically local-specific, expert-driven, and tree-planting-focused. In this study, a framework that captures interactions among the environment, tree and human demographic information is built. This framework provides a composite indicator, namely a tree priority planting or priority protection index (PPI), that can be integrated within a decision support system such as i-Tree Landscape to provide nationally consistent and locally relevant ways to strategically optimize tree planting and management locations across the entire United States. Three scenarios with the human health concerns are tested in a case study of New York City. The analyses are conducted at the census block group scale that is the finest-level scale available at i-Tree Landscape. The resulting PPI maps are analyzed using spatial statistics and compared against each other to investigate the impacts of alternative investments of limited public resources. The results show that: (1) tree priority patterns change greatly with alternative objectives; (2) adding more indicators to build PPIs lead to more diverse tree priority patterns as high (or low) values of different indicators are often not geographically coincident; (3) incorporating more indicators may not necessarily provide more useful information because the influences of individual indicators may be reduced and diluted by a higher level of aggregation; and (4) disaggregating PPIs may reveal corresponding contributions of individual indicators. Applying the proposed framework to build PPIs has important implications for tree priority effort, scientific exploration, education, and public engagement.
               ",unknown,1412
10.1016/j.jveb.2020.04.002,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85085039848,scopus,2020-05-01,scopus,acoustic stress in domestic dogs (canis familiaris) living around football stadiums,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085039848&origin=inward,"
                  Football matches, as a common urban activity, produce high levels of noise because of vehicular traffic, screams, whistles, and firework displays. Fear of noise is one of the most commonly reported behavior problems in dogs (Canis familiaris); however, few studies have focused on the effects of repeated exposition to noise in these animals. Here, we investigated whether the noise produced during football matches may act as a stress source for dogs living near football stadiums. We measured noise levels on days with and without matches, in the surrounding neighborhoods of two football stadiums of Minas Gerais, Brazil: (a) Arena Independência (AI) and (b) Governador Magalhães Pinto (GMP). Noise measurements were carried out by using a sound level meter, in distances varying between 0 and 400 m from the stadiums. A structured interview was conducted to dog owners living nearby AI, about the dogs' characteristics, demographics, and behavior. Noise records of days with and without matches were compared, and questionnaires were analyzed (general linear models) to identify possible correlations between the reported changes in the animals’ behaviors on days with football matches and the distance these animals lived from the stadium. Higher noise levels were recorded on football match days than on days without matches, with a greater effect of these between 0 and 249 m (D1) than between 250 and 400 from AI. Around Governador Magalhães Pinto, we recorded greater noise levels on days without football matches compared with match days, with no difference between distances. Fear/anxiety behaviors were reported as more intense on days with football matches, in comparison with days without matches for 90.6% of the study dogs. The occurrence of matches made specific fear-related behaviors of the dogs—severe trembling, excessive salivation, agitation/restlessness, whining, and appetite loss—more frequent. The proximity of AI intensified the exhibition of general fear/anxiety behaviors and made intense trembling and excessive salivation more frequent at D1, in relation to D2. On the other hand, dogs living at D2 barked, howled, and lost appetite more often than those living at D1. Our results suggested a detrimental influence of noise from football matches on the behavior of dogs living around football stadiums. These data are relevant for public policy-making to promote preventive and mitigating measures, to improve the quality of life of both the dog and the human populations in the surroundings of football stadiums.
               ",unknown,1414
10.1016/s1473-3099(19)30584-5,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85082187787,scopus,2020-04-01,scopus,"efficacy, immunogenicity, and safety of an oral influenza vaccine: a placebo-controlled and active-controlled phase 2 human challenge study",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082187787&origin=inward,"
                  Background
                  Influenza is an important public health problem and existing vaccines are not completely protective. New vaccines that protect by alternative mechanisms are needed to improve efficacy of influenza vaccines. In 2015, we did a phase 1 trial of an oral influenza vaccine, VXA-A1.1. A favourable safety profile and robust immunogenicity results in that trial supported progression of the vaccine to the current phase 2 trial. The aim of this study was to evaluate efficacy of the vaccine in a human influenza challenge model.
               
                  Methods
                  We did a single-site, placebo-controlled and active-controlled, phase 2 study at WCCT Global, Costa Mesa, CA, USA. Eligible individuals had an initial A/California/H1N1 haemagglutination inhibition titre of less than 20 and were aged 18–49 years and in good health. Individuals were randomly assigned (2:2:1) to receive a single immunisation of either 1011 infectious units of VXA-A1.1 (a monovalent tablet vaccine) orally, a full human dose of quadrivalent inactivated influenza vaccine (IIV) via intramuscular injection, or matched placebo. Randomisation was done by computer-generated assignments with block size of five. An unmasked pharmacist provided the appropriate vaccines and placebos to the administrating nurse. Individuals receiving the treatments, investigators, and staff were all masked to group assignments. 90 days after immunisation, individuals without clinically significant symptoms or signs of influenza, an oral temperature of higher than 37·9°C, a positive result for respiratory viral shedding on a Biofire test, and any investigator-assessed contraindications were challenged intranasally with 0·5 mL wild-type A/CA/like(H1N1)pdm09 influenza virus. The primary outcomes were safety, which was assessed in all immunised participants through 365 days, and influenza-positive illness after viral challenge, which was assessed in individuals that received the viral challenge and the required number of assessments post viral challenge. This trial is registered with ClinicalTrials.gov, number NCT02918006.
               
                  Results
                  Between Aug 31, 2016, and Jan 23, 2017, 374 individuals were assessed for eligibility, of whom 179 were randomly assigned to receive either VXA-A1.1 (n=71 [one individual did not provide a diary card, thus the solicited events were assessed in 70 individuals]), IIV (n=72), or placebo (n=36). Between Dec 2, 2016, and April 26, 2017, 143 eligible individuals (58 in the VXA-A1.1 group, 54 in the IIV group, and 31 in the placebo group) were challenged with influenza virus. VXA-A1.1 was well tolerated with no serious or medically significant adverse events. The most prevalent solicited adverse events for each of the treatment groups after immunisation were headache in the VXA-A1.1 (in five [7%] of 70 participants) and placebo (in seven [19%] of 36 participants) groups and tenderness at injection site in the IIV group (in 19 [26%] of 72 participants) Influenza-positive illness after challenge was detected in 17 (29%) of 58 individuals in the VXA-A1.1 group, 19 (35%) of 54 in the IIV group, and 15 (48%) of 31 in the placebo group.
               
                  Interpretation
                  Orally administered VXA-A1.1 was well tolerated and generated protective immunity against virus shedding, similar to a licensed intramuscular IIV. These results represent a major step forward in developing a safe and effective oral influenza vaccine.
               
                  Funding
                  Department of Health and Human Services, Office of the Assistant Secretary for Preparedness and Response, and Biomedical Advanced Research and Development Authority.
               ",unknown,1418
10.1016/j.watres.2019.115435,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85077660807,scopus,2020-03-15,scopus,"microbial source tracking (mst) in chattahoochee river national recreation area: seasonal and precipitation trends in mst marker concentrations, and associations with e. coli levels, pathogenic marker presence, and land use",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077660807&origin=inward,"Escherichia coli levels in recreational waters are often used to predict when fecal-associated pathogen levels are a human health risk. The reach of the Chattahoochee River that flows through the Chattahoochee River National Recreation Area (CRNRA), located in the Atlanta-metropolitan area, is a popular recreation area that frequently exceeds the U.S. Environmental Protection Agency beach action value (BAV) for E. coli. A BacteriALERT program has been implemented to provide real-time E. coli estimates in the reach and notify the public of potentially harmful levels of fecal-associated pathogens as indicated by surrogate models based on real-time turbidity measurements from continuous water quality monitoring stations. However, E. coli does not provide information about the sources of fecal contamination and its accuracy as a human health indicator is questionable when sources of contamination are non-human. The objectives of our study were to investigate, within the Park and surrounding watersheds, seasonal and precipitation-related patterns in microbial source tracking marker concentrations of possible sources (human, dog, and ruminant), assess correlations between source contamination levels and culturable E. coli levels, determine which sources best explained model-based E. coli estimates above the BAV and detection of esp2 (a marker for the esp gene associated with pathogenic strains of Enterococcus faecium and Enterococcus faecalis), and investigate associations between source contamination levels and land use features. Three BacteriALERT sites on the Chattahoochee River were sampled six times per season in the winter and summer from December 2015 through September 2017, and 11 additional stream sites (synoptic sites) from the CRNRA watershed were sampled once per season. Samples were screened with microbial source tracking (MST) quantitative PCR (qPCR) markers for humans (HF183 Taqman), dogs (DogBact), and ruminants (Rum2Bac), the esp2 qPCR marker, and culturable E. coli. At the BacteriALERT sites, HF183 Taqman concentrations were higher under wet conditions DogBact concentrations were greater in the winter and under wet conditions, and Rum2Bac concentrations were comparatively low throughout the study with no difference across seasons or precipitation conditions. Concentrations of HF183 Taqman, DogBact, and Rum2Bac were positively correlated with culturable E. coli concentrations; however, DogBact had the largest R2 value among the three markers, and the forward stepwise regression indicated it was the best predictor of culturable E. coli concentrations at the BacteriALERT sites. Recursive partitioning indicated that BAV exceedances of model-based E. coli estimates were best explained by DogBact concentrations ≥3 gene copies per mL (CN/mL). Detections of esp2 at BacteriALERT sites were best explained by DogBact concentrations ≥11 CN/mL, while detections of esp2 at synoptic sites were best explained by HF183 Taqman ≥29 CN/mL. At the synoptic sites, HF183 Taqman levels were associated with wastewater treatment plant density. However, this relationship was driven primarily by a single site, suggesting possible conveyance issues in that catchment. esp2 detections at synoptic sites were positively associated with development within a 2-km radius and negatively associated with development within the catchment, suggesting multiple sources of esp2 in the watershed. DogBact and Rum2Bac were not associated with the land use features included in our analyses. Implications for Park management include: 1) fecal contamination levels were highest during wet conditions and in the off season when fewer visitors are expected to be participating in water-based recreation, 2) dogs are likely contributors to fecal contamination in the CRNRA and may be sources of pathogenic bacteria indicating further investigation of the origins of this contamination may be warranted as would be research to understand the human health risks from exposure to dog fecal contamination, and 3) high levels of the human marker at one site in the CRNRA watershed suggests more extensive monitoring in that catchment may locate the origin of human fecal contamination detected during this study.",unknown,1423
10.1016/j.jth.2020.100821,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85078716205,scopus,2020-03-01,scopus,"walk'n'roll: mapping street-level accessibility for different mobility conditions in cuenca, ecuador",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078716205&origin=inward,"
                  Introduction
                  Approximately 66 million people with disabilities (PWD) live in Latin America and the Caribbean. Despite the important advances in regulations for social inclusion in this region, urban public spaces are not accessible to PWD. In this study, we assess street-level accessibility in the city of Cuenca (Ecuador) to examine the disparities for people with different mobility conditions.
               
                  Methods
                  We designed and implemented a mobile-based tool to audit street-level accessibility related to two key issues: the compliance level with the national standard for accessibility (NAMF) and the presence and characteristics of obstacles that hinder pedestrian mobility. A set of 214 street segments were audited by volunteers with different mobility conditions (unrestricted, restricted, or impaired). The collected data were analysed to compute accessibility indices (AIs) for each condition ranging from 0 (totally inaccessible) to 1 (totally accessible). AI values were examined to determine the differences among conditions.
               
                  Results
                  None of the 214 audited street segments fully complied with the NAMF. The AIs were: unrestricted mobility = 0.4, restricted mobility = 0.29, impaired mobility = 0.12. Eighty-three percent of the audited streets were perceived as highly inaccessible by wheelchair users, whereas for individuals with restricted or unrestricted mobility, the rates were 50% and 34%, respectively. Most streets were affected by obstacles that hindered mobility, with curb ramps and driveway ramps having the highest impact on accessibility.
               
                  Conclusion
                  The urban streets of Cuenca are highly inaccessible, mainly due to their flawed design and the presence of obstacles, creating excluding environments for people with mobility restrictions or impairments. National regulations are failing to tackle all the issues that hinder accessibility and must be revised. Cuenca, together with other intermediate cities in Latin America, must redesign their sidewalks and pedestrian facilities to allow universal accessibility.
               ",unknown,1424
10.1016/j.ssaho.2020.100060,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85112151017,scopus,2020-01-01,scopus,detection of self-reported experiences with corruption on twitter using unsupervised machine learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112151017&origin=inward,"Background Corruption is a significant challenge to the future of human development, economic progress, and population health in the post millennium. Corruption, in its different forms of bribery, fraud, waste, collusion, and illicit financial flows, not only leads to waste but can also erode trust in government and public systems. Corruption is also complex and globalized with different forms of corruption occurring across different countries and multiple industries. One critical tool to leverage in the fight against corruption is the use of innovative technologies such as machine learning. Methods In this study, we deployed an unsupervised machine learning methodology using natural language processing to collect and analyze data from the popular social media platform Twitter with the aims of detecting self-reported experiences with corruption, including in the health sector. We collected data from the Twitter public API for keywords associated with corruption and used the biterm topic model to extract themes from the entire corpus of Tweets in order to detect user-generated messages reporting or discussing experiences with corruption. Results We analyzed 22, 180, 425 tweets filtered for corruption-related keywords from January–May 2019. Using a combination of NLP and manual annotation, we detected 2383 tweets from 1556 users that included self-reporting of corruption for two dominant themes: police bribery and healthcare corruption. Overall, we found a small number of users actively reporting experiences with corruption, identified users located in countries that are perceived as having higher levels of corruption by their citizens, and found that the majority of messages included reports of users’ own experiences and/or documentation of corruption. Conclusion Though technology is not a “silver bullet” that can entirely address the multifaceted nature of global corruption, this study demonstrates its potential utility as a force for good to enable better detection, characterize forms of corruption in different sectors, and hopefully inform future anti-corruption efforts. Additionally, the UN Sustainable Development Goals, with shared goals of fighting corruption, improving population health, encouraging technology adoption, and fostering multistakeholder partnerships, may serve as a critical governance space to catalyze technology-driven anti-corruption approaches.",unknown,1429
10.1016/j.ifacol.2021.04.197,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85107879835,scopus,2020-01-01,scopus,cognitive artificial population system: framework and application,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107879835&origin=inward,"
                  Agent-based social simulation has been comprehensively applied in the research of social and ecological systems. At its core is an artificial population, which endogenously drives the system evolution for particular applications, such as urban transportation, reginal economics, analysis of infectious disease transmission, and military simulation. In contrast with the previous population simulations where simple mathematical models are used to ‘reproduce’ actual demographic features, this paper proposes a self-evolutionary digital population system, named as Cognitive Artificial Population System (CAPS). At a more fine-grained level, CAPS focuses on the agent cognitive, reasoning and learning process in their surrounding environment, thus can exploit most advantages from cognitive computing and Artificial Intelligence. As a case study, Chinese population evolution is implemented using the proposed framework. Computational experiments indicate that CAPS is able to achieve good predicted population structures for real social systems.
               ",unknown,1430
10.1016/j.glt.2020.09.004,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85102077498,scopus,2020-01-01,scopus,development of an iot based real-time traffic monitoring system for city governance,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102077498&origin=inward,"A significant amount of research work carried out on traffic management systems, but intelligent traffic monitoring is still an active research topic due to the emerging technologies such as the Internet of Things (IoT) and Artificial Intelligence (AI). The integration of these technologies will facilitate the techniques for better decision making and achieve urban growth. However, the existing traffic prediction methods mostly dedicated to highway and urban traffic management, and limited studies focused on collector roads and closed campuses. Besides, reaching out to the public, and establishing active connections to assist them in decision-making is challenging when the users are not equipped with any smart devices. This research proposes an IoT based system model to collect, process, and store real-time traffic data for such a scenario. The objective is to provide real-time traffic updates on traffic congestion and unusual traffic incidents through roadside message units and thereby improve mobility. These early-warning messages will help citizens to save their time, especially during peak hours. Also, the system broadcasts the traffic updates from the administrative authorities. A prototype is implemented to evaluate the feasibility of the model, and the results of the experiments show good accuracy in vehicle detection and a low relative error in road occupancy estimation. The study is part of the Omani-funded research project, investigating Real-Time Feedback for Adaptive Traffic Signals.",unknown,1432
10.1016/j.trpro.2020.03.145,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85084654014,scopus,2020-01-01,scopus,spatiotemporal variation of taxi demand,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084654014&origin=inward,"The growth of urban areas has made taxi service become increasingly more popular due to its ubiquity and flexibility when compared with, more rigid, public transportation modes. However, in big cities taxi service is still unbalanced, resulting in inefficiencies such as long waiting times and excessive vacant trips. This paper presents an exploratory taxi fleet service analysis and compares two forecast models aimed at predicting the spatiotemporal variation of short-term taxi demand. For this paper, we used a large sample with more than 1 million trips between 2014 and 2017, representing roughly 10% of Lisbon’s fleet. We analysed the spatiotemporal variation between pick-up and drop-off locations and how they are affected by weather conditions and points of interest. More, based on historic data, we built two models to predict the demand, ARIMA and Artificial Neural Network (ANN), and evaluated and compared the performance of both models. This study not only allows the direct comparison of a linear statistical model with a machine learning one, but also leads to a better comprehension of complex interactions surrounding different urban data sources using the taxi service as a probe to better understand urban mobility-on-demand and its needs.",unknown,1459
10.1016/j.chempr.2019.10.003,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85076014574,scopus,2019-12-12,scopus,reengineering a reversible covalent-bonding assembly to optically detect ee in β-chiral primary alcohols,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076014574&origin=inward,"The use of parallel synthesis protocols for asymmetric reaction discovery has increased the need for new methods to rapidly determine enantiomeric excess (ee) values. Most chirality sensing is performed on stereocenters that are α (i.e., proximal) to the target functional group. Finding a general approach to detect more distant point chirality would increase the substrate scope of such assays. Herein, we demonstrate a design principle to “reach out” to more distant stereocenters, in this case β-chirality in primary alcohols. Therefore, we see the design principles established in this work as a step forward in sensing distant point chirality and, eventually, multi-stereocenter relationships.",unknown,1473
10.1016/j.landurbplan.2018.08.028,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85054674618,scopus,2019-11-01,scopus,measuring daily accessed street greenery: a human-scale approach for informing better urban planning practices,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054674618&origin=inward,"
                  The public benefits of visible street greenery have been well recognised in a growing literature. Nevertheless, this issue was rare to be included into urban greenery and planning practices. As a response to this situation, we proposed an actionable approach for quantifying the daily exposure of urban residents to eye-level street greenery by integrating high resolution measurements on both greenery and accessibility. Google Street View (GSV) images in Singapore were collected and extracted through machine learning algorithms to achieve an accurate measurement on visible greenery. Street networks collected from Open Street Map (OSM) were analysed through spatial design network analysis (sDNA) to quantify the accessibility value of each street. The integration of street greenery and accessibility helps to measure greenery from a human-centred perspective, and it provides a decision-support tool for urban planners to highlight areas with prioritisation for planning interventions. Moreover, the performance between GSV-based street greenery and the urban green cover mapped by remote sensing was compared to justify the contribution of this new measurement. It suggested there was a mismatch between these two measurements, i.e., existing top-down viewpoint through satellites might not be equivalent to the benefits enjoyed by city residents. In short, this analytical approach contributes to a growing trend in integrating large, freely-available datasets with machine learning to inform planners, and it makes a step forward for urban planning practices through focusing on the human-scale measurement of accessed street greenery.
               ",unknown,1490
10.1016/j.cose.2019.06.013,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85068406007,scopus,2019-09-01,scopus,intelligent approach to build a deep neural network based ids for cloud environment using combination of machine learning algorithms,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068406007&origin=inward,"
                  The appealing features of Cloud Computing continue to fuel its adoption and its integration in many sectors such industry, governments, education and entertainment. Nevertheless, uploading sensitive data to public cloud storage services poses security risks such as integrity, availability and confidentiality to organizations. Moreover, the open and distributed (decentralized) structure of the cloud has resulted this class of computing, prone to cyber attackers and intruders. Thereby, it is imperative to develop an anomaly network intrusion system to detect and prevent both inside and outside assaults in cloud environment with high detection precision and low false warnings. In this work, we propose an intelligent approach to build automatically an efficient and effective Deep Neural Network (DNN) based anomaly Network IDS using a hybrid optimization framework (IGASAA) based on Improved Genetic Algorithm (IGA) and Simulated Annealing Algorithm (SAA). The IDS resulted is called “MLIDS” (Machine Learning based Intrusion Detection System). Genetic Algorithm (GA) is improved through optimization strategies, namely Parallel Processing and Fitness Value Hashing, which reduce execution time, convergence time and save processing power. Moreover, SAA was incorporated to IGA with the aim to optimize its heuristic search. Our approach consists of using IGASAA in order to search the optimal or near-optimal combination of most relevant values of the parameters included in construction of DNN based IDS or impacting its performance, like feature selection, data normalization, architecture of DNN, activation function, learning rate and Momentum term, which ensure high detection rate, high accuracy and low false alarm rate. For simulation and validation of the proposed method, CloudSim 4.0 simulator platform and three benchmark IDS datasets were used, namely CICIDS2017, NSL-KDD version 2015 and CIDDS-001. The implementation results of our model demonstrate its ability to detect intrusions with high detection accuracy and low false alarm rate, and indicate its superiority in comparison with state-of-the-art methods.
               ",unknown,1508
10.1016/j.compenvurbsys.2018.07.002,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85050137853,scopus,2019-09-01,scopus,soft computing model based financial aware spatiotemporal social network analysis and visualization for smart cities,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050137853&origin=inward,"
                  The era of intelligence is the development of human science and technology at a higher level, bringing a new layout for the financial market. Then, how to realize the good layout of the financial market in the era of intelligence is an important problem facing all the countries in the world. We in the financial industry as the origin and change as the clue, analyzes the physical outlets as the representative of the financial institutions, banking as standard electronic banking and mobile phone to the bank as the representative of the mobile financial development of three major formats. On this basis, we analyze the current mobile phone terminal model of mobile banking three shortcomings, and propose the new mobile financial formats. This new format is tentatively known as “smart financial format”, it has the equipment personality, wearable, low carbon environmental protection, offline interaction, security, privacy and intelligence, efficiency, and other five major characteristics. In the future, the financial format is likely to be achieved by mobile financial formats to the upgrading of intelligent financial formats, to meet the needs of customers wider and deeper levels. The artificial intelligence method has its advantages in dealing with the problems of the economic system. Therefore, the introduction of artificial intelligence methods into the economic control will become a trend. The proposed model is validated through the public databases to verify the effectiveness.
               ",unknown,1510
10.1016/j.scitotenv.2019.05.043,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85065427897,scopus,2019-08-20,scopus,modelling urban cyclists' exposure to black carbon particles using high spatiotemporal data: a statistical approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065427897&origin=inward,"
                  This is a pioneering work in South America to model the exposure of cyclists to black carbon (BC) while riding in an urban area with high spatiotemporal variability of BC concentrations. We report on mobile BC concentrations sampled on 10 biking sessions in the city of Curitiba (Brazil), during rush hours of weekdays, covering four routes and totaling 178 km. Moreover, simultaneous BC measurements were conducted within a street canyon (street and rooftop levels) and at a site located 13 km from the city center. We used two statistical approaches to model the BC concentrations: multiple linear regression (MLR) and a machine-learning technique called random forests (RF). A pool of 25 candidate variables was created, including pollution measurements, traffic characteristics, street geometry and meteorology. The aggregated mean BC concentration within 30-m buffers along the four routes was 7.09 μg m−3, with large spatial variability (5th and 95th percentiles of 1.75 and 16.83 μg m−3, respectively). On average, the concentrations at the street canyon façade (5 m height) were lower than the mobile data but higher than the urban background levels. The MLR model explained a low percentage of variance (24%), but was within the values found in the literature for on-road BC mobile data. RF explained a larger variance (54%) with the additional advantage of having lower requirements for the target and predictor variables. The most impactful predictor for both models was the traffic rate of heavy-duty vehicles. Thus, to reduce the BC exposure of cyclists and residents living close to busy streets, we emphasize the importance of renewing and/or retrofitting the diesel-powered fleet, particularly public buses with old vehicle technologies. Urban planners could also use this valuable information to project bicycle lanes with greater separation from the circulation of heavy-duty diesel vehicles.
               ",unknown,1512
10.1016/j.scs.2019.101602,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85067410268,scopus,2019-08-01,scopus,streetscape augmentation using generative adversarial networks: insights related to health and wellbeing,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067410268&origin=inward,"
                  Deep learning using neural networks has provided advances in image style transfer, merging the content of one image (e.g., a photo) with the style of another (e.g., a painting). Our research shows this concept can be extended to analyse the design of streetscapes in relation to health and wellbeing outcomes. An Australian population health survey (n = 34,000) was used to identify the spatial distribution of health and wellbeing outcomes, including general health and social capital. For each outcome, the most and least desirable locations formed two domains. Streetscape design was sampled using around 80,000 Google Street View images per domain. Generative adversarial networks translated these images from one domain to the other, preserving the main structure of the input image, but transforming the ‘style’ from locations where self-reported health was bad to locations where it was good. These translations indicate that areas in Melbourne with good general health are characterised by sufficient green space and compactness of the urban environment, whilst streetscape imagery related to high social capital contained more and wider footpaths, fewer fences and more grass. Beyond identifying relationships, the method is a first step towards computer-generated design interventions that have the potential to improve population health and wellbeing.
               ",unknown,1518
10.1016/j.dss.2019.03.010,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85064259042,scopus,2019-06-01,scopus,a decision support framework for robust r&amp;d budget allocation using machine learning and optimization,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064259042&origin=inward,"
                  Considering that government funding agencies make decisions on research and development (R&D) budget allocation to support an increasing number of research proposals, effective decision support systems are necessarily required. Motivated by the efforts of the Korean government, we propose a new decision support framework for allocating an R&D budget such that it maximizes the total expected R&D output. The proposed framework incorporates an R&D output prediction model with an optimization technique. We first employ a machine learning algorithm to accurately estimate future R&D output. Then, we apply a robust optimization technique to hedge against uncertainty in the predicted R&D output values. If not properly accounted for, this uncertainty may yield an inefficient budget allocation plan, thus hindering the operation of the R&D budgeting system. We demonstrate the effectiveness of the proposed model by applying it to a national R&D program conducted by the Korean government. Specifically, using the same budget, our budget allocation plan can achieve an output 13.6% greater than the actual R&D output. Thus, our model helps to attain allocation efficiency by systematically allocating budgets. We also observe the price of robustness when our model conservatively allocates budgets in order to hedge against uncertainty in the R&D predictions. Our findings offer insights for both policymakers and researchers related to designing better budget allocation systems for national R&D programs.
               ",unknown,1538
10.1016/j.tele.2019.01.001,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85060548072,scopus,2019-06-01,scopus,uncovering the relationship between point-of-interests-related human mobility and socioeconomic status,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060548072&origin=inward,"
                  In a city or region, understanding the relationship between human mobility and socioeconomic status is critical to public policies formulation, urban design and marketing strategies development. Based on the available massive geo-located human data, existing studies focused almost exclusively on the position attributes (i.e. coordinates) of the locations visited by people to explore the relationship, however, they ignored the category attributes (e.g. restaurant or supermarket) of these locations which imply the purposes (e.g. eating or shopping) behind human movements. A location with coordinates and category information is usually referred to as a point-of-interest (POI). In this paper, we study the relationship between POIs-related human mobility and socioeconomic status at city level. Starting from the location-based social network (i.e. Foursquare) dataset, we find that the check-in numbers of location categories are correlated with socioeconomic indicators, either positively or negatively. To further validate these correlations, we develop and test a multi-task prediction framework based on POIs-related human mobility for forecasting socioeconomic indicators. Extensive experiments on the Foursquare dataset show that the socioeconomic indicators can be well predicted by our proposed framework. Our findings and methods are helpful for modeling human mobility and assessing socioeconomic status.
               ",unknown,1539
10.1016/j.cities.2019.01.006,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85060205322,scopus,2019-06-01,scopus,a computational approach to ‘the image of the city’,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060205322&origin=inward,"In The Image of the City Lynch describes how individuals perceive and recall features in urban spaces. The most distinctive elements in the urban landscape - categorised in paths, nodes, edges, districts and landmarks - give shape to individuals' mental representation of the city. Lynch’s approach has stimulated research into spatial cognition, urban design and artificial intelligence, and it still represents an essential pillar in the analysis of urban dynamics. Nevertheless, an explicit link between The Image of the City and GIScience has not been completely explored yet. In this paper, a computational approach to The Image of the City is proposed. Different perspectives in spatial cognition and GIS research are integrated to obtain a complete Image of the City, in which the most salient elements are shared by a large part of citizens. Nodes, paths and districts were identified through network science techniques. Methods drawn from the information approach to The Image of the City are used to detect landmarks, integrating the complexity of points of reference in their visual, structural and semantic components, as conceptualised by Lynch and successive research. The methods were applied to the central area of Boston and built using freely available spatial datasets. Results were compared to Lynch’s maps to evaluate the methodology: besides a considerable discrepancy with regard to landmarks, a good correspondence for paths, nodes, edges and districts was found.",unknown,1540
10.1016/j.ajem.2018.08.048,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85052144807,scopus,2019-06-01,scopus,clinical effects of reported synthetic cannabinoid exposure in patients admitted to the intensive care unit,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052144807&origin=inward,"
                  Objective
                  To characterize the clinical presentation and hospital course of patients with reported synthetic cannabinoid (SC) exposure requiring Intensive Care Unit (ICU) admission.
               
                  Design
                  Retrospective case series of patients admitted to medical or cardiac ICU.
               
                  Setting
                  Urban tertiary care center.
               
                  Participants
                  Adults ≥18 years old admitted from the emergency department (ED) in 2015.
               
                  Measurements
                  Demographics, Sequential Organ Failure Assessment (SOFA) scores, and clinical parameters documenting the effects and hospital course.
               
                  Results
                  23 patients met inclusion criteria. Median age was 47 years (interquartile range [IQR], 32–54); 83% male; 78% black. Patients were generally tachycardic (HR > 100), (65%) and hypertensive (SBP > 140), (65%) on admission. The initial chest X-ray and ECG were abnormal in 43% and 68% of patients, respectively. Pulmonary edema and tachycardia were the most common findings. Head CT imaging was abnormal in 5% of patients. Troponin was elevated >1.0 ng/ml in 3 of 19 patients (16%). Other exposures detected on admission were marijuana (30%), alcohol (30%), and benzodiazepines (26%). The median SOFA score was 6 on admission and decreased over the next 3 days. SOFA scores were primarily driven by altered neurologic status and respiratory failure. 91% required mechanical ventilation, 30% had seizures as a part of presentation, 18% required vasopressors, and 5% needed dialysis. Median hospital and ICU lengths of stay were 2.6 (IQR 1.4–3.5) and 1.6 (IQR 0.9–2.5) days, respectively. The median hospital charge was $37,008. All patients survived the index hospitalization.
               
                  Conclusions
                  Patients admitted to ICU after SC exposure exhibit significant organ dysfunction, particularly neurologic and respiratory. Prognosis is good with supportive care.
               ",unknown,1541
10.1016/j.habitatint.2019.04.002,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85065167325,scopus,2019-05-01,scopus,"using information and communication technologies to create learning urban public space. (case study: central part of tehran, iran)",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065167325&origin=inward,"
                  This paper addressed how urban public spaces equipped with ICTs could be proper places for their users to learn. To find out the strategies of physical interventions for creating learning urban public spaces, the preferences of their users were derived by conducting interviews in the urban public spaces in the central part of Tehran. The results obtained from a deep survey with open (ended) questions suggested that the users of these public spaces were eager to learn in those spaces via ICTs and were more interested in displays than in any other types of ICTs moreover they gather at Wi-fi zones. The most important thing to learn in urban public places is knowledge, rather than skill or training, the knowledge which is related to the selected urban public spaces, their historical and cultural facts, and their management and participation issues rather than any other types. The most facilitating environmental conditions for learning are the places which have areas to seat or are equipped with seats, where users can gather and socialize easily, and which are not so crowded or empty, and the meeting points. Also, pre-experienced situations effect users’ interest in the use of a especial type of ICTs rather than the other types.
                  These findings inform urban designers, planners, policy makers and local municipality that some practical strategies used for physical interventions in these places for the purpose of creating learning public spaces would be the use of displays on the facades of buildings at different levels of pedestrian views, the installation of Wi-fi zones which have a supporting role to increase the number of learners, and the preparation of suitable seating furniture in the meeting points for long lasting learning.
               ",unknown,1545
10.1016/j.dss.2019.02.006,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85062642529,scopus,2019-04-01,scopus,an optimization-based dss for student-to-teacher assignment: classroom heterogeneity and teacher performance measures,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062642529&origin=inward,"
                  A significant amount of administrator's summer planning is spent attempting to assign hundreds of students to teachers. These assignments must satisfy federal guidelines, parent preferences, and principal preferences while pursuing classroom parity and equity in regards to academic performance, behavioral support, and demographics. In addition, teachers are often the most invested in these decisions given the impact on their day-to-day life and their future performance evaluations. This problem is a variant of a standard problem in operations research; however, administrators often lack the technical expertise or budgets to build and/or implement such models. We present an open-source spreadsheet model-based DSS to this purpose. We formulate the problem as a mixed integer program, present the implemented spreadsheet model interface, and illustrate on a set of student data. This DSS was implemented in the fall of 2018 for the assignment of close to 600 students to 24 classrooms saving dozens of staff and administration hours.
                  With minor adaptions and/or redefinitions, the DSS would provide similar value to many of the roughly 64,000 public primary and middle school principals across the country. Even more generally, the model as presented is a framework for developing and implementing an optimization-based DSS for assistance in creating project teams under a variety of potential preferences and constraints on team composition.
               ",unknown,1549
10.1016/j.landusepol.2018.12.030,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85060115067,scopus,2019-03-01,scopus,monitoring housing rental prices based on social media:an integrated approach of machine-learning algorithms and hedonic modeling to inform equitable housing policies,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060115067&origin=inward,"
                  National land use policies and strategies worldwide have attempted to establish a healthy housing rental market towards urban sustainability. Monitoring fine-scale housing rental prices should provide essential implications for equitable housing policies. However, doing so remains a challenge because aggregated data were traditionally collected at a coarse scale through census or social surveys. On-line housing rental websites (OHRWs) have become popular social media platforms in the housing studies. This paper attempts to demonstrate how to monitor fine-scale housing rental prices based on OHRWs using the case of Shenzhen in China. Employing hedonic model, a set of housing rental determinants are initially selected from three characteristics (neighborhood, location and structure) and at three levels (nearest accessibility, 15-minute walking distance availability and sub-district availability). Housing rent prediction models are then established (respectively for October 2017 and February 2018) using the training samples collected from the OHRWs and six machine-learning algorithms, including random forest regression (RFR), extra-trees regression (ETR), gradient-boosting regression (GBR), support vector regression (SVR), multi-layer perceptron neural network (MLP-NN) and k-nearest neighbor algorithm (k-NN). Thereafter, the relative importance of the determinants is calculated and visualized using partial dependence plots. Finally, the models are used to monitor housing rental price dynamics for all of the communities within Shenzhen. Results show that all of the algorithms except SVR generally present good performance. Among them, RFR and ETR are the best one in October 2017 and February 2018, respectively. Concerning the spatial pattern of housing rental, the high-high clusters merge in the central districts, whereas the low-low clusters are located in the outskirts, and the growth rate is the greatest in the farthest outskirts from the central districts. Each determinant affects the housing rent across different scale and sub-district availability and nearest accessibility are more important than 15-minute walking distance availability. The two most influential determinants are sub-district job opportunity and nearest accessibility to health care facilities. The case of Shenzhen shows that the demonstrated framework, which integrates machine-learning algorithms and the hedonic modeling, is practical and efficient. The approach is believed to provide an essential tool to inform equitable housing policies.
               ",unknown,1556
10.1016/j.procs.2019.09.294,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85076259658,scopus,2019-01-01,scopus,edge computing: smart identity wallet based architecture and user centric,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076259658&origin=inward,"The huge amount of exchanged data in the internet between entities and the quick development of edge computing has increased users frustration about the future generations. We expect a future where transactions will be based on clouds and virtual machines, gathered from sensors and IoT devices and processed by different artificial intelligence algorithms or agents. The speed and backlash change in the world has driven researchers to work on privacy and security for entities’ information and transactions. Different concepts arise and still in their early stage as digital identity, self sovereign identity, global unique identifier and identity of things (IDoT). Therefore, motivated by the recent explosion of interest around Blockchains, we examine whether they make a good fit for the Identity Internet of Things (IDoT) sector. Blockchain a major distributed peer-to-peer network where non-trusting members can interact together without a need for a trusted third party, it actually make available many advantages to providers and consumers and solve data protection features lifelong a transaction existing. As being immutable, transparent and trustful platform the Blockchain allows managing identities and privacy of its nodes information. Our contribution is a new architecture eventually using a public Blockchain and creating a smart identity wallet. It contains standard node data and will also integrate proactive data behavior and the reactive one. The main target is to protect users from sybil attack at early stage. We will define a new digital wallet based on entities behavior in order to prevent 51% attack and Sybil attack",unknown,1571
10.1016/j.procs.2019.09.101,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85076258751,scopus,2019-01-01,scopus,digital transformation process and smes,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076258751&origin=inward,"The process of digital transformation is a period in which entities, universities, public and employees are transformed and; new business models and business practices are formed. Digital transformation is a functional use of web in design, manufacturing, marketing, selling, promotion and is data-oriented management model. Therefore, capabilities of firms are increased and their processes are improved. Digital technologies are developments regarding smartphones, cloud computing, big data, artificial intelligence, robotics systems, internet of things, 3D printing, virtualization, cyber security, sensor technologies, advanced robotics systems, automation and etc. which all is intensely used in many fields of economic and social life.SME’s consist of 99,83 % total enterprises, and 72,7 % of total employment, 50,6 % of total value added and provide 55,1 % of export, take an important role in economy. They should transform their organizational structures and business making cultures starting from manufacturing technologies to management percepts as to get a productive digital transformation process. The fact that SME’s being able to perform cost-benefit analysis of digital technologies and being aware of that technologies is significant.This paper provides a comprehensive view of the factors affecting digital transformation process and describes the digital transformation of the manufacturing sector in Turkey.The programs and softwares that SMEs can use to perform digital transformation, and can benefit from it, are explained. Empirical studies on SMEs shows that SMEs have erratic behaviors in terms of information and communications technology (ICT) investment and need external support to integrate digital transformations in the overall strategy of the firm.",unknown,1572
10.1016/j.envint.2018.10.029,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85055989750,scopus,2018-12-01,scopus,"space-time trends of pm<inf>2.5</inf> constituents in the conterminous united states estimated by a machine learning approach, 2005–2015",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055989750&origin=inward,"Particulate matter with aerodynamic diameter less than 2.5 μm (PM2.5) is a complex mixture of chemical constituents emitted from various emission sources or through secondary reactions/processes; however, PM2.5 is regulated mostly based on its total mass concentration. Studies to identify the impacts on climate change, visibility degradation and public health of different PM2.5 constituents are hindered by limited ground measurements of PM2.5 constituents. In this study, national models were developed based on random forest algorithm, one of machine learning methods that is of high predictive capacity and able to provide interpretable results, to predict concentrations of PM2.5 sulfate, nitrate, organic carbon (OC) and elemental carbon (EC) across the conterminous United States from 2005 to 2015 at the daily level. The random forest models achieved high out-of-bag (OOB) R2 values at the daily level, and the mean OOB R2 values were 0.86, 0.82, 0.71 and 0.75 for sulfate, nitrate, OC and EC, respectively, over 2005–2015. The long-term temporal trends of PM2.5 sulfate, nitrate, OC and EC predictions agreed well with their corresponding ground measurements. The annual mean of predicted PM2.5 sulfate and EC levels across the conterminous United States decreased substantially from 2005 to 2015; while concentrations of predicted PM2.5 nitrate and OC decreased and fluctuated during the study period. The annual prediction maps captured the characterized spatial patterns of the PM2.5 constituents. The distributions of annual mean concentrations of sulfate and nitrate were generally regional in the extent that sulfate decreased from east to west smoothly with enhancement in California and nitrate had higher concentration in Midwest, Metro New York area, and California. OC and EC had regional high concentrations in the Southeast and Northwest as well as localized high levels around urban centers. The spatial patterns of PM2.5 constituents were consistent with the distributions of their emission sources and secondary processes and transportation. Hence, the national models developed in this study could provide supplementary evaluations of spatio-temporal distributions of PM2.5 constituents with full time-space coverages in the conterminous United States, which could be beneficial to assess the impacts of PM2.5 constituents on radiation budgets and visibility degradation, and support exposure assessment for regional to national health studies at county or city levels to understand the acute and chronic toxicity and health impacts of PM2.5 constituents, and consequently provide scientific evidence for making targeted and effective regulations of PM2.5 pollution.",unknown,1592
10.1016/j.cogsys.2018.09.018,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85054337312,scopus,2018-12-01,scopus,modeling collective rule at ancient teotihuacan as a complex adaptive system: communal ritual makes social hierarchy more effective,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054337312&origin=inward,"
                  Experts remain divided about the nature of the sociopolitical system of ancient Teotihuacan, which was one of the earliest and largest urban civilizations of the Americas. Excavations hoping to find compelling evidence of powerful rulers, such as a royal tomb, keep coming away empty-handed. But the alternative possibility of collective rule still remains poorly understood as well. Previously we used a computational model of this city’s hypothetical sociopolitical network to show that in principle collective rule based on communal ritual could be an effective strategy of ensuring widespread social coordination, as long as we assume that the network’s structure could be transformed via social learning and local leaders were not strongly subdivided. Here we extended this model to investigate whether increased social hierarchy could mitigate the negative effects of such strong divisions. We found a special synergy between social hierarchy and communal ritual: only their combination improved the extent of social coordination, whereas the introduction of centralization and top-down influence by themselves had no effect. This finding is consistent with portrayals of the Teotihuacan elite as religious specialists serving the public good, in particular by synchronizing the city’s ritual calendar with the rhythms of the stars.
               ",unknown,1594
10.1016/j.biopha.2018.09.109,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85054183317,scopus,2018-12-01,scopus,long-circulating and fusogenic liposomes loaded with a glucoevatromonoside derivative induce potent antitumor response,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054183317&origin=inward,"Cancer is an important public health problem, being one of the leading causes of death worldwide. Most antineoplastic agents cause severe toxic effects and some types of cancer do not respond or are resistant to the existing pharmacotherapy, necessitating the research and development of new therapeutic strategies. Cardenolides have shown significant antitumor activity due to their ability to inhibit the Na+K+ATPase enzyme, and the expression of this enzyme is increased in tumor cells. Glucoevatromonoside containing peracetylated glucose hydroxyl groups (GEVPG) is a cardenolide derivative that has low solubility in aqueous media, which constitutes a barrier to its potential biological applications. In this context, the use of liposomes represents a promising strategy to deliver GEVPG, thus allowing its intravenous administration. In this study, long-circulating and fusogenic liposomes containing GEVPG (SpHL-GEVPG) were developed, and their chemical and physicochemical properties were evaluated. SpHL-GEVPG presented adequate properties, including a mean diameter of 182.2 ± 2.7 nm, a polydispersity index equal to 0.36 ± 0.03, a zeta potential of –2.37 ± 0.31 mV, and a GEVPG entrapment of 0.38 ± 0.04 mg/mL. Moreover, this formulation showed a good stability after having been stored for 30 days at 4 °C. The cytotoxic studies against breast (MDA-MB-231, MCF-7, and SKBR-3) and lung (A549) cancer cell lines demonstrated that SpHL-GEVPG treatment significantly reduced the cell viability. In addition, the SpHL-GEVPG formulation presented a good selectivity toward these cancer cells. The evaluation of the therapeutic efficacy of the treatment with SpHL-GEVPG showed a potent anticancer effect in an A549 human lung cancer xenograft model. SpHL-GEVPG administered at doses of 1.0 and 2.0 mg/kg (i.v.) induced antitumor effect comparable to paclitaxel given at dose of 10 mg/kg (i.v.) to mice. Therefore, the results of the present work indicate the potential applicability of SpHL-GEVPG as a new anticancer formulation.",unknown,1595
10.1016/j.telpol.2018.01.003,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85041565918,scopus,2018-11-01,scopus,citizen-centered big data analysis-driven governance intelligence framework for smart cities,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041565918&origin=inward,"
                  Sensors and systems within rapidly expanding smart cities produce citizen-centered big data which have potential value to support citizen-centered urban governance decision-making. There exists a wealth of extant conceptual studies, however, further operational studies are needed to establish a specific path towards implementation of such data to governance decision-making with analytical algorithms that are appropriate for each step of the path. This paper proposes a framework for the use of citizen-centered big data analysis to drive governance intelligence in smart cities from two perspectives: urban governance issues and data-analysis algorithms. The framework consists of three layers: 1) A data-merging layer, which builds a citizen-centered panoramic data set for each citizen by merging citizen-related big data from multiple sources in collaborative urban governance via similarity calculation and conflict resolution; 2) a knowledge-discovery layer, which plots the citizen profile and citizen persona at both individual and group levels in terms of urban public service delivery and citizen participation via simple statistical analysis techniques, machine learning, and econometrics methods; and 3) a decision-making layer, which uses ontology models to standardize urban governance-related attributes, personas, and associations to support governance decision-making via data mining and Bayesian Net techniques. Finally, the proposed framework is validated in a case study on blood donation governance in China. This research highlights the value of citizen-centered big data, pushes data-to-decision research from conceptual to operational, synthesizes previously published frameworks for citizen-centered big data analysis in smart cities, and enhances the mutual supplement cross multiple disciplinaries.
               ",unknown,1600
10.1016/j.solener.2018.08.017,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85051639541,scopus,2018-10-01,scopus,a toolkit for multi-scale mapping of the solar energy-generation potential of buildings in urban environments under uncertainty,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051639541&origin=inward,"
                  Many municipalities and public authorities have supported the creation of solar cadastres to map the solar energy-generation potential of existing buildings. Despite advancements in modelling solar potential, most of these tools provide simple evaluations based on benchmarks, neglecting the effect of uncertain environmental conditions and that of the spatial aggregation of multiple buildings. We argue that including such information in the evaluation process can lead to more robust planning decisions and a fairer allocation of public subsidies.
                  To this end, this paper presents a novel method to incorporate uncertainty in the evaluation of the solar electricity generation potential of existing buildings using a multi-scale approach. It also presents a technique to visualise the results through their integration in a 3D-mapping environment and the use of false-colour overlays at different scales.
                  Using multiple simulation scenarios, the method is able to provide information about confidence intervals of summary statistics of production due to variation in two typical uncertain factors: vegetation and weather. The uncertainty in production introduced by these factors is taken into account through pairwise comparisons of nominal values of indicators, calculating a comprehensive ranking of the energy potential of different spatial locations and a corresponding solar score. The analysis is run at different scales, using space- and time-aggregated results, to provide results relevant to decision-makers.
               ",unknown,1606
10.1016/j.envpol.2018.05.044,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85048173436,scopus,2018-10-01,scopus,particulate matter exposure increases jc polyomavirus replication in the human host,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048173436&origin=inward,"
                  Background
                  Human polyomaviruses (HPyVs) asymptomatically infect the human population during childhood and establish latency in the host. Viral reactivation and urinary excretion can occur when the immune system is impaired. Exposure to particulate air pollution, including the PM10/PM2.5 components, is a public health problem and has been linked to several disorders. Studies assessing the relationship between PM10/PM2.5 exposure and viral replication are lacking.
               
                  Objectives
                  To investigate the relationship between HPyVs viruria and PM10/PM2.5 exposures.
               
                  Methods
                  Individual environmental exposure was assessed in 50 healthy adult volunteers using a chemical transport model (CTM) with a municipality resolution for daily PM10 and monitoring stations data for daily PM2.5 exposures. For each subject, a urine sample was collected, and HPyVs (JCPyV, BKPyV, MCPyV, HPyV6, HPyV7 and HPyV9) loads were determined. Zero-inflated negative binomial (ZINB) regression was used to model the count data, as it contained excessive zeros. Covariates were chosen by stepwise selection.
               
                  Results
                  HPyVs DNA was detected in 54% (median:87.6*105 copies/ml) of the urine samples. JCPyV was the prevalent (48%, (median viral load:126*105 copies/ml). Considering the load of the most frequently measured HPyVs, JCPyV, in the count-part of the ZINB model, every unitary in PM measured 2 days before urine collection (PM Day −2) was associated with an increase in JCPyV load (PM10: +4.0%, p-value = 0.002; PM2.5: +3.6%, p-value = 0.005). In the zero-part, the significant predictor was the PM10 measured 5 days before urine collection (+3%, p-value = 0.03).
               
                  Conclusions
                  The environmental levels of PM10/PM2.5 increase the JCPyV viruria. Our findings emphasize the need for studies assessing the influence of air pollution exposure on the risk of viral reactivation.
               ",unknown,1608
10.1016/j.ejmech.2018.06.050,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85049348562,scopus,2018-08-05,scopus,"synthesis and antimicrobial evaluation of amino sugar-based naphthoquinones and isoquinoline-5,8-diones and their halogenated compounds",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049348562&origin=inward,"
                  Antibiotic resistance has emerged as a serious global public health problem and lately very few antibiotics have been discovered and introduced into clinical practice. Therefore, there is an urgent need for the development of antibacterial compounds with new mechanism of action, especially those capable of evading known resistance mechanisms. In this work two series of glycoconjugate and non-glycoconjugate amino compounds derived from of isoquinoline-5,8-dione and 1,4-naphthoquinone and their halogenated derivatives were synthesized and evaluated for antimicrobial activity against Gram-positive (Enterococcus faecalis ATCC 29212, Staphylococcus aureus ATCC 25923, S. epidermidis ATCC 12228, S. simulans ATCC 27851) and Gram-negative bacteria (E. coli ATCC 25922, Proteus mirabilis ATCC 15290, K. pneumoniae ATCC 4352 and P. aeruginosa ATCC 27853) strains of clinical importance. This study revealed that glycoconjugate compounds derived from halogeno-substituted naphthoquinones were more active against Gram-negative strains, which cause infections whose treatment is even more difficult, according to the literature. These molecules were also more active than isoquinoline-5,8-dione analogues with minimum inhibitory concentration (MIC = 4–32 μg/mL) within Clinical and Laboratory Standard Institute MIC values (CLSI 0.08–256 μg/mL). Interestingly the minimal bactericidal concentration (MBC) values of the most active compounds were equal to MIC classifying them as bactericidal agents against Gram-negative bacteria. Sixteen compounds among eighteen carbohydrate-based naphthoquinones tested showed no hemolytic effects on health human erythrocytes whereas more susceptibility to hemolytic cleavage was observed when using non-glycoconjugate amino compounds. In silico Absorption, Distribution, Metabolism, Excretion and Toxicity (ADMET) evaluation also pointed out that these compounds are potential for oral administration with low side effects. In general, this study indicated that these compounds should be exploited in the search for a leading substance in a project aimed at obtaining new antimicrobials more effective against Gram-negative bacteria.
               ",unknown,1611
10.1016/j.energy.2018.04.161,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85056223880,scopus,2018-07-15,scopus,estimating residential energy consumption in metropolitan areas: a microsimulation approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056223880&origin=inward,"
                  Prior research has shown that land use patterns and the spatial configurations of cities have a significant impact on residential energy demand. Given the pressing issues surrounding energy security and climate change, there is renewed interest in developing and retrofitting cities to make them more energy efficient. Yet deriving micro-scale residential energy footprints of metropolitan areas is challenging because high resolution data from energy providers is generally unavailable. In this study, a bottom-up model is proposed to estimate residential energy demand using datasets that are commonly available in the United States. The model applies novel machine learning methods to match records in the Residential Energy Consumption Survey with Public Use Microdata samples. This matching and machine learning produce a synthetic household energy distribution at a neighborhood scale. The model was tested and validated with data from the Atlanta metropolitan region to demonstrate its application and promise.
               ",unknown,1613
10.1016/j.neucom.2017.11.075,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85042642694,scopus,2018-07-12,scopus,weightless neuro-symbolic gps trajectory classification,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042642694&origin=inward,"
                  This paper presents a framework for dealing with the problem of GPS trajectory classification in the context of the Rio de Janeiro’s public transit system (with hundreds or more classes). Such framework combines the versatile WiSARD classifier with a set of rules defined a priori, resulting in a neuro-symbolic learning system with very interesting characteristics and cutting-edge performance. We also verified the influence of different binarization methods in order to adapt raw data to WiSARD, which feeds from binary data only. These ideas were tested against a large data set of trajectories of buses from the city of Rio de Janeiro. The results confirm the practical applicability of those, since the accomplished performance was as good as that of other state-of-the-art rival methods in most test scenarios.
               ",unknown,1614
10.1016/j.jpeds.2017.11.028,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85041113592,scopus,2018-04-01,scopus,peg 3350 administration is not associated with sustained elevation of glycol levels,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041113592&origin=inward,"
                  Objective
                  To determine whether trace amounts of ethylene glycol (EG), diethylene glycol (DEG), or triethylene glycol (TEG) in PEG 3350 are associated with increased blood levels of EG, DEG, or TEG in children receiving daily PEG 3350 therapy.
               
                  Study design
                  Blood samples were drawn from 9 children who were being treated for constipation with PEG 3350 (6-12 years old) before and every 30 minutes for 3 hours after receiving 17 g of PEG 3350. PEG 3350, tap water, and blood samples from 18 age- and sex-matched controls also were analyzed.
               
                  Results
                  Baseline blood levels of EG and TEG did not differ between control and treated groups. DEG levels (median [IQR]) were lower in the PEG 3350 group (40.13 ng/mL [36.69, 63.94] vs 92.83 ng/mL [51.06, 128.93], P = .008). After PEG 3350 dose, levels of EG (390.51 ng/mL [326.06, 624.55]) and TEG (2.21 ng/mL [0, 4.5]) peaked at 90 minutes at 1032.81 ng/mL (826.84, 1486.13) (P = .009) and 35.17 ng/mL (15.81, 45.13) (P = .0005), respectively. DEG levels did not significantly change. Standard 17-g doses of PEG 3350 in 8 oz (237 mL) of water resulted in concentrations (mean ± SD) of EG, DEG, and TEG of 1.32 ± 0.23 µg/mL, 0.18 ± 0.03 µg/mL, and 0.12 ± 0.01 µg/mL, respectively. EG, DEG, and TEG levels in public water supply were 0.07 µg/mL, 0.21 µg/mL, and 0.02 µg/mL, respectively.
               
                  Conclusions
                  Daily PEG 3350 therapy in children was not associated with sustained elevation of EG, DEG, or TEG blood levels over levels in matched controls. Although EG and TEG levels increased after a standard dose of PEG 3350, their peak values remained well below toxic levels.
               ",unknown,1628
10.1016/j.giq.2017.11.010,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85039700811,scopus,2018-04-01,scopus,agile local governments: experimentation before implementation,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85039700811&origin=inward,"
                  This paper discusses how local governments can team up for joint service provision, be more adaptive towards new technological and organisational changes and introduce novel services following main industry trends (e.g. predictive analytics, autonomous vehicles and artificial intelligence). The conceptual approach is to use Public Value (PV) as the framework for the organisation and management of government performance, one of the most important successor ‘paradigmettes’ of the New Public Management (NPM). Based on the PV concept, the ‘adaptive model’ for local governments is introduced according to which each procured ICT solution is preceded by agile, open, bottom-up and experimental trial. This model is corroborated via recent empirical evidence from the case of Helsinki and Tallinn which was obtained by observing how city governments collaborate on joint innovation-lab-type structures and conduct agile trials in the field of smart mobility before traditional procurement.
               ",unknown,1629
10.1016/j.apenergy.2017.12.051,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85037973610,scopus,2018-02-15,scopus,predicting electricity consumption for commercial and residential buildings using deep recurrent neural networks,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85037973610&origin=inward,"
                  This paper presents a recurrent neural network model to make medium-to-long term predictions, i.e. time horizon of 
                        
                           ⩾
                           1
                        
                      week, of electricity consumption profiles in commercial and residential buildings at one-hour resolution. Residential and commercial buildings are responsible for a significant fraction of the overall energy consumption in the U.S. With advances in sensors and smart technologies, there is a need for medium to long-term prediction of electricity consumption in residential and commercial buildings at hourly intervals to support decision making pertaining to operations, demand response strategies, and installation of distributed generation systems. The modeler may have limited access to information about building’s schedules and equipment, making data-driven machine learning models attractive. The energy consumption data that is available may also contain blocks of missing data, making time-series predictions difficult. Thus, the main objectives of this paper are: (a) Develop and optimize novel deep recurrent neural network (RNN) models aimed at medium to long term electric load prediction at one-hour resolution; (b) Analyze the relative performance of the model for different types of electricity consumption patterns; and (c) Use the deep NN to perform imputation on an electricity consumption dataset containing segments of missing values. The proposed models were used to predict hourly electricity consumption for the Public Safety Building in Salt Lake City, Utah, and for aggregated hourly electricity consumption in residential buildings in Austin, Texas. For predicting the commercial building’s load profiles, the proposed RNN sequence-to-sequence models generally correspond to lower relative error when compared with the conventional multi-layered perceptron neural network. For predicting aggregate electricity consumption in residential buildings, the proposed model generally does not provide gains in accuracy compared to the multi-layered perceptron model.
               ",unknown,1630
10.1016/j.envint.2017.11.032,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85037699026,scopus,2018-02-01,scopus,development of artificial intelligence approach to forecasting oyster norovirus outbreaks along gulf of mexico coast,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85037699026&origin=inward,"
                  This paper presents an artificial intelligence-based model, called ANN-2Day model, for forecasting, managing and ultimately eliminating the growing risk of oyster norovirus outbreaks. The ANN-2Day model was developed using Artificial Neural Network (ANN) Toolbox in MATLAB Program and 15-years of epidemiological and environmental data for six independent environmental predictors including water temperature, solar radiation, gage height, salinity, wind, and rainfall. It was found that oyster norovirus outbreaks can be forecasted with two-day lead time using the ANN-2Day model and daily data of the six environmental predictors. Forecasting results of the ANN-2Day model indicated that the model was capable of reproducing 19years of historical oyster norovirus outbreaks along the Northern Gulf of Mexico coast with the positive predictive value of 76.82%, the negative predictive value of 100.00%, the sensitivity of 100.00%, the specificity of 99.84%, and the overall accuracy of 99.83%, respectively, demonstrating the efficacy of the ANN-2Day model in predicting the risk of norovirus outbreaks to human health. The 2-day lead time enables public health agencies and oyster harvesters to plan for management interventions and thus makes it possible to achieve a paradigm shift of their daily management and operation from primarily reacting to epidemic incidents of norovirus infection after they have occurred to eliminating (or at least reducing) the risk of costly incidents.
               ",unknown,1633
10.1016/j.jclepro.2017.08.095,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85029165778,scopus,2017-11-10,scopus,a mix-method model for adaptation to climate change in the agricultural sector: a case study for italian wine farms,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029165778&origin=inward,"
                  The negative effects of climate change are predicted to impact the agricultural sector in coming decades. Economic losses and modifications of production processes are fundamental issues to consider in coping with the harmful consequences of climate variability. The literature and empirical evidence show that the wine sector is extremely vulnerable to this risk. These studies show that this sector lacks appropriate adaptation strategies due to the complexity of the analysed systems and interrelations between a number of socio-economic and environmental variables. The present study designed a decision support system to identify adaptation strategies for wine farms undergoing climate change. The tool allows for the analysis of a wine farm's economic performance when it adopts measures to cope with climatic variability. Average values for climate change and extreme events were considered to assess different scenarios. A mix-method approach was applied to integrate probability calculations, complex system analyses and operational research (a metaheuristic approach). The model was tested on a case study located in central Italy (Chianti Classico). To maintain and improve future financial performance, organic farming and adjustments to procedural guidelines were recommended as key strategies. Economic variables, such as the average price of wine, seem to have a strong influence on farms' implementation of adaptive measures. An additional result seems to suggest that insurance schemes in areas producing high quality wine are only suitable when low-level deductibles and public funding are available. The present work shows that the decision support system favours analytical sensitivity to different scenarios and variables related to climate change as well as to socio-economic shifts in the viticulture sector.
               ",unknown,1655
10.1016/j.compenvurbsys.2017.06.003,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85021832419,scopus,2017-09-01,scopus,a machine learning-based method for the large-scale evaluation of the qualities of the urban environment,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021832419&origin=inward,"Given the present size of modern cities, it is beyond the perceptual capacity of most people to develop a good knowledge about the qualities of the urban space at every street corner. Correspondingly, for planners, it is also difficult to accurately answer questions such as ‘where the quality of the physical environment is the most dilapidated in the city that regeneration should be given first consideration’ and ‘in fast urbanising cities, how is the city appearance changing’. To address this issue, in the present study, we present a computer vision method that contains three machine learning models for the large-scale and automatic evaluation on the qualities of the urban environment by leveraging state-of-the-art machine learning techniques and wide-coverage street view images. From various physical qualities that have been identified by previous research to be important for the urban visual experience, we choose two key qualities, the construction and maintenance quality of building facade and the continuity of street wall, to be measured in this research. To test the validity of the proposed method, we compare the machine scores with public rating scores collected on-site from 752 passers-by at 56 locations in the city. We show that the machine learning models can produce a medium-to-good estimation of people's real experience, and the modelling results can be applied in many ways by researchers, planners and local residents.",unknown,1662
10.1016/j.knosys.2017.02.017,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85013382491,scopus,2017-05-01,scopus,a decision support system for detecting serial crimes,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85013382491&origin=inward,"
                  Serial crimes pose a great threat to public security. Linking crimes committed by the same offender can assist the detecting of serial crimes and is of great importance in maintaining public security. Currently, most crime analysts still link serial crimes empirically especially in China and desire quantitative tools to help them. This paper presents a decision support system for crime linkage based on various, including behavioral, features of criminal cases. Its underlying technique is pairwise classification based on similarity, which is interpretable and easy to tune. We design feature similarity algorithms to calculate the pairwise similarities and build up a classifier to determine whether a case pair should belong to a series. A comprehensive case study of a real-world robbery dataset demonstrates its promising performance even with the default setting. This system has been deployed in a public security bureau of China and running for more than one year with positive feedback from users. The use of this system would provide individual officers with strong support in crimes investigation then allow law enforcement agency to save resources, since the system not only can link serial crimes automatically based on a classification model learned from historical crime data, but also has flexibility in training data update and domain experts interaction, including adjusting the key components like similarity matrices and decision thresholds to reach a good tradeoff between caseload and number of true linked pairs.
               ",unknown,1667
10.1016/j.egypro.2017.12.240,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:85041491911,scopus,2017-01-01,scopus,hybrid artificial neural network models for effective prediction and mitigation of urban roadside no<inf>2</inf> pollution,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041491911&origin=inward,"Traffic-related air pollution has been a serious concern amongst policy-makers and the public due to its physiological and environmental impacts. An early warning system based on accurate forecasting tools must therefore be implemented to circumvent the adverse effects of exposure to major air pollutants. A multilayer perceptron neural network was trained and developed using air pollution and meteorological data over a two-year period from a monitoring site in Marylebone Road, Central London to predict roadside concentration values of NO2 24 hours ahead. Several hybrid models were also developed by applying feature selection techniques such as stepwise regression, principal component analysis, and Classification and Regression Trees to the neural network model. Most roadside pollutant variables, e.g., oxides of nitrogen, were found to be significant in predicting NO2. The statistical results reveal overall prediction superiority of the hybrid models to the standalone neural network model.",unknown,1679
10.1016/j.landusepol.2016.06.004,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84973868146,scopus,2016-11-30,scopus,"impacts of land use and amenities on public transport use, urban planning and design",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84973868146&origin=inward,"
                  Various land-use configurations are known to have wide-ranging effects on the dynamics of and within other city components including the transportation system. In this work, we particularly focus on the complex relationship between land-use and transport offering an innovative approach to the problem by using land-use features at two differing levels of granularity (the more general land-use sector types and the more granular amenity structures) to evaluate their impact on public transit ridership in both time and space. To quantify the interdependencies, we explored three machine learning models and demonstrate that the decision tree model performs best in terms of overall performance—good predictive accuracy, generality, computational efficiency, and “interpretability”. Results also reveal that amenity-related features are better predictors than the more general ones, which suggests that high-resolution geo-information can provide more insights into the dependence of transit ridership on land-use. We then demonstrate how the developed framework can be applied to urban planning for transit-oriented development by exploring practicable scenarios based on Singapore's urban plan toward 2030, which includes the development of “regional centers” (RCs) across the city-state. Results show an initial increase in transit ridership as the amount of amenities is increased. This trend, on the other hand, eventually reverses (particularly during peak hours) with continued strategic increase in amenities; a tipping point at 55% increase is identified where ridership begins to decline and at 110%, the predicted ridership begins to fall below current levels. Our in-silico experiments support one of the medium-term land-use transport goals of stakeholders—to alleviate future strains in the transportation system of Singapore through the development of RCs. The model put forward can serve as a good foundation in building decision-support tools that can assist planners in better strategizing and planning land-use configurations, in particular the amenity resource distribution, to influence and shape public transportation demand.
               ",unknown,1694
10.1016/j.envint.2016.07.021,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84992034990,scopus,2016-10-01,scopus,substantial decline of polychlorinated biphenyls serum levels 10 years after public health interventions in a population living near a contaminated site in northern italy,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84992034990&origin=inward,"
                  Introduction
                  Polychlorinated biphenyls (PCBs) are persistent pollutants classified among endocrine disruptors and human carcinogens. In an urban area in Northern Italy (Brescia), a chemical factory producing PCBs from 1938 to 1984 caused soil and food pollution. Since the discovering of the environmental pollution in the area in 2001, various public health interventions have been implemented to avoid human contamination.
               
                  Methods
                  Two independent surveys were carried out in 2003 and 2013 using the same study design and methods. A random sample of the adult general population living in four areas of the town with different soil contamination level was enrolled in both surveys. Investigation included serum levels of 24 PCB congeners and of other common clinical-chemical parameters and questions about residential and occupational history as well as current and past diet and consumption of locally produced food. 537 subjects were tested in the 2003 and 488 in the 2013 survey.
               
                  Results
                  Total PCB serum levels showed a strong correlation with age (Spearman r=0.71 in 2003 and r=0.80 in 2013), but not with gender. Consumers of food produced in the most polluted area had higher total PCB serum levels (median 15.6 and 4.7ng/ml in 2003 and 2013, respectively) than non-consumers (4.1 and 2.3ng/ml) and consumers of food produced in non-polluted areas (6.5 and 1.9ng/ml), with increasing trend of PCB serum levels with cumulative local food intake. PCB serum levels from 2003 to 2013 survey declined by around half in all age groups, especially among the youngest people. The total PCB reduction was found to be due to a combination of a birth cohort effect (−41% every 10years) and survey period effect (−18% in 2013 versus 2003), with medium chlorinated CB congeners showing the most relevant reduction (−30%) between the two surveys. High chlorinated CBs were more strongly associated with birth cohort (−46% every 10years): in the 2013 survey they represented 58% of total PCBs in 60–79years old while they were 37% among the under 40s with a median value 20 times higher in the oldest than in the youngest (3.1 versus 0.16ng/ml).
               
                  Conclusions
                  PCBs serum levels declined from the 2003 to 2013 survey though they are still high in the elderly mainly because of past intake of locally produced contaminated food. Present findings suggest that PCBs food chain contamination and human absorption have been interrupted successfully in the area since discovery of the pollution in 2001.
               ",unknown,1697
10.1016/j.rvsc.2016.03.001,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84960870451,scopus,2016-06-01,scopus,"in vivo and in vitro effectiveness of azadirachta indica-synthesized silver nanocrystals against plasmodium berghei and plasmodium falciparum, and their potential against malaria mosquitoes",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84960870451&origin=inward,"
                  Malaria transmission is a serious emergence in urban and semiurban areas worldwide, becoming a major international public health concern. Malaria is transmitted through the bites of Anopheles mosquitoes. The extensive employ of synthetic pesticides leads to negative effects on human health and the environment. Recently, plant-synthesized nanoparticles have been proposed as highly effective mosquitocides. In this research, we synthesized silver nanoparticles (AgNP) using the Azadirachta indica seed kernel extract as reducing and stabilizing agent. AgNP were characterized by UV–vis spectrophotometry, SEM, EDX, XRD and FTIR spectroscopy. The A. indica seed kernel extract was toxic against Anopheles stephensi larvae and pupae, LC50 were 232.8ppm (larva I), 260.6ppm (II), 290.3ppm (III), 323.4ppm (IV), and 348.4ppm (pupa). AgNP LC50 were 3.9ppm (I), 4.9ppm (II), 5.6ppm (III), 6.5ppm (IV), and 8.2ppm (pupa). The antiplasmodial activity of A. indica seed kernel extract and AgNP was evaluated against CQ-resistant (CQ-r) and CQ-sensitive (CQ-s) strains of Plasmodium falciparum. IC50 of A. indica seed kernel extract were 63.18μg/ml (CQ-s) and 69.24μg/ml (CQ-r). A. indica seed kernel-synthesized AgNP achieved IC50, of 82.41μg/ml (CQ-s) and 86.12μg/ml (CQ-r). However, in vivo anti-plasmodial experiments conducted on Plasmodium berghei infecting albino mice showed moderate activity of the A. indica extract and AgNP. Overall, this study showed that the A. indica-mediated fabrication of AgNP is of interest for a wide array of purposes, ranging from IPM of mosquito vectors to the development of novel and cheap antimalarial drugs.
               ",unknown,1703
10.1016/j.ocecoaman.2016.01.014,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84959450385,scopus,2016-05-01,scopus,"natural and human controls of water quality of an amazon estuary (caeté-pa, brazil)",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84959450385&origin=inward,"
                  Estuarine waters are usually eutrophic due to nutrients input derived from natural and anthropogenic sources. In the Caeté Estuary, nutrients and thermotolerant coliforms input is intensified by the absence of a public sanitation system and by mangrove outwelling. This input is more accentuated in the upper sector of the estuary where 90% of the local population is concentrated and a high incidence of commercial activities (public markets, ice factories and boat repairing among others) is located. As a consequence, eutrophic waters with high concentrations of thermotolerant coliforms (up to 1100 MPN/100 ml) were observed during the dry season when dilution and/or advection are less effective. Eutrophication, though less pronounced, also occurred in the other sectors (middle and lower), but these results seem to indicate that this could be a natural condition of an Amazon region which is naturally enriched with a high amount of nutrients. On the other hand, nutrient concentration values in the upper sector are similar to those found in other highly populated and industrialized Brazilian estuaries. Taking into account that the population increases by about 20% per decade in the region, this growth may lead to a significant increase in human pressure and impacts on the area, mainly during periods when the estuary is more susceptible to retaining nutrients (e.g., during the dry season). In order to improve water quality, the DPSIR framework was used as a guide for proposing potential measures to be implemented in the Caeté Estuary based on (i) urban planning to control sewage discharges, (ii) construction of water treatment infrastructures to reduce untreated effluents inputs and (iii) water use regulations as a function of the current water quality.
               ",unknown,1704
10.1016/j.eti.2016.01.001,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84962732983,scopus,2016-04-01,scopus,"neuro-fuzzy approach to forecasting ozone episodes over the urban area of delhi, india",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84962732983&origin=inward,"
                  Tropospheric ozone is a major air pollution problem, both for public health and for the environment, which is not directly emitted by human activities. It is a secondary pollutant, which produces due to reaction with volatile organic compounds and nitrogen oxide concentrations, both emitted by anthropogenic activities. Therefore, ozone episode has been studied in Delhi, the urban area, where anthropogenic emissions are playing major role in ambient air pollution. The 8-hourly analysis of past data pattern in each season at different monitoring stations in Delhi suggests that the ozone episodes were occurred approximately 31% of the total observations and the summer seasons is the worst performer of ozone concentrations in Delhi. The correlation matrix shows that the dew point temperature, NO2, and relative humidity are the dominating variables for Ozone (O3) concentrations in Delhi. The present study aimed to analyze the ozone episodes in a year and to develop the artificial intelligence based forecasting methodologies over Delhi megacity. Further, the hourly ozone forecasting models have been developed through different modeling techniques, e.g., multiple linear regressions (MLR), artificial neural network (ANN) and artificial intelligence based Neuro-Fuzzy (NF) techniques. The air pollutants as well as meteorological parameters have been used to analyze the ozone episodes. The forecasted results from different models are compared with the observed values with statistical measures, e.g., correlation coefficients (R), normalized mean square error (NMSE), fractional bias (FB) and index of agreement (IOA). The performed statistical analysis has indicated that the artificial intelligence implementations have a more reasonable agreement with the observed values.
               ",unknown,1705
10.1016/j.eswa.2015.08.048,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84945288965,scopus,2016-02-01,scopus,time-evolving o-d matrix estimation using high-speed gps data streams,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84945288965&origin=inward,"
                  Portable digital devices equipped with GPS antennas are ubiquitous sources of continuous information for location-based Expert and Intelligent Systems. The availability of these traces on the human mobility patterns is growing explosively. To mine this data is a fascinating challenge which can produce a big impact on both travelers and transit agencies.
                  This paper proposes a novel incremental framework to maintain statistics on the urban mobility dynamics over a time-evolving origin-destination (O-D) matrix. The main motivation behind such task is to be able to learn from the location-based samples which are continuously being produced, independently on their source, dimensionality or (high) communicational rate. By doing so, the authors aimed to obtain a generalist framework capable of summarizing relevant context-aware information which is able to follow, as close as possible, the stochastic dynamics on the human mobility behavior. Its potential impact ranges Expert Systems for decision support across multiple industries, from demand estimation for public transportation planning till travel time prediction for intelligent routing systems, among others.
                  The proposed methodology settles on three steps: (i) Half-Space trees are used to divide the city area into dense subregions of equal mass. The uncovered regions form an O-D matrix which can be updated by transforming the trees’leaves into conditional nodes (and vice-versa). The (ii) Partioning Incremental Algorithm is then employed to discretize the target variable’s historical values on each matrix cell. Finally, a (iii) dimensional hierarchy is defined to discretize the domains of the independent variables depending on the cell’s samples.
                  A Taxi Network running on a mid-sized city in Portugal was selected as a case study. The Travel Time Estimation (TTE) problem was regarded as a real-world application. Experiments using one million data samples were conducted to validate the methodology. The results obtained highlight the straightforward contribution of this method: it is capable of resisting to the drift while still approximating context-aware solutions through a multidimensional discretization of the feature space. It is a step ahead in estimating the real-time mobility dynamics, regardless of its application field.
               ",unknown,1709
10.1016/j.fertnstert.2014.10.026,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84923205805,scopus,2015-02-01,scopus,novel topical formulation of lidocaine provides significant pain relief for intrauterine device insertion: pharmacokinetic evaluation and randomized placebo-controlled trial,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84923205805&origin=inward,"Objective To investigate the pharmacokinetics, safety, and analgesic efficacy of a novel topical formulation of lidocaine at insertion of an intrauterine device (IUD). Design Randomized controlled trial; phase-I and phase-II studies. Setting University and public hospitals. Patient(s) Women aged ≥18 years who wanted to receive an IUD. Four women were parous in phase I; all in phase II were nulliparous. Intervention(s) A single, 8.5-mL dose of lidocaine formulation (SHACT) was administered (to the portio, cervix, and uterus) with a specially designed applicator. Main Outcome Measure(s) The phase-I study (single-arm) was designed for pharmacokinetic assessment; the phase-II study (randomized) was intended for investigation of efficacy and safety. Result(s) From the phase-I study (15 participants), mean pharmacokinetic values were: maximum plasma concentration: 351 ± 205 ng/mL; time taken to reach maximum concentration: 68 ± 41 minutes; and area under the concentration–time curve from 0 to 180 minutes: 717 ± 421 ng*h/mL. Pain relief was observed with lidocaine vs. placebo in the phase-II study (218 women, randomized). Mean visual analog scale score for maximum pain during the first 10 minutes after IUD insertion was 36% lower with lidocaine than with placebo (28.3 ± 24.6 vs. 44.2 ± 26.0). Pain intensity was also significantly lower in the lidocaine group at 30 minutes. On average, 3 of 4 patients will have less pain with lidocaine than with placebo. Adverse events were similar in the placebo and lidocaine groups. No serious adverse events were reported. Conclusion(s) Lidocaine provides pain relief lasting for 30–60 minutes for women undergoing IUD insertion, without any safety concerns. Further studies of this lidocaine formulation, for IUD insertion and other clinical applications, are planned. Clinical Trial Registration Number 2011-005660-18 and 2011-006220-20 (EudraCT).",unknown,1719
10.1016/j.neucom.2013.07.051,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84897916947,scopus,2014-07-05,scopus,urban bicycles renting systems: modelling and optimization using nature-inspired search methods,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84897916947&origin=inward,"
                  Urban Bicycles Renting Systems (UBRS) are becoming a common and useful component in growing modern cities. For an efficient management and support, the UBRS infrastructure requires the optimation of vehicle routes connecting several bicycle base stations and storage centers. In this study, we model this real-world optimization problem as a capacitated Vehicle Routing Problem (VRP) with multiple depots and the simultaneous need for pickup and delivery at each base station location. Based on the VRP model specification, two nature-inspired computational techniques, evolutionary algorithms and ant colony systems, are presented and their performance in tackling the UBRS problem is investigated. In the evolutionary approach, individuals are encoded as permutations of base stations and then translated to a set of routes subject to the constraints related to vehicle capacity and node demands. In the ant-based approach, ants build complete solutions formed of several subtours servicing a subset of base stations using a single vehicle based on both apriori (the attractiveness of a move based on the known distance or other factors) and aposteriori (pheromone levels accumulated on visited edges) knowledge. Both algorithms are engaged for the UBRS problem using real data from the cities of Barcelona and Valencia. Computational experiments for several scenarios support a good performance of both population-based search methods. Comparative results indicate that better solutions are obtained on the average by the ant colony system approach for both considered cities.
               ",unknown,1733
10.1016/j.bmc.2014.01.036,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84896737372,scopus,2014-03-01,scopus,"antiprotozoan lead discovery by aligning dry and wet screening: prediction, synthesis, and biological assay of novel quinoxalinones",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84896737372&origin=inward,"
                  Protozoan parasites have been one of the most significant public health problems for centuries and several human infections caused by them have massive global impact. Most of the current drugs used to treat these illnesses have been used for decades and have many limitations such as the emergence of drug resistance, severe side-effects, low-to-medium drug efficacy, administration routes, cost, etc. These drugs have been largely neglected as models for drug development because they are majorly used in countries with limited resources and as a consequence with scarce marketing possibilities. Nowadays, there is a pressing need to identify and develop new drug-based antiprotozoan therapies. In an effort to overcome this problem, the main purpose of this study is to develop a QSARs-based ensemble classifier for antiprotozoan drug-like entities from a heterogeneous compounds collection. Here, we use some of the TOMOCOMD-CARDD molecular descriptors and linear discriminant analysis (LDA) to derive individual linear classification functions in order to discriminate between antiprotozoan and non-antiprotozoan compounds as a way to enable the computational screening of virtual combinatorial datasets and/or drugs already approved. Firstly, we construct a wide-spectrum benchmark database comprising of 680 organic chemicals with great structural variability (254 of them antiprotozoan agents and 426 to drugs having other clinical uses). This series of compounds was processed by a k-means cluster analysis in order to design training and predicting sets. In total, seven discriminant functions were obtained, by using the whole set of atom-based linear indices. All the LDA-based QSAR models show accuracies above 85% in the training set and values of Matthews correlation coefficients (
                        C
                     ) vary from 0.70 to 0.86. The external validation set shows rather-good global classifications of around 80% (92.05% for best equation). Later, we developed a multi-agent QSAR classification system, in which the individual QSAR outputs are the inputs of the aforementioned fusion approach. Finally, the fusion model was used for the identification of a novel generation of lead-like antiprotozoan compounds by using ligand-based virtual screening of ‘available’ small molecules (with synthetic feasibility) in our ‘in-house’ library. A new molecular subsystem (quinoxalinones) was then theoretically selected as a promising lead series, and its derivatives subsequently synthesized, structurally characterized, and experimentally assayed by using in vitro screening that took into consideration a battery of five parasite-based assays. The chemicals 11(12) and 16 are the most active (hits) against apicomplexa (sporozoa) and mastigophora (flagellata) subphylum parasites, respectively. Both compounds depicted good activity in every protozoan in vitro panel and they did not show unspecific cytotoxicity on the host cells. The described technical framework seems to be a promising QSAR-classifier tool for the molecular discovery and development of novel classes of broad—antiprotozoan—spectrum drugs, which may meet the dual challenges posed by drug-resistant parasites and the rapid progression of protozoan illnesses.
               ",unknown,1734
10.1016/j.procs.2014.09.059,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84938581023,scopus,2014-01-01,scopus,a new scheme for daily peak wind gust prediction using machine learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84938581023&origin=inward,"A major challenge in meteorology is the forecasting of winds owing to their highly chaotic nature. However, wind forecasts, and in particular daily peak wind gust forecasts, provide the public with a general sense of the risks associated with wind on a given day and are useful in decision making. Additionally, such knowledge is critical for wind energy production. Currently, no operational daily peak wind gust product exists. As such, this project will seek to develop a peak wind gust prediction scheme based on output from an operational numerical weather prediction model. Output from the North American Mesoscale (NAM) model will be used in a support vector regression (SVR) algorithm trained to predict daily peak wind gusts for ten cities commonly impacted by hazardous wind gusts (cities in the Midwest and central Plains) and with interests in wind energy. Output from a kernel principal component analysis of the fully three-dimensional atmosphere as characterized by the NAM forecasts will be used to predict peak wind gusts for each location at 36hours lead time. Ultimately, this initial product will lead to the development of a more robust prediction scheme that could one day transition into an operational forecast model.",unknown,1741
10.1016/j.atmosenv.2013.01.033,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84873729187,scopus,2013-05-01,scopus,temporal trajectories of wet deposition across hydro-climatic regimes: role of urbanization and regulations at u.s. and east asia sites,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84873729187&origin=inward,"
                  Dominant global patterns of urbanization and industrialization contribute to large-scale modification of the drivers for hydrologic and biogeochemical processes, as evident in Asia, Africa, and South America which are experiencing rapid population and economic growth. One manifestation of urbanization and economic development is decreases in air quality, increases in dry/wet deposition fluxes, and growing adverse impacts on public health and ecosystem integrity. We examined available long-term (1980–2010) observational data, gathered at weekly intervals, for wet deposition at 19 urban sites in the U.S., and monitoring data (2000–2009) available for 17 urban sites at a monthly scale in East Asia. Our analyses are based on data for four constituents (SO4
                     2−, NO3
                     −, Ca2+, and Mg2+); differences in atmospheric chemistry and terrestrial sources of these constituents enabled a robust comparative analysis. We examined intra-annual variability and the long-term temporal trajectories of wet deposition fluxes to discern the relative role of anthropogenic and stochastic hydro-climatic forcing. Here, we show that: (1) temporal variability in wet deposition fluxes follows an exponential probability density function at all sites, evidence that stochasticity of rainfall is the dominant control of wet deposition variability; (2) the mean wet deposition flux, μ
                     Ω (ML−2T−1), has decreased in the U.S. over time since enactment of the Clean Air Act, with μ
                     Ω having become homogenized across varying hydro-climatic regimes; and (3) in contrast, μ
                     Ω values for East Asian cities are 3–10 times higher than U.S. cities, attributed to lax regulatory enforcement. Based on the observed patterns, we suggest a stochastic model that generates ellipses within which the μ
                     Ω temporal trajectories are inscribed. In the U.S., anthropogenic forcing (regulations) is dominant in the humid regions, while variability in hydro-climatic forcing explains inter-annual variability in arid regions. Our stochastic analysis facilitates projections of the temporal trajectory shifts in wet deposition fluxes as a result of urbanization and other land-use changes, climate change, and regulatory enforcement.
               ",unknown,1757
10.1016/j.procs.2013.05.171,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84896915753,scopus,2013-01-01,scopus,inspace3d: a middleware for built environment data access and analytics,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84896915753&origin=inward,"Standardisation, archiving, and digital access of spatial data pertaining to built-up environments is an area acquiring increasing attention amongst several interest groups: policy makers, designers and planners, civil engineers, infrastructure management and public service personnel, building users. Initiatives such as the Building Information Model (BIM), Industry Founda- tion Classes (IFC), and CityGML are creating the information-theoretic backbone that guides the crucial aspects of quality, exchange, and interoperability of spatial data at the environmental and urban scale. However, due to the inherent scale, com- plexity, and detailed geometric character of building information data, extracting useful semantic and qualitative knowledge for accomplishing high-level analytical tasks is still an extremely complex and error prone process involving data intensive computing. We propose a uniform spatial data access middleware that can provide a combination of high-level, multi-modal, semantic, and quantitative-qualitative spatial data access and analytical capability. We present the core computational capabil- ities for the proposed middleware and present an overview of the high-level spatial model and its compliance with the industry standard IFC. A key theoretical contribution is a framework for investigating the computational complexity of deriving spatial artefacts within the context of building informatics. Additionally, we empirically investigate the feasibility and practicality of the derivation of spatial artefacts by conducting experiments on seven industry-scale IFC models. The experiment results show that, despite having non-linear polynomial increase with respect to time, deriving spatial artefacts is practical with large designs.",unknown,1764
10.1016/j.jchromb.2012.07.038,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84865748711,scopus,2012-09-15,scopus,a hilic-ms/ms method for the simultaneous determination of seven organic acids in rat urine as biomarkers of exposure to realgar,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84865748711&origin=inward,"
                  Realgar (As4S4) is a traditional medicine used in China and Europe for thousands of years. As an arsenical, the toxicity from realgar has raised public concern. Several organic acids in urine are found to be potential biomarkers of realgar exposure, including taurine, citric, glutamic, lactic, pyruvic, succinic and uric acid. In this study, using hydrophilic interaction liquid chromatography-tandem mass spectrometry (HILIC-MS/MS), a rapid and sensitive method was developed to separate and quantify these compounds in urine. A ZIC®-HILIC column was used for the separation at an isocratic condition of acetonitrile and 10mM ammonium acetate in water. Analytes were detected in multiple-reaction monitoring with negative ionization mode, using ibuprofen as internal standard. Good line arities (R
                     2
                     >0.996) were obtained for all analytes with the limits of detection from 0.2 to 0.7μg/mL. The intra-day and inter-day accuracy ranged from 89.1 to 104.4% and the relative standard deviation (RSD) did not exceed 15.0%. The recovery was more than 80%with RSD less than 14.0%. The validated method was applied to analyze the urine samples of control and reaglar treated rats, and significant changes of these organic acids were observed.
               ",unknown,1766
10.1016/j.inffus.2010.12.002,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:80053984303,scopus,2012-04-01,scopus,distributed information fusion models for regional public health surveillance,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=80053984303&origin=inward,"
                  Biosurveillance systems designed and deployed in the United States and abroad to allow public health authorities to monitor the health of their communities have significant design limitations despite their wide usage. One limitation is the lack of algorithmic solutions to combine local data sources for regional situation awareness. The objective of the current study is to develop and demonstrate the value of automated information fusion methods applied to the distributed neighboring public health sites. A prototype system consisting of distributed Bayesian models was designed to enable informed regional and local cognitive decision support response. The Intelligent Decision Support Network (IDSN) is composed of Bayesian Information Fusion Models (BIFMs) that target a particular syndrome or disease type. Using local data from county health departments in Northern Virginia for the time period between August 2005 and May 2007, we estimated the probability of a gastrointestinal (GI) outbreak in two ways: First, based on data from the local hospitals only; and second, based on the relative probability of outbreak by combining local hospital data and probabilities of GI events from the neighboring counties’ BIFMs. Preliminary findings showed that the network of distributed models detected events that would be undetected without multi-jurisdictional data.
               ",unknown,1772
10.1016/j.fertnstert.2012.05.024,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84865694987,scopus,2012-01-01,scopus,"premature progesterone rise negatively correlated with live birth rate in ivf cycles with gnrh agonist: an analysis of 2,566 cycles",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84865694987&origin=inward,"
                  Objective
                  To investigate the occurrence of premature progesterone rise (PPR) in GnRH agonist long or short protocol, address the relationship between circulating P levels and live birth rates, and explore the possible mechanism through which PPR affects clinical outcomes and the possible factors related to the occurrence of PPR.
               
                  Design
                  Retrospective analysis.
               
                  Setting
                  Reproductive medicine center of a public hospital.
               
                  Patient(s)
                  A total of 2,566 patients receiving in vitro fertilization/intracytoplasmic sperm injection treatment with GnRH agonist long or short protocol.
               
                  Intervention(s)
                  None.
               
                  Main Outcome Measure(s)
                  Live birth rates.
               
                  Result(s)
                  The corresponding incidence of PPR in long or short protocol was 22.86% (393/1,719) or 27.63% (234/847) with the cutoff value of 1.2 ng/mL or 2.0 ng/mL, respectively, being used to define PPR. Live birth rates decreased under the condition of PPR (40.65% vs. 29.77% in long protocol; 30.18% vs. 23.50% in short protocol). Logistic regression analysis showed that serum P level on the day of hCG administration was a strong predictor of live birth rate in both long and short protocols. Live birth rates in frozen embryo transfer cycles had no significant difference between groups with or without PPR (29.31% vs. 25.35% in long protocol; 24.84% vs. 24.22% in short protocol). Multivariate regression analysis showed that exogenous gonadotropin dose, the duration of stimulation, E2 and LH levels on the day of hCG administration, the number of oocytes retrieved, and basal FSH level were all involved in PPR.
               
                  Conclusion(s)
                  In GnRH agonist cycles, PPR negatively correlated with live birth rate in fresh embryo transfer cycles, although no adverse impact on frozen embryo transfer was observed, implying that PPR may have deleterious effects on endometrial receptivity.
               ",unknown,1776
10.1016/j.tra.2012.05.010,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:84865430434,scopus,2012-01-01,scopus,valuing of attributes influencing the attractiveness of suburban train service in mumbai city: a stated preference approach,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84865430434&origin=inward,"
                  The paper presents valuing of qualitative and quantitative travel attributes influencing the attractiveness of suburban train service in Mumbai city, India. A stated preference experiment is designed to capture the data of sub-urban train mode choice behavior. The behavioral data are analyzed using different modeling techniques such as multinomial logit (MNL) and mixed logit (ML). In ML model, the random parameters are assumed to follow constrained triangular distribution, where mean equals its spread. The decomposition of preference heterogeneity around the mean estimate of random parameter is also investigated using ML model. The study shows the influence of headway time and train ride time associated with a particular crowding level (expressed in density of standing passengers/m2) in choosing the sub-urban train mode by calculating their willingness-to-pay (WTP) values and highlights the importance of WTP for addressing policy issues in the reduction of in-vehicle crowding level. The present study documents new findings of the effect of crowding level on train ride time in the context of a developing country and suggests some important directions for future suburban train transport crowding valuation research.
               ",unknown,1777
10.1016/j.agwat.2010.10.007,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:78649895842,scopus,2010-12-30,scopus,faecal contamination and hygiene aspect associated with the use of treated wastewater and canal water for irrigation of potatoes (solanum tuberosum),https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=78649895842&origin=inward,"
                  Clean water has become one of the main limiting factors in agricultural food production in Europe, especially for countries around the Mediterranean, who now face more severe and frequent seasonal water shortages. In order to overcome water shortages the European Water Framework Directive encourages and promotes the use of treated urban wastewater in agriculture. However, the use of poor quality water in agriculture poses potential health risks. The application of wastewater through subsurface drip irrigation lines could possibly overcome public health concerns by minimizing contact with wastewater by farmers, farm workers but it is uncertain if the risk for consumers of wastewater irrigated produces would be acceptable. The objective of the current study was therefore to assess whether subsurface irrigation of potatoes with low quality water was associated with higher food safety and reduced human health risks as compared with surface irrigation. The microbial quality of soil and potatoes irrigated by sprinkler, furrow and subsurface drip irrigation, using treated urban wastewater, canal water and tap water were compared at experimental sites near Belgrade, Serbia and in Bologna, Italy. Water, soil and potato samples were collected from March 2007 to September 2008 and their faecal contamination estimated by enumeration of the faecal indicator Escherichia coli. In addition, water and potatoes in Italy were analysed for the presence of helminth eggs, another important indicator of faecal pollution. A quantitative microbial risk assessment (QMRA) model combined with Monte Carlo simulations was used to assess whether the different irrigation practices and associated health risks complied with guidelines set by the World Health Organization (WHO). The study found low levels of E. coli in irrigation water (Italy mean value: 1.7 colony forming units (cfu)/ml and Serbia 11cfu/ml), as well as in soil (Italy mean: 1.0cfu/g and Serbia 1.1cfu/g). Similar low concentrations of E. coli were found on potatoes (Italy mean: 1.0cfu/g and Serbia 0.0cfu/g). The vast majority (442/516) of the collected different samples were free of E. coli. No helminth eggs were found in any types of irrigation water or on the surface of potatoes. The risk assessment models found the use of treated wastewater to exceed the levels of risks for gastro-intestinal disease (1.0×10−3 disease risk) as recommended by the World Health Organization (WHO) for the accidental ingestion of soil by farmers (Serbia: 0.22 and Italy: 5.7×10−2). However, samples that exceeded disease risks set by the WHO were collected before initiation of wastewater irrigation and were limited to a few numbers of samples, which would indicate environmental contamination not linked to irrigation practice. Disease risk from consumption of potatoes in Italy and in Serbia was found to be within acceptable levels. No relationship was found between E. coli concentrations in irrigation water, soil and produce. Similar lack of association was found for E. coli findings in sprinkler, furrow or subsurface drip irrigated soils and produce. This indicates that subsurface drip irrigation can be practiced while ensuring food safety and protecting the health of consumers and farmers.
               ",unknown,1793
10.1016/j.eswa.2008.10.021,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:60249100481,scopus,2009-05-01,scopus,decision support system for service quality management using customer knowledge in public service organization,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=60249100481&origin=inward,"
                  As the service quality has been reconsidered in the public sector as well as private enterprises, the need for public sectors to adopt principle and practices of private sectors is concerned with customer-focused approach. However, the different business culture of public service organizations makes it difficult to improve service quality. It is required to establish a structured framework that leads employees to make efforts to improve their service delivery processes and supports continuous improvement of service delivery processes based on the data about the process performance from the customer-perceived value-oriented viewpoint.
                  In this paper, we propose a structured framework that identifies the key service processes, validates from customer perspectives and establishes the measurements to monitor based on the data about the process performance. It uses periodic customer satisfaction index (CSI) surveys (S.C. Park) for understanding customer-perceived values. The proposed framework consists of three phases; the questionnaire design, the key process (KP) identification from the integrated viewpoints of importance and contribution, and the key process indicator (KPI) derivation and management. For the application, we established a web-based decision support system for a public service organization for tourism in Korea.
               ",unknown,1804
10.1016/j.agwat.2008.09.008,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:58549119943,scopus,2009-03-01,scopus,information technology and innovative drainage management practices for selenium load reduction from irrigated agriculture to provide stakeholder assurances and meet contaminant mass loading policy objectives,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=58549119943&origin=inward,"
                  Many perceive the implementation of environmental regulatory policy, especially concerning non-point source pollution from irrigated agriculture, as being less efficient in the United States than in many other countries. This is partly a result of the stakeholder involvement process but is also a reflection of the inability to make effective use of Environmental Decision Support Systems (EDSS) to facilitate technical information exchange with stakeholders and to provide a forum for innovative ideas for controlling non-point source pollutant loading. This paper describes one of the success stories where a standardized Environmental Protection Agency (EPA) methodology was modified to better suit regulation of a trace element in agricultural subsurface drainage and information technology was developed to help guide stakeholders, provide assurances to the public and encourage innovation while improving compliance with State water quality objectives. The geographic focus of the paper is the western San Joaquin Valley where, in 1985, evapo-concentration of selenium in agricultural subsurface drainage water, diverted into large ponds within a federal wildlife refuge, caused teratogenecity in waterfowl embryos and in other sensitive wildlife species. The fallout from this environmental disaster was a concerted attempt by State and Federal water agencies to regulate non-point source loads of the trace element selenium. The complexity of selenium hydrogeochemistry, the difficulty and expense of selenium concentration monitoring and political discord between agricultural and environmental interests created challenges to the regulation process. Innovative policy and institutional constructs, supported by environmental monitoring and the web-based data management and dissemination systems, provided essential decision support, created opportunities for adaptive management and ultimately contributed to project success. The paper provides a retrospective on the contentious planning process and offers suggestions as to how the technical and institutional issues could have been resolved faster through early adoption of some of the core principles of sound EDSS design.
               ",unknown,1806
10.1016/j.aap.2007.09.025,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:40449097686,scopus,2008-03-01,scopus,safety improvements through intelligent transport systems: a south african case study based on microscopic simulation modelling,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=40449097686&origin=inward,"
                  Intelligent Transport Systems (ITS) can facilitate the delivery of a wide range of policy objectives. There are six main objectives/benefits identified in the international literature: Safety (reduction of (potential) crashes), mobility (reduction of delays and travel times), efficiency (optimise the use of existing infrastructure), productivity (cost saving), energy/environment and customer satisfaction [Mitretek Systems, 2001. Intelligent Transport System Benefits: 2001 update, Under Contract to the Federal Highway Administration, US Department of Transportation, Washington, DC, US]. In the South African context, there is an interest for measures that can reduce (potential) crashes. In South Africa the number of year on year traffic related fatalities is still increasing. In 2005 the number of fatalities was 15393 (from 14135 in 2004) while the estimated costs for the same period increased from R8.89-billion to R9.99-billion [RTMC, 2007. Interim Road Traffic and Fatal Crash Report 2006, Road Traffic Management Corporation, Pretoria, SA]. Given the extent of the road safety problem and the potential benefits of ITS, the need for further research is apparent. A study with regards to the potential of different types of models (macroscopic, mesoscopic and miscroscopic simulation models) led to the use of Paramics. Two corridors and three types of ITS measures were investigated and safety benefits were estimated.
               ",unknown,1812
10.1016/j.ijinfomgt.2007.12.002,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:40849133740,scopus,2008-01-01,scopus,deconstructing symbiotic dyadic e-health networks: transnational and transgenic perspectives,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=40849133740&origin=inward,"
                  The 21st century continues to witness the transformation of organizational systems globally through the deployment of Information and Communication Technologies (ICT). The health care sector is no exception to the inter-organizational dynamics driven by ICT innovations, such as artificial intelligence, biotechnology, nomadic information systems and nano-technology. This study explores inter-sectorial informational network dynamics by extending Zhu's Wu-Shi-Ren (WSR)-Li framework into the Realpolitik of e-health systems transnationally. This key informant study of the perceptions of inter-sectorial executives engaged in evolving symbiotic dyadic e-health networks in Canada, Germany, Sweden and the United Kingdom appears to support Zhu's WSR-Li model. A key finding of this study is that relational and transactional capital and transformational competencies are essential to the evolution of such e-networks, but they are not sufficient. These components must transcend the boundaries of the dyadic e-networks to engage and integrate transcendent forces in the form of public governance agents rooted in larger socio-political environments. In effect, information system professionals must acquire transcendent knowledge and skill sets to engage the “transgenic” forces, or key socio-political elements, that are critical in evolving inter-sectorial e-networks. Engaging and co-opting these transgenic forces appears to be particularly important in national health care systems, where public governance is a key socio-political value. This underscores the need for further research to explore how transgenic forces influence the evolution of a range of ICT systems inter-sectorially in different socio-political environments.
               ",unknown,1815
10.1016/j.fertnstert.2005.12.040,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:33745491691,scopus,2006-07-01,scopus,a randomized controlled trial of increasing recombinant follicle-stimulating hormone after initiating a gonadotropin-releasing hormone antagonist for in vitro fertilization-embryo transfer,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=33745491691&origin=inward,"
                  Objective
                  Pituitary suppression with a GnRH antagonist before IVF may result in a plateau or decrease in estradiol levels. We sought to investigate the effect of increasing recombinant FSH (rFSH) after starting a GnRH antagonist on estradiol levels, implantation rates, and pregnancy rates.
               
                  Design
                  Prospective, randomized multicenter study.
               
                  Setting
                  Military medical center and private practice.
               
                  Patient(s)
                  Sixty infertile women undergoing IVF who met the appropriate inclusion criteria.
               
                  Intervention(s)
                  Participants were pretreated with combined oral contraceptives (COCs) and received a dose 150–300 IU of rFSH 5 days after taking their last COC. They were randomly assigned to receive their current dose of rFSH (control group) or an additional 75 IU of rFSH (step-up group) after starting a GnRH antagonist. Daily GnRH antagonist injections were started when the lead follicles were 13–14 mm in diameter and continued until hCG was given when two follicles were ≥18 mm. One to three embryos were transferred 3 or 5 days following oocyte retrieval. Women with PCOS, a body mass index >33, a day 3 FSH >14.1 mIU/mL, or prior poor stimulation were excluded.
               
                  Main Outcome Measure(s)
                  The primary endpoints of this pilot study were embryo implantation, pregnancy, and livebirth rates. Secondary endpoints included the amount and days of rFSH; number of days of GnRH antagonist use; estradiol levels on the day of GnRH antagonist initiation, day 1 and day 2 after initiation, and on the day of hCG; endometrial stripe thickness; number of follicles; and number of oocytes.
               
                  Result(s)
                  No differences were reported within the groups with respect to age, BMI, baseline FSH, use of intracytoplasmic sperm injection, vials of rFSH, number of GnRH antagonist injections, changes in estradiol patterns, or peak estradiol level. The control and step-up groups had similar pregnancies (73.3% vs. 63.3%, P=.41), clinical pregnancies (70.0% vs. 60.0%, P=.42), live births (56.7% vs. 60.0%, P=.8), and implantation rates (50.0% and 39.1%, P=.22).
               
                  Conclusion(s)
                  The use of rFSH and a GnRH antagonist in good candidates for IVF resulted in outstanding implantation and pregnancy rates. Increasing the dose of rFSH after starting a GnRH antagonist does not alter the estradiol response or improve the implantation and pregnancy rates.
               ",unknown,1821
10.1016/j.landurbplan.2004.11.012,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:24344443322,scopus,2005-10-15,scopus,capturing impressions of pedestrian landscapes used for healing purposes with decision tree learning,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=24344443322&origin=inward,"
                  In 2002, Medicare health insurance recognized the relationship between pedestrian environments and public health by providing co-pay for health care delivered in residential land use. In this multi-disciplinary experiment, artificial intelligence (AI) and landscape architecture (LA) bridge their respective domains to measure and model the pedestrian reaction to walking environments in small town residential community in Central Texas. In the process, we gained a deeper understanding of the health motivation of walkers and their empirical relationship to various street environments that they used for health purposes. The analytical model we ultimately developed is a flexible tool that facilitates exploration of people's perceptions of the landscape, how the pedestrian landscapes are functioning in the opinion of its users, and how changes to the design of the walking domain may predictably affect physical activity levels with the associated health benefits.
                  A pilot study involving fifty-four participants and six walking environments were used in the development of an analytical model that is significantly site-specific and grass roots oriented. Participant perceptions were measured querying each participant's rating of fifty discrete environmental variables taken. This data was then analyzed using the decision tree algorithm. Our primary objective was to capture the decision-making pattern walkers engage in when deciding to walk in a particular environment specifically for health purposes and to make this available to the designers of pedestrian environments in transportation corridors. The approach gave the designers new insight into the critical variables and the not so critical variables that affected people's decision to walk for health purposes. The results from the analysis defined measurable environmental variables that form the design for pedestrian activity in the six walking environments in the study area.
                  A customized version of decision tree machine learning algorithm rules for designing good pedestrian landscapes for health purposes were extracted from the grass roots surveys. The data indicated that variables influencing the decision to walk for health purposes in the study area included weather, sound, water, light and edge of space. The analytical model derived from the discipline of artificial intelligence facilitated examining a subset of variables and manipulating of individual or group of these variables to better understand how the built environment affected decisions to walk for different purposes. This collaboration was our first phase in developing intelligent tools for designers that provided site-specific user-specific data to the planner or designer of pedestrian space.
               ",unknown,1825
10.1016/s0165-1781(03)00175-6,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:0141794338,scopus,2003-09-30,scopus,"hypothalamic-pituitary-adrenal axis responses to stress in subjects with 3,4-methylenedioxy-methamphetamine ('ecstasy') use history: correlation with dopamine receptor sensitivity",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0141794338&origin=inward,"
                  Fifteen 3,4-methylenedioxymethamphetamine (MDMA, ‘ecstasy’) users who did not have other drug dependencies or prolonged alcohol abuse and 15 control subjects were studied. All the subjects were exposed to the same psychosocial stressor (Stroop Color–Word Interference Task, public speaking and mental arithmetic in front of an audience) 3 weeks after MDMA discontinuation. Plasma concentrations of adrenocorticotropic hormone (ACTH) and cortisol were measured immediately before the tests began and at their end, 30 min later. Growth hormone (GH) responses to the dopaminergic agonist bromocriptine and psychometric measures (Tridimensional Personality Questionnaire, Minnesota Multiphasic Personality Inventory, Buss–Durkee Hostility Inventory) were also obtained 4 weeks after MDMA discontinuation for the same subjects. ACTH and cortisol basal levels were significantly higher in ecstasy users than in control subjects. In contrast, ACTH and cortisol responses to stress were significantly blunted in MDMA users. The sensitivity of dopamine D2 receptors, reflected by GH responses to bromocriptine challenge, was reduced in MDMA users compared with controls. The responsiveness of the hypothalamic–pituitary–adrenal (HPA) axis (ACTH and cortisol delta peaks) correlated directly with GH areas under curves in response to bromocriptine, and inversely with psychometric measures of aggressiveness and novelty seeking. No correlation was found between hormonal measures and the extent of MDMA exposure. Reduced D2 receptor sensitivity, HPA basal hyperactivation and reduced responsiveness to stress may represent a complex neuroendocrine dysfunction associated with MDMA use. The present findings do not exclude the possibility that dopamine dysfunction partly predated MDMA exposure.
               ",unknown,1831
10.1016/s0933-3657(97)00039-0,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:0031884634,scopus,1998-01-01,scopus,disease diagnosis validation in tropix using cbr,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0031884634&origin=inward,"
                  
                     
                        1
                        A PhD research project of the author at the Federal University of Technology, Owerri (FUTO), Nigeria, with bench work at The Center for Advanced Computer Studies, The University of Southwestern Louisiana, Lafayette, LA, USA.
                     
                     
                        1
                     TROPIX is a practical application project initially designed to help improve health care delivery in the rural/semi urban clinics and public hospitals in Nigeria due largely to limited laboratory facilities, medical doctors, and expertise. This paper is devoted to the use of case-based reasoning (CBR) paradigm in concert with statistical association-based reasoning (ABR) for disease diagnosis, validation and therapy selection components of the research. Essentially, tentative disease diagnosis arrived at by some classification method using similarity and dissimilarity aggregate functions, the matched vector functions (MVF), aided by the application of evidence ratio factors (ERF) for tied match cases is passed to the CBR model for validation by reusing past similar cases. The design and organization of the case-library using singular value decomposition (SVD) technique on the disease-attribute decision matrix to generate primary/secondary storage key clusters, as well as the use of domain-specific case-object properties that help to build a good case-base are described in some detail. The paper presents a disease case validation algorithm for appropriate data filtering and therapy selection enhancement from the new case-base.
               ",unknown,1841
10.1016/s0002-9378(11)90780-7,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:0027243085,scopus,1993-01-01,scopus,"meconium testing for cocaine metabolite: prevalence, perceptions, and pitfalls",https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0027243085&origin=inward,"
                  
                     OBJECTIVES: We determined the prevalence of prenatal cocaine use in a racially mixed sample of urban and suburban mothers and correlated its use with maternal demographics and newborn measurements.
                  
                     STUDY DESIGN: Meconium from 621 consecutive newborns delivered at two university-affiliated urban hospitals were assayed for benzoylecgonine. Maternal and infant characteristics were linked anonymously with the results. Statistical analysis included t tests, Fisher's exact test, Duncan's multiple range analysis, and analysis of covariance, with a value of p < 0.05 considered significant.
                  
                     RESULTS: We found that 3.4% of meconium samples had benzoylecgonine levels exceeding 0.1 µg/ml. Its presence was statistically correlated with maternal and neonatal characteristics. A nurse's opinion of cocaine use was correct 22% of the time.
                  
                     CONCLUSIONS: Prenatal cocaine use was statistically associated with multiparity, multigravidity, late-onset and clinic-based prenatal care, public assistance, nonwhite race, and low academic achievement. A nurse's opinion was a poor predictor of maternal cocaine use. Cocaine-exposed infants were significantly smaller, and this correlated best with nonwhite background.
               ",unknown,1842
10.1002/jps.2600811115,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:0027103848,scopus,1992-01-01,scopus,pharmacokinetic concepts in assessing intake of pentachlorophenol by rats after exposure through drinking water,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0027103848&origin=inward,"
                  The objective of this study was to predict concentrations of a toxicant in plasma after exposure to the toxicant through drinking water using basic pharmacokinetic principles. As an example, we studied pentachlorophenol (PCP), a widely used wood preservative of public health concern as an environmental pollutant. We added PCP to the drinking water (30 μg/mL) of five rats for 3 days. Blood was sampled, and water consumption was monitored every 12h on the days 1 and 2 and every 3h on day 3. After a 4-day washout, a PCP dose of 2.5 mg/kg was given intravenously, and blood was withdrawn at selected times for 2 days. PCP concentrations in plasma were measured by capillary gas chromatography. A one-compartment model with zero-order input and kinetic parameters (clearance, volume of distribution, and bioavailability) estimated after intravenous administration adequately predicted PCP concentrations in plasma during exposure to PCP. The average steady-state concentration (C
                     ss), which reflects the overall exposure, was predicted using the clearance (CL) concept [i.e., C
                     ss = (bioavailability • rate of intake)/CL] and compared with the observed value. The data for PCP demonstrate the potential utility of CL and other kinetic concepts in assessing exposure to a toxicant in drinking water, food, or air.
               ",unknown,1843
10.1016/0020-7101(75)90013-6,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',SCOPUS_ID:0016587838,scopus,1975-01-01,scopus,hypothesis processing as a new tool to aid managers of mental health agencies in serving long-term regional interests,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0016587838&origin=inward,"
                  Mental health planning is partly a political process, involving the articulation of the long-range interests of a regional community, formation of consensus among key people and the appropriate investment of authority, power and responsibility. Conflicts between the short-term self-interests of planners and the longer-term general interest usually arise. This paper claims that it is feasible to increase the expected number of cases in which a region's longer-term interest is served without radical changes in existing planning processes or ideologies.
                  The means for doing this are new kinds of information systems that serve planners as tools to increase their awareness about assumptions, hypotheses and problem representations. The conceptual and technological bases for developing such systems stem from progress in artificial intelligence in the direction of hypothesis-processing algorithms. The proposed application to mental health planning is described. Arguments are presented to show how the use of such tools would increase the likelihood that longer-term regional interests are served.
               ",unknown,1847
10.3390/su12208308,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2020-01-01T00:00:00,core,infrastructure is key to make cities sustainable,https://core.ac.uk/download/334952622.pdf,"Infrastructure is all around us: under, above, even inside our built and natural landscapes. Sometimes hidden, sometimes visible. The flows that course through them make our cities, economies, and lives possible. Cities could not even exist without infrastructure. Life is endowed with more possibilities by infrastructure. The centrality of infrastructure is pervasive. Worldwide, cities embrace infrastructure for economic competitiveness, well-being, access, environmental protection and knowledge creation. As cities are crucibles that concentrate the human condition, infrastructures are conduits that enable that concentration and empower human achievement. As infrastructures shape almost every aspect of daily life, this article assays the various ways it currently makes places both less sustainable and resilient, as well as more so, and how we can minimise the former and optimise the latter",unknown,1850
10.2826/236627,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2021-01-01T00:00:00,core,"fostering collaboration through mapping, analysing and interlinking of european entrepreneurial regions. phase ii : regional ecosystem in-depth mapping for the region of navarra",https://core.ac.uk/download/481436147.pdf,The objective of this project is to strengthen the impact of existing actions aimed at further developing start-up and scale-up support. It aims at fostering collaboration across the European Entrepreneurial Regions (EER) and developing and implementing concrete collaborative action plans that will foster scale-ups and entrepreneurs along common thematic priorities,unknown,1854
10.1080/02640414.2022.2049083,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2022-01-01T00:00:00,core,a manifesto for exercise science–a vision for improving the health of the public and planet,https://core.ac.uk/download/492527618.pdf,"In this manifesto, we make the case that Exercise Science can and must do more to improve the health of the public and the planet. Post pandemic, our vision for Exercise Science is one of a maturing scientific discipline reaching outwards from a base of strong empirical evidence to have a profound and sustained positive global impact on health. In each of the three main areas of the discipline–research, teaching, and professional practice–a new and distinctive approach is needed. We propose 12 points of action, in no particular order, for a). quality, rigour, and professional standing, and b). reach, relevance, and public engagement and make numerous suggestions for action and change. We encourage the teachers, researchers and practitioners of Exercise Science to consider and act on these recommendations. We hope that this manifesto can help create a shared sense of purpose amongst the global Exercise Science community and further the principles of equality, diversity and inclusion. To act on these principles, we need to cultivate a discipline that encourages more women, people who experience racism and other forms of discrimination, and people with a disability to become involved in the discipline",unknown,1855
10.32469/10355/93961,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,,core,spatial politics and genre in the 21st century arabic novel in english,https://core.ac.uk/download/568399392.pdf,"This dissertation is a study of four 21st century Arabic novels translated to English, each of which narrates a regionally specific process of state-sanctioned property theft. I argue that the authors of these novels use and subvert conventions from genres of fiction such as the Gothic, detective fiction, and science fiction to counter the logics of state power represented in each novel. The first chapter studies Abdo Khal's Tarmi bi Sharar (Throwing Sparks 2009), which uses Gothic conventions such as monstrosity and narrative frames to challenge the logic of authoritarian impunity that drives property theft in Jeddah, Saudi Arabia. The second chapter analyzes Raja Alem's Tawq al-Hamam (The Dove's Necklace 2010), which counters the elitist individualism at the root of conventional detective fiction with a community of detective-like characters who investigate gentrification in Mecca, Saudi Arabia. The third chapter examines Ahmed Naji's Istikhdam al-Haya (Using Life 2014) for how science fiction tropes of utopia and dystopia offer imaginative tools to rethink the logics of progress and sustainability used to justify green gentrification in Cairo, Egypt. And the fourth chapter reads Ahmed Saadawi's Frankenstein in Baghdad (2014) for how orality and polyphonic narration counter the logics of security and exceptionalism that drive the U.S. occupation of Baghdad and the Iraqi state's theft of property left behind by the displaced and the deceased.Includes bibliographical references",unknown,1857
10.1080/26395916.2020.1821095,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2020-01-01T00:00:00,core,the voices of youth in envisioning positive futures for nature and people,https://core.ac.uk/download/334954043.pdf,"The unpredictable Anthropocene poses the challenge of imagining a radically different, equitable and sustainable world. Looking 100 years ahead is not easy, and especially as millennials, it appears quite bleak. This paper is the outcome of a visioning exercise carried out in a 2-day workshop, attended by 33 young early career professionals under the auspices of IPBES. The process used Nature Futures Framework in an adapted visioning method from the Seeds of Good Anthropocene project. Four groups envisioned more desirable future worlds; where humanity has organised itself, the economy, politics and technology, to achieve improved nature-human well-being. The four visions had differing conceptualisations of this future. However, there were interesting commonalities in their leverage points for transformative change, including an emphasis on community, fundamentally different economic systems based on sharing and technological solutions to foster sustainability and human-nature connectedness. Debates included questioning the possibility of maintaining local biocultural diversity with increased connectivity globally and the prominence of technology for sustainability outcomes. These visions are the first step towards a wider galvanisation of youth visions for a brighter future, which is often missing in the arena where it can be taken seriously, to trigger more transformative pathways towards meeting global goals",unknown,1860
10.4324/9780429196515,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2020-06-10T00:00:00,core,introduction: data is not a property but a relation,https://core.ac.uk/download/372706707.pdf,"+ Data is not a property but a relation

+ Beyond data anxiety

+ Data citizens: emerging socialities and sovereignties

+ The rise and enclosure of user-generated publics

+ Dispossessing data

+ Dataism and the legitimacy of claim",unknown,1862
10.1109/mts.2023.3341463,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2024-01-22T00:00:00,core,the stuff we swim in: regulation alone will not lead to justifiable trust in ai,https://core.ac.uk/download/598038913.pdf,"Recent activity in the field of artificial intelligence (AI) has given rise to large language models (LLMs) such as GPT-4 and Bard. These are undoubtedly impressive achievements, but they raise serious questions about appropriation, accuracy, explainability, accessibility, responsibility, and more. There have been pusillanimous and self-exculpating calls for a halt in development by senior researchers in the field and largely self-serving comments by industry leaders around the potential of AI systems, good or bad. Many of these commentaries leverage misguided conceptions, in the popular imagination, of the competence of machine intelligence, based on some sort of Frankenstein or Terminator-like fiction: however, this leaves it entirely unclear what exactly the relationship between human(ity) and AI, as represented by LLMs or what comes after, is or could be",unknown,1863
10.1191/1474474005eu347oa,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2011-03-01T07:22:00,core,arts of urban exploration,https://www.ssoar.info/ssoar/bitstream/document/23257/1/ssoar-cultgeo-2005-4-pinder-arts_of_urban_exploration.pdf,"This paper addresses ways in which artists and cultural practitioners have recently                been using forms of urban exploration as a means of engaging with, and intervening                in, cities. It takes its cues from recent events on the streets of New York that                involved exploring urban spaces through artistic practices. Walks, games,                investigations and mappings are discussed as manifestations of a form of                ‘psychogeography’, and are set in the context of recent                increasing international interest in practices associated with this term, following                its earlier use by the situationists. The paper argues that experimental modes of                exploration can play a vital role in the development of critical approaches to the                cultural geographies of cities. In particular, discussion centres on the political                significance of these spatial practices, drawing out what they have to say about two                interconnected themes: ‘rights to the city’ and                ‘writing the city’. Through addressing recent cases of                psychogeographical experimentation in terms of these themes, the paper raises broad                questions about artistic practices and urban exploration to introduce this theme                issue on ‘Arts of urban exploration’ and to lead into the                specific discussions in the papers that follow",unknown,1865
10.1145/3139958.3140055,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2017-11-28T00:00:00,core,large-scale mapping of human activity using geo-tagged videos,http://arxiv.org/abs/1706.07911,"This paper is the first work to perform spatio-temporal mapping of human
activity using the visual content of geo-tagged videos. We utilize a recent
deep-learning based video analysis framework, termed hidden two-stream
networks, to recognize a range of activities in YouTube videos. This framework
is efficient and can run in real time or faster which is important for
recognizing events as they occur in streaming video or for reducing latency in
analyzing already captured video. This is, in turn, important for using video
in smart-city applications. We perform a series of experiments to show our
approach is able to accurately map activities both spatially and temporally. We
also demonstrate the advantages of using the visual content over the
tags/titles.Comment: Accepted at ACM SIGSPATIAL 201",unknown,1866
10.1145/3511808.3557153,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2022-10-17T01:00:00,core,predicting multi-level socioeconomic indicators from structural urban imagery,https://core.ac.uk/download/558722480.pdf,"Funding Information: This research has been supported in part by the National Key Research and Development Program of China under Grant 2020YFB2104005; in part by the National Natural Science Foundation of China under Grant U20B2060, and Grant U21B2036; in part by the International Postdoctoral Exchange Fellowship Program (Talent-Introduction Program) under YJ20210274; and in part by the Academy of Finland under Project 319669, Project 319670, Project 325570, Project 326305, Project 325774, and Project 335934. Publisher Copyright: © 2022 Owner/Author.Understanding economic development and designing government policies requires accurate and timely measurements of socioeconomic activities. In this paper, we show how to leverage city structural information and urban imagery like satellite images and street view images to accurately predict multi-level socioeconomic indicators. Our framework consists of four steps. First, we extract structural information from cities by transforming real-world street networks into city graphs (GeoStruct). Second, we design a contrastive learning-based model to refine urban image features by looking at geographic similarity between images, with images that are geographically close together having similar features (GeoCLR). Third, we propose using street segments as containers to adaptively fuse the features of multi-view urban images, including satellite images and street view images (GeoFuse). Finally, given the city graph with a street segment as a node and a neighborhood area as a subgraph, we jointly model street- and neighborhood-level socioeconomic indicator predictions as node and subgraph classification tasks. The novelty of our method is that we introduce city structure to organize multi-view urban images and model the relationships between socioeconomic indicators at different levels. We evaluate our framework on the basis of real-world datasets collected in multiple cities. Our proposed framework improves performance by over 10% when compared to state-of-the-art baselines in terms of prediction accuracy and recall.Peer reviewe",unknown,1869
10.19040/ecocycles.v7i1.185,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2021-01-01T00:00:00,core,design for sustainable cultural landscapes - a whole-systems framework,https://core.ac.uk/download/457643911.pdf,"This article explores how Education for Sustainable Development (ESD) could be used as a guidance framework for the capacity development of those engaged in the process of identifying, protecting, conserving, presenting and transmitting cultural landscapes. It draws insights from the Ecovillage Design Education (EDE) curriculum intended to serve the purpose of educating for the transition to a comprehensive sustainable culture; and incorporates learning objectives from the Education for Sustainable Development Goals report by UNESCO. The framework follows the pattern of the EDE curriculum organised in four dimensions of sustainability and the three dimensions of learning - cognitive, socio-emotional and behavioural. Each of these four dimensions, in turn, contains five modules- thus twenty subject areas in total, all of which need to be considered by sustainable cultural landscape educational programmes. The paper concludes that in order to create a whole-systems guidance framework addressing cultural landscape complexities, a wide variety of viewpoints needs to be considered including community, nature rights and traditional ways of knowing and other panicipatory epistemologies.</p",unknown,1877
10.14619/154-9,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2021-01-01T00:00:00,core,really fake,https://core.ac.uk/download/479582738.pdf,"With anchors in feminist theory, queer discourse, and digital politics, Really Fake rescues ""fakeness"" from the morass of ""fake news"" and rejuvenates ""fake"" as a material and tactical reality. This book treats fakeness as a media object itself: ""Fakes"" are things that travel and circulate through our bodies, sociality, and the technologies that envelop them. Punctuated with anecdotes, experiences, poetry, stories, and a strong feminist ethic and ethos of care, intimacy, and collectivity, Really Fake offers a series of entry points into reframing the debates of fakeness beyond polarized positions of performative outrage",unknown,1880
10.1017/9781108981361.005,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2022-01-01T00:00:00,core,the origins of human swarm problem solving,https://core.ac.uk/download/539602227.pdf,"Chapter 5 in Cultural-historical perspectives on collective intelligence

In the era of digital communication, collective problem solving is increasingly important. Large groups can now resolve issues together in completely different ways, which has transformed the arts, sciences, business, education, technology, and medicine. Collective intelligence is something we share with animals and is different from machine learning and artificial intelligence. To design and utilize human collective intelligence, we must understand how its problem-solving mechanisms work. From democracy in ancient Athens, through the invention of the printing press, to COVID-19, this book analyzes how humans developed the ability to find solutions together. This wide-ranging, thought-provoking book is a game-changer for those working strategically with collective problem solving within organizations and using a variety of innovative methods. It sheds light on how humans work effectively alongside machines to confront challenges that are more urgent than what humanity has faced before. This title is also available as Open Access on Cambridge Core.Chapter 5 argues that the origins of human swarm problem solving can be traced back to group hunting which required rapid problem solving during the hunt, but also planning activities. Collective actions build on synchronization in the sense that every contribution from individual hunters mattered. Another milestone was the emergence of premodern trade, which enabled human groups to utilize informational diversity from non-kin and even strangers. Knowledge was shared in new ways through large gatherings and trade networks. The third major achievement was the establishment of the first democracy in ancient Athens with institutions such as the Assembly of the People, the Council of 500 and the People`s Court. These institutions let a large number of individuals engage in rapid problem solving in a formalized manner. Individuals from all over the Athenian territory met in the city to solve societal problems. These historical examples show that human swarm problem solving is also a story about our ability to solve problems in increasingly larger groups.publishedVersio",unknown,1882
10.31390/gradschool_dissertations.495,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2002-01-01T08:00:00,core,reconceiving curriculum:  an historical approach,https://core.ac.uk/download/217400076.pdf,"This dissertation reconceives curriculum through an historical approach that employs Ludwig Wittgenstein’s later philosophy. Curriculum is more than the knowledge taught in school. Curriculum, as I a theorist conceives it, is concerned with the broader intellectual and ideological ways a society thinks about education. Hence, the current school curriculum’s focus on specific learning outcomes offers a limited view of the knowledge fashioned by a society, thereby offering an intellectual and social history that is highly selective. Wittgenstein’s concept of “language-games” offers curricularists a way to re-include some of these stories. The concept of curriculum emerges at the end of the Renaissance from Peter Ramus’s refinement of the art of dialectic into a pedagogical method of logic. The modern curriculum field arose at the end of the nineteenth century as educators sought to further refine the remnants of scholasticism’s pedagogical practices by employing “social efficiency” and scientific management to more effectively organize American education. Social efficiency and scientific management became the underlying premises of Ralph Tyler’s (1949) rationalization of the school curriculum. During the nineteen seventies, curriculum theorists began disrupting Tyler’s rational foundations by reconceptualizing curriculum using philosophies and theories developed outside of education to alter the language used to describe education. I use Wittgenstein’s later philosophy to further disrupt the school curriculum’s rational underpinnings. Wittgenstein maintains that knowing does not require some internal or external authority, thereby rejecting the empirical and logical foundations of knowledge that underlie Western education. Using a Wittgenstein approach suggests that education is an indirect activity of teaching students the use of words. Wittgenstein suggests that educating students indirectly more closely resemble the kinds of playful activities in which children engage in their ordinary lives. He suggests that learning is a synoptic presentation that connects concepts that emerge from our everyday use of language in new and interesting ways. By asking students to see the resemblances among concepts synoptically, rather than logically, education cannot be reduced to the acquisition of a set of facts, ordered in a sequence of steps. As such, a Wittgensteinian approach reconceives curriculum as an act of language-play",unknown,1885
10.33423/jabe.v23i3.4353,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2021-07-16T01:00:00,core,covid-19 and change,https://core.ac.uk/download/611815059.pdf,"The purpose of this article is to identify and describe the changes that have taken place or are taking place throughout our societal structures as a consequence of the COVID-19 pandemic crisis. It draws from published articles that have reported on the changes in many different areas of research to assemble a picture of the overall impact. It starts with the origin of the pandemic and how the different countries responded. To continue with the changes in many areas. It might change our human species, the world order, globalization, the capitalistic system, governance, civil and human rights, climate and nature, the economy, the influence of science, the food production, the Future of Work, Pensions and Universal Basic Income, the spread of cryptocurrency, the changes in Health Care, family structures and living, the way we learn, travel, religion and mass gatherings. The article discusses the coming out of the lockdown and the scenarios after coming out of the lockdown and ends with the conclusions",unknown,1890
10.36399/gla.pubs.197357,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2019-09-27T01:00:00,core,report on chancellor’s fund the green-blue-grey campus/rain garden project,https://core.ac.uk/download/228164773.pdf,"There can be a tendency in art-science projects for science discourses to instrumentalise arts practices, and for arts discourses to instrumentalise science practices, rather than for each to rigorously challenge and inform the other. 

The Chancellor’s Fund Green-Blue-Grey Campus/Rain Garden project was an attempt to get beyond this limiting instrumentalisation, and to explore new ways of working across the art-science interface in which dialogue can be generated and knowledge produced across diverse discourses, practices, communities and identities. It used innovative art, science and participatory research methodologies to explore, devise and test strategies for successfully integrating green-blue infrastructure in an urban design context, with specific reference to the sustainable design feature of the rain garden in the ongoing University of Glasgow campus redevelopment. 

 

This report introduces the Green-Blue-Grey Campus/Rain Garden project, summarises the aims and methods used, and the principle findings generated, before contextualising the work in the broader fields. It then takes a closer look at the art-science methods used, and shares some of the strategies developed. Finally the report outlines some key practical recommendations for the implementation of the rain garden, which emerged from the deployment of these innovative strategies and methods",unknown,1893
10.17613/hee0-xq82,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2019-01-01T00:00:00,core,in the wakes of rodney king: militant evidence and media activism in the age of viral black death,https://core.ac.uk/download/228319410.pdf,"This essay explores the historical and critical legacy of the Rodney King tape, namely, it’s transformation of the concerns of the field of documentary studies in the turn toward “visible evidence” in the 1990s. This turn privileged the power of visibility, particularly in radical and activist practices, but visibility is a fraught concept for minority subjects. I argue for an approach called “militant evidence” as an expanded and updated framework for media activism and the use of visible evidence. In this formulation, accumulated visible evidence is deployed within larger media and activist ecologies toward an abolition of police violence",unknown,1905
10.1163/9789004239715,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2012-01-01T00:00:00,core,"governing the empire. provincial administration in the almohad caliphate (1224-1269): critical edition, translation, and study of manuscript 4752 of the ḥasaniyya library in rabat containing 77 taqādīm (""appointments"")",https://core.ac.uk/download/31525893.pdf,"traduction anglaise Travis BruceGoverning Empire is a study, accompanied by a re-edition and a French translation, of maǧmūʿ Yaḥyá, the "" compendium of Yaḥyá "", manuscript 4752 in the Library ḥasaniyya in Rabat. This little volume, the surviving copy of which dates from the 16th-17th centuries, reproduces a formulary composed in the late 13th century by Yaḥyá al-Ḫaḏūǧ, a man of letters living at the time of the demise of the Almohad Empire in 1269. It contains 77 acts of appointment of provincial officials, governors, military chiefs, chiefs of Arab tribes, tax-collectors and judges, written between 1224 and 1269. Of this total, 73 acts (two of them identical) concern the Almohad Empire, especially the Maghrebian part, and four concern the anti-Almohad principality of Ibn Hūd al-Mutawakkil of Murcia (r. 1228-1238) in the Iberian Peninsula. The acts reproduced by Yaḥyá belong to the highly codified genre of chancery literature. Written most frequently in rhyming prose (saǧʿ) and intended for proclamation in the great mosques of the Empire, they obey rules of composition and follow rhetorical, syntactical and linguistic procedures which place them --as the compiler asserts-- in the sphere of the adab, that is literature, or more generally the culture of the ""man of good breeding"". Partaking of poetry, sermon, oratory, normative literature and religious discourse, the appointments reproduced there are the expression of a sovereign order, the Almohad imperial order, or the anti-Almohad order of the Hūdi principality of Murcia. Set down in writing and rendered anonymous through the quasi-systematic deletion of proper names, toponyms and dates, these acts were neutralized for the use of successive specialists in the language of power. Performative as they were, they came to be accepted as models and thus were absorbed into the ever-growing thesaurus of reference texts. This pragmatic collection is the last vestige of the most important indigenous authority in the history of the Maghreb. Governing Empire begins by retracing the political history of the Almohad Empire and the stages through which a territory and an authority were built up. It recalls the ideological, political and religious foundations which made Ibn Tūmart possible to unify the Maghreb and al-Andalus in the mid-12th century in the service of a dynasty of Berber origin. ʿAbd al-Mu'min (r. 1130-1162) and his descendants, the Mu'minids, mobilised the strength of the tribes of the time, Berber and Arab, to impose a dogma devised by the greatest of contemporary thinkers. Living witnesses of the islamization and arabization of the Maghreb, this dynasty resolved to reorganize the structures of power and authority to its own advantage. The Almohad sovereigns, who had assumed the title of Caliph in consonance with their pretension to guide all the peoples of Islam (umma), in the manner of the Muʿtazilite in 9th-century Iraq, claimed for themselves the authority to interpret divine law. To that end, jurists and wise men were separated from the interpretative process that the Malikite school had reserved to them since the 9th century, and they were reduced to judicial tasks or enrolled in the chancery services. The literature that the chancery produced, of which the manuscript presented, re-edited and translated here is one of the fundamental examples, plainly reveals this reversal of the relationships of authority between the religious knowledge of the ulemas and the political power of the caliphs. The organization of the ""compendium of Yaḥyá"", which is presented in the second part, throws light on the original ideological concepts predominating at the close of the Almohad era: thus, military and fiscal functions, which belong to the political order --governors, army generals, admirals of the fleet and tax collectors-- are clearly set apart from the judicial functions pertaining to the judges. Law-making devolved upon the sovereign, the sole authorized interpreter of divine law as embodied in the Qur'an and Tradition. The task of creating positive law thus rested entirely with the Caliph-imām, heir to the founder of the Almohad movement, Ibn Tūmart (d. 1130) -- the guide inspired by God, ""impeccable imām and acknowledged Mahdī"". The tasks assigned to the appointed functionaries, the counsels and orders given them, and the instructions addressed to subjects, all clearly reflect the organic conception of society and of imperial authority that characterized the Almohad ideology. That ideology was revolutionary inasmuch as it clearly departed from functionalist approaches, like that implicit in the al-Aḥkām al-Sulṭāniyya wa l-Wilāyat al-Dīniyya (""The Ordinances of Government"") of al-Māwardī (d. 1058). The careful edition and the French translation of Yaḥyá's formulary in the third part of Governing Empire give a good idea of the breadth of literary talent demanded of chancery secretaries, veritable craftsmen of language, simply to produce the decrees of power. The infinite stylistic and lexical variations combine adherence to rigid codes of chancery language with the kind of poetic and rhetorical innovations characteristic of great works of literature. This work on the language of power, at once laborious and skilled, bureaucratic and poetic, puts a voice to a specific authority --the authority of the Almohad caliphs, rooted in a particular time and place: the 13th-century Maghreb. The posthumous compilation of these performative utterances abstracts the language of power and sets Almohad history, dogma and order in the context of the corpus of timeless Islamic authorities. This formulary thus affords a glimpse of the specific nature of and the role played by administrative archives in the mediaeval Muslim world and throws light on the exceptional intricacy of Islamic imperial bureaucracies as exemplified by their chancery, the dīwān al-inšā', literally the ""bureau of creation""",unknown,1917
10.1016/j.engappai.2019.03.013,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2019-03-01T00:00:00,core,on the regulation of personal data distribution in online advertising platforms,https://core.ac.uk/download/200845411.pdf,"Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy.Peer ReviewedPostprint (author's final draft",unknown,1918
10.13109/9783666369117.63,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2009-01-01T00:00:00,core,"from roses to bullets: the rise and decline of

post-soviet colour revolutions",https://core.ac.uk/download/11309303.pdf,"The chapter explores the reasons for the colour revolutions’ successes and failures in the post-Soviet space.  The article starts with an overview on the colour movement from

the first stirrings to the present day. We then propose criteria that will be applied to our analysis, constructed on five variables. The factual analysis of individual countries that follows is built around these five variables",unknown,1920
10.2307/j.ctt1zxsk13,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,,core,gone to pitchipoi,https://core.ac.uk/download/334587756.pdf,"In Gone to Pitchipoi Katz vividly recalls his experience growing up in the turmoil of WWII, and his extraordinary escape from the constant threats of Nazi occupied Poland. Born in 1931 in the picturesque countryside of Ostrowiec Świętokrzyskie, wherein more than a third of the population was Jewish, Katz experienced a constant juxtaposition of traditional ways of life with the tragedies of those years. Deemed unfit for labor camps, Katz was marked for certain death and forced to live on the run in a daily quest for food, shelter, and friendship. He eventually reunited with his sister, Fela, together encountering a series of narrow escapes and forging on to see the day of liberation. This book is a major contribution to our understanding of the fate of Jews in small Polish towns during the Second World War",unknown,2004
10.17613/m6rn3n,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2017-01-01T00:00:00,core,exuded sap,https://core.ac.uk/download/147827675.pdf,Draft of book project written circa 2000,unknown,2009
10.22364/latgram.2021,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,2021-04-20T01:00:00,core,latvian grammar,https://core.ac.uk/download/421067331.pdf,"“Latvian Grammar” was written to make information about the Latvian language and its grammatical system more easily available not only within Latvia, but also beyond its borders. A modern grammar of Latvian written in English is as important for native speakers of Latvian as for those who have learned Latvian as a second language and also is of great value for anyone interested in the culture and history of Latvia or the Latvian language itself. The need for a reference grammar of Latvian written in English is especially important right now due to the existence of a large Latvian diaspora community abroad, particularly in English-speaking countries where children and young people are educated in the language of their home countries rather than in Latvian. A Latvian grammar written in English will also be useful for those who are learning Latvian as a foreign language and wish to learn more about its grammatical system and unique features. Likewise, “Latvian grammar” will be a useful reference and source for examples for teachers of Latvian – both those who teach it to speakers as a school or university subject and those who teach it as a foreign language. There is also considerable demand among linguists abroad for a systematic and dependable description of Latvian written by native speakers of Latvian. Latvian is a rather unique combination of ancient as well as relatively new features, which are of interest to researchers abroad and are important for the typological, cognitive, pragmatic, functional, and contrastive analysis of language.University of Latvia
State research program “Letonika – the history, languages, culture, values of Latvia",unknown,2021
10.17615/ayzm-cd62,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,1963-01-01T00:00:00,core,german verse from the 12th to the 20th century in english translation,https://core.ac.uk/download/323867631.pdf,"This anthology presents in English verse translation a selection of the best of German poetry, together with discussions of the chief authors and literary periods and brief explications of the individual poems. Taking the reader from the Minnesingers' songs of courtly love to Goethe and Rilke, this volume gives an excellent introduction to eight centuries of German poetry",unknown,2022
10.5281/zenodo.888137,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',core,core,,core,gardening according to the bible,https://core.ac.uk/download/pdf/144851289.pdf,"The why of this book “Gardening According to the Bible”

We have tried to find a book that is totally dedicated to defining the direct relationship to core biblical theology precepts to the physical plant and crop growing seasons and thus, to the seasons of our lives. 

We could not find this book and thus, we were inspired to write Gardening According to the Bible (GAB).

As you will see below, GAB is not the traditional style of book.  We purposely resisted making it like a technical document.  We wanted GAB to reflect the variations, blessings, and hardships of the garden of our lives..  Like the most beautiful of gardens and on the richest of farms, GAB is more of a feeling and a warm journey for our souls and hearts as we understand how Jesus loved the garden and used so, so many examples of the garden to teach us how to be good human beings.  We hope you will sense and feel this approach as you walk through presentation within GAB and feel that GAB is much like the garden itself where seeds and plants found their own way. 

We are sure you will find multiple logic points to debate, misspellings, poor document structure and other editing needed improvements.  But, like a summer day at the Seashore, we hope that your walk in the GAB will bring you happiness, joy and understanding",unknown,2023
