id,doi,publisher,database,url,domain,publication_date,algorithm_type,training_schema,algorithm_goal,architecture,title,abstract,status
1,unknown,Neural Computing and Applications,semantic scholar,https://www.semanticscholar.org/paper/4d8fa54a23581d052296c31ae955cf711d01210f,unknown,2018,not defined,not defined,not defined,not defined,ofdm symbol identification by an unsupervised learning system under dynamically changing channel effects,"Orthogonal frequency-division multiplexing (OFDM) is one of the most successful digital communication techniques. Nevertheless, the decrease in inter-symbol interference in quadrature amplitude modulation (QAM) over dispersive channels is still challenging. Different researches recently proposed the idea of using unsupervised learning as an alternative to the classic approaches to equalization of OFDM channels. In those purposes, the identification of a received QAM symbol is possible by the comparison of its position on the in-phase/quadrature (IQ) plane relative to the positions of previously arrived symbols, generally processed by the Kohonen’s Self-Organizing Map (SOM) algorithm. This work presents the SOM unsupervised learning method executed on an embedded system applied to QAM symbols identification. The system is implemented on an FPGA, a configurable digital circuit able to meet the low power and parallel process requirements of mobile applications. Also, in order to extend the classical set of experiments to evaluate our system, this paper proposes a theoretical model of the time-varying scheme representing the transition between different channel characteristics, obtained from real measurements available on a public repository. The model is employed to verify our purpose under dynamically both changing and realistic conditions. On the assumption that it is provided enough IQ symbols for the initial training process, the hardware implementation of SOM is able to track and identify the time-varying distorted QAM constellation. No knowledge of channel characteristics is necessary. The system spends only some microseconds at start-up to reach about 100% performance, and no dedicated training phase is needed afterward.",excluded
2,unknown,2017 11th Asian Control Conference (ASCC),semantic scholar,https://www.semanticscholar.org/paper/a09d1ea248e56f9d82346f8df07be36a4594f49d,unknown,2017,not defined,not defined,not defined,not defined,deep learning for picking point detection in dense cluster,"This paper considers the problem of picking objects in cluster. This requires the robot to reliably detect the picking point for the known or unseen objects under the environment with occlusion, disorder and a variety of objects. We present a novel pipeline to detect picking point based on deep convolutional neural network (CNN). A two-dimensional picking configuration is proposed, thus an extensive data augmentation strategy is enabled and a labeled dataset is established quickly and easily. At last, we demonstrate the implementation of our method on a real robot and show that our method can accurately detect picking point of unseen objects and achieve a pick success of 91% in cluster bin-picking scenario.",experiments
3,unknown,Sciencedirect,semantic scholar,https://www.sciencedirect.com/science/article/pii/S2352711021001837,unknown,2022,not defined,not defined,not defined,not defined,tx2_fcnn_node: an open-source ros compatible tool for monocular depth reconstruction,"We present tx2_fcnn_node a Robot Operating System (ROS) compatible tool that is aimed at seamless integration of various monocular depth reconstruction neural networks to the robotic software based on ROS (which is a de-facto standard in the area of robotics). Our tool simplifies the process of deploying, evaluating, and comparing depth reconstruction neural networks both on real robots and in simulation. We complement our software with a set of the precompiled neural networks which can be used off the shelf, with some of them being able to demonstrate near real-time performance when running onboard compact embedded platforms, e.g. Nvidia Jetson TX2, that are often used nowadays both in academia and industry.",architecture
4,unknown,,semantic scholar,https://www.semanticscholar.org/paper/f47d2bba0458de7953a58619cd4a51ce4cc19d9c,unknown,2020,not defined,not defined,not defined,not defined,acceleration techniques for energy efficient sampling based machine learning,"Deep learning algorithms based on convolutional neural networks (CNNs) have led to major improvements in accuracy for such tasks as object recognition. However, CNNs may not have sufficient robustness when presented with challenging or new scenarios (e.g, from unstructured or changing environments). Alternatively, algorithms based on Monte-Carlo sampling have been widely adapted in robotics and other areas of engineering due to their performance robustness. However, these sampling-based approaches have high computational requirements, making them unsuitable for real-time applications with tight energy constraints. In this paper, we investigate 6 degree-offreedom (6DoF) pose estimation for robot manipulation using this method, which uses rendering combined with sequential MonteCarlo sampling. While potentially very accurate, the significant computational complexity of the algorithm makes it less attractive for mobile robots, where runtime and energy consumption are tightly constrained. To address these challenges, we develop a novel hardware implementation of Monte-Carlo sampling on an FPGA with lower computational complexity and memory usage, while achieving high parallelism and modularization. Our results show 12X–21X improvements in energy efficiency over low-power and high-end GPU implementations, respectively. Moreover, we achieve real time performance without compromising accuracy.",excluded
5,unknown,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),semantic scholar,https://www.semanticscholar.org/paper/7c1949a48e36e92e9bc44906fd5589c4653f9707,unknown,2021,not defined,not defined,not defined,not defined,simultaneous semantic and collision learning for 6-dof grasp pose estimation,"Grasping in cluttered scenes has always been a great challenge for robots, due to the requirement of the ability to well understand the scene and object information. Previous works usually assume that the geometry information of the objects is available, or utilize a step-wise, multi-stage strategy to predict the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF grasp pose estimation as a simultaneous multi-task learning problem. In a unified framework, we jointly predict the feasible 6-DoF grasp poses, instance semantic segmentation, and collision information. The whole framework is jointly optimized and end-to-end differentiable. Our model is evaluated on large-scale benchmarks as well as the real robot system. On the public dataset, our method outperforms prior state-of-the-art methods by a large margin (+4.08 AP). We also demonstrate the implementation of our model on a real robotic platform and show that the robot can accurately grasp target objects in cluttered scenarios with a high success rate. Project link: https://openbyterobotics.github.io/sscl.",experiments
6,unknown,BICA 2016,semantic scholar,https://www.semanticscholar.org/paper/dcbbe1d306b7ef25c2e9c5a5ba1599d79d03909c,unknown,2016,not defined,not defined,not defined,not defined,motivational engine with autonomous sub-goal identification for the multilevel darwinist brain,"Abstract This work proposes a motivational system for an autonomous robot that guides the fulfillment of its goals in a developmental manner, discovering sub-goals not only as a way to simplify goal achievement, but as a way to acquire knowledge in an incremental, modular and reusable fashion. This system has been called MotivEn (Motivational Engine) and we have carried out its initial integration within the Multilevel Darwinist Brain (MDB) cognitive architecture. We describe here the main elements of MotivEn and how they improve the current MDB operation. Moreover, we present in detail a specific implementation of MotivEn and the application results obtained in terms of sub-goal identification when applying it in a real robot experiment with the MDB.",experiments
7,unknown,J. Intell. Manuf.,semantic scholar,https://www.semanticscholar.org/paper/f5255357b82adb03c6c6729fc939bd1b82b22a02,unknown,2020,not defined,not defined,not defined,not defined,a weighted fuzzy c-means clustering method with density peak for anomaly detection in iot-enabled manufacturing process,"Accurate anomaly detection is the premise of production process control and normal execution of production plan. The implementation of Internet of Things (IoT) provides data foundation and guarantee for real-time perception and detection of production state. Taking abundant IoT data as support, a density peak (DP)-weighted fuzzy C-means (WFCM) based clustering method is proposed to detect abnormal situations in production process. Firstly, a features correlation and redundancy measure method based on mutual information (MI) and conditional MI is proposed, unsupervised feature reduction is completed based on the principle of maximum correlation-minimum redundancy. Secondly, a DP-WFCM based clustering model is established to identify clusters with fewer samples to detect production anomalies. DP is used to obtain the initial clustering centers to solve the problem that FCM is sensitive to the initial centers and the clusters number needs to be determined manually in advance. MI-based similarities are introduced as weight coefficients to guide the clustering process, which improves convergence speed and clustering quality. Finally, a real case from an IoT enabled machining workshop is carried out to verify the accuracy and effectiveness of the proposed method in anomaly detection of manufacturing process.",experiments
8,unknown,,semantic scholar,https://www.semanticscholar.org/paper/1c9c12a5c9f3cfd9cd1eefbe818bfb92d37520ac,unknown,2021,not defined,not defined,not defined,not defined,"cloud-based mission control of usv fleet: architecture, implementation and experiments","Abstract In this paper, a cloud-based mission control architecture is proposed to achieve flexible remote access and coordinated mission control among a fleet of unmanned surface vehicles (USVs). First, a cloud-based mission control architecture that renders easy, timely and prioritized remote access to the USVs regardless of the remote operator’s location is proposed. It is achieved by leveraging remote cloud-based technology and local Operating onboard System. Decentralized property of the architecture accomplishes scalable monitoring, remote control, data acquisition and missions sharing for an USV fleet. Second, the related software interfaces are required for this task: the user interface of the remote client that is used for mission control/planning and data visualization and that is applicable across mobile robotic systems; and the back-end interface for the local USVs that bridges robotic and cloud server and provides seamless integration with the well-established Robot Operating System (ROS). ROS is nowadays, the most widely used framework for robotics developments. Furthermore, the proposed cloud-based mission control architecture is implemented on a fleet of real vehicles, H2Omni-X USVs, and the performance of the remote experimentation is demonstrated during sea trials at the Adriatic coast, Croatia, representing the practical contribution of this paper.",architecture
9,unknown,RoMoCo,semantic scholar,https://www.semanticscholar.org/paper/6e6ae08259b22ac769c9a81a62f9752eb2b9e732,unknown,2007,not defined,not defined,not defined,not defined,applying corba technology for the teleoperation of wheeeler,"In this paper, we present development of Wheeeler - the hyper mobile robot. Hyper mobile robots belong to the group of highly articulated robots, sometimes called “snake-like” or serpentine robots. Wheeeler has 7 segments driven by wheels and interconnected by 2 degrees- of-freedom joints (Fig. 28.1). This machine is expected to operate in rough terrain, traverse stairs and trenches, avoid obstacles, or climb over them, and also pass through tight spaces. Our project is in the simulation stage and currently we focus on the communication issues. Although, modeling and tests are performed in simulator (Webots 5 PRO) now, the same control software will work with real robot soon. In this paper, we shortly present the actual version of model; introduce the sensory suite and local controllers’ configuration. In the main paragraph we present the implementation of CORBA technology in client-server communication.",excluded
10,unknown,Neural Processing Letters,semantic scholar,https://www.semanticscholar.org/paper/102f3f5f75401b1a70a6d3af4cf6f5df11ecb925,unknown,2016,not defined,not defined,not defined,not defined,introducing synaptic delays in the neat algorithm to improve modelling in cognitive robotics,"This paper describes and tests an approach to improve the temporal processing capabilities of the neuroevolution of augmenting topologies (NEAT) algorithm. This algorithm is quite popular within the robotics community for the production of trained neural networks without having to determine a priori their size and topology. The main drawback of the traditional NEAT algorithm is that, even though it can implement recurrent synaptic connections, which allow it to perform some time related processing tasks, its capabilities are rather limited, especially when dealing with precise time dependent phenomena. NEAT’s ability to capture the underlying dynamics that correspond to complex time series still has a lot of room for improvement. To address this issue, the paper describes a new implementation of the NEAT algorithm that is able to generate artificial neural networks (ANNs) with trainable time delayed synapses in addition to its previous capacities. We show that this approach, called tao-NEAT improves the behavior of the neural networks obtained when dealing with complex time related processes. Several examples are presented, both dealing with the generation of ANNs that are able to produce complex theoretical signals such as chaotic signals or real data series, as in the case of the monthly number of international airline passengers or monthly $$\hbox {CO}_{2}$$CO2 concentrations. In these examples, t-NEAT clearly improves over the traditional NEAT algorithm in these tasks. A final example of the integration of this approach within a robot cognitive mechanism is also presented, showing the clear improvements it could provide in the modeling required for many cognitive processes.",experiments
11,unknown,ICIAI,semantic scholar,https://www.semanticscholar.org/paper/fab4b767e3ae70e8e04e991e8d626fe6b3184213,unknown,2021,not defined,not defined,not defined,not defined,learning navigation policies for mobile robots in deep reinforcement learning with random network distillation,"Learning navigation policies considers the task of training a model that can find collision-free paths for mobile robots, where various Deep Reinforcement Learning (DRL) methods have been applied with promising results. However, the natural reward function for the task is usually sparse, i.e., obtaining a penalty for the collision and a positive reward for arriving the target position, which makes it difficult to learn. In particular, for some complex navigation environments, it is hard to search a collision-free path by the random exploration, which leads to a rather slow learning speed and solutions with poor performance. In this paper, we propose a DRL based approach to train an end-to-end navigation planner, i.e, the policy neural network, that directly translates the local grid map and the relative goal of the robot into its moving actions. To handle the sparse reward problem, we augment the normal extrinsic reward from the environment with intrinsic reward signals measured by random network distillation (RND). In specific, the intrinsic reward is calculated by two different networks from RND, which encourages the agent to explore a state that has not been seen before. The experimental results show that by augmenting the reward function with intrinsic reward signals by RND, solutions with better performance can be learned more efficiently and more stably in our approach. We also deploy the trained model to a real robot, which can perform collision avoidance in navigation tasks without any parameter tuning. A video of our experiments can be found at https://youtu.be/b1GJrWfO8pw.",experiments
